{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeAdminManager.java",
  "functionName": "logBlockReplicationInfo",
  "functionId": "logBlockReplicationInfo___block-BlockInfo__bc-BlockCollection__srcNode-DatanodeDescriptor__num-NumberReplicas__storages-Iterable__DatanodeStorageInfo__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java",
  "functionStartLine": 356,
  "functionEndLine": 387,
  "numCommitsSeen": 265,
  "timeTaken": 17063,
  "changeHistory": [
    "c93cb6790e0f1c64efd03d859f907a0522010894",
    "fb8932a727f757b2e9c1c61a18145878d0eb77bd",
    "6f81cc0beea00843b44424417f09d8ee12cd7bae",
    "79df1e750ef558afed6d166ce225a23061b36aed",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd",
    "5644137adad30c84e40d2c4719627b3aabc73628",
    "745d04be59accf80feda0ad38efcc74ba362f2ca",
    "6d5da9484185ca9f585195d6da069b9cd5be4044",
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b",
    "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a",
    "ce68f410b05a58ad05965f32ad7f5b246b363a75",
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
    "7e8e983620f3ae3462d115972707c72b7d9cbabd",
    "f0f9a3631fe4950f5cf548f192226836925d0f05",
    "8620a99d1eea163b7505cde0a57e849b1b2a3a6f",
    "be7dd8333a7e56e732171db0781786987de03195",
    "8bd825bb6f35fd6fef397e3ccae0898bf7bed201",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "0b12cc822ddd57e6ecf4f7047f6614419c34580b",
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "c93cb6790e0f1c64efd03d859f907a0522010894": "Ymodifierchange",
    "fb8932a727f757b2e9c1c61a18145878d0eb77bd": "Ybodychange",
    "6f81cc0beea00843b44424417f09d8ee12cd7bae": "Ybodychange",
    "79df1e750ef558afed6d166ce225a23061b36aed": "Yfilerename",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ybodychange",
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd": "Ybodychange",
    "5644137adad30c84e40d2c4719627b3aabc73628": "Ybodychange",
    "745d04be59accf80feda0ad38efcc74ba362f2ca": "Ymultichange(Yparameterchange,Ybodychange)",
    "6d5da9484185ca9f585195d6da069b9cd5be4044": "Ybodychange",
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b": "Ybodychange",
    "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "ce68f410b05a58ad05965f32ad7f5b246b363a75": "Ybodychange",
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a": "Ybodychange",
    "7e8e983620f3ae3462d115972707c72b7d9cbabd": "Ybodychange",
    "f0f9a3631fe4950f5cf548f192226836925d0f05": "Ybodychange",
    "8620a99d1eea163b7505cde0a57e849b1b2a3a6f": "Ybodychange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "8bd825bb6f35fd6fef397e3ccae0898bf7bed201": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "0b12cc822ddd57e6ecf4f7047f6614419c34580b": "Ybodychange",
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a": "Yfilerename",
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13": "Yfilerename",
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c93cb6790e0f1c64efd03d859f907a0522010894": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-14854. Create improved decommission monitor implementation. Contributed by Stephen O\u0027Donnell.\n\nReviewed-by: Inigo Goiri \u003cinigoiri@apache.org\u003e\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "10/12/19 5:16 PM",
      "commitName": "c93cb6790e0f1c64efd03d859f907a0522010894",
      "commitAuthor": "Stephen O\u0027Donnell",
      "commitDateOld": "19/10/19 5:40 PM",
      "commitNameOld": "447f46d9628db54e77f88e2d109587cc7dfd6154",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 52.02,
      "commitsBetweenForRepo": 198,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,32 @@\n-  private void logBlockReplicationInfo(BlockInfo block,\n+  protected void logBlockReplicationInfo(BlockInfo block,\n       BlockCollection bc,\n       DatanodeDescriptor srcNode, NumberReplicas num,\n       Iterable\u003cDatanodeStorageInfo\u003e storages) {\n     if (!NameNode.blockStateChangeLog.isInfoEnabled()) {\n       return;\n     }\n \n     int curReplicas \u003d num.liveReplicas();\n     int curExpectedRedundancy \u003d blockManager.getExpectedRedundancyNum(block);\n     StringBuilder nodeList \u003d new StringBuilder();\n     for (DatanodeStorageInfo storage : storages) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       nodeList.append(node).append(\u0027 \u0027);\n     }\n     NameNode.blockStateChangeLog.info(\n         \"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedRedundancy + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissioned()\n         + \", decommissioning replicas: \" + num.decommissioning()\n         + \", maintenance replicas: \" + num.maintenanceReplicas()\n         + \", live entering maintenance replicas: \"\n         + num.liveEnteringMaintenanceReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress() +\n         \", Is current datanode entering maintenance: \"\n         + srcNode.isEnteringMaintenance());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void logBlockReplicationInfo(BlockInfo block,\n      BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    if (!NameNode.blockStateChangeLog.isInfoEnabled()) {\n      return;\n    }\n\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedRedundancy \u003d blockManager.getExpectedRedundancyNum(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node).append(\u0027 \u0027);\n    }\n    NameNode.blockStateChangeLog.info(\n        \"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedRedundancy + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissioned()\n        + \", decommissioning replicas: \" + num.decommissioning()\n        + \", maintenance replicas: \" + num.maintenanceReplicas()\n        + \", live entering maintenance replicas: \"\n        + num.liveEnteringMaintenanceReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress() +\n        \", Is current datanode entering maintenance: \"\n        + srcNode.isEnteringMaintenance());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java",
      "extendedDetails": {
        "oldValue": "[private]",
        "newValue": "[protected]"
      }
    },
    "fb8932a727f757b2e9c1c61a18145878d0eb77bd": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16029. Consecutive StringBuilder.append can be reused. Contributed by Ayush Saxena.\n",
      "commitDate": "11/01/19 10:54 AM",
      "commitName": "fb8932a727f757b2e9c1c61a18145878d0eb77bd",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "30/10/18 10:43 PM",
      "commitNameOld": "fac9f91b2944cee641049fffcafa6b65e0cf68f2",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 72.55,
      "commitsBetweenForRepo": 463,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,32 @@\n   private void logBlockReplicationInfo(BlockInfo block,\n       BlockCollection bc,\n       DatanodeDescriptor srcNode, NumberReplicas num,\n       Iterable\u003cDatanodeStorageInfo\u003e storages) {\n     if (!NameNode.blockStateChangeLog.isInfoEnabled()) {\n       return;\n     }\n \n     int curReplicas \u003d num.liveReplicas();\n     int curExpectedRedundancy \u003d blockManager.getExpectedRedundancyNum(block);\n     StringBuilder nodeList \u003d new StringBuilder();\n     for (DatanodeStorageInfo storage : storages) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n-      nodeList.append(node);\n-      nodeList.append(\u0027 \u0027);\n+      nodeList.append(node).append(\u0027 \u0027);\n     }\n     NameNode.blockStateChangeLog.info(\n         \"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedRedundancy + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissioned()\n         + \", decommissioning replicas: \" + num.decommissioning()\n         + \", maintenance replicas: \" + num.maintenanceReplicas()\n         + \", live entering maintenance replicas: \"\n         + num.liveEnteringMaintenanceReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress() +\n         \", Is current datanode entering maintenance: \"\n         + srcNode.isEnteringMaintenance());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void logBlockReplicationInfo(BlockInfo block,\n      BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    if (!NameNode.blockStateChangeLog.isInfoEnabled()) {\n      return;\n    }\n\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedRedundancy \u003d blockManager.getExpectedRedundancyNum(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node).append(\u0027 \u0027);\n    }\n    NameNode.blockStateChangeLog.info(\n        \"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedRedundancy + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissioned()\n        + \", decommissioning replicas: \" + num.decommissioning()\n        + \", maintenance replicas: \" + num.maintenanceReplicas()\n        + \", live entering maintenance replicas: \"\n        + num.liveEnteringMaintenanceReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress() +\n        \", Is current datanode entering maintenance: \"\n        + srcNode.isEnteringMaintenance());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java",
      "extendedDetails": {}
    },
    "6f81cc0beea00843b44424417f09d8ee12cd7bae": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13167. DatanodeAdminManager Improvements. Contributed by BELUGA BEHR.\n",
      "commitDate": "20/02/18 3:18 PM",
      "commitName": "6f81cc0beea00843b44424417f09d8ee12cd7bae",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "02/01/18 2:59 PM",
      "commitNameOld": "42a1c98597e6dba2e371510a6b2b6b1fb94e4090",
      "commitAuthorOld": "Manoj Govindassamy",
      "daysBetweenCommits": 49.01,
      "commitsBetweenForRepo": 297,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   private void logBlockReplicationInfo(BlockInfo block,\n       BlockCollection bc,\n       DatanodeDescriptor srcNode, NumberReplicas num,\n       Iterable\u003cDatanodeStorageInfo\u003e storages) {\n     if (!NameNode.blockStateChangeLog.isInfoEnabled()) {\n       return;\n     }\n \n     int curReplicas \u003d num.liveReplicas();\n     int curExpectedRedundancy \u003d blockManager.getExpectedRedundancyNum(block);\n     StringBuilder nodeList \u003d new StringBuilder();\n     for (DatanodeStorageInfo storage : storages) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       nodeList.append(node);\n-      nodeList.append(\" \");\n+      nodeList.append(\u0027 \u0027);\n     }\n     NameNode.blockStateChangeLog.info(\n         \"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedRedundancy + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissioned()\n         + \", decommissioning replicas: \" + num.decommissioning()\n         + \", maintenance replicas: \" + num.maintenanceReplicas()\n         + \", live entering maintenance replicas: \"\n         + num.liveEnteringMaintenanceReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress() +\n         \", Is current datanode entering maintenance: \"\n         + srcNode.isEnteringMaintenance());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void logBlockReplicationInfo(BlockInfo block,\n      BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    if (!NameNode.blockStateChangeLog.isInfoEnabled()) {\n      return;\n    }\n\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedRedundancy \u003d blockManager.getExpectedRedundancyNum(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node);\n      nodeList.append(\u0027 \u0027);\n    }\n    NameNode.blockStateChangeLog.info(\n        \"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedRedundancy + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissioned()\n        + \", decommissioning replicas: \" + num.decommissioning()\n        + \", maintenance replicas: \" + num.maintenanceReplicas()\n        + \", live entering maintenance replicas: \"\n        + num.liveEnteringMaintenanceReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress() +\n        \", Is current datanode entering maintenance: \"\n        + srcNode.isEnteringMaintenance());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java",
      "extendedDetails": {}
    },
    "79df1e750ef558afed6d166ce225a23061b36aed": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-9388. Decommission related code to support Maintenance State for datanodes.\n",
      "commitDate": "02/08/17 2:22 PM",
      "commitName": "79df1e750ef558afed6d166ce225a23061b36aed",
      "commitAuthor": "Manoj Govindassamy",
      "commitDateOld": "02/08/17 12:12 PM",
      "commitNameOld": "12e44e7bdaf53d3720a89d32f0cc2717241bd6b2",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 0.09,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void logBlockReplicationInfo(BlockInfo block,\n      BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    if (!NameNode.blockStateChangeLog.isInfoEnabled()) {\n      return;\n    }\n\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedRedundancy \u003d blockManager.getExpectedRedundancyNum(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    NameNode.blockStateChangeLog.info(\n        \"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedRedundancy + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissioned()\n        + \", decommissioning replicas: \" + num.decommissioning()\n        + \", maintenance replicas: \" + num.maintenanceReplicas()\n        + \", live entering maintenance replicas: \"\n        + num.liveEnteringMaintenanceReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress() +\n        \", Is current datanode entering maintenance: \"\n        + srcNode.isEnteringMaintenance());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminManager.java"
      }
    },
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "13/10/16 11:52 AM",
      "commitNameOld": "332a61fd74fd2a9874319232c583ab5d2c53ff03",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 4.25,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,33 @@\n   private void logBlockReplicationInfo(BlockInfo block,\n       BlockCollection bc,\n       DatanodeDescriptor srcNode, NumberReplicas num,\n       Iterable\u003cDatanodeStorageInfo\u003e storages) {\n     if (!NameNode.blockStateChangeLog.isInfoEnabled()) {\n       return;\n     }\n \n     int curReplicas \u003d num.liveReplicas();\n     int curExpectedRedundancy \u003d blockManager.getExpectedRedundancyNum(block);\n     StringBuilder nodeList \u003d new StringBuilder();\n     for (DatanodeStorageInfo storage : storages) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     NameNode.blockStateChangeLog.info(\n         \"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedRedundancy + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissioned()\n         + \", decommissioning replicas: \" + num.decommissioning()\n+        + \", maintenance replicas: \" + num.maintenanceReplicas()\n+        + \", live entering maintenance replicas: \"\n+        + num.liveEnteringMaintenanceReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n-        + srcNode.isDecommissionInProgress());\n+        + srcNode.isDecommissionInProgress() +\n+        \", Is current datanode entering maintenance: \"\n+        + srcNode.isEnteringMaintenance());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void logBlockReplicationInfo(BlockInfo block,\n      BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    if (!NameNode.blockStateChangeLog.isInfoEnabled()) {\n      return;\n    }\n\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedRedundancy \u003d blockManager.getExpectedRedundancyNum(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    NameNode.blockStateChangeLog.info(\n        \"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedRedundancy + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissioned()\n        + \", decommissioning replicas: \" + num.decommissioning()\n        + \", maintenance replicas: \" + num.maintenanceReplicas()\n        + \", live entering maintenance replicas: \"\n        + num.liveEnteringMaintenanceReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress() +\n        \", Is current datanode entering maintenance: \"\n        + srcNode.isEnteringMaintenance());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10236. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-3]. Contributed by Rakesh R.\n",
      "commitDate": "26/05/16 4:50 PM",
      "commitName": "8c84a2a93c22a93b4ff46dd917f6efb995675fbd",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "25/04/16 10:01 PM",
      "commitNameOld": "5865fe2bf01284993572ea60b3ec3bf8b4492818",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 30.78,
      "commitsBetweenForRepo": 218,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   private void logBlockReplicationInfo(BlockInfo block,\n       BlockCollection bc,\n       DatanodeDescriptor srcNode, NumberReplicas num,\n       Iterable\u003cDatanodeStorageInfo\u003e storages) {\n     if (!NameNode.blockStateChangeLog.isInfoEnabled()) {\n       return;\n     }\n \n     int curReplicas \u003d num.liveReplicas();\n-    int curExpectedReplicas \u003d blockManager.getExpectedReplicaNum(block);\n+    int curExpectedRedundancy \u003d blockManager.getExpectedRedundancyNum(block);\n     StringBuilder nodeList \u003d new StringBuilder();\n     for (DatanodeStorageInfo storage : storages) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     NameNode.blockStateChangeLog.info(\n         \"Block: \" + block + \", Expected Replicas: \"\n-        + curExpectedReplicas + \", live replicas: \" + curReplicas\n+        + curExpectedRedundancy + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissioned()\n         + \", decommissioning replicas: \" + num.decommissioning()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void logBlockReplicationInfo(BlockInfo block,\n      BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    if (!NameNode.blockStateChangeLog.isInfoEnabled()) {\n      return;\n    }\n\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedRedundancy \u003d blockManager.getExpectedRedundancyNum(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    NameNode.blockStateChangeLog.info(\n        \"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedRedundancy + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissioned()\n        + \", decommissioning replicas: \" + num.decommissioning()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "5644137adad30c84e40d2c4719627b3aabc73628": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9941. Do not log StandbyException on NN, other minor logging fixes. Contributed by Arpit Agarwal.\n",
      "commitDate": "14/03/16 9:54 AM",
      "commitName": "5644137adad30c84e40d2c4719627b3aabc73628",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "06/01/16 9:57 PM",
      "commitNameOld": "34cd7cd76505d01ec251e30837c94ab03319a0c1",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 67.46,
      "commitsBetweenForRepo": 463,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,28 @@\n   private void logBlockReplicationInfo(BlockInfo block,\n       BlockCollection bc,\n       DatanodeDescriptor srcNode, NumberReplicas num,\n       Iterable\u003cDatanodeStorageInfo\u003e storages) {\n+    if (!NameNode.blockStateChangeLog.isInfoEnabled()) {\n+      return;\n+    }\n+\n     int curReplicas \u003d num.liveReplicas();\n     int curExpectedReplicas \u003d blockManager.getExpectedReplicaNum(block);\n     StringBuilder nodeList \u003d new StringBuilder();\n     for (DatanodeStorageInfo storage : storages) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n-    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n+    NameNode.blockStateChangeLog.info(\n+        \"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissioned()\n         + \", decommissioning replicas: \" + num.decommissioning()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void logBlockReplicationInfo(BlockInfo block,\n      BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    if (!NameNode.blockStateChangeLog.isInfoEnabled()) {\n      return;\n    }\n\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d blockManager.getExpectedReplicaNum(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    NameNode.blockStateChangeLog.info(\n        \"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissioned()\n        + \", decommissioning replicas: \" + num.decommissioning()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "745d04be59accf80feda0ad38efcc74ba362f2ca": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8823. Move replication factor into individual blocks. Contributed by Haohui Mai.\n",
      "commitDate": "22/08/15 12:09 AM",
      "commitName": "745d04be59accf80feda0ad38efcc74ba362f2ca",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8823. Move replication factor into individual blocks. Contributed by Haohui Mai.\n",
          "commitDate": "22/08/15 12:09 AM",
          "commitName": "745d04be59accf80feda0ad38efcc74ba362f2ca",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "16/06/15 10:03 AM",
          "commitNameOld": "a3990ca41415515b986a41dacefceee1f05622f8",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 66.59,
          "commitsBetweenForRepo": 404,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,23 @@\n-  private static void logBlockReplicationInfo(Block block, BlockCollection bc,\n+  private static void logBlockReplicationInfo(BlockInfo block,\n+      BlockCollection bc,\n       DatanodeDescriptor srcNode, NumberReplicas num,\n       Iterable\u003cDatanodeStorageInfo\u003e storages) {\n     int curReplicas \u003d num.liveReplicas();\n-    int curExpectedReplicas \u003d bc.getPreferredBlockReplication();\n+    int curExpectedReplicas \u003d block.getReplication();\n     StringBuilder nodeList \u003d new StringBuilder();\n     for (DatanodeStorageInfo storage : storages) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissioned()\n         + \", decommissioning replicas: \" + num.decommissioning()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void logBlockReplicationInfo(BlockInfo block,\n      BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d block.getReplication();\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissioned()\n        + \", decommissioning replicas: \" + num.decommissioning()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
          "extendedDetails": {
            "oldValue": "[block-Block, bc-BlockCollection, srcNode-DatanodeDescriptor, num-NumberReplicas, storages-Iterable\u003cDatanodeStorageInfo\u003e]",
            "newValue": "[block-BlockInfo, bc-BlockCollection, srcNode-DatanodeDescriptor, num-NumberReplicas, storages-Iterable\u003cDatanodeStorageInfo\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8823. Move replication factor into individual blocks. Contributed by Haohui Mai.\n",
          "commitDate": "22/08/15 12:09 AM",
          "commitName": "745d04be59accf80feda0ad38efcc74ba362f2ca",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "16/06/15 10:03 AM",
          "commitNameOld": "a3990ca41415515b986a41dacefceee1f05622f8",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 66.59,
          "commitsBetweenForRepo": 404,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,23 @@\n-  private static void logBlockReplicationInfo(Block block, BlockCollection bc,\n+  private static void logBlockReplicationInfo(BlockInfo block,\n+      BlockCollection bc,\n       DatanodeDescriptor srcNode, NumberReplicas num,\n       Iterable\u003cDatanodeStorageInfo\u003e storages) {\n     int curReplicas \u003d num.liveReplicas();\n-    int curExpectedReplicas \u003d bc.getPreferredBlockReplication();\n+    int curExpectedReplicas \u003d block.getReplication();\n     StringBuilder nodeList \u003d new StringBuilder();\n     for (DatanodeStorageInfo storage : storages) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissioned()\n         + \", decommissioning replicas: \" + num.decommissioning()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void logBlockReplicationInfo(BlockInfo block,\n      BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d block.getReplication();\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissioned()\n        + \", decommissioning replicas: \" + num.decommissioning()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "6d5da9484185ca9f585195d6da069b9cd5be4044": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8255. Rename getBlockReplication to getPreferredBlockReplication. (Contributed by Zhe Zhang)\n",
      "commitDate": "12/05/15 6:29 AM",
      "commitName": "6d5da9484185ca9f585195d6da069b9cd5be4044",
      "commitAuthor": "yliu",
      "commitDateOld": "11/04/15 1:23 PM",
      "commitNameOld": "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 30.71,
      "commitsBetweenForRepo": 329,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   private static void logBlockReplicationInfo(Block block, BlockCollection bc,\n       DatanodeDescriptor srcNode, NumberReplicas num,\n       Iterable\u003cDatanodeStorageInfo\u003e storages) {\n     int curReplicas \u003d num.liveReplicas();\n-    int curExpectedReplicas \u003d bc.getBlockReplication();\n+    int curExpectedReplicas \u003d bc.getPreferredBlockReplication();\n     StringBuilder nodeList \u003d new StringBuilder();\n     for (DatanodeStorageInfo storage : storages) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissioned()\n         + \", decommissioning replicas: \" + num.decommissioning()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static void logBlockReplicationInfo(Block block, BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d bc.getPreferredBlockReplication();\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissioned()\n        + \", decommissioning replicas: \" + num.decommissioning()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7933. fsck should also report decommissioning replicas. Contributed by Xiaoyu Yao.\n",
      "commitDate": "11/04/15 1:23 PM",
      "commitName": "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b",
      "commitAuthor": "cnauroth",
      "commitDateOld": "08/04/15 3:52 PM",
      "commitNameOld": "6af0d74a75f0f58d5e92e2e91e87735b9a62bb12",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 2.9,
      "commitsBetweenForRepo": 34,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,22 @@\n   private static void logBlockReplicationInfo(Block block, BlockCollection bc,\n       DatanodeDescriptor srcNode, NumberReplicas num,\n       Iterable\u003cDatanodeStorageInfo\u003e storages) {\n     int curReplicas \u003d num.liveReplicas();\n     int curExpectedReplicas \u003d bc.getBlockReplication();\n     StringBuilder nodeList \u003d new StringBuilder();\n     for (DatanodeStorageInfo storage : storages) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n-        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n+        + \", decommissioned replicas: \" + num.decommissioned()\n+        + \", decommissioning replicas: \" + num.decommissioning()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static void logBlockReplicationInfo(Block block, BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d bc.getBlockReplication();\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissioned()\n        + \", decommissioning replicas: \" + num.decommissioning()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
      "extendedDetails": {}
    },
    "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-7411. Change decommission logic to throttle by blocks rather\nthan nodes in each interval. Contributed by Andrew Wang\n",
      "commitDate": "08/03/15 6:31 PM",
      "commitName": "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a",
      "commitAuthor": "Chris Douglas",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7411. Change decommission logic to throttle by blocks rather\nthan nodes in each interval. Contributed by Andrew Wang\n",
          "commitDate": "08/03/15 6:31 PM",
          "commitName": "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "08/03/15 2:47 PM",
          "commitNameOld": "7ce3c7635392c32f0504191ddd8417fb20509caa",
          "commitAuthorOld": "Junping Du",
          "daysBetweenCommits": 0.16,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n-      NumberReplicas num) {\n+  private static void logBlockReplicationInfo(Block block, BlockCollection bc,\n+      DatanodeDescriptor srcNode, NumberReplicas num,\n+      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n     int curReplicas \u003d num.liveReplicas();\n-    int curExpectedReplicas \u003d getReplication(block);\n-    BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n+    int curExpectedReplicas \u003d bc.getBlockReplication();\n     StringBuilder nodeList \u003d new StringBuilder();\n-    for(DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n+    for (DatanodeStorageInfo storage : storages) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissionedReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void logBlockReplicationInfo(Block block, BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d bc.getBlockReplication();\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
            "oldMethodName": "logBlockReplicationInfo",
            "newMethodName": "logBlockReplicationInfo"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7411. Change decommission logic to throttle by blocks rather\nthan nodes in each interval. Contributed by Andrew Wang\n",
          "commitDate": "08/03/15 6:31 PM",
          "commitName": "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "08/03/15 2:47 PM",
          "commitNameOld": "7ce3c7635392c32f0504191ddd8417fb20509caa",
          "commitAuthorOld": "Junping Du",
          "daysBetweenCommits": 0.16,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n-      NumberReplicas num) {\n+  private static void logBlockReplicationInfo(Block block, BlockCollection bc,\n+      DatanodeDescriptor srcNode, NumberReplicas num,\n+      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n     int curReplicas \u003d num.liveReplicas();\n-    int curExpectedReplicas \u003d getReplication(block);\n-    BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n+    int curExpectedReplicas \u003d bc.getBlockReplication();\n     StringBuilder nodeList \u003d new StringBuilder();\n-    for(DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n+    for (DatanodeStorageInfo storage : storages) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissionedReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void logBlockReplicationInfo(Block block, BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d bc.getBlockReplication();\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[private, static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7411. Change decommission logic to throttle by blocks rather\nthan nodes in each interval. Contributed by Andrew Wang\n",
          "commitDate": "08/03/15 6:31 PM",
          "commitName": "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "08/03/15 2:47 PM",
          "commitNameOld": "7ce3c7635392c32f0504191ddd8417fb20509caa",
          "commitAuthorOld": "Junping Du",
          "daysBetweenCommits": 0.16,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n-      NumberReplicas num) {\n+  private static void logBlockReplicationInfo(Block block, BlockCollection bc,\n+      DatanodeDescriptor srcNode, NumberReplicas num,\n+      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n     int curReplicas \u003d num.liveReplicas();\n-    int curExpectedReplicas \u003d getReplication(block);\n-    BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n+    int curExpectedReplicas \u003d bc.getBlockReplication();\n     StringBuilder nodeList \u003d new StringBuilder();\n-    for(DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n+    for (DatanodeStorageInfo storage : storages) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissionedReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void logBlockReplicationInfo(Block block, BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d bc.getBlockReplication();\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7411. Change decommission logic to throttle by blocks rather\nthan nodes in each interval. Contributed by Andrew Wang\n",
          "commitDate": "08/03/15 6:31 PM",
          "commitName": "6ee0d32b98bc3aa5ed42859f1325d5a14fd1722a",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "08/03/15 2:47 PM",
          "commitNameOld": "7ce3c7635392c32f0504191ddd8417fb20509caa",
          "commitAuthorOld": "Junping Du",
          "daysBetweenCommits": 0.16,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,21 @@\n-  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n-      NumberReplicas num) {\n+  private static void logBlockReplicationInfo(Block block, BlockCollection bc,\n+      DatanodeDescriptor srcNode, NumberReplicas num,\n+      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n     int curReplicas \u003d num.liveReplicas();\n-    int curExpectedReplicas \u003d getReplication(block);\n-    BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n+    int curExpectedReplicas \u003d bc.getBlockReplication();\n     StringBuilder nodeList \u003d new StringBuilder();\n-    for(DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n+    for (DatanodeStorageInfo storage : storages) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissionedReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void logBlockReplicationInfo(Block block, BlockCollection bc,\n      DatanodeDescriptor srcNode, NumberReplicas num,\n      Iterable\u003cDatanodeStorageInfo\u003e storages) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d bc.getBlockReplication();\n    StringBuilder nodeList \u003d new StringBuilder();\n    for (DatanodeStorageInfo storage : storages) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java",
          "extendedDetails": {
            "oldValue": "[block-Block, srcNode-DatanodeDescriptor, num-NumberReplicas]",
            "newValue": "[block-Block, bc-BlockCollection, srcNode-DatanodeDescriptor, num-NumberReplicas, storages-Iterable\u003cDatanodeStorageInfo\u003e]"
          }
        }
      ]
    },
    "ce68f410b05a58ad05965f32ad7f5b246b363a75": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5285. Flatten INodeFile hierarchy: Replace INodeFileUnderConstruction and INodeFileUnderConstructionWithSnapshot with FileUnderContructionFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544389 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/11/13 5:39 PM",
      "commitName": "ce68f410b05a58ad05965f32ad7f5b246b363a75",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "17/11/13 8:18 PM",
      "commitNameOld": "e3d7ef36ef7dd31b295b1f1d86a1bfa7887ca771",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 3.89,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n       NumberReplicas num) {\n     int curReplicas \u003d num.liveReplicas();\n     int curExpectedReplicas \u003d getReplication(block);\n     BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n     StringBuilder nodeList \u003d new StringBuilder();\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissionedReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n-        + \", Is Open File: \" + (bc instanceof MutableBlockCollection)\n+        + \", Is Open File: \" + bc.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n      NumberReplicas num) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d getReplication(block);\n    BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + bc.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5009. Include storage information in the LocatedBlock.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1519691 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/09/13 7:03 AM",
      "commitName": "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "27/08/13 11:30 PM",
      "commitNameOld": "5d9d702607913685eab0d8ad077040ddc82bf085",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 6.31,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,21 @@\n   private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n       NumberReplicas num) {\n     int curReplicas \u003d num.liveReplicas();\n     int curExpectedReplicas \u003d getReplication(block);\n     BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n-    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n     StringBuilder nodeList \u003d new StringBuilder();\n-    while (nodeIter.hasNext()) {\n-      DatanodeDescriptor node \u003d nodeIter.next();\n+    for(DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n+      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissionedReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + (bc instanceof MutableBlockCollection)\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n      NumberReplicas num) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d getReplication(block);\n    BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + (bc instanceof MutableBlockCollection)\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "7e8e983620f3ae3462d115972707c72b7d9cbabd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3369. Rename {get|set|add}INode(..) methods in BlockManager and BlocksMap to {get|set|add}BlockCollection(..).  Contributed by John George\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1336909 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/05/12 2:41 PM",
      "commitName": "7e8e983620f3ae3462d115972707c72b7d9cbabd",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "10/05/12 2:59 AM",
      "commitNameOld": "f1ff05bf47a7dfb670bc63e4e6e58d74f6b5b4a7",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.49,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n       NumberReplicas num) {\n     int curReplicas \u003d num.liveReplicas();\n     int curExpectedReplicas \u003d getReplication(block);\n-    BlockCollection fileINode \u003d blocksMap.getINode(block);\n+    BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n     StringBuilder nodeList \u003d new StringBuilder();\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissionedReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n-        + \", Is Open File: \" + (fileINode instanceof MutableBlockCollection)\n+        + \", Is Open File: \" + (bc instanceof MutableBlockCollection)\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n      NumberReplicas num) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d getReplication(block);\n    BlockCollection bc \u003d blocksMap.getBlockCollection(block);\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + (bc instanceof MutableBlockCollection)\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "f0f9a3631fe4950f5cf548f192226836925d0f05": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3363. Define BlockCollection and MutableBlockCollection interfaces so that INodeFile and INodeFileUnderConstruction do not have to be used in block management.  Contributed by John George\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1335304 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/05/12 5:06 PM",
      "commitName": "f0f9a3631fe4950f5cf548f192226836925d0f05",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "01/05/12 4:02 PM",
      "commitNameOld": "8620a99d1eea163b7505cde0a57e849b1b2a3a6f",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 6.04,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n       NumberReplicas num) {\n     int curReplicas \u003d num.liveReplicas();\n     int curExpectedReplicas \u003d getReplication(block);\n-    INodeFile fileINode \u003d blocksMap.getINode(block);\n+    BlockCollection fileINode \u003d blocksMap.getINode(block);\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n     StringBuilder nodeList \u003d new StringBuilder();\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissionedReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n-        + \", Is Open File: \" + fileINode.isUnderConstruction()\n+        + \", Is Open File: \" + (fileINode instanceof MutableBlockCollection)\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n      NumberReplicas num) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d getReplication(block);\n    BlockCollection fileINode \u003d blocksMap.getINode(block);\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + (fileINode instanceof MutableBlockCollection)\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "8620a99d1eea163b7505cde0a57e849b1b2a3a6f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3339. Change INode to package private.  Contributed by John George\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1332876 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/05/12 4:02 PM",
      "commitName": "8620a99d1eea163b7505cde0a57e849b1b2a3a6f",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "12/04/12 2:28 PM",
      "commitNameOld": "4f230adc13c70b09083a928b9dc65fa404e6d177",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 19.07,
      "commitsBetweenForRepo": 119,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n       NumberReplicas num) {\n     int curReplicas \u003d num.liveReplicas();\n     int curExpectedReplicas \u003d getReplication(block);\n-    INode fileINode \u003d blocksMap.getINode(block);\n+    INodeFile fileINode \u003d blocksMap.getINode(block);\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n     StringBuilder nodeList \u003d new StringBuilder();\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n       nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissionedReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + fileINode.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n      NumberReplicas num) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d getReplication(block);\n    INodeFile fileINode \u003d blocksMap.getINode(block);\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + fileINode.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "31/03/12 12:58 PM",
      "commitNameOld": "8bd825bb6f35fd6fef397e3ccae0898bf7bed201",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 1.09,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n       NumberReplicas num) {\n     int curReplicas \u003d num.liveReplicas();\n     int curExpectedReplicas \u003d getReplication(block);\n     INode fileINode \u003d blocksMap.getINode(block);\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n     StringBuilder nodeList \u003d new StringBuilder();\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n-      nodeList.append(node.getName());\n+      nodeList.append(node);\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissionedReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + fileINode.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n-        + srcNode.getName() + \", Is current datanode decommissioning: \"\n+        + srcNode + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n      NumberReplicas num) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d getReplication(block);\n    INode fileINode \u003d blocksMap.getINode(block);\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      nodeList.append(node);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + fileINode.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "8bd825bb6f35fd6fef397e3ccae0898bf7bed201": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3164. Move DatanodeInfo#hostName to DatanodeID. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1307890 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/03/12 12:58 PM",
      "commitName": "8bd825bb6f35fd6fef397e3ccae0898bf7bed201",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "01/03/12 5:32 PM",
      "commitNameOld": "7be4e5bd222c6f1c40f88ee8b24b1587e157a87e",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 29.77,
      "commitsBetweenForRepo": 173,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n       NumberReplicas num) {\n     int curReplicas \u003d num.liveReplicas();\n     int curExpectedReplicas \u003d getReplication(block);\n     INode fileINode \u003d blocksMap.getINode(block);\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n     StringBuilder nodeList \u003d new StringBuilder();\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n-      nodeList.append(node.name);\n+      nodeList.append(node.getName());\n       nodeList.append(\" \");\n     }\n     LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissionedReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + fileINode.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n-        + srcNode.name + \", Is current datanode decommissioning: \"\n+        + srcNode.getName() + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n      NumberReplicas num) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d getReplication(block);\n    INode fileINode \u003d blocksMap.getINode(block);\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      nodeList.append(node.getName());\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + fileINode.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode.getName() + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n      NumberReplicas num) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d getReplication(block);\n    INode fileINode \u003d blocksMap.getINode(block);\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      nodeList.append(node.name);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + fileINode.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode.name + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n      NumberReplicas num) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d getReplication(block);\n    INode fileINode \u003d blocksMap.getINode(block);\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      nodeList.append(node.name);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + fileINode.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode.name + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "0b12cc822ddd57e6ecf4f7047f6614419c34580b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2200.  Change FSNamesystem.LOG to package private.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151344 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/11 10:59 PM",
      "commitName": "0b12cc822ddd57e6ecf4f7047f6614419c34580b",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "26/07/11 10:46 PM",
      "commitNameOld": "969a263188f7015261719fe45fa1505121ebb80e",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n       NumberReplicas num) {\n     int curReplicas \u003d num.liveReplicas();\n     int curExpectedReplicas \u003d getReplication(block);\n     INode fileINode \u003d blocksMap.getINode(block);\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n     StringBuilder nodeList \u003d new StringBuilder();\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n       nodeList.append(node.name);\n       nodeList.append(\" \");\n     }\n-    FSNamesystem.LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n+    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n         + curExpectedReplicas + \", live replicas: \" + curReplicas\n         + \", corrupt replicas: \" + num.corruptReplicas()\n         + \", decommissioned replicas: \" + num.decommissionedReplicas()\n         + \", excess replicas: \" + num.excessReplicas()\n         + \", Is Open File: \" + fileINode.isUnderConstruction()\n         + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n         + srcNode.name + \", Is current datanode decommissioning: \"\n         + srcNode.isDecommissionInProgress());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n      NumberReplicas num) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d getReplication(block);\n    INode fileINode \u003d blocksMap.getINode(block);\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      nodeList.append(node.name);\n      nodeList.append(\" \");\n    }\n    LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + fileINode.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode.name + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 6:31 PM",
      "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "28/06/11 5:26 PM",
      "commitNameOld": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n      NumberReplicas num) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d getReplication(block);\n    INode fileINode \u003d blocksMap.getINode(block);\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      nodeList.append(node.name);\n      nodeList.append(\" \");\n    }\n    FSNamesystem.LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + fileINode.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode.name + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
        "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13": {
      "type": "Yfilerename",
      "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 5:26 PM",
      "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "28/06/11 4:57 PM",
      "commitNameOld": "d58e3efe9269efe00c309ed0e9726d2f94bcd03a",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.02,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n      NumberReplicas num) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d getReplication(block);\n    INode fileINode \u003d blocksMap.getINode(block);\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      nodeList.append(node.name);\n      nodeList.append(\" \");\n    }\n    FSNamesystem.LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + fileINode.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode.name + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java"
      }
    },
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 4:43 PM",
      "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "28/06/11 9:21 AM",
      "commitNameOld": "1834fb99f516b2f2cd5e0ab1f89d407f98a7237a",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 0.31,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n      NumberReplicas num) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d getReplication(block);\n    INode fileINode \u003d blocksMap.getINode(block);\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      nodeList.append(node.name);\n      nodeList.append(\" \");\n    }\n    FSNamesystem.LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + fileINode.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode.name + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
        "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,22 @@\n+  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n+      NumberReplicas num) {\n+    int curReplicas \u003d num.liveReplicas();\n+    int curExpectedReplicas \u003d getReplication(block);\n+    INode fileINode \u003d blocksMap.getINode(block);\n+    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n+    StringBuilder nodeList \u003d new StringBuilder();\n+    while (nodeIter.hasNext()) {\n+      DatanodeDescriptor node \u003d nodeIter.next();\n+      nodeList.append(node.name);\n+      nodeList.append(\" \");\n+    }\n+    FSNamesystem.LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n+        + curExpectedReplicas + \", live replicas: \" + curReplicas\n+        + \", corrupt replicas: \" + num.corruptReplicas()\n+        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n+        + \", excess replicas: \" + num.excessReplicas()\n+        + \", Is Open File: \" + fileINode.isUnderConstruction()\n+        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n+        + srcNode.name + \", Is current datanode decommissioning: \"\n+        + srcNode.isDecommissionInProgress());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void logBlockReplicationInfo(Block block, DatanodeDescriptor srcNode,\n      NumberReplicas num) {\n    int curReplicas \u003d num.liveReplicas();\n    int curExpectedReplicas \u003d getReplication(block);\n    INode fileINode \u003d blocksMap.getINode(block);\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(block);\n    StringBuilder nodeList \u003d new StringBuilder();\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      nodeList.append(node.name);\n      nodeList.append(\" \");\n    }\n    FSNamesystem.LOG.info(\"Block: \" + block + \", Expected Replicas: \"\n        + curExpectedReplicas + \", live replicas: \" + curReplicas\n        + \", corrupt replicas: \" + num.corruptReplicas()\n        + \", decommissioned replicas: \" + num.decommissionedReplicas()\n        + \", excess replicas: \" + num.excessReplicas()\n        + \", Is Open File: \" + fileINode.isUnderConstruction()\n        + \", Datanodes having this block: \" + nodeList + \", Current Datanode: \"\n        + srcNode.name + \", Is current datanode decommissioning: \"\n        + srcNode.isDecommissionInProgress());\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java"
    }
  }
}