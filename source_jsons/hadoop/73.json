{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "WriteCtx.java",
  "functionName": "dumpData",
  "functionId": "dumpData___dumpOut-FileOutputStream__raf-RandomAccessFile",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java",
  "functionStartLine": 150,
  "functionEndLine": 183,
  "numCommitsSeen": 12,
  "timeTaken": 1164,
  "changeHistory": [
    "6e4562b844dfbbbdc0074323900eb69ee2a3e9c2",
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53",
    "28e3d09230971b32f74284311931525cb7ad1b7c",
    "58d75576c4d2a03d4954174bc223ed0334b34fee",
    "37f587563a943a827fbff865f5302bac6d202415"
  ],
  "changeHistoryShort": {
    "6e4562b844dfbbbdc0074323900eb69ee2a3e9c2": "Ybodychange",
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53": "Ybodychange",
    "28e3d09230971b32f74284311931525cb7ad1b7c": "Ymultichange(Ymodifierchange,Ybodychange)",
    "58d75576c4d2a03d4954174bc223ed0334b34fee": "Ybodychange",
    "37f587563a943a827fbff865f5302bac6d202415": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6e4562b844dfbbbdc0074323900eb69ee2a3e9c2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9245. Fix findbugs warnings in hdfs-nfs/WriteCtx. Contributed by Mingliang Liu.\n",
      "commitDate": "10/11/15 4:35 PM",
      "commitName": "6e4562b844dfbbbdc0074323900eb69ee2a3e9c2",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "28/09/15 6:45 PM",
      "commitNameOld": "151fca5032719e561226ef278e002739073c23ec",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 42.95,
      "commitsBetweenForRepo": 349,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   long dumpData(FileOutputStream dumpOut, RandomAccessFile raf)\n       throws IOException {\n     if (dataState !\u003d DataState.ALLOW_DUMP) {\n       if (LOG.isTraceEnabled()) {\n         LOG.trace(\"No need to dump with status(replied,dataState):\" + \"(\"\n             + replied + \",\" + dataState + \")\");\n       }\n       return 0;\n     }\n \n     // Resized write should not allow dump\n-    Preconditions.checkState(originalCount \u003d\u003d INVALID_ORIGINAL_COUNT);\n+    Preconditions.checkState(getOriginalCount() \u003d\u003d INVALID_ORIGINAL_COUNT);\n \n     this.raf \u003d raf;\n     dumpFileOffset \u003d dumpOut.getChannel().position();\n     dumpOut.write(data.array(), 0, count);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"After dump, new dumpFileOffset:\" + dumpFileOffset);\n     }\n     // it is possible that while we dump the data, the data is also being\n     // written back to HDFS. After dump, if the writing back has not finished\n     // yet, we change its flag to DUMPED and set the data to null. Otherwise\n     // this WriteCtx instance should have been removed from the buffer.\n     if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n       synchronized (this) {\n         if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n           data \u003d null;\n           dataState \u003d DataState.DUMPED;\n           return count;\n         }\n       }\n     }\n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long dumpData(FileOutputStream dumpOut, RandomAccessFile raf)\n      throws IOException {\n    if (dataState !\u003d DataState.ALLOW_DUMP) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"No need to dump with status(replied,dataState):\" + \"(\"\n            + replied + \",\" + dataState + \")\");\n      }\n      return 0;\n    }\n\n    // Resized write should not allow dump\n    Preconditions.checkState(getOriginalCount() \u003d\u003d INVALID_ORIGINAL_COUNT);\n\n    this.raf \u003d raf;\n    dumpFileOffset \u003d dumpOut.getChannel().position();\n    dumpOut.write(data.array(), 0, count);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"After dump, new dumpFileOffset:\" + dumpFileOffset);\n    }\n    // it is possible that while we dump the data, the data is also being\n    // written back to HDFS. After dump, if the writing back has not finished\n    // yet, we change its flag to DUMPED and set the data to null. Otherwise\n    // this WriteCtx instance should have been removed from the buffer.\n    if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n      synchronized (this) {\n        if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n          data \u003d null;\n          dataState \u003d DataState.DUMPED;\n          return count;\n        }\n      }\n    }\n    return 0;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java",
      "extendedDetails": {}
    },
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5259. Support client which combines appended data with old data before sends it to NFS server. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529730 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/10/13 7:57 PM",
      "commitName": "caa4abd30cfc4361c7bc9f212a9092840d7c3b53",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "23/09/13 1:02 PM",
      "commitNameOld": "28e3d09230971b32f74284311931525cb7ad1b7c",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 13.29,
      "commitsBetweenForRepo": 110,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,34 @@\n   long dumpData(FileOutputStream dumpOut, RandomAccessFile raf)\n       throws IOException {\n     if (dataState !\u003d DataState.ALLOW_DUMP) {\n       if (LOG.isTraceEnabled()) {\n         LOG.trace(\"No need to dump with status(replied,dataState):\" + \"(\"\n             + replied + \",\" + dataState + \")\");\n       }\n       return 0;\n     }\n+\n+    // Resized write should not allow dump\n+    Preconditions.checkState(originalCount \u003d\u003d INVALID_ORIGINAL_COUNT);\n+\n     this.raf \u003d raf;\n     dumpFileOffset \u003d dumpOut.getChannel().position();\n-    dumpOut.write(data, 0, count);\n+    dumpOut.write(data.array(), 0, count);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"After dump, new dumpFileOffset:\" + dumpFileOffset);\n     }\n     // it is possible that while we dump the data, the data is also being\n     // written back to HDFS. After dump, if the writing back has not finished\n     // yet, we change its flag to DUMPED and set the data to null. Otherwise\n     // this WriteCtx instance should have been removed from the buffer.\n     if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n       synchronized (this) {\n         if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n           data \u003d null;\n           dataState \u003d DataState.DUMPED;\n           return count;\n         }\n       }\n     }\n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long dumpData(FileOutputStream dumpOut, RandomAccessFile raf)\n      throws IOException {\n    if (dataState !\u003d DataState.ALLOW_DUMP) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"No need to dump with status(replied,dataState):\" + \"(\"\n            + replied + \",\" + dataState + \")\");\n      }\n      return 0;\n    }\n\n    // Resized write should not allow dump\n    Preconditions.checkState(originalCount \u003d\u003d INVALID_ORIGINAL_COUNT);\n\n    this.raf \u003d raf;\n    dumpFileOffset \u003d dumpOut.getChannel().position();\n    dumpOut.write(data.array(), 0, count);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"After dump, new dumpFileOffset:\" + dumpFileOffset);\n    }\n    // it is possible that while we dump the data, the data is also being\n    // written back to HDFS. After dump, if the writing back has not finished\n    // yet, we change its flag to DUMPED and set the data to null. Otherwise\n    // this WriteCtx instance should have been removed from the buffer.\n    if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n      synchronized (this) {\n        if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n          data \u003d null;\n          dataState \u003d DataState.DUMPED;\n          return count;\n        }\n      }\n    }\n    return 0;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java",
      "extendedDetails": {}
    },
    "28e3d09230971b32f74284311931525cb7ad1b7c": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-4971. Move IO operations out of locking in OpenFileCtx. Contributed by Jing Zhao and Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1525681 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/09/13 1:02 PM",
      "commitName": "28e3d09230971b32f74284311931525cb7ad1b7c",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-4971. Move IO operations out of locking in OpenFileCtx. Contributed by Jing Zhao and Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1525681 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/09/13 1:02 PM",
          "commitName": "28e3d09230971b32f74284311931525cb7ad1b7c",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "10/07/13 10:01 AM",
          "commitNameOld": "58d75576c4d2a03d4954174bc223ed0334b34fee",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 75.13,
          "commitsBetweenForRepo": 436,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,30 @@\n-  public long dumpData(FileOutputStream dumpOut, RandomAccessFile raf)\n+  long dumpData(FileOutputStream dumpOut, RandomAccessFile raf)\n       throws IOException {\n     if (dataState !\u003d DataState.ALLOW_DUMP) {\n       if (LOG.isTraceEnabled()) {\n         LOG.trace(\"No need to dump with status(replied,dataState):\" + \"(\"\n             + replied + \",\" + dataState + \")\");\n       }\n       return 0;\n     }\n     this.raf \u003d raf;\n     dumpFileOffset \u003d dumpOut.getChannel().position();\n     dumpOut.write(data, 0, count);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"After dump, new dumpFileOffset:\" + dumpFileOffset);\n     }\n-    data \u003d null;\n-    dataState \u003d DataState.DUMPED;\n-    return count;\n+    // it is possible that while we dump the data, the data is also being\n+    // written back to HDFS. After dump, if the writing back has not finished\n+    // yet, we change its flag to DUMPED and set the data to null. Otherwise\n+    // this WriteCtx instance should have been removed from the buffer.\n+    if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n+      synchronized (this) {\n+        if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n+          data \u003d null;\n+          dataState \u003d DataState.DUMPED;\n+          return count;\n+        }\n+      }\n+    }\n+    return 0;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  long dumpData(FileOutputStream dumpOut, RandomAccessFile raf)\n      throws IOException {\n    if (dataState !\u003d DataState.ALLOW_DUMP) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"No need to dump with status(replied,dataState):\" + \"(\"\n            + replied + \",\" + dataState + \")\");\n      }\n      return 0;\n    }\n    this.raf \u003d raf;\n    dumpFileOffset \u003d dumpOut.getChannel().position();\n    dumpOut.write(data, 0, count);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"After dump, new dumpFileOffset:\" + dumpFileOffset);\n    }\n    // it is possible that while we dump the data, the data is also being\n    // written back to HDFS. After dump, if the writing back has not finished\n    // yet, we change its flag to DUMPED and set the data to null. Otherwise\n    // this WriteCtx instance should have been removed from the buffer.\n    if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n      synchronized (this) {\n        if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n          data \u003d null;\n          dataState \u003d DataState.DUMPED;\n          return count;\n        }\n      }\n    }\n    return 0;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4971. Move IO operations out of locking in OpenFileCtx. Contributed by Jing Zhao and Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1525681 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/09/13 1:02 PM",
          "commitName": "28e3d09230971b32f74284311931525cb7ad1b7c",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "10/07/13 10:01 AM",
          "commitNameOld": "58d75576c4d2a03d4954174bc223ed0334b34fee",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 75.13,
          "commitsBetweenForRepo": 436,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,30 @@\n-  public long dumpData(FileOutputStream dumpOut, RandomAccessFile raf)\n+  long dumpData(FileOutputStream dumpOut, RandomAccessFile raf)\n       throws IOException {\n     if (dataState !\u003d DataState.ALLOW_DUMP) {\n       if (LOG.isTraceEnabled()) {\n         LOG.trace(\"No need to dump with status(replied,dataState):\" + \"(\"\n             + replied + \",\" + dataState + \")\");\n       }\n       return 0;\n     }\n     this.raf \u003d raf;\n     dumpFileOffset \u003d dumpOut.getChannel().position();\n     dumpOut.write(data, 0, count);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"After dump, new dumpFileOffset:\" + dumpFileOffset);\n     }\n-    data \u003d null;\n-    dataState \u003d DataState.DUMPED;\n-    return count;\n+    // it is possible that while we dump the data, the data is also being\n+    // written back to HDFS. After dump, if the writing back has not finished\n+    // yet, we change its flag to DUMPED and set the data to null. Otherwise\n+    // this WriteCtx instance should have been removed from the buffer.\n+    if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n+      synchronized (this) {\n+        if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n+          data \u003d null;\n+          dataState \u003d DataState.DUMPED;\n+          return count;\n+        }\n+      }\n+    }\n+    return 0;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  long dumpData(FileOutputStream dumpOut, RandomAccessFile raf)\n      throws IOException {\n    if (dataState !\u003d DataState.ALLOW_DUMP) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"No need to dump with status(replied,dataState):\" + \"(\"\n            + replied + \",\" + dataState + \")\");\n      }\n      return 0;\n    }\n    this.raf \u003d raf;\n    dumpFileOffset \u003d dumpOut.getChannel().position();\n    dumpOut.write(data, 0, count);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"After dump, new dumpFileOffset:\" + dumpFileOffset);\n    }\n    // it is possible that while we dump the data, the data is also being\n    // written back to HDFS. After dump, if the writing back has not finished\n    // yet, we change its flag to DUMPED and set the data to null. Otherwise\n    // this WriteCtx instance should have been removed from the buffer.\n    if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n      synchronized (this) {\n        if (dataState \u003d\u003d DataState.ALLOW_DUMP) {\n          data \u003d null;\n          dataState \u003d DataState.DUMPED;\n          return count;\n        }\n      }\n    }\n    return 0;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java",
          "extendedDetails": {}
        }
      ]
    },
    "58d75576c4d2a03d4954174bc223ed0334b34fee": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4962. Use enum for nfs constants. Contributed by Tsz Wo (Nicholas) SZE.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1501851 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/07/13 10:01 AM",
      "commitName": "58d75576c4d2a03d4954174bc223ed0334b34fee",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "02/07/13 10:31 AM",
      "commitNameOld": "37f587563a943a827fbff865f5302bac6d202415",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 7.98,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   public long dumpData(FileOutputStream dumpOut, RandomAccessFile raf)\n       throws IOException {\n-    if (dataState !\u003d ALLOW_DUMP) {\n+    if (dataState !\u003d DataState.ALLOW_DUMP) {\n       if (LOG.isTraceEnabled()) {\n         LOG.trace(\"No need to dump with status(replied,dataState):\" + \"(\"\n             + replied + \",\" + dataState + \")\");\n       }\n       return 0;\n     }\n     this.raf \u003d raf;\n     dumpFileOffset \u003d dumpOut.getChannel().position();\n     dumpOut.write(data, 0, count);\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"After dump, new dumpFileOffset:\" + dumpFileOffset);\n     }\n     data \u003d null;\n-    dataState \u003d DUMPED;\n+    dataState \u003d DataState.DUMPED;\n     return count;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public long dumpData(FileOutputStream dumpOut, RandomAccessFile raf)\n      throws IOException {\n    if (dataState !\u003d DataState.ALLOW_DUMP) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"No need to dump with status(replied,dataState):\" + \"(\"\n            + replied + \",\" + dataState + \")\");\n      }\n      return 0;\n    }\n    this.raf \u003d raf;\n    dumpFileOffset \u003d dumpOut.getChannel().position();\n    dumpOut.write(data, 0, count);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"After dump, new dumpFileOffset:\" + dumpFileOffset);\n    }\n    data \u003d null;\n    dataState \u003d DataState.DUMPED;\n    return count;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java",
      "extendedDetails": {}
    },
    "37f587563a943a827fbff865f5302bac6d202415": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4762 Provide HDFS based NFSv3 and Mountd implementation. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1499029 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/07/13 10:31 AM",
      "commitName": "37f587563a943a827fbff865f5302bac6d202415",
      "commitAuthor": "Brandon Li",
      "diff": "@@ -0,0 +1,19 @@\n+  public long dumpData(FileOutputStream dumpOut, RandomAccessFile raf)\n+      throws IOException {\n+    if (dataState !\u003d ALLOW_DUMP) {\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(\"No need to dump with status(replied,dataState):\" + \"(\"\n+            + replied + \",\" + dataState + \")\");\n+      }\n+      return 0;\n+    }\n+    this.raf \u003d raf;\n+    dumpFileOffset \u003d dumpOut.getChannel().position();\n+    dumpOut.write(data, 0, count);\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"After dump, new dumpFileOffset:\" + dumpFileOffset);\n+    }\n+    data \u003d null;\n+    dataState \u003d DUMPED;\n+    return count;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public long dumpData(FileOutputStream dumpOut, RandomAccessFile raf)\n      throws IOException {\n    if (dataState !\u003d ALLOW_DUMP) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"No need to dump with status(replied,dataState):\" + \"(\"\n            + replied + \",\" + dataState + \")\");\n      }\n      return 0;\n    }\n    this.raf \u003d raf;\n    dumpFileOffset \u003d dumpOut.getChannel().position();\n    dumpOut.write(data, 0, count);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"After dump, new dumpFileOffset:\" + dumpFileOffset);\n    }\n    data \u003d null;\n    dataState \u003d DUMPED;\n    return count;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteCtx.java"
    }
  }
}