{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DirectoryScanner.java",
  "functionName": "getVolumeReports",
  "functionId": "getVolumeReports",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
  "functionStartLine": 597,
  "functionEndLine": 632,
  "numCommitsSeen": 109,
  "timeTaken": 9369,
  "changeHistory": [
    "1dc0adfac0ee4821c67366728c70be9b59477b0f",
    "cc933cba77c147153e463415fc192cee2d53a1ef",
    "e1a28f95b8ffcb86300148f10a23b710f8388341",
    "f67149ab08bb49381def6c535ab4c4610e0a4221",
    "7a3c381b39887a02e944fa98287afd0eb4db3560",
    "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1",
    "4f75b15628a76881efc39054612dc128e23d27be",
    "a75673cbd88215afd98e9d6ac31a9d93062048eb",
    "662b1887af4e39f3eadd7dda4953c7f2529b43bc",
    "9e31bf675dd92183a9a74a66b7caf1a080581d65",
    "b6ffb08a467f1b5bc89e5114c462c3117b337be6",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "1dc0adfac0ee4821c67366728c70be9b59477b0f": "Ymultichange(Yrename,Yreturntypechange,Ybodychange)",
    "cc933cba77c147153e463415fc192cee2d53a1ef": "Ymodifierchange",
    "e1a28f95b8ffcb86300148f10a23b710f8388341": "Ybodychange",
    "f67149ab08bb49381def6c535ab4c4610e0a4221": "Ybodychange",
    "7a3c381b39887a02e944fa98287afd0eb4db3560": "Ybodychange",
    "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1": "Ybodychange",
    "4f75b15628a76881efc39054612dc128e23d27be": "Ybodychange",
    "a75673cbd88215afd98e9d6ac31a9d93062048eb": "Ybodychange",
    "662b1887af4e39f3eadd7dda4953c7f2529b43bc": "Ybodychange",
    "9e31bf675dd92183a9a74a66b7caf1a080581d65": "Ybodychange",
    "b6ffb08a467f1b5bc89e5114c462c3117b337be6": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "1dc0adfac0ee4821c67366728c70be9b59477b0f": {
      "type": "Ymultichange(Yrename,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-13947. Review of DirectoryScanner Class. Contributed by BELUGA BEHR.\n",
      "commitDate": "03/10/18 11:19 AM",
      "commitName": "1dc0adfac0ee4821c67366728c70be9b59477b0f",
      "commitAuthor": "Inigo Goiri",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-13947. Review of DirectoryScanner Class. Contributed by BELUGA BEHR.\n",
          "commitDate": "03/10/18 11:19 AM",
          "commitName": "1dc0adfac0ee4821c67366728c70be9b59477b0f",
          "commitAuthor": "Inigo Goiri",
          "commitDateOld": "06/09/18 2:48 PM",
          "commitNameOld": "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
          "commitAuthorOld": "Giovanni Matteo Fumarola",
          "daysBetweenCommits": 26.86,
          "commitsBetweenForRepo": 283,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,36 @@\n-  public Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n-    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n-    ScanInfoPerBlockPool[] dirReports \u003d null;\n+  public Collection\u003cScanInfoVolumeReport\u003e getVolumeReports() {\n+    List\u003cScanInfoVolumeReport\u003e volReports \u003d new ArrayList\u003c\u003e();\n+    List\u003cFuture\u003cScanInfoVolumeReport\u003e\u003e compilersInProgress \u003d new ArrayList\u003c\u003e();\n+\n     // First get list of data directories\n     try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n         dataset.getFsVolumeReferences()) {\n \n-      // Use an array since the threads may return out of order and\n-      // compilersInProgress#keySet may return out of order as well.\n-      dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n-\n-      Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n-          new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n-\n-      for (int i \u003d 0; i \u003c volumes.size(); i++) {\n-        if (volumes.get(i).getStorageType() \u003d\u003d StorageType.PROVIDED) {\n-          // Disable scanning PROVIDED volumes to keep overhead low\n-          continue;\n+      for (final FsVolumeSpi volume : volumes) {\n+        // Disable scanning PROVIDED volumes to keep overhead low\n+        if (volume.getStorageType() !\u003d StorageType.PROVIDED) {\n+          ReportCompiler reportCompiler \u003d new ReportCompiler(volume);\n+          Future\u003cScanInfoVolumeReport\u003e result \u003d\n+              reportCompileThreadPool.submit(reportCompiler);\n+          compilersInProgress.add(result);\n         }\n-        ReportCompiler reportCompiler \u003d\n-            new ReportCompiler(datanode, volumes.get(i));\n-        Future\u003cScanInfoPerBlockPool\u003e result \u003d\n-            reportCompileThreadPool.submit(reportCompiler);\n-        compilersInProgress.put(i, result);\n       }\n \n-      for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n-          compilersInProgress.entrySet()) {\n-        Integer index \u003d report.getKey();\n+      for (Future\u003cScanInfoVolumeReport\u003e future : compilersInProgress) {\n         try {\n-          dirReports[index] \u003d report.getValue().get();\n-\n-          // If our compiler threads were interrupted, give up on this run\n-          if (dirReports[index] \u003d\u003d null) {\n-            dirReports \u003d null;\n+          final ScanInfoVolumeReport result \u003d future.get();\n+          if (!CollectionUtils.addIgnoreNull(volReports, result)) {\n+            // This compiler thread were interrupted, give up on this run\n+            volReports.clear();\n             break;\n           }\n         } catch (Exception ex) {\n-          FsVolumeSpi fsVolumeSpi \u003d volumes.get(index);\n-          LOG.error(\"Error compiling report for the volume, StorageId: \"\n-              + fsVolumeSpi.getStorageID(), ex);\n-          // Continue scanning the other volumes\n+          LOG.warn(\"Error compiling report. Continuing.\", ex);\n         }\n       }\n     } catch (IOException e) {\n       LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n     }\n-    if (dirReports !\u003d null) {\n-      // Compile consolidated report for all the volumes\n-      for (ScanInfoPerBlockPool report : dirReports) {\n-        if(report !\u003d null){\n-          list.addAll(report);\n-        }\n-      }\n-    }\n-    return list.toSortedArrays();\n+\n+    return volReports;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public Collection\u003cScanInfoVolumeReport\u003e getVolumeReports() {\n    List\u003cScanInfoVolumeReport\u003e volReports \u003d new ArrayList\u003c\u003e();\n    List\u003cFuture\u003cScanInfoVolumeReport\u003e\u003e compilersInProgress \u003d new ArrayList\u003c\u003e();\n\n    // First get list of data directories\n    try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n        dataset.getFsVolumeReferences()) {\n\n      for (final FsVolumeSpi volume : volumes) {\n        // Disable scanning PROVIDED volumes to keep overhead low\n        if (volume.getStorageType() !\u003d StorageType.PROVIDED) {\n          ReportCompiler reportCompiler \u003d new ReportCompiler(volume);\n          Future\u003cScanInfoVolumeReport\u003e result \u003d\n              reportCompileThreadPool.submit(reportCompiler);\n          compilersInProgress.add(result);\n        }\n      }\n\n      for (Future\u003cScanInfoVolumeReport\u003e future : compilersInProgress) {\n        try {\n          final ScanInfoVolumeReport result \u003d future.get();\n          if (!CollectionUtils.addIgnoreNull(volReports, result)) {\n            // This compiler thread were interrupted, give up on this run\n            volReports.clear();\n            break;\n          }\n        } catch (Exception ex) {\n          LOG.warn(\"Error compiling report. Continuing.\", ex);\n        }\n      }\n    } catch (IOException e) {\n      LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n    }\n\n    return volReports;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
          "extendedDetails": {
            "oldValue": "getDiskReport",
            "newValue": "getVolumeReports"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-13947. Review of DirectoryScanner Class. Contributed by BELUGA BEHR.\n",
          "commitDate": "03/10/18 11:19 AM",
          "commitName": "1dc0adfac0ee4821c67366728c70be9b59477b0f",
          "commitAuthor": "Inigo Goiri",
          "commitDateOld": "06/09/18 2:48 PM",
          "commitNameOld": "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
          "commitAuthorOld": "Giovanni Matteo Fumarola",
          "daysBetweenCommits": 26.86,
          "commitsBetweenForRepo": 283,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,36 @@\n-  public Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n-    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n-    ScanInfoPerBlockPool[] dirReports \u003d null;\n+  public Collection\u003cScanInfoVolumeReport\u003e getVolumeReports() {\n+    List\u003cScanInfoVolumeReport\u003e volReports \u003d new ArrayList\u003c\u003e();\n+    List\u003cFuture\u003cScanInfoVolumeReport\u003e\u003e compilersInProgress \u003d new ArrayList\u003c\u003e();\n+\n     // First get list of data directories\n     try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n         dataset.getFsVolumeReferences()) {\n \n-      // Use an array since the threads may return out of order and\n-      // compilersInProgress#keySet may return out of order as well.\n-      dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n-\n-      Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n-          new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n-\n-      for (int i \u003d 0; i \u003c volumes.size(); i++) {\n-        if (volumes.get(i).getStorageType() \u003d\u003d StorageType.PROVIDED) {\n-          // Disable scanning PROVIDED volumes to keep overhead low\n-          continue;\n+      for (final FsVolumeSpi volume : volumes) {\n+        // Disable scanning PROVIDED volumes to keep overhead low\n+        if (volume.getStorageType() !\u003d StorageType.PROVIDED) {\n+          ReportCompiler reportCompiler \u003d new ReportCompiler(volume);\n+          Future\u003cScanInfoVolumeReport\u003e result \u003d\n+              reportCompileThreadPool.submit(reportCompiler);\n+          compilersInProgress.add(result);\n         }\n-        ReportCompiler reportCompiler \u003d\n-            new ReportCompiler(datanode, volumes.get(i));\n-        Future\u003cScanInfoPerBlockPool\u003e result \u003d\n-            reportCompileThreadPool.submit(reportCompiler);\n-        compilersInProgress.put(i, result);\n       }\n \n-      for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n-          compilersInProgress.entrySet()) {\n-        Integer index \u003d report.getKey();\n+      for (Future\u003cScanInfoVolumeReport\u003e future : compilersInProgress) {\n         try {\n-          dirReports[index] \u003d report.getValue().get();\n-\n-          // If our compiler threads were interrupted, give up on this run\n-          if (dirReports[index] \u003d\u003d null) {\n-            dirReports \u003d null;\n+          final ScanInfoVolumeReport result \u003d future.get();\n+          if (!CollectionUtils.addIgnoreNull(volReports, result)) {\n+            // This compiler thread were interrupted, give up on this run\n+            volReports.clear();\n             break;\n           }\n         } catch (Exception ex) {\n-          FsVolumeSpi fsVolumeSpi \u003d volumes.get(index);\n-          LOG.error(\"Error compiling report for the volume, StorageId: \"\n-              + fsVolumeSpi.getStorageID(), ex);\n-          // Continue scanning the other volumes\n+          LOG.warn(\"Error compiling report. Continuing.\", ex);\n         }\n       }\n     } catch (IOException e) {\n       LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n     }\n-    if (dirReports !\u003d null) {\n-      // Compile consolidated report for all the volumes\n-      for (ScanInfoPerBlockPool report : dirReports) {\n-        if(report !\u003d null){\n-          list.addAll(report);\n-        }\n-      }\n-    }\n-    return list.toSortedArrays();\n+\n+    return volReports;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public Collection\u003cScanInfoVolumeReport\u003e getVolumeReports() {\n    List\u003cScanInfoVolumeReport\u003e volReports \u003d new ArrayList\u003c\u003e();\n    List\u003cFuture\u003cScanInfoVolumeReport\u003e\u003e compilersInProgress \u003d new ArrayList\u003c\u003e();\n\n    // First get list of data directories\n    try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n        dataset.getFsVolumeReferences()) {\n\n      for (final FsVolumeSpi volume : volumes) {\n        // Disable scanning PROVIDED volumes to keep overhead low\n        if (volume.getStorageType() !\u003d StorageType.PROVIDED) {\n          ReportCompiler reportCompiler \u003d new ReportCompiler(volume);\n          Future\u003cScanInfoVolumeReport\u003e result \u003d\n              reportCompileThreadPool.submit(reportCompiler);\n          compilersInProgress.add(result);\n        }\n      }\n\n      for (Future\u003cScanInfoVolumeReport\u003e future : compilersInProgress) {\n        try {\n          final ScanInfoVolumeReport result \u003d future.get();\n          if (!CollectionUtils.addIgnoreNull(volReports, result)) {\n            // This compiler thread were interrupted, give up on this run\n            volReports.clear();\n            break;\n          }\n        } catch (Exception ex) {\n          LOG.warn(\"Error compiling report. Continuing.\", ex);\n        }\n      }\n    } catch (IOException e) {\n      LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n    }\n\n    return volReports;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
          "extendedDetails": {
            "oldValue": "Map\u003cString,ScanInfo[]\u003e",
            "newValue": "Collection\u003cScanInfoVolumeReport\u003e"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13947. Review of DirectoryScanner Class. Contributed by BELUGA BEHR.\n",
          "commitDate": "03/10/18 11:19 AM",
          "commitName": "1dc0adfac0ee4821c67366728c70be9b59477b0f",
          "commitAuthor": "Inigo Goiri",
          "commitDateOld": "06/09/18 2:48 PM",
          "commitNameOld": "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
          "commitAuthorOld": "Giovanni Matteo Fumarola",
          "daysBetweenCommits": 26.86,
          "commitsBetweenForRepo": 283,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,36 @@\n-  public Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n-    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n-    ScanInfoPerBlockPool[] dirReports \u003d null;\n+  public Collection\u003cScanInfoVolumeReport\u003e getVolumeReports() {\n+    List\u003cScanInfoVolumeReport\u003e volReports \u003d new ArrayList\u003c\u003e();\n+    List\u003cFuture\u003cScanInfoVolumeReport\u003e\u003e compilersInProgress \u003d new ArrayList\u003c\u003e();\n+\n     // First get list of data directories\n     try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n         dataset.getFsVolumeReferences()) {\n \n-      // Use an array since the threads may return out of order and\n-      // compilersInProgress#keySet may return out of order as well.\n-      dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n-\n-      Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n-          new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n-\n-      for (int i \u003d 0; i \u003c volumes.size(); i++) {\n-        if (volumes.get(i).getStorageType() \u003d\u003d StorageType.PROVIDED) {\n-          // Disable scanning PROVIDED volumes to keep overhead low\n-          continue;\n+      for (final FsVolumeSpi volume : volumes) {\n+        // Disable scanning PROVIDED volumes to keep overhead low\n+        if (volume.getStorageType() !\u003d StorageType.PROVIDED) {\n+          ReportCompiler reportCompiler \u003d new ReportCompiler(volume);\n+          Future\u003cScanInfoVolumeReport\u003e result \u003d\n+              reportCompileThreadPool.submit(reportCompiler);\n+          compilersInProgress.add(result);\n         }\n-        ReportCompiler reportCompiler \u003d\n-            new ReportCompiler(datanode, volumes.get(i));\n-        Future\u003cScanInfoPerBlockPool\u003e result \u003d\n-            reportCompileThreadPool.submit(reportCompiler);\n-        compilersInProgress.put(i, result);\n       }\n \n-      for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n-          compilersInProgress.entrySet()) {\n-        Integer index \u003d report.getKey();\n+      for (Future\u003cScanInfoVolumeReport\u003e future : compilersInProgress) {\n         try {\n-          dirReports[index] \u003d report.getValue().get();\n-\n-          // If our compiler threads were interrupted, give up on this run\n-          if (dirReports[index] \u003d\u003d null) {\n-            dirReports \u003d null;\n+          final ScanInfoVolumeReport result \u003d future.get();\n+          if (!CollectionUtils.addIgnoreNull(volReports, result)) {\n+            // This compiler thread were interrupted, give up on this run\n+            volReports.clear();\n             break;\n           }\n         } catch (Exception ex) {\n-          FsVolumeSpi fsVolumeSpi \u003d volumes.get(index);\n-          LOG.error(\"Error compiling report for the volume, StorageId: \"\n-              + fsVolumeSpi.getStorageID(), ex);\n-          // Continue scanning the other volumes\n+          LOG.warn(\"Error compiling report. Continuing.\", ex);\n         }\n       }\n     } catch (IOException e) {\n       LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n     }\n-    if (dirReports !\u003d null) {\n-      // Compile consolidated report for all the volumes\n-      for (ScanInfoPerBlockPool report : dirReports) {\n-        if(report !\u003d null){\n-          list.addAll(report);\n-        }\n-      }\n-    }\n-    return list.toSortedArrays();\n+\n+    return volReports;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public Collection\u003cScanInfoVolumeReport\u003e getVolumeReports() {\n    List\u003cScanInfoVolumeReport\u003e volReports \u003d new ArrayList\u003c\u003e();\n    List\u003cFuture\u003cScanInfoVolumeReport\u003e\u003e compilersInProgress \u003d new ArrayList\u003c\u003e();\n\n    // First get list of data directories\n    try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n        dataset.getFsVolumeReferences()) {\n\n      for (final FsVolumeSpi volume : volumes) {\n        // Disable scanning PROVIDED volumes to keep overhead low\n        if (volume.getStorageType() !\u003d StorageType.PROVIDED) {\n          ReportCompiler reportCompiler \u003d new ReportCompiler(volume);\n          Future\u003cScanInfoVolumeReport\u003e result \u003d\n              reportCompileThreadPool.submit(reportCompiler);\n          compilersInProgress.add(result);\n        }\n      }\n\n      for (Future\u003cScanInfoVolumeReport\u003e future : compilersInProgress) {\n        try {\n          final ScanInfoVolumeReport result \u003d future.get();\n          if (!CollectionUtils.addIgnoreNull(volReports, result)) {\n            // This compiler thread were interrupted, give up on this run\n            volReports.clear();\n            break;\n          }\n        } catch (Exception ex) {\n          LOG.warn(\"Error compiling report. Continuing.\", ex);\n        }\n      }\n    } catch (IOException e) {\n      LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n    }\n\n    return volReports;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
          "extendedDetails": {}
        }
      ]
    },
    "cc933cba77c147153e463415fc192cee2d53a1ef": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-12685. [READ] FsVolumeImpl exception when scanning Provided storage volume\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "cc933cba77c147153e463415fc192cee2d53a1ef",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "e1a28f95b8ffcb86300148f10a23b710f8388341",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n-  private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n+  public Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n     ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n     ScanInfoPerBlockPool[] dirReports \u003d null;\n     // First get list of data directories\n     try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n         dataset.getFsVolumeReferences()) {\n \n       // Use an array since the threads may return out of order and\n       // compilersInProgress#keySet may return out of order as well.\n       dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n \n       Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n           new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n \n       for (int i \u003d 0; i \u003c volumes.size(); i++) {\n         if (volumes.get(i).getStorageType() \u003d\u003d StorageType.PROVIDED) {\n           // Disable scanning PROVIDED volumes to keep overhead low\n           continue;\n         }\n         ReportCompiler reportCompiler \u003d\n             new ReportCompiler(datanode, volumes.get(i));\n         Future\u003cScanInfoPerBlockPool\u003e result \u003d\n             reportCompileThreadPool.submit(reportCompiler);\n         compilersInProgress.put(i, result);\n       }\n \n       for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n           compilersInProgress.entrySet()) {\n         Integer index \u003d report.getKey();\n         try {\n           dirReports[index] \u003d report.getValue().get();\n \n           // If our compiler threads were interrupted, give up on this run\n           if (dirReports[index] \u003d\u003d null) {\n             dirReports \u003d null;\n             break;\n           }\n         } catch (Exception ex) {\n           FsVolumeSpi fsVolumeSpi \u003d volumes.get(index);\n           LOG.error(\"Error compiling report for the volume, StorageId: \"\n               + fsVolumeSpi.getStorageID(), ex);\n           // Continue scanning the other volumes\n         }\n       }\n     } catch (IOException e) {\n       LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n     }\n     if (dirReports !\u003d null) {\n       // Compile consolidated report for all the volumes\n       for (ScanInfoPerBlockPool report : dirReports) {\n         if(report !\u003d null){\n           list.addAll(report);\n         }\n       }\n     }\n     return list.toSortedArrays();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n    ScanInfoPerBlockPool[] dirReports \u003d null;\n    // First get list of data directories\n    try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n        dataset.getFsVolumeReferences()) {\n\n      // Use an array since the threads may return out of order and\n      // compilersInProgress#keySet may return out of order as well.\n      dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n\n      Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n          new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n\n      for (int i \u003d 0; i \u003c volumes.size(); i++) {\n        if (volumes.get(i).getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          // Disable scanning PROVIDED volumes to keep overhead low\n          continue;\n        }\n        ReportCompiler reportCompiler \u003d\n            new ReportCompiler(datanode, volumes.get(i));\n        Future\u003cScanInfoPerBlockPool\u003e result \u003d\n            reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n\n      for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n          compilersInProgress.entrySet()) {\n        Integer index \u003d report.getKey();\n        try {\n          dirReports[index] \u003d report.getValue().get();\n\n          // If our compiler threads were interrupted, give up on this run\n          if (dirReports[index] \u003d\u003d null) {\n            dirReports \u003d null;\n            break;\n          }\n        } catch (Exception ex) {\n          FsVolumeSpi fsVolumeSpi \u003d volumes.get(index);\n          LOG.error(\"Error compiling report for the volume, StorageId: \"\n              + fsVolumeSpi.getStorageID(), ex);\n          // Continue scanning the other volumes\n        }\n      }\n    } catch (IOException e) {\n      LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n    }\n    if (dirReports !\u003d null) {\n      // Compile consolidated report for all the volumes\n      for (ScanInfoPerBlockPool report : dirReports) {\n        if(report !\u003d null){\n          list.addAll(report);\n        }\n      }\n    }\n    return list.toSortedArrays();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
      "extendedDetails": {
        "oldValue": "[private]",
        "newValue": "[public]"
      }
    },
    "e1a28f95b8ffcb86300148f10a23b710f8388341": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12777. [READ] Reduce memory and CPU footprint for PROVIDED volumes.\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "e1a28f95b8ffcb86300148f10a23b710f8388341",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "b668eb91556b8c85c2b4925808ccb1f769031c20",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,57 @@\n   private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n     ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n     ScanInfoPerBlockPool[] dirReports \u003d null;\n     // First get list of data directories\n     try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n         dataset.getFsVolumeReferences()) {\n \n       // Use an array since the threads may return out of order and\n       // compilersInProgress#keySet may return out of order as well.\n       dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n \n       Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n           new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n \n       for (int i \u003d 0; i \u003c volumes.size(); i++) {\n+        if (volumes.get(i).getStorageType() \u003d\u003d StorageType.PROVIDED) {\n+          // Disable scanning PROVIDED volumes to keep overhead low\n+          continue;\n+        }\n         ReportCompiler reportCompiler \u003d\n             new ReportCompiler(datanode, volumes.get(i));\n         Future\u003cScanInfoPerBlockPool\u003e result \u003d\n             reportCompileThreadPool.submit(reportCompiler);\n         compilersInProgress.put(i, result);\n       }\n \n       for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n           compilersInProgress.entrySet()) {\n         Integer index \u003d report.getKey();\n         try {\n           dirReports[index] \u003d report.getValue().get();\n \n           // If our compiler threads were interrupted, give up on this run\n           if (dirReports[index] \u003d\u003d null) {\n             dirReports \u003d null;\n             break;\n           }\n         } catch (Exception ex) {\n           FsVolumeSpi fsVolumeSpi \u003d volumes.get(index);\n           LOG.error(\"Error compiling report for the volume, StorageId: \"\n               + fsVolumeSpi.getStorageID(), ex);\n           // Continue scanning the other volumes\n         }\n       }\n     } catch (IOException e) {\n       LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n     }\n     if (dirReports !\u003d null) {\n       // Compile consolidated report for all the volumes\n       for (ScanInfoPerBlockPool report : dirReports) {\n         if(report !\u003d null){\n           list.addAll(report);\n         }\n       }\n     }\n     return list.toSortedArrays();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n    ScanInfoPerBlockPool[] dirReports \u003d null;\n    // First get list of data directories\n    try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n        dataset.getFsVolumeReferences()) {\n\n      // Use an array since the threads may return out of order and\n      // compilersInProgress#keySet may return out of order as well.\n      dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n\n      Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n          new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n\n      for (int i \u003d 0; i \u003c volumes.size(); i++) {\n        if (volumes.get(i).getStorageType() \u003d\u003d StorageType.PROVIDED) {\n          // Disable scanning PROVIDED volumes to keep overhead low\n          continue;\n        }\n        ReportCompiler reportCompiler \u003d\n            new ReportCompiler(datanode, volumes.get(i));\n        Future\u003cScanInfoPerBlockPool\u003e result \u003d\n            reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n\n      for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n          compilersInProgress.entrySet()) {\n        Integer index \u003d report.getKey();\n        try {\n          dirReports[index] \u003d report.getValue().get();\n\n          // If our compiler threads were interrupted, give up on this run\n          if (dirReports[index] \u003d\u003d null) {\n            dirReports \u003d null;\n            break;\n          }\n        } catch (Exception ex) {\n          FsVolumeSpi fsVolumeSpi \u003d volumes.get(index);\n          LOG.error(\"Error compiling report for the volume, StorageId: \"\n              + fsVolumeSpi.getStorageID(), ex);\n          // Continue scanning the other volumes\n        }\n      }\n    } catch (IOException e) {\n      LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n    }\n    if (dirReports !\u003d null) {\n      // Compile consolidated report for all the volumes\n      for (ScanInfoPerBlockPool report : dirReports) {\n        if(report !\u003d null){\n          list.addAll(report);\n        }\n      }\n    }\n    return list.toSortedArrays();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
      "extendedDetails": {}
    },
    "f67149ab08bb49381def6c535ab4c4610e0a4221": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7764. DirectoryScanner shouldn\u0027t abort the scan if one directory had an error (Rakesh R via cmccabe)\n",
      "commitDate": "28/01/16 7:54 PM",
      "commitName": "f67149ab08bb49381def6c535ab4c4610e0a4221",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "29/09/15 2:56 PM",
      "commitNameOld": "8703301b466cbc37ef53a96a55bcf6412792d5cf",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 121.25,
      "commitsBetweenForRepo": 828,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,53 @@\n   private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n     ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n     ScanInfoPerBlockPool[] dirReports \u003d null;\n     // First get list of data directories\n     try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n         dataset.getFsVolumeReferences()) {\n \n       // Use an array since the threads may return out of order and\n       // compilersInProgress#keySet may return out of order as well.\n       dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n \n       Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n           new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n \n       for (int i \u003d 0; i \u003c volumes.size(); i++) {\n         ReportCompiler reportCompiler \u003d\n             new ReportCompiler(datanode, volumes.get(i));\n         Future\u003cScanInfoPerBlockPool\u003e result \u003d\n             reportCompileThreadPool.submit(reportCompiler);\n         compilersInProgress.put(i, result);\n       }\n \n       for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n           compilersInProgress.entrySet()) {\n+        Integer index \u003d report.getKey();\n         try {\n-          dirReports[report.getKey()] \u003d report.getValue().get();\n+          dirReports[index] \u003d report.getValue().get();\n \n           // If our compiler threads were interrupted, give up on this run\n-          if (dirReports[report.getKey()] \u003d\u003d null) {\n+          if (dirReports[index] \u003d\u003d null) {\n             dirReports \u003d null;\n             break;\n           }\n         } catch (Exception ex) {\n-          LOG.error(\"Error compiling report\", ex);\n-          // Propagate ex to DataBlockScanner to deal with\n-          throw new RuntimeException(ex);\n+          FsVolumeSpi fsVolumeSpi \u003d volumes.get(index);\n+          LOG.error(\"Error compiling report for the volume, StorageId: \"\n+              + fsVolumeSpi.getStorageID(), ex);\n+          // Continue scanning the other volumes\n         }\n       }\n     } catch (IOException e) {\n       LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n     }\n     if (dirReports !\u003d null) {\n       // Compile consolidated report for all the volumes\n       for (ScanInfoPerBlockPool report : dirReports) {\n-        list.addAll(report);\n+        if(report !\u003d null){\n+          list.addAll(report);\n+        }\n       }\n     }\n     return list.toSortedArrays();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n    ScanInfoPerBlockPool[] dirReports \u003d null;\n    // First get list of data directories\n    try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n        dataset.getFsVolumeReferences()) {\n\n      // Use an array since the threads may return out of order and\n      // compilersInProgress#keySet may return out of order as well.\n      dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n\n      Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n          new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n\n      for (int i \u003d 0; i \u003c volumes.size(); i++) {\n        ReportCompiler reportCompiler \u003d\n            new ReportCompiler(datanode, volumes.get(i));\n        Future\u003cScanInfoPerBlockPool\u003e result \u003d\n            reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n\n      for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n          compilersInProgress.entrySet()) {\n        Integer index \u003d report.getKey();\n        try {\n          dirReports[index] \u003d report.getValue().get();\n\n          // If our compiler threads were interrupted, give up on this run\n          if (dirReports[index] \u003d\u003d null) {\n            dirReports \u003d null;\n            break;\n          }\n        } catch (Exception ex) {\n          FsVolumeSpi fsVolumeSpi \u003d volumes.get(index);\n          LOG.error(\"Error compiling report for the volume, StorageId: \"\n              + fsVolumeSpi.getStorageID(), ex);\n          // Continue scanning the other volumes\n        }\n      }\n    } catch (IOException e) {\n      LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n    }\n    if (dirReports !\u003d null) {\n      // Compile consolidated report for all the volumes\n      for (ScanInfoPerBlockPool report : dirReports) {\n        if(report !\u003d null){\n          list.addAll(report);\n        }\n      }\n    }\n    return list.toSortedArrays();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
      "extendedDetails": {}
    },
    "7a3c381b39887a02e944fa98287afd0eb4db3560": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8873. Allow the directoryScanner to be rate-limited (Daniel Templeton via Colin P. McCabe)\n",
      "commitDate": "26/09/15 4:09 AM",
      "commitName": "7a3c381b39887a02e944fa98287afd0eb4db3560",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "19/05/15 10:50 AM",
      "commitNameOld": "470c87dbc6c24dd3b370f1ad9e7ab1f6dabd2080",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 129.72,
      "commitsBetweenForRepo": 819,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,49 @@\n   private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n     ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n     ScanInfoPerBlockPool[] dirReports \u003d null;\n     // First get list of data directories\n     try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n         dataset.getFsVolumeReferences()) {\n \n       // Use an array since the threads may return out of order and\n       // compilersInProgress#keySet may return out of order as well.\n       dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n \n       Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n           new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n \n       for (int i \u003d 0; i \u003c volumes.size(); i++) {\n         ReportCompiler reportCompiler \u003d\n             new ReportCompiler(datanode, volumes.get(i));\n         Future\u003cScanInfoPerBlockPool\u003e result \u003d\n             reportCompileThreadPool.submit(reportCompiler);\n         compilersInProgress.put(i, result);\n       }\n \n       for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n           compilersInProgress.entrySet()) {\n         try {\n           dirReports[report.getKey()] \u003d report.getValue().get();\n+\n+          // If our compiler threads were interrupted, give up on this run\n+          if (dirReports[report.getKey()] \u003d\u003d null) {\n+            dirReports \u003d null;\n+            break;\n+          }\n         } catch (Exception ex) {\n           LOG.error(\"Error compiling report\", ex);\n           // Propagate ex to DataBlockScanner to deal with\n           throw new RuntimeException(ex);\n         }\n       }\n     } catch (IOException e) {\n       LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n     }\n     if (dirReports !\u003d null) {\n       // Compile consolidated report for all the volumes\n       for (ScanInfoPerBlockPool report : dirReports) {\n         list.addAll(report);\n       }\n     }\n     return list.toSortedArrays();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n    ScanInfoPerBlockPool[] dirReports \u003d null;\n    // First get list of data directories\n    try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n        dataset.getFsVolumeReferences()) {\n\n      // Use an array since the threads may return out of order and\n      // compilersInProgress#keySet may return out of order as well.\n      dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n\n      Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n          new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n\n      for (int i \u003d 0; i \u003c volumes.size(); i++) {\n        ReportCompiler reportCompiler \u003d\n            new ReportCompiler(datanode, volumes.get(i));\n        Future\u003cScanInfoPerBlockPool\u003e result \u003d\n            reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n\n      for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n          compilersInProgress.entrySet()) {\n        try {\n          dirReports[report.getKey()] \u003d report.getValue().get();\n\n          // If our compiler threads were interrupted, give up on this run\n          if (dirReports[report.getKey()] \u003d\u003d null) {\n            dirReports \u003d null;\n            break;\n          }\n        } catch (Exception ex) {\n          LOG.error(\"Error compiling report\", ex);\n          // Propagate ex to DataBlockScanner to deal with\n          throw new RuntimeException(ex);\n        }\n      }\n    } catch (IOException e) {\n      LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n    }\n    if (dirReports !\u003d null) {\n      // Compile consolidated report for all the volumes\n      for (ScanInfoPerBlockPool report : dirReports) {\n        list.addAll(report);\n      }\n    }\n    return list.toSortedArrays();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
      "extendedDetails": {}
    },
    "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7758. Retire FsDatasetSpi#getVolumes() and use FsDatasetSpi#getVolumeRefs() instead (Lei (Eddy) Xu via Colin P. McCabe)\n",
      "commitDate": "05/05/15 11:08 AM",
      "commitName": "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.05,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,43 @@\n   private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n+    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n+    ScanInfoPerBlockPool[] dirReports \u003d null;\n     // First get list of data directories\n-    final List\u003c? extends FsVolumeSpi\u003e volumes \u003d dataset.getVolumes();\n+    try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n+        dataset.getFsVolumeReferences()) {\n \n-    // Use an array since the threads may return out of order and\n-    // compilersInProgress#keySet may return out of order as well.\n-    ScanInfoPerBlockPool[] dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n+      // Use an array since the threads may return out of order and\n+      // compilersInProgress#keySet may return out of order as well.\n+      dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n \n-    Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n-      new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n+      Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n+          new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n \n-    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n-      if (isValid(dataset, volumes.get(i))) {\n+      for (int i \u003d 0; i \u003c volumes.size(); i++) {\n         ReportCompiler reportCompiler \u003d\n-          new ReportCompiler(datanode,volumes.get(i));\n-        Future\u003cScanInfoPerBlockPool\u003e result \u003d \n-          reportCompileThreadPool.submit(reportCompiler);\n+            new ReportCompiler(datanode, volumes.get(i));\n+        Future\u003cScanInfoPerBlockPool\u003e result \u003d\n+            reportCompileThreadPool.submit(reportCompiler);\n         compilersInProgress.put(i, result);\n       }\n+\n+      for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n+          compilersInProgress.entrySet()) {\n+        try {\n+          dirReports[report.getKey()] \u003d report.getValue().get();\n+        } catch (Exception ex) {\n+          LOG.error(\"Error compiling report\", ex);\n+          // Propagate ex to DataBlockScanner to deal with\n+          throw new RuntimeException(ex);\n+        }\n+      }\n+    } catch (IOException e) {\n+      LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n     }\n-    \n-    for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n-        compilersInProgress.entrySet()) {\n-      try {\n-        dirReports[report.getKey()] \u003d report.getValue().get();\n-      } catch (Exception ex) {\n-        LOG.error(\"Error compiling report\", ex);\n-        // Propagate ex to DataBlockScanner to deal with\n-        throw new RuntimeException(ex);\n+    if (dirReports !\u003d null) {\n+      // Compile consolidated report for all the volumes\n+      for (ScanInfoPerBlockPool report : dirReports) {\n+        list.addAll(report);\n       }\n     }\n-\n-    // Compile consolidated report for all the volumes\n-    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n-    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n-      if (isValid(dataset, volumes.get(i))) {\n-        // volume is still valid\n-        list.addAll(dirReports[i]);\n-      }\n-    }\n-\n     return list.toSortedArrays();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n    ScanInfoPerBlockPool[] dirReports \u003d null;\n    // First get list of data directories\n    try (FsDatasetSpi.FsVolumeReferences volumes \u003d\n        dataset.getFsVolumeReferences()) {\n\n      // Use an array since the threads may return out of order and\n      // compilersInProgress#keySet may return out of order as well.\n      dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n\n      Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n          new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n\n      for (int i \u003d 0; i \u003c volumes.size(); i++) {\n        ReportCompiler reportCompiler \u003d\n            new ReportCompiler(datanode, volumes.get(i));\n        Future\u003cScanInfoPerBlockPool\u003e result \u003d\n            reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n\n      for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n          compilersInProgress.entrySet()) {\n        try {\n          dirReports[report.getKey()] \u003d report.getValue().get();\n        } catch (Exception ex) {\n          LOG.error(\"Error compiling report\", ex);\n          // Propagate ex to DataBlockScanner to deal with\n          throw new RuntimeException(ex);\n        }\n      }\n    } catch (IOException e) {\n      LOG.error(\"Unexpected IOException by closing FsVolumeReference\", e);\n    }\n    if (dirReports !\u003d null) {\n      // Compile consolidated report for all the volumes\n      for (ScanInfoPerBlockPool report : dirReports) {\n        list.addAll(report);\n      }\n    }\n    return list.toSortedArrays();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
      "extendedDetails": {}
    },
    "4f75b15628a76881efc39054612dc128e23d27be": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6753. Initialize checkDisk when DirectoryScanner not able to get files list for scanning (Contributed by J.Andreina)\n",
      "commitDate": "27/02/15 3:06 AM",
      "commitName": "4f75b15628a76881efc39054612dc128e23d27be",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "26/02/15 11:58 AM",
      "commitNameOld": "f0c980abed3843923e0eb16b626fa27334195eda",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.63,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,43 @@\n   private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n     // First get list of data directories\n     final List\u003c? extends FsVolumeSpi\u003e volumes \u003d dataset.getVolumes();\n \n     // Use an array since the threads may return out of order and\n     // compilersInProgress#keySet may return out of order as well.\n     ScanInfoPerBlockPool[] dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n \n     Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n       new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n \n     for (int i \u003d 0; i \u003c volumes.size(); i++) {\n       if (isValid(dataset, volumes.get(i))) {\n         ReportCompiler reportCompiler \u003d\n-          new ReportCompiler(volumes.get(i));\n+          new ReportCompiler(datanode,volumes.get(i));\n         Future\u003cScanInfoPerBlockPool\u003e result \u003d \n           reportCompileThreadPool.submit(reportCompiler);\n         compilersInProgress.put(i, result);\n       }\n     }\n     \n     for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n         compilersInProgress.entrySet()) {\n       try {\n         dirReports[report.getKey()] \u003d report.getValue().get();\n       } catch (Exception ex) {\n         LOG.error(\"Error compiling report\", ex);\n         // Propagate ex to DataBlockScanner to deal with\n         throw new RuntimeException(ex);\n       }\n     }\n \n     // Compile consolidated report for all the volumes\n     ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n     for (int i \u003d 0; i \u003c volumes.size(); i++) {\n       if (isValid(dataset, volumes.get(i))) {\n         // volume is still valid\n         list.addAll(dirReports[i]);\n       }\n     }\n \n     return list.toSortedArrays();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n    // First get list of data directories\n    final List\u003c? extends FsVolumeSpi\u003e volumes \u003d dataset.getVolumes();\n\n    // Use an array since the threads may return out of order and\n    // compilersInProgress#keySet may return out of order as well.\n    ScanInfoPerBlockPool[] dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n\n    Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n      new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (isValid(dataset, volumes.get(i))) {\n        ReportCompiler reportCompiler \u003d\n          new ReportCompiler(datanode,volumes.get(i));\n        Future\u003cScanInfoPerBlockPool\u003e result \u003d \n          reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n    }\n    \n    for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n        compilersInProgress.entrySet()) {\n      try {\n        dirReports[report.getKey()] \u003d report.getValue().get();\n      } catch (Exception ex) {\n        LOG.error(\"Error compiling report\", ex);\n        // Propagate ex to DataBlockScanner to deal with\n        throw new RuntimeException(ex);\n      }\n    }\n\n    // Compile consolidated report for all the volumes\n    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (isValid(dataset, volumes.get(i))) {\n        // volume is still valid\n        list.addAll(dirReports[i]);\n      }\n    }\n\n    return list.toSortedArrays();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
      "extendedDetails": {}
    },
    "a75673cbd88215afd98e9d6ac31a9d93062048eb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4058. DirectoryScanner may fail with IOOB if the directory scanning threads return out of volume order. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1398612 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/10/12 7:12 PM",
      "commitName": "a75673cbd88215afd98e9d6ac31a9d93062048eb",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "21/08/12 2:18 PM",
      "commitNameOld": "6c0ccb5989c2053f5a1ebab0dd9fdb7b4019fda8",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 55.2,
      "commitsBetweenForRepo": 337,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,43 @@\n   private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n     // First get list of data directories\n     final List\u003c? extends FsVolumeSpi\u003e volumes \u003d dataset.getVolumes();\n-    ArrayList\u003cScanInfoPerBlockPool\u003e dirReports \u003d\n-      new ArrayList\u003cScanInfoPerBlockPool\u003e(volumes.size());\n-    \n+\n+    // Use an array since the threads may return out of order and\n+    // compilersInProgress#keySet may return out of order as well.\n+    ScanInfoPerBlockPool[] dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n+\n     Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n       new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n+\n     for (int i \u003d 0; i \u003c volumes.size(); i++) {\n-      if (!isValid(dataset, volumes.get(i))) {\n-        // volume is invalid\n-        dirReports.add(i, null);\n-      } else {\n+      if (isValid(dataset, volumes.get(i))) {\n         ReportCompiler reportCompiler \u003d\n           new ReportCompiler(volumes.get(i));\n         Future\u003cScanInfoPerBlockPool\u003e result \u003d \n           reportCompileThreadPool.submit(reportCompiler);\n         compilersInProgress.put(i, result);\n       }\n     }\n     \n     for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n         compilersInProgress.entrySet()) {\n       try {\n-        dirReports.add(report.getKey(), report.getValue().get());\n+        dirReports[report.getKey()] \u003d report.getValue().get();\n       } catch (Exception ex) {\n         LOG.error(\"Error compiling report\", ex);\n         // Propagate ex to DataBlockScanner to deal with\n         throw new RuntimeException(ex);\n       }\n     }\n \n     // Compile consolidated report for all the volumes\n     ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n     for (int i \u003d 0; i \u003c volumes.size(); i++) {\n       if (isValid(dataset, volumes.get(i))) {\n         // volume is still valid\n-        list.addAll(dirReports.get(i));\n+        list.addAll(dirReports[i]);\n       }\n     }\n \n     return list.toSortedArrays();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n    // First get list of data directories\n    final List\u003c? extends FsVolumeSpi\u003e volumes \u003d dataset.getVolumes();\n\n    // Use an array since the threads may return out of order and\n    // compilersInProgress#keySet may return out of order as well.\n    ScanInfoPerBlockPool[] dirReports \u003d new ScanInfoPerBlockPool[volumes.size()];\n\n    Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n      new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (isValid(dataset, volumes.get(i))) {\n        ReportCompiler reportCompiler \u003d\n          new ReportCompiler(volumes.get(i));\n        Future\u003cScanInfoPerBlockPool\u003e result \u003d \n          reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n    }\n    \n    for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n        compilersInProgress.entrySet()) {\n      try {\n        dirReports[report.getKey()] \u003d report.getValue().get();\n      } catch (Exception ex) {\n        LOG.error(\"Error compiling report\", ex);\n        // Propagate ex to DataBlockScanner to deal with\n        throw new RuntimeException(ex);\n      }\n    }\n\n    // Compile consolidated report for all the volumes\n    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (isValid(dataset, volumes.get(i))) {\n        // volume is still valid\n        list.addAll(dirReports[i]);\n      }\n    }\n\n    return list.toSortedArrays();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
      "extendedDetails": {}
    },
    "662b1887af4e39f3eadd7dda4953c7f2529b43bc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3088. Move FSDatasetInterface inner classes to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1301661 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/03/12 10:32 AM",
      "commitName": "662b1887af4e39f3eadd7dda4953c7f2529b43bc",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "01/03/12 1:58 PM",
      "commitNameOld": "9e31bf675dd92183a9a74a66b7caf1a080581d65",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 14.82,
      "commitsBetweenForRepo": 88,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,43 @@\n   private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n     // First get list of data directories\n-    final List\u003c? extends FSVolumeInterface\u003e volumes \u003d dataset.getVolumes();\n+    final List\u003c? extends FsVolumeSpi\u003e volumes \u003d dataset.getVolumes();\n     ArrayList\u003cScanInfoPerBlockPool\u003e dirReports \u003d\n       new ArrayList\u003cScanInfoPerBlockPool\u003e(volumes.size());\n     \n     Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n       new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n     for (int i \u003d 0; i \u003c volumes.size(); i++) {\n       if (!isValid(dataset, volumes.get(i))) {\n         // volume is invalid\n         dirReports.add(i, null);\n       } else {\n         ReportCompiler reportCompiler \u003d\n           new ReportCompiler(volumes.get(i));\n         Future\u003cScanInfoPerBlockPool\u003e result \u003d \n           reportCompileThreadPool.submit(reportCompiler);\n         compilersInProgress.put(i, result);\n       }\n     }\n     \n     for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n         compilersInProgress.entrySet()) {\n       try {\n         dirReports.add(report.getKey(), report.getValue().get());\n       } catch (Exception ex) {\n         LOG.error(\"Error compiling report\", ex);\n         // Propagate ex to DataBlockScanner to deal with\n         throw new RuntimeException(ex);\n       }\n     }\n \n     // Compile consolidated report for all the volumes\n     ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n     for (int i \u003d 0; i \u003c volumes.size(); i++) {\n       if (isValid(dataset, volumes.get(i))) {\n         // volume is still valid\n         list.addAll(dirReports.get(i));\n       }\n     }\n \n     return list.toSortedArrays();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n    // First get list of data directories\n    final List\u003c? extends FsVolumeSpi\u003e volumes \u003d dataset.getVolumes();\n    ArrayList\u003cScanInfoPerBlockPool\u003e dirReports \u003d\n      new ArrayList\u003cScanInfoPerBlockPool\u003e(volumes.size());\n    \n    Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n      new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (!isValid(dataset, volumes.get(i))) {\n        // volume is invalid\n        dirReports.add(i, null);\n      } else {\n        ReportCompiler reportCompiler \u003d\n          new ReportCompiler(volumes.get(i));\n        Future\u003cScanInfoPerBlockPool\u003e result \u003d \n          reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n    }\n    \n    for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n        compilersInProgress.entrySet()) {\n      try {\n        dirReports.add(report.getKey(), report.getValue().get());\n      } catch (Exception ex) {\n        LOG.error(\"Error compiling report\", ex);\n        // Propagate ex to DataBlockScanner to deal with\n        throw new RuntimeException(ex);\n      }\n    }\n\n    // Compile consolidated report for all the volumes\n    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (isValid(dataset, volumes.get(i))) {\n        // volume is still valid\n        list.addAll(dirReports.get(i));\n      }\n    }\n\n    return list.toSortedArrays();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
      "extendedDetails": {}
    },
    "9e31bf675dd92183a9a74a66b7caf1a080581d65": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3021. Use generic type to declare FSDatasetInterface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1295929 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/03/12 1:58 PM",
      "commitName": "9e31bf675dd92183a9a74a66b7caf1a080581d65",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/02/12 12:58 PM",
      "commitNameOld": "b6ffb08a467f1b5bc89e5114c462c3117b337be6",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 22.04,
      "commitsBetweenForRepo": 133,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,43 @@\n   private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n     // First get list of data directories\n-    final List\u003cFSVolumeInterface\u003e volumes \u003d dataset.getVolumes();\n+    final List\u003c? extends FSVolumeInterface\u003e volumes \u003d dataset.getVolumes();\n     ArrayList\u003cScanInfoPerBlockPool\u003e dirReports \u003d\n       new ArrayList\u003cScanInfoPerBlockPool\u003e(volumes.size());\n     \n     Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n       new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n     for (int i \u003d 0; i \u003c volumes.size(); i++) {\n       if (!isValid(dataset, volumes.get(i))) {\n         // volume is invalid\n         dirReports.add(i, null);\n       } else {\n         ReportCompiler reportCompiler \u003d\n           new ReportCompiler(volumes.get(i));\n         Future\u003cScanInfoPerBlockPool\u003e result \u003d \n           reportCompileThreadPool.submit(reportCompiler);\n         compilersInProgress.put(i, result);\n       }\n     }\n     \n     for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n         compilersInProgress.entrySet()) {\n       try {\n         dirReports.add(report.getKey(), report.getValue().get());\n       } catch (Exception ex) {\n         LOG.error(\"Error compiling report\", ex);\n         // Propagate ex to DataBlockScanner to deal with\n         throw new RuntimeException(ex);\n       }\n     }\n \n     // Compile consolidated report for all the volumes\n     ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n     for (int i \u003d 0; i \u003c volumes.size(); i++) {\n       if (isValid(dataset, volumes.get(i))) {\n         // volume is still valid\n         list.addAll(dirReports.get(i));\n       }\n     }\n \n     return list.toSortedArrays();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n    // First get list of data directories\n    final List\u003c? extends FSVolumeInterface\u003e volumes \u003d dataset.getVolumes();\n    ArrayList\u003cScanInfoPerBlockPool\u003e dirReports \u003d\n      new ArrayList\u003cScanInfoPerBlockPool\u003e(volumes.size());\n    \n    Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n      new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (!isValid(dataset, volumes.get(i))) {\n        // volume is invalid\n        dirReports.add(i, null);\n      } else {\n        ReportCompiler reportCompiler \u003d\n          new ReportCompiler(volumes.get(i));\n        Future\u003cScanInfoPerBlockPool\u003e result \u003d \n          reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n    }\n    \n    for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n        compilersInProgress.entrySet()) {\n      try {\n        dirReports.add(report.getKey(), report.getValue().get());\n      } catch (Exception ex) {\n        LOG.error(\"Error compiling report\", ex);\n        // Propagate ex to DataBlockScanner to deal with\n        throw new RuntimeException(ex);\n      }\n    }\n\n    // Compile consolidated report for all the volumes\n    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (isValid(dataset, volumes.get(i))) {\n        // volume is still valid\n        list.addAll(dirReports.get(i));\n      }\n    }\n\n    return list.toSortedArrays();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
      "extendedDetails": {}
    },
    "b6ffb08a467f1b5bc89e5114c462c3117b337be6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2887. FSVolume, is a part of FSDatasetInterface implementation, should not be referred outside FSDataset.  A new FSVolumeInterface is defined.  The BlockVolumeChoosingPolicy.chooseVolume(..) method signature is also updated.  (szetszwo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1242087 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/02/12 12:58 PM",
      "commitName": "b6ffb08a467f1b5bc89e5114c462c3117b337be6",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 167.86,
      "commitsBetweenForRepo": 1072,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,43 @@\n   private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n     // First get list of data directories\n-    List\u003cFSVolume\u003e volumes \u003d dataset.volumes.getVolumes();\n+    final List\u003cFSVolumeInterface\u003e volumes \u003d dataset.getVolumes();\n     ArrayList\u003cScanInfoPerBlockPool\u003e dirReports \u003d\n       new ArrayList\u003cScanInfoPerBlockPool\u003e(volumes.size());\n     \n     Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n       new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n     for (int i \u003d 0; i \u003c volumes.size(); i++) {\n-      if (!dataset.volumes.isValid(volumes.get(i))) { // volume is still valid\n+      if (!isValid(dataset, volumes.get(i))) {\n+        // volume is invalid\n         dirReports.add(i, null);\n       } else {\n         ReportCompiler reportCompiler \u003d\n           new ReportCompiler(volumes.get(i));\n         Future\u003cScanInfoPerBlockPool\u003e result \u003d \n           reportCompileThreadPool.submit(reportCompiler);\n         compilersInProgress.put(i, result);\n       }\n     }\n     \n     for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n         compilersInProgress.entrySet()) {\n       try {\n         dirReports.add(report.getKey(), report.getValue().get());\n       } catch (Exception ex) {\n         LOG.error(\"Error compiling report\", ex);\n         // Propagate ex to DataBlockScanner to deal with\n         throw new RuntimeException(ex);\n       }\n     }\n \n     // Compile consolidated report for all the volumes\n     ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n     for (int i \u003d 0; i \u003c volumes.size(); i++) {\n-      if (dataset.volumes.isValid(volumes.get(i))) { // volume is still valid\n+      if (isValid(dataset, volumes.get(i))) {\n+        // volume is still valid\n         list.addAll(dirReports.get(i));\n       }\n     }\n \n     return list.toSortedArrays();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n    // First get list of data directories\n    final List\u003cFSVolumeInterface\u003e volumes \u003d dataset.getVolumes();\n    ArrayList\u003cScanInfoPerBlockPool\u003e dirReports \u003d\n      new ArrayList\u003cScanInfoPerBlockPool\u003e(volumes.size());\n    \n    Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n      new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (!isValid(dataset, volumes.get(i))) {\n        // volume is invalid\n        dirReports.add(i, null);\n      } else {\n        ReportCompiler reportCompiler \u003d\n          new ReportCompiler(volumes.get(i));\n        Future\u003cScanInfoPerBlockPool\u003e result \u003d \n          reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n    }\n    \n    for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n        compilersInProgress.entrySet()) {\n      try {\n        dirReports.add(report.getKey(), report.getValue().get());\n      } catch (Exception ex) {\n        LOG.error(\"Error compiling report\", ex);\n        // Propagate ex to DataBlockScanner to deal with\n        throw new RuntimeException(ex);\n      }\n    }\n\n    // Compile consolidated report for all the volumes\n    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (isValid(dataset, volumes.get(i))) {\n        // volume is still valid\n        list.addAll(dirReports.get(i));\n      }\n    }\n\n    return list.toSortedArrays();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n    // First get list of data directories\n    List\u003cFSVolume\u003e volumes \u003d dataset.volumes.getVolumes();\n    ArrayList\u003cScanInfoPerBlockPool\u003e dirReports \u003d\n      new ArrayList\u003cScanInfoPerBlockPool\u003e(volumes.size());\n    \n    Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n      new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (!dataset.volumes.isValid(volumes.get(i))) { // volume is still valid\n        dirReports.add(i, null);\n      } else {\n        ReportCompiler reportCompiler \u003d\n          new ReportCompiler(volumes.get(i));\n        Future\u003cScanInfoPerBlockPool\u003e result \u003d \n          reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n    }\n    \n    for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n        compilersInProgress.entrySet()) {\n      try {\n        dirReports.add(report.getKey(), report.getValue().get());\n      } catch (Exception ex) {\n        LOG.error(\"Error compiling report\", ex);\n        // Propagate ex to DataBlockScanner to deal with\n        throw new RuntimeException(ex);\n      }\n    }\n\n    // Compile consolidated report for all the volumes\n    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (dataset.volumes.isValid(volumes.get(i))) { // volume is still valid\n        list.addAll(dirReports.get(i));\n      }\n    }\n\n    return list.toSortedArrays();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n    // First get list of data directories\n    List\u003cFSVolume\u003e volumes \u003d dataset.volumes.getVolumes();\n    ArrayList\u003cScanInfoPerBlockPool\u003e dirReports \u003d\n      new ArrayList\u003cScanInfoPerBlockPool\u003e(volumes.size());\n    \n    Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n      new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (!dataset.volumes.isValid(volumes.get(i))) { // volume is still valid\n        dirReports.add(i, null);\n      } else {\n        ReportCompiler reportCompiler \u003d\n          new ReportCompiler(volumes.get(i));\n        Future\u003cScanInfoPerBlockPool\u003e result \u003d \n          reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n    }\n    \n    for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n        compilersInProgress.entrySet()) {\n      try {\n        dirReports.add(report.getKey(), report.getValue().get());\n      } catch (Exception ex) {\n        LOG.error(\"Error compiling report\", ex);\n        // Propagate ex to DataBlockScanner to deal with\n        throw new RuntimeException(ex);\n      }\n    }\n\n    // Compile consolidated report for all the volumes\n    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (dataset.volumes.isValid(volumes.get(i))) { // volume is still valid\n        list.addAll(dirReports.get(i));\n      }\n    }\n\n    return list.toSortedArrays();\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,41 @@\n+  private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n+    // First get list of data directories\n+    List\u003cFSVolume\u003e volumes \u003d dataset.volumes.getVolumes();\n+    ArrayList\u003cScanInfoPerBlockPool\u003e dirReports \u003d\n+      new ArrayList\u003cScanInfoPerBlockPool\u003e(volumes.size());\n+    \n+    Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n+      new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n+    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n+      if (!dataset.volumes.isValid(volumes.get(i))) { // volume is still valid\n+        dirReports.add(i, null);\n+      } else {\n+        ReportCompiler reportCompiler \u003d\n+          new ReportCompiler(volumes.get(i));\n+        Future\u003cScanInfoPerBlockPool\u003e result \u003d \n+          reportCompileThreadPool.submit(reportCompiler);\n+        compilersInProgress.put(i, result);\n+      }\n+    }\n+    \n+    for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n+        compilersInProgress.entrySet()) {\n+      try {\n+        dirReports.add(report.getKey(), report.getValue().get());\n+      } catch (Exception ex) {\n+        LOG.error(\"Error compiling report\", ex);\n+        // Propagate ex to DataBlockScanner to deal with\n+        throw new RuntimeException(ex);\n+      }\n+    }\n+\n+    // Compile consolidated report for all the volumes\n+    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n+    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n+      if (dataset.volumes.isValid(volumes.get(i))) { // volume is still valid\n+        list.addAll(dirReports.get(i));\n+      }\n+    }\n+\n+    return list.toSortedArrays();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private Map\u003cString, ScanInfo[]\u003e getDiskReport() {\n    // First get list of data directories\n    List\u003cFSVolume\u003e volumes \u003d dataset.volumes.getVolumes();\n    ArrayList\u003cScanInfoPerBlockPool\u003e dirReports \u003d\n      new ArrayList\u003cScanInfoPerBlockPool\u003e(volumes.size());\n    \n    Map\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e compilersInProgress \u003d\n      new HashMap\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e();\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (!dataset.volumes.isValid(volumes.get(i))) { // volume is still valid\n        dirReports.add(i, null);\n      } else {\n        ReportCompiler reportCompiler \u003d\n          new ReportCompiler(volumes.get(i));\n        Future\u003cScanInfoPerBlockPool\u003e result \u003d \n          reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n    }\n    \n    for (Entry\u003cInteger, Future\u003cScanInfoPerBlockPool\u003e\u003e report :\n        compilersInProgress.entrySet()) {\n      try {\n        dirReports.add(report.getKey(), report.getValue().get());\n      } catch (Exception ex) {\n        LOG.error(\"Error compiling report\", ex);\n        // Propagate ex to DataBlockScanner to deal with\n        throw new RuntimeException(ex);\n      }\n    }\n\n    // Compile consolidated report for all the volumes\n    ScanInfoPerBlockPool list \u003d new ScanInfoPerBlockPool();\n    for (int i \u003d 0; i \u003c volumes.size(); i++) {\n      if (dataset.volumes.isValid(volumes.get(i))) { // volume is still valid\n        list.addAll(dirReports.get(i));\n      }\n    }\n\n    return list.toSortedArrays();\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java"
    }
  }
}