{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PageBlobInputStream.java",
  "functionName": "getPageBlobDataSize",
  "functionId": "getPageBlobDataSize___blob-CloudPageBlobWrapper__opContext-OperationContext",
  "sourceFilePath": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobInputStream.java",
  "functionStartLine": 87,
  "functionEndLine": 114,
  "numCommitsSeen": 7,
  "timeTaken": 2046,
  "changeHistory": [
    "c45784bc9031353b938f4756473937cca759b3dc",
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40"
  ],
  "changeHistoryShort": {
    "c45784bc9031353b938f4756473937cca759b3dc": "Yrename",
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c45784bc9031353b938f4756473937cca759b3dc": {
      "type": "Yrename",
      "commitMessage": "HADOOP-12073. Azure FileSystem PageBlobInputStream does not return -1 on EOF. Contributed by Ivan Mitic.\n",
      "commitDate": "08/06/15 10:42 PM",
      "commitName": "c45784bc9031353b938f4756473937cca759b3dc",
      "commitAuthor": "cnauroth",
      "commitDateOld": "06/03/15 3:25 PM",
      "commitNameOld": "608ebd52bafecf980e9726d397c786a9c2422eba",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 94.26,
      "commitsBetweenForRepo": 874,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n-  public static long getPageBlobSize(CloudPageBlobWrapper blob,\n+  public static long getPageBlobDataSize(CloudPageBlobWrapper blob,\n       OperationContext opContext) throws IOException, StorageException {\n     // Get the page ranges for the blob. There should be one range starting\n     // at byte 0, but we tolerate (and ignore) ranges after the first one.\n     ArrayList\u003cPageRange\u003e pageRanges \u003d\n         blob.downloadPageRanges(new BlobRequestOptions(), opContext);\n     if (pageRanges.size() \u003d\u003d 0) {\n       return 0;\n     }\n     if (pageRanges.get(0).getStartOffset() !\u003d 0) {\n       // Not expected: we always upload our page blobs as a contiguous range\n       // starting at byte 0.\n       throw badStartRangeException(blob, pageRanges.get(0));\n     }\n     long totalRawBlobSize \u003d pageRanges.get(0).getEndOffset() + 1;\n \n     // Get the last page.\n     long lastPageStart \u003d totalRawBlobSize - PAGE_SIZE;\n     ByteArrayOutputStream baos \u003d \n         new ByteArrayOutputStream(PageBlobFormatHelpers.PAGE_SIZE);\n     blob.downloadRange(lastPageStart, PAGE_SIZE, baos,\n         new BlobRequestOptions(), opContext);\n \n     byte[] lastPage \u003d baos.toByteArray();\n     short lastPageSize \u003d getPageSize(blob, lastPage, 0);\n     long totalNumberOfPages \u003d totalRawBlobSize / PAGE_SIZE;\n     return (totalNumberOfPages - 1) * PAGE_DATA_SIZE + lastPageSize;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static long getPageBlobDataSize(CloudPageBlobWrapper blob,\n      OperationContext opContext) throws IOException, StorageException {\n    // Get the page ranges for the blob. There should be one range starting\n    // at byte 0, but we tolerate (and ignore) ranges after the first one.\n    ArrayList\u003cPageRange\u003e pageRanges \u003d\n        blob.downloadPageRanges(new BlobRequestOptions(), opContext);\n    if (pageRanges.size() \u003d\u003d 0) {\n      return 0;\n    }\n    if (pageRanges.get(0).getStartOffset() !\u003d 0) {\n      // Not expected: we always upload our page blobs as a contiguous range\n      // starting at byte 0.\n      throw badStartRangeException(blob, pageRanges.get(0));\n    }\n    long totalRawBlobSize \u003d pageRanges.get(0).getEndOffset() + 1;\n\n    // Get the last page.\n    long lastPageStart \u003d totalRawBlobSize - PAGE_SIZE;\n    ByteArrayOutputStream baos \u003d \n        new ByteArrayOutputStream(PageBlobFormatHelpers.PAGE_SIZE);\n    blob.downloadRange(lastPageStart, PAGE_SIZE, baos,\n        new BlobRequestOptions(), opContext);\n\n    byte[] lastPage \u003d baos.toByteArray();\n    short lastPageSize \u003d getPageSize(blob, lastPage, 0);\n    long totalNumberOfPages \u003d totalRawBlobSize / PAGE_SIZE;\n    return (totalNumberOfPages - 1) * PAGE_DATA_SIZE + lastPageSize;\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobInputStream.java",
      "extendedDetails": {
        "oldValue": "getPageBlobSize",
        "newValue": "getPageBlobDataSize"
      }
    },
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-10809. hadoop-azure: page blob support. Contributed by Dexter Bradshaw, Mostafa Elhemali, Eric Hanson, and Mike Liddell.\n",
      "commitDate": "08/10/14 2:20 PM",
      "commitName": "2217e2f8ff418b88eac6ad36cafe3a9795a11f40",
      "commitAuthor": "cnauroth",
      "diff": "@@ -0,0 +1,28 @@\n+  public static long getPageBlobSize(CloudPageBlobWrapper blob,\n+      OperationContext opContext) throws IOException, StorageException {\n+    // Get the page ranges for the blob. There should be one range starting\n+    // at byte 0, but we tolerate (and ignore) ranges after the first one.\n+    ArrayList\u003cPageRange\u003e pageRanges \u003d\n+        blob.downloadPageRanges(new BlobRequestOptions(), opContext);\n+    if (pageRanges.size() \u003d\u003d 0) {\n+      return 0;\n+    }\n+    if (pageRanges.get(0).getStartOffset() !\u003d 0) {\n+      // Not expected: we always upload our page blobs as a contiguous range\n+      // starting at byte 0.\n+      throw badStartRangeException(blob, pageRanges.get(0));\n+    }\n+    long totalRawBlobSize \u003d pageRanges.get(0).getEndOffset() + 1;\n+\n+    // Get the last page.\n+    long lastPageStart \u003d totalRawBlobSize - PAGE_SIZE;\n+    ByteArrayOutputStream baos \u003d \n+        new ByteArrayOutputStream(PageBlobFormatHelpers.PAGE_SIZE);\n+    blob.downloadRange(lastPageStart, PAGE_SIZE, baos,\n+        new BlobRequestOptions(), opContext);\n+\n+    byte[] lastPage \u003d baos.toByteArray();\n+    short lastPageSize \u003d getPageSize(blob, lastPage, 0);\n+    long totalNumberOfPages \u003d totalRawBlobSize / PAGE_SIZE;\n+    return (totalNumberOfPages - 1) * PAGE_DATA_SIZE + lastPageSize;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static long getPageBlobSize(CloudPageBlobWrapper blob,\n      OperationContext opContext) throws IOException, StorageException {\n    // Get the page ranges for the blob. There should be one range starting\n    // at byte 0, but we tolerate (and ignore) ranges after the first one.\n    ArrayList\u003cPageRange\u003e pageRanges \u003d\n        blob.downloadPageRanges(new BlobRequestOptions(), opContext);\n    if (pageRanges.size() \u003d\u003d 0) {\n      return 0;\n    }\n    if (pageRanges.get(0).getStartOffset() !\u003d 0) {\n      // Not expected: we always upload our page blobs as a contiguous range\n      // starting at byte 0.\n      throw badStartRangeException(blob, pageRanges.get(0));\n    }\n    long totalRawBlobSize \u003d pageRanges.get(0).getEndOffset() + 1;\n\n    // Get the last page.\n    long lastPageStart \u003d totalRawBlobSize - PAGE_SIZE;\n    ByteArrayOutputStream baos \u003d \n        new ByteArrayOutputStream(PageBlobFormatHelpers.PAGE_SIZE);\n    blob.downloadRange(lastPageStart, PAGE_SIZE, baos,\n        new BlobRequestOptions(), opContext);\n\n    byte[] lastPage \u003d baos.toByteArray();\n    short lastPageSize \u003d getPageSize(blob, lastPage, 0);\n    long totalNumberOfPages \u003d totalRawBlobSize / PAGE_SIZE;\n    return (totalNumberOfPages - 1) * PAGE_DATA_SIZE + lastPageSize;\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobInputStream.java"
    }
  }
}