{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockPoolSlice.java",
  "functionName": "saveDfsUsed",
  "functionId": "saveDfsUsed",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
  "functionStartLine": 331,
  "functionEndLine": 354,
  "numCommitsSeen": 58,
  "timeTaken": 3999,
  "changeHistory": [
    "7a3188d054481b9bd563e337901e93476303ce7f",
    "603f3ef1386048111940b66f3a0750ab84d0588f",
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389",
    "c07f7fa8ff752436726239d938e0461236839acf",
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
    "c8182ea76412e49c0c98ee252321c584fabb4c59",
    "fa6e59891c6125ae83fd601dbbcf928685f5dbfd"
  ],
  "changeHistoryShort": {
    "7a3188d054481b9bd563e337901e93476303ce7f": "Ybodychange",
    "603f3ef1386048111940b66f3a0750ab84d0588f": "Ybodychange",
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389": "Ybodychange",
    "c07f7fa8ff752436726239d938e0461236839acf": "Ybodychange",
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": "Ybodychange",
    "c8182ea76412e49c0c98ee252321c584fabb4c59": "Ybodychange",
    "fa6e59891c6125ae83fd601dbbcf928685f5dbfd": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7a3188d054481b9bd563e337901e93476303ce7f": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16282. Avoid FileStream to improve performance. Contributed by Ayush Saxena.\n",
      "commitDate": "02/05/19 12:58 PM",
      "commitName": "7a3188d054481b9bd563e337901e93476303ce7f",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "08/10/18 7:33 PM",
      "commitNameOld": "1043795f7fe44c98a34f8ea3cea708c801c3043b",
      "commitAuthorOld": "Yiqun Lin",
      "daysBetweenCommits": 205.73,
      "commitsBetweenForRepo": 1546,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   void saveDfsUsed() {\n     File outFile \u003d new File(currentDir, DU_CACHE_FILE);\n     if (!fileIoProvider.deleteWithExistsCheck(volume, outFile)) {\n       FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n         outFile.getParent());\n     }\n \n     try {\n       long used \u003d getDfsUsed();\n       try (Writer out \u003d new OutputStreamWriter(\n-          new FileOutputStream(outFile), \"UTF-8\")) {\n+          Files.newOutputStream(outFile.toPath()), \"UTF-8\")) {\n         // mtime is written last, so that truncated writes won\u0027t be valid.\n         out.write(Long.toString(used) + \" \" + Long.toString(timer.now()));\n         // This is only called as part of the volume shutdown.\n         // We explicitly avoid calling flush with fileIoProvider which triggers\n         // volume check upon io exception to avoid cyclic volume checks.\n         out.flush();\n       }\n     } catch (IOException ioe) {\n       // If write failed, the volume might be bad. Since the cache file is\n       // not critical, log the error and continue.\n       FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void saveDfsUsed() {\n    File outFile \u003d new File(currentDir, DU_CACHE_FILE);\n    if (!fileIoProvider.deleteWithExistsCheck(volume, outFile)) {\n      FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n        outFile.getParent());\n    }\n\n    try {\n      long used \u003d getDfsUsed();\n      try (Writer out \u003d new OutputStreamWriter(\n          Files.newOutputStream(outFile.toPath()), \"UTF-8\")) {\n        // mtime is written last, so that truncated writes won\u0027t be valid.\n        out.write(Long.toString(used) + \" \" + Long.toString(timer.now()));\n        // This is only called as part of the volume shutdown.\n        // We explicitly avoid calling flush with fileIoProvider which triggers\n        // volume check upon io exception to avoid cyclic volume checks.\n        out.flush();\n      }\n    } catch (IOException ioe) {\n      // If write failed, the volume might be bad. Since the cache file is\n      // not critical, log the error and continue.\n      FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "603f3ef1386048111940b66f3a0750ab84d0588f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11274. Datanode should only check the failed volume upon IO errors. Contributed by Xiaoyu Yao.\n",
      "commitDate": "28/12/16 10:08 PM",
      "commitName": "603f3ef1386048111940b66f3a0750ab84d0588f",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "14/12/16 11:18 AM",
      "commitNameOld": "6ba9587d370fbf39c129c08c00ebbb894ccc1389",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 14.45,
      "commitsBetweenForRepo": 60,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,24 @@\n   void saveDfsUsed() {\n     File outFile \u003d new File(currentDir, DU_CACHE_FILE);\n     if (!fileIoProvider.deleteWithExistsCheck(volume, outFile)) {\n       FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n         outFile.getParent());\n     }\n \n     try {\n       long used \u003d getDfsUsed();\n       try (Writer out \u003d new OutputStreamWriter(\n           new FileOutputStream(outFile), \"UTF-8\")) {\n         // mtime is written last, so that truncated writes won\u0027t be valid.\n         out.write(Long.toString(used) + \" \" + Long.toString(timer.now()));\n-        fileIoProvider.flush(volume, out);\n+        // This is only called as part of the volume shutdown.\n+        // We explicitly avoid calling flush with fileIoProvider which triggers\n+        // volume check upon io exception to avoid cyclic volume checks.\n+        out.flush();\n       }\n     } catch (IOException ioe) {\n       // If write failed, the volume might be bad. Since the cache file is\n       // not critical, log the error and continue.\n       FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void saveDfsUsed() {\n    File outFile \u003d new File(currentDir, DU_CACHE_FILE);\n    if (!fileIoProvider.deleteWithExistsCheck(volume, outFile)) {\n      FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n        outFile.getParent());\n    }\n\n    try {\n      long used \u003d getDfsUsed();\n      try (Writer out \u003d new OutputStreamWriter(\n          new FileOutputStream(outFile), \"UTF-8\")) {\n        // mtime is written last, so that truncated writes won\u0027t be valid.\n        out.write(Long.toString(used) + \" \" + Long.toString(timer.now()));\n        // This is only called as part of the volume shutdown.\n        // We explicitly avoid calling flush with fileIoProvider which triggers\n        // volume check upon io exception to avoid cyclic volume checks.\n        out.flush();\n      }\n    } catch (IOException ioe) {\n      // If write failed, the volume might be bad. Since the cache file is\n      // not critical, log the error and continue.\n      FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10958. Add instrumentation hooks around Datanode disk IO.\n",
      "commitDate": "14/12/16 11:18 AM",
      "commitName": "6ba9587d370fbf39c129c08c00ebbb894ccc1389",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "06/12/16 11:05 AM",
      "commitNameOld": "df983b524ab68ea0c70cee9033bfff2d28052cbf",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 8.01,
      "commitsBetweenForRepo": 51,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   void saveDfsUsed() {\n     File outFile \u003d new File(currentDir, DU_CACHE_FILE);\n-    if (outFile.exists() \u0026\u0026 !outFile.delete()) {\n+    if (!fileIoProvider.deleteWithExistsCheck(volume, outFile)) {\n       FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n         outFile.getParent());\n     }\n \n     try {\n       long used \u003d getDfsUsed();\n       try (Writer out \u003d new OutputStreamWriter(\n           new FileOutputStream(outFile), \"UTF-8\")) {\n         // mtime is written last, so that truncated writes won\u0027t be valid.\n         out.write(Long.toString(used) + \" \" + Long.toString(timer.now()));\n-        out.flush();\n+        fileIoProvider.flush(volume, out);\n       }\n     } catch (IOException ioe) {\n       // If write failed, the volume might be bad. Since the cache file is\n       // not critical, log the error and continue.\n       FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void saveDfsUsed() {\n    File outFile \u003d new File(currentDir, DU_CACHE_FILE);\n    if (!fileIoProvider.deleteWithExistsCheck(volume, outFile)) {\n      FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n        outFile.getParent());\n    }\n\n    try {\n      long used \u003d getDfsUsed();\n      try (Writer out \u003d new OutputStreamWriter(\n          new FileOutputStream(outFile), \"UTF-8\")) {\n        // mtime is written last, so that truncated writes won\u0027t be valid.\n        out.write(Long.toString(used) + \" \" + Long.toString(timer.now()));\n        fileIoProvider.flush(volume, out);\n      }\n    } catch (IOException ioe) {\n      // If write failed, the volume might be bad. Since the cache file is\n      // not critical, log the error and continue.\n      FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "c07f7fa8ff752436726239d938e0461236839acf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9624. DataNode start slowly due to the initial DU command operations. (Lin Yiqun via wang)\n",
      "commitDate": "15/01/16 11:28 AM",
      "commitName": "c07f7fa8ff752436726239d938e0461236839acf",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "29/09/15 1:20 AM",
      "commitNameOld": "d6fa34e014b0e2a61b24f05dd08ebe12354267fd",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 108.46,
      "commitsBetweenForRepo": 737,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   void saveDfsUsed() {\n     File outFile \u003d new File(currentDir, DU_CACHE_FILE);\n     if (outFile.exists() \u0026\u0026 !outFile.delete()) {\n       FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n         outFile.getParent());\n     }\n \n     try {\n       long used \u003d getDfsUsed();\n       try (Writer out \u003d new OutputStreamWriter(\n           new FileOutputStream(outFile), \"UTF-8\")) {\n         // mtime is written last, so that truncated writes won\u0027t be valid.\n-        out.write(Long.toString(used) + \" \" + Long.toString(Time.now()));\n+        out.write(Long.toString(used) + \" \" + Long.toString(timer.now()));\n         out.flush();\n       }\n     } catch (IOException ioe) {\n       // If write failed, the volume might be bad. Since the cache file is\n       // not critical, log the error and continue.\n       FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void saveDfsUsed() {\n    File outFile \u003d new File(currentDir, DU_CACHE_FILE);\n    if (outFile.exists() \u0026\u0026 !outFile.delete()) {\n      FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n        outFile.getParent());\n    }\n\n    try {\n      long used \u003d getDfsUsed();\n      try (Writer out \u003d new OutputStreamWriter(\n          new FileOutputStream(outFile), \"UTF-8\")) {\n        // mtime is written last, so that truncated writes won\u0027t be valid.\n        out.write(Long.toString(used) + \" \" + Long.toString(timer.now()));\n        out.flush();\n      }\n    } catch (IOException ioe) {\n      // If write failed, the volume might be bad. Since the cache file is\n      // not critical, log the error and continue.\n      FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7515. Fix new findbugs warnings in hadoop-hdfs. Contributed by Haohui Mai.\n",
      "commitDate": "11/12/14 12:36 PM",
      "commitName": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/11/14 9:57 AM",
      "commitNameOld": "058af60c56207907f2bedf76df4284e86d923e0c",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 15.11,
      "commitsBetweenForRepo": 103,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,21 @@\n   void saveDfsUsed() {\n     File outFile \u003d new File(currentDir, DU_CACHE_FILE);\n     if (outFile.exists() \u0026\u0026 !outFile.delete()) {\n       FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n         outFile.getParent());\n     }\n \n-    FileWriter out \u003d null;\n     try {\n       long used \u003d getDfsUsed();\n-      if (used \u003e 0) {\n-        out \u003d new FileWriter(outFile);\n+      try (Writer out \u003d new OutputStreamWriter(\n+          new FileOutputStream(outFile), \"UTF-8\")) {\n         // mtime is written last, so that truncated writes won\u0027t be valid.\n         out.write(Long.toString(used) + \" \" + Long.toString(Time.now()));\n         out.flush();\n-        out.close();\n-        out \u003d null;\n       }\n     } catch (IOException ioe) {\n       // If write failed, the volume might be bad. Since the cache file is\n       // not critical, log the error and continue.\n       FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n-    } finally {\n-      IOUtils.cleanup(null, out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void saveDfsUsed() {\n    File outFile \u003d new File(currentDir, DU_CACHE_FILE);\n    if (outFile.exists() \u0026\u0026 !outFile.delete()) {\n      FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n        outFile.getParent());\n    }\n\n    try {\n      long used \u003d getDfsUsed();\n      try (Writer out \u003d new OutputStreamWriter(\n          new FileOutputStream(outFile), \"UTF-8\")) {\n        // mtime is written last, so that truncated writes won\u0027t be valid.\n        out.write(Long.toString(used) + \" \" + Long.toString(Time.now()));\n        out.flush();\n      }\n    } catch (IOException ioe) {\n      // If write failed, the volume might be bad. Since the cache file is\n      // not critical, log the error and continue.\n      FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "c8182ea76412e49c0c98ee252321c584fabb4c59": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6020. Fix the five findbugs warnings. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1572165 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/02/14 9:07 AM",
      "commitName": "c8182ea76412e49c0c98ee252321c584fabb4c59",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "25/02/14 11:27 AM",
      "commitNameOld": "fa6e59891c6125ae83fd601dbbcf928685f5dbfd",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.9,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,26 @@\n   void saveDfsUsed() {\n     File outFile \u003d new File(currentDir, DU_CACHE_FILE);\n-    if (outFile.exists()) {\n-      outFile.delete();\n+    if (outFile.exists() \u0026\u0026 !outFile.delete()) {\n+      FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n+        outFile.getParent());\n     }\n \n+    FileWriter out \u003d null;\n     try {\n       long used \u003d getDfsUsed();\n       if (used \u003e 0) {\n-        FileWriter out \u003d new FileWriter(outFile);\n+        out \u003d new FileWriter(outFile);\n         // mtime is written last, so that truncated writes won\u0027t be valid.\n         out.write(Long.toString(used) + \" \" + Long.toString(Time.now()));\n         out.flush();\n         out.close();\n+        out \u003d null;\n       }\n     } catch (IOException ioe) {\n       // If write failed, the volume might be bad. Since the cache file is\n       // not critical, log the error and continue.\n       FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n+    } finally {\n+      IOUtils.cleanup(null, out);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void saveDfsUsed() {\n    File outFile \u003d new File(currentDir, DU_CACHE_FILE);\n    if (outFile.exists() \u0026\u0026 !outFile.delete()) {\n      FsDatasetImpl.LOG.warn(\"Failed to delete old dfsUsed file in \" +\n        outFile.getParent());\n    }\n\n    FileWriter out \u003d null;\n    try {\n      long used \u003d getDfsUsed();\n      if (used \u003e 0) {\n        out \u003d new FileWriter(outFile);\n        // mtime is written last, so that truncated writes won\u0027t be valid.\n        out.write(Long.toString(used) + \" \" + Long.toString(Time.now()));\n        out.flush();\n        out.close();\n        out \u003d null;\n      }\n    } catch (IOException ioe) {\n      // If write failed, the volume might be bad. Since the cache file is\n      // not critical, log the error and continue.\n      FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n    } finally {\n      IOUtils.cleanup(null, out);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java",
      "extendedDetails": {}
    },
    "fa6e59891c6125ae83fd601dbbcf928685f5dbfd": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5498. Improve datanode startup time. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1571797 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/02/14 11:27 AM",
      "commitName": "fa6e59891c6125ae83fd601dbbcf928685f5dbfd",
      "commitAuthor": "Kihwal Lee",
      "diff": "@@ -0,0 +1,21 @@\n+  void saveDfsUsed() {\n+    File outFile \u003d new File(currentDir, DU_CACHE_FILE);\n+    if (outFile.exists()) {\n+      outFile.delete();\n+    }\n+\n+    try {\n+      long used \u003d getDfsUsed();\n+      if (used \u003e 0) {\n+        FileWriter out \u003d new FileWriter(outFile);\n+        // mtime is written last, so that truncated writes won\u0027t be valid.\n+        out.write(Long.toString(used) + \" \" + Long.toString(Time.now()));\n+        out.flush();\n+        out.close();\n+      }\n+    } catch (IOException ioe) {\n+      // If write failed, the volume might be bad. Since the cache file is\n+      // not critical, log the error and continue.\n+      FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void saveDfsUsed() {\n    File outFile \u003d new File(currentDir, DU_CACHE_FILE);\n    if (outFile.exists()) {\n      outFile.delete();\n    }\n\n    try {\n      long used \u003d getDfsUsed();\n      if (used \u003e 0) {\n        FileWriter out \u003d new FileWriter(outFile);\n        // mtime is written last, so that truncated writes won\u0027t be valid.\n        out.write(Long.toString(used) + \" \" + Long.toString(Time.now()));\n        out.flush();\n        out.close();\n      }\n    } catch (IOException ioe) {\n      // If write failed, the volume might be bad. Since the cache file is\n      // not critical, log the error and continue.\n      FsDatasetImpl.LOG.warn(\"Failed to write dfsUsed to \" + outFile, ioe);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java"
    }
  }
}