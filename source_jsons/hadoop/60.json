{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "OpenFileCtxCache.java",
  "functionName": "put",
  "functionId": "put___h-FileHandle__context-OpenFileCtx",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java",
  "functionStartLine": 118,
  "functionEndLine": 145,
  "numCommitsSeen": 10,
  "timeTaken": 2346,
  "changeHistory": [
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
    "7b1fa5693efc687492776d43ab482601cbb30dfd",
    "1b5cceaffbdde50a87ede81552dc380832db8e79",
    "b9522e86a55564c2ccb5ca3f1ca871965cbe74de",
    "b4564103e4709caa1135f6ccc2864d90e54f2ac9",
    "3fccdec6e0a8e9305fc75921211c3745eddb9c45"
  ],
  "changeHistoryShort": {
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": "Ybodychange",
    "7b1fa5693efc687492776d43ab482601cbb30dfd": "Ybodychange",
    "1b5cceaffbdde50a87ede81552dc380832db8e79": "Ybodychange",
    "b9522e86a55564c2ccb5ca3f1ca871965cbe74de": "Ybodychange",
    "b4564103e4709caa1135f6ccc2864d90e54f2ac9": "Ybodychange",
    "3fccdec6e0a8e9305fc75921211c3745eddb9c45": "Yintroduced"
  },
  "changeHistoryDetails": {
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13695. Move logging to slf4j in HDFS package. Contributed by Ian Pickering.\n",
      "commitDate": "06/09/18 2:48 PM",
      "commitName": "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "27/08/18 10:18 AM",
      "commitNameOld": "7b1fa5693efc687492776d43ab482601cbb30dfd",
      "commitAuthorOld": "Giovanni Matteo Fumarola",
      "daysBetweenCommits": 10.19,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   boolean put(FileHandle h, OpenFileCtx context) {\n     OpenFileCtx toEvict \u003d null;\n     synchronized (this) {\n       Preconditions.checkState(size() \u003c\u003d this.maxStreams,\n           \"stream cache size \" + size() + \"  is larger than maximum\" + this\n               .maxStreams);\n       if (size() \u003d\u003d this.maxStreams) {\n         Entry\u003cFileHandle, OpenFileCtx\u003e pairs \u003d getEntryToEvict();\n         if (pairs \u003d\u003dnull) {\n           return false;\n         } else {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Evict stream ctx: \" + pairs.getValue());\n           }\n           toEvict \u003d openFileMap.remove(pairs.getKey());\n           Preconditions.checkState(toEvict \u003d\u003d pairs.getValue(),\n               \"The deleted entry is not the same as odlest found.\");\n         }\n       }\n       openFileMap.put(h, context);\n     }\n     \n     // Cleanup the old stream outside the lock\n     if (toEvict !\u003d null) {\n-      toEvict.cleanupWithLogger();\n+      toEvict.cleanup();\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean put(FileHandle h, OpenFileCtx context) {\n    OpenFileCtx toEvict \u003d null;\n    synchronized (this) {\n      Preconditions.checkState(size() \u003c\u003d this.maxStreams,\n          \"stream cache size \" + size() + \"  is larger than maximum\" + this\n              .maxStreams);\n      if (size() \u003d\u003d this.maxStreams) {\n        Entry\u003cFileHandle, OpenFileCtx\u003e pairs \u003d getEntryToEvict();\n        if (pairs \u003d\u003dnull) {\n          return false;\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Evict stream ctx: \" + pairs.getValue());\n          }\n          toEvict \u003d openFileMap.remove(pairs.getKey());\n          Preconditions.checkState(toEvict \u003d\u003d pairs.getValue(),\n              \"The deleted entry is not the same as odlest found.\");\n        }\n      }\n      openFileMap.put(h, context);\n    }\n    \n    // Cleanup the old stream outside the lock\n    if (toEvict !\u003d null) {\n      toEvict.cleanup();\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java",
      "extendedDetails": {}
    },
    "7b1fa5693efc687492776d43ab482601cbb30dfd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13849. Migrate logging to slf4j in hadoop-hdfs-httpfs, hadoop-hdfs-nfs, hadoop-hdfs-rbf, hadoop-hdfs-native-client. Contributed by Ian Pickering.\n",
      "commitDate": "27/08/18 10:18 AM",
      "commitName": "7b1fa5693efc687492776d43ab482601cbb30dfd",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "10/10/17 10:38 AM",
      "commitNameOld": "d6602b5f39833611b4afa4581552f6c4c37e23a8",
      "commitAuthorOld": "Jitendra Pandey",
      "daysBetweenCommits": 320.99,
      "commitsBetweenForRepo": 2846,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   boolean put(FileHandle h, OpenFileCtx context) {\n     OpenFileCtx toEvict \u003d null;\n     synchronized (this) {\n       Preconditions.checkState(size() \u003c\u003d this.maxStreams,\n           \"stream cache size \" + size() + \"  is larger than maximum\" + this\n               .maxStreams);\n       if (size() \u003d\u003d this.maxStreams) {\n         Entry\u003cFileHandle, OpenFileCtx\u003e pairs \u003d getEntryToEvict();\n         if (pairs \u003d\u003dnull) {\n           return false;\n         } else {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Evict stream ctx: \" + pairs.getValue());\n           }\n           toEvict \u003d openFileMap.remove(pairs.getKey());\n           Preconditions.checkState(toEvict \u003d\u003d pairs.getValue(),\n               \"The deleted entry is not the same as odlest found.\");\n         }\n       }\n       openFileMap.put(h, context);\n     }\n     \n     // Cleanup the old stream outside the lock\n     if (toEvict !\u003d null) {\n-      toEvict.cleanup();\n+      toEvict.cleanupWithLogger();\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean put(FileHandle h, OpenFileCtx context) {\n    OpenFileCtx toEvict \u003d null;\n    synchronized (this) {\n      Preconditions.checkState(size() \u003c\u003d this.maxStreams,\n          \"stream cache size \" + size() + \"  is larger than maximum\" + this\n              .maxStreams);\n      if (size() \u003d\u003d this.maxStreams) {\n        Entry\u003cFileHandle, OpenFileCtx\u003e pairs \u003d getEntryToEvict();\n        if (pairs \u003d\u003dnull) {\n          return false;\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Evict stream ctx: \" + pairs.getValue());\n          }\n          toEvict \u003d openFileMap.remove(pairs.getKey());\n          Preconditions.checkState(toEvict \u003d\u003d pairs.getValue(),\n              \"The deleted entry is not the same as odlest found.\");\n        }\n      }\n      openFileMap.put(h, context);\n    }\n    \n    // Cleanup the old stream outside the lock\n    if (toEvict !\u003d null) {\n      toEvict.cleanupWithLogger();\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java",
      "extendedDetails": {}
    },
    "1b5cceaffbdde50a87ede81552dc380832db8e79": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-11201. Spelling errors in the logging, help, assertions and exception messages. Contributed by Grant Sohn.\"\n\nThis reverts commit b9522e86a55564c2ccb5ca3f1ca871965cbe74de.\n",
      "commitDate": "05/12/16 10:54 AM",
      "commitName": "1b5cceaffbdde50a87ede81552dc380832db8e79",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "05/12/16 10:48 AM",
      "commitNameOld": "b9522e86a55564c2ccb5ca3f1ca871965cbe74de",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   boolean put(FileHandle h, OpenFileCtx context) {\n     OpenFileCtx toEvict \u003d null;\n     synchronized (this) {\n       Preconditions.checkState(size() \u003c\u003d this.maxStreams,\n           \"stream cache size \" + size() + \"  is larger than maximum\" + this\n               .maxStreams);\n       if (size() \u003d\u003d this.maxStreams) {\n         Entry\u003cFileHandle, OpenFileCtx\u003e pairs \u003d getEntryToEvict();\n         if (pairs \u003d\u003dnull) {\n           return false;\n         } else {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Evict stream ctx: \" + pairs.getValue());\n           }\n           toEvict \u003d openFileMap.remove(pairs.getKey());\n           Preconditions.checkState(toEvict \u003d\u003d pairs.getValue(),\n-              \"The deleted entry is not the same as oldest found.\");\n+              \"The deleted entry is not the same as odlest found.\");\n         }\n       }\n       openFileMap.put(h, context);\n     }\n     \n     // Cleanup the old stream outside the lock\n     if (toEvict !\u003d null) {\n       toEvict.cleanup();\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean put(FileHandle h, OpenFileCtx context) {\n    OpenFileCtx toEvict \u003d null;\n    synchronized (this) {\n      Preconditions.checkState(size() \u003c\u003d this.maxStreams,\n          \"stream cache size \" + size() + \"  is larger than maximum\" + this\n              .maxStreams);\n      if (size() \u003d\u003d this.maxStreams) {\n        Entry\u003cFileHandle, OpenFileCtx\u003e pairs \u003d getEntryToEvict();\n        if (pairs \u003d\u003dnull) {\n          return false;\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Evict stream ctx: \" + pairs.getValue());\n          }\n          toEvict \u003d openFileMap.remove(pairs.getKey());\n          Preconditions.checkState(toEvict \u003d\u003d pairs.getValue(),\n              \"The deleted entry is not the same as odlest found.\");\n        }\n      }\n      openFileMap.put(h, context);\n    }\n    \n    // Cleanup the old stream outside the lock\n    if (toEvict !\u003d null) {\n      toEvict.cleanup();\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java",
      "extendedDetails": {}
    },
    "b9522e86a55564c2ccb5ca3f1ca871965cbe74de": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11201. Spelling errors in the logging, help, assertions and exception messages. Contributed by Grant Sohn.\n",
      "commitDate": "05/12/16 10:48 AM",
      "commitName": "b9522e86a55564c2ccb5ca3f1ca871965cbe74de",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "19/10/16 5:20 PM",
      "commitNameOld": "b4564103e4709caa1135f6ccc2864d90e54f2ac9",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 46.77,
      "commitsBetweenForRepo": 378,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   boolean put(FileHandle h, OpenFileCtx context) {\n     OpenFileCtx toEvict \u003d null;\n     synchronized (this) {\n       Preconditions.checkState(size() \u003c\u003d this.maxStreams,\n           \"stream cache size \" + size() + \"  is larger than maximum\" + this\n               .maxStreams);\n       if (size() \u003d\u003d this.maxStreams) {\n         Entry\u003cFileHandle, OpenFileCtx\u003e pairs \u003d getEntryToEvict();\n         if (pairs \u003d\u003dnull) {\n           return false;\n         } else {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Evict stream ctx: \" + pairs.getValue());\n           }\n           toEvict \u003d openFileMap.remove(pairs.getKey());\n           Preconditions.checkState(toEvict \u003d\u003d pairs.getValue(),\n-              \"The deleted entry is not the same as odlest found.\");\n+              \"The deleted entry is not the same as oldest found.\");\n         }\n       }\n       openFileMap.put(h, context);\n     }\n     \n     // Cleanup the old stream outside the lock\n     if (toEvict !\u003d null) {\n       toEvict.cleanup();\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean put(FileHandle h, OpenFileCtx context) {\n    OpenFileCtx toEvict \u003d null;\n    synchronized (this) {\n      Preconditions.checkState(size() \u003c\u003d this.maxStreams,\n          \"stream cache size \" + size() + \"  is larger than maximum\" + this\n              .maxStreams);\n      if (size() \u003d\u003d this.maxStreams) {\n        Entry\u003cFileHandle, OpenFileCtx\u003e pairs \u003d getEntryToEvict();\n        if (pairs \u003d\u003dnull) {\n          return false;\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Evict stream ctx: \" + pairs.getValue());\n          }\n          toEvict \u003d openFileMap.remove(pairs.getKey());\n          Preconditions.checkState(toEvict \u003d\u003d pairs.getValue(),\n              \"The deleted entry is not the same as oldest found.\");\n        }\n      }\n      openFileMap.put(h, context);\n    }\n    \n    // Cleanup the old stream outside the lock\n    if (toEvict !\u003d null) {\n      toEvict.cleanup();\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java",
      "extendedDetails": {}
    },
    "b4564103e4709caa1135f6ccc2864d90e54f2ac9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10752. Several log refactoring/improvement suggestion in HDFS. Contributed by Hanisha Koneru.\n",
      "commitDate": "19/10/16 5:20 PM",
      "commitName": "b4564103e4709caa1135f6ccc2864d90e54f2ac9",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "22/10/14 9:27 PM",
      "commitNameOld": "d71d40a63d198991077d5babd70be5e9787a53f1",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 727.83,
      "commitsBetweenForRepo": 5526,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   boolean put(FileHandle h, OpenFileCtx context) {\n     OpenFileCtx toEvict \u003d null;\n     synchronized (this) {\n-      Preconditions.checkState(openFileMap.size() \u003c\u003d this.maxStreams,\n-          \"stream cache size \" + openFileMap.size()\n-              + \"  is larger than maximum\" + this.maxStreams);\n-      if (openFileMap.size() \u003d\u003d this.maxStreams) {\n+      Preconditions.checkState(size() \u003c\u003d this.maxStreams,\n+          \"stream cache size \" + size() + \"  is larger than maximum\" + this\n+              .maxStreams);\n+      if (size() \u003d\u003d this.maxStreams) {\n         Entry\u003cFileHandle, OpenFileCtx\u003e pairs \u003d getEntryToEvict();\n         if (pairs \u003d\u003dnull) {\n           return false;\n         } else {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Evict stream ctx: \" + pairs.getValue());\n           }\n           toEvict \u003d openFileMap.remove(pairs.getKey());\n           Preconditions.checkState(toEvict \u003d\u003d pairs.getValue(),\n               \"The deleted entry is not the same as odlest found.\");\n         }\n       }\n       openFileMap.put(h, context);\n     }\n     \n     // Cleanup the old stream outside the lock\n     if (toEvict !\u003d null) {\n       toEvict.cleanup();\n     }\n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean put(FileHandle h, OpenFileCtx context) {\n    OpenFileCtx toEvict \u003d null;\n    synchronized (this) {\n      Preconditions.checkState(size() \u003c\u003d this.maxStreams,\n          \"stream cache size \" + size() + \"  is larger than maximum\" + this\n              .maxStreams);\n      if (size() \u003d\u003d this.maxStreams) {\n        Entry\u003cFileHandle, OpenFileCtx\u003e pairs \u003d getEntryToEvict();\n        if (pairs \u003d\u003dnull) {\n          return false;\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Evict stream ctx: \" + pairs.getValue());\n          }\n          toEvict \u003d openFileMap.remove(pairs.getKey());\n          Preconditions.checkState(toEvict \u003d\u003d pairs.getValue(),\n              \"The deleted entry is not the same as odlest found.\");\n        }\n      }\n      openFileMap.put(h, context);\n    }\n    \n    // Cleanup the old stream outside the lock\n    if (toEvict !\u003d null) {\n      toEvict.cleanup();\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java",
      "extendedDetails": {}
    },
    "3fccdec6e0a8e9305fc75921211c3745eddb9c45": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5364. Add OpenFileCtx cache. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1539834 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/11/13 1:49 PM",
      "commitName": "3fccdec6e0a8e9305fc75921211c3745eddb9c45",
      "commitAuthor": "Brandon Li",
      "diff": "@@ -0,0 +1,28 @@\n+  boolean put(FileHandle h, OpenFileCtx context) {\n+    OpenFileCtx toEvict \u003d null;\n+    synchronized (this) {\n+      Preconditions.checkState(openFileMap.size() \u003c\u003d this.maxStreams,\n+          \"stream cache size \" + openFileMap.size()\n+              + \"  is larger than maximum\" + this.maxStreams);\n+      if (openFileMap.size() \u003d\u003d this.maxStreams) {\n+        Entry\u003cFileHandle, OpenFileCtx\u003e pairs \u003d getEntryToEvict();\n+        if (pairs \u003d\u003dnull) {\n+          return false;\n+        } else {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Evict stream ctx: \" + pairs.getValue());\n+          }\n+          toEvict \u003d openFileMap.remove(pairs.getKey());\n+          Preconditions.checkState(toEvict \u003d\u003d pairs.getValue(),\n+              \"The deleted entry is not the same as odlest found.\");\n+        }\n+      }\n+      openFileMap.put(h, context);\n+    }\n+    \n+    // Cleanup the old stream outside the lock\n+    if (toEvict !\u003d null) {\n+      toEvict.cleanup();\n+    }\n+    return true;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  boolean put(FileHandle h, OpenFileCtx context) {\n    OpenFileCtx toEvict \u003d null;\n    synchronized (this) {\n      Preconditions.checkState(openFileMap.size() \u003c\u003d this.maxStreams,\n          \"stream cache size \" + openFileMap.size()\n              + \"  is larger than maximum\" + this.maxStreams);\n      if (openFileMap.size() \u003d\u003d this.maxStreams) {\n        Entry\u003cFileHandle, OpenFileCtx\u003e pairs \u003d getEntryToEvict();\n        if (pairs \u003d\u003dnull) {\n          return false;\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Evict stream ctx: \" + pairs.getValue());\n          }\n          toEvict \u003d openFileMap.remove(pairs.getKey());\n          Preconditions.checkState(toEvict \u003d\u003d pairs.getValue(),\n              \"The deleted entry is not the same as odlest found.\");\n        }\n      }\n      openFileMap.put(h, context);\n    }\n    \n    // Cleanup the old stream outside the lock\n    if (toEvict !\u003d null) {\n      toEvict.cleanup();\n    }\n    return true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtxCache.java"
    }
  }
}