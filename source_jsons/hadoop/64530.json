{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "S3AFileSystem.java",
  "functionName": "initialize",
  "functionId": "initialize___name-URI__originalConf-Configuration",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
  "functionStartLine": 317,
  "functionEndLine": 464,
  "numCommitsSeen": 193,
  "timeTaken": 21715,
  "changeHistory": [
    "e77767bb1e8dfb8b0bd7af4664c900f7238b4fa0",
    "56dee667707926f3796c7757be1a133a362f05c9",
    "990063d2af0a37e9474949f33128805e34c3f016",
    "dca19fc3aa509949daf29bc902b2f74760407fc4",
    "559ee277f50716a9a8c736ba3b655aad9f616e96",
    "e02b1023c2f42b6792d2941c1f987ae06259b021",
    "c58e11bf521d746842ce16724211a2a0339d7b61",
    "34747c373f40a33d36a2e21ecb33fa791ccd939f",
    "e02eb24e0a9139418120027b694492e0738df20a",
    "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
    "a36274d69947648dbe82721220cc5240ec5d396d",
    "0af4011580878566213016af0c32633eabd15100",
    "6fa229891e06eea62cb9634efde755f40247e816",
    "f365957c6326f88734bc0a5d01cfb7eac713db20",
    "6d0bffe17eadedd60d4599427248b0db4a7c5502",
    "7f783970364930cc461d1a73833bc58cdd10553e",
    "d7152332b32a575c3a92e3f4c44b95e58462528d",
    "046b8768af8a07a9e10ce43f538d6ac16e7fa5f3",
    "da9a39eed138210de29b59b90c449b28da1c04f9",
    "8110d6a0d59e7dc2ddb25fa424fab188c5e9ce35",
    "7ac88244c54ce483729af3d2736d9f4731e230ca",
    "1093a73689912f78547e6d23023be2fd1c7ddc85",
    "f274fe33ea359d26a31efec42a856320a0dbb5f4",
    "35ad9b1dd279b769381ea1625d9bf776c309c5cb",
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c",
    "47011d7dd300b0c74bb6cfe25b918c479d718f4f",
    "49467165a57fb77932d1d526796624b88ebacd91",
    "5bbca80428ffbe776650652de86a3bba885edb31",
    "621b43e254afaff708cd6fc4698b29628f6abc33",
    "667966c13c1e09077c2e2088bd66c9d7851dd14e",
    "2e30aa72e01de7b5774fcb312406a393221e0908",
    "2158496f6bed5f9d14751b82bd5d43b9fd786b95",
    "839b690ed5edc2ac4984640d58c005bb63cd8a07",
    "e648b6e1382336af69434dfbf9161bced3caa244",
    "a1761a841e95ef7d2296ac3e40b3a26d97787eab",
    "3372e940303149d6258e0b72c54d72f080f0daa2",
    "6c348c56918973fd988b110e79231324a8befe12",
    "d152557cf7f4d2288524c222fcbaf152bdc038b0",
    "763f0497bb996e331e40caed9ca0af966f5b3fac",
    "822d661b8fcc42bec6eea958d9fd02ef1aaa4b6c",
    "37362c2f922b8d038002e61132b110ae4dd6d5ba",
    "4ee3543625c77c06d566fe81644d21c607d6d74d",
    "4aefe119a0203c03cdc893dcb3330fd37f26f0ee",
    "c58a59f7081d55dd2108545ebf9ee48cf43ca944",
    "39ec1515a205952eda7e171408a8b83eceb4abde",
    "757050ff355d40bc28f9dbfd0c0083c5f337d270",
    "27c4e90efce04e1b1302f668b5eb22412e00d033",
    "def2a6d3856452d5c804f04e5bf485541a3bc53a",
    "76fab26c5c02cef38924d04136407489fd9457d9",
    "29ae25801380b94442253c4202dee782dc4713f5",
    "bff7c90a5686de106ca7a866982412c5dfa01632",
    "d5403747b57b1e294e533ce17f197e7be8f5339c",
    "64443490d7f189e8e42d284615f3814ef751395a",
    "15b7076ad5f2ae92d231140b2f8cebc392a92c87",
    "709ff99cff4124823bde631e272af7be9a22f83b",
    "00b80958d862dbcc08d6f186c09963d3351ba0fd",
    "4e7ad4f0a88bd36bc91db6b1bd311d7f5c6bebee",
    "2908fe4ec52f78d74e4207274a34d88d54cd468f",
    "000ca83ea3aeb3687625c857bcc0762fd2887db2",
    "27d8395867f665fea1360087325cda5ed70efd0c",
    "6ba52d88ec11444cbac946ffadbc645acd0657de",
    "0ac760a58d96b36ab30e9d60679bbea6365ef120",
    "24d920b80eb3626073925a1d0b6dcf148add8cc0"
  ],
  "changeHistoryShort": {
    "e77767bb1e8dfb8b0bd7af4664c900f7238b4fa0": "Ybodychange",
    "56dee667707926f3796c7757be1a133a362f05c9": "Ybodychange",
    "990063d2af0a37e9474949f33128805e34c3f016": "Ybodychange",
    "dca19fc3aa509949daf29bc902b2f74760407fc4": "Ybodychange",
    "559ee277f50716a9a8c736ba3b655aad9f616e96": "Ybodychange",
    "e02b1023c2f42b6792d2941c1f987ae06259b021": "Ybodychange",
    "c58e11bf521d746842ce16724211a2a0339d7b61": "Ybodychange",
    "34747c373f40a33d36a2e21ecb33fa791ccd939f": "Ybodychange",
    "e02eb24e0a9139418120027b694492e0738df20a": "Ybodychange",
    "f9cc9e162175444efe9d5b07ecb9a795f750ca3c": "Ybodychange",
    "a36274d69947648dbe82721220cc5240ec5d396d": "Ybodychange",
    "0af4011580878566213016af0c32633eabd15100": "Ybodychange",
    "6fa229891e06eea62cb9634efde755f40247e816": "Ybodychange",
    "f365957c6326f88734bc0a5d01cfb7eac713db20": "Ybodychange",
    "6d0bffe17eadedd60d4599427248b0db4a7c5502": "Ybodychange",
    "7f783970364930cc461d1a73833bc58cdd10553e": "Ybodychange",
    "d7152332b32a575c3a92e3f4c44b95e58462528d": "Ybodychange",
    "046b8768af8a07a9e10ce43f538d6ac16e7fa5f3": "Ybodychange",
    "da9a39eed138210de29b59b90c449b28da1c04f9": "Ybodychange",
    "8110d6a0d59e7dc2ddb25fa424fab188c5e9ce35": "Ybodychange",
    "7ac88244c54ce483729af3d2736d9f4731e230ca": "Ybodychange",
    "1093a73689912f78547e6d23023be2fd1c7ddc85": "Ybodychange",
    "f274fe33ea359d26a31efec42a856320a0dbb5f4": "Ybodychange",
    "35ad9b1dd279b769381ea1625d9bf776c309c5cb": "Ybodychange",
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c": "Ybodychange",
    "47011d7dd300b0c74bb6cfe25b918c479d718f4f": "Ybodychange",
    "49467165a57fb77932d1d526796624b88ebacd91": "Ybodychange",
    "5bbca80428ffbe776650652de86a3bba885edb31": "Ybodychange",
    "621b43e254afaff708cd6fc4698b29628f6abc33": "Ybodychange",
    "667966c13c1e09077c2e2088bd66c9d7851dd14e": "Ybodychange",
    "2e30aa72e01de7b5774fcb312406a393221e0908": "Ybodychange",
    "2158496f6bed5f9d14751b82bd5d43b9fd786b95": "Ybodychange",
    "839b690ed5edc2ac4984640d58c005bb63cd8a07": "Ybodychange",
    "e648b6e1382336af69434dfbf9161bced3caa244": "Ymultichange(Yparameterchange,Ybodychange)",
    "a1761a841e95ef7d2296ac3e40b3a26d97787eab": "Ybodychange",
    "3372e940303149d6258e0b72c54d72f080f0daa2": "Ybodychange",
    "6c348c56918973fd988b110e79231324a8befe12": "Ybodychange",
    "d152557cf7f4d2288524c222fcbaf152bdc038b0": "Ybodychange",
    "763f0497bb996e331e40caed9ca0af966f5b3fac": "Ybodychange",
    "822d661b8fcc42bec6eea958d9fd02ef1aaa4b6c": "Ybodychange",
    "37362c2f922b8d038002e61132b110ae4dd6d5ba": "Ybodychange",
    "4ee3543625c77c06d566fe81644d21c607d6d74d": "Ybodychange",
    "4aefe119a0203c03cdc893dcb3330fd37f26f0ee": "Ybodychange",
    "c58a59f7081d55dd2108545ebf9ee48cf43ca944": "Ybodychange",
    "39ec1515a205952eda7e171408a8b83eceb4abde": "Ybodychange",
    "757050ff355d40bc28f9dbfd0c0083c5f337d270": "Ybodychange",
    "27c4e90efce04e1b1302f668b5eb22412e00d033": "Ybodychange",
    "def2a6d3856452d5c804f04e5bf485541a3bc53a": "Ybodychange",
    "76fab26c5c02cef38924d04136407489fd9457d9": "Ybodychange",
    "29ae25801380b94442253c4202dee782dc4713f5": "Ybodychange",
    "bff7c90a5686de106ca7a866982412c5dfa01632": "Ybodychange",
    "d5403747b57b1e294e533ce17f197e7be8f5339c": "Ybodychange",
    "64443490d7f189e8e42d284615f3814ef751395a": "Ybodychange",
    "15b7076ad5f2ae92d231140b2f8cebc392a92c87": "Ybodychange",
    "709ff99cff4124823bde631e272af7be9a22f83b": "Ybodychange",
    "00b80958d862dbcc08d6f186c09963d3351ba0fd": "Ybodychange",
    "4e7ad4f0a88bd36bc91db6b1bd311d7f5c6bebee": "Ybodychange",
    "2908fe4ec52f78d74e4207274a34d88d54cd468f": "Ybodychange",
    "000ca83ea3aeb3687625c857bcc0762fd2887db2": "Ybodychange",
    "27d8395867f665fea1360087325cda5ed70efd0c": "Ybodychange",
    "6ba52d88ec11444cbac946ffadbc645acd0657de": "Ybodychange",
    "0ac760a58d96b36ab30e9d60679bbea6365ef120": "Ybodychange",
    "24d920b80eb3626073925a1d0b6dcf148add8cc0": "Yintroduced"
  },
  "changeHistoryDetails": {
    "e77767bb1e8dfb8b0bd7af4664c900f7238b4fa0": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16711.\n\nThis adds a new option fs.s3a.bucket.probe, range (0-2) to\ncontrol which probe for a bucket existence to perform on startup.\n\n0: no checks\n1: v1 check (as has been performend until now)\n2: v2 bucket check, which also incudes a permission check. Default.\n\nWhen set to 0, bucket existence checks won\u0027t be done\nduring initialization thus making it faster.\nWhen the bucket is not available in S3,\nor if fs.s3a.endpoint points to the wrong instance of a private S3 store\nconsecutive calls like listing, read, write etc. will fail with\nan UnknownStoreException.\n\nContributed by:\n  * Mukund Thakur (main patch and tests)\n  * Rajesh Balamohan (v0 list and performance tests)\n  * lqjacklee (HADOOP-15990/v2 list)\n  * Steve Loughran (UnknownStoreException support)\n\n       modified:   hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java\n       modified:   hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java\n       modified:   hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ARetryPolicy.java\n       modified:   hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AUtils.java\n       new file:   hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/UnknownStoreException.java\n       new file:   hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/ErrorTranslation.java\n       modified:   hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/index.md\n       modified:   hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/performance.md\n       modified:   hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/troubleshooting_s3a.md\n       modified:   hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/AbstractS3AMockTest.java\n       new file:   hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/ITestS3ABucketExistence.java\n       modified:   hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/MockS3ClientFactory.java\n       modified:   hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3AExceptionTranslation.java\n       modified:   hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/AbstractS3GuardToolTestBase.java\n       modified:   hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/ITestS3GuardToolDynamoDB.java\n       modified:   hadoop-tools/hadoop-aws/src/test/resources/core-site.xml\n\nChange-Id: Ic174f803e655af172d81c1274ed92b51bdceb384\n",
      "commitDate": "21/02/20 5:44 AM",
      "commitName": "e77767bb1e8dfb8b0bd7af4664c900f7238b4fa0",
      "commitAuthor": "Mukund Thakur",
      "commitDateOld": "13/02/20 11:09 AM",
      "commitNameOld": "56dee667707926f3796c7757be1a133a362f05c9",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 7.77,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,150 +1,148 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     try {\n       LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n       // clone the configuration into one with propagated bucket options\n       Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n       // patch the Hadoop security providers\n       patchSecurityCredentialProviders(conf);\n       // look for delegation token support early.\n       boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n       if (delegationTokensEnabled) {\n         LOG.debug(\"Using delegation tokens\");\n       }\n       // set the URI, this will do any fixup of the URI to remove secrets,\n       // canonicalize.\n       setUri(name, delegationTokensEnabled);\n       super.initialize(uri, conf);\n       setConf(conf);\n \n       // look for encryption data\n       // DT Bindings may override this\n       setEncryptionSecrets(new EncryptionSecrets(\n           getEncryptionAlgorithm(bucket, conf),\n           getServerSideEncryptionKey(bucket, getConf())));\n \n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n       owner \u003d UserGroupInformation.getCurrentUser();\n       username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n           FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       initThreadPools(conf);\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       signerManager \u003d new SignerManager(bucket, this, conf, owner);\n       signerManager.initCustomSigners();\n \n       // creates the AWS client, including overriding auth chain if\n       // the FS came with a DT\n       // this may do some patching of the configuration (e.g. setting\n       // the encryption algorithms)\n       bindAWSClient(name, delegationTokensEnabled);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       // This initiates a probe against S3 for the bucket existing.\n-      // It is where all network and authentication configuration issues\n-      // surface, and is potentially slow.\n-      verifyBucketExists();\n+      doBucketProbing();\n \n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n       LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       // instantiate S3 Select support\n       selectBinding \u003d new SelectBinding(writeHelper);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n           DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n \n       setMetadataStore(S3Guard.getMetadataStore(this, ttlTimeProvider));\n       allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n \n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n             getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n       }\n \n       // LOG if S3Guard is disabled on the warn level set in config\n       if (!hasMetadataStore()) {\n         String warnLevel \u003d conf.getTrimmed(S3GUARD_DISABLED_WARN_LEVEL,\n             DEFAULT_S3GUARD_DISABLED_WARN_LEVEL);\n         S3Guard.logS3GuardDisabled(LOG, warnLevel, bucket);\n       }\n \n       initMultipartUploads(conf);\n \n       pageSize \u003d intOption(getConf(), BULK_DELETE_PAGE_SIZE,\n           BULK_DELETE_PAGE_SIZE_DEFAULT, 0);\n     } catch (AmazonClientException e) {\n       // amazon client exception: stop all services then throw the translation\n       stopAllServices();\n       throw translateException(\"initializing \", new Path(name), e);\n     } catch (IOException | RuntimeException e) {\n       // other exceptions: stop the services.\n       stopAllServices();\n       throw e;\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    try {\n      LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n      // clone the configuration into one with propagated bucket options\n      Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n      // patch the Hadoop security providers\n      patchSecurityCredentialProviders(conf);\n      // look for delegation token support early.\n      boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n      if (delegationTokensEnabled) {\n        LOG.debug(\"Using delegation tokens\");\n      }\n      // set the URI, this will do any fixup of the URI to remove secrets,\n      // canonicalize.\n      setUri(name, delegationTokensEnabled);\n      super.initialize(uri, conf);\n      setConf(conf);\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n          FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      initThreadPools(conf);\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      signerManager \u003d new SignerManager(bucket, this, conf, owner);\n      signerManager.initCustomSigners();\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      // This initiates a probe against S3 for the bucket existing.\n      doBucketProbing();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n      LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      // instantiate S3 Select support\n      selectBinding \u003d new SelectBinding(writeHelper);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n          DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n\n      setMetadataStore(S3Guard.getMetadataStore(this, ttlTimeProvider));\n      allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n            getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n      }\n\n      // LOG if S3Guard is disabled on the warn level set in config\n      if (!hasMetadataStore()) {\n        String warnLevel \u003d conf.getTrimmed(S3GUARD_DISABLED_WARN_LEVEL,\n            DEFAULT_S3GUARD_DISABLED_WARN_LEVEL);\n        S3Guard.logS3GuardDisabled(LOG, warnLevel, bucket);\n      }\n\n      initMultipartUploads(conf);\n\n      pageSize \u003d intOption(getConf(), BULK_DELETE_PAGE_SIZE,\n          BULK_DELETE_PAGE_SIZE_DEFAULT, 0);\n    } catch (AmazonClientException e) {\n      // amazon client exception: stop all services then throw the translation\n      stopAllServices();\n      throw translateException(\"initializing \", new Path(name), e);\n    } catch (IOException | RuntimeException e) {\n      // other exceptions: stop the services.\n      stopAllServices();\n      throw e;\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "56dee667707926f3796c7757be1a133a362f05c9": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16823. Large DeleteObject requests are their own Thundering Herd.\n\nContributed by Steve Loughran.\n\nDuring S3A rename() and delete() calls, the list of objects delete is\nbuilt up into batches of a thousand and then POSTed in a single large\nDeleteObjects request.\n\nBut as the IO capacity allowed on an S3 partition may only be 3500 writes\nper second *and* each entry in that POST counts as a single write, then\none of those posts alone can trigger throttling on an already loaded\nS3 directory tree. Which can trigger backoff and retry, with the same\nthousand entry post, and so recreate the exact same problem.\n\nFixes\n\n* Page size for delete object requests is set in\n  fs.s3a.bulk.delete.page.size; the default is 250.\n* The property fs.s3a.experimental.aws.s3.throttling (default\u003dtrue)\n  can be set to false to disable throttle retry logic in the AWS\n  client SDK -it is all handled in the S3A client. This\n  gives more visibility in to when operations are being throttled\n* Bulk delete throttling events are logged to the log\n  org.apache.hadoop.fs.s3a.throttled log at INFO; if this appears\n  often then choose a smaller page size.\n* The metric \"store_io_throttled\" adds the entire count of delete\n  requests when a single DeleteObjects request is throttled.\n* A new quantile, \"store_io_throttle_rate\" can track throttling\n  load over time.\n* DynamoDB metastore throttle resilience issues have also been\n  identified and fixed. Note: the fs.s3a.experimental.aws.s3.throttling\n  flag does not apply to DDB IO precisely because there may still be\n  lurking issues there and it safest to rely on the DynamoDB client\n  SDK.\n\nChange-Id: I00f85cdd94fc008864d060533f6bd4870263fd84\n",
      "commitDate": "13/02/20 11:09 AM",
      "commitName": "56dee667707926f3796c7757be1a133a362f05c9",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "30/01/20 2:16 AM",
      "commitNameOld": "5977360878e6780bd04842c8a2156f9848e1d088",
      "commitAuthorOld": "Mustafa İman",
      "daysBetweenCommits": 14.37,
      "commitsBetweenForRepo": 35,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,147 +1,150 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     try {\n       LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n       // clone the configuration into one with propagated bucket options\n       Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n       // patch the Hadoop security providers\n       patchSecurityCredentialProviders(conf);\n       // look for delegation token support early.\n       boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n       if (delegationTokensEnabled) {\n         LOG.debug(\"Using delegation tokens\");\n       }\n       // set the URI, this will do any fixup of the URI to remove secrets,\n       // canonicalize.\n       setUri(name, delegationTokensEnabled);\n       super.initialize(uri, conf);\n       setConf(conf);\n \n       // look for encryption data\n       // DT Bindings may override this\n       setEncryptionSecrets(new EncryptionSecrets(\n           getEncryptionAlgorithm(bucket, conf),\n           getServerSideEncryptionKey(bucket, getConf())));\n \n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n       owner \u003d UserGroupInformation.getCurrentUser();\n       username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n           FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       initThreadPools(conf);\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       signerManager \u003d new SignerManager(bucket, this, conf, owner);\n       signerManager.initCustomSigners();\n \n       // creates the AWS client, including overriding auth chain if\n       // the FS came with a DT\n       // this may do some patching of the configuration (e.g. setting\n       // the encryption algorithms)\n       bindAWSClient(name, delegationTokensEnabled);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       // This initiates a probe against S3 for the bucket existing.\n       // It is where all network and authentication configuration issues\n       // surface, and is potentially slow.\n       verifyBucketExists();\n \n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n       LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       // instantiate S3 Select support\n       selectBinding \u003d new SelectBinding(writeHelper);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n           DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n \n       setMetadataStore(S3Guard.getMetadataStore(this, ttlTimeProvider));\n       allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n \n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n             getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n       }\n \n       // LOG if S3Guard is disabled on the warn level set in config\n       if (!hasMetadataStore()) {\n         String warnLevel \u003d conf.getTrimmed(S3GUARD_DISABLED_WARN_LEVEL,\n             DEFAULT_S3GUARD_DISABLED_WARN_LEVEL);\n         S3Guard.logS3GuardDisabled(LOG, warnLevel, bucket);\n       }\n \n       initMultipartUploads(conf);\n+\n+      pageSize \u003d intOption(getConf(), BULK_DELETE_PAGE_SIZE,\n+          BULK_DELETE_PAGE_SIZE_DEFAULT, 0);\n     } catch (AmazonClientException e) {\n       // amazon client exception: stop all services then throw the translation\n       stopAllServices();\n       throw translateException(\"initializing \", new Path(name), e);\n     } catch (IOException | RuntimeException e) {\n       // other exceptions: stop the services.\n       stopAllServices();\n       throw e;\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    try {\n      LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n      // clone the configuration into one with propagated bucket options\n      Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n      // patch the Hadoop security providers\n      patchSecurityCredentialProviders(conf);\n      // look for delegation token support early.\n      boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n      if (delegationTokensEnabled) {\n        LOG.debug(\"Using delegation tokens\");\n      }\n      // set the URI, this will do any fixup of the URI to remove secrets,\n      // canonicalize.\n      setUri(name, delegationTokensEnabled);\n      super.initialize(uri, conf);\n      setConf(conf);\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n          FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      initThreadPools(conf);\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      signerManager \u003d new SignerManager(bucket, this, conf, owner);\n      signerManager.initCustomSigners();\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      // This initiates a probe against S3 for the bucket existing.\n      // It is where all network and authentication configuration issues\n      // surface, and is potentially slow.\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n      LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      // instantiate S3 Select support\n      selectBinding \u003d new SelectBinding(writeHelper);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n          DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n\n      setMetadataStore(S3Guard.getMetadataStore(this, ttlTimeProvider));\n      allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n            getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n      }\n\n      // LOG if S3Guard is disabled on the warn level set in config\n      if (!hasMetadataStore()) {\n        String warnLevel \u003d conf.getTrimmed(S3GUARD_DISABLED_WARN_LEVEL,\n            DEFAULT_S3GUARD_DISABLED_WARN_LEVEL);\n        S3Guard.logS3GuardDisabled(LOG, warnLevel, bucket);\n      }\n\n      initMultipartUploads(conf);\n\n      pageSize \u003d intOption(getConf(), BULK_DELETE_PAGE_SIZE,\n          BULK_DELETE_PAGE_SIZE_DEFAULT, 0);\n    } catch (AmazonClientException e) {\n      // amazon client exception: stop all services then throw the translation\n      stopAllServices();\n      throw translateException(\"initializing \", new Path(name), e);\n    } catch (IOException | RuntimeException e) {\n      // other exceptions: stop the services.\n      stopAllServices();\n      throw e;\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "990063d2af0a37e9474949f33128805e34c3f016": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16665. Filesystems to be closed if they failed during initialize().\n\nContributed by Steve Loughran.\n\nThis FileSystem instantiation so if an IOException or RuntimeException is\nraised in the invocation of FileSystem.initialize() then a best-effort\nattempt is made to close the FS instance; exceptions raised that there\nare swallowed.\n\nThe S3AFileSystem is also modified to do its own cleanup if an\nIOException is raised during its initialize() process, it being the\nFS we know has the \"potential\" to leak threads, especially in\nextension points (e.g AWS Authenticators) which spawn threads.\n\nChange-Id: Ib84073a606c9d53bf53cbfca4629876a03894f04\n",
      "commitDate": "12/11/19 10:17 AM",
      "commitName": "990063d2af0a37e9474949f33128805e34c3f016",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "04/11/19 3:55 AM",
      "commitNameOld": "dca19fc3aa509949daf29bc902b2f74760407fc4",
      "commitAuthorOld": "Gabor Bota",
      "daysBetweenCommits": 8.27,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,138 +1,147 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n-    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n-    // clone the configuration into one with propagated bucket options\n-    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n-    // patch the Hadoop security providers\n-    patchSecurityCredentialProviders(conf);\n-    // look for delegation token support early.\n-    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n-    if (delegationTokensEnabled) {\n-      LOG.debug(\"Using delegation tokens\");\n-    }\n-    // set the URI, this will do any fixup of the URI to remove secrets,\n-    // canonicalize.\n-    setUri(name, delegationTokensEnabled);\n-    super.initialize(uri, conf);\n-    setConf(conf);\n     try {\n+      LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n+      // clone the configuration into one with propagated bucket options\n+      Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n+      // patch the Hadoop security providers\n+      patchSecurityCredentialProviders(conf);\n+      // look for delegation token support early.\n+      boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n+      if (delegationTokensEnabled) {\n+        LOG.debug(\"Using delegation tokens\");\n+      }\n+      // set the URI, this will do any fixup of the URI to remove secrets,\n+      // canonicalize.\n+      setUri(name, delegationTokensEnabled);\n+      super.initialize(uri, conf);\n+      setConf(conf);\n \n       // look for encryption data\n       // DT Bindings may override this\n       setEncryptionSecrets(new EncryptionSecrets(\n           getEncryptionAlgorithm(bucket, conf),\n           getServerSideEncryptionKey(bucket, getConf())));\n \n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n       owner \u003d UserGroupInformation.getCurrentUser();\n       username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n           FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       initThreadPools(conf);\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       signerManager \u003d new SignerManager(bucket, this, conf, owner);\n       signerManager.initCustomSigners();\n \n       // creates the AWS client, including overriding auth chain if\n       // the FS came with a DT\n       // this may do some patching of the configuration (e.g. setting\n       // the encryption algorithms)\n       bindAWSClient(name, delegationTokensEnabled);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n+      // This initiates a probe against S3 for the bucket existing.\n+      // It is where all network and authentication configuration issues\n+      // surface, and is potentially slow.\n       verifyBucketExists();\n \n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n       LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       // instantiate S3 Select support\n       selectBinding \u003d new SelectBinding(writeHelper);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n           DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n \n       setMetadataStore(S3Guard.getMetadataStore(this, ttlTimeProvider));\n       allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n \n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n             getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n       }\n \n       // LOG if S3Guard is disabled on the warn level set in config\n       if (!hasMetadataStore()) {\n         String warnLevel \u003d conf.getTrimmed(S3GUARD_DISABLED_WARN_LEVEL,\n             DEFAULT_S3GUARD_DISABLED_WARN_LEVEL);\n         S3Guard.logS3GuardDisabled(LOG, warnLevel, bucket);\n       }\n \n       initMultipartUploads(conf);\n     } catch (AmazonClientException e) {\n+      // amazon client exception: stop all services then throw the translation\n+      stopAllServices();\n       throw translateException(\"initializing \", new Path(name), e);\n+    } catch (IOException | RuntimeException e) {\n+      // other exceptions: stop the services.\n+      stopAllServices();\n+      throw e;\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    try {\n      LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n      // clone the configuration into one with propagated bucket options\n      Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n      // patch the Hadoop security providers\n      patchSecurityCredentialProviders(conf);\n      // look for delegation token support early.\n      boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n      if (delegationTokensEnabled) {\n        LOG.debug(\"Using delegation tokens\");\n      }\n      // set the URI, this will do any fixup of the URI to remove secrets,\n      // canonicalize.\n      setUri(name, delegationTokensEnabled);\n      super.initialize(uri, conf);\n      setConf(conf);\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n          FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      initThreadPools(conf);\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      signerManager \u003d new SignerManager(bucket, this, conf, owner);\n      signerManager.initCustomSigners();\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      // This initiates a probe against S3 for the bucket existing.\n      // It is where all network and authentication configuration issues\n      // surface, and is potentially slow.\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n      LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      // instantiate S3 Select support\n      selectBinding \u003d new SelectBinding(writeHelper);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n          DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n\n      setMetadataStore(S3Guard.getMetadataStore(this, ttlTimeProvider));\n      allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n            getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n      }\n\n      // LOG if S3Guard is disabled on the warn level set in config\n      if (!hasMetadataStore()) {\n        String warnLevel \u003d conf.getTrimmed(S3GUARD_DISABLED_WARN_LEVEL,\n            DEFAULT_S3GUARD_DISABLED_WARN_LEVEL);\n        S3Guard.logS3GuardDisabled(LOG, warnLevel, bucket);\n      }\n\n      initMultipartUploads(conf);\n    } catch (AmazonClientException e) {\n      // amazon client exception: stop all services then throw the translation\n      stopAllServices();\n      throw translateException(\"initializing \", new Path(name), e);\n    } catch (IOException | RuntimeException e) {\n      // other exceptions: stop the services.\n      stopAllServices();\n      throw e;\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "dca19fc3aa509949daf29bc902b2f74760407fc4": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16484. S3A to warn or fail if S3Guard is disabled (#1661). Contributed by Gabor Bota.\n\n\r\n",
      "commitDate": "04/11/19 3:55 AM",
      "commitName": "dca19fc3aa509949daf29bc902b2f74760407fc4",
      "commitAuthor": "Gabor Bota",
      "commitDateOld": "23/10/19 8:32 AM",
      "commitNameOld": "1d5d7d0989e9ee2f4527dc47ba5c80e1c38f641a",
      "commitAuthorOld": "Phil Zampino",
      "daysBetweenCommits": 11.85,
      "commitsBetweenForRepo": 34,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,130 +1,138 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     // patch the Hadoop security providers\n     patchSecurityCredentialProviders(conf);\n     // look for delegation token support early.\n     boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n     if (delegationTokensEnabled) {\n       LOG.debug(\"Using delegation tokens\");\n     }\n     // set the URI, this will do any fixup of the URI to remove secrets,\n     // canonicalize.\n     setUri(name, delegationTokensEnabled);\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n \n       // look for encryption data\n       // DT Bindings may override this\n       setEncryptionSecrets(new EncryptionSecrets(\n           getEncryptionAlgorithm(bucket, conf),\n           getServerSideEncryptionKey(bucket, getConf())));\n \n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n       owner \u003d UserGroupInformation.getCurrentUser();\n       username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n           FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       initThreadPools(conf);\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       signerManager \u003d new SignerManager(bucket, this, conf, owner);\n       signerManager.initCustomSigners();\n \n       // creates the AWS client, including overriding auth chain if\n       // the FS came with a DT\n       // this may do some patching of the configuration (e.g. setting\n       // the encryption algorithms)\n       bindAWSClient(name, delegationTokensEnabled);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n       LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       // instantiate S3 Select support\n       selectBinding \u003d new SelectBinding(writeHelper);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n           DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n \n       setMetadataStore(S3Guard.getMetadataStore(this, ttlTimeProvider));\n       allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n \n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n             getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n       }\n+\n+      // LOG if S3Guard is disabled on the warn level set in config\n+      if (!hasMetadataStore()) {\n+        String warnLevel \u003d conf.getTrimmed(S3GUARD_DISABLED_WARN_LEVEL,\n+            DEFAULT_S3GUARD_DISABLED_WARN_LEVEL);\n+        S3Guard.logS3GuardDisabled(LOG, warnLevel, bucket);\n+      }\n+\n       initMultipartUploads(conf);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    // patch the Hadoop security providers\n    patchSecurityCredentialProviders(conf);\n    // look for delegation token support early.\n    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n    if (delegationTokensEnabled) {\n      LOG.debug(\"Using delegation tokens\");\n    }\n    // set the URI, this will do any fixup of the URI to remove secrets,\n    // canonicalize.\n    setUri(name, delegationTokensEnabled);\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n          FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      initThreadPools(conf);\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      signerManager \u003d new SignerManager(bucket, this, conf, owner);\n      signerManager.initCustomSigners();\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n      LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      // instantiate S3 Select support\n      selectBinding \u003d new SelectBinding(writeHelper);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n          DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n\n      setMetadataStore(S3Guard.getMetadataStore(this, ttlTimeProvider));\n      allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n            getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n      }\n\n      // LOG if S3Guard is disabled on the warn level set in config\n      if (!hasMetadataStore()) {\n        String warnLevel \u003d conf.getTrimmed(S3GUARD_DISABLED_WARN_LEVEL,\n            DEFAULT_S3GUARD_DISABLED_WARN_LEVEL);\n        S3Guard.logS3GuardDisabled(LOG, warnLevel, bucket);\n      }\n\n      initMultipartUploads(conf);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "559ee277f50716a9a8c736ba3b655aad9f616e96": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16599. Allow a SignerInitializer to be specified along with a Custom Signer\n\n",
      "commitDate": "02/10/19 4:03 PM",
      "commitName": "559ee277f50716a9a8c736ba3b655aad9f616e96",
      "commitAuthor": "Siddharth Seth",
      "commitDateOld": "01/10/19 10:11 AM",
      "commitNameOld": "1921e94292f0820985a0cfbf8922a2a1a67fe921",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 1.24,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,130 +1,130 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     // patch the Hadoop security providers\n     patchSecurityCredentialProviders(conf);\n     // look for delegation token support early.\n     boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n     if (delegationTokensEnabled) {\n       LOG.debug(\"Using delegation tokens\");\n     }\n     // set the URI, this will do any fixup of the URI to remove secrets,\n     // canonicalize.\n     setUri(name, delegationTokensEnabled);\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n \n       // look for encryption data\n       // DT Bindings may override this\n       setEncryptionSecrets(new EncryptionSecrets(\n           getEncryptionAlgorithm(bucket, conf),\n           getServerSideEncryptionKey(bucket, getConf())));\n \n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n       owner \u003d UserGroupInformation.getCurrentUser();\n       username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n           FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       initThreadPools(conf);\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n-      signerManager \u003d new SignerManager();\n-      signerManager.initCustomSigners(conf);\n+      signerManager \u003d new SignerManager(bucket, this, conf, owner);\n+      signerManager.initCustomSigners();\n \n       // creates the AWS client, including overriding auth chain if\n       // the FS came with a DT\n       // this may do some patching of the configuration (e.g. setting\n       // the encryption algorithms)\n       bindAWSClient(name, delegationTokensEnabled);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n       LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       // instantiate S3 Select support\n       selectBinding \u003d new SelectBinding(writeHelper);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n           DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n \n       setMetadataStore(S3Guard.getMetadataStore(this, ttlTimeProvider));\n       allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n \n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n             getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n       }\n       initMultipartUploads(conf);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    // patch the Hadoop security providers\n    patchSecurityCredentialProviders(conf);\n    // look for delegation token support early.\n    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n    if (delegationTokensEnabled) {\n      LOG.debug(\"Using delegation tokens\");\n    }\n    // set the URI, this will do any fixup of the URI to remove secrets,\n    // canonicalize.\n    setUri(name, delegationTokensEnabled);\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n          FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      initThreadPools(conf);\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      signerManager \u003d new SignerManager(bucket, this, conf, owner);\n      signerManager.initCustomSigners();\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n      LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      // instantiate S3 Select support\n      selectBinding \u003d new SelectBinding(writeHelper);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n          DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n\n      setMetadataStore(S3Guard.getMetadataStore(this, ttlTimeProvider));\n      allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n            getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n      }\n      initMultipartUploads(conf);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "e02b1023c2f42b6792d2941c1f987ae06259b021": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16445. Allow separate custom signing algorithms for S3 and DDB (#1332)\n\n",
      "commitDate": "20/09/19 11:20 PM",
      "commitName": "e02b1023c2f42b6792d2941c1f987ae06259b021",
      "commitAuthor": "Siddharth Seth",
      "commitDateOld": "12/09/19 4:12 AM",
      "commitNameOld": "4e273a31f66013b7c20e8114451f5bc6c741f2cc",
      "commitAuthorOld": "Gabor Bota",
      "daysBetweenCommits": 8.8,
      "commitsBetweenForRepo": 88,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,127 +1,130 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     // patch the Hadoop security providers\n     patchSecurityCredentialProviders(conf);\n     // look for delegation token support early.\n     boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n     if (delegationTokensEnabled) {\n       LOG.debug(\"Using delegation tokens\");\n     }\n     // set the URI, this will do any fixup of the URI to remove secrets,\n     // canonicalize.\n     setUri(name, delegationTokensEnabled);\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n \n       // look for encryption data\n       // DT Bindings may override this\n       setEncryptionSecrets(new EncryptionSecrets(\n           getEncryptionAlgorithm(bucket, conf),\n           getServerSideEncryptionKey(bucket, getConf())));\n \n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n       owner \u003d UserGroupInformation.getCurrentUser();\n       username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n           FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       initThreadPools(conf);\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n+      signerManager \u003d new SignerManager();\n+      signerManager.initCustomSigners(conf);\n+\n       // creates the AWS client, including overriding auth chain if\n       // the FS came with a DT\n       // this may do some patching of the configuration (e.g. setting\n       // the encryption algorithms)\n       bindAWSClient(name, delegationTokensEnabled);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n       LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       // instantiate S3 Select support\n       selectBinding \u003d new SelectBinding(writeHelper);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n           DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n \n       setMetadataStore(S3Guard.getMetadataStore(this, ttlTimeProvider));\n       allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n \n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n             getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n       }\n       initMultipartUploads(conf);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    // patch the Hadoop security providers\n    patchSecurityCredentialProviders(conf);\n    // look for delegation token support early.\n    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n    if (delegationTokensEnabled) {\n      LOG.debug(\"Using delegation tokens\");\n    }\n    // set the URI, this will do any fixup of the URI to remove secrets,\n    // canonicalize.\n    setUri(name, delegationTokensEnabled);\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n          FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      initThreadPools(conf);\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      signerManager \u003d new SignerManager();\n      signerManager.initCustomSigners(conf);\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n      LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      // instantiate S3 Select support\n      selectBinding \u003d new SelectBinding(writeHelper);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n          DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n\n      setMetadataStore(S3Guard.getMetadataStore(this, ttlTimeProvider));\n      allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n            getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n      }\n      initMultipartUploads(conf);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "c58e11bf521d746842ce16724211a2a0339d7b61": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16383. Pass ITtlTimeProvider instance in initialize method in MetadataStore interface.  Contributed by Gabor Bota. (#1009) \n\n\r\n",
      "commitDate": "17/07/19 7:24 AM",
      "commitName": "c58e11bf521d746842ce16724211a2a0339d7b61",
      "commitAuthor": "Gabor Bota",
      "commitDateOld": "16/07/19 5:14 PM",
      "commitNameOld": "5672efa5c7184970c8f9e430ff8c36121f3a836d",
      "commitAuthorOld": "Sean Mackrory",
      "daysBetweenCommits": 0.59,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,127 +1,127 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     // patch the Hadoop security providers\n     patchSecurityCredentialProviders(conf);\n     // look for delegation token support early.\n     boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n     if (delegationTokensEnabled) {\n       LOG.debug(\"Using delegation tokens\");\n     }\n     // set the URI, this will do any fixup of the URI to remove secrets,\n     // canonicalize.\n     setUri(name, delegationTokensEnabled);\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n \n       // look for encryption data\n       // DT Bindings may override this\n       setEncryptionSecrets(new EncryptionSecrets(\n           getEncryptionAlgorithm(bucket, conf),\n           getServerSideEncryptionKey(bucket, getConf())));\n \n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n       owner \u003d UserGroupInformation.getCurrentUser();\n       username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n           FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       initThreadPools(conf);\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       // creates the AWS client, including overriding auth chain if\n       // the FS came with a DT\n       // this may do some patching of the configuration (e.g. setting\n       // the encryption algorithms)\n       bindAWSClient(name, delegationTokensEnabled);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n       LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       // instantiate S3 Select support\n       selectBinding \u003d new SelectBinding(writeHelper);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n           DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n \n-      setMetadataStore(S3Guard.getMetadataStore(this));\n+      setMetadataStore(S3Guard.getMetadataStore(this, ttlTimeProvider));\n       allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n \n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n             getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n       }\n       initMultipartUploads(conf);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    // patch the Hadoop security providers\n    patchSecurityCredentialProviders(conf);\n    // look for delegation token support early.\n    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n    if (delegationTokensEnabled) {\n      LOG.debug(\"Using delegation tokens\");\n    }\n    // set the URI, this will do any fixup of the URI to remove secrets,\n    // canonicalize.\n    setUri(name, delegationTokensEnabled);\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n          FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      initThreadPools(conf);\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n      LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      // instantiate S3 Select support\n      selectBinding \u003d new SelectBinding(writeHelper);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n          DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n\n      setMetadataStore(S3Guard.getMetadataStore(this, ttlTimeProvider));\n      allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n            getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n      }\n      initMultipartUploads(conf);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "34747c373f40a33d36a2e21ecb33fa791ccd939f": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16396. Allow authoritative mode on a subdirectory. (#1043)\n\n",
      "commitDate": "03/07/19 11:04 AM",
      "commitName": "34747c373f40a33d36a2e21ecb33fa791ccd939f",
      "commitAuthor": "Sean Mackrory",
      "commitDateOld": "20/06/19 1:56 AM",
      "commitNameOld": "e02eb24e0a9139418120027b694492e0738df20a",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 13.38,
      "commitsBetweenForRepo": 143,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,125 +1,127 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     // patch the Hadoop security providers\n     patchSecurityCredentialProviders(conf);\n     // look for delegation token support early.\n     boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n     if (delegationTokensEnabled) {\n       LOG.debug(\"Using delegation tokens\");\n     }\n     // set the URI, this will do any fixup of the URI to remove secrets,\n     // canonicalize.\n     setUri(name, delegationTokensEnabled);\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n \n       // look for encryption data\n       // DT Bindings may override this\n       setEncryptionSecrets(new EncryptionSecrets(\n           getEncryptionAlgorithm(bucket, conf),\n           getServerSideEncryptionKey(bucket, getConf())));\n \n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n       owner \u003d UserGroupInformation.getCurrentUser();\n       username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n           FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       initThreadPools(conf);\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       // creates the AWS client, including overriding auth chain if\n       // the FS came with a DT\n       // this may do some patching of the configuration (e.g. setting\n       // the encryption algorithms)\n       bindAWSClient(name, delegationTokensEnabled);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n       LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       // instantiate S3 Select support\n       selectBinding \u003d new SelectBinding(writeHelper);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n           DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n-      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n+      allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n+      allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n+\n       if (hasMetadataStore()) {\n-        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n-            getMetadataStore(), allowAuthoritative);\n+        LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n+            getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n       }\n       initMultipartUploads(conf);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    // patch the Hadoop security providers\n    patchSecurityCredentialProviders(conf);\n    // look for delegation token support early.\n    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n    if (delegationTokensEnabled) {\n      LOG.debug(\"Using delegation tokens\");\n    }\n    // set the URI, this will do any fixup of the URI to remove secrets,\n    // canonicalize.\n    setUri(name, delegationTokensEnabled);\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n          FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      initThreadPools(conf);\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n      LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      // instantiate S3 Select support\n      selectBinding \u003d new SelectBinding(writeHelper);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n          DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritativeMetadataStore \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      allowAuthoritativePaths \u003d S3Guard.getAuthoritativePaths(this);\n\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative store\u003d{}, authoritative path\u003d{}\",\n            getMetadataStore(), allowAuthoritativeMetadataStore, allowAuthoritativePaths);\n      }\n      initMultipartUploads(conf);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "e02eb24e0a9139418120027b694492e0738df20a": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15183. S3Guard store becomes inconsistent after partial failure of rename.\n\nContributed by Steve Loughran.\n\nChange-Id: I825b0bc36be960475d2d259b1cdab45ae1bb78eb\n",
      "commitDate": "20/06/19 1:56 AM",
      "commitName": "e02eb24e0a9139418120027b694492e0738df20a",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "16/06/19 9:05 AM",
      "commitNameOld": "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
      "commitAuthorOld": "Gabor Bota",
      "daysBetweenCommits": 3.7,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,127 +1,125 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     // patch the Hadoop security providers\n     patchSecurityCredentialProviders(conf);\n     // look for delegation token support early.\n     boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n     if (delegationTokensEnabled) {\n       LOG.debug(\"Using delegation tokens\");\n     }\n     // set the URI, this will do any fixup of the URI to remove secrets,\n     // canonicalize.\n     setUri(name, delegationTokensEnabled);\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n \n       // look for encryption data\n       // DT Bindings may override this\n       setEncryptionSecrets(new EncryptionSecrets(\n           getEncryptionAlgorithm(bucket, conf),\n           getServerSideEncryptionKey(bucket, getConf())));\n \n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n       owner \u003d UserGroupInformation.getCurrentUser();\n       username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n           FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       initThreadPools(conf);\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       // creates the AWS client, including overriding auth chain if\n       // the FS came with a DT\n       // this may do some patching of the configuration (e.g. setting\n       // the encryption algorithms)\n       bindAWSClient(name, delegationTokensEnabled);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n       LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       // instantiate S3 Select support\n       selectBinding \u003d new SelectBinding(writeHelper);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n+      long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n+          DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n+      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n-      if (hasMetadataStore()) {\n-        long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n-            DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n-        ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n-      }\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    // patch the Hadoop security providers\n    patchSecurityCredentialProviders(conf);\n    // look for delegation token support early.\n    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n    if (delegationTokensEnabled) {\n      LOG.debug(\"Using delegation tokens\");\n    }\n    // set the URI, this will do any fixup of the URI to remove secrets,\n    // canonicalize.\n    setUri(name, delegationTokensEnabled);\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n          FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      initThreadPools(conf);\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n      LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      // instantiate S3 Select support\n      selectBinding \u003d new SelectBinding(writeHelper);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n          DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "f9cc9e162175444efe9d5b07ecb9a795f750ca3c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16279. S3Guard: Implement time-based (TTL) expiry for entries (and tombstones).\n\nContributed by Gabor Bota.\n\nChange-Id: I73a2d2861901dedfe7a0e783b310fbb95e7c1af9\n",
      "commitDate": "16/06/19 9:05 AM",
      "commitName": "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
      "commitAuthor": "Gabor Bota",
      "commitDateOld": "19/05/19 2:29 PM",
      "commitNameOld": "a36274d69947648dbe82721220cc5240ec5d396d",
      "commitAuthorOld": "Ben Roling",
      "daysBetweenCommits": 27.77,
      "commitsBetweenForRepo": 198,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,125 +1,127 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     // patch the Hadoop security providers\n     patchSecurityCredentialProviders(conf);\n     // look for delegation token support early.\n     boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n     if (delegationTokensEnabled) {\n       LOG.debug(\"Using delegation tokens\");\n     }\n     // set the URI, this will do any fixup of the URI to remove secrets,\n     // canonicalize.\n     setUri(name, delegationTokensEnabled);\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n \n       // look for encryption data\n       // DT Bindings may override this\n       setEncryptionSecrets(new EncryptionSecrets(\n           getEncryptionAlgorithm(bucket, conf),\n           getServerSideEncryptionKey(bucket, getConf())));\n \n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n       owner \u003d UserGroupInformation.getCurrentUser();\n       username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n           FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       initThreadPools(conf);\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       // creates the AWS client, including overriding auth chain if\n       // the FS came with a DT\n       // this may do some patching of the configuration (e.g. setting\n       // the encryption algorithms)\n       bindAWSClient(name, delegationTokensEnabled);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n       LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       // instantiate S3 Select support\n       selectBinding \u003d new SelectBinding(writeHelper);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n-      long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n-          DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n-      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n+      if (hasMetadataStore()) {\n+        long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n+            DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n+        ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n+      }\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    // patch the Hadoop security providers\n    patchSecurityCredentialProviders(conf);\n    // look for delegation token support early.\n    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n    if (delegationTokensEnabled) {\n      LOG.debug(\"Using delegation tokens\");\n    }\n    // set the URI, this will do any fixup of the URI to remove secrets,\n    // canonicalize.\n    setUri(name, delegationTokensEnabled);\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n          FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      initThreadPools(conf);\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n      LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      // instantiate S3 Select support\n      selectBinding \u003d new SelectBinding(writeHelper);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n      if (hasMetadataStore()) {\n        long authDirTtl \u003d conf.getTimeDuration(METADATASTORE_METADATA_TTL,\n            DEFAULT_METADATASTORE_METADATA_TTL, TimeUnit.MILLISECONDS);\n        ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n      }\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "a36274d69947648dbe82721220cc5240ec5d396d": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16085. S3Guard: use object version or etags to protect against inconsistent read after replace/overwrite.\n\nContributed by Ben Roling.\n\nS3Guard will now track the etag of uploaded files and, if an S3\nbucket is versioned, the object version.\n\nYou can then control how to react to a mismatch between the data\nin the DynamoDB table and that in the store: warn, fail, or, when\nusing versions, return the original value.\n\nThis adds two new columns to the table: etag and version.\nThis is transparent to older S3A clients -but when such clients\nadd/update data to the S3Guard table, they will not add these values.\nAs a result, the etag/version checks will not work with files uploaded by older clients.\n\nFor a consistent experience, upgrade all clients to use the latest hadoop version.\n",
      "commitDate": "19/05/19 2:29 PM",
      "commitName": "a36274d69947648dbe82721220cc5240ec5d396d",
      "commitAuthor": "Ben Roling",
      "commitDateOld": "30/04/19 3:53 AM",
      "commitNameOld": "0af4011580878566213016af0c32633eabd15100",
      "commitAuthorOld": "Ben Roling",
      "daysBetweenCommits": 19.44,
      "commitsBetweenForRepo": 90,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,144 +1,125 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     // patch the Hadoop security providers\n     patchSecurityCredentialProviders(conf);\n     // look for delegation token support early.\n     boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n     if (delegationTokensEnabled) {\n       LOG.debug(\"Using delegation tokens\");\n     }\n     // set the URI, this will do any fixup of the URI to remove secrets,\n     // canonicalize.\n     setUri(name, delegationTokensEnabled);\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n \n       // look for encryption data\n       // DT Bindings may override this\n       setEncryptionSecrets(new EncryptionSecrets(\n           getEncryptionAlgorithm(bucket, conf),\n           getServerSideEncryptionKey(bucket, getConf())));\n \n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n       owner \u003d UserGroupInformation.getCurrentUser();\n       username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n           FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n-      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n-      if (maxThreads \u003c 2) {\n-        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n-        maxThreads \u003d 2;\n-      }\n-      int totalTasks \u003d intOption(conf,\n-          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n-      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n-          DEFAULT_KEEPALIVE_TIME, 0);\n-      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n-          maxThreads,\n-          maxThreads + totalTasks,\n-          keepAliveTime, TimeUnit.SECONDS,\n-          \"s3a-transfer-shared\");\n-      unboundedThreadPool \u003d new ThreadPoolExecutor(\n-          maxThreads, Integer.MAX_VALUE,\n-          keepAliveTime, TimeUnit.SECONDS,\n-          new LinkedBlockingQueue\u003cRunnable\u003e(),\n-          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n-              \"s3a-transfer-unbounded\"));\n+      initThreadPools(conf);\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       // creates the AWS client, including overriding auth chain if\n       // the FS came with a DT\n       // this may do some patching of the configuration (e.g. setting\n       // the encryption algorithms)\n       bindAWSClient(name, delegationTokensEnabled);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n       LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       // instantiate S3 Select support\n       selectBinding \u003d new SelectBinding(writeHelper);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n       long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n           DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    // patch the Hadoop security providers\n    patchSecurityCredentialProviders(conf);\n    // look for delegation token support early.\n    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n    if (delegationTokensEnabled) {\n      LOG.debug(\"Using delegation tokens\");\n    }\n    // set the URI, this will do any fixup of the URI to remove secrets,\n    // canonicalize.\n    setUri(name, delegationTokensEnabled);\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n          FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      initThreadPools(conf);\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n      LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      // instantiate S3 Select support\n      selectBinding \u003d new SelectBinding(writeHelper);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n      long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n          DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "0af4011580878566213016af0c32633eabd15100": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16221. S3Guard: add option to fail operation on metadata write failure.\n",
      "commitDate": "30/04/19 3:53 AM",
      "commitName": "0af4011580878566213016af0c32633eabd15100",
      "commitAuthor": "Ben Roling",
      "commitDateOld": "28/03/19 8:59 AM",
      "commitNameOld": "b5db2383832881034d57d836a8135a07a2bd1cf4",
      "commitAuthorOld": "Gabor Bota",
      "daysBetweenCommits": 32.79,
      "commitsBetweenForRepo": 211,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,141 +1,144 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     // patch the Hadoop security providers\n     patchSecurityCredentialProviders(conf);\n     // look for delegation token support early.\n     boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n     if (delegationTokensEnabled) {\n       LOG.debug(\"Using delegation tokens\");\n     }\n     // set the URI, this will do any fixup of the URI to remove secrets,\n     // canonicalize.\n     setUri(name, delegationTokensEnabled);\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n \n       // look for encryption data\n       // DT Bindings may override this\n       setEncryptionSecrets(new EncryptionSecrets(\n           getEncryptionAlgorithm(bucket, conf),\n           getServerSideEncryptionKey(bucket, getConf())));\n \n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n       owner \u003d UserGroupInformation.getCurrentUser();\n       username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n+      failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n+          FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n+\n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       // creates the AWS client, including overriding auth chain if\n       // the FS came with a DT\n       // this may do some patching of the configuration (e.g. setting\n       // the encryption algorithms)\n       bindAWSClient(name, delegationTokensEnabled);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n       LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       // instantiate S3 Select support\n       selectBinding \u003d new SelectBinding(writeHelper);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n       long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n           DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    // patch the Hadoop security providers\n    patchSecurityCredentialProviders(conf);\n    // look for delegation token support early.\n    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n    if (delegationTokensEnabled) {\n      LOG.debug(\"Using delegation tokens\");\n    }\n    // set the URI, this will do any fixup of the URI to remove secrets,\n    // canonicalize.\n    setUri(name, delegationTokensEnabled);\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      failOnMetadataWriteError \u003d conf.getBoolean(FAIL_ON_METADATA_WRITE_ERROR,\n          FAIL_ON_METADATA_WRITE_ERROR_DEFAULT);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n      LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      // instantiate S3 Select support\n      selectBinding \u003d new SelectBinding(writeHelper);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n      long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n          DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "6fa229891e06eea62cb9634efde755f40247e816": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15625. S3A input stream to use etags/version number to detect changed source files.\n\nAuthor: Ben Roling \u003cben.roling@gmail.com\u003e\n\nInitial patch from Brahma Reddy Battula.\n",
      "commitDate": "13/03/19 1:37 PM",
      "commitName": "6fa229891e06eea62cb9634efde755f40247e816",
      "commitAuthor": "Ben Roling",
      "commitDateOld": "05/02/19 3:51 AM",
      "commitNameOld": "f365957c6326f88734bc0a5d01cfb7eac713db20",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 36.37,
      "commitsBetweenForRepo": 327,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,139 +1,141 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     // patch the Hadoop security providers\n     patchSecurityCredentialProviders(conf);\n     // look for delegation token support early.\n     boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n     if (delegationTokensEnabled) {\n       LOG.debug(\"Using delegation tokens\");\n     }\n     // set the URI, this will do any fixup of the URI to remove secrets,\n     // canonicalize.\n     setUri(name, delegationTokensEnabled);\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n \n       // look for encryption data\n       // DT Bindings may override this\n       setEncryptionSecrets(new EncryptionSecrets(\n           getEncryptionAlgorithm(bucket, conf),\n           getServerSideEncryptionKey(bucket, getConf())));\n \n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n       owner \u003d UserGroupInformation.getCurrentUser();\n       username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       // creates the AWS client, including overriding auth chain if\n       // the FS came with a DT\n       // this may do some patching of the configuration (e.g. setting\n       // the encryption algorithms)\n       bindAWSClient(name, delegationTokensEnabled);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n+      changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n+      LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       // instantiate S3 Select support\n       selectBinding \u003d new SelectBinding(writeHelper);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n       long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n           DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    // patch the Hadoop security providers\n    patchSecurityCredentialProviders(conf);\n    // look for delegation token support early.\n    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n    if (delegationTokensEnabled) {\n      LOG.debug(\"Using delegation tokens\");\n    }\n    // set the URI, this will do any fixup of the URI to remove secrets,\n    // canonicalize.\n    setUri(name, delegationTokensEnabled);\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      changeDetectionPolicy \u003d ChangeDetectionPolicy.getPolicy(conf);\n      LOG.debug(\"Change detection policy \u003d {}\", changeDetectionPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      // instantiate S3 Select support\n      selectBinding \u003d new SelectBinding(writeHelper);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n      long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n          DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "f365957c6326f88734bc0a5d01cfb7eac713db20": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15229. Add FileSystem builder-based openFile() API to match createFile();\nS3A to implement S3 Select through this API.\n\nThe new openFile() API is asynchronous, and implemented across FileSystem and FileContext.\n\nThe MapReduce V2 inputs are moved to this API, and you can actually set must/may\noptions to pass in.\n\nThis is more useful for setting things like s3a seek policy than for S3 select,\nas the existing input format/record readers can\u0027t handle S3 select output where\nthe stream is shorter than the file length, and splitting plain text is suboptimal.\nFuture work is needed there.\n\nIn the meantime, any/all filesystem connectors are now free to add their own filesystem-specific\nconfiguration parameters which can be set in jobs and used to set filesystem input stream\noptions (seek policy, retry, encryption secrets, etc).\n\nContributed by Steve Loughran\n",
      "commitDate": "05/02/19 3:51 AM",
      "commitName": "f365957c6326f88734bc0a5d01cfb7eac713db20",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "14/01/19 9:59 AM",
      "commitNameOld": "6d0bffe17eadedd60d4599427248b0db4a7c5502",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 21.74,
      "commitsBetweenForRepo": 167,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,136 +1,139 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     // patch the Hadoop security providers\n     patchSecurityCredentialProviders(conf);\n     // look for delegation token support early.\n     boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n     if (delegationTokensEnabled) {\n       LOG.debug(\"Using delegation tokens\");\n     }\n     // set the URI, this will do any fixup of the URI to remove secrets,\n     // canonicalize.\n     setUri(name, delegationTokensEnabled);\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n \n       // look for encryption data\n       // DT Bindings may override this\n       setEncryptionSecrets(new EncryptionSecrets(\n           getEncryptionAlgorithm(bucket, conf),\n           getServerSideEncryptionKey(bucket, getConf())));\n \n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n       owner \u003d UserGroupInformation.getCurrentUser();\n       username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       // creates the AWS client, including overriding auth chain if\n       // the FS came with a DT\n       // this may do some patching of the configuration (e.g. setting\n       // the encryption algorithms)\n       bindAWSClient(name, delegationTokensEnabled);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n+      // instantiate S3 Select support\n+      selectBinding \u003d new SelectBinding(writeHelper);\n+\n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n       long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n           DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    // patch the Hadoop security providers\n    patchSecurityCredentialProviders(conf);\n    // look for delegation token support early.\n    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n    if (delegationTokensEnabled) {\n      LOG.debug(\"Using delegation tokens\");\n    }\n    // set the URI, this will do any fixup of the URI to remove secrets,\n    // canonicalize.\n    setUri(name, delegationTokensEnabled);\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      // instantiate S3 Select support\n      selectBinding \u003d new SelectBinding(writeHelper);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n      long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n          DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "6d0bffe17eadedd60d4599427248b0db4a7c5502": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14556. S3A to support Delegation Tokens.\n\nContributed by Steve Loughran and Daryn Sharp.\n",
      "commitDate": "14/01/19 9:59 AM",
      "commitName": "6d0bffe17eadedd60d4599427248b0db4a7c5502",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "07/01/19 9:51 PM",
      "commitNameOld": "7f783970364930cc461d1a73833bc58cdd10553e",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 6.51,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,123 +1,136 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n-    setUri(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n+    // patch the Hadoop security providers\n     patchSecurityCredentialProviders(conf);\n-    super.initialize(name, conf);\n+    // look for delegation token support early.\n+    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n+    if (delegationTokensEnabled) {\n+      LOG.debug(\"Using delegation tokens\");\n+    }\n+    // set the URI, this will do any fixup of the URI to remove secrets,\n+    // canonicalize.\n+    setUri(name, delegationTokensEnabled);\n+    super.initialize(uri, conf);\n     setConf(conf);\n     try {\n-      instrumentation \u003d new S3AInstrumentation(name);\n+\n+      // look for encryption data\n+      // DT Bindings may override this\n+      setEncryptionSecrets(new EncryptionSecrets(\n+          getEncryptionAlgorithm(bucket, conf),\n+          getServerSideEncryptionKey(bucket, getConf())));\n+\n+      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n+      instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n-      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n+      owner \u003d UserGroupInformation.getCurrentUser();\n+      username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n-\n-      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n-          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n-          S3ClientFactory.class);\n-\n-      credentials \u003d createAWSCredentialProviderSet(name, conf);\n-      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n-          .createS3Client(name, bucket, credentials);\n-      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n+      // creates the AWS client, including overriding auth chain if\n+      // the FS came with a DT\n+      // this may do some patching of the configuration (e.g. setting\n+      // the encryption algorithms)\n+      bindAWSClient(name, delegationTokensEnabled);\n+\n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n-      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(bucket, conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n       long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n           DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    // patch the Hadoop security providers\n    patchSecurityCredentialProviders(conf);\n    // look for delegation token support early.\n    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n    if (delegationTokensEnabled) {\n      LOG.debug(\"Using delegation tokens\");\n    }\n    // set the URI, this will do any fixup of the URI to remove secrets,\n    // canonicalize.\n    setUri(name, delegationTokensEnabled);\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n      long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n          DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "7f783970364930cc461d1a73833bc58cdd10553e": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HADOOP-14556. S3A to support Delegation Tokens.\"\n\nThis reverts commit d7152332b32a575c3a92e3f4c44b95e58462528d.\n",
      "commitDate": "07/01/19 9:51 PM",
      "commitName": "7f783970364930cc461d1a73833bc58cdd10553e",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "07/01/19 5:18 AM",
      "commitNameOld": "d7152332b32a575c3a92e3f4c44b95e58462528d",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 0.69,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,136 +1,123 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n+    setUri(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n-    // patch the Hadoop security providers\n     patchSecurityCredentialProviders(conf);\n-    // look for delegation token support early.\n-    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n-    if (delegationTokensEnabled) {\n-      LOG.debug(\"Using delegation tokens\");\n-    }\n-    // set the URI, this will do any fixup of the URI to remove secrets,\n-    // canonicalize.\n-    setUri(name, delegationTokensEnabled);\n-    super.initialize(uri, conf);\n+    super.initialize(name, conf);\n     setConf(conf);\n     try {\n-\n-      // look for encryption data\n-      // DT Bindings may override this\n-      setEncryptionSecrets(new EncryptionSecrets(\n-          getEncryptionAlgorithm(bucket, conf),\n-          getServerSideEncryptionKey(bucket, getConf())));\n-\n-      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n-      instrumentation \u003d new S3AInstrumentation(uri);\n+      instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n-      owner \u003d UserGroupInformation.getCurrentUser();\n-      username \u003d owner.getShortUserName();\n+      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n+\n+      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n+          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n+          S3ClientFactory.class);\n+\n+      credentials \u003d createAWSCredentialProviderSet(name, conf);\n+      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n+          .createS3Client(name, bucket, credentials);\n+      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n-      // creates the AWS client, including overriding auth chain if\n-      // the FS came with a DT\n-      // this may do some patching of the configuration (e.g. setting\n-      // the encryption algorithms)\n-      bindAWSClient(name, delegationTokensEnabled);\n-\n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n+      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(bucket, conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n       long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n           DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    setUri(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n\n      credentials \u003d createAWSCredentialProviderSet(name, conf);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name, bucket, credentials);\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(bucket, conf);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n      long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n          DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "d7152332b32a575c3a92e3f4c44b95e58462528d": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14556. S3A to support Delegation Tokens.\n\nContributed by Steve Loughran.\n",
      "commitDate": "07/01/19 5:18 AM",
      "commitName": "d7152332b32a575c3a92e3f4c44b95e58462528d",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "28/11/18 9:45 AM",
      "commitNameOld": "5d96b74f33ca716c9fe4fadb046f79ed488a3059",
      "commitAuthorOld": "Sean Mackrory",
      "daysBetweenCommits": 39.81,
      "commitsBetweenForRepo": 242,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,123 +1,136 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n-    setUri(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n+    // patch the Hadoop security providers\n     patchSecurityCredentialProviders(conf);\n-    super.initialize(name, conf);\n+    // look for delegation token support early.\n+    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n+    if (delegationTokensEnabled) {\n+      LOG.debug(\"Using delegation tokens\");\n+    }\n+    // set the URI, this will do any fixup of the URI to remove secrets,\n+    // canonicalize.\n+    setUri(name, delegationTokensEnabled);\n+    super.initialize(uri, conf);\n     setConf(conf);\n     try {\n-      instrumentation \u003d new S3AInstrumentation(name);\n+\n+      // look for encryption data\n+      // DT Bindings may override this\n+      setEncryptionSecrets(new EncryptionSecrets(\n+          getEncryptionAlgorithm(bucket, conf),\n+          getServerSideEncryptionKey(bucket, getConf())));\n+\n+      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n+      instrumentation \u003d new S3AInstrumentation(uri);\n \n       // Username is the current user at the time the FS was instantiated.\n-      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n+      owner \u003d UserGroupInformation.getCurrentUser();\n+      username \u003d owner.getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n-\n-      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n-          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n-          S3ClientFactory.class);\n-\n-      credentials \u003d createAWSCredentialProviderSet(name, conf);\n-      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n-          .createS3Client(name, bucket, credentials);\n-      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n+      // creates the AWS client, including overriding auth chain if\n+      // the FS came with a DT\n+      // this may do some patching of the configuration (e.g. setting\n+      // the encryption algorithms)\n+      bindAWSClient(name, delegationTokensEnabled);\n+\n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n-      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(bucket, conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n       long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n           DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n       ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    // patch the Hadoop security providers\n    patchSecurityCredentialProviders(conf);\n    // look for delegation token support early.\n    boolean delegationTokensEnabled \u003d hasDelegationTokenBinding(conf);\n    if (delegationTokensEnabled) {\n      LOG.debug(\"Using delegation tokens\");\n    }\n    // set the URI, this will do any fixup of the URI to remove secrets,\n    // canonicalize.\n    setUri(name, delegationTokensEnabled);\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n\n      // look for encryption data\n      // DT Bindings may override this\n      setEncryptionSecrets(new EncryptionSecrets(\n          getEncryptionAlgorithm(bucket, conf),\n          getServerSideEncryptionKey(bucket, getConf())));\n\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      instrumentation \u003d new S3AInstrumentation(uri);\n\n      // Username is the current user at the time the FS was instantiated.\n      owner \u003d UserGroupInformation.getCurrentUser();\n      username \u003d owner.getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      // creates the AWS client, including overriding auth chain if\n      // the FS came with a DT\n      // this may do some patching of the configuration (e.g. setting\n      // the encryption algorithms)\n      bindAWSClient(name, delegationTokensEnabled);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n      long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n          DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "046b8768af8a07a9e10ce43f538d6ac16e7fa5f3": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15621 S3Guard: Implement time-based (TTL) expiry for Authoritative Directory Listing. Contributed by Gabor Bota\n",
      "commitDate": "02/10/18 9:22 PM",
      "commitName": "046b8768af8a07a9e10ce43f538d6ac16e7fa5f3",
      "commitAuthor": "Aaron Fabbri",
      "commitDateOld": "12/09/18 9:04 PM",
      "commitNameOld": "d7c0a08a1c077752918a8cf1b4f1900ce2721899",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 20.01,
      "commitsBetweenForRepo": 199,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,120 +1,123 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     setUri(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n \n       credentials \u003d createAWSCredentialProviderSet(name, conf);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name, bucket, credentials);\n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(bucket, conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n+      long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n+          DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n+      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    setUri(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n\n      credentials \u003d createAWSCredentialProviderSet(name, conf);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name, bucket, credentials);\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(bucket, conf);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n      long authDirTtl \u003d conf.getLong(METADATASTORE_AUTHORITATIVE_DIR_TTL,\n          DEFAULT_METADATASTORE_AUTHORITATIVE_DIR_TTL);\n      ttlTimeProvider \u003d new S3Guard.TtlTimeProvider(authDirTtl);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "da9a39eed138210de29b59b90c449b28da1c04f9": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15583. Stabilize S3A Assumed Role support.\nContributed by Steve Loughran.\n",
      "commitDate": "08/08/18 10:57 PM",
      "commitName": "da9a39eed138210de29b59b90c449b28da1c04f9",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "27/06/18 10:37 PM",
      "commitNameOld": "2b2399d623539ab68e71a38fa9fbfc9a405bddb8",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 42.01,
      "commitsBetweenForRepo": 267,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,118 +1,120 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     setUri(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n+\n+      credentials \u003d createAWSCredentialProviderSet(name, conf);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n-          .createS3Client(name);\n+          .createS3Client(name, bucket, credentials);\n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n           onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(bucket, conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    setUri(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n\n      credentials \u003d createAWSCredentialProviderSet(name, conf);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name, bucket, credentials);\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(bucket, conf);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "8110d6a0d59e7dc2ddb25fa424fab188c5e9ce35": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13761. S3Guard: implement retries for DDB failures and throttling; translate exceptions.\nContributed by Aaron Fabbri.\n",
      "commitDate": "05/03/18 6:06 AM",
      "commitName": "8110d6a0d59e7dc2ddb25fa424fab188c5e9ce35",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "18/02/18 5:19 AM",
      "commitNameOld": "4d4dde5112e9ee6b37cbdea17104c5a4c6870bd5",
      "commitAuthorOld": "fang zhenyi",
      "daysBetweenCommits": 15.03,
      "commitsBetweenForRepo": 93,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,116 +1,118 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     setUri(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name);\n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n+      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n+          onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(bucket, conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    setUri(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name);\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      s3guardInvoker \u003d new Invoker(new S3GuardExistsRetryPolicy(getConf()),\n          onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(bucket, conf);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "7ac88244c54ce483729af3d2736d9f4731e230ca": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14507. Extend per-bucket secret key config with explicit getPassword() on fs.s3a.$bucket.secret.key.\nContributed by Steve Loughran.\n",
      "commitDate": "16/02/18 8:37 AM",
      "commitName": "7ac88244c54ce483729af3d2736d9f4731e230ca",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "15/02/18 7:57 AM",
      "commitNameOld": "9a013b255f301c557c3868dc1ad657202e9e7a67",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 1.03,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,116 +1,116 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     setUri(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name);\n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n       writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n-      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n+      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(bucket, conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    setUri(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name);\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(bucket, conf);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "1093a73689912f78547e6d23023be2fd1c7ddc85": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13974. S3Guard CLI to support list/purge of pending multipart commits.\nContributed by Aaron Fabbri\n",
      "commitDate": "18/01/18 5:13 AM",
      "commitName": "1093a73689912f78547e6d23023be2fd1c7ddc85",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "18/01/18 4:35 AM",
      "commitNameOld": "f274fe33ea359d26a31efec42a856320a0dbb5f4",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 0.03,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,115 +1,116 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     setUri(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name);\n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n+      writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    setUri(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name);\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "f274fe33ea359d26a31efec42a856320a0dbb5f4": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HADOOP-13974. S3Guard CLI to support list/purge of pending multipart commits.\"\n\nThis reverts commit 35ad9b1dd279b769381ea1625d9bf776c309c5cb.\n",
      "commitDate": "18/01/18 4:35 AM",
      "commitName": "f274fe33ea359d26a31efec42a856320a0dbb5f4",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "17/01/18 12:05 AM",
      "commitNameOld": "268ab4e0279b3e40f4a627d3dfe91e2a3523a8cc",
      "commitAuthorOld": "Aaron Fabbri",
      "daysBetweenCommits": 1.19,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,116 +1,115 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     setUri(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name);\n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n-      writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    setUri(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name);\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "35ad9b1dd279b769381ea1625d9bf776c309c5cb": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13974. S3Guard CLI to support list/purge of pending multipart commits.\nContributed by Aaron Fabbri\n",
      "commitDate": "18/12/17 1:19 PM",
      "commitName": "35ad9b1dd279b769381ea1625d9bf776c309c5cb",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "13/12/17 7:14 PM",
      "commitNameOld": "f86c81d923ecce9d1c9fb691bbc78e93b4a65ae7",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 4.75,
      "commitsBetweenForRepo": 64,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,115 +1,116 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     setUri(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name);\n       invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n+      writeHelper \u003d new WriteOperationHelper(this, getConf());\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n       LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n       boolean magicCommitterEnabled \u003d conf.getBoolean(\n           CommitConstants.MAGIC_COMMITTER_ENABLED,\n           CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n       LOG.debug(\"Filesystem support for magic committers {} enabled\",\n           magicCommitterEnabled ? \"is\" : \"is not\");\n       committerIntegration \u003d new MagicCommitIntegration(\n           this, magicCommitterEnabled);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n       initMultipartUploads(conf);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    setUri(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name);\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n      writeHelper \u003d new WriteOperationHelper(this, getConf());\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13786 Add S3A committer for zero-rename commits to S3 endpoints.\nContributed by Steve Loughran and Ryan Blue.\n",
      "commitDate": "22/11/17 7:28 AM",
      "commitName": "de8b6ca5ef8614de6d6277b7617e27c788b0555c",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "25/09/17 3:59 PM",
      "commitNameOld": "47011d7dd300b0c74bb6cfe25b918c479d718f4f",
      "commitAuthorOld": "Aaron Fabbri",
      "daysBetweenCommits": 57.69,
      "commitsBetweenForRepo": 477,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,115 +1,115 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n-    uri \u003d S3xLoginHelper.buildFSURI(name);\n+    setUri(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n+    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name);\n+      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n-      storageStatistics \u003d (S3AStorageStatistics)\n-          GlobalStorageStatistics.INSTANCE\n-              .put(S3AStorageStatistics.NAME,\n-                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n-                    @Override\n-                    public StorageStatistics provide() {\n-                      return new S3AStorageStatistics();\n-                    }\n-                  });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n-      initMultipartUploads(conf);\n-\n       serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n+      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n+      boolean magicCommitterEnabled \u003d conf.getBoolean(\n+          CommitConstants.MAGIC_COMMITTER_ENABLED,\n+          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n+      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n+          magicCommitterEnabled ? \"is\" : \"is not\");\n+      committerIntegration \u003d new MagicCommitIntegration(\n+          this, magicCommitterEnabled);\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n-      metadataStore \u003d S3Guard.getMetadataStore(this);\n+      setMetadataStore(S3Guard.getMetadataStore(this));\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n       if (hasMetadataStore()) {\n         LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n             getMetadataStore(), allowAuthoritative);\n       }\n+      initMultipartUploads(conf);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    setUri(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    LOG.debug(\"Initializing S3AFileSystem for {}\", bucket);\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name);\n      invoker \u003d new Invoker(new S3ARetryPolicy(getConf()), onRetry);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n      LOG.debug(\"Input fadvise policy \u003d {}\", inputPolicy);\n      boolean magicCommitterEnabled \u003d conf.getBoolean(\n          CommitConstants.MAGIC_COMMITTER_ENABLED,\n          CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);\n      LOG.debug(\"Filesystem support for magic committers {} enabled\",\n          magicCommitterEnabled ? \"is\" : \"is not\");\n      committerIntegration \u003d new MagicCommitIntegration(\n          this, magicCommitterEnabled);\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      setMetadataStore(S3Guard.getMetadataStore(this));\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n      initMultipartUploads(conf);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "47011d7dd300b0c74bb6cfe25b918c479d718f4f": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14220 Enhance S3GuardTool with bucket-info and set-capacity commands, tests. Contributed by Steve Loughran\n",
      "commitDate": "25/09/17 3:59 PM",
      "commitName": "47011d7dd300b0c74bb6cfe25b918c479d718f4f",
      "commitAuthor": "Aaron Fabbri",
      "commitDateOld": "14/09/17 2:10 PM",
      "commitNameOld": "49467165a57fb77932d1d526796624b88ebacd91",
      "commitAuthorOld": "Aaron Fabbri",
      "daysBetweenCommits": 11.08,
      "commitsBetweenForRepo": 91,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,111 +1,115 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     uri \u003d S3xLoginHelper.buildFSURI(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n \n       boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n       if (!blockUploadEnabled) {\n         LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n       blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n           DEFAULT_FAST_UPLOAD_BUFFER);\n       partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n       blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n       blockOutputActiveBlocks \u003d intOption(conf,\n           FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n       LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n               \" queue limit\u003d{}\",\n           blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       metadataStore \u003d S3Guard.getMetadataStore(this);\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n+      if (hasMetadataStore()) {\n+        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n+            getMetadataStore(), allowAuthoritative);\n+      }\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    uri \u003d S3xLoginHelper.buildFSURI(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      metadataStore \u003d S3Guard.getMetadataStore(this);\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n      if (hasMetadataStore()) {\n        LOG.debug(\"Using metadata store {}, authoritative\u003d{}\",\n            getMetadataStore(), allowAuthoritative);\n      }\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "49467165a57fb77932d1d526796624b88ebacd91": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14738 Remove S3N and obsolete bits of S3A; rework docs.  Contributed by Steve Loughran.\n",
      "commitDate": "14/09/17 2:10 PM",
      "commitName": "49467165a57fb77932d1d526796624b88ebacd91",
      "commitAuthor": "Aaron Fabbri",
      "commitDateOld": "08/09/17 4:07 AM",
      "commitNameOld": "5bbca80428ffbe776650652de86a3bba885edb31",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 6.42,
      "commitsBetweenForRepo": 95,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,112 +1,111 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     uri \u003d S3xLoginHelper.buildFSURI(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n       if (listVersion \u003c 1 || listVersion \u003e 2) {\n         LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n             \"version 2\", listVersion);\n       }\n       useListV1 \u003d (listVersion \u003d\u003d 1);\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n \n-      blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n+      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n \n-      if (blockUploadEnabled) {\n-        blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n-            DEFAULT_FAST_UPLOAD_BUFFER);\n-        partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n-        blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n-        blockOutputActiveBlocks \u003d intOption(conf,\n-            FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n-        LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n-                \" queue limit\u003d{}\",\n-            blockOutputBuffer, partSize, blockOutputActiveBlocks);\n-      } else {\n-        LOG.debug(\"Using S3AOutputStream\");\n+      if (!blockUploadEnabled) {\n+        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n       }\n+      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n+          DEFAULT_FAST_UPLOAD_BUFFER);\n+      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n+      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n+      blockOutputActiveBlocks \u003d intOption(conf,\n+          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n+      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n+              \" queue limit\u003d{}\",\n+          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n \n       metadataStore \u003d S3Guard.getMetadataStore(this);\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    uri \u003d S3xLoginHelper.buildFSURI(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n\n      boolean blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, true);\n\n      if (!blockUploadEnabled) {\n        LOG.warn(\"The \\\"slow\\\" output stream is no longer supported\");\n      }\n      blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n          DEFAULT_FAST_UPLOAD_BUFFER);\n      partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n      blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n      blockOutputActiveBlocks \u003d intOption(conf,\n          FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n      LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n              \" queue limit\u003d{}\",\n          blockOutputBuffer, partSize, blockOutputActiveBlocks);\n\n      metadataStore \u003d S3Guard.getMetadataStore(this);\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "5bbca80428ffbe776650652de86a3bba885edb31": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13421. Switch to v2 of the S3 List Objects API in S3A.\nContributed by Aaron Fabbri\n",
      "commitDate": "08/09/17 4:07 AM",
      "commitName": "5bbca80428ffbe776650652de86a3bba885edb31",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "01/09/17 6:13 AM",
      "commitNameOld": "621b43e254afaff708cd6fc4698b29628f6abc33",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 6.91,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,105 +1,112 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     uri \u003d S3xLoginHelper.buildFSURI(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n+      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n+      if (listVersion \u003c 1 || listVersion \u003e 2) {\n+        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n+            \"version 2\", listVersion);\n+      }\n+      useListV1 \u003d (listVersion \u003d\u003d 1);\n+\n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n \n       blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n \n       if (blockUploadEnabled) {\n         blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n             DEFAULT_FAST_UPLOAD_BUFFER);\n         partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n         blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n         blockOutputActiveBlocks \u003d intOption(conf,\n             FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n         LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                 \" queue limit\u003d{}\",\n             blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       } else {\n         LOG.debug(\"Using S3AOutputStream\");\n       }\n \n       metadataStore \u003d S3Guard.getMetadataStore(this);\n       allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n           DEFAULT_METADATASTORE_AUTHORITATIVE);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    uri \u003d S3xLoginHelper.buildFSURI(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      int listVersion \u003d conf.getInt(LIST_VERSION, DEFAULT_LIST_VERSION);\n      if (listVersion \u003c 1 || listVersion \u003e 2) {\n        LOG.warn(\"Configured fs.s3a.list.version {} is invalid, forcing \" +\n            \"version 2\", listVersion);\n      }\n      useListV1 \u003d (listVersion \u003d\u003d 1);\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n\n      blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n\n      if (blockUploadEnabled) {\n        blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n            DEFAULT_FAST_UPLOAD_BUFFER);\n        partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n        blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n        blockOutputActiveBlocks \u003d intOption(conf,\n            FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n        LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                \" queue limit\u003d{}\",\n            blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      } else {\n        LOG.debug(\"Using S3AOutputStream\");\n      }\n\n      metadataStore \u003d S3Guard.getMetadataStore(this);\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "621b43e254afaff708cd6fc4698b29628f6abc33": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13345 HS3Guard: Improved Consistency for S3A.\nContributed by: Chris Nauroth, Aaron Fabbri, Mingliang Liu, Lei (Eddy) Xu,\nSean Mackrory, Steve Loughran and others.\n",
      "commitDate": "01/09/17 6:13 AM",
      "commitName": "621b43e254afaff708cd6fc4698b29628f6abc33",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "05/06/17 11:26 AM",
      "commitNameOld": "6aeda55bb8f741d9dafd41f6dfbf1a88acdd4003",
      "commitAuthorOld": "Mingliang Liu",
      "daysBetweenCommits": 87.78,
      "commitsBetweenForRepo": 591,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,105 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     uri \u003d S3xLoginHelper.buildFSURI(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n \n       blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n \n       if (blockUploadEnabled) {\n         blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n             DEFAULT_FAST_UPLOAD_BUFFER);\n         partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n         blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n         blockOutputActiveBlocks \u003d intOption(conf,\n             FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n         LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                 \" queue limit\u003d{}\",\n             blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       } else {\n         LOG.debug(\"Using S3AOutputStream\");\n       }\n+\n+      metadataStore \u003d S3Guard.getMetadataStore(this);\n+      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n+          DEFAULT_METADATASTORE_AUTHORITATIVE);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    uri \u003d S3xLoginHelper.buildFSURI(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n\n      blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n\n      if (blockUploadEnabled) {\n        blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n            DEFAULT_FAST_UPLOAD_BUFFER);\n        partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n        blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n        blockOutputActiveBlocks \u003d intOption(conf,\n            FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n        LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                \" queue limit\u003d{}\",\n            blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      } else {\n        LOG.debug(\"Using S3AOutputStream\");\n      }\n\n      metadataStore \u003d S3Guard.getMetadataStore(this);\n      allowAuthoritative \u003d conf.getBoolean(METADATASTORE_AUTHORITATIVE,\n          DEFAULT_METADATASTORE_AUTHORITATIVE);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "667966c13c1e09077c2e2088bd66c9d7851dd14e": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14324. Refine S3 server-side-encryption key as encryption secret; improve error reporting and diagnostics. Contributed by Steve Loughran\n",
      "commitDate": "20/04/17 5:13 PM",
      "commitName": "667966c13c1e09077c2e2088bd66c9d7851dd14e",
      "commitAuthor": "Mingliang Liu",
      "commitDateOld": "12/04/17 2:30 PM",
      "commitNameOld": "b053fdc547ddbb6322674142a14010683006d123",
      "commitAuthorOld": "Mingliang Liu",
      "daysBetweenCommits": 8.11,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,112 +1,101 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     uri \u003d S3xLoginHelper.buildFSURI(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n-      serverSideEncryptionAlgorithm \u003d S3AEncryptionMethods.getMethod(\n-          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM));\n-      if(S3AEncryptionMethods.SSE_C.equals(serverSideEncryptionAlgorithm) \u0026\u0026\n-          StringUtils.isBlank(getServerSideEncryptionKey(getConf()))) {\n-        throw new IOException(Constants.SSE_C_NO_KEY_ERROR);\n-      }\n-      if(S3AEncryptionMethods.SSE_S3.equals(serverSideEncryptionAlgorithm) \u0026\u0026\n-          StringUtils.isNotBlank(getServerSideEncryptionKey(\n-            getConf()))) {\n-        throw new IOException(Constants.SSE_S3_WITH_KEY_ERROR);\n-      }\n-      LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n+      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n \n       blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n \n       if (blockUploadEnabled) {\n         blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n             DEFAULT_FAST_UPLOAD_BUFFER);\n         partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n         blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n         blockOutputActiveBlocks \u003d intOption(conf,\n             FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n         LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                 \" queue limit\u003d{}\",\n             blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       } else {\n         LOG.debug(\"Using S3AOutputStream\");\n       }\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    uri \u003d S3xLoginHelper.buildFSURI(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d getEncryptionAlgorithm(conf);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n\n      blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n\n      if (blockUploadEnabled) {\n        blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n            DEFAULT_FAST_UPLOAD_BUFFER);\n        partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n        blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n        blockOutputActiveBlocks \u003d intOption(conf,\n            FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n        LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                \" queue limit\u003d{}\",\n            blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      } else {\n        LOG.debug(\"Using S3AOutputStream\");\n      }\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "2e30aa72e01de7b5774fcb312406a393221e0908": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14135. Remove URI parameter in AWSCredentialProvider constructors. Contributed by Mingliang Liu\n",
      "commitDate": "23/03/17 11:33 AM",
      "commitName": "2e30aa72e01de7b5774fcb312406a393221e0908",
      "commitAuthor": "Mingliang Liu",
      "commitDateOld": "23/03/17 5:54 AM",
      "commitNameOld": "a5a4867f3b193a137a6260d539da7e21f02ffab3",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 0.24,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,112 +1,112 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     uri \u003d S3xLoginHelper.buildFSURI(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n-          .createS3Client(name, uri);\n+          .createS3Client(name);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n       unboundedThreadPool \u003d new ThreadPoolExecutor(\n           maxThreads, Integer.MAX_VALUE,\n           keepAliveTime, TimeUnit.SECONDS,\n           new LinkedBlockingQueue\u003cRunnable\u003e(),\n           BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n               \"s3a-transfer-unbounded\"));\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d S3AEncryptionMethods.getMethod(\n           conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM));\n       if(S3AEncryptionMethods.SSE_C.equals(serverSideEncryptionAlgorithm) \u0026\u0026\n           StringUtils.isBlank(getServerSideEncryptionKey(getConf()))) {\n         throw new IOException(Constants.SSE_C_NO_KEY_ERROR);\n       }\n       if(S3AEncryptionMethods.SSE_S3.equals(serverSideEncryptionAlgorithm) \u0026\u0026\n           StringUtils.isNotBlank(getServerSideEncryptionKey(\n             getConf()))) {\n         throw new IOException(Constants.SSE_S3_WITH_KEY_ERROR);\n       }\n       LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n \n       blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n \n       if (blockUploadEnabled) {\n         blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n             DEFAULT_FAST_UPLOAD_BUFFER);\n         partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n         blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n         blockOutputActiveBlocks \u003d intOption(conf,\n             FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n         LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                 \" queue limit\u003d{}\",\n             blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       } else {\n         LOG.debug(\"Using S3AOutputStream\");\n       }\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    uri \u003d S3xLoginHelper.buildFSURI(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d S3AEncryptionMethods.getMethod(\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM));\n      if(S3AEncryptionMethods.SSE_C.equals(serverSideEncryptionAlgorithm) \u0026\u0026\n          StringUtils.isBlank(getServerSideEncryptionKey(getConf()))) {\n        throw new IOException(Constants.SSE_C_NO_KEY_ERROR);\n      }\n      if(S3AEncryptionMethods.SSE_S3.equals(serverSideEncryptionAlgorithm) \u0026\u0026\n          StringUtils.isNotBlank(getServerSideEncryptionKey(\n            getConf()))) {\n        throw new IOException(Constants.SSE_S3_WITH_KEY_ERROR);\n      }\n      LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n\n      blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n\n      if (blockUploadEnabled) {\n        blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n            DEFAULT_FAST_UPLOAD_BUFFER);\n        partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n        blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n        blockOutputActiveBlocks \u003d intOption(conf,\n            FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n        LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                \" queue limit\u003d{}\",\n            blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      } else {\n        LOG.debug(\"Using S3AOutputStream\");\n      }\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "2158496f6bed5f9d14751b82bd5d43b9fd786b95": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13826. S3A Deadlock in multipart copy due to thread pool limits. Contributed by  Sean Mackrory.\n\n(cherry picked from commit e3a74e0369e6e2217d1280179b390227fe1b1684)\n",
      "commitDate": "21/02/17 10:28 AM",
      "commitName": "2158496f6bed5f9d14751b82bd5d43b9fd786b95",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "11/02/17 1:59 PM",
      "commitNameOld": "839b690ed5edc2ac4984640d58c005bb63cd8a07",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 9.85,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,106 +1,112 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     uri \u003d S3xLoginHelper.buildFSURI(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name, uri);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n-      threadPoolExecutor \u003d BlockingThreadPoolExecutorService.newInstance(\n+      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n+      unboundedThreadPool \u003d new ThreadPoolExecutor(\n+          maxThreads, Integer.MAX_VALUE,\n+          keepAliveTime, TimeUnit.SECONDS,\n+          new LinkedBlockingQueue\u003cRunnable\u003e(),\n+          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n+              \"s3a-transfer-unbounded\"));\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d S3AEncryptionMethods.getMethod(\n           conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM));\n       if(S3AEncryptionMethods.SSE_C.equals(serverSideEncryptionAlgorithm) \u0026\u0026\n           StringUtils.isBlank(getServerSideEncryptionKey(getConf()))) {\n         throw new IOException(Constants.SSE_C_NO_KEY_ERROR);\n       }\n       if(S3AEncryptionMethods.SSE_S3.equals(serverSideEncryptionAlgorithm) \u0026\u0026\n           StringUtils.isNotBlank(getServerSideEncryptionKey(\n             getConf()))) {\n         throw new IOException(Constants.SSE_S3_WITH_KEY_ERROR);\n       }\n       LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n \n       blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n \n       if (blockUploadEnabled) {\n         blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n             DEFAULT_FAST_UPLOAD_BUFFER);\n         partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n         blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n         blockOutputActiveBlocks \u003d intOption(conf,\n             FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n         LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                 \" queue limit\u003d{}\",\n             blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       } else {\n         LOG.debug(\"Using S3AOutputStream\");\n       }\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    uri \u003d S3xLoginHelper.buildFSURI(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name, uri);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      boundedThreadPool \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n      unboundedThreadPool \u003d new ThreadPoolExecutor(\n          maxThreads, Integer.MAX_VALUE,\n          keepAliveTime, TimeUnit.SECONDS,\n          new LinkedBlockingQueue\u003cRunnable\u003e(),\n          BlockingThreadPoolExecutorService.newDaemonThreadFactory(\n              \"s3a-transfer-unbounded\"));\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d S3AEncryptionMethods.getMethod(\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM));\n      if(S3AEncryptionMethods.SSE_C.equals(serverSideEncryptionAlgorithm) \u0026\u0026\n          StringUtils.isBlank(getServerSideEncryptionKey(getConf()))) {\n        throw new IOException(Constants.SSE_C_NO_KEY_ERROR);\n      }\n      if(S3AEncryptionMethods.SSE_S3.equals(serverSideEncryptionAlgorithm) \u0026\u0026\n          StringUtils.isNotBlank(getServerSideEncryptionKey(\n            getConf()))) {\n        throw new IOException(Constants.SSE_S3_WITH_KEY_ERROR);\n      }\n      LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n\n      blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n\n      if (blockUploadEnabled) {\n        blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n            DEFAULT_FAST_UPLOAD_BUFFER);\n        partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n        blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n        blockOutputActiveBlocks \u003d intOption(conf,\n            FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n        LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                \" queue limit\u003d{}\",\n            blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      } else {\n        LOG.debug(\"Using S3AOutputStream\");\n      }\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "839b690ed5edc2ac4984640d58c005bb63cd8a07": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13075. Add support for SSE-KMS and SSE-C in s3a filesystem. (Steve Moist via lei)\n",
      "commitDate": "11/02/17 1:59 PM",
      "commitName": "839b690ed5edc2ac4984640d58c005bb63cd8a07",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "11/01/17 9:25 AM",
      "commitNameOld": "e648b6e1382336af69434dfbf9161bced3caa244",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 31.19,
      "commitsBetweenForRepo": 140,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,97 +1,106 @@\n   public void initialize(URI name, Configuration originalConf)\n       throws IOException {\n     uri \u003d S3xLoginHelper.buildFSURI(name);\n     // get the host; this is guaranteed to be non-null, non-empty\n     bucket \u003d name.getHost();\n     // clone the configuration into one with propagated bucket options\n     Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n     patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name, uri);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       threadPoolExecutor \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n-      serverSideEncryptionAlgorithm \u003d\n-          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n+      serverSideEncryptionAlgorithm \u003d S3AEncryptionMethods.getMethod(\n+          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM));\n+      if(S3AEncryptionMethods.SSE_C.equals(serverSideEncryptionAlgorithm) \u0026\u0026\n+          StringUtils.isBlank(getServerSideEncryptionKey(getConf()))) {\n+        throw new IOException(Constants.SSE_C_NO_KEY_ERROR);\n+      }\n+      if(S3AEncryptionMethods.SSE_S3.equals(serverSideEncryptionAlgorithm) \u0026\u0026\n+          StringUtils.isNotBlank(getServerSideEncryptionKey(\n+            getConf()))) {\n+        throw new IOException(Constants.SSE_S3_WITH_KEY_ERROR);\n+      }\n       LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n \n       blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n \n       if (blockUploadEnabled) {\n         blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n             DEFAULT_FAST_UPLOAD_BUFFER);\n         partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n         blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n         blockOutputActiveBlocks \u003d intOption(conf,\n             FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n         LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                 \" queue limit\u003d{}\",\n             blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       } else {\n         LOG.debug(\"Using S3AOutputStream\");\n       }\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    uri \u003d S3xLoginHelper.buildFSURI(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name, uri);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      threadPoolExecutor \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d S3AEncryptionMethods.getMethod(\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM));\n      if(S3AEncryptionMethods.SSE_C.equals(serverSideEncryptionAlgorithm) \u0026\u0026\n          StringUtils.isBlank(getServerSideEncryptionKey(getConf()))) {\n        throw new IOException(Constants.SSE_C_NO_KEY_ERROR);\n      }\n      if(S3AEncryptionMethods.SSE_S3.equals(serverSideEncryptionAlgorithm) \u0026\u0026\n          StringUtils.isNotBlank(getServerSideEncryptionKey(\n            getConf()))) {\n        throw new IOException(Constants.SSE_S3_WITH_KEY_ERROR);\n      }\n      LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n\n      blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n\n      if (blockUploadEnabled) {\n        blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n            DEFAULT_FAST_UPLOAD_BUFFER);\n        partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n        blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n        blockOutputActiveBlocks \u003d intOption(conf,\n            FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n        LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                \" queue limit\u003d{}\",\n            blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      } else {\n        LOG.debug(\"Using S3AOutputStream\");\n      }\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "e648b6e1382336af69434dfbf9161bced3caa244": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-13336 S3A to support per-bucket configuration. Contributed by Steve Loughran\n",
      "commitDate": "11/01/17 9:25 AM",
      "commitName": "e648b6e1382336af69434dfbf9161bced3caa244",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-13336 S3A to support per-bucket configuration. Contributed by Steve Loughran\n",
          "commitDate": "11/01/17 9:25 AM",
          "commitName": "e648b6e1382336af69434dfbf9161bced3caa244",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "28/11/16 4:30 PM",
          "commitNameOld": "d60a60be8aa450c44d3be69d26c88025e253ac0c",
          "commitAuthorOld": "Mingliang Liu",
          "daysBetweenCommits": 43.7,
          "commitsBetweenForRepo": 226,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,92 +1,97 @@\n-  public void initialize(URI name, Configuration conf) throws IOException {\n+  public void initialize(URI name, Configuration originalConf)\n+      throws IOException {\n+    uri \u003d S3xLoginHelper.buildFSURI(name);\n+    // get the host; this is guaranteed to be non-null, non-empty\n+    bucket \u003d name.getHost();\n+    // clone the configuration into one with propagated bucket options\n+    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n+    patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n-      uri \u003d S3xLoginHelper.buildFSURI(name);\n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n-      bucket \u003d name.getHost();\n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name, uri);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       threadPoolExecutor \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d\n           conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n       LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n \n       blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n \n       if (blockUploadEnabled) {\n         blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n             DEFAULT_FAST_UPLOAD_BUFFER);\n         partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n         blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n         blockOutputActiveBlocks \u003d intOption(conf,\n             FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n         LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                 \" queue limit\u003d{}\",\n             blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       } else {\n         LOG.debug(\"Using S3AOutputStream\");\n       }\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    uri \u003d S3xLoginHelper.buildFSURI(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name, uri);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      threadPoolExecutor \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n      LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n\n      blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n\n      if (blockUploadEnabled) {\n        blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n            DEFAULT_FAST_UPLOAD_BUFFER);\n        partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n        blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n        blockOutputActiveBlocks \u003d intOption(conf,\n            FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n        LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                \" queue limit\u003d{}\",\n            blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      } else {\n        LOG.debug(\"Using S3AOutputStream\");\n      }\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
          "extendedDetails": {
            "oldValue": "[name-URI, conf-Configuration]",
            "newValue": "[name-URI, originalConf-Configuration]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-13336 S3A to support per-bucket configuration. Contributed by Steve Loughran\n",
          "commitDate": "11/01/17 9:25 AM",
          "commitName": "e648b6e1382336af69434dfbf9161bced3caa244",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "28/11/16 4:30 PM",
          "commitNameOld": "d60a60be8aa450c44d3be69d26c88025e253ac0c",
          "commitAuthorOld": "Mingliang Liu",
          "daysBetweenCommits": 43.7,
          "commitsBetweenForRepo": 226,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,92 +1,97 @@\n-  public void initialize(URI name, Configuration conf) throws IOException {\n+  public void initialize(URI name, Configuration originalConf)\n+      throws IOException {\n+    uri \u003d S3xLoginHelper.buildFSURI(name);\n+    // get the host; this is guaranteed to be non-null, non-empty\n+    bucket \u003d name.getHost();\n+    // clone the configuration into one with propagated bucket options\n+    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n+    patchSecurityCredentialProviders(conf);\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n-      uri \u003d S3xLoginHelper.buildFSURI(name);\n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n-      bucket \u003d name.getHost();\n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name, uri);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n           DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       threadPoolExecutor \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d\n           conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n       LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n \n       blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n \n       if (blockUploadEnabled) {\n         blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n             DEFAULT_FAST_UPLOAD_BUFFER);\n         partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n         blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n         blockOutputActiveBlocks \u003d intOption(conf,\n             FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n         LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                 \" queue limit\u003d{}\",\n             blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       } else {\n         LOG.debug(\"Using S3AOutputStream\");\n       }\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void initialize(URI name, Configuration originalConf)\n      throws IOException {\n    uri \u003d S3xLoginHelper.buildFSURI(name);\n    // get the host; this is guaranteed to be non-null, non-empty\n    bucket \u003d name.getHost();\n    // clone the configuration into one with propagated bucket options\n    Configuration conf \u003d propagateBucketOptions(originalConf, bucket);\n    patchSecurityCredentialProviders(conf);\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name, uri);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      threadPoolExecutor \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n      LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n\n      blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n\n      if (blockUploadEnabled) {\n        blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n            DEFAULT_FAST_UPLOAD_BUFFER);\n        partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n        blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n        blockOutputActiveBlocks \u003d intOption(conf,\n            FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n        LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                \" queue limit\u003d{}\",\n            blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      } else {\n        LOG.debug(\"Using S3AOutputStream\");\n      }\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "a1761a841e95ef7d2296ac3e40b3a26d97787eab": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13680. fs.s3a.readahead.range to use getLongBytes. Contributed by Abhishek Modi.\n",
      "commitDate": "31/10/16 1:54 PM",
      "commitName": "a1761a841e95ef7d2296ac3e40b3a26d97787eab",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "24/10/16 9:54 PM",
      "commitNameOld": "3372e940303149d6258e0b72c54d72f080f0daa2",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 6.67,
      "commitsBetweenForRepo": 91,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,91 +1,92 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       uri \u003d S3xLoginHelper.buildFSURI(name);\n       // Username is the current user at the time the FS was instantiated.\n       username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n       workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       bucket \u003d name.getHost();\n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name, uri);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n-      longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n+      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n-      readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n+      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n+          DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       threadPoolExecutor \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d\n           conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n       LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n \n       blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n \n       if (blockUploadEnabled) {\n         blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n             DEFAULT_FAST_UPLOAD_BUFFER);\n         partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n         blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n         blockOutputActiveBlocks \u003d intOption(conf,\n             FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n         LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                 \" queue limit\u003d{}\",\n             blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       } else {\n         LOG.debug(\"Using S3AOutputStream\");\n       }\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      uri \u003d S3xLoginHelper.buildFSURI(name);\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      bucket \u003d name.getHost();\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name, uri);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longBytesOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longBytesOption(conf, READAHEAD_RANGE,\n          DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      threadPoolExecutor \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n      LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n\n      blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n\n      if (blockUploadEnabled) {\n        blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n            DEFAULT_FAST_UPLOAD_BUFFER);\n        partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n        blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n        blockOutputActiveBlocks \u003d intOption(conf,\n            FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n        LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                \" queue limit\u003d{}\",\n            blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      } else {\n        LOG.debug(\"Using S3AOutputStream\");\n      }\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "3372e940303149d6258e0b72c54d72f080f0daa2": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12774. s3a should use UGI.getCurrentUser.getShortname() for username. Contributed by Steve Loughran.\n",
      "commitDate": "24/10/16 9:54 PM",
      "commitName": "3372e940303149d6258e0b72c54d72f080f0daa2",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "20/10/16 6:50 AM",
      "commitNameOld": "9ae270af02c243993f853513c731cb268430e492",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 4.63,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,89 +1,91 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       uri \u003d S3xLoginHelper.buildFSURI(name);\n-      workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n+      // Username is the current user at the time the FS was instantiated.\n+      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n+      workingDir \u003d new Path(\"/user\", username)\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       bucket \u003d name.getHost();\n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name, uri);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d getMultipartSizeProperty(conf,\n           MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       multiPartThreshold \u003d getMultipartSizeProperty(conf,\n           MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n       //check but do not store the block size\n       longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d intOption(conf,\n           MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n       long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n           DEFAULT_KEEPALIVE_TIME, 0);\n       threadPoolExecutor \u003d BlockingThreadPoolExecutorService.newInstance(\n           maxThreads,\n           maxThreads + totalTasks,\n           keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d\n           conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n       LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n \n       blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n \n       if (blockUploadEnabled) {\n         blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n             DEFAULT_FAST_UPLOAD_BUFFER);\n         partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n         blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n         blockOutputActiveBlocks \u003d intOption(conf,\n             FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n         LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                 \" queue limit\u003d{}\",\n             blockOutputBuffer, partSize, blockOutputActiveBlocks);\n       } else {\n         LOG.debug(\"Using S3AOutputStream\");\n       }\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      uri \u003d S3xLoginHelper.buildFSURI(name);\n      // Username is the current user at the time the FS was instantiated.\n      username \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n      workingDir \u003d new Path(\"/user\", username)\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      bucket \u003d name.getHost();\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name, uri);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      threadPoolExecutor \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n      LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n\n      blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n\n      if (blockUploadEnabled) {\n        blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n            DEFAULT_FAST_UPLOAD_BUFFER);\n        partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n        blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n        blockOutputActiveBlocks \u003d intOption(conf,\n            FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n        LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                \" queue limit\u003d{}\",\n            blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      } else {\n        LOG.debug(\"Using S3AOutputStream\");\n      }\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "6c348c56918973fd988b110e79231324a8befe12": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13560. S3ABlockOutputStream to support huge (many GB) file writes. Contributed by Steve Loughran\n",
      "commitDate": "18/10/16 1:16 PM",
      "commitName": "6c348c56918973fd988b110e79231324a8befe12",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "07/10/16 4:51 AM",
      "commitNameOld": "ebd4f39a393e5fa9a810c6a36b749549229a53df",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 11.35,
      "commitsBetweenForRepo": 80,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,79 +1,89 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       uri \u003d S3xLoginHelper.buildFSURI(name);\n       workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       bucket \u003d name.getHost();\n \n       Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n           S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n           S3ClientFactory.class);\n       s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n           .createS3Client(name, uri);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n-      partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n-      if (partSize \u003c 5 * 1024 * 1024) {\n-        LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n-        partSize \u003d 5 * 1024 * 1024;\n-      }\n+      partSize \u003d getMultipartSizeProperty(conf,\n+          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n+      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n+          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n-      multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n-          DEFAULT_MIN_MULTIPART_THRESHOLD);\n-      if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n-        LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n-        multiPartThreshold \u003d 5 * 1024 * 1024;\n-      }\n       //check but do not store the block size\n       longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n-      int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n-      if (totalTasks \u003c 1) {\n-        LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n-        totalTasks \u003d 1;\n-      }\n-      long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n-      threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n-          maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n+      int totalTasks \u003d intOption(conf,\n+          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n+      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n+          DEFAULT_KEEPALIVE_TIME, 0);\n+      threadPoolExecutor \u003d BlockingThreadPoolExecutorService.newInstance(\n+          maxThreads,\n+          maxThreads + totalTasks,\n+          keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d\n           conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n+      LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n+\n+      blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n+\n+      if (blockUploadEnabled) {\n+        blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n+            DEFAULT_FAST_UPLOAD_BUFFER);\n+        partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n+        blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n+        blockOutputActiveBlocks \u003d intOption(conf,\n+            FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n+        LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n+                \" queue limit\u003d{}\",\n+            blockOutputBuffer, partSize, blockOutputActiveBlocks);\n+      } else {\n+        LOG.debug(\"Using S3AOutputStream\");\n+      }\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      uri \u003d S3xLoginHelper.buildFSURI(name);\n      workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      bucket \u003d name.getHost();\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name, uri);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d getMultipartSizeProperty(conf,\n          MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      multiPartThreshold \u003d getMultipartSizeProperty(conf,\n          MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n      //check but do not store the block size\n      longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d intOption(conf,\n          MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS, 1);\n      long keepAliveTime \u003d longOption(conf, KEEPALIVE_TIME,\n          DEFAULT_KEEPALIVE_TIME, 0);\n      threadPoolExecutor \u003d BlockingThreadPoolExecutorService.newInstance(\n          maxThreads,\n          maxThreads + totalTasks,\n          keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n      LOG.debug(\"Using encryption {}\", serverSideEncryptionAlgorithm);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n\n      blockUploadEnabled \u003d conf.getBoolean(FAST_UPLOAD, DEFAULT_FAST_UPLOAD);\n\n      if (blockUploadEnabled) {\n        blockOutputBuffer \u003d conf.getTrimmed(FAST_UPLOAD_BUFFER,\n            DEFAULT_FAST_UPLOAD_BUFFER);\n        partSize \u003d ensureOutputParameterInRange(MULTIPART_SIZE, partSize);\n        blockFactory \u003d S3ADataBlocks.createFactory(this, blockOutputBuffer);\n        blockOutputActiveBlocks \u003d intOption(conf,\n            FAST_UPLOAD_ACTIVE_BLOCKS, DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS, 1);\n        LOG.debug(\"Using S3ABlockOutputStream with buffer \u003d {}; block\u003d{};\" +\n                \" queue limit\u003d{}\",\n            blockOutputBuffer, partSize, blockOutputActiveBlocks);\n      } else {\n        LOG.debug(\"Using S3AOutputStream\");\n      }\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "d152557cf7f4d2288524c222fcbaf152bdc038b0": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13447. Refactor S3AFileSystem to support introduction of separate metadata repository and tests. Contributed by Chris Nauroth.\n",
      "commitDate": "06/09/16 9:36 AM",
      "commitName": "d152557cf7f4d2288524c222fcbaf152bdc038b0",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "23/08/16 2:12 PM",
      "commitNameOld": "c37346d0e3f9d39d0aec7a9c5bda3e9772aa969b",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 13.81,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,105 +1,79 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       uri \u003d S3xLoginHelper.buildFSURI(name);\n       workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       bucket \u003d name.getHost();\n \n-      AWSCredentialsProvider credentials \u003d\n-          createAWSCredentialProviderSet(name, conf, uri);\n-\n-      ClientConfiguration awsConf \u003d new ClientConfiguration();\n-      awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n-          DEFAULT_MAXIMUM_CONNECTIONS, 1));\n-      boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n-          DEFAULT_SECURE_CONNECTIONS);\n-      awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n-      awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n-          DEFAULT_MAX_ERROR_RETRIES, 0));\n-      awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n-          DEFAULT_ESTABLISH_TIMEOUT, 0));\n-      awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n-          DEFAULT_SOCKET_TIMEOUT, 0));\n-      int sockSendBuffer \u003d intOption(conf, SOCKET_SEND_BUFFER,\n-          DEFAULT_SOCKET_SEND_BUFFER, 2048);\n-      int sockRecvBuffer \u003d intOption(conf, SOCKET_RECV_BUFFER,\n-          DEFAULT_SOCKET_RECV_BUFFER, 2048);\n-      awsConf.setSocketBufferSizeHints(sockSendBuffer, sockRecvBuffer);\n-      String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n-      if (!signerOverride.isEmpty()) {\n-        LOG.debug(\"Signer override \u003d {}\", signerOverride);\n-        awsConf.setSignerOverride(signerOverride);\n-      }\n-\n-      initProxySupport(conf, awsConf, secureConnections);\n-\n-      initUserAgent(conf, awsConf);\n-\n-      initAmazonS3Client(conf, credentials, awsConf);\n+      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n+          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n+          S3ClientFactory.class);\n+      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n+          .createS3Client(name, uri);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       if (partSize \u003c 5 * 1024 * 1024) {\n         LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n         partSize \u003d 5 * 1024 * 1024;\n       }\n \n       multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n           DEFAULT_MIN_MULTIPART_THRESHOLD);\n       if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n         LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n         multiPartThreshold \u003d 5 * 1024 * 1024;\n       }\n       //check but do not store the block size\n       longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n       if (totalTasks \u003c 1) {\n         LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n         totalTasks \u003d 1;\n       }\n       long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n       threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n           maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d\n           conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      uri \u003d S3xLoginHelper.buildFSURI(name);\n      workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      bucket \u003d name.getHost();\n\n      Class\u003c? extends S3ClientFactory\u003e s3ClientFactoryClass \u003d conf.getClass(\n          S3_CLIENT_FACTORY_IMPL, DEFAULT_S3_CLIENT_FACTORY_IMPL,\n          S3ClientFactory.class);\n      s3 \u003d ReflectionUtils.newInstance(s3ClientFactoryClass, conf)\n          .createS3Client(name, uri);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      if (partSize \u003c 5 * 1024 * 1024) {\n        LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n        partSize \u003d 5 * 1024 * 1024;\n      }\n\n      multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n          DEFAULT_MIN_MULTIPART_THRESHOLD);\n      if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n        LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n        multiPartThreshold \u003d 5 * 1024 * 1024;\n      }\n      //check but do not store the block size\n      longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n      if (totalTasks \u003c 1) {\n        LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n        totalTasks \u003d 1;\n      }\n      long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n      threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n          maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "763f0497bb996e331e40caed9ca0af966f5b3fac": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13252. Tune S3A provider plugin mechanism. Contributed by Steve Loughran.\n",
      "commitDate": "19/08/16 10:48 AM",
      "commitName": "763f0497bb996e331e40caed9ca0af966f5b3fac",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "17/08/16 2:54 PM",
      "commitNameOld": "822d661b8fcc42bec6eea958d9fd02ef1aaa4b6c",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 1.83,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,105 +1,105 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       uri \u003d S3xLoginHelper.buildFSURI(name);\n       workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       bucket \u003d name.getHost();\n \n       AWSCredentialsProvider credentials \u003d\n-          getAWSCredentialsProvider(name, conf);\n+          createAWSCredentialProviderSet(name, conf, uri);\n \n       ClientConfiguration awsConf \u003d new ClientConfiguration();\n       awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n           DEFAULT_MAXIMUM_CONNECTIONS, 1));\n       boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n           DEFAULT_SECURE_CONNECTIONS);\n       awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n       awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n           DEFAULT_MAX_ERROR_RETRIES, 0));\n       awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n           DEFAULT_ESTABLISH_TIMEOUT, 0));\n       awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n           DEFAULT_SOCKET_TIMEOUT, 0));\n       int sockSendBuffer \u003d intOption(conf, SOCKET_SEND_BUFFER,\n           DEFAULT_SOCKET_SEND_BUFFER, 2048);\n       int sockRecvBuffer \u003d intOption(conf, SOCKET_RECV_BUFFER,\n           DEFAULT_SOCKET_RECV_BUFFER, 2048);\n       awsConf.setSocketBufferSizeHints(sockSendBuffer, sockRecvBuffer);\n       String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n       if (!signerOverride.isEmpty()) {\n         LOG.debug(\"Signer override \u003d {}\", signerOverride);\n         awsConf.setSignerOverride(signerOverride);\n       }\n \n       initProxySupport(conf, awsConf, secureConnections);\n \n       initUserAgent(conf, awsConf);\n \n       initAmazonS3Client(conf, credentials, awsConf);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       listing \u003d new Listing(this);\n       partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       if (partSize \u003c 5 * 1024 * 1024) {\n         LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n         partSize \u003d 5 * 1024 * 1024;\n       }\n \n       multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n           DEFAULT_MIN_MULTIPART_THRESHOLD);\n       if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n         LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n         multiPartThreshold \u003d 5 * 1024 * 1024;\n       }\n       //check but do not store the block size\n       longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n       if (totalTasks \u003c 1) {\n         LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n         totalTasks \u003d 1;\n       }\n       long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n       threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n           maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d\n           conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      uri \u003d S3xLoginHelper.buildFSURI(name);\n      workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      bucket \u003d name.getHost();\n\n      AWSCredentialsProvider credentials \u003d\n          createAWSCredentialProviderSet(name, conf, uri);\n\n      ClientConfiguration awsConf \u003d new ClientConfiguration();\n      awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n          DEFAULT_MAXIMUM_CONNECTIONS, 1));\n      boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n          DEFAULT_SECURE_CONNECTIONS);\n      awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n      awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n          DEFAULT_MAX_ERROR_RETRIES, 0));\n      awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n          DEFAULT_ESTABLISH_TIMEOUT, 0));\n      awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n          DEFAULT_SOCKET_TIMEOUT, 0));\n      int sockSendBuffer \u003d intOption(conf, SOCKET_SEND_BUFFER,\n          DEFAULT_SOCKET_SEND_BUFFER, 2048);\n      int sockRecvBuffer \u003d intOption(conf, SOCKET_RECV_BUFFER,\n          DEFAULT_SOCKET_RECV_BUFFER, 2048);\n      awsConf.setSocketBufferSizeHints(sockSendBuffer, sockRecvBuffer);\n      String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n      if (!signerOverride.isEmpty()) {\n        LOG.debug(\"Signer override \u003d {}\", signerOverride);\n        awsConf.setSignerOverride(signerOverride);\n      }\n\n      initProxySupport(conf, awsConf, secureConnections);\n\n      initUserAgent(conf, awsConf);\n\n      initAmazonS3Client(conf, credentials, awsConf);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      if (partSize \u003c 5 * 1024 * 1024) {\n        LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n        partSize \u003d 5 * 1024 * 1024;\n      }\n\n      multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n          DEFAULT_MIN_MULTIPART_THRESHOLD);\n      if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n        LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n        multiPartThreshold \u003d 5 * 1024 * 1024;\n      }\n      //check but do not store the block size\n      longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n      if (totalTasks \u003c 1) {\n        LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n        totalTasks \u003d 1;\n      }\n      long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n      threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n          maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "822d661b8fcc42bec6eea958d9fd02ef1aaa4b6c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13208. S3A listFiles(recursive\u003dtrue) to do a bulk listObjects instead of walking the pseudo-tree of directories. Contributed by Steve Loughran.\n",
      "commitDate": "17/08/16 2:54 PM",
      "commitName": "822d661b8fcc42bec6eea958d9fd02ef1aaa4b6c",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "25/07/16 7:50 AM",
      "commitNameOld": "86ae218893d018638e937c2528c8e84336254da7",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 23.29,
      "commitsBetweenForRepo": 177,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,105 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       uri \u003d S3xLoginHelper.buildFSURI(name);\n       workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       bucket \u003d name.getHost();\n \n       AWSCredentialsProvider credentials \u003d\n           getAWSCredentialsProvider(name, conf);\n \n       ClientConfiguration awsConf \u003d new ClientConfiguration();\n       awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n           DEFAULT_MAXIMUM_CONNECTIONS, 1));\n       boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n           DEFAULT_SECURE_CONNECTIONS);\n       awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n       awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n           DEFAULT_MAX_ERROR_RETRIES, 0));\n       awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n           DEFAULT_ESTABLISH_TIMEOUT, 0));\n       awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n           DEFAULT_SOCKET_TIMEOUT, 0));\n       int sockSendBuffer \u003d intOption(conf, SOCKET_SEND_BUFFER,\n           DEFAULT_SOCKET_SEND_BUFFER, 2048);\n       int sockRecvBuffer \u003d intOption(conf, SOCKET_RECV_BUFFER,\n           DEFAULT_SOCKET_RECV_BUFFER, 2048);\n       awsConf.setSocketBufferSizeHints(sockSendBuffer, sockRecvBuffer);\n       String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n       if (!signerOverride.isEmpty()) {\n         LOG.debug(\"Signer override \u003d {}\", signerOverride);\n         awsConf.setSignerOverride(signerOverride);\n       }\n \n       initProxySupport(conf, awsConf, secureConnections);\n \n       initUserAgent(conf, awsConf);\n \n       initAmazonS3Client(conf, credentials, awsConf);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n+      listing \u003d new Listing(this);\n       partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       if (partSize \u003c 5 * 1024 * 1024) {\n         LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n         partSize \u003d 5 * 1024 * 1024;\n       }\n \n       multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n           DEFAULT_MIN_MULTIPART_THRESHOLD);\n       if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n         LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n         multiPartThreshold \u003d 5 * 1024 * 1024;\n       }\n       //check but do not store the block size\n       longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n       if (totalTasks \u003c 1) {\n         LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n         totalTasks \u003d 1;\n       }\n       long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n       threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n           maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d\n           conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      uri \u003d S3xLoginHelper.buildFSURI(name);\n      workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      bucket \u003d name.getHost();\n\n      AWSCredentialsProvider credentials \u003d\n          getAWSCredentialsProvider(name, conf);\n\n      ClientConfiguration awsConf \u003d new ClientConfiguration();\n      awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n          DEFAULT_MAXIMUM_CONNECTIONS, 1));\n      boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n          DEFAULT_SECURE_CONNECTIONS);\n      awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n      awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n          DEFAULT_MAX_ERROR_RETRIES, 0));\n      awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n          DEFAULT_ESTABLISH_TIMEOUT, 0));\n      awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n          DEFAULT_SOCKET_TIMEOUT, 0));\n      int sockSendBuffer \u003d intOption(conf, SOCKET_SEND_BUFFER,\n          DEFAULT_SOCKET_SEND_BUFFER, 2048);\n      int sockRecvBuffer \u003d intOption(conf, SOCKET_RECV_BUFFER,\n          DEFAULT_SOCKET_RECV_BUFFER, 2048);\n      awsConf.setSocketBufferSizeHints(sockSendBuffer, sockRecvBuffer);\n      String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n      if (!signerOverride.isEmpty()) {\n        LOG.debug(\"Signer override \u003d {}\", signerOverride);\n        awsConf.setSignerOverride(signerOverride);\n      }\n\n      initProxySupport(conf, awsConf, secureConnections);\n\n      initUserAgent(conf, awsConf);\n\n      initAmazonS3Client(conf, credentials, awsConf);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      listing \u003d new Listing(this);\n      partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      if (partSize \u003c 5 * 1024 * 1024) {\n        LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n        partSize \u003d 5 * 1024 * 1024;\n      }\n\n      multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n          DEFAULT_MIN_MULTIPART_THRESHOLD);\n      if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n        LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n        multiPartThreshold \u003d 5 * 1024 * 1024;\n      }\n      //check but do not store the block size\n      longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n      if (totalTasks \u003c 1) {\n        LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n        totalTasks \u003d 1;\n      }\n      long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n      threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n          maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "37362c2f922b8d038002e61132b110ae4dd6d5ba": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13212 Provide an option to set the socket buffers in S3AFileSystem (Rajesh Balamohan)\n",
      "commitDate": "20/07/16 5:42 AM",
      "commitName": "37362c2f922b8d038002e61132b110ae4dd6d5ba",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "22/06/16 7:45 AM",
      "commitNameOld": "4ee3543625c77c06d566fe81644d21c607d6d74d",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 27.91,
      "commitsBetweenForRepo": 318,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,99 +1,104 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       uri \u003d S3xLoginHelper.buildFSURI(name);\n       workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       bucket \u003d name.getHost();\n \n       AWSCredentialsProvider credentials \u003d\n           getAWSCredentialsProvider(name, conf);\n \n       ClientConfiguration awsConf \u003d new ClientConfiguration();\n       awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n           DEFAULT_MAXIMUM_CONNECTIONS, 1));\n       boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n           DEFAULT_SECURE_CONNECTIONS);\n       awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n       awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n           DEFAULT_MAX_ERROR_RETRIES, 0));\n       awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n           DEFAULT_ESTABLISH_TIMEOUT, 0));\n       awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n           DEFAULT_SOCKET_TIMEOUT, 0));\n+      int sockSendBuffer \u003d intOption(conf, SOCKET_SEND_BUFFER,\n+          DEFAULT_SOCKET_SEND_BUFFER, 2048);\n+      int sockRecvBuffer \u003d intOption(conf, SOCKET_RECV_BUFFER,\n+          DEFAULT_SOCKET_RECV_BUFFER, 2048);\n+      awsConf.setSocketBufferSizeHints(sockSendBuffer, sockRecvBuffer);\n       String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n       if (!signerOverride.isEmpty()) {\n         LOG.debug(\"Signer override \u003d {}\", signerOverride);\n         awsConf.setSignerOverride(signerOverride);\n       }\n \n       initProxySupport(conf, awsConf, secureConnections);\n \n       initUserAgent(conf, awsConf);\n \n       initAmazonS3Client(conf, credentials, awsConf);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       if (partSize \u003c 5 * 1024 * 1024) {\n         LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n         partSize \u003d 5 * 1024 * 1024;\n       }\n \n       multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n           DEFAULT_MIN_MULTIPART_THRESHOLD);\n       if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n         LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n         multiPartThreshold \u003d 5 * 1024 * 1024;\n       }\n       //check but do not store the block size\n       longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n       if (totalTasks \u003c 1) {\n         LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n         totalTasks \u003d 1;\n       }\n       long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n       threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n           maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d\n           conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n       inputPolicy \u003d S3AInputPolicy.getPolicy(\n           conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      uri \u003d S3xLoginHelper.buildFSURI(name);\n      workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      bucket \u003d name.getHost();\n\n      AWSCredentialsProvider credentials \u003d\n          getAWSCredentialsProvider(name, conf);\n\n      ClientConfiguration awsConf \u003d new ClientConfiguration();\n      awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n          DEFAULT_MAXIMUM_CONNECTIONS, 1));\n      boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n          DEFAULT_SECURE_CONNECTIONS);\n      awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n      awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n          DEFAULT_MAX_ERROR_RETRIES, 0));\n      awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n          DEFAULT_ESTABLISH_TIMEOUT, 0));\n      awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n          DEFAULT_SOCKET_TIMEOUT, 0));\n      int sockSendBuffer \u003d intOption(conf, SOCKET_SEND_BUFFER,\n          DEFAULT_SOCKET_SEND_BUFFER, 2048);\n      int sockRecvBuffer \u003d intOption(conf, SOCKET_RECV_BUFFER,\n          DEFAULT_SOCKET_RECV_BUFFER, 2048);\n      awsConf.setSocketBufferSizeHints(sockSendBuffer, sockRecvBuffer);\n      String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n      if (!signerOverride.isEmpty()) {\n        LOG.debug(\"Signer override \u003d {}\", signerOverride);\n        awsConf.setSignerOverride(signerOverride);\n      }\n\n      initProxySupport(conf, awsConf, secureConnections);\n\n      initUserAgent(conf, awsConf);\n\n      initAmazonS3Client(conf, credentials, awsConf);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      if (partSize \u003c 5 * 1024 * 1024) {\n        LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n        partSize \u003d 5 * 1024 * 1024;\n      }\n\n      multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n          DEFAULT_MIN_MULTIPART_THRESHOLD);\n      if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n        LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n        multiPartThreshold \u003d 5 * 1024 * 1024;\n      }\n      //check but do not store the block size\n      longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n      if (totalTasks \u003c 1) {\n        LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n        totalTasks \u003d 1;\n      }\n      long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n      threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n          maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "4ee3543625c77c06d566fe81644d21c607d6d74d": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13203 S3A: Support fadvise \"random\" mode for high performance readPositioned() reads. Contributed by Rajesh Balamohan and stevel.\n",
      "commitDate": "22/06/16 7:45 AM",
      "commitName": "4ee3543625c77c06d566fe81644d21c607d6d74d",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "16/06/16 11:13 AM",
      "commitNameOld": "4aefe119a0203c03cdc893dcb3330fd37f26f0ee",
      "commitAuthorOld": "Ravi Prakash",
      "daysBetweenCommits": 5.86,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,97 +1,99 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       uri \u003d S3xLoginHelper.buildFSURI(name);\n       workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       bucket \u003d name.getHost();\n \n       AWSCredentialsProvider credentials \u003d\n           getAWSCredentialsProvider(name, conf);\n \n       ClientConfiguration awsConf \u003d new ClientConfiguration();\n       awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n           DEFAULT_MAXIMUM_CONNECTIONS, 1));\n       boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n           DEFAULT_SECURE_CONNECTIONS);\n       awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n       awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n           DEFAULT_MAX_ERROR_RETRIES, 0));\n       awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n           DEFAULT_ESTABLISH_TIMEOUT, 0));\n       awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n           DEFAULT_SOCKET_TIMEOUT, 0));\n       String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n       if (!signerOverride.isEmpty()) {\n         LOG.debug(\"Signer override \u003d {}\", signerOverride);\n         awsConf.setSignerOverride(signerOverride);\n       }\n \n       initProxySupport(conf, awsConf, secureConnections);\n \n       initUserAgent(conf, awsConf);\n \n       initAmazonS3Client(conf, credentials, awsConf);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       if (partSize \u003c 5 * 1024 * 1024) {\n         LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n         partSize \u003d 5 * 1024 * 1024;\n       }\n \n       multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n           DEFAULT_MIN_MULTIPART_THRESHOLD);\n       if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n         LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n         multiPartThreshold \u003d 5 * 1024 * 1024;\n       }\n       //check but do not store the block size\n       longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n       if (totalTasks \u003c 1) {\n         LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n         totalTasks \u003d 1;\n       }\n       long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n       threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n           maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d\n           conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n+      inputPolicy \u003d S3AInputPolicy.getPolicy(\n+          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      uri \u003d S3xLoginHelper.buildFSURI(name);\n      workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      bucket \u003d name.getHost();\n\n      AWSCredentialsProvider credentials \u003d\n          getAWSCredentialsProvider(name, conf);\n\n      ClientConfiguration awsConf \u003d new ClientConfiguration();\n      awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n          DEFAULT_MAXIMUM_CONNECTIONS, 1));\n      boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n          DEFAULT_SECURE_CONNECTIONS);\n      awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n      awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n          DEFAULT_MAX_ERROR_RETRIES, 0));\n      awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n          DEFAULT_ESTABLISH_TIMEOUT, 0));\n      awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n          DEFAULT_SOCKET_TIMEOUT, 0));\n      String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n      if (!signerOverride.isEmpty()) {\n        LOG.debug(\"Signer override \u003d {}\", signerOverride);\n        awsConf.setSignerOverride(signerOverride);\n      }\n\n      initProxySupport(conf, awsConf, secureConnections);\n\n      initUserAgent(conf, awsConf);\n\n      initAmazonS3Client(conf, credentials, awsConf);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      if (partSize \u003c 5 * 1024 * 1024) {\n        LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n        partSize \u003d 5 * 1024 * 1024;\n      }\n\n      multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n          DEFAULT_MIN_MULTIPART_THRESHOLD);\n      if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n        LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n        multiPartThreshold \u003d 5 * 1024 * 1024;\n      }\n      //check but do not store the block size\n      longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n      if (totalTasks \u003c 1) {\n        LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n        totalTasks \u003d 1;\n      }\n      long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n      threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n          maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n      inputPolicy \u003d S3AInputPolicy.getPolicy(\n          conf.getTrimmed(INPUT_FADVISE, INPUT_FADV_NORMAL));\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "4aefe119a0203c03cdc893dcb3330fd37f26f0ee": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-3733. \"s3x:\" URLs break when Secret Key contains a slash, even if encoded. Contributed by Steve Loughran.\n",
      "commitDate": "16/06/16 11:13 AM",
      "commitName": "4aefe119a0203c03cdc893dcb3330fd37f26f0ee",
      "commitAuthor": "Ravi Prakash",
      "commitDateOld": "09/06/16 9:28 AM",
      "commitNameOld": "656c460c0e79ee144d6ef48d85cec04a1af3b2cc",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 7.07,
      "commitsBetweenForRepo": 40,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,97 +1,97 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n-      uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n+      uri \u003d S3xLoginHelper.buildFSURI(name);\n       workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       bucket \u003d name.getHost();\n \n       AWSCredentialsProvider credentials \u003d\n           getAWSCredentialsProvider(name, conf);\n \n       ClientConfiguration awsConf \u003d new ClientConfiguration();\n       awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n           DEFAULT_MAXIMUM_CONNECTIONS, 1));\n       boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n           DEFAULT_SECURE_CONNECTIONS);\n       awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n       awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n           DEFAULT_MAX_ERROR_RETRIES, 0));\n       awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n           DEFAULT_ESTABLISH_TIMEOUT, 0));\n       awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n           DEFAULT_SOCKET_TIMEOUT, 0));\n       String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n       if (!signerOverride.isEmpty()) {\n         LOG.debug(\"Signer override \u003d {}\", signerOverride);\n         awsConf.setSignerOverride(signerOverride);\n       }\n \n       initProxySupport(conf, awsConf, secureConnections);\n \n       initUserAgent(conf, awsConf);\n \n       initAmazonS3Client(conf, credentials, awsConf);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       if (partSize \u003c 5 * 1024 * 1024) {\n         LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n         partSize \u003d 5 * 1024 * 1024;\n       }\n \n       multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n           DEFAULT_MIN_MULTIPART_THRESHOLD);\n       if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n         LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n         multiPartThreshold \u003d 5 * 1024 * 1024;\n       }\n       //check but do not store the block size\n       longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n       storageStatistics \u003d (S3AStorageStatistics)\n           GlobalStorageStatistics.INSTANCE\n               .put(S3AStorageStatistics.NAME,\n                   new GlobalStorageStatistics.StorageStatisticsProvider() {\n                     @Override\n                     public StorageStatistics provide() {\n                       return new S3AStorageStatistics();\n                     }\n                   });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n       if (totalTasks \u003c 1) {\n         LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n         totalTasks \u003d 1;\n       }\n       long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n       threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n           maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d\n           conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      uri \u003d S3xLoginHelper.buildFSURI(name);\n      workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      bucket \u003d name.getHost();\n\n      AWSCredentialsProvider credentials \u003d\n          getAWSCredentialsProvider(name, conf);\n\n      ClientConfiguration awsConf \u003d new ClientConfiguration();\n      awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n          DEFAULT_MAXIMUM_CONNECTIONS, 1));\n      boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n          DEFAULT_SECURE_CONNECTIONS);\n      awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n      awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n          DEFAULT_MAX_ERROR_RETRIES, 0));\n      awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n          DEFAULT_ESTABLISH_TIMEOUT, 0));\n      awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n          DEFAULT_SOCKET_TIMEOUT, 0));\n      String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n      if (!signerOverride.isEmpty()) {\n        LOG.debug(\"Signer override \u003d {}\", signerOverride);\n        awsConf.setSignerOverride(signerOverride);\n      }\n\n      initProxySupport(conf, awsConf, secureConnections);\n\n      initUserAgent(conf, awsConf);\n\n      initAmazonS3Client(conf, credentials, awsConf);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      if (partSize \u003c 5 * 1024 * 1024) {\n        LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n        partSize \u003d 5 * 1024 * 1024;\n      }\n\n      multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n          DEFAULT_MIN_MULTIPART_THRESHOLD);\n      if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n        LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n        multiPartThreshold \u003d 5 * 1024 * 1024;\n      }\n      //check but do not store the block size\n      longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n      if (totalTasks \u003c 1) {\n        LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n        totalTasks \u003d 1;\n      }\n      long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n      threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n          maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "c58a59f7081d55dd2108545ebf9ee48cf43ca944": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13171. Add StorageStatistics to S3A; instrument some more operations. Contributed by Steve Loughran.\n",
      "commitDate": "03/06/16 8:55 AM",
      "commitName": "c58a59f7081d55dd2108545ebf9ee48cf43ca944",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "01/06/16 2:49 PM",
      "commitNameOld": "16b1cc7af9bd63b65ef50e1056f275a7baf111a2",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 1.75,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,88 +1,97 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n     setConf(conf);\n     try {\n       instrumentation \u003d new S3AInstrumentation(name);\n \n       uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n       workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n           .makeQualified(this.uri, this.getWorkingDirectory());\n \n       bucket \u003d name.getHost();\n \n       AWSCredentialsProvider credentials \u003d\n           getAWSCredentialsProvider(name, conf);\n \n       ClientConfiguration awsConf \u003d new ClientConfiguration();\n       awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n           DEFAULT_MAXIMUM_CONNECTIONS, 1));\n       boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n           DEFAULT_SECURE_CONNECTIONS);\n       awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n       awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n           DEFAULT_MAX_ERROR_RETRIES, 0));\n       awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n           DEFAULT_ESTABLISH_TIMEOUT, 0));\n       awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n           DEFAULT_SOCKET_TIMEOUT, 0));\n       String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n       if (!signerOverride.isEmpty()) {\n         LOG.debug(\"Signer override \u003d {}\", signerOverride);\n         awsConf.setSignerOverride(signerOverride);\n       }\n \n       initProxySupport(conf, awsConf, secureConnections);\n \n       initUserAgent(conf, awsConf);\n \n       initAmazonS3Client(conf, credentials, awsConf);\n \n       maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n       partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n       if (partSize \u003c 5 * 1024 * 1024) {\n         LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n         partSize \u003d 5 * 1024 * 1024;\n       }\n \n       multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n           DEFAULT_MIN_MULTIPART_THRESHOLD);\n       if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n         LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n         multiPartThreshold \u003d 5 * 1024 * 1024;\n       }\n       //check but do not store the block size\n       longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n       enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n       readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n+      storageStatistics \u003d (S3AStorageStatistics)\n+          GlobalStorageStatistics.INSTANCE\n+              .put(S3AStorageStatistics.NAME,\n+                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n+                    @Override\n+                    public StorageStatistics provide() {\n+                      return new S3AStorageStatistics();\n+                    }\n+                  });\n \n       int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n       if (maxThreads \u003c 2) {\n         LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n         maxThreads \u003d 2;\n       }\n       int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n       if (totalTasks \u003c 1) {\n         LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n         totalTasks \u003d 1;\n       }\n       long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n       threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n           maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n           \"s3a-transfer-shared\");\n \n       initTransferManager();\n \n       initCannedAcls(conf);\n \n       verifyBucketExists();\n \n       initMultipartUploads(conf);\n \n       serverSideEncryptionAlgorithm \u003d\n           conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n     } catch (AmazonClientException e) {\n       throw translateException(\"initializing \", new Path(name), e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n      workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      bucket \u003d name.getHost();\n\n      AWSCredentialsProvider credentials \u003d\n          getAWSCredentialsProvider(name, conf);\n\n      ClientConfiguration awsConf \u003d new ClientConfiguration();\n      awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n          DEFAULT_MAXIMUM_CONNECTIONS, 1));\n      boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n          DEFAULT_SECURE_CONNECTIONS);\n      awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n      awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n          DEFAULT_MAX_ERROR_RETRIES, 0));\n      awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n          DEFAULT_ESTABLISH_TIMEOUT, 0));\n      awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n          DEFAULT_SOCKET_TIMEOUT, 0));\n      String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n      if (!signerOverride.isEmpty()) {\n        LOG.debug(\"Signer override \u003d {}\", signerOverride);\n        awsConf.setSignerOverride(signerOverride);\n      }\n\n      initProxySupport(conf, awsConf, secureConnections);\n\n      initUserAgent(conf, awsConf);\n\n      initAmazonS3Client(conf, credentials, awsConf);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      if (partSize \u003c 5 * 1024 * 1024) {\n        LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n        partSize \u003d 5 * 1024 * 1024;\n      }\n\n      multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n          DEFAULT_MIN_MULTIPART_THRESHOLD);\n      if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n        LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n        multiPartThreshold \u003d 5 * 1024 * 1024;\n      }\n      //check but do not store the block size\n      longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n      storageStatistics \u003d (S3AStorageStatistics)\n          GlobalStorageStatistics.INSTANCE\n              .put(S3AStorageStatistics.NAME,\n                  new GlobalStorageStatistics.StorageStatisticsProvider() {\n                    @Override\n                    public StorageStatistics provide() {\n                      return new S3AStorageStatistics();\n                    }\n                  });\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n      if (totalTasks \u003c 1) {\n        LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n        totalTasks \u003d 1;\n      }\n      long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n      threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n          maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "39ec1515a205952eda7e171408a8b83eceb4abde": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13130. s3a failures can surface as RTEs, not IOEs. (Steve Loughran)\n",
      "commitDate": "21/05/16 8:39 AM",
      "commitName": "39ec1515a205952eda7e171408a8b83eceb4abde",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "20/05/16 5:52 AM",
      "commitNameOld": "757050ff355d40bc28f9dbfd0c0083c5f337d270",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 1.12,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,88 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n     setConf(conf);\n-    instrumentation \u003d new S3AInstrumentation(name);\n+    try {\n+      instrumentation \u003d new S3AInstrumentation(name);\n \n-    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n-    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n-        .makeQualified(this.uri, this.getWorkingDirectory());\n+      uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n+      workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n+          .makeQualified(this.uri, this.getWorkingDirectory());\n \n-    bucket \u003d name.getHost();\n+      bucket \u003d name.getHost();\n \n-    AWSCredentialsProvider credentials \u003d getAWSCredentialsProvider(name, conf);\n+      AWSCredentialsProvider credentials \u003d\n+          getAWSCredentialsProvider(name, conf);\n \n-    ClientConfiguration awsConf \u003d new ClientConfiguration();\n-    awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n-        DEFAULT_MAXIMUM_CONNECTIONS, 1));\n-    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n-        DEFAULT_SECURE_CONNECTIONS);\n-    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n-    awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n-        DEFAULT_MAX_ERROR_RETRIES, 0));\n-    awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n-        DEFAULT_ESTABLISH_TIMEOUT, 0));\n-    awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n-        DEFAULT_SOCKET_TIMEOUT, 0));\n-    String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n-    if (!signerOverride.isEmpty()) {\n-      LOG.debug(\"Signer override \u003d {}\", signerOverride);\n-      awsConf.setSignerOverride(signerOverride);\n+      ClientConfiguration awsConf \u003d new ClientConfiguration();\n+      awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n+          DEFAULT_MAXIMUM_CONNECTIONS, 1));\n+      boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n+          DEFAULT_SECURE_CONNECTIONS);\n+      awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n+      awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n+          DEFAULT_MAX_ERROR_RETRIES, 0));\n+      awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n+          DEFAULT_ESTABLISH_TIMEOUT, 0));\n+      awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n+          DEFAULT_SOCKET_TIMEOUT, 0));\n+      String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n+      if (!signerOverride.isEmpty()) {\n+        LOG.debug(\"Signer override \u003d {}\", signerOverride);\n+        awsConf.setSignerOverride(signerOverride);\n+      }\n+\n+      initProxySupport(conf, awsConf, secureConnections);\n+\n+      initUserAgent(conf, awsConf);\n+\n+      initAmazonS3Client(conf, credentials, awsConf);\n+\n+      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n+      partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n+      if (partSize \u003c 5 * 1024 * 1024) {\n+        LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n+        partSize \u003d 5 * 1024 * 1024;\n+      }\n+\n+      multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n+          DEFAULT_MIN_MULTIPART_THRESHOLD);\n+      if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n+        LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n+        multiPartThreshold \u003d 5 * 1024 * 1024;\n+      }\n+      //check but do not store the block size\n+      longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n+      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n+\n+      readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n+\n+      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n+      if (maxThreads \u003c 2) {\n+        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n+        maxThreads \u003d 2;\n+      }\n+      int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n+      if (totalTasks \u003c 1) {\n+        LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n+        totalTasks \u003d 1;\n+      }\n+      long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n+      threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n+          maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n+          \"s3a-transfer-shared\");\n+\n+      initTransferManager();\n+\n+      initCannedAcls(conf);\n+\n+      verifyBucketExists();\n+\n+      initMultipartUploads(conf);\n+\n+      serverSideEncryptionAlgorithm \u003d\n+          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n+    } catch (AmazonClientException e) {\n+      throw translateException(\"initializing \", new Path(name), e);\n     }\n \n-    initProxySupport(conf, awsConf, secureConnections);\n-\n-    initUserAgent(conf, awsConf);\n-\n-    initAmazonS3Client(conf, credentials, awsConf);\n-\n-    maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n-    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n-    if (partSize \u003c 5 * 1024 * 1024) {\n-      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n-      partSize \u003d 5 * 1024 * 1024;\n-    }\n-\n-    multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n-        DEFAULT_MIN_MULTIPART_THRESHOLD);\n-    if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n-      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n-      multiPartThreshold \u003d 5 * 1024 * 1024;\n-    }\n-    //check but do not store the block size\n-    longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n-    enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n-    readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n-\n-    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n-    if (maxThreads \u003c 2) {\n-      LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n-      maxThreads \u003d 2;\n-    }\n-    int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n-    if (totalTasks \u003c 1) {\n-      LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n-      totalTasks \u003d 1;\n-    }\n-    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n-    threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n-        maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n-        \"s3a-transfer-shared\");\n-\n-    initTransferManager();\n-\n-    initCannedAcls(conf);\n-\n-    if (!s3.doesBucketExist(bucket)) {\n-      throw new FileNotFoundException(\"Bucket \" + bucket + \" does not exist\");\n-    }\n-\n-    initMultipartUploads(conf);\n-\n-    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n-\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n    setConf(conf);\n    try {\n      instrumentation \u003d new S3AInstrumentation(name);\n\n      uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n      workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n          .makeQualified(this.uri, this.getWorkingDirectory());\n\n      bucket \u003d name.getHost();\n\n      AWSCredentialsProvider credentials \u003d\n          getAWSCredentialsProvider(name, conf);\n\n      ClientConfiguration awsConf \u003d new ClientConfiguration();\n      awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n          DEFAULT_MAXIMUM_CONNECTIONS, 1));\n      boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n          DEFAULT_SECURE_CONNECTIONS);\n      awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n      awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n          DEFAULT_MAX_ERROR_RETRIES, 0));\n      awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n          DEFAULT_ESTABLISH_TIMEOUT, 0));\n      awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n          DEFAULT_SOCKET_TIMEOUT, 0));\n      String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n      if (!signerOverride.isEmpty()) {\n        LOG.debug(\"Signer override \u003d {}\", signerOverride);\n        awsConf.setSignerOverride(signerOverride);\n      }\n\n      initProxySupport(conf, awsConf, secureConnections);\n\n      initUserAgent(conf, awsConf);\n\n      initAmazonS3Client(conf, credentials, awsConf);\n\n      maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n      partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n      if (partSize \u003c 5 * 1024 * 1024) {\n        LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n        partSize \u003d 5 * 1024 * 1024;\n      }\n\n      multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n          DEFAULT_MIN_MULTIPART_THRESHOLD);\n      if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n        LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n        multiPartThreshold \u003d 5 * 1024 * 1024;\n      }\n      //check but do not store the block size\n      longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n      enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n      readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n\n      int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n      if (maxThreads \u003c 2) {\n        LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n        maxThreads \u003d 2;\n      }\n      int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n      if (totalTasks \u003c 1) {\n        LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n        totalTasks \u003d 1;\n      }\n      long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n      threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n          maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n          \"s3a-transfer-shared\");\n\n      initTransferManager();\n\n      initCannedAcls(conf);\n\n      verifyBucketExists();\n\n      initMultipartUploads(conf);\n\n      serverSideEncryptionAlgorithm \u003d\n          conf.getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n    } catch (AmazonClientException e) {\n      throw translateException(\"initializing \", new Path(name), e);\n    }\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "757050ff355d40bc28f9dbfd0c0083c5f337d270": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12723 S3A: Add ability to plug in any AWSCredentialsProvider. Contributed by Steven Wong.\n",
      "commitDate": "20/05/16 5:52 AM",
      "commitName": "757050ff355d40bc28f9dbfd0c0083c5f337d270",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "17/05/16 5:19 AM",
      "commitNameOld": "08ea07f1b8edbc38c99015c81a62ca127a247bf7",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 3.02,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,83 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n     setConf(conf);\n     instrumentation \u003d new S3AInstrumentation(name);\n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n         .makeQualified(this.uri, this.getWorkingDirectory());\n \n-    AWSAccessKeys creds \u003d getAWSAccessKeys(name, conf);\n-\n-    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n-        new BasicAWSCredentialsProvider(\n-            creds.getAccessKey(), creds.getAccessSecret()),\n-        new InstanceProfileCredentialsProvider(),\n-        new AnonymousAWSCredentialsProvider()\n-    );\n-\n     bucket \u003d name.getHost();\n \n+    AWSCredentialsProvider credentials \u003d getAWSCredentialsProvider(name, conf);\n+\n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n         DEFAULT_MAXIMUM_CONNECTIONS, 1));\n     boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n         DEFAULT_SECURE_CONNECTIONS);\n     awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n         DEFAULT_MAX_ERROR_RETRIES, 0));\n     awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n         DEFAULT_ESTABLISH_TIMEOUT, 0));\n     awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n         DEFAULT_SOCKET_TIMEOUT, 0));\n     String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n     if (!signerOverride.isEmpty()) {\n       LOG.debug(\"Signer override \u003d {}\", signerOverride);\n       awsConf.setSignerOverride(signerOverride);\n     }\n \n     initProxySupport(conf, awsConf, secureConnections);\n \n     initUserAgent(conf, awsConf);\n \n     initAmazonS3Client(conf, credentials, awsConf);\n \n     maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n         DEFAULT_MIN_MULTIPART_THRESHOLD);\n     if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       multiPartThreshold \u003d 5 * 1024 * 1024;\n     }\n     //check but do not store the block size\n     longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n     enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n     readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n \n     int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n     if (maxThreads \u003c 2) {\n       LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n       maxThreads \u003d 2;\n     }\n     int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n     if (totalTasks \u003c 1) {\n       LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n       totalTasks \u003d 1;\n     }\n     long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n     threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n         maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n         \"s3a-transfer-shared\");\n \n     initTransferManager();\n \n     initCannedAcls(conf);\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new FileNotFoundException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     initMultipartUploads(conf);\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n    setConf(conf);\n    instrumentation \u003d new S3AInstrumentation(name);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n        .makeQualified(this.uri, this.getWorkingDirectory());\n\n    bucket \u003d name.getHost();\n\n    AWSCredentialsProvider credentials \u003d getAWSCredentialsProvider(name, conf);\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n        DEFAULT_MAXIMUM_CONNECTIONS, 1));\n    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n        DEFAULT_SECURE_CONNECTIONS);\n    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n        DEFAULT_MAX_ERROR_RETRIES, 0));\n    awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n        DEFAULT_ESTABLISH_TIMEOUT, 0));\n    awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n        DEFAULT_SOCKET_TIMEOUT, 0));\n    String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n    if (!signerOverride.isEmpty()) {\n      LOG.debug(\"Signer override \u003d {}\", signerOverride);\n      awsConf.setSignerOverride(signerOverride);\n    }\n\n    initProxySupport(conf, awsConf, secureConnections);\n\n    initUserAgent(conf, awsConf);\n\n    initAmazonS3Client(conf, credentials, awsConf);\n\n    maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n        DEFAULT_MIN_MULTIPART_THRESHOLD);\n    if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      multiPartThreshold \u003d 5 * 1024 * 1024;\n    }\n    //check but do not store the block size\n    longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n    enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n    readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    if (maxThreads \u003c 2) {\n      LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n      maxThreads \u003d 2;\n    }\n    int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n    if (totalTasks \u003c 1) {\n      LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n      totalTasks \u003d 1;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n        maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n        \"s3a-transfer-shared\");\n\n    initTransferManager();\n\n    initCannedAcls(conf);\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new FileNotFoundException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    initMultipartUploads(conf);\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "27c4e90efce04e1b1302f668b5eb22412e00d033": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13028 add low level counter metrics for S3A; use in read performance tests. contributed by: stevel\npatch includes\nHADOOP-12844 Recover when S3A fails on IOException in read()\nHADOOP-13058 S3A FS fails during init against a read-only FS if multipart purge\nHADOOP-13047 S3a Forward seek in stream length to be configurable\n",
      "commitDate": "12/05/16 11:24 AM",
      "commitName": "27c4e90efce04e1b1302f668b5eb22412e00d033",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "12/05/16 5:57 AM",
      "commitNameOld": "def2a6d3856452d5c804f04e5bf485541a3bc53a",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 0.23,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,86 +1,90 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n+    setConf(conf);\n+    instrumentation \u003d new S3AInstrumentation(name);\n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n-    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n-        this.getWorkingDirectory());\n+    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n+        .makeQualified(this.uri, this.getWorkingDirectory());\n \n     AWSAccessKeys creds \u003d getAWSAccessKeys(name, conf);\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n         new BasicAWSCredentialsProvider(\n             creds.getAccessKey(), creds.getAccessSecret()),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n-    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS,\n-      DEFAULT_MAXIMUM_CONNECTIONS));\n+    awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n+        DEFAULT_MAXIMUM_CONNECTIONS, 1));\n     boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n         DEFAULT_SECURE_CONNECTIONS);\n     awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n-    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES,\n-      DEFAULT_MAX_ERROR_RETRIES));\n-    awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n-        DEFAULT_ESTABLISH_TIMEOUT));\n-    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT,\n-      DEFAULT_SOCKET_TIMEOUT));\n+    awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n+        DEFAULT_MAX_ERROR_RETRIES, 0));\n+    awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n+        DEFAULT_ESTABLISH_TIMEOUT, 0));\n+    awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n+        DEFAULT_SOCKET_TIMEOUT, 0));\n     String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n-    if(!signerOverride.isEmpty()) {\n+    if (!signerOverride.isEmpty()) {\n+      LOG.debug(\"Signer override \u003d {}\", signerOverride);\n       awsConf.setSignerOverride(signerOverride);\n     }\n \n     initProxySupport(conf, awsConf, secureConnections);\n \n     initUserAgent(conf, awsConf);\n \n     initAmazonS3Client(conf, credentials, awsConf);\n \n-    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n+    maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n-    multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n-      DEFAULT_MIN_MULTIPART_THRESHOLD);\n-    enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n-\n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n+    multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n+        DEFAULT_MIN_MULTIPART_THRESHOLD);\n     if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       multiPartThreshold \u003d 5 * 1024 * 1024;\n     }\n+    //check but do not store the block size\n+    longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n+    enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n+    readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n \n     int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n     if (maxThreads \u003c 2) {\n       LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n       maxThreads \u003d 2;\n     }\n     int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n     if (totalTasks \u003c 1) {\n       LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n       totalTasks \u003d 1;\n     }\n     long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n     threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n         maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n         \"s3a-transfer-shared\");\n \n     initTransferManager();\n \n     initCannedAcls(conf);\n \n     if (!s3.doesBucketExist(bucket)) {\n-      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n+      throw new FileNotFoundException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     initMultipartUploads(conf);\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n-    setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n    setConf(conf);\n    instrumentation \u003d new S3AInstrumentation(name);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\"))\n        .makeQualified(this.uri, this.getWorkingDirectory());\n\n    AWSAccessKeys creds \u003d getAWSAccessKeys(name, conf);\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(\n            creds.getAccessKey(), creds.getAccessSecret()),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(intOption(conf, MAXIMUM_CONNECTIONS,\n        DEFAULT_MAXIMUM_CONNECTIONS, 1));\n    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n        DEFAULT_SECURE_CONNECTIONS);\n    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(intOption(conf, MAX_ERROR_RETRIES,\n        DEFAULT_MAX_ERROR_RETRIES, 0));\n    awsConf.setConnectionTimeout(intOption(conf, ESTABLISH_TIMEOUT,\n        DEFAULT_ESTABLISH_TIMEOUT, 0));\n    awsConf.setSocketTimeout(intOption(conf, SOCKET_TIMEOUT,\n        DEFAULT_SOCKET_TIMEOUT, 0));\n    String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n    if (!signerOverride.isEmpty()) {\n      LOG.debug(\"Signer override \u003d {}\", signerOverride);\n      awsConf.setSignerOverride(signerOverride);\n    }\n\n    initProxySupport(conf, awsConf, secureConnections);\n\n    initUserAgent(conf, awsConf);\n\n    initAmazonS3Client(conf, credentials, awsConf);\n\n    maxKeys \u003d intOption(conf, MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS, 1);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n        DEFAULT_MIN_MULTIPART_THRESHOLD);\n    if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      multiPartThreshold \u003d 5 * 1024 * 1024;\n    }\n    //check but do not store the block size\n    longOption(conf, FS_S3A_BLOCK_SIZE, DEFAULT_BLOCKSIZE, 1);\n    enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n    readAhead \u003d longOption(conf, READAHEAD_RANGE, DEFAULT_READAHEAD_RANGE, 0);\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    if (maxThreads \u003c 2) {\n      LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n      maxThreads \u003d 2;\n    }\n    int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n    if (totalTasks \u003c 1) {\n      LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n      totalTasks \u003d 1;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n        maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n        \"s3a-transfer-shared\");\n\n    initTransferManager();\n\n    initCannedAcls(conf);\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new FileNotFoundException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    initMultipartUploads(conf);\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "def2a6d3856452d5c804f04e5bf485541a3bc53a": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13122 Customize User-Agent header sent in HTTP requests by S3A. Chris Nauroth via stevel.\n",
      "commitDate": "12/05/16 5:57 AM",
      "commitName": "def2a6d3856452d5c804f04e5bf485541a3bc53a",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "22/04/16 3:25 AM",
      "commitNameOld": "19f0f9608e31203523943f008ac701b6f3d7973c",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 20.11,
      "commitsBetweenForRepo": 115,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,86 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n     AWSAccessKeys creds \u003d getAWSAccessKeys(name, conf);\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n         new BasicAWSCredentialsProvider(\n             creds.getAccessKey(), creds.getAccessSecret()),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS,\n       DEFAULT_MAXIMUM_CONNECTIONS));\n     boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n         DEFAULT_SECURE_CONNECTIONS);\n     awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES,\n       DEFAULT_MAX_ERROR_RETRIES));\n     awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n         DEFAULT_ESTABLISH_TIMEOUT));\n     awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT,\n       DEFAULT_SOCKET_TIMEOUT));\n     String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n     if(!signerOverride.isEmpty()) {\n       awsConf.setSignerOverride(signerOverride);\n     }\n \n     initProxySupport(conf, awsConf, secureConnections);\n \n+    initUserAgent(conf, awsConf);\n+\n     initAmazonS3Client(conf, credentials, awsConf);\n \n     maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n     multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n       DEFAULT_MIN_MULTIPART_THRESHOLD);\n     enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       multiPartThreshold \u003d 5 * 1024 * 1024;\n     }\n \n     int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n     if (maxThreads \u003c 2) {\n       LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n       maxThreads \u003d 2;\n     }\n     int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n     if (totalTasks \u003c 1) {\n       LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n       totalTasks \u003d 1;\n     }\n     long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n     threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n         maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n         \"s3a-transfer-shared\");\n \n     initTransferManager();\n \n     initCannedAcls(conf);\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     initMultipartUploads(conf);\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    AWSAccessKeys creds \u003d getAWSAccessKeys(name, conf);\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(\n            creds.getAccessKey(), creds.getAccessSecret()),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS,\n      DEFAULT_MAXIMUM_CONNECTIONS));\n    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n        DEFAULT_SECURE_CONNECTIONS);\n    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES,\n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n        DEFAULT_ESTABLISH_TIMEOUT));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT,\n      DEFAULT_SOCKET_TIMEOUT));\n    String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n    if(!signerOverride.isEmpty()) {\n      awsConf.setSignerOverride(signerOverride);\n    }\n\n    initProxySupport(conf, awsConf, secureConnections);\n\n    initUserAgent(conf, awsConf);\n\n    initAmazonS3Client(conf, credentials, awsConf);\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n    enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      multiPartThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    if (maxThreads \u003c 2) {\n      LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n      maxThreads \u003d 2;\n    }\n    int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n    if (totalTasks \u003c 1) {\n      LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n      totalTasks \u003d 1;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n        maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n        \"s3a-transfer-shared\");\n\n    initTransferManager();\n\n    initCannedAcls(conf);\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    initMultipartUploads(conf);\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "76fab26c5c02cef38924d04136407489fd9457d9": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12548. Read s3a creds from a Credential Provider. Contributed by Larry McCay.\n",
      "commitDate": "17/02/16 12:19 PM",
      "commitName": "76fab26c5c02cef38924d04136407489fd9457d9",
      "commitAuthor": "cnauroth",
      "commitDateOld": "06/02/16 7:05 AM",
      "commitNameOld": "29ae25801380b94442253c4202dee782dc4713f5",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 11.22,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,163 +1,84 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n-    // Try to get our credentials or just connect anonymously\n-    String accessKey \u003d conf.get(ACCESS_KEY, null);\n-    String secretKey \u003d conf.get(SECRET_KEY, null);\n-\n-    String userInfo \u003d name.getUserInfo();\n-    if (userInfo !\u003d null) {\n-      int index \u003d userInfo.indexOf(\u0027:\u0027);\n-      if (index !\u003d -1) {\n-        accessKey \u003d userInfo.substring(0, index);\n-        secretKey \u003d userInfo.substring(index + 1);\n-      } else {\n-        accessKey \u003d userInfo;\n-      }\n-    }\n+    AWSAccessKeys creds \u003d getAWSAccessKeys(name, conf);\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n-        new BasicAWSCredentialsProvider(accessKey, secretKey),\n+        new BasicAWSCredentialsProvider(\n+            creds.getAccessKey(), creds.getAccessSecret()),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n       DEFAULT_MAXIMUM_CONNECTIONS));\n     boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n         DEFAULT_SECURE_CONNECTIONS);\n     awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n       DEFAULT_MAX_ERROR_RETRIES));\n     awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n         DEFAULT_ESTABLISH_TIMEOUT));\n     awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n       DEFAULT_SOCKET_TIMEOUT));\n     String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n     if(!signerOverride.isEmpty()) {\n       awsConf.setSignerOverride(signerOverride);\n     }\n \n-    String proxyHost \u003d conf.getTrimmed(PROXY_HOST, \"\");\n-    int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n-    if (!proxyHost.isEmpty()) {\n-      awsConf.setProxyHost(proxyHost);\n-      if (proxyPort \u003e\u003d 0) {\n-        awsConf.setProxyPort(proxyPort);\n-      } else {\n-        if (secureConnections) {\n-          LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n-          awsConf.setProxyPort(443);\n-        } else {\n-          LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n-          awsConf.setProxyPort(80);\n-        }\n-      }\n-      String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n-      String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n-      if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n-        String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n-            PROXY_PASSWORD + \" set without the other.\";\n-        LOG.error(msg);\n-        throw new IllegalArgumentException(msg);\n-      }\n-      awsConf.setProxyUsername(proxyUsername);\n-      awsConf.setProxyPassword(proxyPassword);\n-      awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n-      awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n-                \"domain {} as workstation {}\", awsConf.getProxyHost(),\n-            awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n-            awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n-            awsConf.getProxyWorkstation());\n-      }\n-    } else if (proxyPort \u003e\u003d 0) {\n-      String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n-      LOG.error(msg);\n-      throw new IllegalArgumentException(msg);\n-    }\n+    initProxySupport(conf, awsConf, secureConnections);\n \n-    s3 \u003d new AmazonS3Client(credentials, awsConf);\n-    String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n-    if (!endPoint.isEmpty()) {\n-      try {\n-        s3.setEndpoint(endPoint);\n-      } catch (IllegalArgumentException e) {\n-        String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n-        LOG.error(msg);\n-        throw new IllegalArgumentException(msg, e);\n-      }\n-    }\n+    initAmazonS3Client(conf, credentials, awsConf);\n \n     maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n     multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n       DEFAULT_MIN_MULTIPART_THRESHOLD);\n     enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       multiPartThreshold \u003d 5 * 1024 * 1024;\n     }\n \n     int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n     if (maxThreads \u003c 2) {\n       LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n       maxThreads \u003d 2;\n     }\n     int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n     if (totalTasks \u003c 1) {\n       LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n       totalTasks \u003d 1;\n     }\n     long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n     threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n         maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n         \"s3a-transfer-shared\");\n \n-    TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n-    transferConfiguration.setMinimumUploadPartSize(partSize);\n-    transferConfiguration.setMultipartUploadThreshold(multiPartThreshold);\n+    initTransferManager();\n \n-    transfers \u003d new TransferManager(s3, threadPoolExecutor);\n-    transfers.setConfiguration(transferConfiguration);\n-\n-    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n-    if (!cannedACLName.isEmpty()) {\n-      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n-    } else {\n-      cannedACL \u003d null;\n-    }\n+    initCannedAcls(conf);\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n-    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n-      DEFAULT_PURGE_EXISTING_MULTIPART);\n-    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n-      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n-\n-    if (purgeExistingMultipart) {\n-      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n-\n-      transfers.abortMultipartUploads(bucket, purgeBefore);\n-    }\n+    initMultipartUploads(conf);\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    AWSAccessKeys creds \u003d getAWSAccessKeys(name, conf);\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(\n            creds.getAccessKey(), creds.getAccessSecret()),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n      DEFAULT_MAXIMUM_CONNECTIONS));\n    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n        DEFAULT_SECURE_CONNECTIONS);\n    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n        DEFAULT_ESTABLISH_TIMEOUT));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n      DEFAULT_SOCKET_TIMEOUT));\n    String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n    if(!signerOverride.isEmpty()) {\n      awsConf.setSignerOverride(signerOverride);\n    }\n\n    initProxySupport(conf, awsConf, secureConnections);\n\n    initAmazonS3Client(conf, credentials, awsConf);\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n    enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      multiPartThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    if (maxThreads \u003c 2) {\n      LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n      maxThreads \u003d 2;\n    }\n    int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n    if (totalTasks \u003c 1) {\n      LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n      totalTasks \u003d 1;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n        maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n        \"s3a-transfer-shared\");\n\n    initTransferManager();\n\n    initCannedAcls(conf);\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    initMultipartUploads(conf);\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "29ae25801380b94442253c4202dee782dc4713f5": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12292. Make use of DeleteObjects optional.  (Thomas Demoor via stevel)\n",
      "commitDate": "06/02/16 7:05 AM",
      "commitName": "29ae25801380b94442253c4202dee782dc4713f5",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "12/01/16 12:19 PM",
      "commitNameOld": "126705f67eaa6d866f7572fbddf133c5d7552353",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 24.78,
      "commitsBetweenForRepo": 186,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,162 +1,163 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n     // Try to get our credentials or just connect anonymously\n     String accessKey \u003d conf.get(ACCESS_KEY, null);\n     String secretKey \u003d conf.get(SECRET_KEY, null);\n \n     String userInfo \u003d name.getUserInfo();\n     if (userInfo !\u003d null) {\n       int index \u003d userInfo.indexOf(\u0027:\u0027);\n       if (index !\u003d -1) {\n         accessKey \u003d userInfo.substring(0, index);\n         secretKey \u003d userInfo.substring(index + 1);\n       } else {\n         accessKey \u003d userInfo;\n       }\n     }\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n         new BasicAWSCredentialsProvider(accessKey, secretKey),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n       DEFAULT_MAXIMUM_CONNECTIONS));\n     boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n         DEFAULT_SECURE_CONNECTIONS);\n     awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n       DEFAULT_MAX_ERROR_RETRIES));\n     awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n         DEFAULT_ESTABLISH_TIMEOUT));\n     awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n       DEFAULT_SOCKET_TIMEOUT));\n     String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n     if(!signerOverride.isEmpty()) {\n       awsConf.setSignerOverride(signerOverride);\n     }\n \n     String proxyHost \u003d conf.getTrimmed(PROXY_HOST, \"\");\n     int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n     if (!proxyHost.isEmpty()) {\n       awsConf.setProxyHost(proxyHost);\n       if (proxyPort \u003e\u003d 0) {\n         awsConf.setProxyPort(proxyPort);\n       } else {\n         if (secureConnections) {\n           LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n           awsConf.setProxyPort(443);\n         } else {\n           LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n           awsConf.setProxyPort(80);\n         }\n       }\n       String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n       String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n       if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n         String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n             PROXY_PASSWORD + \" set without the other.\";\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg);\n       }\n       awsConf.setProxyUsername(proxyUsername);\n       awsConf.setProxyPassword(proxyPassword);\n       awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n       awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                 \"domain {} as workstation {}\", awsConf.getProxyHost(),\n             awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n             awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n             awsConf.getProxyWorkstation());\n       }\n     } else if (proxyPort \u003e\u003d 0) {\n       String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n       LOG.error(msg);\n       throw new IllegalArgumentException(msg);\n     }\n \n     s3 \u003d new AmazonS3Client(credentials, awsConf);\n     String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n     if (!endPoint.isEmpty()) {\n       try {\n         s3.setEndpoint(endPoint);\n       } catch (IllegalArgumentException e) {\n         String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg, e);\n       }\n     }\n \n     maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n     multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n       DEFAULT_MIN_MULTIPART_THRESHOLD);\n+    enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       multiPartThreshold \u003d 5 * 1024 * 1024;\n     }\n \n     int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n     if (maxThreads \u003c 2) {\n       LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n       maxThreads \u003d 2;\n     }\n     int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n     if (totalTasks \u003c 1) {\n       LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n       totalTasks \u003d 1;\n     }\n     long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n     threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n         maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n         \"s3a-transfer-shared\");\n \n     TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n     transferConfiguration.setMinimumUploadPartSize(partSize);\n     transferConfiguration.setMultipartUploadThreshold(multiPartThreshold);\n \n     transfers \u003d new TransferManager(s3, threadPoolExecutor);\n     transfers.setConfiguration(transferConfiguration);\n \n     String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n     if (!cannedACLName.isEmpty()) {\n       cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n     } else {\n       cannedACL \u003d null;\n     }\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n       DEFAULT_PURGE_EXISTING_MULTIPART);\n     long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n       DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n \n     if (purgeExistingMultipart) {\n       Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n \n       transfers.abortMultipartUploads(bucket, purgeBefore);\n     }\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    // Try to get our credentials or just connect anonymously\n    String accessKey \u003d conf.get(ACCESS_KEY, null);\n    String secretKey \u003d conf.get(SECRET_KEY, null);\n\n    String userInfo \u003d name.getUserInfo();\n    if (userInfo !\u003d null) {\n      int index \u003d userInfo.indexOf(\u0027:\u0027);\n      if (index !\u003d -1) {\n        accessKey \u003d userInfo.substring(0, index);\n        secretKey \u003d userInfo.substring(index + 1);\n      } else {\n        accessKey \u003d userInfo;\n      }\n    }\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(accessKey, secretKey),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n      DEFAULT_MAXIMUM_CONNECTIONS));\n    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n        DEFAULT_SECURE_CONNECTIONS);\n    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n        DEFAULT_ESTABLISH_TIMEOUT));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n      DEFAULT_SOCKET_TIMEOUT));\n    String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n    if(!signerOverride.isEmpty()) {\n      awsConf.setSignerOverride(signerOverride);\n    }\n\n    String proxyHost \u003d conf.getTrimmed(PROXY_HOST, \"\");\n    int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n    if (!proxyHost.isEmpty()) {\n      awsConf.setProxyHost(proxyHost);\n      if (proxyPort \u003e\u003d 0) {\n        awsConf.setProxyPort(proxyPort);\n      } else {\n        if (secureConnections) {\n          LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n          awsConf.setProxyPort(443);\n        } else {\n          LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n          awsConf.setProxyPort(80);\n        }\n      }\n      String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n      String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n      if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n        String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n            PROXY_PASSWORD + \" set without the other.\";\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg);\n      }\n      awsConf.setProxyUsername(proxyUsername);\n      awsConf.setProxyPassword(proxyPassword);\n      awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n      awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                \"domain {} as workstation {}\", awsConf.getProxyHost(),\n            awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n            awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n            awsConf.getProxyWorkstation());\n      }\n    } else if (proxyPort \u003e\u003d 0) {\n      String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n      LOG.error(msg);\n      throw new IllegalArgumentException(msg);\n    }\n\n    s3 \u003d new AmazonS3Client(credentials, awsConf);\n    String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n    if (!endPoint.isEmpty()) {\n      try {\n        s3.setEndpoint(endPoint);\n      } catch (IllegalArgumentException e) {\n        String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg, e);\n      }\n    }\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n    enableMultiObjectsDelete \u003d conf.getBoolean(ENABLE_MULTI_DELETE, true);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      multiPartThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    if (maxThreads \u003c 2) {\n      LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n      maxThreads \u003d 2;\n    }\n    int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n    if (totalTasks \u003c 1) {\n      LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n      totalTasks \u003d 1;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n        maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n        \"s3a-transfer-shared\");\n\n    TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n    transferConfiguration.setMinimumUploadPartSize(partSize);\n    transferConfiguration.setMultipartUploadThreshold(multiPartThreshold);\n\n    transfers \u003d new TransferManager(s3, threadPoolExecutor);\n    transfers.setConfiguration(transferConfiguration);\n\n    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n    if (!cannedACLName.isEmpty()) {\n      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL \u003d null;\n    }\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n      DEFAULT_PURGE_EXISTING_MULTIPART);\n    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n\n    if (purgeExistingMultipart) {\n      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n\n      transfers.abortMultipartUploads(bucket, purgeBefore);\n    }\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "bff7c90a5686de106ca7a866982412c5dfa01632": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11684. S3a to use thread pool that blocks clients. (Thomas Demoor and Aaron Fabbri via lei)\n",
      "commitDate": "05/11/15 6:35 PM",
      "commitName": "bff7c90a5686de106ca7a866982412c5dfa01632",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "25/09/15 10:33 PM",
      "commitNameOld": "7fe521b1dd49f81ae325f78cf531cfff15be6641",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 40.88,
      "commitsBetweenForRepo": 343,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,168 +1,162 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n     // Try to get our credentials or just connect anonymously\n     String accessKey \u003d conf.get(ACCESS_KEY, null);\n     String secretKey \u003d conf.get(SECRET_KEY, null);\n \n     String userInfo \u003d name.getUserInfo();\n     if (userInfo !\u003d null) {\n       int index \u003d userInfo.indexOf(\u0027:\u0027);\n       if (index !\u003d -1) {\n         accessKey \u003d userInfo.substring(0, index);\n         secretKey \u003d userInfo.substring(index + 1);\n       } else {\n         accessKey \u003d userInfo;\n       }\n     }\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n         new BasicAWSCredentialsProvider(accessKey, secretKey),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n       DEFAULT_MAXIMUM_CONNECTIONS));\n     boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n         DEFAULT_SECURE_CONNECTIONS);\n     awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n       DEFAULT_MAX_ERROR_RETRIES));\n     awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n         DEFAULT_ESTABLISH_TIMEOUT));\n     awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n       DEFAULT_SOCKET_TIMEOUT));\n     String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n     if(!signerOverride.isEmpty()) {\n       awsConf.setSignerOverride(signerOverride);\n     }\n \n     String proxyHost \u003d conf.getTrimmed(PROXY_HOST, \"\");\n     int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n     if (!proxyHost.isEmpty()) {\n       awsConf.setProxyHost(proxyHost);\n       if (proxyPort \u003e\u003d 0) {\n         awsConf.setProxyPort(proxyPort);\n       } else {\n         if (secureConnections) {\n           LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n           awsConf.setProxyPort(443);\n         } else {\n           LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n           awsConf.setProxyPort(80);\n         }\n       }\n       String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n       String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n       if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n         String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n             PROXY_PASSWORD + \" set without the other.\";\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg);\n       }\n       awsConf.setProxyUsername(proxyUsername);\n       awsConf.setProxyPassword(proxyPassword);\n       awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n       awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                 \"domain {} as workstation {}\", awsConf.getProxyHost(),\n             awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n             awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n             awsConf.getProxyWorkstation());\n       }\n     } else if (proxyPort \u003e\u003d 0) {\n       String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n       LOG.error(msg);\n       throw new IllegalArgumentException(msg);\n     }\n \n     s3 \u003d new AmazonS3Client(credentials, awsConf);\n     String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n     if (!endPoint.isEmpty()) {\n       try {\n         s3.setEndpoint(endPoint);\n       } catch (IllegalArgumentException e) {\n         String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg, e);\n       }\n     }\n \n     maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n     multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n       DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       multiPartThreshold \u003d 5 * 1024 * 1024;\n     }\n \n     int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n-    int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n-    if (maxThreads \u003d\u003d 0) {\n-      maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n+    if (maxThreads \u003c 2) {\n+      LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n+      maxThreads \u003d 2;\n     }\n-    if (coreThreads \u003d\u003d 0) {\n-      coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n+    int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n+    if (totalTasks \u003c 1) {\n+      LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n+      totalTasks \u003d 1;\n     }\n     long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n-    LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n-      new LinkedBlockingQueue\u003c\u003e(maxThreads *\n-        conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n-    threadPoolExecutor \u003d new ThreadPoolExecutor(\n-        coreThreads,\n-        maxThreads,\n-        keepAliveTime,\n-        TimeUnit.SECONDS,\n-        workQueue,\n-        newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n-    threadPoolExecutor.allowCoreThreadTimeOut(true);\n+    threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n+        maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n+        \"s3a-transfer-shared\");\n \n     TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n     transferConfiguration.setMinimumUploadPartSize(partSize);\n     transferConfiguration.setMultipartUploadThreshold(multiPartThreshold);\n \n     transfers \u003d new TransferManager(s3, threadPoolExecutor);\n     transfers.setConfiguration(transferConfiguration);\n \n     String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n     if (!cannedACLName.isEmpty()) {\n       cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n     } else {\n       cannedACL \u003d null;\n     }\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n       DEFAULT_PURGE_EXISTING_MULTIPART);\n     long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n       DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n \n     if (purgeExistingMultipart) {\n       Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n \n       transfers.abortMultipartUploads(bucket, purgeBefore);\n     }\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    // Try to get our credentials or just connect anonymously\n    String accessKey \u003d conf.get(ACCESS_KEY, null);\n    String secretKey \u003d conf.get(SECRET_KEY, null);\n\n    String userInfo \u003d name.getUserInfo();\n    if (userInfo !\u003d null) {\n      int index \u003d userInfo.indexOf(\u0027:\u0027);\n      if (index !\u003d -1) {\n        accessKey \u003d userInfo.substring(0, index);\n        secretKey \u003d userInfo.substring(index + 1);\n      } else {\n        accessKey \u003d userInfo;\n      }\n    }\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(accessKey, secretKey),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n      DEFAULT_MAXIMUM_CONNECTIONS));\n    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n        DEFAULT_SECURE_CONNECTIONS);\n    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n        DEFAULT_ESTABLISH_TIMEOUT));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n      DEFAULT_SOCKET_TIMEOUT));\n    String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n    if(!signerOverride.isEmpty()) {\n      awsConf.setSignerOverride(signerOverride);\n    }\n\n    String proxyHost \u003d conf.getTrimmed(PROXY_HOST, \"\");\n    int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n    if (!proxyHost.isEmpty()) {\n      awsConf.setProxyHost(proxyHost);\n      if (proxyPort \u003e\u003d 0) {\n        awsConf.setProxyPort(proxyPort);\n      } else {\n        if (secureConnections) {\n          LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n          awsConf.setProxyPort(443);\n        } else {\n          LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n          awsConf.setProxyPort(80);\n        }\n      }\n      String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n      String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n      if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n        String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n            PROXY_PASSWORD + \" set without the other.\";\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg);\n      }\n      awsConf.setProxyUsername(proxyUsername);\n      awsConf.setProxyPassword(proxyPassword);\n      awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n      awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                \"domain {} as workstation {}\", awsConf.getProxyHost(),\n            awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n            awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n            awsConf.getProxyWorkstation());\n      }\n    } else if (proxyPort \u003e\u003d 0) {\n      String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n      LOG.error(msg);\n      throw new IllegalArgumentException(msg);\n    }\n\n    s3 \u003d new AmazonS3Client(credentials, awsConf);\n    String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n    if (!endPoint.isEmpty()) {\n      try {\n        s3.setEndpoint(endPoint);\n      } catch (IllegalArgumentException e) {\n        String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg, e);\n      }\n    }\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      multiPartThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    if (maxThreads \u003c 2) {\n      LOG.warn(MAX_THREADS + \" must be at least 2: forcing to 2.\");\n      maxThreads \u003d 2;\n    }\n    int totalTasks \u003d conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS);\n    if (totalTasks \u003c 1) {\n      LOG.warn(MAX_TOTAL_TASKS + \"must be at least 1: forcing to 1.\");\n      totalTasks \u003d 1;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    threadPoolExecutor \u003d new BlockingThreadPoolExecutorService(maxThreads,\n        maxThreads + totalTasks, keepAliveTime, TimeUnit.SECONDS,\n        \"s3a-transfer-shared\");\n\n    TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n    transferConfiguration.setMinimumUploadPartSize(partSize);\n    transferConfiguration.setMultipartUploadThreshold(multiPartThreshold);\n\n    transfers \u003d new TransferManager(s3, threadPoolExecutor);\n    transfers.setConfiguration(transferConfiguration);\n\n    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n    if (!cannedACLName.isEmpty()) {\n      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL \u003d null;\n    }\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n      DEFAULT_PURGE_EXISTING_MULTIPART);\n    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n\n    if (purgeExistingMultipart) {\n      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n\n      transfers.abortMultipartUploads(bucket, purgeBefore);\n    }\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "d5403747b57b1e294e533ce17f197e7be8f5339c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12269. Update aws-sdk dependency to 1.10.6 (Thomas Demoor via Lei (Eddy) Xu)\n",
      "commitDate": "04/08/15 6:51 PM",
      "commitName": "d5403747b57b1e294e533ce17f197e7be8f5339c",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "08/03/15 11:22 AM",
      "commitNameOld": "64443490d7f189e8e42d284615f3814ef751395a",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 149.31,
      "commitsBetweenForRepo": 1216,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,164 +1,168 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n     // Try to get our credentials or just connect anonymously\n     String accessKey \u003d conf.get(ACCESS_KEY, null);\n     String secretKey \u003d conf.get(SECRET_KEY, null);\n \n     String userInfo \u003d name.getUserInfo();\n     if (userInfo !\u003d null) {\n       int index \u003d userInfo.indexOf(\u0027:\u0027);\n       if (index !\u003d -1) {\n         accessKey \u003d userInfo.substring(0, index);\n         secretKey \u003d userInfo.substring(index + 1);\n       } else {\n         accessKey \u003d userInfo;\n       }\n     }\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n         new BasicAWSCredentialsProvider(accessKey, secretKey),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n       DEFAULT_MAXIMUM_CONNECTIONS));\n     boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n         DEFAULT_SECURE_CONNECTIONS);\n     awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n       DEFAULT_MAX_ERROR_RETRIES));\n     awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n         DEFAULT_ESTABLISH_TIMEOUT));\n     awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n       DEFAULT_SOCKET_TIMEOUT));\n+    String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n+    if(!signerOverride.isEmpty()) {\n+      awsConf.setSignerOverride(signerOverride);\n+    }\n \n-    String proxyHost \u003d conf.getTrimmed(PROXY_HOST,\"\");\n+    String proxyHost \u003d conf.getTrimmed(PROXY_HOST, \"\");\n     int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n     if (!proxyHost.isEmpty()) {\n       awsConf.setProxyHost(proxyHost);\n       if (proxyPort \u003e\u003d 0) {\n         awsConf.setProxyPort(proxyPort);\n       } else {\n         if (secureConnections) {\n           LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n           awsConf.setProxyPort(443);\n         } else {\n           LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n           awsConf.setProxyPort(80);\n         }\n       }\n       String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n       String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n       if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n         String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n             PROXY_PASSWORD + \" set without the other.\";\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg);\n       }\n       awsConf.setProxyUsername(proxyUsername);\n       awsConf.setProxyPassword(proxyPassword);\n       awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n       awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                 \"domain {} as workstation {}\", awsConf.getProxyHost(),\n             awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n             awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n             awsConf.getProxyWorkstation());\n       }\n     } else if (proxyPort \u003e\u003d 0) {\n       String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n       LOG.error(msg);\n       throw new IllegalArgumentException(msg);\n     }\n \n     s3 \u003d new AmazonS3Client(credentials, awsConf);\n     String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n     if (!endPoint.isEmpty()) {\n       try {\n         s3.setEndpoint(endPoint);\n       } catch (IllegalArgumentException e) {\n         String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg, e);\n       }\n     }\n \n     maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n-    multiPartThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD,\n+    multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n       DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       multiPartThreshold \u003d 5 * 1024 * 1024;\n     }\n \n     int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n     int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n     if (maxThreads \u003d\u003d 0) {\n       maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     if (coreThreads \u003d\u003d 0) {\n       coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n     LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n       new LinkedBlockingQueue\u003c\u003e(maxThreads *\n         conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n     threadPoolExecutor \u003d new ThreadPoolExecutor(\n         coreThreads,\n         maxThreads,\n         keepAliveTime,\n         TimeUnit.SECONDS,\n         workQueue,\n         newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n     threadPoolExecutor.allowCoreThreadTimeOut(true);\n \n     TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n     transferConfiguration.setMinimumUploadPartSize(partSize);\n     transferConfiguration.setMultipartUploadThreshold(multiPartThreshold);\n \n     transfers \u003d new TransferManager(s3, threadPoolExecutor);\n     transfers.setConfiguration(transferConfiguration);\n \n     String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n     if (!cannedACLName.isEmpty()) {\n       cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n     } else {\n       cannedACL \u003d null;\n     }\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n       DEFAULT_PURGE_EXISTING_MULTIPART);\n     long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n       DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n \n     if (purgeExistingMultipart) {\n       Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n \n       transfers.abortMultipartUploads(bucket, purgeBefore);\n     }\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    // Try to get our credentials or just connect anonymously\n    String accessKey \u003d conf.get(ACCESS_KEY, null);\n    String secretKey \u003d conf.get(SECRET_KEY, null);\n\n    String userInfo \u003d name.getUserInfo();\n    if (userInfo !\u003d null) {\n      int index \u003d userInfo.indexOf(\u0027:\u0027);\n      if (index !\u003d -1) {\n        accessKey \u003d userInfo.substring(0, index);\n        secretKey \u003d userInfo.substring(index + 1);\n      } else {\n        accessKey \u003d userInfo;\n      }\n    }\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(accessKey, secretKey),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n      DEFAULT_MAXIMUM_CONNECTIONS));\n    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n        DEFAULT_SECURE_CONNECTIONS);\n    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n        DEFAULT_ESTABLISH_TIMEOUT));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n      DEFAULT_SOCKET_TIMEOUT));\n    String signerOverride \u003d conf.getTrimmed(SIGNING_ALGORITHM, \"\");\n    if(!signerOverride.isEmpty()) {\n      awsConf.setSignerOverride(signerOverride);\n    }\n\n    String proxyHost \u003d conf.getTrimmed(PROXY_HOST, \"\");\n    int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n    if (!proxyHost.isEmpty()) {\n      awsConf.setProxyHost(proxyHost);\n      if (proxyPort \u003e\u003d 0) {\n        awsConf.setProxyPort(proxyPort);\n      } else {\n        if (secureConnections) {\n          LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n          awsConf.setProxyPort(443);\n        } else {\n          LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n          awsConf.setProxyPort(80);\n        }\n      }\n      String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n      String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n      if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n        String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n            PROXY_PASSWORD + \" set without the other.\";\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg);\n      }\n      awsConf.setProxyUsername(proxyUsername);\n      awsConf.setProxyPassword(proxyPassword);\n      awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n      awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                \"domain {} as workstation {}\", awsConf.getProxyHost(),\n            awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n            awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n            awsConf.getProxyWorkstation());\n      }\n    } else if (proxyPort \u003e\u003d 0) {\n      String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n      LOG.error(msg);\n      throw new IllegalArgumentException(msg);\n    }\n\n    s3 \u003d new AmazonS3Client(credentials, awsConf);\n    String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n    if (!endPoint.isEmpty()) {\n      try {\n        s3.setEndpoint(endPoint);\n      } catch (IllegalArgumentException e) {\n        String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg, e);\n      }\n    }\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    multiPartThreshold \u003d conf.getLong(MIN_MULTIPART_THRESHOLD,\n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      multiPartThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n    if (maxThreads \u003d\u003d 0) {\n      maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    if (coreThreads \u003d\u003d 0) {\n      coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n      new LinkedBlockingQueue\u003c\u003e(maxThreads *\n        conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n    threadPoolExecutor \u003d new ThreadPoolExecutor(\n        coreThreads,\n        maxThreads,\n        keepAliveTime,\n        TimeUnit.SECONDS,\n        workQueue,\n        newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n    threadPoolExecutor.allowCoreThreadTimeOut(true);\n\n    TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n    transferConfiguration.setMinimumUploadPartSize(partSize);\n    transferConfiguration.setMultipartUploadThreshold(multiPartThreshold);\n\n    transfers \u003d new TransferManager(s3, threadPoolExecutor);\n    transfers.setConfiguration(transferConfiguration);\n\n    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n    if (!cannedACLName.isEmpty()) {\n      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL \u003d null;\n    }\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n      DEFAULT_PURGE_EXISTING_MULTIPART);\n    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n\n    if (purgeExistingMultipart) {\n      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n\n      transfers.abortMultipartUploads(bucket, purgeBefore);\n    }\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "64443490d7f189e8e42d284615f3814ef751395a": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11670. Regression: s3a auth setup broken. (Adam Budde via stevel)\n",
      "commitDate": "08/03/15 11:22 AM",
      "commitName": "64443490d7f189e8e42d284615f3814ef751395a",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "03/03/15 4:18 PM",
      "commitNameOld": "15b7076ad5f2ae92d231140b2f8cebc392a92c87",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 4.75,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,154 +1,164 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n     // Try to get our credentials or just connect anonymously\n-    S3Credentials s3Credentials \u003d new S3Credentials();\n-    s3Credentials.initialize(name, conf);\n+    String accessKey \u003d conf.get(ACCESS_KEY, null);\n+    String secretKey \u003d conf.get(SECRET_KEY, null);\n+\n+    String userInfo \u003d name.getUserInfo();\n+    if (userInfo !\u003d null) {\n+      int index \u003d userInfo.indexOf(\u0027:\u0027);\n+      if (index !\u003d -1) {\n+        accessKey \u003d userInfo.substring(0, index);\n+        secretKey \u003d userInfo.substring(index + 1);\n+      } else {\n+        accessKey \u003d userInfo;\n+      }\n+    }\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n-        new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n-                                        s3Credentials.getSecretAccessKey()),\n+        new BasicAWSCredentialsProvider(accessKey, secretKey),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n       DEFAULT_MAXIMUM_CONNECTIONS));\n     boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n         DEFAULT_SECURE_CONNECTIONS);\n     awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n       DEFAULT_MAX_ERROR_RETRIES));\n     awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n         DEFAULT_ESTABLISH_TIMEOUT));\n     awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n       DEFAULT_SOCKET_TIMEOUT));\n \n     String proxyHost \u003d conf.getTrimmed(PROXY_HOST,\"\");\n     int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n     if (!proxyHost.isEmpty()) {\n       awsConf.setProxyHost(proxyHost);\n       if (proxyPort \u003e\u003d 0) {\n         awsConf.setProxyPort(proxyPort);\n       } else {\n         if (secureConnections) {\n           LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n           awsConf.setProxyPort(443);\n         } else {\n           LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n           awsConf.setProxyPort(80);\n         }\n       }\n       String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n       String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n       if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n         String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n             PROXY_PASSWORD + \" set without the other.\";\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg);\n       }\n       awsConf.setProxyUsername(proxyUsername);\n       awsConf.setProxyPassword(proxyPassword);\n       awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n       awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                 \"domain {} as workstation {}\", awsConf.getProxyHost(),\n             awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n             awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n             awsConf.getProxyWorkstation());\n       }\n     } else if (proxyPort \u003e\u003d 0) {\n       String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n       LOG.error(msg);\n       throw new IllegalArgumentException(msg);\n     }\n \n     s3 \u003d new AmazonS3Client(credentials, awsConf);\n     String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n     if (!endPoint.isEmpty()) {\n       try {\n         s3.setEndpoint(endPoint);\n       } catch (IllegalArgumentException e) {\n         String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg, e);\n       }\n     }\n \n     maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n     multiPartThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD,\n       DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       multiPartThreshold \u003d 5 * 1024 * 1024;\n     }\n \n     int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n     int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n     if (maxThreads \u003d\u003d 0) {\n       maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     if (coreThreads \u003d\u003d 0) {\n       coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n     LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n       new LinkedBlockingQueue\u003c\u003e(maxThreads *\n         conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n     threadPoolExecutor \u003d new ThreadPoolExecutor(\n         coreThreads,\n         maxThreads,\n         keepAliveTime,\n         TimeUnit.SECONDS,\n         workQueue,\n         newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n     threadPoolExecutor.allowCoreThreadTimeOut(true);\n \n     TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n     transferConfiguration.setMinimumUploadPartSize(partSize);\n     transferConfiguration.setMultipartUploadThreshold(multiPartThreshold);\n \n     transfers \u003d new TransferManager(s3, threadPoolExecutor);\n     transfers.setConfiguration(transferConfiguration);\n \n     String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n     if (!cannedACLName.isEmpty()) {\n       cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n     } else {\n       cannedACL \u003d null;\n     }\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n       DEFAULT_PURGE_EXISTING_MULTIPART);\n     long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n       DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n \n     if (purgeExistingMultipart) {\n       Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n \n       transfers.abortMultipartUploads(bucket, purgeBefore);\n     }\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    // Try to get our credentials or just connect anonymously\n    String accessKey \u003d conf.get(ACCESS_KEY, null);\n    String secretKey \u003d conf.get(SECRET_KEY, null);\n\n    String userInfo \u003d name.getUserInfo();\n    if (userInfo !\u003d null) {\n      int index \u003d userInfo.indexOf(\u0027:\u0027);\n      if (index !\u003d -1) {\n        accessKey \u003d userInfo.substring(0, index);\n        secretKey \u003d userInfo.substring(index + 1);\n      } else {\n        accessKey \u003d userInfo;\n      }\n    }\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(accessKey, secretKey),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n      DEFAULT_MAXIMUM_CONNECTIONS));\n    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n        DEFAULT_SECURE_CONNECTIONS);\n    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n        DEFAULT_ESTABLISH_TIMEOUT));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n      DEFAULT_SOCKET_TIMEOUT));\n\n    String proxyHost \u003d conf.getTrimmed(PROXY_HOST,\"\");\n    int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n    if (!proxyHost.isEmpty()) {\n      awsConf.setProxyHost(proxyHost);\n      if (proxyPort \u003e\u003d 0) {\n        awsConf.setProxyPort(proxyPort);\n      } else {\n        if (secureConnections) {\n          LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n          awsConf.setProxyPort(443);\n        } else {\n          LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n          awsConf.setProxyPort(80);\n        }\n      }\n      String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n      String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n      if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n        String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n            PROXY_PASSWORD + \" set without the other.\";\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg);\n      }\n      awsConf.setProxyUsername(proxyUsername);\n      awsConf.setProxyPassword(proxyPassword);\n      awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n      awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                \"domain {} as workstation {}\", awsConf.getProxyHost(),\n            awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n            awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n            awsConf.getProxyWorkstation());\n      }\n    } else if (proxyPort \u003e\u003d 0) {\n      String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n      LOG.error(msg);\n      throw new IllegalArgumentException(msg);\n    }\n\n    s3 \u003d new AmazonS3Client(credentials, awsConf);\n    String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n    if (!endPoint.isEmpty()) {\n      try {\n        s3.setEndpoint(endPoint);\n      } catch (IllegalArgumentException e) {\n        String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg, e);\n      }\n    }\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    multiPartThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD,\n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      multiPartThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n    if (maxThreads \u003d\u003d 0) {\n      maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    if (coreThreads \u003d\u003d 0) {\n      coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n      new LinkedBlockingQueue\u003c\u003e(maxThreads *\n        conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n    threadPoolExecutor \u003d new ThreadPoolExecutor(\n        coreThreads,\n        maxThreads,\n        keepAliveTime,\n        TimeUnit.SECONDS,\n        workQueue,\n        newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n    threadPoolExecutor.allowCoreThreadTimeOut(true);\n\n    TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n    transferConfiguration.setMinimumUploadPartSize(partSize);\n    transferConfiguration.setMultipartUploadThreshold(multiPartThreshold);\n\n    transfers \u003d new TransferManager(s3, threadPoolExecutor);\n    transfers.setConfiguration(transferConfiguration);\n\n    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n    if (!cannedACLName.isEmpty()) {\n      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL \u003d null;\n    }\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n      DEFAULT_PURGE_EXISTING_MULTIPART);\n    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n\n    if (purgeExistingMultipart) {\n      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n\n      transfers.abortMultipartUploads(bucket, purgeBefore);\n    }\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "15b7076ad5f2ae92d231140b2f8cebc392a92c87": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11183. Memory-based S3AOutputstream. (Thomas Demoor via stevel)\n",
      "commitDate": "03/03/15 4:18 PM",
      "commitName": "15b7076ad5f2ae92d231140b2f8cebc392a92c87",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "21/02/15 4:03 AM",
      "commitNameOld": "709ff99cff4124823bde631e272af7be9a22f83b",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 10.51,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,154 +1,154 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n     // Try to get our credentials or just connect anonymously\n     S3Credentials s3Credentials \u003d new S3Credentials();\n     s3Credentials.initialize(name, conf);\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n         new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                         s3Credentials.getSecretAccessKey()),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n       DEFAULT_MAXIMUM_CONNECTIONS));\n     boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n         DEFAULT_SECURE_CONNECTIONS);\n     awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n       DEFAULT_MAX_ERROR_RETRIES));\n     awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n         DEFAULT_ESTABLISH_TIMEOUT));\n     awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n       DEFAULT_SOCKET_TIMEOUT));\n \n     String proxyHost \u003d conf.getTrimmed(PROXY_HOST,\"\");\n     int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n     if (!proxyHost.isEmpty()) {\n       awsConf.setProxyHost(proxyHost);\n       if (proxyPort \u003e\u003d 0) {\n         awsConf.setProxyPort(proxyPort);\n       } else {\n         if (secureConnections) {\n           LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n           awsConf.setProxyPort(443);\n         } else {\n           LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n           awsConf.setProxyPort(80);\n         }\n       }\n       String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n       String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n       if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n         String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n             PROXY_PASSWORD + \" set without the other.\";\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg);\n       }\n       awsConf.setProxyUsername(proxyUsername);\n       awsConf.setProxyPassword(proxyPassword);\n       awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n       awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                 \"domain {} as workstation {}\", awsConf.getProxyHost(),\n             awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n             awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n             awsConf.getProxyWorkstation());\n       }\n     } else if (proxyPort \u003e\u003d 0) {\n       String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n       LOG.error(msg);\n       throw new IllegalArgumentException(msg);\n     }\n \n     s3 \u003d new AmazonS3Client(credentials, awsConf);\n     String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n     if (!endPoint.isEmpty()) {\n       try {\n         s3.setEndpoint(endPoint);\n       } catch (IllegalArgumentException e) {\n         String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg, e);\n       }\n     }\n \n     maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n-    partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n+    multiPartThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD,\n       DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n-    if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n+    if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n-      partSizeThreshold \u003d 5 * 1024 * 1024;\n+      multiPartThreshold \u003d 5 * 1024 * 1024;\n     }\n \n     int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n     int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n     if (maxThreads \u003d\u003d 0) {\n       maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     if (coreThreads \u003d\u003d 0) {\n       coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n     LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n       new LinkedBlockingQueue\u003c\u003e(maxThreads *\n         conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n-    ThreadPoolExecutor tpe \u003d new ThreadPoolExecutor(\n+    threadPoolExecutor \u003d new ThreadPoolExecutor(\n         coreThreads,\n         maxThreads,\n         keepAliveTime,\n         TimeUnit.SECONDS,\n         workQueue,\n         newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n-    tpe.allowCoreThreadTimeOut(true);\n+    threadPoolExecutor.allowCoreThreadTimeOut(true);\n \n     TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n     transferConfiguration.setMinimumUploadPartSize(partSize);\n-    transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n+    transferConfiguration.setMultipartUploadThreshold(multiPartThreshold);\n \n-    transfers \u003d new TransferManager(s3, tpe);\n+    transfers \u003d new TransferManager(s3, threadPoolExecutor);\n     transfers.setConfiguration(transferConfiguration);\n \n     String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n     if (!cannedACLName.isEmpty()) {\n       cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n     } else {\n       cannedACL \u003d null;\n     }\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n       DEFAULT_PURGE_EXISTING_MULTIPART);\n     long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n       DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n \n     if (purgeExistingMultipart) {\n       Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n \n       transfers.abortMultipartUploads(bucket, purgeBefore);\n     }\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    // Try to get our credentials or just connect anonymously\n    S3Credentials s3Credentials \u003d new S3Credentials();\n    s3Credentials.initialize(name, conf);\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                        s3Credentials.getSecretAccessKey()),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n      DEFAULT_MAXIMUM_CONNECTIONS));\n    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n        DEFAULT_SECURE_CONNECTIONS);\n    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n        DEFAULT_ESTABLISH_TIMEOUT));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n      DEFAULT_SOCKET_TIMEOUT));\n\n    String proxyHost \u003d conf.getTrimmed(PROXY_HOST,\"\");\n    int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n    if (!proxyHost.isEmpty()) {\n      awsConf.setProxyHost(proxyHost);\n      if (proxyPort \u003e\u003d 0) {\n        awsConf.setProxyPort(proxyPort);\n      } else {\n        if (secureConnections) {\n          LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n          awsConf.setProxyPort(443);\n        } else {\n          LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n          awsConf.setProxyPort(80);\n        }\n      }\n      String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n      String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n      if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n        String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n            PROXY_PASSWORD + \" set without the other.\";\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg);\n      }\n      awsConf.setProxyUsername(proxyUsername);\n      awsConf.setProxyPassword(proxyPassword);\n      awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n      awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                \"domain {} as workstation {}\", awsConf.getProxyHost(),\n            awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n            awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n            awsConf.getProxyWorkstation());\n      }\n    } else if (proxyPort \u003e\u003d 0) {\n      String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n      LOG.error(msg);\n      throw new IllegalArgumentException(msg);\n    }\n\n    s3 \u003d new AmazonS3Client(credentials, awsConf);\n    String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n    if (!endPoint.isEmpty()) {\n      try {\n        s3.setEndpoint(endPoint);\n      } catch (IllegalArgumentException e) {\n        String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg, e);\n      }\n    }\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    multiPartThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD,\n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (multiPartThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      multiPartThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n    if (maxThreads \u003d\u003d 0) {\n      maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    if (coreThreads \u003d\u003d 0) {\n      coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n      new LinkedBlockingQueue\u003c\u003e(maxThreads *\n        conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n    threadPoolExecutor \u003d new ThreadPoolExecutor(\n        coreThreads,\n        maxThreads,\n        keepAliveTime,\n        TimeUnit.SECONDS,\n        workQueue,\n        newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n    threadPoolExecutor.allowCoreThreadTimeOut(true);\n\n    TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n    transferConfiguration.setMinimumUploadPartSize(partSize);\n    transferConfiguration.setMultipartUploadThreshold(multiPartThreshold);\n\n    transfers \u003d new TransferManager(s3, threadPoolExecutor);\n    transfers.setConfiguration(transferConfiguration);\n\n    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n    if (!cannedACLName.isEmpty()) {\n      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL \u003d null;\n    }\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n      DEFAULT_PURGE_EXISTING_MULTIPART);\n    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n\n    if (purgeExistingMultipart) {\n      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n\n      transfers.abortMultipartUploads(bucket, purgeBefore);\n    }\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "709ff99cff4124823bde631e272af7be9a22f83b": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11584 s3a file block size set to 0 in getFileStatus. (Brahma Reddy Battula via stevel)\n",
      "commitDate": "21/02/15 4:03 AM",
      "commitName": "709ff99cff4124823bde631e272af7be9a22f83b",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "20/02/15 12:51 PM",
      "commitNameOld": "aa1c437b6a806de612f030a68984c606c623f1d9",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 0.63,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,154 +1,154 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n     // Try to get our credentials or just connect anonymously\n     S3Credentials s3Credentials \u003d new S3Credentials();\n     s3Credentials.initialize(name, conf);\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n         new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                         s3Credentials.getSecretAccessKey()),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n       DEFAULT_MAXIMUM_CONNECTIONS));\n     boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n         DEFAULT_SECURE_CONNECTIONS);\n     awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n       DEFAULT_MAX_ERROR_RETRIES));\n     awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n         DEFAULT_ESTABLISH_TIMEOUT));\n     awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n       DEFAULT_SOCKET_TIMEOUT));\n \n     String proxyHost \u003d conf.getTrimmed(PROXY_HOST,\"\");\n     int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n     if (!proxyHost.isEmpty()) {\n       awsConf.setProxyHost(proxyHost);\n       if (proxyPort \u003e\u003d 0) {\n         awsConf.setProxyPort(proxyPort);\n       } else {\n         if (secureConnections) {\n           LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n           awsConf.setProxyPort(443);\n         } else {\n           LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n           awsConf.setProxyPort(80);\n         }\n       }\n       String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n       String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n       if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n         String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n             PROXY_PASSWORD + \" set without the other.\";\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg);\n       }\n       awsConf.setProxyUsername(proxyUsername);\n       awsConf.setProxyPassword(proxyPassword);\n       awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n       awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                 \"domain {} as workstation {}\", awsConf.getProxyHost(),\n             awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n             awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n             awsConf.getProxyWorkstation());\n       }\n     } else if (proxyPort \u003e\u003d 0) {\n       String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n       LOG.error(msg);\n       throw new IllegalArgumentException(msg);\n     }\n \n     s3 \u003d new AmazonS3Client(credentials, awsConf);\n     String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n     if (!endPoint.isEmpty()) {\n       try {\n         s3.setEndpoint(endPoint);\n       } catch (IllegalArgumentException e) {\n         String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg, e);\n       }\n     }\n \n     maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n     partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n       DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       partSizeThreshold \u003d 5 * 1024 * 1024;\n     }\n \n     int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n     int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n     if (maxThreads \u003d\u003d 0) {\n       maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     if (coreThreads \u003d\u003d 0) {\n       coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n     LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n-      new LinkedBlockingQueue\u003cRunnable\u003e(maxThreads *\n+      new LinkedBlockingQueue\u003c\u003e(maxThreads *\n         conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n     ThreadPoolExecutor tpe \u003d new ThreadPoolExecutor(\n         coreThreads,\n         maxThreads,\n         keepAliveTime,\n         TimeUnit.SECONDS,\n         workQueue,\n         newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n     tpe.allowCoreThreadTimeOut(true);\n \n     TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n     transferConfiguration.setMinimumUploadPartSize(partSize);\n     transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n \n     transfers \u003d new TransferManager(s3, tpe);\n     transfers.setConfiguration(transferConfiguration);\n \n     String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n     if (!cannedACLName.isEmpty()) {\n       cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n     } else {\n       cannedACL \u003d null;\n     }\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n       DEFAULT_PURGE_EXISTING_MULTIPART);\n     long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n       DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n \n     if (purgeExistingMultipart) {\n       Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n \n       transfers.abortMultipartUploads(bucket, purgeBefore);\n     }\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    // Try to get our credentials or just connect anonymously\n    S3Credentials s3Credentials \u003d new S3Credentials();\n    s3Credentials.initialize(name, conf);\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                        s3Credentials.getSecretAccessKey()),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n      DEFAULT_MAXIMUM_CONNECTIONS));\n    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n        DEFAULT_SECURE_CONNECTIONS);\n    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n        DEFAULT_ESTABLISH_TIMEOUT));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n      DEFAULT_SOCKET_TIMEOUT));\n\n    String proxyHost \u003d conf.getTrimmed(PROXY_HOST,\"\");\n    int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n    if (!proxyHost.isEmpty()) {\n      awsConf.setProxyHost(proxyHost);\n      if (proxyPort \u003e\u003d 0) {\n        awsConf.setProxyPort(proxyPort);\n      } else {\n        if (secureConnections) {\n          LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n          awsConf.setProxyPort(443);\n        } else {\n          LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n          awsConf.setProxyPort(80);\n        }\n      }\n      String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n      String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n      if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n        String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n            PROXY_PASSWORD + \" set without the other.\";\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg);\n      }\n      awsConf.setProxyUsername(proxyUsername);\n      awsConf.setProxyPassword(proxyPassword);\n      awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n      awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                \"domain {} as workstation {}\", awsConf.getProxyHost(),\n            awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n            awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n            awsConf.getProxyWorkstation());\n      }\n    } else if (proxyPort \u003e\u003d 0) {\n      String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n      LOG.error(msg);\n      throw new IllegalArgumentException(msg);\n    }\n\n    s3 \u003d new AmazonS3Client(credentials, awsConf);\n    String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n    if (!endPoint.isEmpty()) {\n      try {\n        s3.setEndpoint(endPoint);\n      } catch (IllegalArgumentException e) {\n        String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg, e);\n      }\n    }\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      partSizeThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n    if (maxThreads \u003d\u003d 0) {\n      maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    if (coreThreads \u003d\u003d 0) {\n      coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n      new LinkedBlockingQueue\u003c\u003e(maxThreads *\n        conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n    ThreadPoolExecutor tpe \u003d new ThreadPoolExecutor(\n        coreThreads,\n        maxThreads,\n        keepAliveTime,\n        TimeUnit.SECONDS,\n        workQueue,\n        newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n    tpe.allowCoreThreadTimeOut(true);\n\n    TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n    transferConfiguration.setMinimumUploadPartSize(partSize);\n    transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n\n    transfers \u003d new TransferManager(s3, tpe);\n    transfers.setConfiguration(transferConfiguration);\n\n    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n    if (!cannedACLName.isEmpty()) {\n      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL \u003d null;\n    }\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n      DEFAULT_PURGE_EXISTING_MULTIPART);\n    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n\n    if (purgeExistingMultipart) {\n      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n\n      transfers.abortMultipartUploads(bucket, purgeBefore);\n    }\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "00b80958d862dbcc08d6f186c09963d3351ba0fd": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11521. Make connection timeout configurable in s3a. (Thomas Demoor via stevel)\n",
      "commitDate": "17/02/15 12:06 PM",
      "commitName": "00b80958d862dbcc08d6f186c09963d3351ba0fd",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "05/02/15 4:20 AM",
      "commitNameOld": "4e7ad4f0a88bd36bc91db6b1bd311d7f5c6bebee",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 12.32,
      "commitsBetweenForRepo": 158,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,152 +1,154 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n     // Try to get our credentials or just connect anonymously\n     S3Credentials s3Credentials \u003d new S3Credentials();\n     s3Credentials.initialize(name, conf);\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n         new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                         s3Credentials.getSecretAccessKey()),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n       DEFAULT_MAXIMUM_CONNECTIONS));\n     boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n         DEFAULT_SECURE_CONNECTIONS);\n     awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n       DEFAULT_MAX_ERROR_RETRIES));\n+    awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n+        DEFAULT_ESTABLISH_TIMEOUT));\n     awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n       DEFAULT_SOCKET_TIMEOUT));\n \n     String proxyHost \u003d conf.getTrimmed(PROXY_HOST,\"\");\n     int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n     if (!proxyHost.isEmpty()) {\n       awsConf.setProxyHost(proxyHost);\n       if (proxyPort \u003e\u003d 0) {\n         awsConf.setProxyPort(proxyPort);\n       } else {\n         if (secureConnections) {\n           LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n           awsConf.setProxyPort(443);\n         } else {\n           LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n           awsConf.setProxyPort(80);\n         }\n       }\n       String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n       String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n       if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n         String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n             PROXY_PASSWORD + \" set without the other.\";\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg);\n       }\n       awsConf.setProxyUsername(proxyUsername);\n       awsConf.setProxyPassword(proxyPassword);\n       awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n       awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                 \"domain {} as workstation {}\", awsConf.getProxyHost(),\n             awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n             awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n             awsConf.getProxyWorkstation());\n       }\n     } else if (proxyPort \u003e\u003d 0) {\n       String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n       LOG.error(msg);\n       throw new IllegalArgumentException(msg);\n     }\n \n     s3 \u003d new AmazonS3Client(credentials, awsConf);\n     String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n     if (!endPoint.isEmpty()) {\n       try {\n         s3.setEndpoint(endPoint);\n       } catch (IllegalArgumentException e) {\n         String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg, e);\n       }\n     }\n \n     maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n     partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n       DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       partSizeThreshold \u003d 5 * 1024 * 1024;\n     }\n \n     int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n     int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n     if (maxThreads \u003d\u003d 0) {\n       maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     if (coreThreads \u003d\u003d 0) {\n       coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n     LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n       new LinkedBlockingQueue\u003cRunnable\u003e(maxThreads *\n         conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n     ThreadPoolExecutor tpe \u003d new ThreadPoolExecutor(\n         coreThreads,\n         maxThreads,\n         keepAliveTime,\n         TimeUnit.SECONDS,\n         workQueue,\n         newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n     tpe.allowCoreThreadTimeOut(true);\n \n     TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n     transferConfiguration.setMinimumUploadPartSize(partSize);\n     transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n \n     transfers \u003d new TransferManager(s3, tpe);\n     transfers.setConfiguration(transferConfiguration);\n \n     String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n     if (!cannedACLName.isEmpty()) {\n       cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n     } else {\n       cannedACL \u003d null;\n     }\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n       DEFAULT_PURGE_EXISTING_MULTIPART);\n     long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n       DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n \n     if (purgeExistingMultipart) {\n       Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n \n       transfers.abortMultipartUploads(bucket, purgeBefore);\n     }\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    // Try to get our credentials or just connect anonymously\n    S3Credentials s3Credentials \u003d new S3Credentials();\n    s3Credentials.initialize(name, conf);\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                        s3Credentials.getSecretAccessKey()),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n      DEFAULT_MAXIMUM_CONNECTIONS));\n    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n        DEFAULT_SECURE_CONNECTIONS);\n    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setConnectionTimeout(conf.getInt(ESTABLISH_TIMEOUT,\n        DEFAULT_ESTABLISH_TIMEOUT));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n      DEFAULT_SOCKET_TIMEOUT));\n\n    String proxyHost \u003d conf.getTrimmed(PROXY_HOST,\"\");\n    int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n    if (!proxyHost.isEmpty()) {\n      awsConf.setProxyHost(proxyHost);\n      if (proxyPort \u003e\u003d 0) {\n        awsConf.setProxyPort(proxyPort);\n      } else {\n        if (secureConnections) {\n          LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n          awsConf.setProxyPort(443);\n        } else {\n          LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n          awsConf.setProxyPort(80);\n        }\n      }\n      String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n      String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n      if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n        String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n            PROXY_PASSWORD + \" set without the other.\";\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg);\n      }\n      awsConf.setProxyUsername(proxyUsername);\n      awsConf.setProxyPassword(proxyPassword);\n      awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n      awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                \"domain {} as workstation {}\", awsConf.getProxyHost(),\n            awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n            awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n            awsConf.getProxyWorkstation());\n      }\n    } else if (proxyPort \u003e\u003d 0) {\n      String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n      LOG.error(msg);\n      throw new IllegalArgumentException(msg);\n    }\n\n    s3 \u003d new AmazonS3Client(credentials, awsConf);\n    String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n    if (!endPoint.isEmpty()) {\n      try {\n        s3.setEndpoint(endPoint);\n      } catch (IllegalArgumentException e) {\n        String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg, e);\n      }\n    }\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      partSizeThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n    if (maxThreads \u003d\u003d 0) {\n      maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    if (coreThreads \u003d\u003d 0) {\n      coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n      new LinkedBlockingQueue\u003cRunnable\u003e(maxThreads *\n        conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n    ThreadPoolExecutor tpe \u003d new ThreadPoolExecutor(\n        coreThreads,\n        maxThreads,\n        keepAliveTime,\n        TimeUnit.SECONDS,\n        workQueue,\n        newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n    tpe.allowCoreThreadTimeOut(true);\n\n    TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n    transferConfiguration.setMinimumUploadPartSize(partSize);\n    transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n\n    transfers \u003d new TransferManager(s3, tpe);\n    transfers.setConfiguration(transferConfiguration);\n\n    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n    if (!cannedACLName.isEmpty()) {\n      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL \u003d null;\n    }\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n      DEFAULT_PURGE_EXISTING_MULTIPART);\n    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n\n    if (purgeExistingMultipart) {\n      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n\n      transfers.abortMultipartUploads(bucket, purgeBefore);\n    }\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "4e7ad4f0a88bd36bc91db6b1bd311d7f5c6bebee": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11463 Replace method-local TransferManager object with S3AFileSystem#transfers. (Ted Yu via stevel)\n",
      "commitDate": "05/02/15 4:20 AM",
      "commitName": "4e7ad4f0a88bd36bc91db6b1bd311d7f5c6bebee",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "17/01/15 10:40 AM",
      "commitNameOld": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 18.74,
      "commitsBetweenForRepo": 152,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,153 +1,152 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n     // Try to get our credentials or just connect anonymously\n     S3Credentials s3Credentials \u003d new S3Credentials();\n     s3Credentials.initialize(name, conf);\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n         new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                         s3Credentials.getSecretAccessKey()),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n       DEFAULT_MAXIMUM_CONNECTIONS));\n     boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n         DEFAULT_SECURE_CONNECTIONS);\n     awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n       DEFAULT_MAX_ERROR_RETRIES));\n     awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n       DEFAULT_SOCKET_TIMEOUT));\n \n     String proxyHost \u003d conf.getTrimmed(PROXY_HOST,\"\");\n     int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n     if (!proxyHost.isEmpty()) {\n       awsConf.setProxyHost(proxyHost);\n       if (proxyPort \u003e\u003d 0) {\n         awsConf.setProxyPort(proxyPort);\n       } else {\n         if (secureConnections) {\n           LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n           awsConf.setProxyPort(443);\n         } else {\n           LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n           awsConf.setProxyPort(80);\n         }\n       }\n       String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n       String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n       if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n         String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n             PROXY_PASSWORD + \" set without the other.\";\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg);\n       }\n       awsConf.setProxyUsername(proxyUsername);\n       awsConf.setProxyPassword(proxyPassword);\n       awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n       awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                 \"domain {} as workstation {}\", awsConf.getProxyHost(),\n             awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n             awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n             awsConf.getProxyWorkstation());\n       }\n     } else if (proxyPort \u003e\u003d 0) {\n       String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n       LOG.error(msg);\n       throw new IllegalArgumentException(msg);\n     }\n \n     s3 \u003d new AmazonS3Client(credentials, awsConf);\n     String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n     if (!endPoint.isEmpty()) {\n       try {\n         s3.setEndpoint(endPoint);\n       } catch (IllegalArgumentException e) {\n         String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg, e);\n       }\n     }\n \n     maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n     partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n       DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       partSizeThreshold \u003d 5 * 1024 * 1024;\n     }\n \n     int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n     int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n     if (maxThreads \u003d\u003d 0) {\n       maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     if (coreThreads \u003d\u003d 0) {\n       coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n     LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n       new LinkedBlockingQueue\u003cRunnable\u003e(maxThreads *\n         conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n     ThreadPoolExecutor tpe \u003d new ThreadPoolExecutor(\n         coreThreads,\n         maxThreads,\n         keepAliveTime,\n         TimeUnit.SECONDS,\n         workQueue,\n         newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n     tpe.allowCoreThreadTimeOut(true);\n \n     TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n     transferConfiguration.setMinimumUploadPartSize(partSize);\n     transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n \n     transfers \u003d new TransferManager(s3, tpe);\n     transfers.setConfiguration(transferConfiguration);\n \n     String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n     if (!cannedACLName.isEmpty()) {\n       cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n     } else {\n       cannedACL \u003d null;\n     }\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n       DEFAULT_PURGE_EXISTING_MULTIPART);\n     long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n       DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n \n     if (purgeExistingMultipart) {\n       Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n \n       transfers.abortMultipartUploads(bucket, purgeBefore);\n-      transfers.shutdownNow(false);\n     }\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    // Try to get our credentials or just connect anonymously\n    S3Credentials s3Credentials \u003d new S3Credentials();\n    s3Credentials.initialize(name, conf);\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                        s3Credentials.getSecretAccessKey()),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n      DEFAULT_MAXIMUM_CONNECTIONS));\n    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n        DEFAULT_SECURE_CONNECTIONS);\n    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n      DEFAULT_SOCKET_TIMEOUT));\n\n    String proxyHost \u003d conf.getTrimmed(PROXY_HOST,\"\");\n    int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n    if (!proxyHost.isEmpty()) {\n      awsConf.setProxyHost(proxyHost);\n      if (proxyPort \u003e\u003d 0) {\n        awsConf.setProxyPort(proxyPort);\n      } else {\n        if (secureConnections) {\n          LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n          awsConf.setProxyPort(443);\n        } else {\n          LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n          awsConf.setProxyPort(80);\n        }\n      }\n      String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n      String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n      if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n        String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n            PROXY_PASSWORD + \" set without the other.\";\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg);\n      }\n      awsConf.setProxyUsername(proxyUsername);\n      awsConf.setProxyPassword(proxyPassword);\n      awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n      awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                \"domain {} as workstation {}\", awsConf.getProxyHost(),\n            awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n            awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n            awsConf.getProxyWorkstation());\n      }\n    } else if (proxyPort \u003e\u003d 0) {\n      String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n      LOG.error(msg);\n      throw new IllegalArgumentException(msg);\n    }\n\n    s3 \u003d new AmazonS3Client(credentials, awsConf);\n    String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n    if (!endPoint.isEmpty()) {\n      try {\n        s3.setEndpoint(endPoint);\n      } catch (IllegalArgumentException e) {\n        String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg, e);\n      }\n    }\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      partSizeThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n    if (maxThreads \u003d\u003d 0) {\n      maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    if (coreThreads \u003d\u003d 0) {\n      coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n      new LinkedBlockingQueue\u003cRunnable\u003e(maxThreads *\n        conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n    ThreadPoolExecutor tpe \u003d new ThreadPoolExecutor(\n        coreThreads,\n        maxThreads,\n        keepAliveTime,\n        TimeUnit.SECONDS,\n        workQueue,\n        newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n    tpe.allowCoreThreadTimeOut(true);\n\n    TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n    transferConfiguration.setMinimumUploadPartSize(partSize);\n    transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n\n    transfers \u003d new TransferManager(s3, tpe);\n    transfers.setConfiguration(transferConfiguration);\n\n    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n    if (!cannedACLName.isEmpty()) {\n      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL \u003d null;\n    }\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n      DEFAULT_PURGE_EXISTING_MULTIPART);\n    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n\n    if (purgeExistingMultipart) {\n      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n\n      transfers.abortMultipartUploads(bucket, purgeBefore);\n    }\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "2908fe4ec52f78d74e4207274a34d88d54cd468f": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11171 Enable using a proxy server to connect to S3a. (Thomas Demoor via stevel)\n",
      "commitDate": "17/01/15 10:40 AM",
      "commitName": "2908fe4ec52f78d74e4207274a34d88d54cd468f",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "16/01/15 2:15 AM",
      "commitNameOld": "000ca83ea3aeb3687625c857bcc0762fd2887db2",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 1.35,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,112 +1,153 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n     // Try to get our credentials or just connect anonymously\n     S3Credentials s3Credentials \u003d new S3Credentials();\n     s3Credentials.initialize(name, conf);\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n         new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                         s3Credentials.getSecretAccessKey()),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n       DEFAULT_MAXIMUM_CONNECTIONS));\n-    awsConf.setProtocol(conf.getBoolean(SECURE_CONNECTIONS, \n-      DEFAULT_SECURE_CONNECTIONS) ?  Protocol.HTTPS : Protocol.HTTP);\n+    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n+        DEFAULT_SECURE_CONNECTIONS);\n+    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n       DEFAULT_MAX_ERROR_RETRIES));\n     awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n       DEFAULT_SOCKET_TIMEOUT));\n \n+    String proxyHost \u003d conf.getTrimmed(PROXY_HOST,\"\");\n+    int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n+    if (!proxyHost.isEmpty()) {\n+      awsConf.setProxyHost(proxyHost);\n+      if (proxyPort \u003e\u003d 0) {\n+        awsConf.setProxyPort(proxyPort);\n+      } else {\n+        if (secureConnections) {\n+          LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n+          awsConf.setProxyPort(443);\n+        } else {\n+          LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n+          awsConf.setProxyPort(80);\n+        }\n+      }\n+      String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n+      String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n+      if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n+        String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n+            PROXY_PASSWORD + \" set without the other.\";\n+        LOG.error(msg);\n+        throw new IllegalArgumentException(msg);\n+      }\n+      awsConf.setProxyUsername(proxyUsername);\n+      awsConf.setProxyPassword(proxyPassword);\n+      awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n+      awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n+                \"domain {} as workstation {}\", awsConf.getProxyHost(),\n+            awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n+            awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n+            awsConf.getProxyWorkstation());\n+      }\n+    } else if (proxyPort \u003e\u003d 0) {\n+      String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n+      LOG.error(msg);\n+      throw new IllegalArgumentException(msg);\n+    }\n+\n     s3 \u003d new AmazonS3Client(credentials, awsConf);\n     String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n     if (!endPoint.isEmpty()) {\n       try {\n         s3.setEndpoint(endPoint);\n       } catch (IllegalArgumentException e) {\n         String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n         LOG.error(msg);\n         throw new IllegalArgumentException(msg, e);\n       }\n     }\n \n     maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n     partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n       DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       partSizeThreshold \u003d 5 * 1024 * 1024;\n     }\n \n     int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n     int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n     if (maxThreads \u003d\u003d 0) {\n       maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     if (coreThreads \u003d\u003d 0) {\n       coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n     LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n       new LinkedBlockingQueue\u003cRunnable\u003e(maxThreads *\n         conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n     ThreadPoolExecutor tpe \u003d new ThreadPoolExecutor(\n         coreThreads,\n         maxThreads,\n         keepAliveTime,\n         TimeUnit.SECONDS,\n         workQueue,\n         newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n     tpe.allowCoreThreadTimeOut(true);\n \n     TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n     transferConfiguration.setMinimumUploadPartSize(partSize);\n     transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n \n     transfers \u003d new TransferManager(s3, tpe);\n     transfers.setConfiguration(transferConfiguration);\n \n     String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n     if (!cannedACLName.isEmpty()) {\n       cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n     } else {\n       cannedACL \u003d null;\n     }\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n       DEFAULT_PURGE_EXISTING_MULTIPART);\n     long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n       DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n \n     if (purgeExistingMultipart) {\n       Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n \n       transfers.abortMultipartUploads(bucket, purgeBefore);\n       transfers.shutdownNow(false);\n     }\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    // Try to get our credentials or just connect anonymously\n    S3Credentials s3Credentials \u003d new S3Credentials();\n    s3Credentials.initialize(name, conf);\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                        s3Credentials.getSecretAccessKey()),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n      DEFAULT_MAXIMUM_CONNECTIONS));\n    boolean secureConnections \u003d conf.getBoolean(SECURE_CONNECTIONS,\n        DEFAULT_SECURE_CONNECTIONS);\n    awsConf.setProtocol(secureConnections ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n      DEFAULT_SOCKET_TIMEOUT));\n\n    String proxyHost \u003d conf.getTrimmed(PROXY_HOST,\"\");\n    int proxyPort \u003d conf.getInt(PROXY_PORT, -1);\n    if (!proxyHost.isEmpty()) {\n      awsConf.setProxyHost(proxyHost);\n      if (proxyPort \u003e\u003d 0) {\n        awsConf.setProxyPort(proxyPort);\n      } else {\n        if (secureConnections) {\n          LOG.warn(\"Proxy host set without port. Using HTTPS default 443\");\n          awsConf.setProxyPort(443);\n        } else {\n          LOG.warn(\"Proxy host set without port. Using HTTP default 80\");\n          awsConf.setProxyPort(80);\n        }\n      }\n      String proxyUsername \u003d conf.getTrimmed(PROXY_USERNAME);\n      String proxyPassword \u003d conf.getTrimmed(PROXY_PASSWORD);\n      if ((proxyUsername \u003d\u003d null) !\u003d (proxyPassword \u003d\u003d null)) {\n        String msg \u003d \"Proxy error: \" + PROXY_USERNAME + \" or \" +\n            PROXY_PASSWORD + \" set without the other.\";\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg);\n      }\n      awsConf.setProxyUsername(proxyUsername);\n      awsConf.setProxyPassword(proxyPassword);\n      awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));\n      awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Using proxy server {}:{} as user {} with password {} on \" +\n                \"domain {} as workstation {}\", awsConf.getProxyHost(),\n            awsConf.getProxyPort(), String.valueOf(awsConf.getProxyUsername()),\n            awsConf.getProxyPassword(), awsConf.getProxyDomain(),\n            awsConf.getProxyWorkstation());\n      }\n    } else if (proxyPort \u003e\u003d 0) {\n      String msg \u003d \"Proxy error: \" + PROXY_PORT + \" set without \" + PROXY_HOST;\n      LOG.error(msg);\n      throw new IllegalArgumentException(msg);\n    }\n\n    s3 \u003d new AmazonS3Client(credentials, awsConf);\n    String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n    if (!endPoint.isEmpty()) {\n      try {\n        s3.setEndpoint(endPoint);\n      } catch (IllegalArgumentException e) {\n        String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg, e);\n      }\n    }\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      partSizeThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n    if (maxThreads \u003d\u003d 0) {\n      maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    if (coreThreads \u003d\u003d 0) {\n      coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n      new LinkedBlockingQueue\u003cRunnable\u003e(maxThreads *\n        conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n    ThreadPoolExecutor tpe \u003d new ThreadPoolExecutor(\n        coreThreads,\n        maxThreads,\n        keepAliveTime,\n        TimeUnit.SECONDS,\n        workQueue,\n        newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n    tpe.allowCoreThreadTimeOut(true);\n\n    TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n    transferConfiguration.setMinimumUploadPartSize(partSize);\n    transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n\n    transfers \u003d new TransferManager(s3, tpe);\n    transfers.setConfiguration(transferConfiguration);\n\n    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n    if (!cannedACLName.isEmpty()) {\n      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL \u003d null;\n    }\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n      DEFAULT_PURGE_EXISTING_MULTIPART);\n    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n\n    if (purgeExistingMultipart) {\n      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n\n      transfers.abortMultipartUploads(bucket, purgeBefore);\n      transfers.shutdownNow(false);\n    }\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "000ca83ea3aeb3687625c857bcc0762fd2887db2": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11261 Set custom endpoint for S3A. (Thomas Demoor via stevel)\n",
      "commitDate": "16/01/15 2:15 AM",
      "commitName": "000ca83ea3aeb3687625c857bcc0762fd2887db2",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "05/01/15 5:00 AM",
      "commitNameOld": "27d8395867f665fea1360087325cda5ed70efd0c",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 10.89,
      "commitsBetweenForRepo": 76,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,102 +1,112 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n     // Try to get our credentials or just connect anonymously\n     S3Credentials s3Credentials \u003d new S3Credentials();\n     s3Credentials.initialize(name, conf);\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n         new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                         s3Credentials.getSecretAccessKey()),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n       DEFAULT_MAXIMUM_CONNECTIONS));\n     awsConf.setProtocol(conf.getBoolean(SECURE_CONNECTIONS, \n       DEFAULT_SECURE_CONNECTIONS) ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n       DEFAULT_MAX_ERROR_RETRIES));\n     awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n       DEFAULT_SOCKET_TIMEOUT));\n \n     s3 \u003d new AmazonS3Client(credentials, awsConf);\n+    String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n+    if (!endPoint.isEmpty()) {\n+      try {\n+        s3.setEndpoint(endPoint);\n+      } catch (IllegalArgumentException e) {\n+        String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n+        LOG.error(msg);\n+        throw new IllegalArgumentException(msg, e);\n+      }\n+    }\n \n     maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n     partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n       DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       partSizeThreshold \u003d 5 * 1024 * 1024;\n     }\n \n     int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n     int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n     if (maxThreads \u003d\u003d 0) {\n       maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     if (coreThreads \u003d\u003d 0) {\n       coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n     }\n     long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n     LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n       new LinkedBlockingQueue\u003cRunnable\u003e(maxThreads *\n         conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n     ThreadPoolExecutor tpe \u003d new ThreadPoolExecutor(\n         coreThreads,\n         maxThreads,\n         keepAliveTime,\n         TimeUnit.SECONDS,\n         workQueue,\n         newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n     tpe.allowCoreThreadTimeOut(true);\n \n     TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n     transferConfiguration.setMinimumUploadPartSize(partSize);\n     transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n \n     transfers \u003d new TransferManager(s3, tpe);\n     transfers.setConfiguration(transferConfiguration);\n \n     String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n     if (!cannedACLName.isEmpty()) {\n       cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n     } else {\n       cannedACL \u003d null;\n     }\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n       DEFAULT_PURGE_EXISTING_MULTIPART);\n     long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n       DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n \n     if (purgeExistingMultipart) {\n       Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n \n       transfers.abortMultipartUploads(bucket, purgeBefore);\n       transfers.shutdownNow(false);\n     }\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    // Try to get our credentials or just connect anonymously\n    S3Credentials s3Credentials \u003d new S3Credentials();\n    s3Credentials.initialize(name, conf);\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                        s3Credentials.getSecretAccessKey()),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n      DEFAULT_MAXIMUM_CONNECTIONS));\n    awsConf.setProtocol(conf.getBoolean(SECURE_CONNECTIONS, \n      DEFAULT_SECURE_CONNECTIONS) ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n      DEFAULT_SOCKET_TIMEOUT));\n\n    s3 \u003d new AmazonS3Client(credentials, awsConf);\n    String endPoint \u003d conf.getTrimmed(ENDPOINT,\"\");\n    if (!endPoint.isEmpty()) {\n      try {\n        s3.setEndpoint(endPoint);\n      } catch (IllegalArgumentException e) {\n        String msg \u003d \"Incorrect endpoint: \"  + e.getMessage();\n        LOG.error(msg);\n        throw new IllegalArgumentException(msg, e);\n      }\n    }\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      partSizeThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n    if (maxThreads \u003d\u003d 0) {\n      maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    if (coreThreads \u003d\u003d 0) {\n      coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n      new LinkedBlockingQueue\u003cRunnable\u003e(maxThreads *\n        conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n    ThreadPoolExecutor tpe \u003d new ThreadPoolExecutor(\n        coreThreads,\n        maxThreads,\n        keepAliveTime,\n        TimeUnit.SECONDS,\n        workQueue,\n        newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n    tpe.allowCoreThreadTimeOut(true);\n\n    TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n    transferConfiguration.setMinimumUploadPartSize(partSize);\n    transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n\n    transfers \u003d new TransferManager(s3, tpe);\n    transfers.setConfiguration(transferConfiguration);\n\n    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n    if (!cannedACLName.isEmpty()) {\n      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL \u003d null;\n    }\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n      DEFAULT_PURGE_EXISTING_MULTIPART);\n    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n\n    if (purgeExistingMultipart) {\n      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n\n      transfers.abortMultipartUploads(bucket, purgeBefore);\n      transfers.shutdownNow(false);\n    }\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "27d8395867f665fea1360087325cda5ed70efd0c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11446 S3AOutputStream should use shared thread pool to avoid OutOfMemoryError\n",
      "commitDate": "05/01/15 5:00 AM",
      "commitName": "27d8395867f665fea1360087325cda5ed70efd0c",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "09/12/14 8:45 PM",
      "commitNameOld": "2e98ad34ce64a9e5184c53447004de20a637f927",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 26.34,
      "commitsBetweenForRepo": 126,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,102 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n-\n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n     // Try to get our credentials or just connect anonymously\n     S3Credentials s3Credentials \u003d new S3Credentials();\n     s3Credentials.initialize(name, conf);\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n         new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                         s3Credentials.getSecretAccessKey()),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n       DEFAULT_MAXIMUM_CONNECTIONS));\n     awsConf.setProtocol(conf.getBoolean(SECURE_CONNECTIONS, \n       DEFAULT_SECURE_CONNECTIONS) ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n       DEFAULT_MAX_ERROR_RETRIES));\n     awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n       DEFAULT_SOCKET_TIMEOUT));\n \n     s3 \u003d new AmazonS3Client(credentials, awsConf);\n \n     maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n     partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n       DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       partSizeThreshold \u003d 5 * 1024 * 1024;\n     }\n \n+    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n+    int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n+    if (maxThreads \u003d\u003d 0) {\n+      maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n+    }\n+    if (coreThreads \u003d\u003d 0) {\n+      coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n+    }\n+    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n+    LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n+      new LinkedBlockingQueue\u003cRunnable\u003e(maxThreads *\n+        conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n+    ThreadPoolExecutor tpe \u003d new ThreadPoolExecutor(\n+        coreThreads,\n+        maxThreads,\n+        keepAliveTime,\n+        TimeUnit.SECONDS,\n+        workQueue,\n+        newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n+    tpe.allowCoreThreadTimeOut(true);\n+\n+    TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n+    transferConfiguration.setMinimumUploadPartSize(partSize);\n+    transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n+\n+    transfers \u003d new TransferManager(s3, tpe);\n+    transfers.setConfiguration(transferConfiguration);\n+\n     String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n     if (!cannedACLName.isEmpty()) {\n       cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n     } else {\n       cannedACL \u003d null;\n     }\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n       DEFAULT_PURGE_EXISTING_MULTIPART);\n     long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n       DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n \n     if (purgeExistingMultipart) {\n-      TransferManager transferManager \u003d new TransferManager(s3);\n       Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n \n-      transferManager.abortMultipartUploads(bucket, purgeBefore);\n-      transferManager.shutdownNow(false);\n+      transfers.abortMultipartUploads(bucket, purgeBefore);\n+      transfers.shutdownNow(false);\n     }\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    // Try to get our credentials or just connect anonymously\n    S3Credentials s3Credentials \u003d new S3Credentials();\n    s3Credentials.initialize(name, conf);\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                        s3Credentials.getSecretAccessKey()),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n      DEFAULT_MAXIMUM_CONNECTIONS));\n    awsConf.setProtocol(conf.getBoolean(SECURE_CONNECTIONS, \n      DEFAULT_SECURE_CONNECTIONS) ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n      DEFAULT_SOCKET_TIMEOUT));\n\n    s3 \u003d new AmazonS3Client(credentials, awsConf);\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      partSizeThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    int maxThreads \u003d conf.getInt(MAX_THREADS, DEFAULT_MAX_THREADS);\n    int coreThreads \u003d conf.getInt(CORE_THREADS, DEFAULT_CORE_THREADS);\n    if (maxThreads \u003d\u003d 0) {\n      maxThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    if (coreThreads \u003d\u003d 0) {\n      coreThreads \u003d Runtime.getRuntime().availableProcessors() * 8;\n    }\n    long keepAliveTime \u003d conf.getLong(KEEPALIVE_TIME, DEFAULT_KEEPALIVE_TIME);\n    LinkedBlockingQueue\u003cRunnable\u003e workQueue \u003d\n      new LinkedBlockingQueue\u003cRunnable\u003e(maxThreads *\n        conf.getInt(MAX_TOTAL_TASKS, DEFAULT_MAX_TOTAL_TASKS));\n    ThreadPoolExecutor tpe \u003d new ThreadPoolExecutor(\n        coreThreads,\n        maxThreads,\n        keepAliveTime,\n        TimeUnit.SECONDS,\n        workQueue,\n        newDaemonThreadFactory(\"s3a-transfer-shared-\"));\n    tpe.allowCoreThreadTimeOut(true);\n\n    TransferManagerConfiguration transferConfiguration \u003d new TransferManagerConfiguration();\n    transferConfiguration.setMinimumUploadPartSize(partSize);\n    transferConfiguration.setMultipartUploadThreshold(partSizeThreshold);\n\n    transfers \u003d new TransferManager(s3, tpe);\n    transfers.setConfiguration(transferConfiguration);\n\n    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n    if (!cannedACLName.isEmpty()) {\n      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL \u003d null;\n    }\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n      DEFAULT_PURGE_EXISTING_MULTIPART);\n    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n\n    if (purgeExistingMultipart) {\n      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n\n      transfers.abortMultipartUploads(bucket, purgeBefore);\n      transfers.shutdownNow(false);\n    }\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "6ba52d88ec11444cbac946ffadbc645acd0657de": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10714. AmazonS3Client.deleteObjects() need to be limited to 1000 entries per call. Contributed by Juan Yu.\n",
      "commitDate": "05/11/14 5:17 PM",
      "commitName": "6ba52d88ec11444cbac946ffadbc645acd0657de",
      "commitAuthor": "Aaron T. Myers",
      "commitDateOld": "15/09/14 4:49 PM",
      "commitNameOld": "0ac760a58d96b36ab30e9d60679bbea6365ef120",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 51.06,
      "commitsBetweenForRepo": 536,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,86 +1,76 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n     // Try to get our credentials or just connect anonymously\n-    String accessKey \u003d conf.get(ACCESS_KEY, null);\n-    String secretKey \u003d conf.get(SECRET_KEY, null);\n-\n-    String userInfo \u003d name.getUserInfo();\n-    if (userInfo !\u003d null) {\n-      int index \u003d userInfo.indexOf(\u0027:\u0027);\n-      if (index !\u003d -1) {\n-        accessKey \u003d userInfo.substring(0, index);\n-        secretKey \u003d userInfo.substring(index + 1);\n-      } else {\n-        accessKey \u003d userInfo;\n-      }\n-    }\n+    S3Credentials s3Credentials \u003d new S3Credentials();\n+    s3Credentials.initialize(name, conf);\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n-        new BasicAWSCredentialsProvider(accessKey, secretKey),\n+        new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n+                                        s3Credentials.getSecretAccessKey()),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n     awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n       DEFAULT_MAXIMUM_CONNECTIONS));\n     awsConf.setProtocol(conf.getBoolean(SECURE_CONNECTIONS, \n       DEFAULT_SECURE_CONNECTIONS) ?  Protocol.HTTPS : Protocol.HTTP);\n     awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n       DEFAULT_MAX_ERROR_RETRIES));\n     awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n       DEFAULT_SOCKET_TIMEOUT));\n \n     s3 \u003d new AmazonS3Client(credentials, awsConf);\n \n     maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n     partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n     partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n       DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n       LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n       LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       partSizeThreshold \u003d 5 * 1024 * 1024;\n     }\n \n     String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n     if (!cannedACLName.isEmpty()) {\n       cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n     } else {\n       cannedACL \u003d null;\n     }\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n     boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n       DEFAULT_PURGE_EXISTING_MULTIPART);\n     long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n       DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n \n     if (purgeExistingMultipart) {\n       TransferManager transferManager \u003d new TransferManager(s3);\n       Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n \n       transferManager.abortMultipartUploads(bucket, purgeBefore);\n       transferManager.shutdownNow(false);\n     }\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    // Try to get our credentials or just connect anonymously\n    S3Credentials s3Credentials \u003d new S3Credentials();\n    s3Credentials.initialize(name, conf);\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(s3Credentials.getAccessKey(),\n                                        s3Credentials.getSecretAccessKey()),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n      DEFAULT_MAXIMUM_CONNECTIONS));\n    awsConf.setProtocol(conf.getBoolean(SECURE_CONNECTIONS, \n      DEFAULT_SECURE_CONNECTIONS) ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n      DEFAULT_SOCKET_TIMEOUT));\n\n    s3 \u003d new AmazonS3Client(credentials, awsConf);\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      partSizeThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n    if (!cannedACLName.isEmpty()) {\n      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL \u003d null;\n    }\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n      DEFAULT_PURGE_EXISTING_MULTIPART);\n    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n\n    if (purgeExistingMultipart) {\n      TransferManager transferManager \u003d new TransferManager(s3);\n      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n\n      transferManager.abortMultipartUploads(bucket, purgeBefore);\n      transferManager.shutdownNow(false);\n    }\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "0ac760a58d96b36ab30e9d60679bbea6365ef120": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11091. Eliminate old configuration parameter names from s3a (dsw via cmccabe)\n",
      "commitDate": "15/09/14 4:49 PM",
      "commitName": "0ac760a58d96b36ab30e9d60679bbea6365ef120",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "15/09/14 8:27 AM",
      "commitNameOld": "24d920b80eb3626073925a1d0b6dcf148add8cc0",
      "commitAuthorOld": "Aaron T. Myers",
      "daysBetweenCommits": 0.35,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,86 @@\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n \n \n     uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n     workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n         this.getWorkingDirectory());\n \n     // Try to get our credentials or just connect anonymously\n-    String accessKey \u003d conf.get(NEW_ACCESS_KEY, conf.get(OLD_ACCESS_KEY, null));\n-    String secretKey \u003d conf.get(NEW_SECRET_KEY, conf.get(OLD_SECRET_KEY, null));\n+    String accessKey \u003d conf.get(ACCESS_KEY, null);\n+    String secretKey \u003d conf.get(SECRET_KEY, null);\n \n     String userInfo \u003d name.getUserInfo();\n     if (userInfo !\u003d null) {\n       int index \u003d userInfo.indexOf(\u0027:\u0027);\n       if (index !\u003d -1) {\n         accessKey \u003d userInfo.substring(0, index);\n         secretKey \u003d userInfo.substring(index + 1);\n       } else {\n         accessKey \u003d userInfo;\n       }\n     }\n \n     AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n         new BasicAWSCredentialsProvider(accessKey, secretKey),\n         new InstanceProfileCredentialsProvider(),\n         new AnonymousAWSCredentialsProvider()\n     );\n \n     bucket \u003d name.getHost();\n \n     ClientConfiguration awsConf \u003d new ClientConfiguration();\n-    awsConf.setMaxConnections(conf.getInt(NEW_MAXIMUM_CONNECTIONS, \n-      conf.getInt(OLD_MAXIMUM_CONNECTIONS, DEFAULT_MAXIMUM_CONNECTIONS)));\n-    awsConf.setProtocol(conf.getBoolean(NEW_SECURE_CONNECTIONS, \n-      conf.getBoolean(OLD_SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS)) ? \n-        Protocol.HTTPS : Protocol.HTTP);\n-    awsConf.setMaxErrorRetry(conf.getInt(NEW_MAX_ERROR_RETRIES, \n-      conf.getInt(OLD_MAX_ERROR_RETRIES, DEFAULT_MAX_ERROR_RETRIES)));\n-    awsConf.setSocketTimeout(conf.getInt(NEW_SOCKET_TIMEOUT, \n-      conf.getInt(OLD_SOCKET_TIMEOUT, DEFAULT_SOCKET_TIMEOUT)));\n+    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n+      DEFAULT_MAXIMUM_CONNECTIONS));\n+    awsConf.setProtocol(conf.getBoolean(SECURE_CONNECTIONS, \n+      DEFAULT_SECURE_CONNECTIONS) ?  Protocol.HTTPS : Protocol.HTTP);\n+    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n+      DEFAULT_MAX_ERROR_RETRIES));\n+    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n+      DEFAULT_SOCKET_TIMEOUT));\n \n     s3 \u003d new AmazonS3Client(credentials, awsConf);\n \n-    maxKeys \u003d conf.getInt(NEW_MAX_PAGING_KEYS, \n-      conf.getInt(OLD_MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS));\n-    partSize \u003d conf.getLong(NEW_MULTIPART_SIZE, \n-      conf.getLong(OLD_MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE));\n-    partSizeThreshold \u003d conf.getInt(NEW_MIN_MULTIPART_THRESHOLD, \n-      conf.getInt(OLD_MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD));\n+    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n+    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n+    partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n+      DEFAULT_MIN_MULTIPART_THRESHOLD);\n \n     if (partSize \u003c 5 * 1024 * 1024) {\n-      LOG.error(NEW_MULTIPART_SIZE + \" must be at least 5 MB\");\n+      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n       partSize \u003d 5 * 1024 * 1024;\n     }\n \n     if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n-      LOG.error(NEW_MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n+      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n       partSizeThreshold \u003d 5 * 1024 * 1024;\n     }\n \n-    String cannedACLName \u003d conf.get(NEW_CANNED_ACL, \n-      conf.get(OLD_CANNED_ACL, DEFAULT_CANNED_ACL));\n+    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n     if (!cannedACLName.isEmpty()) {\n       cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n     } else {\n       cannedACL \u003d null;\n     }\n \n     if (!s3.doesBucketExist(bucket)) {\n       throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n     }\n \n-    boolean purgeExistingMultipart \u003d conf.getBoolean(NEW_PURGE_EXISTING_MULTIPART, \n-      conf.getBoolean(OLD_PURGE_EXISTING_MULTIPART, DEFAULT_PURGE_EXISTING_MULTIPART));\n-    long purgeExistingMultipartAge \u003d conf.getLong(NEW_PURGE_EXISTING_MULTIPART_AGE, \n-      conf.getLong(OLD_PURGE_EXISTING_MULTIPART_AGE, DEFAULT_PURGE_EXISTING_MULTIPART_AGE));\n+    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n+      DEFAULT_PURGE_EXISTING_MULTIPART);\n+    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n+      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n \n     if (purgeExistingMultipart) {\n       TransferManager transferManager \u003d new TransferManager(s3);\n       Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n \n       transferManager.abortMultipartUploads(bucket, purgeBefore);\n       transferManager.shutdownNow(false);\n     }\n \n     serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n \n     setConf(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    // Try to get our credentials or just connect anonymously\n    String accessKey \u003d conf.get(ACCESS_KEY, null);\n    String secretKey \u003d conf.get(SECRET_KEY, null);\n\n    String userInfo \u003d name.getUserInfo();\n    if (userInfo !\u003d null) {\n      int index \u003d userInfo.indexOf(\u0027:\u0027);\n      if (index !\u003d -1) {\n        accessKey \u003d userInfo.substring(0, index);\n        secretKey \u003d userInfo.substring(index + 1);\n      } else {\n        accessKey \u003d userInfo;\n      }\n    }\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(accessKey, secretKey),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(MAXIMUM_CONNECTIONS, \n      DEFAULT_MAXIMUM_CONNECTIONS));\n    awsConf.setProtocol(conf.getBoolean(SECURE_CONNECTIONS, \n      DEFAULT_SECURE_CONNECTIONS) ?  Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(MAX_ERROR_RETRIES, \n      DEFAULT_MAX_ERROR_RETRIES));\n    awsConf.setSocketTimeout(conf.getInt(SOCKET_TIMEOUT, \n      DEFAULT_SOCKET_TIMEOUT));\n\n    s3 \u003d new AmazonS3Client(credentials, awsConf);\n\n    maxKeys \u003d conf.getInt(MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS);\n    partSize \u003d conf.getLong(MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE);\n    partSizeThreshold \u003d conf.getInt(MIN_MULTIPART_THRESHOLD, \n      DEFAULT_MIN_MULTIPART_THRESHOLD);\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      partSizeThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    String cannedACLName \u003d conf.get(CANNED_ACL, DEFAULT_CANNED_ACL);\n    if (!cannedACLName.isEmpty()) {\n      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL \u003d null;\n    }\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    boolean purgeExistingMultipart \u003d conf.getBoolean(PURGE_EXISTING_MULTIPART, \n      DEFAULT_PURGE_EXISTING_MULTIPART);\n    long purgeExistingMultipartAge \u003d conf.getLong(PURGE_EXISTING_MULTIPART_AGE, \n      DEFAULT_PURGE_EXISTING_MULTIPART_AGE);\n\n    if (purgeExistingMultipart) {\n      TransferManager transferManager \u003d new TransferManager(s3);\n      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n\n      transferManager.abortMultipartUploads(bucket, purgeBefore);\n      transferManager.shutdownNow(false);\n    }\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java",
      "extendedDetails": {}
    },
    "24d920b80eb3626073925a1d0b6dcf148add8cc0": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-10400. Incorporate new S3A FileSystem implementation. Contributed by Jordan Mendelson and Dave Wang.\n",
      "commitDate": "15/09/14 8:27 AM",
      "commitName": "24d920b80eb3626073925a1d0b6dcf148add8cc0",
      "commitAuthor": "Aaron T. Myers",
      "diff": "@@ -0,0 +1,90 @@\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    super.initialize(name, conf);\n+\n+\n+    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n+    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n+        this.getWorkingDirectory());\n+\n+    // Try to get our credentials or just connect anonymously\n+    String accessKey \u003d conf.get(NEW_ACCESS_KEY, conf.get(OLD_ACCESS_KEY, null));\n+    String secretKey \u003d conf.get(NEW_SECRET_KEY, conf.get(OLD_SECRET_KEY, null));\n+\n+    String userInfo \u003d name.getUserInfo();\n+    if (userInfo !\u003d null) {\n+      int index \u003d userInfo.indexOf(\u0027:\u0027);\n+      if (index !\u003d -1) {\n+        accessKey \u003d userInfo.substring(0, index);\n+        secretKey \u003d userInfo.substring(index + 1);\n+      } else {\n+        accessKey \u003d userInfo;\n+      }\n+    }\n+\n+    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n+        new BasicAWSCredentialsProvider(accessKey, secretKey),\n+        new InstanceProfileCredentialsProvider(),\n+        new AnonymousAWSCredentialsProvider()\n+    );\n+\n+    bucket \u003d name.getHost();\n+\n+    ClientConfiguration awsConf \u003d new ClientConfiguration();\n+    awsConf.setMaxConnections(conf.getInt(NEW_MAXIMUM_CONNECTIONS, \n+      conf.getInt(OLD_MAXIMUM_CONNECTIONS, DEFAULT_MAXIMUM_CONNECTIONS)));\n+    awsConf.setProtocol(conf.getBoolean(NEW_SECURE_CONNECTIONS, \n+      conf.getBoolean(OLD_SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS)) ? \n+        Protocol.HTTPS : Protocol.HTTP);\n+    awsConf.setMaxErrorRetry(conf.getInt(NEW_MAX_ERROR_RETRIES, \n+      conf.getInt(OLD_MAX_ERROR_RETRIES, DEFAULT_MAX_ERROR_RETRIES)));\n+    awsConf.setSocketTimeout(conf.getInt(NEW_SOCKET_TIMEOUT, \n+      conf.getInt(OLD_SOCKET_TIMEOUT, DEFAULT_SOCKET_TIMEOUT)));\n+\n+    s3 \u003d new AmazonS3Client(credentials, awsConf);\n+\n+    maxKeys \u003d conf.getInt(NEW_MAX_PAGING_KEYS, \n+      conf.getInt(OLD_MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS));\n+    partSize \u003d conf.getLong(NEW_MULTIPART_SIZE, \n+      conf.getLong(OLD_MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE));\n+    partSizeThreshold \u003d conf.getInt(NEW_MIN_MULTIPART_THRESHOLD, \n+      conf.getInt(OLD_MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD));\n+\n+    if (partSize \u003c 5 * 1024 * 1024) {\n+      LOG.error(NEW_MULTIPART_SIZE + \" must be at least 5 MB\");\n+      partSize \u003d 5 * 1024 * 1024;\n+    }\n+\n+    if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n+      LOG.error(NEW_MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n+      partSizeThreshold \u003d 5 * 1024 * 1024;\n+    }\n+\n+    String cannedACLName \u003d conf.get(NEW_CANNED_ACL, \n+      conf.get(OLD_CANNED_ACL, DEFAULT_CANNED_ACL));\n+    if (!cannedACLName.isEmpty()) {\n+      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n+    } else {\n+      cannedACL \u003d null;\n+    }\n+\n+    if (!s3.doesBucketExist(bucket)) {\n+      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n+    }\n+\n+    boolean purgeExistingMultipart \u003d conf.getBoolean(NEW_PURGE_EXISTING_MULTIPART, \n+      conf.getBoolean(OLD_PURGE_EXISTING_MULTIPART, DEFAULT_PURGE_EXISTING_MULTIPART));\n+    long purgeExistingMultipartAge \u003d conf.getLong(NEW_PURGE_EXISTING_MULTIPART_AGE, \n+      conf.getLong(OLD_PURGE_EXISTING_MULTIPART_AGE, DEFAULT_PURGE_EXISTING_MULTIPART_AGE));\n+\n+    if (purgeExistingMultipart) {\n+      TransferManager transferManager \u003d new TransferManager(s3);\n+      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n+\n+      transferManager.abortMultipartUploads(bucket, purgeBefore);\n+      transferManager.shutdownNow(false);\n+    }\n+\n+    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n+\n+    setConf(conf);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI name, Configuration conf) throws IOException {\n    super.initialize(name, conf);\n\n\n    uri \u003d URI.create(name.getScheme() + \"://\" + name.getAuthority());\n    workingDir \u003d new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this.uri,\n        this.getWorkingDirectory());\n\n    // Try to get our credentials or just connect anonymously\n    String accessKey \u003d conf.get(NEW_ACCESS_KEY, conf.get(OLD_ACCESS_KEY, null));\n    String secretKey \u003d conf.get(NEW_SECRET_KEY, conf.get(OLD_SECRET_KEY, null));\n\n    String userInfo \u003d name.getUserInfo();\n    if (userInfo !\u003d null) {\n      int index \u003d userInfo.indexOf(\u0027:\u0027);\n      if (index !\u003d -1) {\n        accessKey \u003d userInfo.substring(0, index);\n        secretKey \u003d userInfo.substring(index + 1);\n      } else {\n        accessKey \u003d userInfo;\n      }\n    }\n\n    AWSCredentialsProviderChain credentials \u003d new AWSCredentialsProviderChain(\n        new BasicAWSCredentialsProvider(accessKey, secretKey),\n        new InstanceProfileCredentialsProvider(),\n        new AnonymousAWSCredentialsProvider()\n    );\n\n    bucket \u003d name.getHost();\n\n    ClientConfiguration awsConf \u003d new ClientConfiguration();\n    awsConf.setMaxConnections(conf.getInt(NEW_MAXIMUM_CONNECTIONS, \n      conf.getInt(OLD_MAXIMUM_CONNECTIONS, DEFAULT_MAXIMUM_CONNECTIONS)));\n    awsConf.setProtocol(conf.getBoolean(NEW_SECURE_CONNECTIONS, \n      conf.getBoolean(OLD_SECURE_CONNECTIONS, DEFAULT_SECURE_CONNECTIONS)) ? \n        Protocol.HTTPS : Protocol.HTTP);\n    awsConf.setMaxErrorRetry(conf.getInt(NEW_MAX_ERROR_RETRIES, \n      conf.getInt(OLD_MAX_ERROR_RETRIES, DEFAULT_MAX_ERROR_RETRIES)));\n    awsConf.setSocketTimeout(conf.getInt(NEW_SOCKET_TIMEOUT, \n      conf.getInt(OLD_SOCKET_TIMEOUT, DEFAULT_SOCKET_TIMEOUT)));\n\n    s3 \u003d new AmazonS3Client(credentials, awsConf);\n\n    maxKeys \u003d conf.getInt(NEW_MAX_PAGING_KEYS, \n      conf.getInt(OLD_MAX_PAGING_KEYS, DEFAULT_MAX_PAGING_KEYS));\n    partSize \u003d conf.getLong(NEW_MULTIPART_SIZE, \n      conf.getLong(OLD_MULTIPART_SIZE, DEFAULT_MULTIPART_SIZE));\n    partSizeThreshold \u003d conf.getInt(NEW_MIN_MULTIPART_THRESHOLD, \n      conf.getInt(OLD_MIN_MULTIPART_THRESHOLD, DEFAULT_MIN_MULTIPART_THRESHOLD));\n\n    if (partSize \u003c 5 * 1024 * 1024) {\n      LOG.error(NEW_MULTIPART_SIZE + \" must be at least 5 MB\");\n      partSize \u003d 5 * 1024 * 1024;\n    }\n\n    if (partSizeThreshold \u003c 5 * 1024 * 1024) {\n      LOG.error(NEW_MIN_MULTIPART_THRESHOLD + \" must be at least 5 MB\");\n      partSizeThreshold \u003d 5 * 1024 * 1024;\n    }\n\n    String cannedACLName \u003d conf.get(NEW_CANNED_ACL, \n      conf.get(OLD_CANNED_ACL, DEFAULT_CANNED_ACL));\n    if (!cannedACLName.isEmpty()) {\n      cannedACL \u003d CannedAccessControlList.valueOf(cannedACLName);\n    } else {\n      cannedACL \u003d null;\n    }\n\n    if (!s3.doesBucketExist(bucket)) {\n      throw new IOException(\"Bucket \" + bucket + \" does not exist\");\n    }\n\n    boolean purgeExistingMultipart \u003d conf.getBoolean(NEW_PURGE_EXISTING_MULTIPART, \n      conf.getBoolean(OLD_PURGE_EXISTING_MULTIPART, DEFAULT_PURGE_EXISTING_MULTIPART));\n    long purgeExistingMultipartAge \u003d conf.getLong(NEW_PURGE_EXISTING_MULTIPART_AGE, \n      conf.getLong(OLD_PURGE_EXISTING_MULTIPART_AGE, DEFAULT_PURGE_EXISTING_MULTIPART_AGE));\n\n    if (purgeExistingMultipart) {\n      TransferManager transferManager \u003d new TransferManager(s3);\n      Date purgeBefore \u003d new Date(new Date().getTime() - purgeExistingMultipartAge*1000);\n\n      transferManager.abortMultipartUploads(bucket, purgeBefore);\n      transferManager.shutdownNow(false);\n    }\n\n    serverSideEncryptionAlgorithm \u003d conf.get(SERVER_SIDE_ENCRYPTION_ALGORITHM);\n\n    setConf(conf);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java"
    }
  }
}