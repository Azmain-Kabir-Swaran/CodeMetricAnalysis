{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PBHelperClient.java",
  "functionName": "convert",
  "functionId": "convert___fs-FsServerDefaults",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
  "functionStartLine": 2244,
  "functionEndLine": 2260,
  "numCommitsSeen": 234,
  "timeTaken": 8960,
  "changeHistory": [
    "b76b843c8bd3906aaa5ad633d8a939aebc671907",
    "23b1a7bdf1b546c1e29d7010cf139b6d700461fc",
    "18432130a7f580f206adf023507678c534487f2e",
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
    "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21",
    "e0ce1b247550c6c89c292fb328c91d4b091a1473",
    "b0ea77303ba62a400376ca32c63c5b138f32cbe7",
    "18c5bc86ca256beb9d4ccd6588c0b0ebe9dfcbd0",
    "9b4a7900c7dfc0590316eedaa97144f938885651",
    "48da033901d3471ef176a94104158546152353e9",
    "7a59150bff64fc81f838de586eacd6d062172605",
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9"
  ],
  "changeHistoryShort": {
    "b76b843c8bd3906aaa5ad633d8a939aebc671907": "Ybodychange",
    "23b1a7bdf1b546c1e29d7010cf139b6d700461fc": "Ybodychange",
    "18432130a7f580f206adf023507678c534487f2e": "Ybodychange",
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ymultichange(Ymovefromfile,Ybodychange)",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": "Ybodychange",
    "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21": "Ybodychange",
    "e0ce1b247550c6c89c292fb328c91d4b091a1473": "Ybodychange",
    "b0ea77303ba62a400376ca32c63c5b138f32cbe7": "Ybodychange",
    "18c5bc86ca256beb9d4ccd6588c0b0ebe9dfcbd0": "Ybodychange",
    "9b4a7900c7dfc0590316eedaa97144f938885651": "Ybodychange",
    "48da033901d3471ef176a94104158546152353e9": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
    "7a59150bff64fc81f838de586eacd6d062172605": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b76b843c8bd3906aaa5ad633d8a939aebc671907": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13371. NPE for FsServerDefaults.getKeyProviderUri() for clientProtocol communication between 2.7 and 3.X. Contributed by Sherwood Zheng.\n",
      "commitDate": "24/06/19 5:52 PM",
      "commitName": "b76b843c8bd3906aaa5ad633d8a939aebc671907",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "13/02/19 12:40 PM",
      "commitNameOld": "024c87291cb4cc67282fe5645fb827427cc581c6",
      "commitAuthorOld": "Chen Liang",
      "daysBetweenCommits": 131.18,
      "commitsBetweenForRepo": 1009,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,17 @@\n   public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n     if (fs \u003d\u003d null) return null;\n-    return FsServerDefaultsProto.newBuilder().\n+    FsServerDefaultsProto.Builder builder \u003d FsServerDefaultsProto.newBuilder().\n         setBlockSize(fs.getBlockSize()).\n         setBytesPerChecksum(fs.getBytesPerChecksum()).\n         setWritePacketSize(fs.getWritePacketSize())\n         .setReplication(fs.getReplication())\n         .setFileBufferSize(fs.getFileBufferSize())\n         .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n         .setTrashInterval(fs.getTrashInterval())\n         .setChecksumType(convert(fs.getChecksumType()))\n-        .setKeyProviderUri(fs.getKeyProviderUri())\n-        .setPolicyId(fs.getDefaultStoragePolicyId())\n-        .build();\n+        .setPolicyId(fs.getDefaultStoragePolicyId());\n+    if (fs.getKeyProviderUri() !\u003d null) {\n+      builder.setKeyProviderUri(fs.getKeyProviderUri());\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n    if (fs \u003d\u003d null) return null;\n    FsServerDefaultsProto.Builder builder \u003d FsServerDefaultsProto.newBuilder().\n        setBlockSize(fs.getBlockSize()).\n        setBytesPerChecksum(fs.getBytesPerChecksum()).\n        setWritePacketSize(fs.getWritePacketSize())\n        .setReplication(fs.getReplication())\n        .setFileBufferSize(fs.getFileBufferSize())\n        .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n        .setTrashInterval(fs.getTrashInterval())\n        .setChecksumType(convert(fs.getChecksumType()))\n        .setPolicyId(fs.getDefaultStoragePolicyId());\n    if (fs.getKeyProviderUri() !\u003d null) {\n      builder.setKeyProviderUri(fs.getKeyProviderUri());\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {}
    },
    "23b1a7bdf1b546c1e29d7010cf139b6d700461fc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11163. Mover should move the file blocks to default storage once policy is unset. Contributed by Surendra Singh Lilhore.\n",
      "commitDate": "11/04/17 10:03 PM",
      "commitName": "23b1a7bdf1b546c1e29d7010cf139b6d700461fc",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "04/04/17 1:38 PM",
      "commitNameOld": "18432130a7f580f206adf023507678c534487f2e",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 7.35,
      "commitsBetweenForRepo": 54,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,15 @@\n   public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n     if (fs \u003d\u003d null) return null;\n     return FsServerDefaultsProto.newBuilder().\n         setBlockSize(fs.getBlockSize()).\n         setBytesPerChecksum(fs.getBytesPerChecksum()).\n         setWritePacketSize(fs.getWritePacketSize())\n         .setReplication(fs.getReplication())\n         .setFileBufferSize(fs.getFileBufferSize())\n         .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n         .setTrashInterval(fs.getTrashInterval())\n         .setChecksumType(convert(fs.getChecksumType()))\n         .setKeyProviderUri(fs.getKeyProviderUri())\n+        .setPolicyId(fs.getDefaultStoragePolicyId())\n         .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n    if (fs \u003d\u003d null) return null;\n    return FsServerDefaultsProto.newBuilder().\n        setBlockSize(fs.getBlockSize()).\n        setBytesPerChecksum(fs.getBytesPerChecksum()).\n        setWritePacketSize(fs.getWritePacketSize())\n        .setReplication(fs.getReplication())\n        .setFileBufferSize(fs.getFileBufferSize())\n        .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n        .setTrashInterval(fs.getTrashInterval())\n        .setChecksumType(convert(fs.getChecksumType()))\n        .setKeyProviderUri(fs.getKeyProviderUri())\n        .setPolicyId(fs.getDefaultStoragePolicyId())\n        .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {}
    },
    "18432130a7f580f206adf023507678c534487f2e": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14104. Client should always ask namenode for kms provider path. Contributed by Rushabh S Shah.\n",
      "commitDate": "04/04/17 1:38 PM",
      "commitName": "18432130a7f580f206adf023507678c534487f2e",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "06/03/17 4:39 PM",
      "commitNameOld": "b5adc5c3011f111f86d232cb33ec522547f68a95",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 28.83,
      "commitsBetweenForRepo": 172,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,14 @@\n   public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n     if (fs \u003d\u003d null) return null;\n     return FsServerDefaultsProto.newBuilder().\n         setBlockSize(fs.getBlockSize()).\n         setBytesPerChecksum(fs.getBytesPerChecksum()).\n         setWritePacketSize(fs.getWritePacketSize())\n         .setReplication(fs.getReplication())\n         .setFileBufferSize(fs.getFileBufferSize())\n         .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n         .setTrashInterval(fs.getTrashInterval())\n         .setChecksumType(convert(fs.getChecksumType()))\n+        .setKeyProviderUri(fs.getKeyProviderUri())\n         .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n    if (fs \u003d\u003d null) return null;\n    return FsServerDefaultsProto.newBuilder().\n        setBlockSize(fs.getBlockSize()).\n        setBytesPerChecksum(fs.getBytesPerChecksum()).\n        setWritePacketSize(fs.getWritePacketSize())\n        .setReplication(fs.getReplication())\n        .setFileBufferSize(fs.getFileBufferSize())\n        .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n        .setTrashInterval(fs.getTrashInterval())\n        .setChecksumType(convert(fs.getChecksumType()))\n        .setKeyProviderUri(fs.getKeyProviderUri())\n        .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {}
    },
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
          "commitDate": "21/09/15 6:53 PM",
          "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "21/09/15 5:51 PM",
          "commitNameOld": "8e01b0d97ac3d74b049a801dfa1cc6e77d8f680a",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,13 +1,13 @@\n   public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n     if (fs \u003d\u003d null) return null;\n     return FsServerDefaultsProto.newBuilder().\n       setBlockSize(fs.getBlockSize()).\n       setBytesPerChecksum(fs.getBytesPerChecksum()).\n       setWritePacketSize(fs.getWritePacketSize())\n       .setReplication(fs.getReplication())\n       .setFileBufferSize(fs.getFileBufferSize())\n       .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n       .setTrashInterval(fs.getTrashInterval())\n-      .setChecksumType(PBHelperClient.convert(fs.getChecksumType()))\n+      .setChecksumType(convert(fs.getChecksumType()))\n       .build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n    if (fs \u003d\u003d null) return null;\n    return FsServerDefaultsProto.newBuilder().\n      setBlockSize(fs.getBlockSize()).\n      setBytesPerChecksum(fs.getBytesPerChecksum()).\n      setWritePacketSize(fs.getWritePacketSize())\n      .setReplication(fs.getReplication())\n      .setFileBufferSize(fs.getFileBufferSize())\n      .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n      .setTrashInterval(fs.getTrashInterval())\n      .setChecksumType(convert(fs.getChecksumType()))\n      .build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
            "oldMethodName": "convert",
            "newMethodName": "convert"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
          "commitDate": "21/09/15 6:53 PM",
          "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "21/09/15 5:51 PM",
          "commitNameOld": "8e01b0d97ac3d74b049a801dfa1cc6e77d8f680a",
          "commitAuthorOld": "Chris Douglas",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,13 +1,13 @@\n   public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n     if (fs \u003d\u003d null) return null;\n     return FsServerDefaultsProto.newBuilder().\n       setBlockSize(fs.getBlockSize()).\n       setBytesPerChecksum(fs.getBytesPerChecksum()).\n       setWritePacketSize(fs.getWritePacketSize())\n       .setReplication(fs.getReplication())\n       .setFileBufferSize(fs.getFileBufferSize())\n       .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n       .setTrashInterval(fs.getTrashInterval())\n-      .setChecksumType(PBHelperClient.convert(fs.getChecksumType()))\n+      .setChecksumType(convert(fs.getChecksumType()))\n       .build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n    if (fs \u003d\u003d null) return null;\n    return FsServerDefaultsProto.newBuilder().\n      setBlockSize(fs.getBlockSize()).\n      setBytesPerChecksum(fs.getBytesPerChecksum()).\n      setWritePacketSize(fs.getWritePacketSize())\n      .setReplication(fs.getReplication())\n      .setFileBufferSize(fs.getFileBufferSize())\n      .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n      .setTrashInterval(fs.getTrashInterval())\n      .setChecksumType(convert(fs.getChecksumType()))\n      .build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
          "extendedDetails": {}
        }
      ]
    },
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8934. Move ShortCircuitShm to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "22/08/15 1:31 PM",
      "commitName": "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "14/07/15 12:42 PM",
      "commitNameOld": "979c9ca2ca89e99dc7165abfa29c78d66de43d9a",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 39.03,
      "commitsBetweenForRepo": 217,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n   public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n     if (fs \u003d\u003d null) return null;\n     return FsServerDefaultsProto.newBuilder().\n       setBlockSize(fs.getBlockSize()).\n       setBytesPerChecksum(fs.getBytesPerChecksum()).\n       setWritePacketSize(fs.getWritePacketSize())\n       .setReplication(fs.getReplication())\n       .setFileBufferSize(fs.getFileBufferSize())\n       .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n       .setTrashInterval(fs.getTrashInterval())\n-      .setChecksumType(PBHelper.convert(fs.getChecksumType()))\n+      .setChecksumType(PBHelperClient.convert(fs.getChecksumType()))\n       .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n    if (fs \u003d\u003d null) return null;\n    return FsServerDefaultsProto.newBuilder().\n      setBlockSize(fs.getBlockSize()).\n      setBytesPerChecksum(fs.getBytesPerChecksum()).\n      setWritePacketSize(fs.getWritePacketSize())\n      .setReplication(fs.getReplication())\n      .setFileBufferSize(fs.getFileBufferSize())\n      .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n      .setTrashInterval(fs.getTrashInterval())\n      .setChecksumType(PBHelperClient.convert(fs.getChecksumType()))\n      .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4363. Combine PBHelper and HdfsProtoUtil and remove redundant methods. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1431088 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 1:20 PM",
      "commitName": "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "08/01/13 1:05 PM",
      "commitNameOld": "5cdb7e5ce7f0c3129749be8f29e2f11c0e0f2269",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 1.01,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n   public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n     if (fs \u003d\u003d null) return null;\n     return FsServerDefaultsProto.newBuilder().\n       setBlockSize(fs.getBlockSize()).\n       setBytesPerChecksum(fs.getBytesPerChecksum()).\n       setWritePacketSize(fs.getWritePacketSize())\n       .setReplication(fs.getReplication())\n       .setFileBufferSize(fs.getFileBufferSize())\n       .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n       .setTrashInterval(fs.getTrashInterval())\n-      .setChecksumType(HdfsProtoUtil.toProto(fs.getChecksumType()))\n+      .setChecksumType(PBHelper.convert(fs.getChecksumType()))\n       .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n    if (fs \u003d\u003d null) return null;\n    return FsServerDefaultsProto.newBuilder().\n      setBlockSize(fs.getBlockSize()).\n      setBytesPerChecksum(fs.getBytesPerChecksum()).\n      setWritePacketSize(fs.getWritePacketSize())\n      .setReplication(fs.getReplication())\n      .setFileBufferSize(fs.getFileBufferSize())\n      .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n      .setTrashInterval(fs.getTrashInterval())\n      .setChecksumType(PBHelper.convert(fs.getChecksumType()))\n      .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "e0ce1b247550c6c89c292fb328c91d4b091a1473": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4046. Rename ChecksumTypeProto enum NULL since it is illegal in C/C++. Contributed by Binglin Chang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1406011 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/11/12 3:49 PM",
      "commitName": "e0ce1b247550c6c89c292fb328c91d4b091a1473",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "24/08/12 1:38 PM",
      "commitNameOld": "a8ff29266934d6d1455cc2f0af16856dbbf1796d",
      "commitAuthorOld": "",
      "daysBetweenCommits": 73.13,
      "commitsBetweenForRepo": 418,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n   public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n     if (fs \u003d\u003d null) return null;\n     return FsServerDefaultsProto.newBuilder().\n       setBlockSize(fs.getBlockSize()).\n       setBytesPerChecksum(fs.getBytesPerChecksum()).\n       setWritePacketSize(fs.getWritePacketSize())\n       .setReplication(fs.getReplication())\n       .setFileBufferSize(fs.getFileBufferSize())\n       .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n       .setTrashInterval(fs.getTrashInterval())\n-      .setChecksumType(ChecksumTypeProto.valueOf(fs.getChecksumType().name()))\n+      .setChecksumType(HdfsProtoUtil.toProto(fs.getChecksumType()))\n       .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n    if (fs \u003d\u003d null) return null;\n    return FsServerDefaultsProto.newBuilder().\n      setBlockSize(fs.getBlockSize()).\n      setBytesPerChecksum(fs.getBytesPerChecksum()).\n      setWritePacketSize(fs.getWritePacketSize())\n      .setReplication(fs.getReplication())\n      .setFileBufferSize(fs.getFileBufferSize())\n      .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n      .setTrashInterval(fs.getTrashInterval())\n      .setChecksumType(HdfsProtoUtil.toProto(fs.getChecksumType()))\n      .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "b0ea77303ba62a400376ca32c63c5b138f32cbe7": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-8240. Add a new API to allow users to specify a checksum type on FileSystem.create(..).  Contributed by Kihwal Lee\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1374696 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/12 10:46 PM",
      "commitName": "b0ea77303ba62a400376ca32c63c5b138f32cbe7",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "17/08/12 4:22 PM",
      "commitNameOld": "18c5bc86ca256beb9d4ccd6588c0b0ebe9dfcbd0",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 1.27,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,11 +1,13 @@\n   public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n     if (fs \u003d\u003d null) return null;\n     return FsServerDefaultsProto.newBuilder().\n       setBlockSize(fs.getBlockSize()).\n       setBytesPerChecksum(fs.getBytesPerChecksum()).\n       setWritePacketSize(fs.getWritePacketSize())\n       .setReplication(fs.getReplication())\n       .setFileBufferSize(fs.getFileBufferSize())\n       .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n-      .setTrashInterval(fs.getTrashInterval()).build();\n+      .setTrashInterval(fs.getTrashInterval())\n+      .setChecksumType(ChecksumTypeProto.valueOf(fs.getChecksumType().name()))\n+      .build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n    if (fs \u003d\u003d null) return null;\n    return FsServerDefaultsProto.newBuilder().\n      setBlockSize(fs.getBlockSize()).\n      setBytesPerChecksum(fs.getBytesPerChecksum()).\n      setWritePacketSize(fs.getWritePacketSize())\n      .setReplication(fs.getReplication())\n      .setFileBufferSize(fs.getFileBufferSize())\n      .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n      .setTrashInterval(fs.getTrashInterval())\n      .setChecksumType(ChecksumTypeProto.valueOf(fs.getChecksumType().name()))\n      .build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "18c5bc86ca256beb9d4ccd6588c0b0ebe9dfcbd0": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-8689. Make trash a server side configuration option. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1374472 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/08/12 4:22 PM",
      "commitName": "18c5bc86ca256beb9d4ccd6588c0b0ebe9dfcbd0",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "07/08/12 9:40 AM",
      "commitNameOld": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 10.28,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,11 @@\n   public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n     if (fs \u003d\u003d null) return null;\n     return FsServerDefaultsProto.newBuilder().\n       setBlockSize(fs.getBlockSize()).\n       setBytesPerChecksum(fs.getBytesPerChecksum()).\n       setWritePacketSize(fs.getWritePacketSize())\n       .setReplication(fs.getReplication())\n       .setFileBufferSize(fs.getFileBufferSize())\n-      .setEncryptDataTransfer(fs.getEncryptDataTransfer()).build();\n+      .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n+      .setTrashInterval(fs.getTrashInterval()).build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n    if (fs \u003d\u003d null) return null;\n    return FsServerDefaultsProto.newBuilder().\n      setBlockSize(fs.getBlockSize()).\n      setBytesPerChecksum(fs.getBytesPerChecksum()).\n      setWritePacketSize(fs.getWritePacketSize())\n      .setReplication(fs.getReplication())\n      .setFileBufferSize(fs.getFileBufferSize())\n      .setEncryptDataTransfer(fs.getEncryptDataTransfer())\n      .setTrashInterval(fs.getTrashInterval()).build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "9b4a7900c7dfc0590316eedaa97144f938885651": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 9:40 AM",
      "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "15/05/12 12:03 PM",
      "commitNameOld": "7428aeca8666aeaf5f6682efbdb5349f44d1753e",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 83.9,
      "commitsBetweenForRepo": 412,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,7 +1,10 @@\n   public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n     if (fs \u003d\u003d null) return null;\n     return FsServerDefaultsProto.newBuilder().\n       setBlockSize(fs.getBlockSize()).\n       setBytesPerChecksum(fs.getBytesPerChecksum()).\n-      setWritePacketSize(fs.getWritePacketSize()).setReplication(fs.getReplication()).setFileBufferSize(fs.getFileBufferSize()).build();\n+      setWritePacketSize(fs.getWritePacketSize())\n+      .setReplication(fs.getReplication())\n+      .setFileBufferSize(fs.getFileBufferSize())\n+      .setEncryptDataTransfer(fs.getEncryptDataTransfer()).build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n    if (fs \u003d\u003d null) return null;\n    return FsServerDefaultsProto.newBuilder().\n      setBlockSize(fs.getBlockSize()).\n      setBytesPerChecksum(fs.getBytesPerChecksum()).\n      setWritePacketSize(fs.getWritePacketSize())\n      .setReplication(fs.getReplication())\n      .setFileBufferSize(fs.getFileBufferSize())\n      .setEncryptDataTransfer(fs.getEncryptDataTransfer()).build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "48da033901d3471ef176a94104158546152353e9": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/12/11 9:36 PM",
      "commitName": "48da033901d3471ef176a94104158546152353e9",
      "commitAuthor": "Sanjay Radia",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,7 @@\n-  public static BlockTokenIdentifierProto convert(\n-      Token\u003cBlockTokenIdentifier\u003e token) {\n-    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n-    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n-    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n-        .setKind(token.getKind().toString()).setPassword(password)\n-        .setService(token.getService().toString()).build();\n+  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n+    if (fs \u003d\u003d null) return null;\n+    return FsServerDefaultsProto.newBuilder().\n+      setBlockSize(fs.getBlockSize()).\n+      setBytesPerChecksum(fs.getBytesPerChecksum()).\n+      setWritePacketSize(fs.getWritePacketSize()).setReplication(fs.getReplication()).setFileBufferSize(fs.getFileBufferSize()).build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n    if (fs \u003d\u003d null) return null;\n    return FsServerDefaultsProto.newBuilder().\n      setBlockSize(fs.getBlockSize()).\n      setBytesPerChecksum(fs.getBytesPerChecksum()).\n      setWritePacketSize(fs.getWritePacketSize()).setReplication(fs.getReplication()).setFileBufferSize(fs.getFileBufferSize()).build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "[token-Token\u003cBlockTokenIdentifier\u003e]",
            "newValue": "[fs-FsServerDefaults]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,7 @@\n-  public static BlockTokenIdentifierProto convert(\n-      Token\u003cBlockTokenIdentifier\u003e token) {\n-    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n-    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n-    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n-        .setKind(token.getKind().toString()).setPassword(password)\n-        .setService(token.getService().toString()).build();\n+  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n+    if (fs \u003d\u003d null) return null;\n+    return FsServerDefaultsProto.newBuilder().\n+      setBlockSize(fs.getBlockSize()).\n+      setBytesPerChecksum(fs.getBytesPerChecksum()).\n+      setWritePacketSize(fs.getWritePacketSize()).setReplication(fs.getReplication()).setFileBufferSize(fs.getFileBufferSize()).build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n    if (fs \u003d\u003d null) return null;\n    return FsServerDefaultsProto.newBuilder().\n      setBlockSize(fs.getBlockSize()).\n      setBytesPerChecksum(fs.getBytesPerChecksum()).\n      setWritePacketSize(fs.getWritePacketSize()).setReplication(fs.getReplication()).setFileBufferSize(fs.getFileBufferSize()).build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "BlockTokenIdentifierProto",
            "newValue": "FsServerDefaultsProto"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,7 @@\n-  public static BlockTokenIdentifierProto convert(\n-      Token\u003cBlockTokenIdentifier\u003e token) {\n-    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n-    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n-    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n-        .setKind(token.getKind().toString()).setPassword(password)\n-        .setService(token.getService().toString()).build();\n+  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n+    if (fs \u003d\u003d null) return null;\n+    return FsServerDefaultsProto.newBuilder().\n+      setBlockSize(fs.getBlockSize()).\n+      setBytesPerChecksum(fs.getBytesPerChecksum()).\n+      setWritePacketSize(fs.getWritePacketSize()).setReplication(fs.getReplication()).setFileBufferSize(fs.getFileBufferSize()).build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static FsServerDefaultsProto convert(FsServerDefaults fs) {\n    if (fs \u003d\u003d null) return null;\n    return FsServerDefaultsProto.newBuilder().\n      setBlockSize(fs.getBlockSize()).\n      setBytesPerChecksum(fs.getBytesPerChecksum()).\n      setWritePacketSize(fs.getWritePacketSize()).setReplication(fs.getReplication()).setFileBufferSize(fs.getFileBufferSize()).build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {}
        }
      ]
    },
    "7a59150bff64fc81f838de586eacd6d062172605": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-2629. Implement protobuf service for InterDatanodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1211206 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/12/11 2:19 PM",
      "commitName": "7a59150bff64fc81f838de586eacd6d062172605",
      "commitAuthor": "Suresh Srinivas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2629. Implement protobuf service for InterDatanodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1211206 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/11 2:19 PM",
          "commitName": "7a59150bff64fc81f838de586eacd6d062172605",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "05/12/11 4:25 PM",
          "commitNameOld": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.91,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,8 @@\n-  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n-    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n-    int i \u003d 0;\n-    for (BlockWithLocationsProto entry : b) {\n-      ret[i++] \u003d convert(entry);\n-    }\n-    return ret;\n+  public static BlockTokenIdentifierProto convert(\n+      Token\u003cBlockTokenIdentifier\u003e token) {\n+    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n+    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n+    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n+        .setKind(token.getKind().toString()).setPassword(password)\n+        .setService(token.getService().toString()).build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockTokenIdentifierProto convert(\n      Token\u003cBlockTokenIdentifier\u003e token) {\n    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n        .setKind(token.getKind().toString()).setPassword(password)\n        .setService(token.getService().toString()).build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "[b-List\u003cBlockWithLocationsProto\u003e]",
            "newValue": "[token-Token\u003cBlockTokenIdentifier\u003e]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-2629. Implement protobuf service for InterDatanodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1211206 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/11 2:19 PM",
          "commitName": "7a59150bff64fc81f838de586eacd6d062172605",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "05/12/11 4:25 PM",
          "commitNameOld": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.91,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,8 @@\n-  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n-    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n-    int i \u003d 0;\n-    for (BlockWithLocationsProto entry : b) {\n-      ret[i++] \u003d convert(entry);\n-    }\n-    return ret;\n+  public static BlockTokenIdentifierProto convert(\n+      Token\u003cBlockTokenIdentifier\u003e token) {\n+    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n+    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n+    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n+        .setKind(token.getKind().toString()).setPassword(password)\n+        .setService(token.getService().toString()).build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockTokenIdentifierProto convert(\n      Token\u003cBlockTokenIdentifier\u003e token) {\n    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n        .setKind(token.getKind().toString()).setPassword(password)\n        .setService(token.getService().toString()).build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "BlockWithLocations[]",
            "newValue": "BlockTokenIdentifierProto"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2629. Implement protobuf service for InterDatanodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1211206 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/11 2:19 PM",
          "commitName": "7a59150bff64fc81f838de586eacd6d062172605",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "05/12/11 4:25 PM",
          "commitNameOld": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.91,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,8 @@\n-  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n-    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n-    int i \u003d 0;\n-    for (BlockWithLocationsProto entry : b) {\n-      ret[i++] \u003d convert(entry);\n-    }\n-    return ret;\n+  public static BlockTokenIdentifierProto convert(\n+      Token\u003cBlockTokenIdentifier\u003e token) {\n+    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n+    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n+    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n+        .setKind(token.getKind().toString()).setPassword(password)\n+        .setService(token.getService().toString()).build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockTokenIdentifierProto convert(\n      Token\u003cBlockTokenIdentifier\u003e token) {\n    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n        .setKind(token.getKind().toString()).setPassword(password)\n        .setService(token.getService().toString()).build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {}
        }
      ]
    },
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2618. Implement protobuf service for NamenodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1210719 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/12/11 4:25 PM",
      "commitName": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
      "commitAuthor": "Suresh Srinivas",
      "diff": "@@ -0,0 +1,8 @@\n+  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n+    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n+    int i \u003d 0;\n+    for (BlockWithLocationsProto entry : b) {\n+      ret[i++] \u003d convert(entry);\n+    }\n+    return ret;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n    int i \u003d 0;\n    for (BlockWithLocationsProto entry : b) {\n      ret[i++] \u003d convert(entry);\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java"
    }
  }
}