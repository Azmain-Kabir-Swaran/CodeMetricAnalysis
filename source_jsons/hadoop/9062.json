{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSNamesystem.java",
  "functionName": "delete",
  "functionId": "delete___src-String__recursive-boolean__logRetryCache-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
  "functionStartLine": 3266,
  "functionEndLine": 3295,
  "numCommitsSeen": 1366,
  "timeTaken": 56378,
  "changeHistory": [
    "1824aee9da4056de0fb638906b2172e486bbebe7",
    "4b95c242eca540455a4d5d0899aaf73b6064b5ea",
    "f600fbb6c4987c69292faea6b5abf022bb213ffd",
    "84a1321f6aa0af6895564a7c47f8f264656f0294",
    "ff0b99eafeda035ebe0dc82cfe689808047a8893",
    "3fa33b5c2c289ceaced30c6c5451f3569110459d",
    "7817674a3a4d097b647dd77f1345787dd376d5ea",
    "c79e7f7d997596e0c38ae4cddff2bd0910581c16",
    "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
    "8e253cb93030642f5a7324bad0f161cd0ad33206",
    "8c7a7e619699386f9e6991842558d78aa0c8053d",
    "1b531c1dbb452a6192fad411605d2baaa3831bcd",
    "d8ca9c655b3582596c756781f83253f644d1053f",
    "df2fb006b28bf1907fe3c54255e5f6bbb7698285",
    "d866f81edbd70121a9e29e5d25be67e1c464397e",
    "a85a0293c77f7cf0471d242458f04ec61e0129bc",
    "7d1c8d92f9b4b83c6ee154cd9ff70724bc61599f",
    "0270889b4e7f241620b2c3c297ec6530d96a7db5",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "7fac946ac983e31613fd62836c8ac9c4a579210a",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "1824aee9da4056de0fb638906b2172e486bbebe7": "Ybodychange",
    "4b95c242eca540455a4d5d0899aaf73b6064b5ea": "Ybodychange",
    "f600fbb6c4987c69292faea6b5abf022bb213ffd": "Ybodychange",
    "84a1321f6aa0af6895564a7c47f8f264656f0294": "Ybodychange",
    "ff0b99eafeda035ebe0dc82cfe689808047a8893": "Ybodychange",
    "3fa33b5c2c289ceaced30c6c5451f3569110459d": "Ybodychange",
    "7817674a3a4d097b647dd77f1345787dd376d5ea": "Ybodychange",
    "c79e7f7d997596e0c38ae4cddff2bd0910581c16": "Ybodychange",
    "24315e7d374a1ddd4329b64350cf96fc9ab6f59c": "Ymultichange(Yexceptionschange,Ybodychange)",
    "8e253cb93030642f5a7324bad0f161cd0ad33206": "Ymultichange(Yparameterchange,Ybodychange)",
    "8c7a7e619699386f9e6991842558d78aa0c8053d": "Ybodychange",
    "1b531c1dbb452a6192fad411605d2baaa3831bcd": "Ybodychange",
    "d8ca9c655b3582596c756781f83253f644d1053f": "Ybodychange",
    "df2fb006b28bf1907fe3c54255e5f6bbb7698285": "Ybodychange",
    "d866f81edbd70121a9e29e5d25be67e1c464397e": "Ybodychange",
    "a85a0293c77f7cf0471d242458f04ec61e0129bc": "Ybodychange",
    "7d1c8d92f9b4b83c6ee154cd9ff70724bc61599f": "Ybodychange",
    "0270889b4e7f241620b2c3c297ec6530d96a7db5": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "7fac946ac983e31613fd62836c8ac9c4a579210a": "Ymodifierchange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "1824aee9da4056de0fb638906b2172e486bbebe7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15217 Add more information to longest write/read lock held log\n\n",
      "commitDate": "18/04/20 1:52 PM",
      "commitName": "1824aee9da4056de0fb638906b2172e486bbebe7",
      "commitAuthor": "Toshihiro Suzuki",
      "commitDateOld": "25/03/20 10:28 AM",
      "commitNameOld": "a700803a18fb957d2799001a2ce1dcb70f75c080",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 24.14,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,30 @@\n   boolean delete(String src, boolean recursive, boolean logRetryCache)\n       throws IOException {\n     final String operationName \u003d \"delete\";\n     BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n     checkOperation(OperationCategory.WRITE);\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n     FSPermissionChecker.setOperationType(operationName);\n     boolean ret \u003d false;\n     try {\n       writeLock();\n       try {\n         checkOperation(OperationCategory.WRITE);\n         checkNameNodeSafeMode(\"Cannot delete \" + src);\n         toRemovedBlocks \u003d FSDirDeleteOp.delete(\n             this, pc, src, recursive, logRetryCache);\n         ret \u003d toRemovedBlocks !\u003d null;\n       } finally {\n-        writeUnlock(operationName);\n+        writeUnlock(operationName, getLockReportInfoSupplier(src));\n       }\n     } catch (AccessControlException e) {\n       logAuditEvent(false, operationName, src);\n       throw e;\n     }\n     getEditLog().logSync();\n     if (toRemovedBlocks !\u003d null) {\n       removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n     }\n     logAuditEvent(true, operationName, src);\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    final String operationName \u003d \"delete\";\n    BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    FSPermissionChecker.setOperationType(operationName);\n    boolean ret \u003d false;\n    try {\n      writeLock();\n      try {\n        checkOperation(OperationCategory.WRITE);\n        checkNameNodeSafeMode(\"Cannot delete \" + src);\n        toRemovedBlocks \u003d FSDirDeleteOp.delete(\n            this, pc, src, recursive, logRetryCache);\n        ret \u003d toRemovedBlocks !\u003d null;\n      } finally {\n        writeUnlock(operationName, getLockReportInfoSupplier(src));\n      }\n    } catch (AccessControlException e) {\n      logAuditEvent(false, operationName, src);\n      throw e;\n    }\n    getEditLog().logSync();\n    if (toRemovedBlocks !\u003d null) {\n      removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n    }\n    logAuditEvent(true, operationName, src);\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "4b95c242eca540455a4d5d0899aaf73b6064b5ea": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14743. Enhance INodeAttributeProvider/ AccessControlEnforcer Interface in HDFS to support Authorization of mkdir, rm, rmdir, copy, move etc... (#1829)\n\nReviewed-by: Xiaoyu Yao \u003cxyao@apache.org\u003e",
      "commitDate": "13/03/20 11:29 AM",
      "commitName": "4b95c242eca540455a4d5d0899aaf73b6064b5ea",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "27/02/20 8:49 AM",
      "commitNameOld": "cd2c6b1aac470991b9b90339ce2721ba179e7c48",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 15.07,
      "commitsBetweenForRepo": 51,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,30 @@\n   boolean delete(String src, boolean recursive, boolean logRetryCache)\n       throws IOException {\n     final String operationName \u003d \"delete\";\n     BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n     checkOperation(OperationCategory.WRITE);\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n+    FSPermissionChecker.setOperationType(operationName);\n     boolean ret \u003d false;\n     try {\n       writeLock();\n       try {\n         checkOperation(OperationCategory.WRITE);\n         checkNameNodeSafeMode(\"Cannot delete \" + src);\n         toRemovedBlocks \u003d FSDirDeleteOp.delete(\n             this, pc, src, recursive, logRetryCache);\n         ret \u003d toRemovedBlocks !\u003d null;\n       } finally {\n         writeUnlock(operationName);\n       }\n     } catch (AccessControlException e) {\n       logAuditEvent(false, operationName, src);\n       throw e;\n     }\n     getEditLog().logSync();\n     if (toRemovedBlocks !\u003d null) {\n       removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n     }\n     logAuditEvent(true, operationName, src);\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    final String operationName \u003d \"delete\";\n    BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    FSPermissionChecker.setOperationType(operationName);\n    boolean ret \u003d false;\n    try {\n      writeLock();\n      try {\n        checkOperation(OperationCategory.WRITE);\n        checkNameNodeSafeMode(\"Cannot delete \" + src);\n        toRemovedBlocks \u003d FSDirDeleteOp.delete(\n            this, pc, src, recursive, logRetryCache);\n        ret \u003d toRemovedBlocks !\u003d null;\n      } finally {\n        writeUnlock(operationName);\n      }\n    } catch (AccessControlException e) {\n      logAuditEvent(false, operationName, src);\n      throw e;\n    }\n    getEditLog().logSync();\n    if (toRemovedBlocks !\u003d null) {\n      removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n    }\n    logAuditEvent(true, operationName, src);\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "f600fbb6c4987c69292faea6b5abf022bb213ffd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11246. FSNameSystem#logAuditEvent should be called outside the read or write locks. Contributed by He Xiaoqiao, Kuhu Shukla.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\nCo-authored-by: Kuhu Shukla \u003ckshukla@apache.org\u003e\n",
      "commitDate": "29/08/19 10:10 AM",
      "commitName": "f600fbb6c4987c69292faea6b5abf022bb213ffd",
      "commitAuthor": "He Xiaoqiao",
      "commitDateOld": "27/08/19 3:26 PM",
      "commitNameOld": "dde9399b37bffb77da17c025f0b9b673d7088bc6",
      "commitAuthorOld": "He Xiaoqiao",
      "daysBetweenCommits": 1.78,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,29 @@\n   boolean delete(String src, boolean recursive, boolean logRetryCache)\n       throws IOException {\n     final String operationName \u003d \"delete\";\n     BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n     checkOperation(OperationCategory.WRITE);\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n-    writeLock();\n     boolean ret \u003d false;\n     try {\n-      checkOperation(OperationCategory.WRITE);\n-      checkNameNodeSafeMode(\"Cannot delete \" + src);\n-      toRemovedBlocks \u003d FSDirDeleteOp.delete(\n-          this, pc, src, recursive, logRetryCache);\n-      ret \u003d toRemovedBlocks !\u003d null;\n+      writeLock();\n+      try {\n+        checkOperation(OperationCategory.WRITE);\n+        checkNameNodeSafeMode(\"Cannot delete \" + src);\n+        toRemovedBlocks \u003d FSDirDeleteOp.delete(\n+            this, pc, src, recursive, logRetryCache);\n+        ret \u003d toRemovedBlocks !\u003d null;\n+      } finally {\n+        writeUnlock(operationName);\n+      }\n     } catch (AccessControlException e) {\n       logAuditEvent(false, operationName, src);\n       throw e;\n-    } finally {\n-      writeUnlock(operationName);\n     }\n     getEditLog().logSync();\n     if (toRemovedBlocks !\u003d null) {\n       removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n     }\n     logAuditEvent(true, operationName, src);\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    final String operationName \u003d \"delete\";\n    BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    boolean ret \u003d false;\n    try {\n      writeLock();\n      try {\n        checkOperation(OperationCategory.WRITE);\n        checkNameNodeSafeMode(\"Cannot delete \" + src);\n        toRemovedBlocks \u003d FSDirDeleteOp.delete(\n            this, pc, src, recursive, logRetryCache);\n        ret \u003d toRemovedBlocks !\u003d null;\n      } finally {\n        writeUnlock(operationName);\n      }\n    } catch (AccessControlException e) {\n      logAuditEvent(false, operationName, src);\n      throw e;\n    }\n    getEditLog().logSync();\n    if (toRemovedBlocks !\u003d null) {\n      removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n    }\n    logAuditEvent(true, operationName, src);\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "84a1321f6aa0af6895564a7c47f8f264656f0294": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13136. Avoid taking FSN lock while doing group member lookup for FSD permission check. Contributed by Xiaoyu Yao.\n",
      "commitDate": "22/02/18 11:32 AM",
      "commitName": "84a1321f6aa0af6895564a7c47f8f264656f0294",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "15/02/18 1:32 PM",
      "commitNameOld": "47473952e56b0380147d42f4110ad03c2276c961",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 6.92,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,27 @@\n   boolean delete(String src, boolean recursive, boolean logRetryCache)\n       throws IOException {\n     final String operationName \u003d \"delete\";\n     BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n+    checkOperation(OperationCategory.WRITE);\n+    final FSPermissionChecker pc \u003d getPermissionChecker();\n     writeLock();\n     boolean ret \u003d false;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n       toRemovedBlocks \u003d FSDirDeleteOp.delete(\n-          this, src, recursive, logRetryCache);\n+          this, pc, src, recursive, logRetryCache);\n       ret \u003d toRemovedBlocks !\u003d null;\n     } catch (AccessControlException e) {\n       logAuditEvent(false, operationName, src);\n       throw e;\n     } finally {\n       writeUnlock(operationName);\n     }\n     getEditLog().logSync();\n     if (toRemovedBlocks !\u003d null) {\n       removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n     }\n     logAuditEvent(true, operationName, src);\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    final String operationName \u003d \"delete\";\n    BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    writeLock();\n    boolean ret \u003d false;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      toRemovedBlocks \u003d FSDirDeleteOp.delete(\n          this, pc, src, recursive, logRetryCache);\n      ret \u003d toRemovedBlocks !\u003d null;\n    } catch (AccessControlException e) {\n      logAuditEvent(false, operationName, src);\n      throw e;\n    } finally {\n      writeUnlock(operationName);\n    }\n    getEditLog().logSync();\n    if (toRemovedBlocks !\u003d null) {\n      removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n    }\n    logAuditEvent(true, operationName, src);\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "ff0b99eafeda035ebe0dc82cfe689808047a8893": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10872. Add MutableRate metrics for FSNamesystemLock operations. Contributed by Erik Krogen.\n",
      "commitDate": "14/11/16 11:05 AM",
      "commitName": "ff0b99eafeda035ebe0dc82cfe689808047a8893",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "08/11/16 6:17 PM",
      "commitNameOld": "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 5.7,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,25 @@\n   boolean delete(String src, boolean recursive, boolean logRetryCache)\n       throws IOException {\n+    final String operationName \u003d \"delete\";\n     BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n     writeLock();\n     boolean ret \u003d false;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n       toRemovedBlocks \u003d FSDirDeleteOp.delete(\n           this, src, recursive, logRetryCache);\n       ret \u003d toRemovedBlocks !\u003d null;\n     } catch (AccessControlException e) {\n-      logAuditEvent(false, \"delete\", src);\n+      logAuditEvent(false, operationName, src);\n       throw e;\n     } finally {\n-      writeUnlock();\n+      writeUnlock(operationName);\n     }\n     getEditLog().logSync();\n     if (toRemovedBlocks !\u003d null) {\n       removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n     }\n-    logAuditEvent(true, \"delete\", src);\n+    logAuditEvent(true, operationName, src);\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    final String operationName \u003d \"delete\";\n    BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n    writeLock();\n    boolean ret \u003d false;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      toRemovedBlocks \u003d FSDirDeleteOp.delete(\n          this, src, recursive, logRetryCache);\n      ret \u003d toRemovedBlocks !\u003d null;\n    } catch (AccessControlException e) {\n      logAuditEvent(false, operationName, src);\n      throw e;\n    } finally {\n      writeUnlock(operationName);\n    }\n    getEditLog().logSync();\n    if (toRemovedBlocks !\u003d null) {\n      removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n    }\n    logAuditEvent(true, operationName, src);\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "3fa33b5c2c289ceaced30c6c5451f3569110459d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9430 Remove waitForLoadingFSImage since checkNNStartup has ensured image loaded and namenode started. (Brahma Reddy Battula via mingma)\n",
      "commitDate": "04/12/15 9:47 AM",
      "commitName": "3fa33b5c2c289ceaced30c6c5451f3569110459d",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "01/12/15 4:09 PM",
      "commitNameOld": "a49cc74b4c72195dee1dfb6f9548e5e411dff553",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.74,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,24 @@\n   boolean delete(String src, boolean recursive, boolean logRetryCache)\n       throws IOException {\n-    waitForLoadingFSImage();\n     BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n     writeLock();\n     boolean ret \u003d false;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n       toRemovedBlocks \u003d FSDirDeleteOp.delete(\n           this, src, recursive, logRetryCache);\n       ret \u003d toRemovedBlocks !\u003d null;\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"delete\", src);\n       throw e;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (toRemovedBlocks !\u003d null) {\n       removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n     }\n     logAuditEvent(true, \"delete\", src);\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n    writeLock();\n    boolean ret \u003d false;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      toRemovedBlocks \u003d FSDirDeleteOp.delete(\n          this, src, recursive, logRetryCache);\n      ret \u003d toRemovedBlocks !\u003d null;\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"delete\", src);\n      throw e;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (toRemovedBlocks !\u003d null) {\n      removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n    }\n    logAuditEvent(true, \"delete\", src);\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "7817674a3a4d097b647dd77f1345787dd376d5ea": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7609. Avoid retry cache collision when Standby NameNode loading edits. Contributed by Ming Ma.\n",
      "commitDate": "29/05/15 11:05 AM",
      "commitName": "7817674a3a4d097b647dd77f1345787dd376d5ea",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "27/05/15 3:42 PM",
      "commitNameOld": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.81,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,25 @@\n   boolean delete(String src, boolean recursive, boolean logRetryCache)\n       throws IOException {\n     waitForLoadingFSImage();\n-    checkOperation(OperationCategory.WRITE);\n     BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n     writeLock();\n     boolean ret \u003d false;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n       toRemovedBlocks \u003d FSDirDeleteOp.delete(\n           this, src, recursive, logRetryCache);\n       ret \u003d toRemovedBlocks !\u003d null;\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"delete\", src);\n       throw e;\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (toRemovedBlocks !\u003d null) {\n       removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n     }\n     logAuditEvent(true, \"delete\", src);\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    waitForLoadingFSImage();\n    BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n    writeLock();\n    boolean ret \u003d false;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      toRemovedBlocks \u003d FSDirDeleteOp.delete(\n          this, src, recursive, logRetryCache);\n      ret \u003d toRemovedBlocks !\u003d null;\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"delete\", src);\n      throw e;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (toRemovedBlocks !\u003d null) {\n      removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n    }\n    logAuditEvent(true, \"delete\", src);\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "c79e7f7d997596e0c38ae4cddff2bd0910581c16": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8273. FSNamesystem#Delete() should not call logSync() when holding the lock. Contributed by Haohui Mai.\n",
      "commitDate": "28/04/15 6:05 PM",
      "commitName": "c79e7f7d997596e0c38ae4cddff2bd0910581c16",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "24/04/15 10:23 AM",
      "commitNameOld": "cf6c8a1b4ee70dd45c2e42ac61999e61a05db035",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 4.32,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,26 @@\n   boolean delete(String src, boolean recursive, boolean logRetryCache)\n       throws IOException {\n     waitForLoadingFSImage();\n     checkOperation(OperationCategory.WRITE);\n     BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n     writeLock();\n     boolean ret \u003d false;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot delete \" + src);\n       toRemovedBlocks \u003d FSDirDeleteOp.delete(\n           this, src, recursive, logRetryCache);\n       ret \u003d toRemovedBlocks !\u003d null;\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"delete\", src);\n       throw e;\n     } finally {\n       writeUnlock();\n     }\n+    getEditLog().logSync();\n     if (toRemovedBlocks !\u003d null) {\n       removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n     }\n     logAuditEvent(true, \"delete\", src);\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    waitForLoadingFSImage();\n    checkOperation(OperationCategory.WRITE);\n    BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n    writeLock();\n    boolean ret \u003d false;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      toRemovedBlocks \u003d FSDirDeleteOp.delete(\n          this, src, recursive, logRetryCache);\n      ret \u003d toRemovedBlocks !\u003d null;\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"delete\", src);\n      throw e;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (toRemovedBlocks !\u003d null) {\n      removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n    }\n    logAuditEvent(true, \"delete\", src);\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "24315e7d374a1ddd4329b64350cf96fc9ab6f59c": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
      "commitDate": "17/01/15 12:56 PM",
      "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "15/01/15 2:39 PM",
          "commitNameOld": "44eed6cbc97649c15177f9b36f6b119cc1900f7a",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 1.93,
          "commitsBetweenForRepo": 10,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,13 +1,25 @@\n   boolean delete(String src, boolean recursive, boolean logRetryCache)\n-      throws AccessControlException, SafeModeException,\n-      UnresolvedLinkException, IOException {\n-\n+      throws IOException {\n+    waitForLoadingFSImage();\n+    checkOperation(OperationCategory.WRITE);\n+    BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n+    writeLock();\n     boolean ret \u003d false;\n     try {\n-      ret \u003d deleteInt(src, recursive, logRetryCache);\n+      checkOperation(OperationCategory.WRITE);\n+      checkNameNodeSafeMode(\"Cannot delete \" + src);\n+      toRemovedBlocks \u003d FSDirDeleteOp.delete(\n+          this, src, recursive, logRetryCache);\n+      ret \u003d toRemovedBlocks !\u003d null;\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"delete\", src);\n       throw e;\n+    } finally {\n+      writeUnlock();\n     }\n+    if (toRemovedBlocks !\u003d null) {\n+      removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n+    }\n+    logAuditEvent(true, \"delete\", src);\n     return ret;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean delete(String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    waitForLoadingFSImage();\n    checkOperation(OperationCategory.WRITE);\n    BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n    writeLock();\n    boolean ret \u003d false;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      toRemovedBlocks \u003d FSDirDeleteOp.delete(\n          this, src, recursive, logRetryCache);\n      ret \u003d toRemovedBlocks !\u003d null;\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"delete\", src);\n      throw e;\n    } finally {\n      writeUnlock();\n    }\n    if (toRemovedBlocks !\u003d null) {\n      removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n    }\n    logAuditEvent(true, \"delete\", src);\n    return ret;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[AccessControlException, SafeModeException, UnresolvedLinkException, IOException]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7573. Consolidate the implementation of delete() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "17/01/15 12:56 PM",
          "commitName": "24315e7d374a1ddd4329b64350cf96fc9ab6f59c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "15/01/15 2:39 PM",
          "commitNameOld": "44eed6cbc97649c15177f9b36f6b119cc1900f7a",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 1.93,
          "commitsBetweenForRepo": 10,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,13 +1,25 @@\n   boolean delete(String src, boolean recursive, boolean logRetryCache)\n-      throws AccessControlException, SafeModeException,\n-      UnresolvedLinkException, IOException {\n-\n+      throws IOException {\n+    waitForLoadingFSImage();\n+    checkOperation(OperationCategory.WRITE);\n+    BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n+    writeLock();\n     boolean ret \u003d false;\n     try {\n-      ret \u003d deleteInt(src, recursive, logRetryCache);\n+      checkOperation(OperationCategory.WRITE);\n+      checkNameNodeSafeMode(\"Cannot delete \" + src);\n+      toRemovedBlocks \u003d FSDirDeleteOp.delete(\n+          this, src, recursive, logRetryCache);\n+      ret \u003d toRemovedBlocks !\u003d null;\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"delete\", src);\n       throw e;\n+    } finally {\n+      writeUnlock();\n     }\n+    if (toRemovedBlocks !\u003d null) {\n+      removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n+    }\n+    logAuditEvent(true, \"delete\", src);\n     return ret;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean delete(String src, boolean recursive, boolean logRetryCache)\n      throws IOException {\n    waitForLoadingFSImage();\n    checkOperation(OperationCategory.WRITE);\n    BlocksMapUpdateInfo toRemovedBlocks \u003d null;\n    writeLock();\n    boolean ret \u003d false;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      toRemovedBlocks \u003d FSDirDeleteOp.delete(\n          this, src, recursive, logRetryCache);\n      ret \u003d toRemovedBlocks !\u003d null;\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"delete\", src);\n      throw e;\n    } finally {\n      writeUnlock();\n    }\n    if (toRemovedBlocks !\u003d null) {\n      removeBlocks(toRemovedBlocks); // Incremental deletion of blocks\n    }\n    logAuditEvent(true, \"delete\", src);\n    return ret;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "8e253cb93030642f5a7324bad0f161cd0ad33206": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7412. Move RetryCache to NameNodeRpcServer. Contributed by Haohui Mai.\n",
      "commitDate": "24/11/14 11:11 AM",
      "commitName": "8e253cb93030642f5a7324bad0f161cd0ad33206",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7412. Move RetryCache to NameNodeRpcServer. Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 11:11 AM",
          "commitName": "8e253cb93030642f5a7324bad0f161cd0ad33206",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 10:46 AM",
          "commitNameOld": "daacbc18d739d030822df0b75205eeb067f89850",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,13 @@\n-  boolean delete(String src, boolean recursive)\n+  boolean delete(String src, boolean recursive, boolean logRetryCache)\n       throws AccessControlException, SafeModeException,\n       UnresolvedLinkException, IOException {\n-    CacheEntry cacheEntry \u003d RetryCache.waitForCompletion(retryCache);\n-    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n-      return true; // Return previous response\n-    }\n+\n     boolean ret \u003d false;\n     try {\n-      ret \u003d deleteInt(src, recursive, cacheEntry !\u003d null);\n+      ret \u003d deleteInt(src, recursive, logRetryCache);\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"delete\", src);\n       throw e;\n-    } finally {\n-      RetryCache.setState(cacheEntry, ret);\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean delete(String src, boolean recursive, boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      UnresolvedLinkException, IOException {\n\n    boolean ret \u003d false;\n    try {\n      ret \u003d deleteInt(src, recursive, logRetryCache);\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"delete\", src);\n      throw e;\n    }\n    return ret;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[src-String, recursive-boolean]",
            "newValue": "[src-String, recursive-boolean, logRetryCache-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7412. Move RetryCache to NameNodeRpcServer. Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 11:11 AM",
          "commitName": "8e253cb93030642f5a7324bad0f161cd0ad33206",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 10:46 AM",
          "commitNameOld": "daacbc18d739d030822df0b75205eeb067f89850",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,13 @@\n-  boolean delete(String src, boolean recursive)\n+  boolean delete(String src, boolean recursive, boolean logRetryCache)\n       throws AccessControlException, SafeModeException,\n       UnresolvedLinkException, IOException {\n-    CacheEntry cacheEntry \u003d RetryCache.waitForCompletion(retryCache);\n-    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n-      return true; // Return previous response\n-    }\n+\n     boolean ret \u003d false;\n     try {\n-      ret \u003d deleteInt(src, recursive, cacheEntry !\u003d null);\n+      ret \u003d deleteInt(src, recursive, logRetryCache);\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"delete\", src);\n       throw e;\n-    } finally {\n-      RetryCache.setState(cacheEntry, ret);\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean delete(String src, boolean recursive, boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      UnresolvedLinkException, IOException {\n\n    boolean ret \u003d false;\n    try {\n      ret \u003d deleteInt(src, recursive, logRetryCache);\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"delete\", src);\n      throw e;\n    }\n    return ret;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "8c7a7e619699386f9e6991842558d78aa0c8053d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/07/13 12:51 AM",
      "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "26/07/13 4:59 PM",
      "commitNameOld": "dc17bda4b677e30c02c2a9a053895a43e41f7a12",
      "commitAuthorOld": "Konstantin Boudnik",
      "daysBetweenCommits": 3.33,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   boolean delete(String src, boolean recursive)\n       throws AccessControlException, SafeModeException,\n       UnresolvedLinkException, IOException {\n     CacheEntry cacheEntry \u003d RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n       return true; // Return previous response\n     }\n     boolean ret \u003d false;\n     try {\n-      ret \u003d deleteInt(src, recursive);\n+      ret \u003d deleteInt(src, recursive, cacheEntry !\u003d null);\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"delete\", src);\n       throw e;\n     } finally {\n       RetryCache.setState(cacheEntry, ret);\n     }\n     return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive)\n      throws AccessControlException, SafeModeException,\n      UnresolvedLinkException, IOException {\n    CacheEntry cacheEntry \u003d RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n      return true; // Return previous response\n    }\n    boolean ret \u003d false;\n    try {\n      ret \u003d deleteInt(src, recursive, cacheEntry !\u003d null);\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"delete\", src);\n      throw e;\n    } finally {\n      RetryCache.setState(cacheEntry, ret);\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "1b531c1dbb452a6192fad411605d2baaa3831bcd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4979. Implement retry cache on Namenode. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1507170 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/07/13 6:09 PM",
      "commitName": "1b531c1dbb452a6192fad411605d2baaa3831bcd",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "24/07/13 5:32 PM",
      "commitNameOld": "f138ae68f9be0ae072a6a4ee50e94a1608c90edb",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.03,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,18 @@\n   boolean delete(String src, boolean recursive)\n       throws AccessControlException, SafeModeException,\n       UnresolvedLinkException, IOException {\n+    CacheEntry cacheEntry \u003d RetryCache.waitForCompletion(retryCache);\n+    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n+      return true; // Return previous response\n+    }\n+    boolean ret \u003d false;\n     try {\n-      return deleteInt(src, recursive);\n+      ret \u003d deleteInt(src, recursive);\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"delete\", src);\n       throw e;\n+    } finally {\n+      RetryCache.setState(cacheEntry, ret);\n     }\n+    return ret;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive)\n      throws AccessControlException, SafeModeException,\n      UnresolvedLinkException, IOException {\n    CacheEntry cacheEntry \u003d RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n      return true; // Return previous response\n    }\n    boolean ret \u003d false;\n    try {\n      ret \u003d deleteInt(src, recursive);\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"delete\", src);\n      throw e;\n    } finally {\n      RetryCache.setState(cacheEntry, ret);\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "d8ca9c655b3582596c756781f83253f644d1053f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4532. RPC call queue may fill due to current user lookup (daryn)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1452435 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/03/13 10:36 AM",
      "commitName": "d8ca9c655b3582596c756781f83253f644d1053f",
      "commitAuthor": "Daryn Sharp",
      "commitDateOld": "28/02/13 1:13 PM",
      "commitNameOld": "2e02b926644ba80243ba7421cb609133db41d583",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 3.89,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,10 @@\n   boolean delete(String src, boolean recursive)\n       throws AccessControlException, SafeModeException,\n       UnresolvedLinkException, IOException {\n     try {\n       return deleteInt(src, recursive);\n     } catch (AccessControlException e) {\n-      if (isAuditEnabled() \u0026\u0026 isExternalInvocation()) {\n-        logAuditEvent(false, UserGroupInformation.getCurrentUser(),\n-                      getRemoteIp(),\n-                      \"delete\", src, null, null);\n-      }\n+      logAuditEvent(false, \"delete\", src);\n       throw e;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive)\n      throws AccessControlException, SafeModeException,\n      UnresolvedLinkException, IOException {\n    try {\n      return deleteInt(src, recursive);\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"delete\", src);\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "df2fb006b28bf1907fe3c54255e5f6bbb7698285": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3680. Allow customized audit logging in HDFS FSNamesystem. Contributed by Marcelo Vanzin.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1418114 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/12/12 2:27 PM",
      "commitName": "df2fb006b28bf1907fe3c54255e5f6bbb7698285",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "05/12/12 11:20 PM",
      "commitNameOld": "8bb0dc34e4f14698bea104be6294acb4954358ca",
      "commitAuthorOld": "Konstantin Shvachko",
      "daysBetweenCommits": 0.63,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   boolean delete(String src, boolean recursive)\n       throws AccessControlException, SafeModeException,\n       UnresolvedLinkException, IOException {\n     try {\n       return deleteInt(src, recursive);\n     } catch (AccessControlException e) {\n-      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n+      if (isAuditEnabled() \u0026\u0026 isExternalInvocation()) {\n         logAuditEvent(false, UserGroupInformation.getCurrentUser(),\n                       getRemoteIp(),\n                       \"delete\", src, null, null);\n       }\n       throw e;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive)\n      throws AccessControlException, SafeModeException,\n      UnresolvedLinkException, IOException {\n    try {\n      return deleteInt(src, recursive);\n    } catch (AccessControlException e) {\n      if (isAuditEnabled() \u0026\u0026 isExternalInvocation()) {\n        logAuditEvent(false, UserGroupInformation.getCurrentUser(),\n                      getRemoteIp(),\n                      \"delete\", src, null, null);\n      }\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "d866f81edbd70121a9e29e5d25be67e1c464397e": {
      "type": "Ybodychange",
      "commitMessage": "Reverting initial commit of HDFS-3680 pending further comments.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1415797 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/11/12 11:19 AM",
      "commitName": "d866f81edbd70121a9e29e5d25be67e1c464397e",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "30/11/12 11:11 AM",
      "commitNameOld": "a85a0293c77f7cf0471d242458f04ec61e0129bc",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   boolean delete(String src, boolean recursive)\n       throws AccessControlException, SafeModeException,\n       UnresolvedLinkException, IOException {\n     try {\n       return deleteInt(src, recursive);\n     } catch (AccessControlException e) {\n-      if (isAuditEnabled() \u0026\u0026 isExternalInvocation()) {\n+      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n         logAuditEvent(false, UserGroupInformation.getCurrentUser(),\n                       getRemoteIp(),\n                       \"delete\", src, null, null);\n       }\n       throw e;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive)\n      throws AccessControlException, SafeModeException,\n      UnresolvedLinkException, IOException {\n    try {\n      return deleteInt(src, recursive);\n    } catch (AccessControlException e) {\n      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        logAuditEvent(false, UserGroupInformation.getCurrentUser(),\n                      getRemoteIp(),\n                      \"delete\", src, null, null);\n      }\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "a85a0293c77f7cf0471d242458f04ec61e0129bc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3680. Allow customized audit logging in HDFS FSNamesystem. Contributed by Marcelo Vanzin.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1415794 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/11/12 11:11 AM",
      "commitName": "a85a0293c77f7cf0471d242458f04ec61e0129bc",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "19/11/12 6:00 PM",
      "commitNameOld": "573c41c2666e084f3988a288bb40d2305fc23d8f",
      "commitAuthorOld": "Konstantin Shvachko",
      "daysBetweenCommits": 10.72,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   boolean delete(String src, boolean recursive)\n       throws AccessControlException, SafeModeException,\n       UnresolvedLinkException, IOException {\n     try {\n       return deleteInt(src, recursive);\n     } catch (AccessControlException e) {\n-      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n+      if (isAuditEnabled() \u0026\u0026 isExternalInvocation()) {\n         logAuditEvent(false, UserGroupInformation.getCurrentUser(),\n                       getRemoteIp(),\n                       \"delete\", src, null, null);\n       }\n       throw e;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive)\n      throws AccessControlException, SafeModeException,\n      UnresolvedLinkException, IOException {\n    try {\n      return deleteInt(src, recursive);\n    } catch (AccessControlException e) {\n      if (isAuditEnabled() \u0026\u0026 isExternalInvocation()) {\n        logAuditEvent(false, UserGroupInformation.getCurrentUser(),\n                      getRemoteIp(),\n                      \"delete\", src, null, null);\n      }\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "7d1c8d92f9b4b83c6ee154cd9ff70724bc61599f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3733. Audit logs should include WebHDFS access. Contributed by Andy Isaacson\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1379278 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/08/12 9:08 PM",
      "commitName": "7d1c8d92f9b4b83c6ee154cd9ff70724bc61599f",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "28/08/12 3:09 PM",
      "commitNameOld": "d4d2bf73a9181a5bfdc0fd99328c7ee4ec998b4e",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 2.25,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   boolean delete(String src, boolean recursive)\n       throws AccessControlException, SafeModeException,\n       UnresolvedLinkException, IOException {\n     try {\n       return deleteInt(src, recursive);\n     } catch (AccessControlException e) {\n       if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n         logAuditEvent(false, UserGroupInformation.getCurrentUser(),\n-                      Server.getRemoteIp(),\n+                      getRemoteIp(),\n                       \"delete\", src, null, null);\n       }\n       throw e;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive)\n      throws AccessControlException, SafeModeException,\n      UnresolvedLinkException, IOException {\n    try {\n      return deleteInt(src, recursive);\n    } catch (AccessControlException e) {\n      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        logAuditEvent(false, UserGroupInformation.getCurrentUser(),\n                      getRemoteIp(),\n                      \"delete\", src, null, null);\n      }\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "0270889b4e7f241620b2c3c297ec6530d96a7db5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3535. Audit logging should log denied accesses. Contributed by Andy Isaacson\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1354144 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/06/12 11:14 AM",
      "commitName": "0270889b4e7f241620b2c3c297ec6530d96a7db5",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "11/06/12 6:55 PM",
      "commitNameOld": "543f86631bf07053a045d5dabcad16fb8f9eff97",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 14.68,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n-    boolean delete(String src, boolean recursive)\n-        throws AccessControlException, SafeModeException,\n-               UnresolvedLinkException, IOException {\n-      if (NameNode.stateChangeLog.isDebugEnabled()) {\n-        NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n-      }\n-      boolean status \u003d deleteInternal(src, recursive, true);\n-      if (status \u0026\u0026 auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n-        logAuditEvent(UserGroupInformation.getCurrentUser(),\n+  boolean delete(String src, boolean recursive)\n+      throws AccessControlException, SafeModeException,\n+      UnresolvedLinkException, IOException {\n+    try {\n+      return deleteInt(src, recursive);\n+    } catch (AccessControlException e) {\n+      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n+        logAuditEvent(false, UserGroupInformation.getCurrentUser(),\n                       Server.getRemoteIp(),\n                       \"delete\", src, null, null);\n       }\n-      return status;\n-    }\n\\ No newline at end of file\n+      throw e;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  boolean delete(String src, boolean recursive)\n      throws AccessControlException, SafeModeException,\n      UnresolvedLinkException, IOException {\n    try {\n      return deleteInt(src, recursive);\n    } catch (AccessControlException e) {\n      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        logAuditEvent(false, UserGroupInformation.getCurrentUser(),\n                      Server.getRemoteIp(),\n                      \"delete\", src, null, null);\n      }\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    boolean delete(String src, boolean recursive)\n        throws AccessControlException, SafeModeException,\n               UnresolvedLinkException, IOException {\n      if (NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n      }\n      boolean status \u003d deleteInternal(src, recursive, true);\n      if (status \u0026\u0026 auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        logAuditEvent(UserGroupInformation.getCurrentUser(),\n                      Server.getRemoteIp(),\n                      \"delete\", src, null, null);\n      }\n      return status;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    boolean delete(String src, boolean recursive)\n        throws AccessControlException, SafeModeException,\n               UnresolvedLinkException, IOException {\n      if (NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n      }\n      boolean status \u003d deleteInternal(src, recursive, true);\n      if (status \u0026\u0026 auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        logAuditEvent(UserGroupInformation.getCurrentUser(),\n                      Server.getRemoteIp(),\n                      \"delete\", src, null, null);\n      }\n      return status;\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "7fac946ac983e31613fd62836c8ac9c4a579210a": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-2108. Move datanode heartbeat handling from namenode package to blockmanagement package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154042 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/08/11 3:55 PM",
      "commitName": "7fac946ac983e31613fd62836c8ac9c4a579210a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "01/08/11 6:57 AM",
      "commitNameOld": "d68e38b78d9687987c4de2046ce9aa0016685e98",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 3.37,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n-    public boolean delete(String src, boolean recursive)\n+    boolean delete(String src, boolean recursive)\n         throws AccessControlException, SafeModeException,\n                UnresolvedLinkException, IOException {\n       if (NameNode.stateChangeLog.isDebugEnabled()) {\n         NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n       }\n       boolean status \u003d deleteInternal(src, recursive, true);\n       if (status \u0026\u0026 auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n         logAuditEvent(UserGroupInformation.getCurrentUser(),\n                       Server.getRemoteIp(),\n                       \"delete\", src, null, null);\n       }\n       return status;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    boolean delete(String src, boolean recursive)\n        throws AccessControlException, SafeModeException,\n               UnresolvedLinkException, IOException {\n      if (NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n      }\n      boolean status \u003d deleteInternal(src, recursive, true);\n      if (status \u0026\u0026 auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        logAuditEvent(UserGroupInformation.getCurrentUser(),\n                      Server.getRemoteIp(),\n                      \"delete\", src, null, null);\n      }\n      return status;\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldValue": "[public]",
        "newValue": "[]"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,14 @@\n+    public boolean delete(String src, boolean recursive)\n+        throws AccessControlException, SafeModeException,\n+               UnresolvedLinkException, IOException {\n+      if (NameNode.stateChangeLog.isDebugEnabled()) {\n+        NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n+      }\n+      boolean status \u003d deleteInternal(src, recursive, true);\n+      if (status \u0026\u0026 auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n+        logAuditEvent(UserGroupInformation.getCurrentUser(),\n+                      Server.getRemoteIp(),\n+                      \"delete\", src, null, null);\n+      }\n+      return status;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public boolean delete(String src, boolean recursive)\n        throws AccessControlException, SafeModeException,\n               UnresolvedLinkException, IOException {\n      if (NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\"DIR* NameSystem.delete: \" + src);\n      }\n      boolean status \u003d deleteInternal(src, recursive, true);\n      if (status \u0026\u0026 auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        logAuditEvent(UserGroupInformation.getCurrentUser(),\n                      Server.getRemoteIp(),\n                      \"delete\", src, null, null);\n      }\n      return status;\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
    }
  }
}