{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DeleteOperation.java",
  "functionName": "deleteDirectoryTree",
  "functionId": "deleteDirectoryTree___path-Path(modifiers-final)__dirKey-String(modifiers-final)",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DeleteOperation.java",
  "functionStartLine": 308,
  "functionEndLine": 385,
  "numCommitsSeen": 3,
  "timeTaken": 1260,
  "changeHistory": [
    "511df1e837b19ccb9271520589452d82d50ac69d"
  ],
  "changeHistoryShort": {
    "511df1e837b19ccb9271520589452d82d50ac69d": "Yintroduced"
  },
  "changeHistoryDetails": {
    "511df1e837b19ccb9271520589452d82d50ac69d": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-16430. S3AFilesystem.delete to incrementally update s3guard with deletions\n\nContributed by Steve Loughran.\n\nThis overlaps the scanning for directory entries with batched calls to S3 DELETE and updates of the S3Guard tables.\nIt also uses S3Guard to list the files to delete, so find newly created files even when S3 listings are not use consistent.\n\nFor path which the client considers S3Guard to be authoritative, we also do a recursive LIST of the store and delete files; this is to find unindexed files and do guarantee that the delete(path, true) call really does delete everything underneath.\n\nChange-Id: Ice2f6e940c506e0b3a78fa534a99721b1698708e\n",
      "commitDate": "05/09/19 6:25 AM",
      "commitName": "511df1e837b19ccb9271520589452d82d50ac69d",
      "commitAuthor": "Steve Loughran",
      "diff": "@@ -0,0 +1,77 @@\n+  protected void deleteDirectoryTree(final Path path,\n+      final String dirKey) throws IOException {\n+    // create an operation state so that the store can manage the bulk\n+    // operation if it needs to\n+    operationState \u003d S3Guard.initiateBulkWrite(\n+        metadataStore,\n+        BulkOperationState.OperationType.Delete,\n+        path);\n+    try (DurationInfo ignored \u003d\n+             new DurationInfo(LOG, false, \"deleting %s\", dirKey)) {\n+\n+      // init the lists of keys and paths to delete\n+      resetDeleteList();\n+      deleteFuture \u003d null;\n+\n+      // list files including any under tombstones through S3Guard\n+      LOG.debug(\"Getting objects for directory prefix {} to delete\", dirKey);\n+      final RemoteIterator\u003cS3ALocatedFileStatus\u003e locatedFiles \u003d\n+          callbacks.listFilesAndEmptyDirectories(path, status, false, true);\n+\n+      // iterate through and delete. The next() call will block when a new S3\n+      // page is required; this any active delete submitted to the executor\n+      // will run in parallel with this.\n+      while (locatedFiles.hasNext()) {\n+        // get the next entry in the listing.\n+        S3AFileStatus child \u003d locatedFiles.next().toS3AFileStatus();\n+        queueForDeletion(child);\n+      }\n+      LOG.debug(\"Deleting final batch of listed files\");\n+      submitNextBatch();\n+      maybeAwaitCompletion(deleteFuture);\n+\n+      // if s3guard is authoritative we follow up with a bulk list and\n+      // delete process on S3 this helps recover from any situation where S3\n+      // and S3Guard have become inconsistent.\n+      // This is only needed for auth paths; by performing the previous listing\n+      // without tombstone filtering, any files returned by the non-auth\n+      // S3 list which were hidden under tombstones will have been found\n+      // and deleted.\n+\n+      if (callbacks.allowAuthoritative(path)) {\n+        LOG.debug(\"Path is authoritatively guarded;\"\n+            + \" listing files on S3 for completeness\");\n+        // let the ongoing delete finish to avoid duplicates\n+        final RemoteIterator\u003cS3AFileStatus\u003e objects \u003d\n+            callbacks.listObjects(path, dirKey);\n+\n+        // iterate through and delete. The next() call will block when a new S3\n+        // page is required; this any active delete submitted to the executor\n+        // will run in parallel with this.\n+        while (objects.hasNext()) {\n+          // get the next entry in the listing.\n+          extraFilesDeleted++;\n+          queueForDeletion(deletionKey(objects.next()), null);\n+        }\n+        if (extraFilesDeleted \u003e 0) {\n+          LOG.debug(\"Raw S3 Scan found {} extra file(s) to delete\",\n+              extraFilesDeleted);\n+          // there is no more data:\n+          // await any ongoing operation\n+          submitNextBatch();\n+          maybeAwaitCompletion(deleteFuture);\n+        }\n+      }\n+\n+      // final cleanup of the directory tree in the metastore, including the\n+      // directory entry itself.\n+      try (DurationInfo ignored2 \u003d\n+               new DurationInfo(LOG, false, \"Delete metastore\")) {\n+        metadataStore.deleteSubtree(path, operationState);\n+      }\n+    } finally {\n+      IOUtils.cleanupWithLogger(LOG, operationState);\n+    }\n+    LOG.debug(\"Delete \\\"{}\\\" completed; deleted {} objects\", path,\n+        filesDeleted);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected void deleteDirectoryTree(final Path path,\n      final String dirKey) throws IOException {\n    // create an operation state so that the store can manage the bulk\n    // operation if it needs to\n    operationState \u003d S3Guard.initiateBulkWrite(\n        metadataStore,\n        BulkOperationState.OperationType.Delete,\n        path);\n    try (DurationInfo ignored \u003d\n             new DurationInfo(LOG, false, \"deleting %s\", dirKey)) {\n\n      // init the lists of keys and paths to delete\n      resetDeleteList();\n      deleteFuture \u003d null;\n\n      // list files including any under tombstones through S3Guard\n      LOG.debug(\"Getting objects for directory prefix {} to delete\", dirKey);\n      final RemoteIterator\u003cS3ALocatedFileStatus\u003e locatedFiles \u003d\n          callbacks.listFilesAndEmptyDirectories(path, status, false, true);\n\n      // iterate through and delete. The next() call will block when a new S3\n      // page is required; this any active delete submitted to the executor\n      // will run in parallel with this.\n      while (locatedFiles.hasNext()) {\n        // get the next entry in the listing.\n        S3AFileStatus child \u003d locatedFiles.next().toS3AFileStatus();\n        queueForDeletion(child);\n      }\n      LOG.debug(\"Deleting final batch of listed files\");\n      submitNextBatch();\n      maybeAwaitCompletion(deleteFuture);\n\n      // if s3guard is authoritative we follow up with a bulk list and\n      // delete process on S3 this helps recover from any situation where S3\n      // and S3Guard have become inconsistent.\n      // This is only needed for auth paths; by performing the previous listing\n      // without tombstone filtering, any files returned by the non-auth\n      // S3 list which were hidden under tombstones will have been found\n      // and deleted.\n\n      if (callbacks.allowAuthoritative(path)) {\n        LOG.debug(\"Path is authoritatively guarded;\"\n            + \" listing files on S3 for completeness\");\n        // let the ongoing delete finish to avoid duplicates\n        final RemoteIterator\u003cS3AFileStatus\u003e objects \u003d\n            callbacks.listObjects(path, dirKey);\n\n        // iterate through and delete. The next() call will block when a new S3\n        // page is required; this any active delete submitted to the executor\n        // will run in parallel with this.\n        while (objects.hasNext()) {\n          // get the next entry in the listing.\n          extraFilesDeleted++;\n          queueForDeletion(deletionKey(objects.next()), null);\n        }\n        if (extraFilesDeleted \u003e 0) {\n          LOG.debug(\"Raw S3 Scan found {} extra file(s) to delete\",\n              extraFilesDeleted);\n          // there is no more data:\n          // await any ongoing operation\n          submitNextBatch();\n          maybeAwaitCompletion(deleteFuture);\n        }\n      }\n\n      // final cleanup of the directory tree in the metastore, including the\n      // directory entry itself.\n      try (DurationInfo ignored2 \u003d\n               new DurationInfo(LOG, false, \"Delete metastore\")) {\n        metadataStore.deleteSubtree(path, operationState);\n      }\n    } finally {\n      IOUtils.cleanupWithLogger(LOG, operationState);\n    }\n    LOG.debug(\"Delete \\\"{}\\\" completed; deleted {} objects\", path,\n        filesDeleted);\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/DeleteOperation.java"
    }
  }
}