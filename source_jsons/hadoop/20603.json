{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "InputSampler.java",
  "functionName": "getSample",
  "functionId": "getSample___inf-InputFormat__K,V____job-JobConf",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/InputSampler.java",
  "functionStartLine": 159,
  "functionEndLine": 205,
  "numCommitsSeen": 5,
  "timeTaken": 1153,
  "changeHistory": [
    "f6167ef6c6026d8d3a9c567de7ac3cffe7fffcec"
  ],
  "changeHistoryShort": {
    "f6167ef6c6026d8d3a9c567de7ac3cffe7fffcec": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f6167ef6c6026d8d3a9c567de7ac3cffe7fffcec": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-5157. Bring back old sampler related code so that we can support binary compatibility with hadoop-1 sorter example. Contributed by Zhijie Shen.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1480474 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/05/13 2:42 PM",
      "commitName": "f6167ef6c6026d8d3a9c567de7ac3cffe7fffcec",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "diff": "@@ -0,0 +1,24 @@\n+    public K[] getSample(InputFormat\u003cK,V\u003e inf, JobConf job) throws IOException {\n+      InputSplit[] splits \u003d inf.getSplits(job, job.getNumMapTasks());\n+      ArrayList\u003cK\u003e samples \u003d new ArrayList\u003cK\u003e();\n+      int splitsToSample \u003d Math.min(maxSplitsSampled, splits.length);\n+      int splitStep \u003d splits.length / splitsToSample;\n+      long records \u003d 0;\n+      long kept \u003d 0;\n+      for (int i \u003d 0; i \u003c splitsToSample; ++i) {\n+        RecordReader\u003cK,V\u003e reader \u003d inf.getRecordReader(splits[i * splitStep],\n+            job, Reporter.NULL);\n+        K key \u003d reader.createKey();\n+        V value \u003d reader.createValue();\n+        while (reader.next(key, value)) {\n+          ++records;\n+          if ((double) kept / records \u003c freq) {\n+            ++kept;\n+            samples.add(key);\n+            key \u003d reader.createKey();\n+          }\n+        }\n+        reader.close();\n+      }\n+      return (K[])samples.toArray();\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public K[] getSample(InputFormat\u003cK,V\u003e inf, JobConf job) throws IOException {\n      InputSplit[] splits \u003d inf.getSplits(job, job.getNumMapTasks());\n      ArrayList\u003cK\u003e samples \u003d new ArrayList\u003cK\u003e();\n      int splitsToSample \u003d Math.min(maxSplitsSampled, splits.length);\n      int splitStep \u003d splits.length / splitsToSample;\n      long records \u003d 0;\n      long kept \u003d 0;\n      for (int i \u003d 0; i \u003c splitsToSample; ++i) {\n        RecordReader\u003cK,V\u003e reader \u003d inf.getRecordReader(splits[i * splitStep],\n            job, Reporter.NULL);\n        K key \u003d reader.createKey();\n        V value \u003d reader.createValue();\n        while (reader.next(key, value)) {\n          ++records;\n          if ((double) kept / records \u003c freq) {\n            ++kept;\n            samples.add(key);\n            key \u003d reader.createKey();\n          }\n        }\n        reader.close();\n      }\n      return (K[])samples.toArray();\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/InputSampler.java"
    }
  }
}