{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirectory.java",
  "functionName": "updateSpaceForCompleteBlock",
  "functionId": "updateSpaceForCompleteBlock___completeBlk-BlockInfo__inodes-INodesInPath",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
  "functionStartLine": 1073,
  "functionEndLine": 1110,
  "numCommitsSeen": 321,
  "timeTaken": 2584,
  "changeHistory": [
    "3085a604300ed76d06a0011bd5555e419897b6cd",
    "a5bb88c8e0fd4bd19b6d377fecbe1d2d441514f6"
  ],
  "changeHistoryShort": {
    "3085a604300ed76d06a0011bd5555e419897b6cd": "Ybodychange",
    "a5bb88c8e0fd4bd19b6d377fecbe1d2d441514f6": "Yintroduced"
  },
  "changeHistoryDetails": {
    "3085a604300ed76d06a0011bd5555e419897b6cd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8112. Relax permission checking for EC related operations.\n",
      "commitDate": "03/03/17 1:00 PM",
      "commitName": "3085a604300ed76d06a0011bd5555e419897b6cd",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "24/10/16 3:14 PM",
      "commitNameOld": "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 129.95,
      "commitsBetweenForRepo": 799,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,38 @@\n   public void updateSpaceForCompleteBlock(BlockInfo completeBlk,\n       INodesInPath inodes) throws IOException {\n     assert namesystem.hasWriteLock();\n     INodesInPath iip \u003d inodes !\u003d null ? inodes :\n         INodesInPath.fromINode(namesystem.getBlockCollection(completeBlk));\n     INodeFile fileINode \u003d iip.getLastINode().asFile();\n     // Adjust disk space consumption if required\n     final long diff;\n     final short replicationFactor;\n     if (fileINode.isStriped()) {\n       final ErasureCodingPolicy ecPolicy \u003d\n-          FSDirErasureCodingOp.getErasureCodingPolicy(namesystem, iip);\n+          FSDirErasureCodingOp\n+              .unprotectedGetErasureCodingPolicy(namesystem, iip);\n       final short numDataUnits \u003d (short) ecPolicy.getNumDataUnits();\n       final short numParityUnits \u003d (short) ecPolicy.getNumParityUnits();\n \n       final long numBlocks \u003d numDataUnits + numParityUnits;\n       final long fullBlockGroupSize \u003d\n           fileINode.getPreferredBlockSize() * numBlocks;\n \n       final BlockInfoStriped striped \u003d\n           new BlockInfoStriped(completeBlk, ecPolicy);\n       final long actualBlockGroupSize \u003d striped.spaceConsumed();\n \n       diff \u003d fullBlockGroupSize - actualBlockGroupSize;\n       replicationFactor \u003d (short) 1;\n     } else {\n       diff \u003d fileINode.getPreferredBlockSize() - completeBlk.getNumBytes();\n       replicationFactor \u003d fileINode.getFileReplication();\n     }\n     if (diff \u003e 0) {\n       try {\n         updateSpaceConsumed(iip, 0, -diff, replicationFactor);\n       } catch (IOException e) {\n         LOG.warn(\"Unexpected exception while updating disk space.\", e);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void updateSpaceForCompleteBlock(BlockInfo completeBlk,\n      INodesInPath inodes) throws IOException {\n    assert namesystem.hasWriteLock();\n    INodesInPath iip \u003d inodes !\u003d null ? inodes :\n        INodesInPath.fromINode(namesystem.getBlockCollection(completeBlk));\n    INodeFile fileINode \u003d iip.getLastINode().asFile();\n    // Adjust disk space consumption if required\n    final long diff;\n    final short replicationFactor;\n    if (fileINode.isStriped()) {\n      final ErasureCodingPolicy ecPolicy \u003d\n          FSDirErasureCodingOp\n              .unprotectedGetErasureCodingPolicy(namesystem, iip);\n      final short numDataUnits \u003d (short) ecPolicy.getNumDataUnits();\n      final short numParityUnits \u003d (short) ecPolicy.getNumParityUnits();\n\n      final long numBlocks \u003d numDataUnits + numParityUnits;\n      final long fullBlockGroupSize \u003d\n          fileINode.getPreferredBlockSize() * numBlocks;\n\n      final BlockInfoStriped striped \u003d\n          new BlockInfoStriped(completeBlk, ecPolicy);\n      final long actualBlockGroupSize \u003d striped.spaceConsumed();\n\n      diff \u003d fullBlockGroupSize - actualBlockGroupSize;\n      replicationFactor \u003d (short) 1;\n    } else {\n      diff \u003d fileINode.getPreferredBlockSize() - completeBlk.getNumBytes();\n      replicationFactor \u003d fileINode.getFileReplication();\n    }\n    if (diff \u003e 0) {\n      try {\n        updateSpaceConsumed(iip, 0, -diff, replicationFactor);\n      } catch (IOException e) {\n        LOG.warn(\"Unexpected exception while updating disk space.\", e);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "a5bb88c8e0fd4bd19b6d377fecbe1d2d441514f6": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-10843. Update space quota when a UC block is completed rather than committed. Contributed by Erik Krogen.",
      "commitDate": "23/09/16 10:37 AM",
      "commitName": "a5bb88c8e0fd4bd19b6d377fecbe1d2d441514f6",
      "commitAuthor": "Konstantin V Shvachko",
      "diff": "@@ -0,0 +1,37 @@\n+  public void updateSpaceForCompleteBlock(BlockInfo completeBlk,\n+      INodesInPath inodes) throws IOException {\n+    assert namesystem.hasWriteLock();\n+    INodesInPath iip \u003d inodes !\u003d null ? inodes :\n+        INodesInPath.fromINode(namesystem.getBlockCollection(completeBlk));\n+    INodeFile fileINode \u003d iip.getLastINode().asFile();\n+    // Adjust disk space consumption if required\n+    final long diff;\n+    final short replicationFactor;\n+    if (fileINode.isStriped()) {\n+      final ErasureCodingPolicy ecPolicy \u003d\n+          FSDirErasureCodingOp.getErasureCodingPolicy(namesystem, iip);\n+      final short numDataUnits \u003d (short) ecPolicy.getNumDataUnits();\n+      final short numParityUnits \u003d (short) ecPolicy.getNumParityUnits();\n+\n+      final long numBlocks \u003d numDataUnits + numParityUnits;\n+      final long fullBlockGroupSize \u003d\n+          fileINode.getPreferredBlockSize() * numBlocks;\n+\n+      final BlockInfoStriped striped \u003d\n+          new BlockInfoStriped(completeBlk, ecPolicy);\n+      final long actualBlockGroupSize \u003d striped.spaceConsumed();\n+\n+      diff \u003d fullBlockGroupSize - actualBlockGroupSize;\n+      replicationFactor \u003d (short) 1;\n+    } else {\n+      diff \u003d fileINode.getPreferredBlockSize() - completeBlk.getNumBytes();\n+      replicationFactor \u003d fileINode.getFileReplication();\n+    }\n+    if (diff \u003e 0) {\n+      try {\n+        updateSpaceConsumed(iip, 0, -diff, replicationFactor);\n+      } catch (IOException e) {\n+        LOG.warn(\"Unexpected exception while updating disk space.\", e);\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void updateSpaceForCompleteBlock(BlockInfo completeBlk,\n      INodesInPath inodes) throws IOException {\n    assert namesystem.hasWriteLock();\n    INodesInPath iip \u003d inodes !\u003d null ? inodes :\n        INodesInPath.fromINode(namesystem.getBlockCollection(completeBlk));\n    INodeFile fileINode \u003d iip.getLastINode().asFile();\n    // Adjust disk space consumption if required\n    final long diff;\n    final short replicationFactor;\n    if (fileINode.isStriped()) {\n      final ErasureCodingPolicy ecPolicy \u003d\n          FSDirErasureCodingOp.getErasureCodingPolicy(namesystem, iip);\n      final short numDataUnits \u003d (short) ecPolicy.getNumDataUnits();\n      final short numParityUnits \u003d (short) ecPolicy.getNumParityUnits();\n\n      final long numBlocks \u003d numDataUnits + numParityUnits;\n      final long fullBlockGroupSize \u003d\n          fileINode.getPreferredBlockSize() * numBlocks;\n\n      final BlockInfoStriped striped \u003d\n          new BlockInfoStriped(completeBlk, ecPolicy);\n      final long actualBlockGroupSize \u003d striped.spaceConsumed();\n\n      diff \u003d fullBlockGroupSize - actualBlockGroupSize;\n      replicationFactor \u003d (short) 1;\n    } else {\n      diff \u003d fileINode.getPreferredBlockSize() - completeBlk.getNumBytes();\n      replicationFactor \u003d fileINode.getFileReplication();\n    }\n    if (diff \u003e 0) {\n      try {\n        updateSpaceConsumed(iip, 0, -diff, replicationFactor);\n      } catch (IOException e) {\n        LOG.warn(\"Unexpected exception while updating disk space.\", e);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java"
    }
  }
}