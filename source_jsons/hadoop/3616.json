{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSInputStream.java",
  "functionName": "read",
  "functionId": "read___position-long__buffer-byte[]__offset-int__length-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
  "functionStartLine": 1463,
  "functionEndLine": 1471,
  "numCommitsSeen": 143,
  "timeTaken": 8426,
  "changeHistory": [
    "5d748bd056a32f2c6922514cd0c5b31d866a9919",
    "401db4fc65140979fe7665983e36905e886df971",
    "843ee8d59d8bacbca0d87ccf0790772e39d16138",
    "7905788db94d560e6668af0d4bed22b326961aaf",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "892ade689f9bcce76daae8f66fc00a49bee8548e",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "7f6ed7fe365166e8075359f1d0ad035fa876c70f",
    "17db74a1c1972392a5aba48a3e0334dcd6c76487",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "5d748bd056a32f2c6922514cd0c5b31d866a9919": "Ybodychange",
    "401db4fc65140979fe7665983e36905e886df971": "Ybodychange",
    "843ee8d59d8bacbca0d87ccf0790772e39d16138": "Ybodychange",
    "7905788db94d560e6668af0d4bed22b326961aaf": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "892ade689f9bcce76daae8f66fc00a49bee8548e": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "7f6ed7fe365166e8075359f1d0ad035fa876c70f": "Ybodychange",
    "17db74a1c1972392a5aba48a3e0334dcd6c76487": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "5d748bd056a32f2c6922514cd0c5b31d866a9919": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13702. Remove HTrace hooks from DFSClient to reduce CPU usage. Contributed by Todd Lipcon.\n",
      "commitDate": "02/07/18 3:11 AM",
      "commitName": "5d748bd056a32f2c6922514cd0c5b31d866a9919",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "02/07/18 3:02 AM",
      "commitNameOld": "6ba99741086170b83c38d3e7e715d9e8046a1e00",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,9 @@\n   public int read(long position, byte[] buffer, int offset, int length)\n       throws IOException {\n     validatePositionedReadArgs(position, buffer, offset, length);\n     if (length \u003d\u003d 0) {\n       return 0;\n     }\n-    try (TraceScope scope \u003d dfsClient.\n-        newReaderTraceScope(\"DFSInputStream#byteArrayPread\",\n-            src, position, length)) {\n-      ByteBuffer bb \u003d ByteBuffer.wrap(buffer, offset, length);\n-      int retLen \u003d pread(position, bb);\n-      if (retLen \u003c length) {\n-        dfsClient.addRetLenToReaderScope(scope, retLen);\n-      }\n-      return retLen;\n-    }\n+    ByteBuffer bb \u003d ByteBuffer.wrap(buffer, offset, length);\n+    return pread(position, bb);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int read(long position, byte[] buffer, int offset, int length)\n      throws IOException {\n    validatePositionedReadArgs(position, buffer, offset, length);\n    if (length \u003d\u003d 0) {\n      return 0;\n    }\n    ByteBuffer bb \u003d ByteBuffer.wrap(buffer, offset, length);\n    return pread(position, bb);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "401db4fc65140979fe7665983e36905e886df971": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
      "commitDate": "08/09/16 11:54 AM",
      "commitName": "401db4fc65140979fe7665983e36905e886df971",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "24/08/16 6:57 AM",
      "commitNameOld": "793447f79924c97c2b562d5e41fa85adf19673fe",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 15.21,
      "commitsBetweenForRepo": 83,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,17 @@\n   public int read(long position, byte[] buffer, int offset, int length)\n       throws IOException {\n     validatePositionedReadArgs(position, buffer, offset, length);\n     if (length \u003d\u003d 0) {\n       return 0;\n     }\n     try (TraceScope scope \u003d dfsClient.\n         newReaderTraceScope(\"DFSInputStream#byteArrayPread\",\n             src, position, length)) {\n-      int retLen \u003d pread(position, buffer, offset, length);\n+      ByteBuffer bb \u003d ByteBuffer.wrap(buffer, offset, length);\n+      int retLen \u003d pread(position, bb);\n       if (retLen \u003c length) {\n         dfsClient.addRetLenToReaderScope(scope, retLen);\n       }\n       return retLen;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int read(long position, byte[] buffer, int offset, int length)\n      throws IOException {\n    validatePositionedReadArgs(position, buffer, offset, length);\n    if (length \u003d\u003d 0) {\n      return 0;\n    }\n    try (TraceScope scope \u003d dfsClient.\n        newReaderTraceScope(\"DFSInputStream#byteArrayPread\",\n            src, position, length)) {\n      ByteBuffer bb \u003d ByteBuffer.wrap(buffer, offset, length);\n      int retLen \u003d pread(position, bb);\n      if (retLen \u003c length) {\n        dfsClient.addRetLenToReaderScope(scope, retLen);\n      }\n      return retLen;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "843ee8d59d8bacbca0d87ccf0790772e39d16138": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12994. Specify PositionedReadable, add contract tests, fix problems. Contributed by Steve Loughran.\n",
      "commitDate": "08/04/16 1:36 PM",
      "commitName": "843ee8d59d8bacbca0d87ccf0790772e39d16138",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "28/03/16 12:44 AM",
      "commitNameOld": "d8383c687c95dbb37effa307ab2d41497da1cfc2",
      "commitAuthorOld": "Walter Su",
      "daysBetweenCommits": 11.54,
      "commitsBetweenForRepo": 84,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,16 @@\n   public int read(long position, byte[] buffer, int offset, int length)\n       throws IOException {\n+    validatePositionedReadArgs(position, buffer, offset, length);\n+    if (length \u003d\u003d 0) {\n+      return 0;\n+    }\n     try (TraceScope scope \u003d dfsClient.\n         newReaderTraceScope(\"DFSInputStream#byteArrayPread\",\n             src, position, length)) {\n       int retLen \u003d pread(position, buffer, offset, length);\n       if (retLen \u003c length) {\n         dfsClient.addRetLenToReaderScope(scope, retLen);\n       }\n       return retLen;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int read(long position, byte[] buffer, int offset, int length)\n      throws IOException {\n    validatePositionedReadArgs(position, buffer, offset, length);\n    if (length \u003d\u003d 0) {\n      return 0;\n    }\n    try (TraceScope scope \u003d dfsClient.\n        newReaderTraceScope(\"DFSInputStream#byteArrayPread\",\n            src, position, length)) {\n      int retLen \u003d pread(position, buffer, offset, length);\n      if (retLen \u003c length) {\n        dfsClient.addRetLenToReaderScope(scope, retLen);\n      }\n      return retLen;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "7905788db94d560e6668af0d4bed22b326961aaf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9576: HTrace: collect position/length information on read operations (zhz via cmccabe)\n",
      "commitDate": "20/01/16 11:26 AM",
      "commitName": "7905788db94d560e6668af0d4bed22b326961aaf",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "08/01/16 9:13 AM",
      "commitNameOld": "38c4c14472996562eb3d610649246770c2888c6b",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 12.09,
      "commitsBetweenForRepo": 82,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,7 +1,12 @@\n   public int read(long position, byte[] buffer, int offset, int length)\n       throws IOException {\n-    try (TraceScope ignored \u003d dfsClient.\n-        newPathTraceScope(\"DFSInputStream#byteArrayPread\", src)) {\n-      return pread(position, buffer, offset, length);\n+    try (TraceScope scope \u003d dfsClient.\n+        newReaderTraceScope(\"DFSInputStream#byteArrayPread\",\n+            src, position, length)) {\n+      int retLen \u003d pread(position, buffer, offset, length);\n+      if (retLen \u003c length) {\n+        dfsClient.addRetLenToReaderScope(scope, retLen);\n+      }\n+      return retLen;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int read(long position, byte[] buffer, int offset, int length)\n      throws IOException {\n    try (TraceScope scope \u003d dfsClient.\n        newReaderTraceScope(\"DFSInputStream#byteArrayPread\",\n            src, position, length)) {\n      int retLen \u003d pread(position, buffer, offset, length);\n      if (retLen \u003c length) {\n        dfsClient.addRetLenToReaderScope(scope, retLen);\n      }\n      return retLen;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,7 @@\n   public int read(long position, byte[] buffer, int offset, int length)\n       throws IOException {\n-    TraceScope scope \u003d dfsClient.\n-        newPathTraceScope(\"DFSInputStream#byteArrayPread\", src);\n-    try {\n+    try (TraceScope ignored \u003d dfsClient.\n+        newPathTraceScope(\"DFSInputStream#byteArrayPread\", src)) {\n       return pread(position, buffer, offset, length);\n-    } finally {\n-      scope.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int read(long position, byte[] buffer, int offset, int length)\n      throws IOException {\n    try (TraceScope ignored \u003d dfsClient.\n        newPathTraceScope(\"DFSInputStream#byteArrayPread\", src)) {\n      return pread(position, buffer, offset, length);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "892ade689f9bcce76daae8f66fc00a49bee8548e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
      "commitDate": "28/09/15 7:42 AM",
      "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "26/09/15 11:08 AM",
      "commitNameOld": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.86,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,10 @@\n   public int read(long position, byte[] buffer, int offset, int length)\n       throws IOException {\n-    TraceScope scope \u003d\n-        dfsClient.getPathTraceScope(\"DFSInputStream#byteArrayPread\", src);\n+    TraceScope scope \u003d dfsClient.\n+        newPathTraceScope(\"DFSInputStream#byteArrayPread\", src);\n     try {\n       return pread(position, buffer, offset, length);\n     } finally {\n       scope.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int read(long position, byte[] buffer, int offset, int length)\n      throws IOException {\n    TraceScope scope \u003d dfsClient.\n        newPathTraceScope(\"DFSInputStream#byteArrayPread\", src);\n    try {\n      return pread(position, buffer, offset, length);\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public int read(long position, byte[] buffer, int offset, int length)\n      throws IOException {\n    TraceScope scope \u003d\n        dfsClient.getPathTraceScope(\"DFSInputStream#byteArrayPread\", src);\n    try {\n      return pread(position, buffer, offset, length);\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "7f6ed7fe365166e8075359f1d0ad035fa876c70f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7055. Add tracing to DFSInputStream (cmccabe)\n",
      "commitDate": "03/10/14 1:35 PM",
      "commitName": "7f6ed7fe365166e8075359f1d0ad035fa876c70f",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "15/07/14 2:10 PM",
      "commitNameOld": "56c0bd4d37ab13b6cbcf860eda852da603ab2f62",
      "commitAuthorOld": "",
      "daysBetweenCommits": 79.98,
      "commitsBetweenForRepo": 820,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,51 +1,10 @@\n   public int read(long position, byte[] buffer, int offset, int length)\n-    throws IOException {\n-    // sanity checks\n-    dfsClient.checkOpen();\n-    if (closed) {\n-      throw new IOException(\"Stream closed\");\n+      throws IOException {\n+    TraceScope scope \u003d\n+        dfsClient.getPathTraceScope(\"DFSInputStream#byteArrayPread\", src);\n+    try {\n+      return pread(position, buffer, offset, length);\n+    } finally {\n+      scope.close();\n     }\n-    failures \u003d 0;\n-    long filelen \u003d getFileLength();\n-    if ((position \u003c 0) || (position \u003e\u003d filelen)) {\n-      return -1;\n-    }\n-    int realLen \u003d length;\n-    if ((position + length) \u003e filelen) {\n-      realLen \u003d (int)(filelen - position);\n-    }\n-    \n-    // determine the block and byte range within the block\n-    // corresponding to position and realLen\n-    List\u003cLocatedBlock\u003e blockRange \u003d getBlockRange(position, realLen);\n-    int remaining \u003d realLen;\n-    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n-      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n-    for (LocatedBlock blk : blockRange) {\n-      long targetStart \u003d position - blk.getStartOffset();\n-      long bytesToRead \u003d Math.min(remaining, blk.getBlockSize() - targetStart);\n-      try {\n-        if (dfsClient.isHedgedReadsEnabled()) {\n-          hedgedFetchBlockByteRange(blk, targetStart, targetStart + bytesToRead\n-              - 1, buffer, offset, corruptedBlockMap);\n-        } else {\n-          fetchBlockByteRange(blk, targetStart, targetStart + bytesToRead - 1,\n-              buffer, offset, corruptedBlockMap);\n-        }\n-      } finally {\n-        // Check and report if any block replicas are corrupted.\n-        // BlockMissingException may be caught if all block replicas are\n-        // corrupted.\n-        reportCheckSumFailure(corruptedBlockMap, blk.getLocations().length);\n-      }\n-\n-      remaining -\u003d bytesToRead;\n-      position +\u003d bytesToRead;\n-      offset +\u003d bytesToRead;\n-    }\n-    assert remaining \u003d\u003d 0 : \"Wrong number of bytes read.\";\n-    if (dfsClient.stats !\u003d null) {\n-      dfsClient.stats.incrementBytesRead(realLen);\n-    }\n-    return realLen;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int read(long position, byte[] buffer, int offset, int length)\n      throws IOException {\n    TraceScope scope \u003d\n        dfsClient.getPathTraceScope(\"DFSInputStream#byteArrayPread\", src);\n    try {\n      return pread(position, buffer, offset, length);\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "17db74a1c1972392a5aba48a3e0334dcd6c76487": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5776 Support \u0027hedged\u0027 reads in DFSClient\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1571466 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/02/14 2:34 PM",
      "commitName": "17db74a1c1972392a5aba48a3e0334dcd6c76487",
      "commitAuthor": "Michael Stack",
      "commitDateOld": "12/02/14 11:08 AM",
      "commitNameOld": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 12.14,
      "commitsBetweenForRepo": 95,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,51 @@\n   public int read(long position, byte[] buffer, int offset, int length)\n     throws IOException {\n     // sanity checks\n     dfsClient.checkOpen();\n     if (closed) {\n       throw new IOException(\"Stream closed\");\n     }\n     failures \u003d 0;\n     long filelen \u003d getFileLength();\n     if ((position \u003c 0) || (position \u003e\u003d filelen)) {\n       return -1;\n     }\n     int realLen \u003d length;\n     if ((position + length) \u003e filelen) {\n       realLen \u003d (int)(filelen - position);\n     }\n     \n     // determine the block and byte range within the block\n     // corresponding to position and realLen\n     List\u003cLocatedBlock\u003e blockRange \u003d getBlockRange(position, realLen);\n     int remaining \u003d realLen;\n     Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n       \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n     for (LocatedBlock blk : blockRange) {\n       long targetStart \u003d position - blk.getStartOffset();\n       long bytesToRead \u003d Math.min(remaining, blk.getBlockSize() - targetStart);\n       try {\n-        fetchBlockByteRange(blk, targetStart, \n-            targetStart + bytesToRead - 1, buffer, offset, corruptedBlockMap);\n+        if (dfsClient.isHedgedReadsEnabled()) {\n+          hedgedFetchBlockByteRange(blk, targetStart, targetStart + bytesToRead\n+              - 1, buffer, offset, corruptedBlockMap);\n+        } else {\n+          fetchBlockByteRange(blk, targetStart, targetStart + bytesToRead - 1,\n+              buffer, offset, corruptedBlockMap);\n+        }\n       } finally {\n         // Check and report if any block replicas are corrupted.\n         // BlockMissingException may be caught if all block replicas are\n         // corrupted.\n         reportCheckSumFailure(corruptedBlockMap, blk.getLocations().length);\n       }\n \n       remaining -\u003d bytesToRead;\n       position +\u003d bytesToRead;\n       offset +\u003d bytesToRead;\n     }\n     assert remaining \u003d\u003d 0 : \"Wrong number of bytes read.\";\n     if (dfsClient.stats !\u003d null) {\n       dfsClient.stats.incrementBytesRead(realLen);\n     }\n     return realLen;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int read(long position, byte[] buffer, int offset, int length)\n    throws IOException {\n    // sanity checks\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    failures \u003d 0;\n    long filelen \u003d getFileLength();\n    if ((position \u003c 0) || (position \u003e\u003d filelen)) {\n      return -1;\n    }\n    int realLen \u003d length;\n    if ((position + length) \u003e filelen) {\n      realLen \u003d (int)(filelen - position);\n    }\n    \n    // determine the block and byte range within the block\n    // corresponding to position and realLen\n    List\u003cLocatedBlock\u003e blockRange \u003d getBlockRange(position, realLen);\n    int remaining \u003d realLen;\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    for (LocatedBlock blk : blockRange) {\n      long targetStart \u003d position - blk.getStartOffset();\n      long bytesToRead \u003d Math.min(remaining, blk.getBlockSize() - targetStart);\n      try {\n        if (dfsClient.isHedgedReadsEnabled()) {\n          hedgedFetchBlockByteRange(blk, targetStart, targetStart + bytesToRead\n              - 1, buffer, offset, corruptedBlockMap);\n        } else {\n          fetchBlockByteRange(blk, targetStart, targetStart + bytesToRead - 1,\n              buffer, offset, corruptedBlockMap);\n        }\n      } finally {\n        // Check and report if any block replicas are corrupted.\n        // BlockMissingException may be caught if all block replicas are\n        // corrupted.\n        reportCheckSumFailure(corruptedBlockMap, blk.getLocations().length);\n      }\n\n      remaining -\u003d bytesToRead;\n      position +\u003d bytesToRead;\n      offset +\u003d bytesToRead;\n    }\n    assert remaining \u003d\u003d 0 : \"Wrong number of bytes read.\";\n    if (dfsClient.stats !\u003d null) {\n      dfsClient.stats.incrementBytesRead(realLen);\n    }\n    return realLen;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public int read(long position, byte[] buffer, int offset, int length)\n    throws IOException {\n    // sanity checks\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    failures \u003d 0;\n    long filelen \u003d getFileLength();\n    if ((position \u003c 0) || (position \u003e\u003d filelen)) {\n      return -1;\n    }\n    int realLen \u003d length;\n    if ((position + length) \u003e filelen) {\n      realLen \u003d (int)(filelen - position);\n    }\n    \n    // determine the block and byte range within the block\n    // corresponding to position and realLen\n    List\u003cLocatedBlock\u003e blockRange \u003d getBlockRange(position, realLen);\n    int remaining \u003d realLen;\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    for (LocatedBlock blk : blockRange) {\n      long targetStart \u003d position - blk.getStartOffset();\n      long bytesToRead \u003d Math.min(remaining, blk.getBlockSize() - targetStart);\n      try {\n        fetchBlockByteRange(blk, targetStart, \n            targetStart + bytesToRead - 1, buffer, offset, corruptedBlockMap);\n      } finally {\n        // Check and report if any block replicas are corrupted.\n        // BlockMissingException may be caught if all block replicas are\n        // corrupted.\n        reportCheckSumFailure(corruptedBlockMap, blk.getLocations().length);\n      }\n\n      remaining -\u003d bytesToRead;\n      position +\u003d bytesToRead;\n      offset +\u003d bytesToRead;\n    }\n    assert remaining \u003d\u003d 0 : \"Wrong number of bytes read.\";\n    if (dfsClient.stats !\u003d null) {\n      dfsClient.stats.incrementBytesRead(realLen);\n    }\n    return realLen;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public int read(long position, byte[] buffer, int offset, int length)\n    throws IOException {\n    // sanity checks\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    failures \u003d 0;\n    long filelen \u003d getFileLength();\n    if ((position \u003c 0) || (position \u003e\u003d filelen)) {\n      return -1;\n    }\n    int realLen \u003d length;\n    if ((position + length) \u003e filelen) {\n      realLen \u003d (int)(filelen - position);\n    }\n    \n    // determine the block and byte range within the block\n    // corresponding to position and realLen\n    List\u003cLocatedBlock\u003e blockRange \u003d getBlockRange(position, realLen);\n    int remaining \u003d realLen;\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    for (LocatedBlock blk : blockRange) {\n      long targetStart \u003d position - blk.getStartOffset();\n      long bytesToRead \u003d Math.min(remaining, blk.getBlockSize() - targetStart);\n      try {\n        fetchBlockByteRange(blk, targetStart, \n            targetStart + bytesToRead - 1, buffer, offset, corruptedBlockMap);\n      } finally {\n        // Check and report if any block replicas are corrupted.\n        // BlockMissingException may be caught if all block replicas are\n        // corrupted.\n        reportCheckSumFailure(corruptedBlockMap, blk.getLocations().length);\n      }\n\n      remaining -\u003d bytesToRead;\n      position +\u003d bytesToRead;\n      offset +\u003d bytesToRead;\n    }\n    assert remaining \u003d\u003d 0 : \"Wrong number of bytes read.\";\n    if (dfsClient.stats !\u003d null) {\n      dfsClient.stats.incrementBytesRead(realLen);\n    }\n    return realLen;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,46 @@\n+  public int read(long position, byte[] buffer, int offset, int length)\n+    throws IOException {\n+    // sanity checks\n+    dfsClient.checkOpen();\n+    if (closed) {\n+      throw new IOException(\"Stream closed\");\n+    }\n+    failures \u003d 0;\n+    long filelen \u003d getFileLength();\n+    if ((position \u003c 0) || (position \u003e\u003d filelen)) {\n+      return -1;\n+    }\n+    int realLen \u003d length;\n+    if ((position + length) \u003e filelen) {\n+      realLen \u003d (int)(filelen - position);\n+    }\n+    \n+    // determine the block and byte range within the block\n+    // corresponding to position and realLen\n+    List\u003cLocatedBlock\u003e blockRange \u003d getBlockRange(position, realLen);\n+    int remaining \u003d realLen;\n+    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n+      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n+    for (LocatedBlock blk : blockRange) {\n+      long targetStart \u003d position - blk.getStartOffset();\n+      long bytesToRead \u003d Math.min(remaining, blk.getBlockSize() - targetStart);\n+      try {\n+        fetchBlockByteRange(blk, targetStart, \n+            targetStart + bytesToRead - 1, buffer, offset, corruptedBlockMap);\n+      } finally {\n+        // Check and report if any block replicas are corrupted.\n+        // BlockMissingException may be caught if all block replicas are\n+        // corrupted.\n+        reportCheckSumFailure(corruptedBlockMap, blk.getLocations().length);\n+      }\n+\n+      remaining -\u003d bytesToRead;\n+      position +\u003d bytesToRead;\n+      offset +\u003d bytesToRead;\n+    }\n+    assert remaining \u003d\u003d 0 : \"Wrong number of bytes read.\";\n+    if (dfsClient.stats !\u003d null) {\n+      dfsClient.stats.incrementBytesRead(realLen);\n+    }\n+    return realLen;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public int read(long position, byte[] buffer, int offset, int length)\n    throws IOException {\n    // sanity checks\n    dfsClient.checkOpen();\n    if (closed) {\n      throw new IOException(\"Stream closed\");\n    }\n    failures \u003d 0;\n    long filelen \u003d getFileLength();\n    if ((position \u003c 0) || (position \u003e\u003d filelen)) {\n      return -1;\n    }\n    int realLen \u003d length;\n    if ((position + length) \u003e filelen) {\n      realLen \u003d (int)(filelen - position);\n    }\n    \n    // determine the block and byte range within the block\n    // corresponding to position and realLen\n    List\u003cLocatedBlock\u003e blockRange \u003d getBlockRange(position, realLen);\n    int remaining \u003d realLen;\n    Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \n      \u003d new HashMap\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e();\n    for (LocatedBlock blk : blockRange) {\n      long targetStart \u003d position - blk.getStartOffset();\n      long bytesToRead \u003d Math.min(remaining, blk.getBlockSize() - targetStart);\n      try {\n        fetchBlockByteRange(blk, targetStart, \n            targetStart + bytesToRead - 1, buffer, offset, corruptedBlockMap);\n      } finally {\n        // Check and report if any block replicas are corrupted.\n        // BlockMissingException may be caught if all block replicas are\n        // corrupted.\n        reportCheckSumFailure(corruptedBlockMap, blk.getLocations().length);\n      }\n\n      remaining -\u003d bytesToRead;\n      position +\u003d bytesToRead;\n      offset +\u003d bytesToRead;\n    }\n    assert remaining \u003d\u003d 0 : \"Wrong number of bytes read.\";\n    if (dfsClient.stats !\u003d null) {\n      dfsClient.stats.incrementBytesRead(realLen);\n    }\n    return realLen;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSInputStream.java"
    }
  }
}