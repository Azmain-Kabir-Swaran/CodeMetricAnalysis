{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataNode.java",
  "functionName": "requestShortCircuitFdsForRead",
  "functionId": "requestShortCircuitFdsForRead___blk-ExtendedBlock(modifiers-final)__token-Token__BlockTokenIdentifier__(modifiers-final)__maxVersion-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
  "functionStartLine": 1981,
  "functionEndLine": 2008,
  "numCommitsSeen": 358,
  "timeTaken": 2020,
  "changeHistory": [
    "dfcb331ba3516264398121c9f23af3a79c0509cc"
  ],
  "changeHistoryShort": {
    "dfcb331ba3516264398121c9f23af3a79c0509cc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "dfcb331ba3516264398121c9f23af3a79c0509cc": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13076: [SPS]: Addendum. Resolve conflicts after rebasing branch to trunk. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "dfcb331ba3516264398121c9f23af3a79c0509cc",
      "commitAuthor": "Rakesh Radhakrishnan",
      "diff": "@@ -0,0 +1,28 @@\n+  FileInputStream[] requestShortCircuitFdsForRead(final ExtendedBlock blk,\n+      final Token\u003cBlockTokenIdentifier\u003e token, int maxVersion) \n+          throws ShortCircuitFdsUnsupportedException,\n+            ShortCircuitFdsVersionException, IOException {\n+    if (fileDescriptorPassingDisabledReason !\u003d null) {\n+      throw new ShortCircuitFdsUnsupportedException(\n+          fileDescriptorPassingDisabledReason);\n+    }\n+    int blkVersion \u003d CURRENT_BLOCK_FORMAT_VERSION;\n+    if (maxVersion \u003c blkVersion) {\n+      throw new ShortCircuitFdsVersionException(\"Your client is too old \" +\n+        \"to read this block!  Its format version is \" + \n+        blkVersion + \", but the highest format version you can read is \" +\n+        maxVersion);\n+    }\n+    metrics.incrBlocksGetLocalPathInfo();\n+    FileInputStream fis[] \u003d new FileInputStream[2];\n+    \n+    try {\n+      fis[0] \u003d (FileInputStream)data.getBlockInputStream(blk, 0);\n+      fis[1] \u003d DatanodeUtil.getMetaDataInputStream(blk, data);\n+    } catch (ClassCastException e) {\n+      LOG.debug(\"requestShortCircuitFdsForRead failed\", e);\n+      throw new ShortCircuitFdsUnsupportedException(\"This DataNode\u0027s \" +\n+          \"FsDatasetSpi does not support short-circuit local reads\");\n+    }\n+    return fis;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  FileInputStream[] requestShortCircuitFdsForRead(final ExtendedBlock blk,\n      final Token\u003cBlockTokenIdentifier\u003e token, int maxVersion) \n          throws ShortCircuitFdsUnsupportedException,\n            ShortCircuitFdsVersionException, IOException {\n    if (fileDescriptorPassingDisabledReason !\u003d null) {\n      throw new ShortCircuitFdsUnsupportedException(\n          fileDescriptorPassingDisabledReason);\n    }\n    int blkVersion \u003d CURRENT_BLOCK_FORMAT_VERSION;\n    if (maxVersion \u003c blkVersion) {\n      throw new ShortCircuitFdsVersionException(\"Your client is too old \" +\n        \"to read this block!  Its format version is \" + \n        blkVersion + \", but the highest format version you can read is \" +\n        maxVersion);\n    }\n    metrics.incrBlocksGetLocalPathInfo();\n    FileInputStream fis[] \u003d new FileInputStream[2];\n    \n    try {\n      fis[0] \u003d (FileInputStream)data.getBlockInputStream(blk, 0);\n      fis[1] \u003d DatanodeUtil.getMetaDataInputStream(blk, data);\n    } catch (ClassCastException e) {\n      LOG.debug(\"requestShortCircuitFdsForRead failed\", e);\n      throw new ShortCircuitFdsUnsupportedException(\"This DataNode\u0027s \" +\n          \"FsDatasetSpi does not support short-circuit local reads\");\n    }\n    return fis;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
    }
  }
}