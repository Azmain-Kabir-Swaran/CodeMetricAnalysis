{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "JournaledEditsCache.java",
  "functionName": "retrieveEdits",
  "functionId": "retrieveEdits___requestedStartTxn-long__maxTxns-int__outputBuffers-List__ByteBuffer__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournaledEditsCache.java",
  "functionStartLine": 166,
  "functionEndLine": 223,
  "numCommitsSeen": 3,
  "timeTaken": 923,
  "changeHistory": [
    "1e22f2bfbb1d9a29f5d4fa641b7a0dabd5b1dbf5",
    "c81ac2ff0220b180cd6cbbf18221290c3783bfd5"
  ],
  "changeHistoryShort": {
    "1e22f2bfbb1d9a29f5d4fa641b7a0dabd5b1dbf5": "Ybodychange",
    "c81ac2ff0220b180cd6cbbf18221290c3783bfd5": "Yintroduced"
  },
  "changeHistoryDetails": {
    "1e22f2bfbb1d9a29f5d4fa641b7a0dabd5b1dbf5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13610. [SBN read] Edit Tail Fast Path Part 4: Cleanup. Integration test, documentation, remove unnecessary dummy sync, minors fixups. Contributed by Erik Krogen.\n",
      "commitDate": "24/12/18 9:34 AM",
      "commitName": "1e22f2bfbb1d9a29f5d4fa641b7a0dabd5b1dbf5",
      "commitAuthor": "Erik Krogen",
      "commitDateOld": "24/12/18 9:33 AM",
      "commitNameOld": "c81ac2ff0220b180cd6cbbf18221290c3783bfd5",
      "commitAuthorOld": "Erik Krogen",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,58 @@\n   int retrieveEdits(long requestedStartTxn, int maxTxns,\n       List\u003cByteBuffer\u003e outputBuffers) throws IOException {\n     int txnCount \u003d 0;\n \n     try (AutoCloseableLock l \u003d readLock.acquire()) {\n-      if (lowestTxnId \u003c 0 || requestedStartTxn \u003c lowestTxnId) {\n+      if (lowestTxnId \u003d\u003d INVALID_TXN_ID || requestedStartTxn \u003c lowestTxnId) {\n         throw getCacheMissException(requestedStartTxn);\n       } else if (requestedStartTxn \u003e highestTxnId) {\n         return 0;\n       }\n       outputBuffers.add(layoutHeader);\n       Iterator\u003cMap.Entry\u003cLong, byte[]\u003e\u003e incrBuffIter \u003d\n           dataMap.tailMap(dataMap.floorKey(requestedStartTxn), true)\n               .entrySet().iterator();\n       long prevTxn \u003d requestedStartTxn;\n       byte[] prevBuf \u003d null;\n       // Stop when maximum transactions reached...\n       while ((txnCount \u003c maxTxns) \u0026\u0026\n           // ... or there are no more entries ...\n           (incrBuffIter.hasNext() || prevBuf !\u003d null)) {\n         long currTxn;\n         byte[] currBuf;\n         if (incrBuffIter.hasNext()) {\n           Map.Entry\u003cLong, byte[]\u003e ent \u003d incrBuffIter.next();\n           currTxn \u003d ent.getKey();\n           currBuf \u003d ent.getValue();\n         } else {\n           // This accounts for the trailing entry\n           currTxn \u003d highestTxnId + 1;\n           currBuf \u003d null;\n         }\n         if (prevBuf !\u003d null) { // True except for the first loop iteration\n           outputBuffers.add(ByteBuffer.wrap(prevBuf));\n           // if prevTxn \u003c requestedStartTxn, the extra transactions will get\n           // removed after the loop, so don\u0027t include them in the txn count\n           txnCount +\u003d currTxn - Math.max(requestedStartTxn, prevTxn);\n         }\n         prevTxn \u003d currTxn;\n         prevBuf \u003d currBuf;\n       }\n       // Release the lock before doing operations on the buffers (deserializing\n       // to find transaction boundaries, and copying into an output buffer)\n     }\n     // Remove extra leading transactions in the first buffer\n     ByteBuffer firstBuf \u003d outputBuffers.get(1); // 0th is the header\n     firstBuf.position(\n         findTransactionPosition(firstBuf.array(), requestedStartTxn));\n     // Remove trailing transactions in the last buffer if necessary\n     if (txnCount \u003e maxTxns) {\n       ByteBuffer lastBuf \u003d outputBuffers.get(outputBuffers.size() - 1);\n       int limit \u003d\n           findTransactionPosition(lastBuf.array(), requestedStartTxn + maxTxns);\n       lastBuf.limit(limit);\n       txnCount \u003d maxTxns;\n     }\n \n     return txnCount;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  int retrieveEdits(long requestedStartTxn, int maxTxns,\n      List\u003cByteBuffer\u003e outputBuffers) throws IOException {\n    int txnCount \u003d 0;\n\n    try (AutoCloseableLock l \u003d readLock.acquire()) {\n      if (lowestTxnId \u003d\u003d INVALID_TXN_ID || requestedStartTxn \u003c lowestTxnId) {\n        throw getCacheMissException(requestedStartTxn);\n      } else if (requestedStartTxn \u003e highestTxnId) {\n        return 0;\n      }\n      outputBuffers.add(layoutHeader);\n      Iterator\u003cMap.Entry\u003cLong, byte[]\u003e\u003e incrBuffIter \u003d\n          dataMap.tailMap(dataMap.floorKey(requestedStartTxn), true)\n              .entrySet().iterator();\n      long prevTxn \u003d requestedStartTxn;\n      byte[] prevBuf \u003d null;\n      // Stop when maximum transactions reached...\n      while ((txnCount \u003c maxTxns) \u0026\u0026\n          // ... or there are no more entries ...\n          (incrBuffIter.hasNext() || prevBuf !\u003d null)) {\n        long currTxn;\n        byte[] currBuf;\n        if (incrBuffIter.hasNext()) {\n          Map.Entry\u003cLong, byte[]\u003e ent \u003d incrBuffIter.next();\n          currTxn \u003d ent.getKey();\n          currBuf \u003d ent.getValue();\n        } else {\n          // This accounts for the trailing entry\n          currTxn \u003d highestTxnId + 1;\n          currBuf \u003d null;\n        }\n        if (prevBuf !\u003d null) { // True except for the first loop iteration\n          outputBuffers.add(ByteBuffer.wrap(prevBuf));\n          // if prevTxn \u003c requestedStartTxn, the extra transactions will get\n          // removed after the loop, so don\u0027t include them in the txn count\n          txnCount +\u003d currTxn - Math.max(requestedStartTxn, prevTxn);\n        }\n        prevTxn \u003d currTxn;\n        prevBuf \u003d currBuf;\n      }\n      // Release the lock before doing operations on the buffers (deserializing\n      // to find transaction boundaries, and copying into an output buffer)\n    }\n    // Remove extra leading transactions in the first buffer\n    ByteBuffer firstBuf \u003d outputBuffers.get(1); // 0th is the header\n    firstBuf.position(\n        findTransactionPosition(firstBuf.array(), requestedStartTxn));\n    // Remove trailing transactions in the last buffer if necessary\n    if (txnCount \u003e maxTxns) {\n      ByteBuffer lastBuf \u003d outputBuffers.get(outputBuffers.size() - 1);\n      int limit \u003d\n          findTransactionPosition(lastBuf.array(), requestedStartTxn + maxTxns);\n      lastBuf.limit(limit);\n      txnCount \u003d maxTxns;\n    }\n\n    return txnCount;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournaledEditsCache.java",
      "extendedDetails": {}
    },
    "c81ac2ff0220b180cd6cbbf18221290c3783bfd5": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13607. [SBN read] Edit Tail Fast Path Part 1: Enhance JournalNode with an in-memory cache of recent edit transactions. Contributed by Erik Krogen.\n",
      "commitDate": "24/12/18 9:33 AM",
      "commitName": "c81ac2ff0220b180cd6cbbf18221290c3783bfd5",
      "commitAuthor": "Erik Krogen",
      "diff": "@@ -0,0 +1,58 @@\n+  int retrieveEdits(long requestedStartTxn, int maxTxns,\n+      List\u003cByteBuffer\u003e outputBuffers) throws IOException {\n+    int txnCount \u003d 0;\n+\n+    try (AutoCloseableLock l \u003d readLock.acquire()) {\n+      if (lowestTxnId \u003c 0 || requestedStartTxn \u003c lowestTxnId) {\n+        throw getCacheMissException(requestedStartTxn);\n+      } else if (requestedStartTxn \u003e highestTxnId) {\n+        return 0;\n+      }\n+      outputBuffers.add(layoutHeader);\n+      Iterator\u003cMap.Entry\u003cLong, byte[]\u003e\u003e incrBuffIter \u003d\n+          dataMap.tailMap(dataMap.floorKey(requestedStartTxn), true)\n+              .entrySet().iterator();\n+      long prevTxn \u003d requestedStartTxn;\n+      byte[] prevBuf \u003d null;\n+      // Stop when maximum transactions reached...\n+      while ((txnCount \u003c maxTxns) \u0026\u0026\n+          // ... or there are no more entries ...\n+          (incrBuffIter.hasNext() || prevBuf !\u003d null)) {\n+        long currTxn;\n+        byte[] currBuf;\n+        if (incrBuffIter.hasNext()) {\n+          Map.Entry\u003cLong, byte[]\u003e ent \u003d incrBuffIter.next();\n+          currTxn \u003d ent.getKey();\n+          currBuf \u003d ent.getValue();\n+        } else {\n+          // This accounts for the trailing entry\n+          currTxn \u003d highestTxnId + 1;\n+          currBuf \u003d null;\n+        }\n+        if (prevBuf !\u003d null) { // True except for the first loop iteration\n+          outputBuffers.add(ByteBuffer.wrap(prevBuf));\n+          // if prevTxn \u003c requestedStartTxn, the extra transactions will get\n+          // removed after the loop, so don\u0027t include them in the txn count\n+          txnCount +\u003d currTxn - Math.max(requestedStartTxn, prevTxn);\n+        }\n+        prevTxn \u003d currTxn;\n+        prevBuf \u003d currBuf;\n+      }\n+      // Release the lock before doing operations on the buffers (deserializing\n+      // to find transaction boundaries, and copying into an output buffer)\n+    }\n+    // Remove extra leading transactions in the first buffer\n+    ByteBuffer firstBuf \u003d outputBuffers.get(1); // 0th is the header\n+    firstBuf.position(\n+        findTransactionPosition(firstBuf.array(), requestedStartTxn));\n+    // Remove trailing transactions in the last buffer if necessary\n+    if (txnCount \u003e maxTxns) {\n+      ByteBuffer lastBuf \u003d outputBuffers.get(outputBuffers.size() - 1);\n+      int limit \u003d\n+          findTransactionPosition(lastBuf.array(), requestedStartTxn + maxTxns);\n+      lastBuf.limit(limit);\n+      txnCount \u003d maxTxns;\n+    }\n+\n+    return txnCount;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  int retrieveEdits(long requestedStartTxn, int maxTxns,\n      List\u003cByteBuffer\u003e outputBuffers) throws IOException {\n    int txnCount \u003d 0;\n\n    try (AutoCloseableLock l \u003d readLock.acquire()) {\n      if (lowestTxnId \u003c 0 || requestedStartTxn \u003c lowestTxnId) {\n        throw getCacheMissException(requestedStartTxn);\n      } else if (requestedStartTxn \u003e highestTxnId) {\n        return 0;\n      }\n      outputBuffers.add(layoutHeader);\n      Iterator\u003cMap.Entry\u003cLong, byte[]\u003e\u003e incrBuffIter \u003d\n          dataMap.tailMap(dataMap.floorKey(requestedStartTxn), true)\n              .entrySet().iterator();\n      long prevTxn \u003d requestedStartTxn;\n      byte[] prevBuf \u003d null;\n      // Stop when maximum transactions reached...\n      while ((txnCount \u003c maxTxns) \u0026\u0026\n          // ... or there are no more entries ...\n          (incrBuffIter.hasNext() || prevBuf !\u003d null)) {\n        long currTxn;\n        byte[] currBuf;\n        if (incrBuffIter.hasNext()) {\n          Map.Entry\u003cLong, byte[]\u003e ent \u003d incrBuffIter.next();\n          currTxn \u003d ent.getKey();\n          currBuf \u003d ent.getValue();\n        } else {\n          // This accounts for the trailing entry\n          currTxn \u003d highestTxnId + 1;\n          currBuf \u003d null;\n        }\n        if (prevBuf !\u003d null) { // True except for the first loop iteration\n          outputBuffers.add(ByteBuffer.wrap(prevBuf));\n          // if prevTxn \u003c requestedStartTxn, the extra transactions will get\n          // removed after the loop, so don\u0027t include them in the txn count\n          txnCount +\u003d currTxn - Math.max(requestedStartTxn, prevTxn);\n        }\n        prevTxn \u003d currTxn;\n        prevBuf \u003d currBuf;\n      }\n      // Release the lock before doing operations on the buffers (deserializing\n      // to find transaction boundaries, and copying into an output buffer)\n    }\n    // Remove extra leading transactions in the first buffer\n    ByteBuffer firstBuf \u003d outputBuffers.get(1); // 0th is the header\n    firstBuf.position(\n        findTransactionPosition(firstBuf.array(), requestedStartTxn));\n    // Remove trailing transactions in the last buffer if necessary\n    if (txnCount \u003e maxTxns) {\n      ByteBuffer lastBuf \u003d outputBuffers.get(outputBuffers.size() - 1);\n      int limit \u003d\n          findTransactionPosition(lastBuf.array(), requestedStartTxn + maxTxns);\n      lastBuf.limit(limit);\n      txnCount \u003d maxTxns;\n    }\n\n    return txnCount;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournaledEditsCache.java"
    }
  }
}