{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RedundantEditLogInputStream.java",
  "functionName": "nextOp",
  "functionId": "nextOp",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java",
  "functionStartLine": 178,
  "functionEndLine": 259,
  "numCommitsSeen": 11,
  "timeTaken": 1974,
  "changeHistory": [
    "a65bb97f5d8bf2eb817923a69bbb966359f736d7",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "9947d8054c035c93c775908a37489efc1ed36dbd"
  ],
  "changeHistoryShort": {
    "a65bb97f5d8bf2eb817923a69bbb966359f736d7": "Ybodychange",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "9947d8054c035c93c775908a37489efc1ed36dbd": "Yintroduced"
  },
  "changeHistoryDetails": {
    "a65bb97f5d8bf2eb817923a69bbb966359f736d7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13791. Limit logging frequency of edit tail related statements. Contributed by Erik Krogen.\n",
      "commitDate": "24/12/18 9:34 AM",
      "commitName": "a65bb97f5d8bf2eb817923a69bbb966359f736d7",
      "commitAuthor": "Chen Liang",
      "commitDateOld": "06/09/18 2:48 PM",
      "commitNameOld": "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
      "commitAuthorOld": "Giovanni Matteo Fumarola",
      "daysBetweenCommits": 108.82,
      "commitsBetweenForRepo": 942,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,82 @@\n   protected FSEditLogOp nextOp() throws IOException {\n     while (true) {\n       switch (state) {\n       case SKIP_UNTIL:\n        try {\n           if (prevTxId !\u003d HdfsServerConstants.INVALID_TXID) {\n-            LOG.info(\"Fast-forwarding stream \u0027\" + streams[curIdx].getName() +\n-                \"\u0027 to transaction ID \" + (prevTxId + 1));\n+            LogAction logAction \u003d fastForwardLoggingHelper.record();\n+            if (logAction.shouldLog()) {\n+              LOG.info(\"Fast-forwarding stream \u0027\" + streams[curIdx].getName() +\n+                  \"\u0027 to transaction ID \" + (prevTxId + 1) +\n+                  LogThrottlingHelper.getLogSupressionMessage(logAction));\n+            }\n             streams[curIdx].skipUntil(prevTxId + 1);\n           }\n         } catch (IOException e) {\n           prevException \u003d e;\n           state \u003d State.STREAM_FAILED;\n         }\n         state \u003d State.OK;\n         break;\n       case OK:\n         try {\n           FSEditLogOp op \u003d streams[curIdx].readOp();\n           if (op \u003d\u003d null) {\n             state \u003d State.EOF;\n             if (streams[curIdx].getLastTxId() \u003d\u003d prevTxId) {\n               return null;\n             } else {\n               throw new PrematureEOFException(\"got premature end-of-file \" +\n                   \"at txid \" + prevTxId + \"; expected file to go up to \" +\n                   streams[curIdx].getLastTxId());\n             }\n           }\n           prevTxId \u003d op.getTransactionId();\n           return op;\n         } catch (IOException e) {\n           prevException \u003d e;\n           state \u003d State.STREAM_FAILED;\n         }\n         break;\n       case STREAM_FAILED:\n         if (curIdx + 1 \u003d\u003d streams.length) {\n           throw prevException;\n         }\n         long oldLast \u003d streams[curIdx].getLastTxId();\n         long newLast \u003d streams[curIdx + 1].getLastTxId();\n         if (newLast \u003c oldLast) {\n           throw new IOException(\"We encountered an error reading \" +\n               streams[curIdx].getName() + \".  During automatic edit log \" +\n               \"failover, we noticed that all of the remaining edit log \" +\n               \"streams are shorter than the current one!  The best \" +\n               \"remaining edit log ends at transaction \" +\n               newLast + \", but we thought we could read up to transaction \" +\n               oldLast + \".  If you continue, metadata will be lost forever!\");\n         }\n         LOG.error(\"Got error reading edit log input stream \" +\n           streams[curIdx].getName() + \"; failing over to edit log \" +\n           streams[curIdx + 1].getName(), prevException);\n         curIdx++;\n         state \u003d State.SKIP_UNTIL;\n         break;\n       case STREAM_FAILED_RESYNC:\n         if (curIdx + 1 \u003d\u003d streams.length) {\n           if (prevException instanceof PrematureEOFException) {\n             // bypass early EOF check\n             state \u003d State.EOF;\n           } else {\n             streams[curIdx].resync();\n             state \u003d State.SKIP_UNTIL;\n           }\n         } else {\n           LOG.error(\"failing over to edit log \" +\n               streams[curIdx + 1].getName());\n           curIdx++;\n           state \u003d State.SKIP_UNTIL;\n         }\n         break;\n       case EOF:\n         return null;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected FSEditLogOp nextOp() throws IOException {\n    while (true) {\n      switch (state) {\n      case SKIP_UNTIL:\n       try {\n          if (prevTxId !\u003d HdfsServerConstants.INVALID_TXID) {\n            LogAction logAction \u003d fastForwardLoggingHelper.record();\n            if (logAction.shouldLog()) {\n              LOG.info(\"Fast-forwarding stream \u0027\" + streams[curIdx].getName() +\n                  \"\u0027 to transaction ID \" + (prevTxId + 1) +\n                  LogThrottlingHelper.getLogSupressionMessage(logAction));\n            }\n            streams[curIdx].skipUntil(prevTxId + 1);\n          }\n        } catch (IOException e) {\n          prevException \u003d e;\n          state \u003d State.STREAM_FAILED;\n        }\n        state \u003d State.OK;\n        break;\n      case OK:\n        try {\n          FSEditLogOp op \u003d streams[curIdx].readOp();\n          if (op \u003d\u003d null) {\n            state \u003d State.EOF;\n            if (streams[curIdx].getLastTxId() \u003d\u003d prevTxId) {\n              return null;\n            } else {\n              throw new PrematureEOFException(\"got premature end-of-file \" +\n                  \"at txid \" + prevTxId + \"; expected file to go up to \" +\n                  streams[curIdx].getLastTxId());\n            }\n          }\n          prevTxId \u003d op.getTransactionId();\n          return op;\n        } catch (IOException e) {\n          prevException \u003d e;\n          state \u003d State.STREAM_FAILED;\n        }\n        break;\n      case STREAM_FAILED:\n        if (curIdx + 1 \u003d\u003d streams.length) {\n          throw prevException;\n        }\n        long oldLast \u003d streams[curIdx].getLastTxId();\n        long newLast \u003d streams[curIdx + 1].getLastTxId();\n        if (newLast \u003c oldLast) {\n          throw new IOException(\"We encountered an error reading \" +\n              streams[curIdx].getName() + \".  During automatic edit log \" +\n              \"failover, we noticed that all of the remaining edit log \" +\n              \"streams are shorter than the current one!  The best \" +\n              \"remaining edit log ends at transaction \" +\n              newLast + \", but we thought we could read up to transaction \" +\n              oldLast + \".  If you continue, metadata will be lost forever!\");\n        }\n        LOG.error(\"Got error reading edit log input stream \" +\n          streams[curIdx].getName() + \"; failing over to edit log \" +\n          streams[curIdx + 1].getName(), prevException);\n        curIdx++;\n        state \u003d State.SKIP_UNTIL;\n        break;\n      case STREAM_FAILED_RESYNC:\n        if (curIdx + 1 \u003d\u003d streams.length) {\n          if (prevException instanceof PrematureEOFException) {\n            // bypass early EOF check\n            state \u003d State.EOF;\n          } else {\n            streams[curIdx].resync();\n            state \u003d State.SKIP_UNTIL;\n          }\n        } else {\n          LOG.error(\"failing over to edit log \" +\n              streams[curIdx + 1].getName());\n          curIdx++;\n          state \u003d State.SKIP_UNTIL;\n        }\n        break;\n      case EOF:\n        return null;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java",
      "extendedDetails": {}
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/09/14 2:02 PM",
      "commitNameOld": "faa4455be512e070fa420084be8d1be5c72f3b08",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 241.83,
      "commitsBetweenForRepo": 2090,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,78 @@\n   protected FSEditLogOp nextOp() throws IOException {\n     while (true) {\n       switch (state) {\n       case SKIP_UNTIL:\n        try {\n-          if (prevTxId !\u003d HdfsConstants.INVALID_TXID) {\n+          if (prevTxId !\u003d HdfsServerConstants.INVALID_TXID) {\n             LOG.info(\"Fast-forwarding stream \u0027\" + streams[curIdx].getName() +\n                 \"\u0027 to transaction ID \" + (prevTxId + 1));\n             streams[curIdx].skipUntil(prevTxId + 1);\n           }\n         } catch (IOException e) {\n           prevException \u003d e;\n           state \u003d State.STREAM_FAILED;\n         }\n         state \u003d State.OK;\n         break;\n       case OK:\n         try {\n           FSEditLogOp op \u003d streams[curIdx].readOp();\n           if (op \u003d\u003d null) {\n             state \u003d State.EOF;\n             if (streams[curIdx].getLastTxId() \u003d\u003d prevTxId) {\n               return null;\n             } else {\n               throw new PrematureEOFException(\"got premature end-of-file \" +\n                   \"at txid \" + prevTxId + \"; expected file to go up to \" +\n                   streams[curIdx].getLastTxId());\n             }\n           }\n           prevTxId \u003d op.getTransactionId();\n           return op;\n         } catch (IOException e) {\n           prevException \u003d e;\n           state \u003d State.STREAM_FAILED;\n         }\n         break;\n       case STREAM_FAILED:\n         if (curIdx + 1 \u003d\u003d streams.length) {\n           throw prevException;\n         }\n         long oldLast \u003d streams[curIdx].getLastTxId();\n         long newLast \u003d streams[curIdx + 1].getLastTxId();\n         if (newLast \u003c oldLast) {\n           throw new IOException(\"We encountered an error reading \" +\n               streams[curIdx].getName() + \".  During automatic edit log \" +\n               \"failover, we noticed that all of the remaining edit log \" +\n               \"streams are shorter than the current one!  The best \" +\n               \"remaining edit log ends at transaction \" +\n               newLast + \", but we thought we could read up to transaction \" +\n               oldLast + \".  If you continue, metadata will be lost forever!\");\n         }\n         LOG.error(\"Got error reading edit log input stream \" +\n           streams[curIdx].getName() + \"; failing over to edit log \" +\n           streams[curIdx + 1].getName(), prevException);\n         curIdx++;\n         state \u003d State.SKIP_UNTIL;\n         break;\n       case STREAM_FAILED_RESYNC:\n         if (curIdx + 1 \u003d\u003d streams.length) {\n           if (prevException instanceof PrematureEOFException) {\n             // bypass early EOF check\n             state \u003d State.EOF;\n           } else {\n             streams[curIdx].resync();\n             state \u003d State.SKIP_UNTIL;\n           }\n         } else {\n           LOG.error(\"failing over to edit log \" +\n               streams[curIdx + 1].getName());\n           curIdx++;\n           state \u003d State.SKIP_UNTIL;\n         }\n         break;\n       case EOF:\n         return null;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected FSEditLogOp nextOp() throws IOException {\n    while (true) {\n      switch (state) {\n      case SKIP_UNTIL:\n       try {\n          if (prevTxId !\u003d HdfsServerConstants.INVALID_TXID) {\n            LOG.info(\"Fast-forwarding stream \u0027\" + streams[curIdx].getName() +\n                \"\u0027 to transaction ID \" + (prevTxId + 1));\n            streams[curIdx].skipUntil(prevTxId + 1);\n          }\n        } catch (IOException e) {\n          prevException \u003d e;\n          state \u003d State.STREAM_FAILED;\n        }\n        state \u003d State.OK;\n        break;\n      case OK:\n        try {\n          FSEditLogOp op \u003d streams[curIdx].readOp();\n          if (op \u003d\u003d null) {\n            state \u003d State.EOF;\n            if (streams[curIdx].getLastTxId() \u003d\u003d prevTxId) {\n              return null;\n            } else {\n              throw new PrematureEOFException(\"got premature end-of-file \" +\n                  \"at txid \" + prevTxId + \"; expected file to go up to \" +\n                  streams[curIdx].getLastTxId());\n            }\n          }\n          prevTxId \u003d op.getTransactionId();\n          return op;\n        } catch (IOException e) {\n          prevException \u003d e;\n          state \u003d State.STREAM_FAILED;\n        }\n        break;\n      case STREAM_FAILED:\n        if (curIdx + 1 \u003d\u003d streams.length) {\n          throw prevException;\n        }\n        long oldLast \u003d streams[curIdx].getLastTxId();\n        long newLast \u003d streams[curIdx + 1].getLastTxId();\n        if (newLast \u003c oldLast) {\n          throw new IOException(\"We encountered an error reading \" +\n              streams[curIdx].getName() + \".  During automatic edit log \" +\n              \"failover, we noticed that all of the remaining edit log \" +\n              \"streams are shorter than the current one!  The best \" +\n              \"remaining edit log ends at transaction \" +\n              newLast + \", but we thought we could read up to transaction \" +\n              oldLast + \".  If you continue, metadata will be lost forever!\");\n        }\n        LOG.error(\"Got error reading edit log input stream \" +\n          streams[curIdx].getName() + \"; failing over to edit log \" +\n          streams[curIdx + 1].getName(), prevException);\n        curIdx++;\n        state \u003d State.SKIP_UNTIL;\n        break;\n      case STREAM_FAILED_RESYNC:\n        if (curIdx + 1 \u003d\u003d streams.length) {\n          if (prevException instanceof PrematureEOFException) {\n            // bypass early EOF check\n            state \u003d State.EOF;\n          } else {\n            streams[curIdx].resync();\n            state \u003d State.SKIP_UNTIL;\n          }\n        } else {\n          LOG.error(\"failing over to edit log \" +\n              streams[curIdx + 1].getName());\n          curIdx++;\n          state \u003d State.SKIP_UNTIL;\n        }\n        break;\n      case EOF:\n        return null;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java",
      "extendedDetails": {}
    },
    "9947d8054c035c93c775908a37489efc1ed36dbd": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3049. During the normal NN startup process, fall back on a different edit log if we see one that is corrupt. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1349114 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/06/12 9:15 PM",
      "commitName": "9947d8054c035c93c775908a37489efc1ed36dbd",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,78 @@\n+  protected FSEditLogOp nextOp() throws IOException {\n+    while (true) {\n+      switch (state) {\n+      case SKIP_UNTIL:\n+       try {\n+          if (prevTxId !\u003d HdfsConstants.INVALID_TXID) {\n+            LOG.info(\"Fast-forwarding stream \u0027\" + streams[curIdx].getName() +\n+                \"\u0027 to transaction ID \" + (prevTxId + 1));\n+            streams[curIdx].skipUntil(prevTxId + 1);\n+          }\n+        } catch (IOException e) {\n+          prevException \u003d e;\n+          state \u003d State.STREAM_FAILED;\n+        }\n+        state \u003d State.OK;\n+        break;\n+      case OK:\n+        try {\n+          FSEditLogOp op \u003d streams[curIdx].readOp();\n+          if (op \u003d\u003d null) {\n+            state \u003d State.EOF;\n+            if (streams[curIdx].getLastTxId() \u003d\u003d prevTxId) {\n+              return null;\n+            } else {\n+              throw new PrematureEOFException(\"got premature end-of-file \" +\n+                  \"at txid \" + prevTxId + \"; expected file to go up to \" +\n+                  streams[curIdx].getLastTxId());\n+            }\n+          }\n+          prevTxId \u003d op.getTransactionId();\n+          return op;\n+        } catch (IOException e) {\n+          prevException \u003d e;\n+          state \u003d State.STREAM_FAILED;\n+        }\n+        break;\n+      case STREAM_FAILED:\n+        if (curIdx + 1 \u003d\u003d streams.length) {\n+          throw prevException;\n+        }\n+        long oldLast \u003d streams[curIdx].getLastTxId();\n+        long newLast \u003d streams[curIdx + 1].getLastTxId();\n+        if (newLast \u003c oldLast) {\n+          throw new IOException(\"We encountered an error reading \" +\n+              streams[curIdx].getName() + \".  During automatic edit log \" +\n+              \"failover, we noticed that all of the remaining edit log \" +\n+              \"streams are shorter than the current one!  The best \" +\n+              \"remaining edit log ends at transaction \" +\n+              newLast + \", but we thought we could read up to transaction \" +\n+              oldLast + \".  If you continue, metadata will be lost forever!\");\n+        }\n+        LOG.error(\"Got error reading edit log input stream \" +\n+          streams[curIdx].getName() + \"; failing over to edit log \" +\n+          streams[curIdx + 1].getName(), prevException);\n+        curIdx++;\n+        state \u003d State.SKIP_UNTIL;\n+        break;\n+      case STREAM_FAILED_RESYNC:\n+        if (curIdx + 1 \u003d\u003d streams.length) {\n+          if (prevException instanceof PrematureEOFException) {\n+            // bypass early EOF check\n+            state \u003d State.EOF;\n+          } else {\n+            streams[curIdx].resync();\n+            state \u003d State.SKIP_UNTIL;\n+          }\n+        } else {\n+          LOG.error(\"failing over to edit log \" +\n+              streams[curIdx + 1].getName());\n+          curIdx++;\n+          state \u003d State.SKIP_UNTIL;\n+        }\n+        break;\n+      case EOF:\n+        return null;\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected FSEditLogOp nextOp() throws IOException {\n    while (true) {\n      switch (state) {\n      case SKIP_UNTIL:\n       try {\n          if (prevTxId !\u003d HdfsConstants.INVALID_TXID) {\n            LOG.info(\"Fast-forwarding stream \u0027\" + streams[curIdx].getName() +\n                \"\u0027 to transaction ID \" + (prevTxId + 1));\n            streams[curIdx].skipUntil(prevTxId + 1);\n          }\n        } catch (IOException e) {\n          prevException \u003d e;\n          state \u003d State.STREAM_FAILED;\n        }\n        state \u003d State.OK;\n        break;\n      case OK:\n        try {\n          FSEditLogOp op \u003d streams[curIdx].readOp();\n          if (op \u003d\u003d null) {\n            state \u003d State.EOF;\n            if (streams[curIdx].getLastTxId() \u003d\u003d prevTxId) {\n              return null;\n            } else {\n              throw new PrematureEOFException(\"got premature end-of-file \" +\n                  \"at txid \" + prevTxId + \"; expected file to go up to \" +\n                  streams[curIdx].getLastTxId());\n            }\n          }\n          prevTxId \u003d op.getTransactionId();\n          return op;\n        } catch (IOException e) {\n          prevException \u003d e;\n          state \u003d State.STREAM_FAILED;\n        }\n        break;\n      case STREAM_FAILED:\n        if (curIdx + 1 \u003d\u003d streams.length) {\n          throw prevException;\n        }\n        long oldLast \u003d streams[curIdx].getLastTxId();\n        long newLast \u003d streams[curIdx + 1].getLastTxId();\n        if (newLast \u003c oldLast) {\n          throw new IOException(\"We encountered an error reading \" +\n              streams[curIdx].getName() + \".  During automatic edit log \" +\n              \"failover, we noticed that all of the remaining edit log \" +\n              \"streams are shorter than the current one!  The best \" +\n              \"remaining edit log ends at transaction \" +\n              newLast + \", but we thought we could read up to transaction \" +\n              oldLast + \".  If you continue, metadata will be lost forever!\");\n        }\n        LOG.error(\"Got error reading edit log input stream \" +\n          streams[curIdx].getName() + \"; failing over to edit log \" +\n          streams[curIdx + 1].getName(), prevException);\n        curIdx++;\n        state \u003d State.SKIP_UNTIL;\n        break;\n      case STREAM_FAILED_RESYNC:\n        if (curIdx + 1 \u003d\u003d streams.length) {\n          if (prevException instanceof PrematureEOFException) {\n            // bypass early EOF check\n            state \u003d State.EOF;\n          } else {\n            streams[curIdx].resync();\n            state \u003d State.SKIP_UNTIL;\n          }\n        } else {\n          LOG.error(\"failing over to edit log \" +\n              streams[curIdx + 1].getName());\n          curIdx++;\n          state \u003d State.SKIP_UNTIL;\n        }\n        break;\n      case EOF:\n        return null;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java"
    }
  }
}