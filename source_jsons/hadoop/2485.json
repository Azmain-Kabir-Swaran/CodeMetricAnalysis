{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PBHelperClient.java",
  "functionName": "convertCreateFlag",
  "functionId": "convertCreateFlag___flag-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
  "functionStartLine": 2262,
  "functionEndLine": 2288,
  "numCommitsSeen": 308,
  "timeTaken": 8538,
  "changeHistory": [
    "c2a52ef9c29459ff9ef3e23b29e14912bfdb1405",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
    "a7bcc9535860214380e235641d1d5d2dd15aee58",
    "991c453ca3ac141a3f286f74af8401f83c38b230",
    "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e",
    "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e",
    "48da033901d3471ef176a94104158546152353e9",
    "38a19bc293dec6221ae96e304fc6ab660d94e706",
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9"
  ],
  "changeHistoryShort": {
    "c2a52ef9c29459ff9ef3e23b29e14912bfdb1405": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ymovefromfile",
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0": "Ybodychange",
    "a7bcc9535860214380e235641d1d5d2dd15aee58": "Ybodychange",
    "991c453ca3ac141a3f286f74af8401f83c38b230": "Yrename",
    "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e": "Ybodychange",
    "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e": "Ybodychange",
    "48da033901d3471ef176a94104158546152353e9": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
    "38a19bc293dec6221ae96e304fc6ab660d94e706": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yparameterchange)",
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c2a52ef9c29459ff9ef3e23b29e14912bfdb1405": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11643. Add shouldReplicate option to create builder. Contributed by SammiChen.\n",
      "commitDate": "04/05/17 11:39 AM",
      "commitName": "c2a52ef9c29459ff9ef3e23b29e14912bfdb1405",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/04/17 10:18 PM",
      "commitNameOld": "cb672a45a0bbd8950b9b5e304c2e03f516945903",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 6.56,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,27 @@\n   public static EnumSetWritable\u003cCreateFlag\u003e convertCreateFlag(int flag) {\n     EnumSet\u003cCreateFlag\u003e result \u003d\n         EnumSet.noneOf(CreateFlag.class);\n     if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n       result.add(CreateFlag.APPEND);\n     }\n     if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n       result.add(CreateFlag.CREATE);\n     }\n     if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE)\n         \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n       result.add(CreateFlag.OVERWRITE);\n     }\n     if ((flag \u0026 CreateFlagProto.LAZY_PERSIST_VALUE)\n         \u003d\u003d CreateFlagProto.LAZY_PERSIST_VALUE) {\n       result.add(CreateFlag.LAZY_PERSIST);\n     }\n     if ((flag \u0026 CreateFlagProto.NEW_BLOCK_VALUE)\n         \u003d\u003d CreateFlagProto.NEW_BLOCK_VALUE) {\n       result.add(CreateFlag.NEW_BLOCK);\n     }\n+    if ((flag \u0026 CreateFlagProto.SHOULD_REPLICATE.getNumber())\n+        \u003d\u003d CreateFlagProto.SHOULD_REPLICATE.getNumber()) {\n+      result.add(CreateFlag.SHOULD_REPLICATE);\n+    }\n     return new EnumSetWritable\u003c\u003e(result, CreateFlag.class);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convertCreateFlag(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d\n        EnumSet.noneOf(CreateFlag.class);\n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n      result.add(CreateFlag.CREATE);\n    }\n    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE)\n        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n      result.add(CreateFlag.OVERWRITE);\n    }\n    if ((flag \u0026 CreateFlagProto.LAZY_PERSIST_VALUE)\n        \u003d\u003d CreateFlagProto.LAZY_PERSIST_VALUE) {\n      result.add(CreateFlag.LAZY_PERSIST);\n    }\n    if ((flag \u0026 CreateFlagProto.NEW_BLOCK_VALUE)\n        \u003d\u003d CreateFlagProto.NEW_BLOCK_VALUE) {\n      result.add(CreateFlag.NEW_BLOCK);\n    }\n    if ((flag \u0026 CreateFlagProto.SHOULD_REPLICATE.getNumber())\n        \u003d\u003d CreateFlagProto.SHOULD_REPLICATE.getNumber()) {\n      result.add(CreateFlag.SHOULD_REPLICATE);\n    }\n    return new EnumSetWritable\u003c\u003e(result, CreateFlag.class);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "22/09/15 1:27 PM",
      "commitNameOld": "1080c3730068177ddd10dc313890ac1f5dc58f1a",
      "commitAuthorOld": "",
      "daysBetweenCommits": 10.92,
      "commitsBetweenForRepo": 91,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public static EnumSetWritable\u003cCreateFlag\u003e convertCreateFlag(int flag) {\n     EnumSet\u003cCreateFlag\u003e result \u003d\n-       EnumSet.noneOf(CreateFlag.class);\n+        EnumSet.noneOf(CreateFlag.class);\n     if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n       result.add(CreateFlag.APPEND);\n     }\n     if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n       result.add(CreateFlag.CREATE);\n     }\n     if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE)\n         \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n       result.add(CreateFlag.OVERWRITE);\n     }\n     if ((flag \u0026 CreateFlagProto.LAZY_PERSIST_VALUE)\n         \u003d\u003d CreateFlagProto.LAZY_PERSIST_VALUE) {\n       result.add(CreateFlag.LAZY_PERSIST);\n     }\n     if ((flag \u0026 CreateFlagProto.NEW_BLOCK_VALUE)\n         \u003d\u003d CreateFlagProto.NEW_BLOCK_VALUE) {\n       result.add(CreateFlag.NEW_BLOCK);\n     }\n-    return new EnumSetWritable\u003cCreateFlag\u003e(result, CreateFlag.class);\n+    return new EnumSetWritable\u003c\u003e(result, CreateFlag.class);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convertCreateFlag(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d\n        EnumSet.noneOf(CreateFlag.class);\n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n      result.add(CreateFlag.CREATE);\n    }\n    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE)\n        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n      result.add(CreateFlag.OVERWRITE);\n    }\n    if ((flag \u0026 CreateFlagProto.LAZY_PERSIST_VALUE)\n        \u003d\u003d CreateFlagProto.LAZY_PERSIST_VALUE) {\n      result.add(CreateFlag.LAZY_PERSIST);\n    }\n    if ((flag \u0026 CreateFlagProto.NEW_BLOCK_VALUE)\n        \u003d\u003d CreateFlagProto.NEW_BLOCK_VALUE) {\n      result.add(CreateFlag.NEW_BLOCK);\n    }\n    return new EnumSetWritable\u003c\u003e(result, CreateFlag.class);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {}
    },
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "21/09/15 5:51 PM",
      "commitNameOld": "8e01b0d97ac3d74b049a801dfa1cc6e77d8f680a",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public static EnumSetWritable\u003cCreateFlag\u003e convertCreateFlag(int flag) {\n-    EnumSet\u003cCreateFlag\u003e result \u003d \n-       EnumSet.noneOf(CreateFlag.class);   \n+    EnumSet\u003cCreateFlag\u003e result \u003d\n+       EnumSet.noneOf(CreateFlag.class);\n     if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n       result.add(CreateFlag.APPEND);\n     }\n     if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n       result.add(CreateFlag.CREATE);\n     }\n-    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n+    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE)\n         \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n       result.add(CreateFlag.OVERWRITE);\n     }\n     if ((flag \u0026 CreateFlagProto.LAZY_PERSIST_VALUE)\n         \u003d\u003d CreateFlagProto.LAZY_PERSIST_VALUE) {\n       result.add(CreateFlag.LAZY_PERSIST);\n     }\n     if ((flag \u0026 CreateFlagProto.NEW_BLOCK_VALUE)\n         \u003d\u003d CreateFlagProto.NEW_BLOCK_VALUE) {\n       result.add(CreateFlag.NEW_BLOCK);\n     }\n     return new EnumSetWritable\u003cCreateFlag\u003e(result, CreateFlag.class);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convertCreateFlag(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d\n       EnumSet.noneOf(CreateFlag.class);\n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n      result.add(CreateFlag.CREATE);\n    }\n    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE)\n        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n      result.add(CreateFlag.OVERWRITE);\n    }\n    if ((flag \u0026 CreateFlagProto.LAZY_PERSIST_VALUE)\n        \u003d\u003d CreateFlagProto.LAZY_PERSIST_VALUE) {\n      result.add(CreateFlag.LAZY_PERSIST);\n    }\n    if ((flag \u0026 CreateFlagProto.NEW_BLOCK_VALUE)\n        \u003d\u003d CreateFlagProto.NEW_BLOCK_VALUE) {\n      result.add(CreateFlag.NEW_BLOCK);\n    }\n    return new EnumSetWritable\u003cCreateFlag\u003e(result, CreateFlag.class);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
        "oldMethodName": "convertCreateFlag",
        "newMethodName": "convertCreateFlag"
      }
    },
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3689. Add support for variable length block. Contributed by Jing Zhao.\n",
      "commitDate": "27/01/15 12:58 PM",
      "commitName": "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "13/01/15 12:24 AM",
      "commitNameOld": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 14.52,
      "commitsBetweenForRepo": 101,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,23 @@\n   public static EnumSetWritable\u003cCreateFlag\u003e convertCreateFlag(int flag) {\n     EnumSet\u003cCreateFlag\u003e result \u003d \n        EnumSet.noneOf(CreateFlag.class);   \n     if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n       result.add(CreateFlag.APPEND);\n     }\n     if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n       result.add(CreateFlag.CREATE);\n     }\n     if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n         \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n       result.add(CreateFlag.OVERWRITE);\n     }\n     if ((flag \u0026 CreateFlagProto.LAZY_PERSIST_VALUE)\n         \u003d\u003d CreateFlagProto.LAZY_PERSIST_VALUE) {\n       result.add(CreateFlag.LAZY_PERSIST);\n     }\n-    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n+    if ((flag \u0026 CreateFlagProto.NEW_BLOCK_VALUE)\n+        \u003d\u003d CreateFlagProto.NEW_BLOCK_VALUE) {\n+      result.add(CreateFlag.NEW_BLOCK);\n+    }\n+    return new EnumSetWritable\u003cCreateFlag\u003e(result, CreateFlag.class);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convertCreateFlag(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d \n       EnumSet.noneOf(CreateFlag.class);   \n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n      result.add(CreateFlag.CREATE);\n    }\n    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n      result.add(CreateFlag.OVERWRITE);\n    }\n    if ((flag \u0026 CreateFlagProto.LAZY_PERSIST_VALUE)\n        \u003d\u003d CreateFlagProto.LAZY_PERSIST_VALUE) {\n      result.add(CreateFlag.LAZY_PERSIST);\n    }\n    if ((flag \u0026 CreateFlagProto.NEW_BLOCK_VALUE)\n        \u003d\u003d CreateFlagProto.NEW_BLOCK_VALUE) {\n      result.add(CreateFlag.NEW_BLOCK);\n    }\n    return new EnumSetWritable\u003cCreateFlag\u003e(result, CreateFlag.class);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "a7bcc9535860214380e235641d1d5d2dd15aee58": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6921. Add LazyPersist flag to FileStatus. (Arpit Agarwal)\n",
      "commitDate": "27/08/14 9:47 PM",
      "commitName": "a7bcc9535860214380e235641d1d5d2dd15aee58",
      "commitAuthor": "arp",
      "commitDateOld": "04/08/14 7:30 PM",
      "commitNameOld": "ac73d416f3cfbc9561286ce4bc7624113db4e751",
      "commitAuthorOld": "",
      "daysBetweenCommits": 23.09,
      "commitsBetweenForRepo": 190,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,19 @@\n   public static EnumSetWritable\u003cCreateFlag\u003e convertCreateFlag(int flag) {\n     EnumSet\u003cCreateFlag\u003e result \u003d \n        EnumSet.noneOf(CreateFlag.class);   \n     if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n       result.add(CreateFlag.APPEND);\n     }\n     if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n       result.add(CreateFlag.CREATE);\n     }\n     if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n         \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n       result.add(CreateFlag.OVERWRITE);\n     }\n+    if ((flag \u0026 CreateFlagProto.LAZY_PERSIST_VALUE)\n+        \u003d\u003d CreateFlagProto.LAZY_PERSIST_VALUE) {\n+      result.add(CreateFlag.LAZY_PERSIST);\n+    }\n     return new EnumSetWritable\u003cCreateFlag\u003e(result);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convertCreateFlag(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d \n       EnumSet.noneOf(CreateFlag.class);   \n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n      result.add(CreateFlag.CREATE);\n    }\n    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n      result.add(CreateFlag.OVERWRITE);\n    }\n    if ((flag \u0026 CreateFlagProto.LAZY_PERSIST_VALUE)\n        \u003d\u003d CreateFlagProto.LAZY_PERSIST_VALUE) {\n      result.add(CreateFlag.LAZY_PERSIST);\n    }\n    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "991c453ca3ac141a3f286f74af8401f83c38b230": {
      "type": "Yrename",
      "commitMessage": "HDFS-5431. Support cachepool-based limit management in path-based caching. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551651 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 10:47 AM",
      "commitName": "991c453ca3ac141a3f286f74af8401f83c38b230",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "05/12/13 3:41 PM",
      "commitNameOld": "00718c2ffaa11cbdabac6f5ef4b2de5dcf9d6859",
      "commitAuthorOld": "",
      "daysBetweenCommits": 11.8,
      "commitsBetweenForRepo": 65,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n-  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n+  public static EnumSetWritable\u003cCreateFlag\u003e convertCreateFlag(int flag) {\n     EnumSet\u003cCreateFlag\u003e result \u003d \n        EnumSet.noneOf(CreateFlag.class);   \n     if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n       result.add(CreateFlag.APPEND);\n     }\n     if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n       result.add(CreateFlag.CREATE);\n     }\n     if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n         \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n       result.add(CreateFlag.OVERWRITE);\n     }\n     return new EnumSetWritable\u003cCreateFlag\u003e(result);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convertCreateFlag(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d \n       EnumSet.noneOf(CreateFlag.class);   \n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n      result.add(CreateFlag.CREATE);\n    }\n    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n      result.add(CreateFlag.OVERWRITE);\n    }\n    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {
        "oldValue": "convert",
        "newValue": "convertCreateFlag"
      }
    },
    "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e": {
      "type": "Ybodychange",
      "commitMessage": "Merge trunk into HA branch.\n\nSeveral conflicts around introduction of protobuf translator for DatanodeProtocol - mostly trivial resolutions.\n\nNB: this does not successfully pass any tests since the HAStatus field needs\nto be integrated into the HeartbeatResponse Protobuf implementation.\nThat will be a separate commit for clearer history.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1214518 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/12/11 2:47 PM",
      "commitName": "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "13/12/11 11:02 AM",
      "commitNameOld": "a0fe4f476ae907c9c070af48a250739a4fb33362",
      "commitAuthorOld": "",
      "daysBetweenCommits": 1.16,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,15 @@\n   public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n     EnumSet\u003cCreateFlag\u003e result \u003d \n        EnumSet.noneOf(CreateFlag.class);   \n     if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n       result.add(CreateFlag.APPEND);\n     }\n+    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n+      result.add(CreateFlag.CREATE);\n+    }\n+    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n+        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n+      result.add(CreateFlag.OVERWRITE);\n+    }\n     return new EnumSetWritable\u003cCreateFlag\u003e(result);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d \n       EnumSet.noneOf(CreateFlag.class);   \n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n      result.add(CreateFlag.CREATE);\n    }\n    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n      result.add(CreateFlag.OVERWRITE);\n    }\n    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e": {
      "type": "Ybodychange",
      "commitMessage": "    HDFS-2669 Enable protobuf rpc for ClientNamenodeProtocol\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1214128 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/12/11 1:27 AM",
      "commitName": "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e",
      "commitAuthor": "Sanjay Radia",
      "commitDateOld": "13/12/11 6:15 PM",
      "commitNameOld": "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 0.3,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,15 @@\n   public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n     EnumSet\u003cCreateFlag\u003e result \u003d \n        EnumSet.noneOf(CreateFlag.class);   \n     if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n       result.add(CreateFlag.APPEND);\n     }\n+    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n+      result.add(CreateFlag.CREATE);\n+    }\n+    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n+        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n+      result.add(CreateFlag.OVERWRITE);\n+    }\n     return new EnumSetWritable\u003cCreateFlag\u003e(result);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d \n       EnumSet.noneOf(CreateFlag.class);   \n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n      result.add(CreateFlag.CREATE);\n    }\n    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n      result.add(CreateFlag.OVERWRITE);\n    }\n    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "48da033901d3471ef176a94104158546152353e9": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/12/11 9:36 PM",
      "commitName": "48da033901d3471ef176a94104158546152353e9",
      "commitAuthor": "Sanjay Radia",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,8 @@\n-  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n-    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n-    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n-      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n+    EnumSet\u003cCreateFlag\u003e result \u003d \n+       EnumSet.noneOf(CreateFlag.class);   \n+    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n+      result.add(CreateFlag.APPEND);\n     }\n-    return builder.build();\n+    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d \n       EnumSet.noneOf(CreateFlag.class);   \n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "[datanodeInfos-DatanodeInfo[]]",
            "newValue": "[flag-int]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,8 @@\n-  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n-    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n-    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n-      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n+    EnumSet\u003cCreateFlag\u003e result \u003d \n+       EnumSet.noneOf(CreateFlag.class);   \n+    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n+      result.add(CreateFlag.APPEND);\n     }\n-    return builder.build();\n+    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d \n       EnumSet.noneOf(CreateFlag.class);   \n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "DatanodeInfosProto",
            "newValue": "EnumSetWritable\u003cCreateFlag\u003e"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,8 @@\n-  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n-    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n-    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n-      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n+    EnumSet\u003cCreateFlag\u003e result \u003d \n+       EnumSet.noneOf(CreateFlag.class);   \n+    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n+      result.add(CreateFlag.APPEND);\n     }\n-    return builder.build();\n+    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d \n       EnumSet.noneOf(CreateFlag.class);   \n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {}
        }
      ]
    },
    "38a19bc293dec6221ae96e304fc6ab660d94e706": {
      "type": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/12/11 12:02 PM",
      "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
      "commitAuthor": "Jitendra Nath Pandey",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/12/11 12:02 PM",
          "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
          "commitAuthor": "Jitendra Nath Pandey",
          "commitDateOld": "09/12/11 11:28 AM",
          "commitNameOld": "66fce20802653ebed492024b01441644e028036a",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,7 @@\n-  private NamespaceInfoProto convert(NamespaceInfo info) {\n-    return NamespaceInfoProto.newBuilder()\n-        .setBlockPoolID(info.getBlockPoolID())\n-        .setBuildVersion(info.getBuildVersion())\n-        .setDistUpgradeVersion(info.getDistributedUpgradeVersion())\n-        .setStorageInfo(PBHelper.convert(info)).build();\n+  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n+    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n+    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n+      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n    }\n    return builder.build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolServerSideTranslatorPB.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
            "oldMethodName": "convert",
            "newMethodName": "convert"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/12/11 12:02 PM",
          "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
          "commitAuthor": "Jitendra Nath Pandey",
          "commitDateOld": "09/12/11 11:28 AM",
          "commitNameOld": "66fce20802653ebed492024b01441644e028036a",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,7 @@\n-  private NamespaceInfoProto convert(NamespaceInfo info) {\n-    return NamespaceInfoProto.newBuilder()\n-        .setBlockPoolID(info.getBlockPoolID())\n-        .setBuildVersion(info.getBuildVersion())\n-        .setDistUpgradeVersion(info.getDistributedUpgradeVersion())\n-        .setStorageInfo(PBHelper.convert(info)).build();\n+  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n+    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n+    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n+      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n    }\n    return builder.build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "NamespaceInfoProto",
            "newValue": "DatanodeInfosProto"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/12/11 12:02 PM",
          "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
          "commitAuthor": "Jitendra Nath Pandey",
          "commitDateOld": "09/12/11 11:28 AM",
          "commitNameOld": "66fce20802653ebed492024b01441644e028036a",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,7 @@\n-  private NamespaceInfoProto convert(NamespaceInfo info) {\n-    return NamespaceInfoProto.newBuilder()\n-        .setBlockPoolID(info.getBlockPoolID())\n-        .setBuildVersion(info.getBuildVersion())\n-        .setDistUpgradeVersion(info.getDistributedUpgradeVersion())\n-        .setStorageInfo(PBHelper.convert(info)).build();\n+  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n+    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n+    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n+      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n    }\n    return builder.build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[public, static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/12/11 12:02 PM",
          "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
          "commitAuthor": "Jitendra Nath Pandey",
          "commitDateOld": "09/12/11 11:28 AM",
          "commitNameOld": "66fce20802653ebed492024b01441644e028036a",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,7 @@\n-  private NamespaceInfoProto convert(NamespaceInfo info) {\n-    return NamespaceInfoProto.newBuilder()\n-        .setBlockPoolID(info.getBlockPoolID())\n-        .setBuildVersion(info.getBuildVersion())\n-        .setDistUpgradeVersion(info.getDistributedUpgradeVersion())\n-        .setStorageInfo(PBHelper.convert(info)).build();\n+  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n+    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n+    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n+      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n    }\n    return builder.build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/12/11 12:02 PM",
          "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
          "commitAuthor": "Jitendra Nath Pandey",
          "commitDateOld": "09/12/11 11:28 AM",
          "commitNameOld": "66fce20802653ebed492024b01441644e028036a",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,7 @@\n-  private NamespaceInfoProto convert(NamespaceInfo info) {\n-    return NamespaceInfoProto.newBuilder()\n-        .setBlockPoolID(info.getBlockPoolID())\n-        .setBuildVersion(info.getBuildVersion())\n-        .setDistUpgradeVersion(info.getDistributedUpgradeVersion())\n-        .setStorageInfo(PBHelper.convert(info)).build();\n+  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n+    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n+    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n+      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n    }\n    return builder.build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "[info-NamespaceInfo]",
            "newValue": "[datanodeInfos-DatanodeInfo[]]"
          }
        }
      ]
    },
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2618. Implement protobuf service for NamenodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1210719 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/12/11 4:25 PM",
      "commitName": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
      "commitAuthor": "Suresh Srinivas",
      "diff": "@@ -0,0 +1,7 @@\n+  private NamespaceInfoProto convert(NamespaceInfo info) {\n+    return NamespaceInfoProto.newBuilder()\n+        .setBlockPoolID(info.getBlockPoolID())\n+        .setBuildVersion(info.getBuildVersion())\n+        .setDistUpgradeVersion(info.getDistributedUpgradeVersion())\n+        .setStorageInfo(PBHelper.convert(info)).build();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private NamespaceInfoProto convert(NamespaceInfo info) {\n    return NamespaceInfoProto.newBuilder()\n        .setBlockPoolID(info.getBlockPoolID())\n        .setBuildVersion(info.getBuildVersion())\n        .setDistUpgradeVersion(info.getDistributedUpgradeVersion())\n        .setStorageInfo(PBHelper.convert(info)).build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolServerSideTranslatorPB.java"
    }
  }
}