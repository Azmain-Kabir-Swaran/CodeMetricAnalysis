{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "HAUtil.java",
  "functionName": "isAtLeastOneActive",
  "functionId": "isAtLeastOneActive___namenodes-List__ClientProtocol__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HAUtil.java",
  "functionStartLine": 355,
  "functionEndLine": 377,
  "numCommitsSeen": 39,
  "timeTaken": 2295,
  "changeHistory": [
    "01bd6ab18fa48f4c7cac1497905b52e547962599",
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de"
  ],
  "changeHistoryShort": {
    "01bd6ab18fa48f4c7cac1497905b52e547962599": "Ybodychange",
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de": "Yintroduced"
  },
  "changeHistoryDetails": {
    "01bd6ab18fa48f4c7cac1497905b52e547962599": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12935. Get ambiguous result for DFSAdmin command in HA mode when only one namenode is up. Contributed by Jianfei Jiang.\n",
      "commitDate": "07/02/18 9:40 AM",
      "commitName": "01bd6ab18fa48f4c7cac1497905b52e547962599",
      "commitAuthor": "Brahma Reddy Battula",
      "commitDateOld": "04/04/17 11:05 PM",
      "commitNameOld": "9e0e430f18d45cfe125dda8d85916edddf79e8d6",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 308.48,
      "commitsBetweenForRepo": 2074,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,23 @@\n   public static boolean isAtLeastOneActive(List\u003cClientProtocol\u003e namenodes)\n       throws IOException {\n+    List\u003cIOException\u003e exceptions \u003d new ArrayList\u003c\u003e();\n     for (ClientProtocol namenode : namenodes) {\n       try {\n         namenode.getFileInfo(\"/\");\n         return true;\n       } catch (RemoteException re) {\n         IOException cause \u003d re.unwrapRemoteException();\n         if (cause instanceof StandbyException) {\n           // This is expected to happen for a standby NN.\n         } else {\n-          throw re;\n+          exceptions.add(re);\n         }\n+      } catch (IOException ioe) {\n+        exceptions.add(ioe);\n       }\n     }\n+    if(!exceptions.isEmpty()){\n+      throw MultipleIOException.createIOException(exceptions);\n+    }\n     return false;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static boolean isAtLeastOneActive(List\u003cClientProtocol\u003e namenodes)\n      throws IOException {\n    List\u003cIOException\u003e exceptions \u003d new ArrayList\u003c\u003e();\n    for (ClientProtocol namenode : namenodes) {\n      try {\n        namenode.getFileInfo(\"/\");\n        return true;\n      } catch (RemoteException re) {\n        IOException cause \u003d re.unwrapRemoteException();\n        if (cause instanceof StandbyException) {\n          // This is expected to happen for a standby NN.\n        } else {\n          exceptions.add(re);\n        }\n      } catch (IOException ioe) {\n        exceptions.add(ioe);\n      }\n    }\n    if(!exceptions.isEmpty()){\n      throw MultipleIOException.createIOException(exceptions);\n    }\n    return false;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HAUtil.java",
      "extendedDetails": {}
    },
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5138. Support HDFS upgrade in HA. Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1561381 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/01/14 12:01 PM",
      "commitName": "edb6dc5f303093c2604cd07b0c0dacf12dbce5de",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,17 @@\n+  public static boolean isAtLeastOneActive(List\u003cClientProtocol\u003e namenodes)\n+      throws IOException {\n+    for (ClientProtocol namenode : namenodes) {\n+      try {\n+        namenode.getFileInfo(\"/\");\n+        return true;\n+      } catch (RemoteException re) {\n+        IOException cause \u003d re.unwrapRemoteException();\n+        if (cause instanceof StandbyException) {\n+          // This is expected to happen for a standby NN.\n+        } else {\n+          throw re;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static boolean isAtLeastOneActive(List\u003cClientProtocol\u003e namenodes)\n      throws IOException {\n    for (ClientProtocol namenode : namenodes) {\n      try {\n        namenode.getFileInfo(\"/\");\n        return true;\n      } catch (RemoteException re) {\n        IOException cause \u003d re.unwrapRemoteException();\n        if (cause instanceof StandbyException) {\n          // This is expected to happen for a standby NN.\n        } else {\n          throw re;\n        }\n      }\n    }\n    return false;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HAUtil.java"
    }
  }
}