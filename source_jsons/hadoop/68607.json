{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "StreamJob.java",
  "functionName": "printUsage",
  "functionId": "printUsage___detailed-boolean",
  "sourceFilePath": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
  "functionStartLine": 505,
  "functionEndLine": 641,
  "numCommitsSeen": 22,
  "timeTaken": 5426,
  "changeHistory": [
    "0a74610d1c7c7f183d2b2d0b7a775add53cf6c94",
    "cc70df98e74142331043a611a3bd8a53ff6a2242",
    "26447229ba2c3d43db978c1b3ce95613669182ee",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "0a74610d1c7c7f183d2b2d0b7a775add53cf6c94": "Ybodychange",
    "cc70df98e74142331043a611a3bd8a53ff6a2242": "Ybodychange",
    "26447229ba2c3d43db978c1b3ce95613669182ee": "Yfilerename",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "0a74610d1c7c7f183d2b2d0b7a775add53cf6c94": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11393. Revert HADOOP_PREFIX, go back to HADOOP_HOME (aw)\n",
      "commitDate": "31/03/16 7:51 AM",
      "commitName": "0a74610d1c7c7f183d2b2d0b7a775add53cf6c94",
      "commitAuthor": "Allen Wittenauer",
      "commitDateOld": "03/06/15 6:41 PM",
      "commitNameOld": "cc70df98e74142331043a611a3bd8a53ff6a2242",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 301.55,
      "commitsBetweenForRepo": 2013,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,137 +1,137 @@\n   private void printUsage(boolean detailed) {\n-    System.out.println(\"Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar\"\n+    System.out.println(\"Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar\"\n         + \" [options]\");\n     System.out.println(\"Options:\");\n     System.out.println(\"  -input          \u003cpath\u003e DFS input file(s) for the Map\"\n         + \" step.\");\n     System.out.println(\"  -output         \u003cpath\u003e DFS output directory for the\"\n         + \" Reduce step.\");\n     System.out.println(\"  -mapper         \u003ccmd|JavaClassName\u003e Optional. Command\"\n         + \" to be run as mapper.\");\n     System.out.println(\"  -combiner       \u003ccmd|JavaClassName\u003e Optional. Command\"\n         + \" to be run as combiner.\");\n     System.out.println(\"  -reducer        \u003ccmd|JavaClassName\u003e Optional. Command\"\n         + \" to be run as reducer.\");\n     System.out.println(\"  -file           \u003cfile\u003e Optional. File/dir to be \"\n         + \"shipped in the Job jar file.\\n\" +\n         \"                  Deprecated. Use generic option \\\"-files\\\" instead.\");\n     System.out.println(\"  -inputformat    \u003cTextInputFormat(default)\"\n         + \"|SequenceFileAsTextInputFormat|JavaClassName\u003e\\n\"\n         + \"                  Optional. The input format class.\");\n     System.out.println(\"  -outputformat   \u003cTextOutputFormat(default)\"\n         + \"|JavaClassName\u003e\\n\"\n         + \"                  Optional. The output format class.\");\n     System.out.println(\"  -partitioner    \u003cJavaClassName\u003e  Optional. The\"\n         + \" partitioner class.\");\n     System.out.println(\"  -numReduceTasks \u003cnum\u003e Optional. Number of reduce \"\n         + \"tasks.\");\n     System.out.println(\"  -inputreader    \u003cspec\u003e Optional. Input recordreader\"\n         + \" spec.\");\n     System.out.println(\"  -cmdenv         \u003cn\u003e\u003d\u003cv\u003e Optional. Pass env.var to\"\n         + \" streaming commands.\");\n     System.out.println(\"  -mapdebug       \u003ccmd\u003e Optional. \"\n         + \"To run this script when a map task fails.\");\n     System.out.println(\"  -reducedebug    \u003ccmd\u003e Optional.\"\n         + \" To run this script when a reduce task fails.\");\n     System.out.println(\"  -io             \u003cidentifier\u003e Optional. Format to use\"\n         + \" for input to and output\");\n     System.out.println(\"                  from mapper/reducer commands\");\n     System.out.println(\"  -lazyOutput     Optional. Lazily create Output.\");\n     System.out.println(\"  -background     Optional. Submit the job and don\u0027t wait till it completes.\");\n     System.out.println(\"  -verbose        Optional. Print verbose output.\");\n     System.out.println(\"  -info           Optional. Print detailed usage.\");\n     System.out.println(\"  -help           Optional. Print help message.\");\n     System.out.println();\n     GenericOptionsParser.printGenericCommandUsage(System.out);\n \n     if (!detailed) {\n       System.out.println();\n       System.out.println(\"For more details about these options:\");\n       System.out.println(\"Use \" +\n-          \"$HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar -info\");\n+          \"$HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\");\n       return;\n     }\n     System.out.println();\n     System.out.println(\"Usage tips:\");\n     System.out.println(\"In -input: globbing on \u003cpath\u003e is supported and can \"\n         + \"have multiple -input\");\n     System.out.println();\n     System.out.println(\"Default Map input format: a line is a record in UTF-8 \"\n         + \"the key part ends at first\");\n     System.out.println(\"  TAB, the rest of the line is the value\");\n     System.out.println();\n     System.out.println(\"To pass a Custom input format:\");\n     System.out.println(\"  -inputformat package.MyInputFormat\");\n     System.out.println();\n     System.out.println(\"Similarly, to pass a custom output format:\");\n     System.out.println(\"  -outputformat package.MyOutputFormat\");\n     System.out.println();\n     System.out.println(\"The files with extensions .class and .jar/.zip,\" +\n         \" specified for the -file\");\n     System.out.println(\"  argument[s], end up in \\\"classes\\\" and \\\"lib\\\" \" +\n         \"directories respectively inside\");\n     System.out.println(\"  the working directory when the mapper and reducer are\"\n         + \" run. All other files\");\n     System.out.println(\"  specified for the -file argument[s]\" +\n         \" end up in the working directory when the\");\n     System.out.println(\"  mapper and reducer are run. The location of this \" +\n         \"working directory is\");\n     System.out.println(\"  unspecified.\");\n     System.out.println();\n     System.out.println(\"To set the number of reduce tasks (num. of output \" +\n         \"files) as, say 10:\");\n     System.out.println(\"  Use -numReduceTasks 10\");\n     System.out.println(\"To skip the sort/combine/shuffle/sort/reduce step:\");\n     System.out.println(\"  Use -numReduceTasks 0\");\n     System.out.println(\"  Map output then becomes a \u0027side-effect \" +\n         \"output\u0027 rather than a reduce input.\");\n     System.out.println(\"  This speeds up processing. This also feels \" +\n         \"more like \\\"in-place\\\" processing\");\n     System.out.println(\"  because the input filename and the map \" +\n         \"input order are preserved.\");\n     System.out.println(\"  This is equivalent to -reducer NONE\");\n     System.out.println();\n     System.out.println(\"To speed up the last maps:\");\n     System.out.println(\"  -D \" + MRJobConfig.MAP_SPECULATIVE + \"\u003dtrue\");\n     System.out.println(\"To speed up the last reduces:\");\n     System.out.println(\"  -D \" + MRJobConfig.REDUCE_SPECULATIVE + \"\u003dtrue\");\n     System.out.println(\"To name the job (appears in the JobTracker Web UI):\");\n     System.out.println(\"  -D \" + MRJobConfig.JOB_NAME + \"\u003d\u0027My Job\u0027\");\n     System.out.println(\"To change the local temp directory:\");\n     System.out.println(\"  -D dfs.data.dir\u003d/tmp/dfs\");\n     System.out.println(\"  -D stream.tmpdir\u003d/tmp/streaming\");\n     System.out.println(\"Additional local temp directories with -jt local:\");\n     System.out.println(\"  -D \" + MRConfig.LOCAL_DIR + \"\u003d/tmp/local\");\n     System.out.println(\"  -D \" + JTConfig.JT_SYSTEM_DIR + \"\u003d/tmp/system\");\n     System.out.println(\"  -D \" + MRConfig.TEMP_DIR + \"\u003d/tmp/temp\");\n     System.out.println(\"To treat tasks with non-zero exit status as SUCCEDED:\");\n     System.out.println(\"  -D stream.non.zero.exit.is.failure\u003dfalse\");\n     System.out.println(\"Use a custom hadoop streaming build along with standard\"\n         + \" hadoop install:\");\n-    System.out.println(\"  $HADOOP_PREFIX/bin/hadoop jar \" +\n+    System.out.println(\"  $HADOOP_HOME/bin/hadoop jar \" +\n         \"/path/my-hadoop-streaming.jar [...]\\\\\");\n     System.out.println(\"    [...] -D stream.shipped.hadoopstreaming\u003d\" +\n         \"/path/my-hadoop-streaming.jar\");\n     System.out.println(\"For more details about jobconf parameters see:\");\n     System.out.println(\"  http://wiki.apache.org/hadoop/JobConfFile\");\n     System.out.println(\"Truncate the values of the job configuration copied\" +\n         \"to the environment at the given length:\");\n     System.out.println(\"   -D stream.jobconf.truncate.limit\u003d-1\");\n     System.out.println(\"To set an environment variable in a streaming \" +\n         \"command:\");\n     System.out.println(\"   -cmdenv EXAMPLE_DIR\u003d/home/example/dictionaries/\");\n     System.out.println();\n     System.out.println(\"Shortcut:\");\n-    System.out.println(\"   setenv HSTREAMING \\\"$HADOOP_PREFIX/bin/hadoop jar \" +\n+    System.out.println(\"   setenv HSTREAMING \\\"$HADOOP_HOME/bin/hadoop jar \" +\n         \"hadoop-streaming.jar\\\"\");\n     System.out.println();\n     System.out.println(\"Example: $HSTREAMING -mapper \" +\n         \"\\\"/usr/local/bin/perl5 filter.pl\\\"\");\n     System.out.println(\"           -file /local/filter.pl -input \" +\n         \"\\\"/logs/0604*/*\\\" [...]\");\n     System.out.println(\"  Ships a script, invokes the non-shipped perl \" +\n         \"interpreter. Shipped files go to\");\n     System.out.println(\"  the working directory so filter.pl is found by perl. \"\n         + \"Input files are all the\");\n     System.out.println(\"  daily logs for days in month 2006-04\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void printUsage(boolean detailed) {\n    System.out.println(\"Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar\"\n        + \" [options]\");\n    System.out.println(\"Options:\");\n    System.out.println(\"  -input          \u003cpath\u003e DFS input file(s) for the Map\"\n        + \" step.\");\n    System.out.println(\"  -output         \u003cpath\u003e DFS output directory for the\"\n        + \" Reduce step.\");\n    System.out.println(\"  -mapper         \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as mapper.\");\n    System.out.println(\"  -combiner       \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as combiner.\");\n    System.out.println(\"  -reducer        \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as reducer.\");\n    System.out.println(\"  -file           \u003cfile\u003e Optional. File/dir to be \"\n        + \"shipped in the Job jar file.\\n\" +\n        \"                  Deprecated. Use generic option \\\"-files\\\" instead.\");\n    System.out.println(\"  -inputformat    \u003cTextInputFormat(default)\"\n        + \"|SequenceFileAsTextInputFormat|JavaClassName\u003e\\n\"\n        + \"                  Optional. The input format class.\");\n    System.out.println(\"  -outputformat   \u003cTextOutputFormat(default)\"\n        + \"|JavaClassName\u003e\\n\"\n        + \"                  Optional. The output format class.\");\n    System.out.println(\"  -partitioner    \u003cJavaClassName\u003e  Optional. The\"\n        + \" partitioner class.\");\n    System.out.println(\"  -numReduceTasks \u003cnum\u003e Optional. Number of reduce \"\n        + \"tasks.\");\n    System.out.println(\"  -inputreader    \u003cspec\u003e Optional. Input recordreader\"\n        + \" spec.\");\n    System.out.println(\"  -cmdenv         \u003cn\u003e\u003d\u003cv\u003e Optional. Pass env.var to\"\n        + \" streaming commands.\");\n    System.out.println(\"  -mapdebug       \u003ccmd\u003e Optional. \"\n        + \"To run this script when a map task fails.\");\n    System.out.println(\"  -reducedebug    \u003ccmd\u003e Optional.\"\n        + \" To run this script when a reduce task fails.\");\n    System.out.println(\"  -io             \u003cidentifier\u003e Optional. Format to use\"\n        + \" for input to and output\");\n    System.out.println(\"                  from mapper/reducer commands\");\n    System.out.println(\"  -lazyOutput     Optional. Lazily create Output.\");\n    System.out.println(\"  -background     Optional. Submit the job and don\u0027t wait till it completes.\");\n    System.out.println(\"  -verbose        Optional. Print verbose output.\");\n    System.out.println(\"  -info           Optional. Print detailed usage.\");\n    System.out.println(\"  -help           Optional. Print help message.\");\n    System.out.println();\n    GenericOptionsParser.printGenericCommandUsage(System.out);\n\n    if (!detailed) {\n      System.out.println();\n      System.out.println(\"For more details about these options:\");\n      System.out.println(\"Use \" +\n          \"$HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\");\n      return;\n    }\n    System.out.println();\n    System.out.println(\"Usage tips:\");\n    System.out.println(\"In -input: globbing on \u003cpath\u003e is supported and can \"\n        + \"have multiple -input\");\n    System.out.println();\n    System.out.println(\"Default Map input format: a line is a record in UTF-8 \"\n        + \"the key part ends at first\");\n    System.out.println(\"  TAB, the rest of the line is the value\");\n    System.out.println();\n    System.out.println(\"To pass a Custom input format:\");\n    System.out.println(\"  -inputformat package.MyInputFormat\");\n    System.out.println();\n    System.out.println(\"Similarly, to pass a custom output format:\");\n    System.out.println(\"  -outputformat package.MyOutputFormat\");\n    System.out.println();\n    System.out.println(\"The files with extensions .class and .jar/.zip,\" +\n        \" specified for the -file\");\n    System.out.println(\"  argument[s], end up in \\\"classes\\\" and \\\"lib\\\" \" +\n        \"directories respectively inside\");\n    System.out.println(\"  the working directory when the mapper and reducer are\"\n        + \" run. All other files\");\n    System.out.println(\"  specified for the -file argument[s]\" +\n        \" end up in the working directory when the\");\n    System.out.println(\"  mapper and reducer are run. The location of this \" +\n        \"working directory is\");\n    System.out.println(\"  unspecified.\");\n    System.out.println();\n    System.out.println(\"To set the number of reduce tasks (num. of output \" +\n        \"files) as, say 10:\");\n    System.out.println(\"  Use -numReduceTasks 10\");\n    System.out.println(\"To skip the sort/combine/shuffle/sort/reduce step:\");\n    System.out.println(\"  Use -numReduceTasks 0\");\n    System.out.println(\"  Map output then becomes a \u0027side-effect \" +\n        \"output\u0027 rather than a reduce input.\");\n    System.out.println(\"  This speeds up processing. This also feels \" +\n        \"more like \\\"in-place\\\" processing\");\n    System.out.println(\"  because the input filename and the map \" +\n        \"input order are preserved.\");\n    System.out.println(\"  This is equivalent to -reducer NONE\");\n    System.out.println();\n    System.out.println(\"To speed up the last maps:\");\n    System.out.println(\"  -D \" + MRJobConfig.MAP_SPECULATIVE + \"\u003dtrue\");\n    System.out.println(\"To speed up the last reduces:\");\n    System.out.println(\"  -D \" + MRJobConfig.REDUCE_SPECULATIVE + \"\u003dtrue\");\n    System.out.println(\"To name the job (appears in the JobTracker Web UI):\");\n    System.out.println(\"  -D \" + MRJobConfig.JOB_NAME + \"\u003d\u0027My Job\u0027\");\n    System.out.println(\"To change the local temp directory:\");\n    System.out.println(\"  -D dfs.data.dir\u003d/tmp/dfs\");\n    System.out.println(\"  -D stream.tmpdir\u003d/tmp/streaming\");\n    System.out.println(\"Additional local temp directories with -jt local:\");\n    System.out.println(\"  -D \" + MRConfig.LOCAL_DIR + \"\u003d/tmp/local\");\n    System.out.println(\"  -D \" + JTConfig.JT_SYSTEM_DIR + \"\u003d/tmp/system\");\n    System.out.println(\"  -D \" + MRConfig.TEMP_DIR + \"\u003d/tmp/temp\");\n    System.out.println(\"To treat tasks with non-zero exit status as SUCCEDED:\");\n    System.out.println(\"  -D stream.non.zero.exit.is.failure\u003dfalse\");\n    System.out.println(\"Use a custom hadoop streaming build along with standard\"\n        + \" hadoop install:\");\n    System.out.println(\"  $HADOOP_HOME/bin/hadoop jar \" +\n        \"/path/my-hadoop-streaming.jar [...]\\\\\");\n    System.out.println(\"    [...] -D stream.shipped.hadoopstreaming\u003d\" +\n        \"/path/my-hadoop-streaming.jar\");\n    System.out.println(\"For more details about jobconf parameters see:\");\n    System.out.println(\"  http://wiki.apache.org/hadoop/JobConfFile\");\n    System.out.println(\"Truncate the values of the job configuration copied\" +\n        \"to the environment at the given length:\");\n    System.out.println(\"   -D stream.jobconf.truncate.limit\u003d-1\");\n    System.out.println(\"To set an environment variable in a streaming \" +\n        \"command:\");\n    System.out.println(\"   -cmdenv EXAMPLE_DIR\u003d/home/example/dictionaries/\");\n    System.out.println();\n    System.out.println(\"Shortcut:\");\n    System.out.println(\"   setenv HSTREAMING \\\"$HADOOP_HOME/bin/hadoop jar \" +\n        \"hadoop-streaming.jar\\\"\");\n    System.out.println();\n    System.out.println(\"Example: $HSTREAMING -mapper \" +\n        \"\\\"/usr/local/bin/perl5 filter.pl\\\"\");\n    System.out.println(\"           -file /local/filter.pl -input \" +\n        \"\\\"/logs/0604*/*\\\" [...]\");\n    System.out.println(\"  Ships a script, invokes the non-shipped perl \" +\n        \"interpreter. Shipped files go to\");\n    System.out.println(\"  the working directory so filter.pl is found by perl. \"\n        + \"Input files are all the\");\n    System.out.println(\"  daily logs for days in month 2006-04\");\n  }",
      "path": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {}
    },
    "cc70df98e74142331043a611a3bd8a53ff6a2242": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5965. Hadoop streaming throws error if list of input files is high. Error is: \"error\u003d7, Argument list too long at if number of input file is high\" (wilfreds via rkanter)\n",
      "commitDate": "03/06/15 6:41 PM",
      "commitName": "cc70df98e74142331043a611a3bd8a53ff6a2242",
      "commitAuthor": "Robert Kanter",
      "commitDateOld": "19/09/14 11:33 AM",
      "commitNameOld": "9f03a7c018bb2c497cd0ef758f1a3e08e8163d06",
      "commitAuthorOld": "Allen Wittenauer",
      "daysBetweenCommits": 257.3,
      "commitsBetweenForRepo": 2217,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,134 +1,137 @@\n   private void printUsage(boolean detailed) {\n     System.out.println(\"Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar\"\n         + \" [options]\");\n     System.out.println(\"Options:\");\n     System.out.println(\"  -input          \u003cpath\u003e DFS input file(s) for the Map\"\n         + \" step.\");\n     System.out.println(\"  -output         \u003cpath\u003e DFS output directory for the\"\n         + \" Reduce step.\");\n     System.out.println(\"  -mapper         \u003ccmd|JavaClassName\u003e Optional. Command\"\n         + \" to be run as mapper.\");\n     System.out.println(\"  -combiner       \u003ccmd|JavaClassName\u003e Optional. Command\"\n         + \" to be run as combiner.\");\n     System.out.println(\"  -reducer        \u003ccmd|JavaClassName\u003e Optional. Command\"\n         + \" to be run as reducer.\");\n     System.out.println(\"  -file           \u003cfile\u003e Optional. File/dir to be \"\n         + \"shipped in the Job jar file.\\n\" +\n         \"                  Deprecated. Use generic option \\\"-files\\\" instead.\");\n     System.out.println(\"  -inputformat    \u003cTextInputFormat(default)\"\n         + \"|SequenceFileAsTextInputFormat|JavaClassName\u003e\\n\"\n         + \"                  Optional. The input format class.\");\n     System.out.println(\"  -outputformat   \u003cTextOutputFormat(default)\"\n         + \"|JavaClassName\u003e\\n\"\n         + \"                  Optional. The output format class.\");\n     System.out.println(\"  -partitioner    \u003cJavaClassName\u003e  Optional. The\"\n         + \" partitioner class.\");\n     System.out.println(\"  -numReduceTasks \u003cnum\u003e Optional. Number of reduce \"\n         + \"tasks.\");\n     System.out.println(\"  -inputreader    \u003cspec\u003e Optional. Input recordreader\"\n         + \" spec.\");\n     System.out.println(\"  -cmdenv         \u003cn\u003e\u003d\u003cv\u003e Optional. Pass env.var to\"\n         + \" streaming commands.\");\n     System.out.println(\"  -mapdebug       \u003ccmd\u003e Optional. \"\n         + \"To run this script when a map task fails.\");\n     System.out.println(\"  -reducedebug    \u003ccmd\u003e Optional.\"\n         + \" To run this script when a reduce task fails.\");\n     System.out.println(\"  -io             \u003cidentifier\u003e Optional. Format to use\"\n         + \" for input to and output\");\n     System.out.println(\"                  from mapper/reducer commands\");\n     System.out.println(\"  -lazyOutput     Optional. Lazily create Output.\");\n     System.out.println(\"  -background     Optional. Submit the job and don\u0027t wait till it completes.\");\n     System.out.println(\"  -verbose        Optional. Print verbose output.\");\n     System.out.println(\"  -info           Optional. Print detailed usage.\");\n     System.out.println(\"  -help           Optional. Print help message.\");\n     System.out.println();\n     GenericOptionsParser.printGenericCommandUsage(System.out);\n \n     if (!detailed) {\n       System.out.println();\n       System.out.println(\"For more details about these options:\");\n       System.out.println(\"Use \" +\n           \"$HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar -info\");\n       return;\n     }\n     System.out.println();\n     System.out.println(\"Usage tips:\");\n     System.out.println(\"In -input: globbing on \u003cpath\u003e is supported and can \"\n         + \"have multiple -input\");\n     System.out.println();\n     System.out.println(\"Default Map input format: a line is a record in UTF-8 \"\n         + \"the key part ends at first\");\n     System.out.println(\"  TAB, the rest of the line is the value\");\n     System.out.println();\n     System.out.println(\"To pass a Custom input format:\");\n     System.out.println(\"  -inputformat package.MyInputFormat\");\n     System.out.println();\n     System.out.println(\"Similarly, to pass a custom output format:\");\n     System.out.println(\"  -outputformat package.MyOutputFormat\");\n     System.out.println();\n     System.out.println(\"The files with extensions .class and .jar/.zip,\" +\n         \" specified for the -file\");\n     System.out.println(\"  argument[s], end up in \\\"classes\\\" and \\\"lib\\\" \" +\n         \"directories respectively inside\");\n     System.out.println(\"  the working directory when the mapper and reducer are\"\n         + \" run. All other files\");\n     System.out.println(\"  specified for the -file argument[s]\" +\n         \" end up in the working directory when the\");\n     System.out.println(\"  mapper and reducer are run. The location of this \" +\n         \"working directory is\");\n     System.out.println(\"  unspecified.\");\n     System.out.println();\n     System.out.println(\"To set the number of reduce tasks (num. of output \" +\n         \"files) as, say 10:\");\n     System.out.println(\"  Use -numReduceTasks 10\");\n     System.out.println(\"To skip the sort/combine/shuffle/sort/reduce step:\");\n     System.out.println(\"  Use -numReduceTasks 0\");\n     System.out.println(\"  Map output then becomes a \u0027side-effect \" +\n         \"output\u0027 rather than a reduce input.\");\n     System.out.println(\"  This speeds up processing. This also feels \" +\n         \"more like \\\"in-place\\\" processing\");\n     System.out.println(\"  because the input filename and the map \" +\n         \"input order are preserved.\");\n     System.out.println(\"  This is equivalent to -reducer NONE\");\n     System.out.println();\n     System.out.println(\"To speed up the last maps:\");\n     System.out.println(\"  -D \" + MRJobConfig.MAP_SPECULATIVE + \"\u003dtrue\");\n     System.out.println(\"To speed up the last reduces:\");\n     System.out.println(\"  -D \" + MRJobConfig.REDUCE_SPECULATIVE + \"\u003dtrue\");\n     System.out.println(\"To name the job (appears in the JobTracker Web UI):\");\n     System.out.println(\"  -D \" + MRJobConfig.JOB_NAME + \"\u003d\u0027My Job\u0027\");\n     System.out.println(\"To change the local temp directory:\");\n     System.out.println(\"  -D dfs.data.dir\u003d/tmp/dfs\");\n     System.out.println(\"  -D stream.tmpdir\u003d/tmp/streaming\");\n     System.out.println(\"Additional local temp directories with -jt local:\");\n     System.out.println(\"  -D \" + MRConfig.LOCAL_DIR + \"\u003d/tmp/local\");\n     System.out.println(\"  -D \" + JTConfig.JT_SYSTEM_DIR + \"\u003d/tmp/system\");\n     System.out.println(\"  -D \" + MRConfig.TEMP_DIR + \"\u003d/tmp/temp\");\n     System.out.println(\"To treat tasks with non-zero exit status as SUCCEDED:\");\n     System.out.println(\"  -D stream.non.zero.exit.is.failure\u003dfalse\");\n     System.out.println(\"Use a custom hadoop streaming build along with standard\"\n         + \" hadoop install:\");\n     System.out.println(\"  $HADOOP_PREFIX/bin/hadoop jar \" +\n         \"/path/my-hadoop-streaming.jar [...]\\\\\");\n     System.out.println(\"    [...] -D stream.shipped.hadoopstreaming\u003d\" +\n         \"/path/my-hadoop-streaming.jar\");\n     System.out.println(\"For more details about jobconf parameters see:\");\n     System.out.println(\"  http://wiki.apache.org/hadoop/JobConfFile\");\n-    System.out.println(\"To set an environement variable in a streaming \" +\n+    System.out.println(\"Truncate the values of the job configuration copied\" +\n+        \"to the environment at the given length:\");\n+    System.out.println(\"   -D stream.jobconf.truncate.limit\u003d-1\");\n+    System.out.println(\"To set an environment variable in a streaming \" +\n         \"command:\");\n     System.out.println(\"   -cmdenv EXAMPLE_DIR\u003d/home/example/dictionaries/\");\n     System.out.println();\n     System.out.println(\"Shortcut:\");\n     System.out.println(\"   setenv HSTREAMING \\\"$HADOOP_PREFIX/bin/hadoop jar \" +\n         \"hadoop-streaming.jar\\\"\");\n     System.out.println();\n     System.out.println(\"Example: $HSTREAMING -mapper \" +\n         \"\\\"/usr/local/bin/perl5 filter.pl\\\"\");\n     System.out.println(\"           -file /local/filter.pl -input \" +\n         \"\\\"/logs/0604*/*\\\" [...]\");\n     System.out.println(\"  Ships a script, invokes the non-shipped perl \" +\n         \"interpreter. Shipped files go to\");\n     System.out.println(\"  the working directory so filter.pl is found by perl. \"\n         + \"Input files are all the\");\n     System.out.println(\"  daily logs for days in month 2006-04\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void printUsage(boolean detailed) {\n    System.out.println(\"Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar\"\n        + \" [options]\");\n    System.out.println(\"Options:\");\n    System.out.println(\"  -input          \u003cpath\u003e DFS input file(s) for the Map\"\n        + \" step.\");\n    System.out.println(\"  -output         \u003cpath\u003e DFS output directory for the\"\n        + \" Reduce step.\");\n    System.out.println(\"  -mapper         \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as mapper.\");\n    System.out.println(\"  -combiner       \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as combiner.\");\n    System.out.println(\"  -reducer        \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as reducer.\");\n    System.out.println(\"  -file           \u003cfile\u003e Optional. File/dir to be \"\n        + \"shipped in the Job jar file.\\n\" +\n        \"                  Deprecated. Use generic option \\\"-files\\\" instead.\");\n    System.out.println(\"  -inputformat    \u003cTextInputFormat(default)\"\n        + \"|SequenceFileAsTextInputFormat|JavaClassName\u003e\\n\"\n        + \"                  Optional. The input format class.\");\n    System.out.println(\"  -outputformat   \u003cTextOutputFormat(default)\"\n        + \"|JavaClassName\u003e\\n\"\n        + \"                  Optional. The output format class.\");\n    System.out.println(\"  -partitioner    \u003cJavaClassName\u003e  Optional. The\"\n        + \" partitioner class.\");\n    System.out.println(\"  -numReduceTasks \u003cnum\u003e Optional. Number of reduce \"\n        + \"tasks.\");\n    System.out.println(\"  -inputreader    \u003cspec\u003e Optional. Input recordreader\"\n        + \" spec.\");\n    System.out.println(\"  -cmdenv         \u003cn\u003e\u003d\u003cv\u003e Optional. Pass env.var to\"\n        + \" streaming commands.\");\n    System.out.println(\"  -mapdebug       \u003ccmd\u003e Optional. \"\n        + \"To run this script when a map task fails.\");\n    System.out.println(\"  -reducedebug    \u003ccmd\u003e Optional.\"\n        + \" To run this script when a reduce task fails.\");\n    System.out.println(\"  -io             \u003cidentifier\u003e Optional. Format to use\"\n        + \" for input to and output\");\n    System.out.println(\"                  from mapper/reducer commands\");\n    System.out.println(\"  -lazyOutput     Optional. Lazily create Output.\");\n    System.out.println(\"  -background     Optional. Submit the job and don\u0027t wait till it completes.\");\n    System.out.println(\"  -verbose        Optional. Print verbose output.\");\n    System.out.println(\"  -info           Optional. Print detailed usage.\");\n    System.out.println(\"  -help           Optional. Print help message.\");\n    System.out.println();\n    GenericOptionsParser.printGenericCommandUsage(System.out);\n\n    if (!detailed) {\n      System.out.println();\n      System.out.println(\"For more details about these options:\");\n      System.out.println(\"Use \" +\n          \"$HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar -info\");\n      return;\n    }\n    System.out.println();\n    System.out.println(\"Usage tips:\");\n    System.out.println(\"In -input: globbing on \u003cpath\u003e is supported and can \"\n        + \"have multiple -input\");\n    System.out.println();\n    System.out.println(\"Default Map input format: a line is a record in UTF-8 \"\n        + \"the key part ends at first\");\n    System.out.println(\"  TAB, the rest of the line is the value\");\n    System.out.println();\n    System.out.println(\"To pass a Custom input format:\");\n    System.out.println(\"  -inputformat package.MyInputFormat\");\n    System.out.println();\n    System.out.println(\"Similarly, to pass a custom output format:\");\n    System.out.println(\"  -outputformat package.MyOutputFormat\");\n    System.out.println();\n    System.out.println(\"The files with extensions .class and .jar/.zip,\" +\n        \" specified for the -file\");\n    System.out.println(\"  argument[s], end up in \\\"classes\\\" and \\\"lib\\\" \" +\n        \"directories respectively inside\");\n    System.out.println(\"  the working directory when the mapper and reducer are\"\n        + \" run. All other files\");\n    System.out.println(\"  specified for the -file argument[s]\" +\n        \" end up in the working directory when the\");\n    System.out.println(\"  mapper and reducer are run. The location of this \" +\n        \"working directory is\");\n    System.out.println(\"  unspecified.\");\n    System.out.println();\n    System.out.println(\"To set the number of reduce tasks (num. of output \" +\n        \"files) as, say 10:\");\n    System.out.println(\"  Use -numReduceTasks 10\");\n    System.out.println(\"To skip the sort/combine/shuffle/sort/reduce step:\");\n    System.out.println(\"  Use -numReduceTasks 0\");\n    System.out.println(\"  Map output then becomes a \u0027side-effect \" +\n        \"output\u0027 rather than a reduce input.\");\n    System.out.println(\"  This speeds up processing. This also feels \" +\n        \"more like \\\"in-place\\\" processing\");\n    System.out.println(\"  because the input filename and the map \" +\n        \"input order are preserved.\");\n    System.out.println(\"  This is equivalent to -reducer NONE\");\n    System.out.println();\n    System.out.println(\"To speed up the last maps:\");\n    System.out.println(\"  -D \" + MRJobConfig.MAP_SPECULATIVE + \"\u003dtrue\");\n    System.out.println(\"To speed up the last reduces:\");\n    System.out.println(\"  -D \" + MRJobConfig.REDUCE_SPECULATIVE + \"\u003dtrue\");\n    System.out.println(\"To name the job (appears in the JobTracker Web UI):\");\n    System.out.println(\"  -D \" + MRJobConfig.JOB_NAME + \"\u003d\u0027My Job\u0027\");\n    System.out.println(\"To change the local temp directory:\");\n    System.out.println(\"  -D dfs.data.dir\u003d/tmp/dfs\");\n    System.out.println(\"  -D stream.tmpdir\u003d/tmp/streaming\");\n    System.out.println(\"Additional local temp directories with -jt local:\");\n    System.out.println(\"  -D \" + MRConfig.LOCAL_DIR + \"\u003d/tmp/local\");\n    System.out.println(\"  -D \" + JTConfig.JT_SYSTEM_DIR + \"\u003d/tmp/system\");\n    System.out.println(\"  -D \" + MRConfig.TEMP_DIR + \"\u003d/tmp/temp\");\n    System.out.println(\"To treat tasks with non-zero exit status as SUCCEDED:\");\n    System.out.println(\"  -D stream.non.zero.exit.is.failure\u003dfalse\");\n    System.out.println(\"Use a custom hadoop streaming build along with standard\"\n        + \" hadoop install:\");\n    System.out.println(\"  $HADOOP_PREFIX/bin/hadoop jar \" +\n        \"/path/my-hadoop-streaming.jar [...]\\\\\");\n    System.out.println(\"    [...] -D stream.shipped.hadoopstreaming\u003d\" +\n        \"/path/my-hadoop-streaming.jar\");\n    System.out.println(\"For more details about jobconf parameters see:\");\n    System.out.println(\"  http://wiki.apache.org/hadoop/JobConfFile\");\n    System.out.println(\"Truncate the values of the job configuration copied\" +\n        \"to the environment at the given length:\");\n    System.out.println(\"   -D stream.jobconf.truncate.limit\u003d-1\");\n    System.out.println(\"To set an environment variable in a streaming \" +\n        \"command:\");\n    System.out.println(\"   -cmdenv EXAMPLE_DIR\u003d/home/example/dictionaries/\");\n    System.out.println();\n    System.out.println(\"Shortcut:\");\n    System.out.println(\"   setenv HSTREAMING \\\"$HADOOP_PREFIX/bin/hadoop jar \" +\n        \"hadoop-streaming.jar\\\"\");\n    System.out.println();\n    System.out.println(\"Example: $HSTREAMING -mapper \" +\n        \"\\\"/usr/local/bin/perl5 filter.pl\\\"\");\n    System.out.println(\"           -file /local/filter.pl -input \" +\n        \"\\\"/logs/0604*/*\\\" [...]\");\n    System.out.println(\"  Ships a script, invokes the non-shipped perl \" +\n        \"interpreter. Shipped files go to\");\n    System.out.println(\"  the working directory so filter.pl is found by perl. \"\n        + \"Input files are all the\");\n    System.out.println(\"  daily logs for days in month 2006-04\");\n  }",
      "path": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {}
    },
    "26447229ba2c3d43db978c1b3ce95613669182ee": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7590. Mavenize streaming and MR examples. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203941 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 5:24 PM",
      "commitName": "26447229ba2c3d43db978c1b3ce95613669182ee",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "18/11/11 1:04 AM",
      "commitNameOld": "905a127850d5e0cba85c2e075f989fa0f5cf129a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.68,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,134 +1,134 @@\n   private void printUsage(boolean detailed) {\n     System.out.println(\"Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar\"\n         + \" [options]\");\n     System.out.println(\"Options:\");\n-    System.out.println(\"  -input          \u003cpath\u003e DFS input file(s) for the Map\" \n+    System.out.println(\"  -input          \u003cpath\u003e DFS input file(s) for the Map\"\n         + \" step.\");\n-    System.out.println(\"  -output         \u003cpath\u003e DFS output directory for the\" \n+    System.out.println(\"  -output         \u003cpath\u003e DFS output directory for the\"\n         + \" Reduce step.\");\n     System.out.println(\"  -mapper         \u003ccmd|JavaClassName\u003e Optional. Command\"\n         + \" to be run as mapper.\");\n     System.out.println(\"  -combiner       \u003ccmd|JavaClassName\u003e Optional. Command\"\n         + \" to be run as combiner.\");\n     System.out.println(\"  -reducer        \u003ccmd|JavaClassName\u003e Optional. Command\"\n         + \" to be run as reducer.\");\n     System.out.println(\"  -file           \u003cfile\u003e Optional. File/dir to be \"\n-        + \"shipped in the Job jar file.\\n\" + \n+        + \"shipped in the Job jar file.\\n\" +\n         \"                  Deprecated. Use generic option \\\"-files\\\" instead.\");\n     System.out.println(\"  -inputformat    \u003cTextInputFormat(default)\"\n         + \"|SequenceFileAsTextInputFormat|JavaClassName\u003e\\n\"\n         + \"                  Optional. The input format class.\");\n     System.out.println(\"  -outputformat   \u003cTextOutputFormat(default)\"\n         + \"|JavaClassName\u003e\\n\"\n         + \"                  Optional. The output format class.\");\n     System.out.println(\"  -partitioner    \u003cJavaClassName\u003e  Optional. The\"\n         + \" partitioner class.\");\n     System.out.println(\"  -numReduceTasks \u003cnum\u003e Optional. Number of reduce \"\n         + \"tasks.\");\n     System.out.println(\"  -inputreader    \u003cspec\u003e Optional. Input recordreader\"\n         + \" spec.\");\n     System.out.println(\"  -cmdenv         \u003cn\u003e\u003d\u003cv\u003e Optional. Pass env.var to\"\n         + \" streaming commands.\");\n     System.out.println(\"  -mapdebug       \u003ccmd\u003e Optional. \"\n         + \"To run this script when a map task fails.\");\n     System.out.println(\"  -reducedebug    \u003ccmd\u003e Optional.\"\n         + \" To run this script when a reduce task fails.\");\n     System.out.println(\"  -io             \u003cidentifier\u003e Optional. Format to use\"\n         + \" for input to and output\");\n     System.out.println(\"                  from mapper/reducer commands\");\n     System.out.println(\"  -lazyOutput     Optional. Lazily create Output.\");\n     System.out.println(\"  -background     Optional. Submit the job and don\u0027t wait till it completes.\");\n     System.out.println(\"  -verbose        Optional. Print verbose output.\");\n     System.out.println(\"  -info           Optional. Print detailed usage.\");\n     System.out.println(\"  -help           Optional. Print help message.\");\n     System.out.println();\n     GenericOptionsParser.printGenericCommandUsage(System.out);\n \n     if (!detailed) {\n-      System.out.println();      \n+      System.out.println();\n       System.out.println(\"For more details about these options:\");\n       System.out.println(\"Use \" +\n           \"$HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar -info\");\n       return;\n     }\n     System.out.println();\n     System.out.println(\"Usage tips:\");\n     System.out.println(\"In -input: globbing on \u003cpath\u003e is supported and can \"\n         + \"have multiple -input\");\n     System.out.println();\n     System.out.println(\"Default Map input format: a line is a record in UTF-8 \"\n         + \"the key part ends at first\");\n     System.out.println(\"  TAB, the rest of the line is the value\");\n     System.out.println();\n     System.out.println(\"To pass a Custom input format:\");\n     System.out.println(\"  -inputformat package.MyInputFormat\");\n     System.out.println();\n     System.out.println(\"Similarly, to pass a custom output format:\");\n     System.out.println(\"  -outputformat package.MyOutputFormat\");\n     System.out.println();\n     System.out.println(\"The files with extensions .class and .jar/.zip,\" +\n         \" specified for the -file\");\n     System.out.println(\"  argument[s], end up in \\\"classes\\\" and \\\"lib\\\" \" +\n         \"directories respectively inside\");\n     System.out.println(\"  the working directory when the mapper and reducer are\"\n         + \" run. All other files\");\n     System.out.println(\"  specified for the -file argument[s]\" +\n         \" end up in the working directory when the\");\n     System.out.println(\"  mapper and reducer are run. The location of this \" +\n         \"working directory is\");\n     System.out.println(\"  unspecified.\");\n     System.out.println();\n     System.out.println(\"To set the number of reduce tasks (num. of output \" +\n         \"files) as, say 10:\");\n     System.out.println(\"  Use -numReduceTasks 10\");\n     System.out.println(\"To skip the sort/combine/shuffle/sort/reduce step:\");\n     System.out.println(\"  Use -numReduceTasks 0\");\n     System.out.println(\"  Map output then becomes a \u0027side-effect \" +\n         \"output\u0027 rather than a reduce input.\");\n     System.out.println(\"  This speeds up processing. This also feels \" +\n         \"more like \\\"in-place\\\" processing\");\n     System.out.println(\"  because the input filename and the map \" +\n         \"input order are preserved.\");\n     System.out.println(\"  This is equivalent to -reducer NONE\");\n     System.out.println();\n     System.out.println(\"To speed up the last maps:\");\n     System.out.println(\"  -D \" + MRJobConfig.MAP_SPECULATIVE + \"\u003dtrue\");\n     System.out.println(\"To speed up the last reduces:\");\n     System.out.println(\"  -D \" + MRJobConfig.REDUCE_SPECULATIVE + \"\u003dtrue\");\n     System.out.println(\"To name the job (appears in the JobTracker Web UI):\");\n     System.out.println(\"  -D \" + MRJobConfig.JOB_NAME + \"\u003d\u0027My Job\u0027\");\n     System.out.println(\"To change the local temp directory:\");\n     System.out.println(\"  -D dfs.data.dir\u003d/tmp/dfs\");\n     System.out.println(\"  -D stream.tmpdir\u003d/tmp/streaming\");\n     System.out.println(\"Additional local temp directories with -jt local:\");\n     System.out.println(\"  -D \" + MRConfig.LOCAL_DIR + \"\u003d/tmp/local\");\n     System.out.println(\"  -D \" + JTConfig.JT_SYSTEM_DIR + \"\u003d/tmp/system\");\n     System.out.println(\"  -D \" + MRConfig.TEMP_DIR + \"\u003d/tmp/temp\");\n-    System.out.println(\"To treat tasks with non-zero exit status as SUCCEDED:\");    \n+    System.out.println(\"To treat tasks with non-zero exit status as SUCCEDED:\");\n     System.out.println(\"  -D stream.non.zero.exit.is.failure\u003dfalse\");\n     System.out.println(\"Use a custom hadoop streaming build along with standard\"\n         + \" hadoop install:\");\n     System.out.println(\"  $HADOOP_PREFIX/bin/hadoop jar \" +\n         \"/path/my-hadoop-streaming.jar [...]\\\\\");\n     System.out.println(\"    [...] -D stream.shipped.hadoopstreaming\u003d\" +\n         \"/path/my-hadoop-streaming.jar\");\n     System.out.println(\"For more details about jobconf parameters see:\");\n     System.out.println(\"  http://wiki.apache.org/hadoop/JobConfFile\");\n     System.out.println(\"To set an environement variable in a streaming \" +\n         \"command:\");\n     System.out.println(\"   -cmdenv EXAMPLE_DIR\u003d/home/example/dictionaries/\");\n     System.out.println();\n     System.out.println(\"Shortcut:\");\n     System.out.println(\"   setenv HSTREAMING \\\"$HADOOP_PREFIX/bin/hadoop jar \" +\n         \"hadoop-streaming.jar\\\"\");\n     System.out.println();\n     System.out.println(\"Example: $HSTREAMING -mapper \" +\n         \"\\\"/usr/local/bin/perl5 filter.pl\\\"\");\n     System.out.println(\"           -file /local/filter.pl -input \" +\n         \"\\\"/logs/0604*/*\\\" [...]\");\n     System.out.println(\"  Ships a script, invokes the non-shipped perl \" +\n         \"interpreter. Shipped files go to\");\n     System.out.println(\"  the working directory so filter.pl is found by perl. \"\n         + \"Input files are all the\");\n     System.out.println(\"  daily logs for days in month 2006-04\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void printUsage(boolean detailed) {\n    System.out.println(\"Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar\"\n        + \" [options]\");\n    System.out.println(\"Options:\");\n    System.out.println(\"  -input          \u003cpath\u003e DFS input file(s) for the Map\"\n        + \" step.\");\n    System.out.println(\"  -output         \u003cpath\u003e DFS output directory for the\"\n        + \" Reduce step.\");\n    System.out.println(\"  -mapper         \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as mapper.\");\n    System.out.println(\"  -combiner       \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as combiner.\");\n    System.out.println(\"  -reducer        \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as reducer.\");\n    System.out.println(\"  -file           \u003cfile\u003e Optional. File/dir to be \"\n        + \"shipped in the Job jar file.\\n\" +\n        \"                  Deprecated. Use generic option \\\"-files\\\" instead.\");\n    System.out.println(\"  -inputformat    \u003cTextInputFormat(default)\"\n        + \"|SequenceFileAsTextInputFormat|JavaClassName\u003e\\n\"\n        + \"                  Optional. The input format class.\");\n    System.out.println(\"  -outputformat   \u003cTextOutputFormat(default)\"\n        + \"|JavaClassName\u003e\\n\"\n        + \"                  Optional. The output format class.\");\n    System.out.println(\"  -partitioner    \u003cJavaClassName\u003e  Optional. The\"\n        + \" partitioner class.\");\n    System.out.println(\"  -numReduceTasks \u003cnum\u003e Optional. Number of reduce \"\n        + \"tasks.\");\n    System.out.println(\"  -inputreader    \u003cspec\u003e Optional. Input recordreader\"\n        + \" spec.\");\n    System.out.println(\"  -cmdenv         \u003cn\u003e\u003d\u003cv\u003e Optional. Pass env.var to\"\n        + \" streaming commands.\");\n    System.out.println(\"  -mapdebug       \u003ccmd\u003e Optional. \"\n        + \"To run this script when a map task fails.\");\n    System.out.println(\"  -reducedebug    \u003ccmd\u003e Optional.\"\n        + \" To run this script when a reduce task fails.\");\n    System.out.println(\"  -io             \u003cidentifier\u003e Optional. Format to use\"\n        + \" for input to and output\");\n    System.out.println(\"                  from mapper/reducer commands\");\n    System.out.println(\"  -lazyOutput     Optional. Lazily create Output.\");\n    System.out.println(\"  -background     Optional. Submit the job and don\u0027t wait till it completes.\");\n    System.out.println(\"  -verbose        Optional. Print verbose output.\");\n    System.out.println(\"  -info           Optional. Print detailed usage.\");\n    System.out.println(\"  -help           Optional. Print help message.\");\n    System.out.println();\n    GenericOptionsParser.printGenericCommandUsage(System.out);\n\n    if (!detailed) {\n      System.out.println();\n      System.out.println(\"For more details about these options:\");\n      System.out.println(\"Use \" +\n          \"$HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar -info\");\n      return;\n    }\n    System.out.println();\n    System.out.println(\"Usage tips:\");\n    System.out.println(\"In -input: globbing on \u003cpath\u003e is supported and can \"\n        + \"have multiple -input\");\n    System.out.println();\n    System.out.println(\"Default Map input format: a line is a record in UTF-8 \"\n        + \"the key part ends at first\");\n    System.out.println(\"  TAB, the rest of the line is the value\");\n    System.out.println();\n    System.out.println(\"To pass a Custom input format:\");\n    System.out.println(\"  -inputformat package.MyInputFormat\");\n    System.out.println();\n    System.out.println(\"Similarly, to pass a custom output format:\");\n    System.out.println(\"  -outputformat package.MyOutputFormat\");\n    System.out.println();\n    System.out.println(\"The files with extensions .class and .jar/.zip,\" +\n        \" specified for the -file\");\n    System.out.println(\"  argument[s], end up in \\\"classes\\\" and \\\"lib\\\" \" +\n        \"directories respectively inside\");\n    System.out.println(\"  the working directory when the mapper and reducer are\"\n        + \" run. All other files\");\n    System.out.println(\"  specified for the -file argument[s]\" +\n        \" end up in the working directory when the\");\n    System.out.println(\"  mapper and reducer are run. The location of this \" +\n        \"working directory is\");\n    System.out.println(\"  unspecified.\");\n    System.out.println();\n    System.out.println(\"To set the number of reduce tasks (num. of output \" +\n        \"files) as, say 10:\");\n    System.out.println(\"  Use -numReduceTasks 10\");\n    System.out.println(\"To skip the sort/combine/shuffle/sort/reduce step:\");\n    System.out.println(\"  Use -numReduceTasks 0\");\n    System.out.println(\"  Map output then becomes a \u0027side-effect \" +\n        \"output\u0027 rather than a reduce input.\");\n    System.out.println(\"  This speeds up processing. This also feels \" +\n        \"more like \\\"in-place\\\" processing\");\n    System.out.println(\"  because the input filename and the map \" +\n        \"input order are preserved.\");\n    System.out.println(\"  This is equivalent to -reducer NONE\");\n    System.out.println();\n    System.out.println(\"To speed up the last maps:\");\n    System.out.println(\"  -D \" + MRJobConfig.MAP_SPECULATIVE + \"\u003dtrue\");\n    System.out.println(\"To speed up the last reduces:\");\n    System.out.println(\"  -D \" + MRJobConfig.REDUCE_SPECULATIVE + \"\u003dtrue\");\n    System.out.println(\"To name the job (appears in the JobTracker Web UI):\");\n    System.out.println(\"  -D \" + MRJobConfig.JOB_NAME + \"\u003d\u0027My Job\u0027\");\n    System.out.println(\"To change the local temp directory:\");\n    System.out.println(\"  -D dfs.data.dir\u003d/tmp/dfs\");\n    System.out.println(\"  -D stream.tmpdir\u003d/tmp/streaming\");\n    System.out.println(\"Additional local temp directories with -jt local:\");\n    System.out.println(\"  -D \" + MRConfig.LOCAL_DIR + \"\u003d/tmp/local\");\n    System.out.println(\"  -D \" + JTConfig.JT_SYSTEM_DIR + \"\u003d/tmp/system\");\n    System.out.println(\"  -D \" + MRConfig.TEMP_DIR + \"\u003d/tmp/temp\");\n    System.out.println(\"To treat tasks with non-zero exit status as SUCCEDED:\");\n    System.out.println(\"  -D stream.non.zero.exit.is.failure\u003dfalse\");\n    System.out.println(\"Use a custom hadoop streaming build along with standard\"\n        + \" hadoop install:\");\n    System.out.println(\"  $HADOOP_PREFIX/bin/hadoop jar \" +\n        \"/path/my-hadoop-streaming.jar [...]\\\\\");\n    System.out.println(\"    [...] -D stream.shipped.hadoopstreaming\u003d\" +\n        \"/path/my-hadoop-streaming.jar\");\n    System.out.println(\"For more details about jobconf parameters see:\");\n    System.out.println(\"  http://wiki.apache.org/hadoop/JobConfFile\");\n    System.out.println(\"To set an environement variable in a streaming \" +\n        \"command:\");\n    System.out.println(\"   -cmdenv EXAMPLE_DIR\u003d/home/example/dictionaries/\");\n    System.out.println();\n    System.out.println(\"Shortcut:\");\n    System.out.println(\"   setenv HSTREAMING \\\"$HADOOP_PREFIX/bin/hadoop jar \" +\n        \"hadoop-streaming.jar\\\"\");\n    System.out.println();\n    System.out.println(\"Example: $HSTREAMING -mapper \" +\n        \"\\\"/usr/local/bin/perl5 filter.pl\\\"\");\n    System.out.println(\"           -file /local/filter.pl -input \" +\n        \"\\\"/logs/0604*/*\\\" [...]\");\n    System.out.println(\"  Ships a script, invokes the non-shipped perl \" +\n        \"interpreter. Shipped files go to\");\n    System.out.println(\"  the working directory so filter.pl is found by perl. \"\n        + \"Input files are all the\");\n    System.out.println(\"  daily logs for days in month 2006-04\");\n  }",
      "path": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
        "newPath": "hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void printUsage(boolean detailed) {\n    System.out.println(\"Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar\"\n        + \" [options]\");\n    System.out.println(\"Options:\");\n    System.out.println(\"  -input          \u003cpath\u003e DFS input file(s) for the Map\" \n        + \" step.\");\n    System.out.println(\"  -output         \u003cpath\u003e DFS output directory for the\" \n        + \" Reduce step.\");\n    System.out.println(\"  -mapper         \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as mapper.\");\n    System.out.println(\"  -combiner       \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as combiner.\");\n    System.out.println(\"  -reducer        \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as reducer.\");\n    System.out.println(\"  -file           \u003cfile\u003e Optional. File/dir to be \"\n        + \"shipped in the Job jar file.\\n\" + \n        \"                  Deprecated. Use generic option \\\"-files\\\" instead.\");\n    System.out.println(\"  -inputformat    \u003cTextInputFormat(default)\"\n        + \"|SequenceFileAsTextInputFormat|JavaClassName\u003e\\n\"\n        + \"                  Optional. The input format class.\");\n    System.out.println(\"  -outputformat   \u003cTextOutputFormat(default)\"\n        + \"|JavaClassName\u003e\\n\"\n        + \"                  Optional. The output format class.\");\n    System.out.println(\"  -partitioner    \u003cJavaClassName\u003e  Optional. The\"\n        + \" partitioner class.\");\n    System.out.println(\"  -numReduceTasks \u003cnum\u003e Optional. Number of reduce \"\n        + \"tasks.\");\n    System.out.println(\"  -inputreader    \u003cspec\u003e Optional. Input recordreader\"\n        + \" spec.\");\n    System.out.println(\"  -cmdenv         \u003cn\u003e\u003d\u003cv\u003e Optional. Pass env.var to\"\n        + \" streaming commands.\");\n    System.out.println(\"  -mapdebug       \u003ccmd\u003e Optional. \"\n        + \"To run this script when a map task fails.\");\n    System.out.println(\"  -reducedebug    \u003ccmd\u003e Optional.\"\n        + \" To run this script when a reduce task fails.\");\n    System.out.println(\"  -io             \u003cidentifier\u003e Optional. Format to use\"\n        + \" for input to and output\");\n    System.out.println(\"                  from mapper/reducer commands\");\n    System.out.println(\"  -lazyOutput     Optional. Lazily create Output.\");\n    System.out.println(\"  -background     Optional. Submit the job and don\u0027t wait till it completes.\");\n    System.out.println(\"  -verbose        Optional. Print verbose output.\");\n    System.out.println(\"  -info           Optional. Print detailed usage.\");\n    System.out.println(\"  -help           Optional. Print help message.\");\n    System.out.println();\n    GenericOptionsParser.printGenericCommandUsage(System.out);\n\n    if (!detailed) {\n      System.out.println();      \n      System.out.println(\"For more details about these options:\");\n      System.out.println(\"Use \" +\n          \"$HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar -info\");\n      return;\n    }\n    System.out.println();\n    System.out.println(\"Usage tips:\");\n    System.out.println(\"In -input: globbing on \u003cpath\u003e is supported and can \"\n        + \"have multiple -input\");\n    System.out.println();\n    System.out.println(\"Default Map input format: a line is a record in UTF-8 \"\n        + \"the key part ends at first\");\n    System.out.println(\"  TAB, the rest of the line is the value\");\n    System.out.println();\n    System.out.println(\"To pass a Custom input format:\");\n    System.out.println(\"  -inputformat package.MyInputFormat\");\n    System.out.println();\n    System.out.println(\"Similarly, to pass a custom output format:\");\n    System.out.println(\"  -outputformat package.MyOutputFormat\");\n    System.out.println();\n    System.out.println(\"The files with extensions .class and .jar/.zip,\" +\n        \" specified for the -file\");\n    System.out.println(\"  argument[s], end up in \\\"classes\\\" and \\\"lib\\\" \" +\n        \"directories respectively inside\");\n    System.out.println(\"  the working directory when the mapper and reducer are\"\n        + \" run. All other files\");\n    System.out.println(\"  specified for the -file argument[s]\" +\n        \" end up in the working directory when the\");\n    System.out.println(\"  mapper and reducer are run. The location of this \" +\n        \"working directory is\");\n    System.out.println(\"  unspecified.\");\n    System.out.println();\n    System.out.println(\"To set the number of reduce tasks (num. of output \" +\n        \"files) as, say 10:\");\n    System.out.println(\"  Use -numReduceTasks 10\");\n    System.out.println(\"To skip the sort/combine/shuffle/sort/reduce step:\");\n    System.out.println(\"  Use -numReduceTasks 0\");\n    System.out.println(\"  Map output then becomes a \u0027side-effect \" +\n        \"output\u0027 rather than a reduce input.\");\n    System.out.println(\"  This speeds up processing. This also feels \" +\n        \"more like \\\"in-place\\\" processing\");\n    System.out.println(\"  because the input filename and the map \" +\n        \"input order are preserved.\");\n    System.out.println(\"  This is equivalent to -reducer NONE\");\n    System.out.println();\n    System.out.println(\"To speed up the last maps:\");\n    System.out.println(\"  -D \" + MRJobConfig.MAP_SPECULATIVE + \"\u003dtrue\");\n    System.out.println(\"To speed up the last reduces:\");\n    System.out.println(\"  -D \" + MRJobConfig.REDUCE_SPECULATIVE + \"\u003dtrue\");\n    System.out.println(\"To name the job (appears in the JobTracker Web UI):\");\n    System.out.println(\"  -D \" + MRJobConfig.JOB_NAME + \"\u003d\u0027My Job\u0027\");\n    System.out.println(\"To change the local temp directory:\");\n    System.out.println(\"  -D dfs.data.dir\u003d/tmp/dfs\");\n    System.out.println(\"  -D stream.tmpdir\u003d/tmp/streaming\");\n    System.out.println(\"Additional local temp directories with -jt local:\");\n    System.out.println(\"  -D \" + MRConfig.LOCAL_DIR + \"\u003d/tmp/local\");\n    System.out.println(\"  -D \" + JTConfig.JT_SYSTEM_DIR + \"\u003d/tmp/system\");\n    System.out.println(\"  -D \" + MRConfig.TEMP_DIR + \"\u003d/tmp/temp\");\n    System.out.println(\"To treat tasks with non-zero exit status as SUCCEDED:\");    \n    System.out.println(\"  -D stream.non.zero.exit.is.failure\u003dfalse\");\n    System.out.println(\"Use a custom hadoop streaming build along with standard\"\n        + \" hadoop install:\");\n    System.out.println(\"  $HADOOP_PREFIX/bin/hadoop jar \" +\n        \"/path/my-hadoop-streaming.jar [...]\\\\\");\n    System.out.println(\"    [...] -D stream.shipped.hadoopstreaming\u003d\" +\n        \"/path/my-hadoop-streaming.jar\");\n    System.out.println(\"For more details about jobconf parameters see:\");\n    System.out.println(\"  http://wiki.apache.org/hadoop/JobConfFile\");\n    System.out.println(\"To set an environement variable in a streaming \" +\n        \"command:\");\n    System.out.println(\"   -cmdenv EXAMPLE_DIR\u003d/home/example/dictionaries/\");\n    System.out.println();\n    System.out.println(\"Shortcut:\");\n    System.out.println(\"   setenv HSTREAMING \\\"$HADOOP_PREFIX/bin/hadoop jar \" +\n        \"hadoop-streaming.jar\\\"\");\n    System.out.println();\n    System.out.println(\"Example: $HSTREAMING -mapper \" +\n        \"\\\"/usr/local/bin/perl5 filter.pl\\\"\");\n    System.out.println(\"           -file /local/filter.pl -input \" +\n        \"\\\"/logs/0604*/*\\\" [...]\");\n    System.out.println(\"  Ships a script, invokes the non-shipped perl \" +\n        \"interpreter. Shipped files go to\");\n    System.out.println(\"  the working directory so filter.pl is found by perl. \"\n        + \"Input files are all the\");\n    System.out.println(\"  daily logs for days in month 2006-04\");\n  }",
      "path": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
        "newPath": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void printUsage(boolean detailed) {\n    System.out.println(\"Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar\"\n        + \" [options]\");\n    System.out.println(\"Options:\");\n    System.out.println(\"  -input          \u003cpath\u003e DFS input file(s) for the Map\" \n        + \" step.\");\n    System.out.println(\"  -output         \u003cpath\u003e DFS output directory for the\" \n        + \" Reduce step.\");\n    System.out.println(\"  -mapper         \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as mapper.\");\n    System.out.println(\"  -combiner       \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as combiner.\");\n    System.out.println(\"  -reducer        \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as reducer.\");\n    System.out.println(\"  -file           \u003cfile\u003e Optional. File/dir to be \"\n        + \"shipped in the Job jar file.\\n\" + \n        \"                  Deprecated. Use generic option \\\"-files\\\" instead.\");\n    System.out.println(\"  -inputformat    \u003cTextInputFormat(default)\"\n        + \"|SequenceFileAsTextInputFormat|JavaClassName\u003e\\n\"\n        + \"                  Optional. The input format class.\");\n    System.out.println(\"  -outputformat   \u003cTextOutputFormat(default)\"\n        + \"|JavaClassName\u003e\\n\"\n        + \"                  Optional. The output format class.\");\n    System.out.println(\"  -partitioner    \u003cJavaClassName\u003e  Optional. The\"\n        + \" partitioner class.\");\n    System.out.println(\"  -numReduceTasks \u003cnum\u003e Optional. Number of reduce \"\n        + \"tasks.\");\n    System.out.println(\"  -inputreader    \u003cspec\u003e Optional. Input recordreader\"\n        + \" spec.\");\n    System.out.println(\"  -cmdenv         \u003cn\u003e\u003d\u003cv\u003e Optional. Pass env.var to\"\n        + \" streaming commands.\");\n    System.out.println(\"  -mapdebug       \u003ccmd\u003e Optional. \"\n        + \"To run this script when a map task fails.\");\n    System.out.println(\"  -reducedebug    \u003ccmd\u003e Optional.\"\n        + \" To run this script when a reduce task fails.\");\n    System.out.println(\"  -io             \u003cidentifier\u003e Optional. Format to use\"\n        + \" for input to and output\");\n    System.out.println(\"                  from mapper/reducer commands\");\n    System.out.println(\"  -lazyOutput     Optional. Lazily create Output.\");\n    System.out.println(\"  -background     Optional. Submit the job and don\u0027t wait till it completes.\");\n    System.out.println(\"  -verbose        Optional. Print verbose output.\");\n    System.out.println(\"  -info           Optional. Print detailed usage.\");\n    System.out.println(\"  -help           Optional. Print help message.\");\n    System.out.println();\n    GenericOptionsParser.printGenericCommandUsage(System.out);\n\n    if (!detailed) {\n      System.out.println();      \n      System.out.println(\"For more details about these options:\");\n      System.out.println(\"Use \" +\n          \"$HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar -info\");\n      return;\n    }\n    System.out.println();\n    System.out.println(\"Usage tips:\");\n    System.out.println(\"In -input: globbing on \u003cpath\u003e is supported and can \"\n        + \"have multiple -input\");\n    System.out.println();\n    System.out.println(\"Default Map input format: a line is a record in UTF-8 \"\n        + \"the key part ends at first\");\n    System.out.println(\"  TAB, the rest of the line is the value\");\n    System.out.println();\n    System.out.println(\"To pass a Custom input format:\");\n    System.out.println(\"  -inputformat package.MyInputFormat\");\n    System.out.println();\n    System.out.println(\"Similarly, to pass a custom output format:\");\n    System.out.println(\"  -outputformat package.MyOutputFormat\");\n    System.out.println();\n    System.out.println(\"The files with extensions .class and .jar/.zip,\" +\n        \" specified for the -file\");\n    System.out.println(\"  argument[s], end up in \\\"classes\\\" and \\\"lib\\\" \" +\n        \"directories respectively inside\");\n    System.out.println(\"  the working directory when the mapper and reducer are\"\n        + \" run. All other files\");\n    System.out.println(\"  specified for the -file argument[s]\" +\n        \" end up in the working directory when the\");\n    System.out.println(\"  mapper and reducer are run. The location of this \" +\n        \"working directory is\");\n    System.out.println(\"  unspecified.\");\n    System.out.println();\n    System.out.println(\"To set the number of reduce tasks (num. of output \" +\n        \"files) as, say 10:\");\n    System.out.println(\"  Use -numReduceTasks 10\");\n    System.out.println(\"To skip the sort/combine/shuffle/sort/reduce step:\");\n    System.out.println(\"  Use -numReduceTasks 0\");\n    System.out.println(\"  Map output then becomes a \u0027side-effect \" +\n        \"output\u0027 rather than a reduce input.\");\n    System.out.println(\"  This speeds up processing. This also feels \" +\n        \"more like \\\"in-place\\\" processing\");\n    System.out.println(\"  because the input filename and the map \" +\n        \"input order are preserved.\");\n    System.out.println(\"  This is equivalent to -reducer NONE\");\n    System.out.println();\n    System.out.println(\"To speed up the last maps:\");\n    System.out.println(\"  -D \" + MRJobConfig.MAP_SPECULATIVE + \"\u003dtrue\");\n    System.out.println(\"To speed up the last reduces:\");\n    System.out.println(\"  -D \" + MRJobConfig.REDUCE_SPECULATIVE + \"\u003dtrue\");\n    System.out.println(\"To name the job (appears in the JobTracker Web UI):\");\n    System.out.println(\"  -D \" + MRJobConfig.JOB_NAME + \"\u003d\u0027My Job\u0027\");\n    System.out.println(\"To change the local temp directory:\");\n    System.out.println(\"  -D dfs.data.dir\u003d/tmp/dfs\");\n    System.out.println(\"  -D stream.tmpdir\u003d/tmp/streaming\");\n    System.out.println(\"Additional local temp directories with -jt local:\");\n    System.out.println(\"  -D \" + MRConfig.LOCAL_DIR + \"\u003d/tmp/local\");\n    System.out.println(\"  -D \" + JTConfig.JT_SYSTEM_DIR + \"\u003d/tmp/system\");\n    System.out.println(\"  -D \" + MRConfig.TEMP_DIR + \"\u003d/tmp/temp\");\n    System.out.println(\"To treat tasks with non-zero exit status as SUCCEDED:\");    \n    System.out.println(\"  -D stream.non.zero.exit.is.failure\u003dfalse\");\n    System.out.println(\"Use a custom hadoop streaming build along with standard\"\n        + \" hadoop install:\");\n    System.out.println(\"  $HADOOP_PREFIX/bin/hadoop jar \" +\n        \"/path/my-hadoop-streaming.jar [...]\\\\\");\n    System.out.println(\"    [...] -D stream.shipped.hadoopstreaming\u003d\" +\n        \"/path/my-hadoop-streaming.jar\");\n    System.out.println(\"For more details about jobconf parameters see:\");\n    System.out.println(\"  http://wiki.apache.org/hadoop/JobConfFile\");\n    System.out.println(\"To set an environement variable in a streaming \" +\n        \"command:\");\n    System.out.println(\"   -cmdenv EXAMPLE_DIR\u003d/home/example/dictionaries/\");\n    System.out.println();\n    System.out.println(\"Shortcut:\");\n    System.out.println(\"   setenv HSTREAMING \\\"$HADOOP_PREFIX/bin/hadoop jar \" +\n        \"hadoop-streaming.jar\\\"\");\n    System.out.println();\n    System.out.println(\"Example: $HSTREAMING -mapper \" +\n        \"\\\"/usr/local/bin/perl5 filter.pl\\\"\");\n    System.out.println(\"           -file /local/filter.pl -input \" +\n        \"\\\"/logs/0604*/*\\\" [...]\");\n    System.out.println(\"  Ships a script, invokes the non-shipped perl \" +\n        \"interpreter. Shipped files go to\");\n    System.out.println(\"  the working directory so filter.pl is found by perl. \"\n        + \"Input files are all the\");\n    System.out.println(\"  daily logs for days in month 2006-04\");\n  }",
      "path": "hadoop-mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java",
        "newPath": "hadoop-mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,134 @@\n+  private void printUsage(boolean detailed) {\n+    System.out.println(\"Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar\"\n+        + \" [options]\");\n+    System.out.println(\"Options:\");\n+    System.out.println(\"  -input          \u003cpath\u003e DFS input file(s) for the Map\" \n+        + \" step.\");\n+    System.out.println(\"  -output         \u003cpath\u003e DFS output directory for the\" \n+        + \" Reduce step.\");\n+    System.out.println(\"  -mapper         \u003ccmd|JavaClassName\u003e Optional. Command\"\n+        + \" to be run as mapper.\");\n+    System.out.println(\"  -combiner       \u003ccmd|JavaClassName\u003e Optional. Command\"\n+        + \" to be run as combiner.\");\n+    System.out.println(\"  -reducer        \u003ccmd|JavaClassName\u003e Optional. Command\"\n+        + \" to be run as reducer.\");\n+    System.out.println(\"  -file           \u003cfile\u003e Optional. File/dir to be \"\n+        + \"shipped in the Job jar file.\\n\" + \n+        \"                  Deprecated. Use generic option \\\"-files\\\" instead.\");\n+    System.out.println(\"  -inputformat    \u003cTextInputFormat(default)\"\n+        + \"|SequenceFileAsTextInputFormat|JavaClassName\u003e\\n\"\n+        + \"                  Optional. The input format class.\");\n+    System.out.println(\"  -outputformat   \u003cTextOutputFormat(default)\"\n+        + \"|JavaClassName\u003e\\n\"\n+        + \"                  Optional. The output format class.\");\n+    System.out.println(\"  -partitioner    \u003cJavaClassName\u003e  Optional. The\"\n+        + \" partitioner class.\");\n+    System.out.println(\"  -numReduceTasks \u003cnum\u003e Optional. Number of reduce \"\n+        + \"tasks.\");\n+    System.out.println(\"  -inputreader    \u003cspec\u003e Optional. Input recordreader\"\n+        + \" spec.\");\n+    System.out.println(\"  -cmdenv         \u003cn\u003e\u003d\u003cv\u003e Optional. Pass env.var to\"\n+        + \" streaming commands.\");\n+    System.out.println(\"  -mapdebug       \u003ccmd\u003e Optional. \"\n+        + \"To run this script when a map task fails.\");\n+    System.out.println(\"  -reducedebug    \u003ccmd\u003e Optional.\"\n+        + \" To run this script when a reduce task fails.\");\n+    System.out.println(\"  -io             \u003cidentifier\u003e Optional. Format to use\"\n+        + \" for input to and output\");\n+    System.out.println(\"                  from mapper/reducer commands\");\n+    System.out.println(\"  -lazyOutput     Optional. Lazily create Output.\");\n+    System.out.println(\"  -background     Optional. Submit the job and don\u0027t wait till it completes.\");\n+    System.out.println(\"  -verbose        Optional. Print verbose output.\");\n+    System.out.println(\"  -info           Optional. Print detailed usage.\");\n+    System.out.println(\"  -help           Optional. Print help message.\");\n+    System.out.println();\n+    GenericOptionsParser.printGenericCommandUsage(System.out);\n+\n+    if (!detailed) {\n+      System.out.println();      \n+      System.out.println(\"For more details about these options:\");\n+      System.out.println(\"Use \" +\n+          \"$HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar -info\");\n+      return;\n+    }\n+    System.out.println();\n+    System.out.println(\"Usage tips:\");\n+    System.out.println(\"In -input: globbing on \u003cpath\u003e is supported and can \"\n+        + \"have multiple -input\");\n+    System.out.println();\n+    System.out.println(\"Default Map input format: a line is a record in UTF-8 \"\n+        + \"the key part ends at first\");\n+    System.out.println(\"  TAB, the rest of the line is the value\");\n+    System.out.println();\n+    System.out.println(\"To pass a Custom input format:\");\n+    System.out.println(\"  -inputformat package.MyInputFormat\");\n+    System.out.println();\n+    System.out.println(\"Similarly, to pass a custom output format:\");\n+    System.out.println(\"  -outputformat package.MyOutputFormat\");\n+    System.out.println();\n+    System.out.println(\"The files with extensions .class and .jar/.zip,\" +\n+        \" specified for the -file\");\n+    System.out.println(\"  argument[s], end up in \\\"classes\\\" and \\\"lib\\\" \" +\n+        \"directories respectively inside\");\n+    System.out.println(\"  the working directory when the mapper and reducer are\"\n+        + \" run. All other files\");\n+    System.out.println(\"  specified for the -file argument[s]\" +\n+        \" end up in the working directory when the\");\n+    System.out.println(\"  mapper and reducer are run. The location of this \" +\n+        \"working directory is\");\n+    System.out.println(\"  unspecified.\");\n+    System.out.println();\n+    System.out.println(\"To set the number of reduce tasks (num. of output \" +\n+        \"files) as, say 10:\");\n+    System.out.println(\"  Use -numReduceTasks 10\");\n+    System.out.println(\"To skip the sort/combine/shuffle/sort/reduce step:\");\n+    System.out.println(\"  Use -numReduceTasks 0\");\n+    System.out.println(\"  Map output then becomes a \u0027side-effect \" +\n+        \"output\u0027 rather than a reduce input.\");\n+    System.out.println(\"  This speeds up processing. This also feels \" +\n+        \"more like \\\"in-place\\\" processing\");\n+    System.out.println(\"  because the input filename and the map \" +\n+        \"input order are preserved.\");\n+    System.out.println(\"  This is equivalent to -reducer NONE\");\n+    System.out.println();\n+    System.out.println(\"To speed up the last maps:\");\n+    System.out.println(\"  -D \" + MRJobConfig.MAP_SPECULATIVE + \"\u003dtrue\");\n+    System.out.println(\"To speed up the last reduces:\");\n+    System.out.println(\"  -D \" + MRJobConfig.REDUCE_SPECULATIVE + \"\u003dtrue\");\n+    System.out.println(\"To name the job (appears in the JobTracker Web UI):\");\n+    System.out.println(\"  -D \" + MRJobConfig.JOB_NAME + \"\u003d\u0027My Job\u0027\");\n+    System.out.println(\"To change the local temp directory:\");\n+    System.out.println(\"  -D dfs.data.dir\u003d/tmp/dfs\");\n+    System.out.println(\"  -D stream.tmpdir\u003d/tmp/streaming\");\n+    System.out.println(\"Additional local temp directories with -jt local:\");\n+    System.out.println(\"  -D \" + MRConfig.LOCAL_DIR + \"\u003d/tmp/local\");\n+    System.out.println(\"  -D \" + JTConfig.JT_SYSTEM_DIR + \"\u003d/tmp/system\");\n+    System.out.println(\"  -D \" + MRConfig.TEMP_DIR + \"\u003d/tmp/temp\");\n+    System.out.println(\"To treat tasks with non-zero exit status as SUCCEDED:\");    \n+    System.out.println(\"  -D stream.non.zero.exit.is.failure\u003dfalse\");\n+    System.out.println(\"Use a custom hadoop streaming build along with standard\"\n+        + \" hadoop install:\");\n+    System.out.println(\"  $HADOOP_PREFIX/bin/hadoop jar \" +\n+        \"/path/my-hadoop-streaming.jar [...]\\\\\");\n+    System.out.println(\"    [...] -D stream.shipped.hadoopstreaming\u003d\" +\n+        \"/path/my-hadoop-streaming.jar\");\n+    System.out.println(\"For more details about jobconf parameters see:\");\n+    System.out.println(\"  http://wiki.apache.org/hadoop/JobConfFile\");\n+    System.out.println(\"To set an environement variable in a streaming \" +\n+        \"command:\");\n+    System.out.println(\"   -cmdenv EXAMPLE_DIR\u003d/home/example/dictionaries/\");\n+    System.out.println();\n+    System.out.println(\"Shortcut:\");\n+    System.out.println(\"   setenv HSTREAMING \\\"$HADOOP_PREFIX/bin/hadoop jar \" +\n+        \"hadoop-streaming.jar\\\"\");\n+    System.out.println();\n+    System.out.println(\"Example: $HSTREAMING -mapper \" +\n+        \"\\\"/usr/local/bin/perl5 filter.pl\\\"\");\n+    System.out.println(\"           -file /local/filter.pl -input \" +\n+        \"\\\"/logs/0604*/*\\\" [...]\");\n+    System.out.println(\"  Ships a script, invokes the non-shipped perl \" +\n+        \"interpreter. Shipped files go to\");\n+    System.out.println(\"  the working directory so filter.pl is found by perl. \"\n+        + \"Input files are all the\");\n+    System.out.println(\"  daily logs for days in month 2006-04\");\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void printUsage(boolean detailed) {\n    System.out.println(\"Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar\"\n        + \" [options]\");\n    System.out.println(\"Options:\");\n    System.out.println(\"  -input          \u003cpath\u003e DFS input file(s) for the Map\" \n        + \" step.\");\n    System.out.println(\"  -output         \u003cpath\u003e DFS output directory for the\" \n        + \" Reduce step.\");\n    System.out.println(\"  -mapper         \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as mapper.\");\n    System.out.println(\"  -combiner       \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as combiner.\");\n    System.out.println(\"  -reducer        \u003ccmd|JavaClassName\u003e Optional. Command\"\n        + \" to be run as reducer.\");\n    System.out.println(\"  -file           \u003cfile\u003e Optional. File/dir to be \"\n        + \"shipped in the Job jar file.\\n\" + \n        \"                  Deprecated. Use generic option \\\"-files\\\" instead.\");\n    System.out.println(\"  -inputformat    \u003cTextInputFormat(default)\"\n        + \"|SequenceFileAsTextInputFormat|JavaClassName\u003e\\n\"\n        + \"                  Optional. The input format class.\");\n    System.out.println(\"  -outputformat   \u003cTextOutputFormat(default)\"\n        + \"|JavaClassName\u003e\\n\"\n        + \"                  Optional. The output format class.\");\n    System.out.println(\"  -partitioner    \u003cJavaClassName\u003e  Optional. The\"\n        + \" partitioner class.\");\n    System.out.println(\"  -numReduceTasks \u003cnum\u003e Optional. Number of reduce \"\n        + \"tasks.\");\n    System.out.println(\"  -inputreader    \u003cspec\u003e Optional. Input recordreader\"\n        + \" spec.\");\n    System.out.println(\"  -cmdenv         \u003cn\u003e\u003d\u003cv\u003e Optional. Pass env.var to\"\n        + \" streaming commands.\");\n    System.out.println(\"  -mapdebug       \u003ccmd\u003e Optional. \"\n        + \"To run this script when a map task fails.\");\n    System.out.println(\"  -reducedebug    \u003ccmd\u003e Optional.\"\n        + \" To run this script when a reduce task fails.\");\n    System.out.println(\"  -io             \u003cidentifier\u003e Optional. Format to use\"\n        + \" for input to and output\");\n    System.out.println(\"                  from mapper/reducer commands\");\n    System.out.println(\"  -lazyOutput     Optional. Lazily create Output.\");\n    System.out.println(\"  -background     Optional. Submit the job and don\u0027t wait till it completes.\");\n    System.out.println(\"  -verbose        Optional. Print verbose output.\");\n    System.out.println(\"  -info           Optional. Print detailed usage.\");\n    System.out.println(\"  -help           Optional. Print help message.\");\n    System.out.println();\n    GenericOptionsParser.printGenericCommandUsage(System.out);\n\n    if (!detailed) {\n      System.out.println();      \n      System.out.println(\"For more details about these options:\");\n      System.out.println(\"Use \" +\n          \"$HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar -info\");\n      return;\n    }\n    System.out.println();\n    System.out.println(\"Usage tips:\");\n    System.out.println(\"In -input: globbing on \u003cpath\u003e is supported and can \"\n        + \"have multiple -input\");\n    System.out.println();\n    System.out.println(\"Default Map input format: a line is a record in UTF-8 \"\n        + \"the key part ends at first\");\n    System.out.println(\"  TAB, the rest of the line is the value\");\n    System.out.println();\n    System.out.println(\"To pass a Custom input format:\");\n    System.out.println(\"  -inputformat package.MyInputFormat\");\n    System.out.println();\n    System.out.println(\"Similarly, to pass a custom output format:\");\n    System.out.println(\"  -outputformat package.MyOutputFormat\");\n    System.out.println();\n    System.out.println(\"The files with extensions .class and .jar/.zip,\" +\n        \" specified for the -file\");\n    System.out.println(\"  argument[s], end up in \\\"classes\\\" and \\\"lib\\\" \" +\n        \"directories respectively inside\");\n    System.out.println(\"  the working directory when the mapper and reducer are\"\n        + \" run. All other files\");\n    System.out.println(\"  specified for the -file argument[s]\" +\n        \" end up in the working directory when the\");\n    System.out.println(\"  mapper and reducer are run. The location of this \" +\n        \"working directory is\");\n    System.out.println(\"  unspecified.\");\n    System.out.println();\n    System.out.println(\"To set the number of reduce tasks (num. of output \" +\n        \"files) as, say 10:\");\n    System.out.println(\"  Use -numReduceTasks 10\");\n    System.out.println(\"To skip the sort/combine/shuffle/sort/reduce step:\");\n    System.out.println(\"  Use -numReduceTasks 0\");\n    System.out.println(\"  Map output then becomes a \u0027side-effect \" +\n        \"output\u0027 rather than a reduce input.\");\n    System.out.println(\"  This speeds up processing. This also feels \" +\n        \"more like \\\"in-place\\\" processing\");\n    System.out.println(\"  because the input filename and the map \" +\n        \"input order are preserved.\");\n    System.out.println(\"  This is equivalent to -reducer NONE\");\n    System.out.println();\n    System.out.println(\"To speed up the last maps:\");\n    System.out.println(\"  -D \" + MRJobConfig.MAP_SPECULATIVE + \"\u003dtrue\");\n    System.out.println(\"To speed up the last reduces:\");\n    System.out.println(\"  -D \" + MRJobConfig.REDUCE_SPECULATIVE + \"\u003dtrue\");\n    System.out.println(\"To name the job (appears in the JobTracker Web UI):\");\n    System.out.println(\"  -D \" + MRJobConfig.JOB_NAME + \"\u003d\u0027My Job\u0027\");\n    System.out.println(\"To change the local temp directory:\");\n    System.out.println(\"  -D dfs.data.dir\u003d/tmp/dfs\");\n    System.out.println(\"  -D stream.tmpdir\u003d/tmp/streaming\");\n    System.out.println(\"Additional local temp directories with -jt local:\");\n    System.out.println(\"  -D \" + MRConfig.LOCAL_DIR + \"\u003d/tmp/local\");\n    System.out.println(\"  -D \" + JTConfig.JT_SYSTEM_DIR + \"\u003d/tmp/system\");\n    System.out.println(\"  -D \" + MRConfig.TEMP_DIR + \"\u003d/tmp/temp\");\n    System.out.println(\"To treat tasks with non-zero exit status as SUCCEDED:\");    \n    System.out.println(\"  -D stream.non.zero.exit.is.failure\u003dfalse\");\n    System.out.println(\"Use a custom hadoop streaming build along with standard\"\n        + \" hadoop install:\");\n    System.out.println(\"  $HADOOP_PREFIX/bin/hadoop jar \" +\n        \"/path/my-hadoop-streaming.jar [...]\\\\\");\n    System.out.println(\"    [...] -D stream.shipped.hadoopstreaming\u003d\" +\n        \"/path/my-hadoop-streaming.jar\");\n    System.out.println(\"For more details about jobconf parameters see:\");\n    System.out.println(\"  http://wiki.apache.org/hadoop/JobConfFile\");\n    System.out.println(\"To set an environement variable in a streaming \" +\n        \"command:\");\n    System.out.println(\"   -cmdenv EXAMPLE_DIR\u003d/home/example/dictionaries/\");\n    System.out.println();\n    System.out.println(\"Shortcut:\");\n    System.out.println(\"   setenv HSTREAMING \\\"$HADOOP_PREFIX/bin/hadoop jar \" +\n        \"hadoop-streaming.jar\\\"\");\n    System.out.println();\n    System.out.println(\"Example: $HSTREAMING -mapper \" +\n        \"\\\"/usr/local/bin/perl5 filter.pl\\\"\");\n    System.out.println(\"           -file /local/filter.pl -input \" +\n        \"\\\"/logs/0604*/*\\\" [...]\");\n    System.out.println(\"  Ships a script, invokes the non-shipped perl \" +\n        \"interpreter. Shipped files go to\");\n    System.out.println(\"  the working directory so filter.pl is found by perl. \"\n        + \"Input files are all the\");\n    System.out.println(\"  daily logs for days in month 2006-04\");\n  }",
      "path": "mapreduce/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java"
    }
  }
}