{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "NameNode.java",
  "functionName": "initialize",
  "functionId": "initialize___conf-Configuration",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
  "functionStartLine": 720,
  "functionEndLine": 785,
  "numCommitsSeen": 231,
  "timeTaken": 13230,
  "changeHistory": [
    "b5698e0c33efd546dfea99980840c6e726795df3",
    "8fc0d04517912766a3102f3e611f7d0fabd2f815",
    "9c35be86e17021202823bfd3c2067ff3b312ce5c",
    "352f994b6484524cdcfcda021046c59905b62f31",
    "b4564103e4709caa1135f6ccc2864d90e54f2ac9",
    "b4be288c5d6801988f555a566c2eb793c88a15a4",
    "2ec438e8f7cd77cb48fd1264781e60a48e331908",
    "65f395226ba6cc3750a268a308e288b916f8df1e",
    "892ade689f9bcce76daae8f66fc00a49bee8548e",
    "a88f31ebf3433392419127816f168136de0a9e77",
    "b82567d45507c50d2f28eff4bbdf3b1a69d4bf1b",
    "6962510f729717f776929708813f99a28e582f34",
    "af6c91a80c299f87af8c42fa685448b596b7615a",
    "eac832f92da084f1fa2b281331db32e01ab05604",
    "7817245d88cb20ece994cc1c5afb3afa0da2661c",
    "2a1ecd00dadb1577da9e02822469e8194f1d3cee",
    "d02baff9a0d8cec92bde751777f3e575da2339c8",
    "1fde8bcc851b5d8f3082f1ed830270874dbf55dd",
    "32076136f7734cb5ca008f10c2088ccd81c2ca99",
    "da8e962e39bd41b73b53966826c82e741b08010b",
    "f00198b16c529bafeb8460427f12de69401941c3",
    "ab0402bc1def44e3d52eea517f4132c460bd5f87",
    "9992cae54120d2742922745c1f513c6bfbde67a9",
    "b0632df93ae5d00180b21983d960d50a45f8fb7a",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "b60772c47ddaefbeffd72bb9dce2a98117538dbc",
    "01cd616d170d5d26a539e51e731e8e73b789b360",
    "6894edebd96a669504295d45c39b73e542272401",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "b5698e0c33efd546dfea99980840c6e726795df3": "Ybodychange",
    "8fc0d04517912766a3102f3e611f7d0fabd2f815": "Ybodychange",
    "9c35be86e17021202823bfd3c2067ff3b312ce5c": "Ybodychange",
    "352f994b6484524cdcfcda021046c59905b62f31": "Ybodychange",
    "b4564103e4709caa1135f6ccc2864d90e54f2ac9": "Ybodychange",
    "b4be288c5d6801988f555a566c2eb793c88a15a4": "Ybodychange",
    "2ec438e8f7cd77cb48fd1264781e60a48e331908": "Ybodychange",
    "65f395226ba6cc3750a268a308e288b916f8df1e": "Ybodychange",
    "892ade689f9bcce76daae8f66fc00a49bee8548e": "Ybodychange",
    "a88f31ebf3433392419127816f168136de0a9e77": "Ybodychange",
    "b82567d45507c50d2f28eff4bbdf3b1a69d4bf1b": "Ybodychange",
    "6962510f729717f776929708813f99a28e582f34": "Ybodychange",
    "af6c91a80c299f87af8c42fa685448b596b7615a": "Ybodychange",
    "eac832f92da084f1fa2b281331db32e01ab05604": "Ybodychange",
    "7817245d88cb20ece994cc1c5afb3afa0da2661c": "Ybodychange",
    "2a1ecd00dadb1577da9e02822469e8194f1d3cee": "Ybodychange",
    "d02baff9a0d8cec92bde751777f3e575da2339c8": "Ybodychange",
    "1fde8bcc851b5d8f3082f1ed830270874dbf55dd": "Ybodychange",
    "32076136f7734cb5ca008f10c2088ccd81c2ca99": "Ybodychange",
    "da8e962e39bd41b73b53966826c82e741b08010b": "Ybodychange",
    "f00198b16c529bafeb8460427f12de69401941c3": "Ybodychange",
    "ab0402bc1def44e3d52eea517f4132c460bd5f87": "Ybodychange",
    "9992cae54120d2742922745c1f513c6bfbde67a9": "Ybodychange",
    "b0632df93ae5d00180b21983d960d50a45f8fb7a": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "b60772c47ddaefbeffd72bb9dce2a98117538dbc": "Ybodychange",
    "01cd616d170d5d26a539e51e731e8e73b789b360": "Ybodychange",
    "6894edebd96a669504295d45c39b73e542272401": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b5698e0c33efd546dfea99980840c6e726795df3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15176. Enable GcTimePercentage Metric in NameNode\u0027s JvmMetrics. Contributed by Jinglun.\n",
      "commitDate": "23/02/20 10:37 AM",
      "commitName": "b5698e0c33efd546dfea99980840c6e726795df3",
      "commitAuthor": "Ayush Saxena",
      "commitDateOld": "29/08/19 2:21 PM",
      "commitNameOld": "3b22fcd377eecedacceb6e37368463b48e0133c8",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 177.89,
      "commitsBetweenForRepo": 822,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,66 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     pauseMonitor \u003d new JvmPauseMonitor();\n     pauseMonitor.init(conf);\n     pauseMonitor.start();\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n \n+    if (conf.getBoolean(DFS_NAMENODE_GC_TIME_MONITOR_ENABLE,\n+        DFS_NAMENODE_GC_TIME_MONITOR_ENABLE_DEFAULT)) {\n+      long observationWindow \u003d conf.getTimeDuration(\n+          DFS_NAMENODE_GC_TIME_MONITOR_OBSERVATION_WINDOW_MS,\n+          DFS_NAMENODE_GC_TIME_MONITOR_OBSERVATION_WINDOW_MS_DEFAULT,\n+          TimeUnit.MILLISECONDS);\n+      long sleepInterval \u003d conf.getTimeDuration(\n+          DFS_NAMENODE_GC_TIME_MONITOR_SLEEP_INTERVAL_MS,\n+          DFS_NAMENODE_GC_TIME_MONITOR_SLEEP_INTERVAL_MS_DEFAULT,\n+          TimeUnit.MILLISECONDS);\n+      gcTimeMonitor \u003d new Builder().observationWindowMs(observationWindow)\n+          .sleepIntervalMs(sleepInterval).build();\n+      gcTimeMonitor.start();\n+      metrics.getJvmMetrics().setGcTimeMonitor(gcTimeMonitor);\n+    }\n+\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n \n     loadNamesystem(conf);\n     startAliasMapServerIfNecessary(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n \n     initReconfigurableBackoffKey();\n \n     if (clientNamenodeAddress \u003d\u003d null) {\n       // This is expected for MiniDFSCluster. Set it now using \n       // the RPC server\u0027s bind address.\n       clientNamenodeAddress \u003d \n           NetUtils.getHostPortString(getNameNodeAddress());\n       LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n           + \" this namenode/service.\");\n     }\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n       if (levelDBAliasMapServer !\u003d null) {\n         httpServer.setAliasMap(levelDBAliasMapServer.getAliasMap());\n       }\n     }\n \n     startCommonServices(conf);\n     startMetricsLogger(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    pauseMonitor \u003d new JvmPauseMonitor();\n    pauseMonitor.init(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    if (conf.getBoolean(DFS_NAMENODE_GC_TIME_MONITOR_ENABLE,\n        DFS_NAMENODE_GC_TIME_MONITOR_ENABLE_DEFAULT)) {\n      long observationWindow \u003d conf.getTimeDuration(\n          DFS_NAMENODE_GC_TIME_MONITOR_OBSERVATION_WINDOW_MS,\n          DFS_NAMENODE_GC_TIME_MONITOR_OBSERVATION_WINDOW_MS_DEFAULT,\n          TimeUnit.MILLISECONDS);\n      long sleepInterval \u003d conf.getTimeDuration(\n          DFS_NAMENODE_GC_TIME_MONITOR_SLEEP_INTERVAL_MS,\n          DFS_NAMENODE_GC_TIME_MONITOR_SLEEP_INTERVAL_MS_DEFAULT,\n          TimeUnit.MILLISECONDS);\n      gcTimeMonitor \u003d new Builder().observationWindowMs(observationWindow)\n          .sleepIntervalMs(sleepInterval).build();\n      gcTimeMonitor.start();\n      metrics.getJvmMetrics().setGcTimeMonitor(gcTimeMonitor);\n    }\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n\n    loadNamesystem(conf);\n    startAliasMapServerIfNecessary(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n\n    initReconfigurableBackoffKey();\n\n    if (clientNamenodeAddress \u003d\u003d null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server\u0027s bind address.\n      clientNamenodeAddress \u003d \n          NetUtils.getHostPortString(getNameNodeAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n      if (levelDBAliasMapServer !\u003d null) {\n        httpServer.setAliasMap(levelDBAliasMapServer.getAliasMap());\n      }\n    }\n\n    startCommonServices(conf);\n    startMetricsLogger(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "8fc0d04517912766a3102f3e611f7d0fabd2f815": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14001. [PROVIDED Storage] bootstrapStandby should manage the InMemoryAliasMap. Contributed by Virajith Jalaparti.\n",
      "commitDate": "07/12/18 6:30 PM",
      "commitName": "8fc0d04517912766a3102f3e611f7d0fabd2f815",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "30/10/18 10:43 PM",
      "commitNameOld": "fac9f91b2944cee641049fffcafa6b65e0cf68f2",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 37.87,
      "commitsBetweenForRepo": 257,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,50 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     pauseMonitor \u003d new JvmPauseMonitor();\n     pauseMonitor.init(conf);\n     pauseMonitor.start();\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n \n     loadNamesystem(conf);\n     startAliasMapServerIfNecessary(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n \n     initReconfigurableBackoffKey();\n \n     if (clientNamenodeAddress \u003d\u003d null) {\n       // This is expected for MiniDFSCluster. Set it now using \n       // the RPC server\u0027s bind address.\n       clientNamenodeAddress \u003d \n           NetUtils.getHostPortString(getNameNodeAddress());\n       LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n           + \" this namenode/service.\");\n     }\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n+      if (levelDBAliasMapServer !\u003d null) {\n+        httpServer.setAliasMap(levelDBAliasMapServer.getAliasMap());\n+      }\n     }\n \n     startCommonServices(conf);\n     startMetricsLogger(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    pauseMonitor \u003d new JvmPauseMonitor();\n    pauseMonitor.init(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n\n    loadNamesystem(conf);\n    startAliasMapServerIfNecessary(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n\n    initReconfigurableBackoffKey();\n\n    if (clientNamenodeAddress \u003d\u003d null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server\u0027s bind address.\n      clientNamenodeAddress \u003d \n          NetUtils.getHostPortString(getNameNodeAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n      if (levelDBAliasMapServer !\u003d null) {\n        httpServer.setAliasMap(levelDBAliasMapServer.getAliasMap());\n      }\n    }\n\n    startCommonServices(conf);\n    startMetricsLogger(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "9c35be86e17021202823bfd3c2067ff3b312ce5c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12713. [READ] Refactor FileRegion and BlockAliasMap to separate out HDFS metadata and PROVIDED storage metadata. Contributed by Ewan Higgs\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "9c35be86e17021202823bfd3c2067ff3b312ce5c",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "352f994b6484524cdcfcda021046c59905b62f31",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,47 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     pauseMonitor \u003d new JvmPauseMonitor();\n     pauseMonitor.init(conf);\n     pauseMonitor.start();\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n \n     loadNamesystem(conf);\n+    startAliasMapServerIfNecessary(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n \n     initReconfigurableBackoffKey();\n \n     if (clientNamenodeAddress \u003d\u003d null) {\n       // This is expected for MiniDFSCluster. Set it now using \n       // the RPC server\u0027s bind address.\n       clientNamenodeAddress \u003d \n           NetUtils.getHostPortString(getNameNodeAddress());\n       LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n           + \" this namenode/service.\");\n     }\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     }\n \n     startCommonServices(conf);\n     startMetricsLogger(conf);\n-    startAliasMapServerIfNecessary(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    pauseMonitor \u003d new JvmPauseMonitor();\n    pauseMonitor.init(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n\n    loadNamesystem(conf);\n    startAliasMapServerIfNecessary(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n\n    initReconfigurableBackoffKey();\n\n    if (clientNamenodeAddress \u003d\u003d null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server\u0027s bind address.\n      clientNamenodeAddress \u003d \n          NetUtils.getHostPortString(getNameNodeAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n\n    startCommonServices(conf);\n    startMetricsLogger(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "352f994b6484524cdcfcda021046c59905b62f31": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12665. [AliasMap] Create a version of the AliasMap that runs in memory in the Namenode (leveldb). Contributed by Ewan Higgs.\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "352f994b6484524cdcfcda021046c59905b62f31",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "31/10/17 10:46 AM",
      "commitNameOld": "5f681fa8216fb43dff8a3d21bf21e91d6c6f6d9c",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 45.34,
      "commitsBetweenForRepo": 385,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,47 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     pauseMonitor \u003d new JvmPauseMonitor();\n     pauseMonitor.init(conf);\n     pauseMonitor.start();\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n \n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n \n     initReconfigurableBackoffKey();\n \n     if (clientNamenodeAddress \u003d\u003d null) {\n       // This is expected for MiniDFSCluster. Set it now using \n       // the RPC server\u0027s bind address.\n       clientNamenodeAddress \u003d \n           NetUtils.getHostPortString(getNameNodeAddress());\n       LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n           + \" this namenode/service.\");\n     }\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     }\n \n     startCommonServices(conf);\n     startMetricsLogger(conf);\n+    startAliasMapServerIfNecessary(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    pauseMonitor \u003d new JvmPauseMonitor();\n    pauseMonitor.init(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n\n    initReconfigurableBackoffKey();\n\n    if (clientNamenodeAddress \u003d\u003d null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server\u0027s bind address.\n      clientNamenodeAddress \u003d \n          NetUtils.getHostPortString(getNameNodeAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n\n    startCommonServices(conf);\n    startMetricsLogger(conf);\n    startAliasMapServerIfNecessary(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "b4564103e4709caa1135f6ccc2864d90e54f2ac9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10752. Several log refactoring/improvement suggestion in HDFS. Contributed by Hanisha Koneru.\n",
      "commitDate": "19/10/16 5:20 PM",
      "commitName": "b4564103e4709caa1135f6ccc2864d90e54f2ac9",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "12/10/16 1:11 PM",
      "commitNameOld": "85cd06f6636f295ad1f3bf2a90063f4714c9cca7",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 7.17,
      "commitsBetweenForRepo": 51,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,46 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     pauseMonitor \u003d new JvmPauseMonitor();\n     pauseMonitor.init(conf);\n     pauseMonitor.start();\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n \n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n \n     initReconfigurableBackoffKey();\n \n     if (clientNamenodeAddress \u003d\u003d null) {\n       // This is expected for MiniDFSCluster. Set it now using \n       // the RPC server\u0027s bind address.\n       clientNamenodeAddress \u003d \n-          NetUtils.getHostPortString(rpcServer.getRpcAddress());\n+          NetUtils.getHostPortString(getNameNodeAddress());\n       LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n           + \" this namenode/service.\");\n     }\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     }\n \n     startCommonServices(conf);\n     startMetricsLogger(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    pauseMonitor \u003d new JvmPauseMonitor();\n    pauseMonitor.init(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n\n    initReconfigurableBackoffKey();\n\n    if (clientNamenodeAddress \u003d\u003d null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server\u0027s bind address.\n      clientNamenodeAddress \u003d \n          NetUtils.getHostPortString(getNameNodeAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n\n    startCommonServices(conf);\n    startMetricsLogger(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "b4be288c5d6801988f555a566c2eb793c88a15a4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10207. Support enable Hadoop IPC backoff without namenode restart. Contributed by Xiaobing Zhou.\n",
      "commitDate": "21/04/16 10:18 AM",
      "commitName": "b4be288c5d6801988f555a566c2eb793c88a15a4",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "13/04/16 4:51 PM",
      "commitNameOld": "5566177c9af913baf380811dbbb1fa7e70235491",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 7.73,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,46 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     pauseMonitor \u003d new JvmPauseMonitor();\n     pauseMonitor.init(conf);\n     pauseMonitor.start();\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n \n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n+\n+    initReconfigurableBackoffKey();\n+\n     if (clientNamenodeAddress \u003d\u003d null) {\n       // This is expected for MiniDFSCluster. Set it now using \n       // the RPC server\u0027s bind address.\n       clientNamenodeAddress \u003d \n           NetUtils.getHostPortString(rpcServer.getRpcAddress());\n       LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n           + \" this namenode/service.\");\n     }\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     }\n \n     startCommonServices(conf);\n     startMetricsLogger(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    pauseMonitor \u003d new JvmPauseMonitor();\n    pauseMonitor.init(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n\n    initReconfigurableBackoffKey();\n\n    if (clientNamenodeAddress \u003d\u003d null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server\u0027s bind address.\n      clientNamenodeAddress \u003d \n          NetUtils.getHostPortString(rpcServer.getRpcAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n\n    startCommonServices(conf);\n    startMetricsLogger(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "2ec438e8f7cd77cb48fd1264781e60a48e331908": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9655. NN should start JVM pause monitor before loading fsimage. (John Zhuge via Lei (Eddy) Xu)\n",
      "commitDate": "20/01/16 2:26 PM",
      "commitName": "2ec438e8f7cd77cb48fd1264781e60a48e331908",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "29/12/15 10:56 AM",
      "commitNameOld": "99cf2ecee9c19231dea3620c053b2d8d71812fd6",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 22.15,
      "commitsBetweenForRepo": 128,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,43 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n+    pauseMonitor \u003d new JvmPauseMonitor();\n+    pauseMonitor.init(conf);\n+    pauseMonitor.start();\n+    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n+\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n \n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n     if (clientNamenodeAddress \u003d\u003d null) {\n       // This is expected for MiniDFSCluster. Set it now using \n       // the RPC server\u0027s bind address.\n       clientNamenodeAddress \u003d \n           NetUtils.getHostPortString(rpcServer.getRpcAddress());\n       LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n           + \" this namenode/service.\");\n     }\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     }\n-    \n-    pauseMonitor \u003d new JvmPauseMonitor();\n-    pauseMonitor.init(conf);\n-    pauseMonitor.start();\n-    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n-    \n+\n     startCommonServices(conf);\n     startMetricsLogger(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    pauseMonitor \u003d new JvmPauseMonitor();\n    pauseMonitor.init(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    if (clientNamenodeAddress \u003d\u003d null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server\u0027s bind address.\n      clientNamenodeAddress \u003d \n          NetUtils.getHostPortString(rpcServer.getRpcAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n\n    startCommonServices(conf);\n    startMetricsLogger(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "65f395226ba6cc3750a268a308e288b916f8df1e": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12321. Make JvmPauseMonitor an AbstractService. (Sunil G via Stevel) [includes HDFS-8947 MAPREDUCE-6462 and YARN-4072]\n",
      "commitDate": "06/12/15 9:43 AM",
      "commitName": "65f395226ba6cc3750a268a308e288b916f8df1e",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "01/12/15 4:09 PM",
      "commitNameOld": "a49cc74b4c72195dee1dfb6f9548e5e411dff553",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 4.73,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,43 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n \n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n     if (clientNamenodeAddress \u003d\u003d null) {\n       // This is expected for MiniDFSCluster. Set it now using \n       // the RPC server\u0027s bind address.\n       clientNamenodeAddress \u003d \n           NetUtils.getHostPortString(rpcServer.getRpcAddress());\n       LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n           + \" this namenode/service.\");\n     }\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     }\n     \n-    pauseMonitor \u003d new JvmPauseMonitor(conf);\n+    pauseMonitor \u003d new JvmPauseMonitor();\n+    pauseMonitor.init(conf);\n     pauseMonitor.start();\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n     \n     startCommonServices(conf);\n     startMetricsLogger(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    if (clientNamenodeAddress \u003d\u003d null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server\u0027s bind address.\n      clientNamenodeAddress \u003d \n          NetUtils.getHostPortString(rpcServer.getRpcAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    \n    pauseMonitor \u003d new JvmPauseMonitor();\n    pauseMonitor.init(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n    \n    startCommonServices(conf);\n    startMetricsLogger(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "892ade689f9bcce76daae8f66fc00a49bee8548e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
      "commitDate": "28/09/15 7:42 AM",
      "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "18/09/15 6:23 PM",
      "commitNameOld": "66b46d0885f7049d0485e0d08b5e7af9762f0a59",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 9.55,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,42 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n \n-    this.spanReceiverHost \u003d\n-      SpanReceiverHost.get(conf, DFSConfigKeys.DFS_SERVER_HTRACE_PREFIX);\n-\n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n     if (clientNamenodeAddress \u003d\u003d null) {\n       // This is expected for MiniDFSCluster. Set it now using \n       // the RPC server\u0027s bind address.\n       clientNamenodeAddress \u003d \n           NetUtils.getHostPortString(rpcServer.getRpcAddress());\n       LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n           + \" this namenode/service.\");\n     }\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     }\n     \n     pauseMonitor \u003d new JvmPauseMonitor(conf);\n     pauseMonitor.start();\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n     \n     startCommonServices(conf);\n     startMetricsLogger(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    if (clientNamenodeAddress \u003d\u003d null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server\u0027s bind address.\n      clientNamenodeAddress \u003d \n          NetUtils.getHostPortString(rpcServer.getRpcAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    \n    pauseMonitor \u003d new JvmPauseMonitor(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n    \n    startCommonServices(conf);\n    startMetricsLogger(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "a88f31ebf3433392419127816f168136de0a9e77": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8880. NameNode metrics logging. (Arpit Agarwal)\n",
      "commitDate": "17/08/15 4:55 PM",
      "commitName": "a88f31ebf3433392419127816f168136de0a9e77",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "08/06/15 9:57 PM",
      "commitNameOld": "927577c87ca19e8b5b75722f78e2def6d9386576",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 69.79,
      "commitsBetweenForRepo": 409,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,45 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n \n     this.spanReceiverHost \u003d\n       SpanReceiverHost.get(conf, DFSConfigKeys.DFS_SERVER_HTRACE_PREFIX);\n \n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n     if (clientNamenodeAddress \u003d\u003d null) {\n       // This is expected for MiniDFSCluster. Set it now using \n       // the RPC server\u0027s bind address.\n       clientNamenodeAddress \u003d \n           NetUtils.getHostPortString(rpcServer.getRpcAddress());\n       LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n           + \" this namenode/service.\");\n     }\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     }\n     \n     pauseMonitor \u003d new JvmPauseMonitor(conf);\n     pauseMonitor.start();\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n     \n     startCommonServices(conf);\n+    startMetricsLogger(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n\n    this.spanReceiverHost \u003d\n      SpanReceiverHost.get(conf, DFSConfigKeys.DFS_SERVER_HTRACE_PREFIX);\n\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    if (clientNamenodeAddress \u003d\u003d null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server\u0027s bind address.\n      clientNamenodeAddress \u003d \n          NetUtils.getHostPortString(rpcServer.getRpcAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    \n    pauseMonitor \u003d new JvmPauseMonitor(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n    \n    startCommonServices(conf);\n    startMetricsLogger(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "b82567d45507c50d2f28eff4bbdf3b1a69d4bf1b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8213. DFSClient should use hdfs.client.htrace HTrace configuration prefix rather than hadoop.htrace (cmccabe)\n",
      "commitDate": "01/05/15 11:19 AM",
      "commitName": "b82567d45507c50d2f28eff4bbdf3b1a69d4bf1b",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "21/04/15 9:59 PM",
      "commitNameOld": "6f8003dc7bc9e8be7b0512c514d370c303faf003",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 9.56,
      "commitsBetweenForRepo": 92,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,44 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n \n-    this.spanReceiverHost \u003d SpanReceiverHost.getInstance(conf);\n+    this.spanReceiverHost \u003d\n+      SpanReceiverHost.get(conf, DFSConfigKeys.DFS_SERVER_HTRACE_PREFIX);\n \n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n     if (clientNamenodeAddress \u003d\u003d null) {\n       // This is expected for MiniDFSCluster. Set it now using \n       // the RPC server\u0027s bind address.\n       clientNamenodeAddress \u003d \n           NetUtils.getHostPortString(rpcServer.getRpcAddress());\n       LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n           + \" this namenode/service.\");\n     }\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     }\n     \n     pauseMonitor \u003d new JvmPauseMonitor(conf);\n     pauseMonitor.start();\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n     \n     startCommonServices(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n\n    this.spanReceiverHost \u003d\n      SpanReceiverHost.get(conf, DFSConfigKeys.DFS_SERVER_HTRACE_PREFIX);\n\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    if (clientNamenodeAddress \u003d\u003d null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server\u0027s bind address.\n      clientNamenodeAddress \u003d \n          NetUtils.getHostPortString(rpcServer.getRpcAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    \n    pauseMonitor \u003d new JvmPauseMonitor(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n    \n    startCommonServices(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "6962510f729717f776929708813f99a28e582f34": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6879. Adding tracing to Hadoop RPC.  Contributed by Masatake Iwasaki.\n",
      "commitDate": "27/08/14 2:12 PM",
      "commitName": "6962510f729717f776929708813f99a28e582f34",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "18/07/14 10:14 AM",
      "commitNameOld": "5f9e52f7459d3dc4ac3a5febd1dc6e00829d30ed",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 40.17,
      "commitsBetweenForRepo": 327,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,43 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n+\n+    this.spanReceiverHost \u003d SpanReceiverHost.getInstance(conf);\n+\n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n     if (clientNamenodeAddress \u003d\u003d null) {\n       // This is expected for MiniDFSCluster. Set it now using \n       // the RPC server\u0027s bind address.\n       clientNamenodeAddress \u003d \n           NetUtils.getHostPortString(rpcServer.getRpcAddress());\n       LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n           + \" this namenode/service.\");\n     }\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     }\n     \n     pauseMonitor \u003d new JvmPauseMonitor(conf);\n     pauseMonitor.start();\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n     \n     startCommonServices(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n\n    this.spanReceiverHost \u003d SpanReceiverHost.getInstance(conf);\n\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    if (clientNamenodeAddress \u003d\u003d null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server\u0027s bind address.\n      clientNamenodeAddress \u003d \n          NetUtils.getHostPortString(rpcServer.getRpcAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    \n    pauseMonitor \u003d new JvmPauseMonitor(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n    \n    startCommonServices(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "af6c91a80c299f87af8c42fa685448b596b7615a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6403. Add metrics for log warnings reported by JVM pauses. Contributed by Yongjun Zhang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1604074 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/06/14 7:38 PM",
      "commitName": "af6c91a80c299f87af8c42fa685448b596b7615a",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "14/05/14 1:45 PM",
      "commitNameOld": "88e76f9c45e57c9fad52864cfd46794fefb02615",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 36.25,
      "commitsBetweenForRepo": 213,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,40 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n     if (clientNamenodeAddress \u003d\u003d null) {\n       // This is expected for MiniDFSCluster. Set it now using \n       // the RPC server\u0027s bind address.\n       clientNamenodeAddress \u003d \n           NetUtils.getHostPortString(rpcServer.getRpcAddress());\n       LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n           + \" this namenode/service.\");\n     }\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     }\n     \n     pauseMonitor \u003d new JvmPauseMonitor(conf);\n     pauseMonitor.start();\n-\n+    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n+    \n     startCommonServices(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    if (clientNamenodeAddress \u003d\u003d null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server\u0027s bind address.\n      clientNamenodeAddress \u003d \n          NetUtils.getHostPortString(rpcServer.getRpcAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    \n    pauseMonitor \u003d new JvmPauseMonitor(conf);\n    pauseMonitor.start();\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n    \n    startCommonServices(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "eac832f92da084f1fa2b281331db32e01ab05604": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6329. WebHdfs does not work if HA is enabled on NN but logical URI is not configured. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1593470 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/05/14 6:46 PM",
      "commitName": "eac832f92da084f1fa2b281331db32e01ab05604",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "24/04/14 10:40 AM",
      "commitNameOld": "cf4bc7fdd49974324b177c99b820587cc5854adb",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 14.34,
      "commitsBetweenForRepo": 65,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,39 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n-    final String nsId \u003d getNameServiceId(conf);\n-    tokenServiceName \u003d HAUtil.isHAEnabled(conf, nsId) ? nsId : NetUtils\n-            .getHostPortString(rpcServer.getRpcAddress());\n+    if (clientNamenodeAddress \u003d\u003d null) {\n+      // This is expected for MiniDFSCluster. Set it now using \n+      // the RPC server\u0027s bind address.\n+      clientNamenodeAddress \u003d \n+          NetUtils.getHostPortString(rpcServer.getRpcAddress());\n+      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n+          + \" this namenode/service.\");\n+    }\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     }\n     \n     pauseMonitor \u003d new JvmPauseMonitor(conf);\n     pauseMonitor.start();\n \n     startCommonServices(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    if (clientNamenodeAddress \u003d\u003d null) {\n      // This is expected for MiniDFSCluster. Set it now using \n      // the RPC server\u0027s bind address.\n      clientNamenodeAddress \u003d \n          NetUtils.getHostPortString(rpcServer.getRpcAddress());\n      LOG.info(\"Clients are to use \" + clientNamenodeAddress + \" to access\"\n          + \" this namenode/service.\");\n    }\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    \n    pauseMonitor \u003d new JvmPauseMonitor(conf);\n    pauseMonitor.start();\n\n    startCommonServices(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "7817245d88cb20ece994cc1c5afb3afa0da2661c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6100. DataNodeWebHdfsMethods does not failover in HA mode. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1579301 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/03/14 10:29 AM",
      "commitName": "7817245d88cb20ece994cc1c5afb3afa0da2661c",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "28/02/14 5:14 PM",
      "commitNameOld": "0a7db7f1791200669a4e0bce1debbf7428b30e96",
      "commitAuthorOld": "",
      "daysBetweenCommits": 18.68,
      "commitsBetweenForRepo": 149,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,34 @@\n   protected void initialize(Configuration conf) throws IOException {\n     if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n       String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n       if (intervals !\u003d null) {\n         conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n           intervals);\n       }\n     }\n \n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n+    final String nsId \u003d getNameServiceId(conf);\n+    tokenServiceName \u003d HAUtil.isHAEnabled(conf, nsId) ? nsId : NetUtils\n+            .getHostPortString(rpcServer.getRpcAddress());\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     }\n     \n     pauseMonitor \u003d new JvmPauseMonitor(conf);\n     pauseMonitor.start();\n \n     startCommonServices(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    final String nsId \u003d getNameServiceId(conf);\n    tokenServiceName \u003d HAUtil.isHAEnabled(conf, nsId) ? nsId : NetUtils\n            .getHostPortString(rpcServer.getRpcAddress());\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    \n    pauseMonitor \u003d new JvmPauseMonitor(conf);\n    pauseMonitor.start();\n\n    startCommonServices(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "2a1ecd00dadb1577da9e02822469e8194f1d3cee": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5220. Expose group resolution time as metric (jxiang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1555976 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/01/14 10:59 AM",
      "commitName": "2a1ecd00dadb1577da9e02822469e8194f1d3cee",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "04/12/13 1:40 PM",
      "commitNameOld": "d02baff9a0d8cec92bde751777f3e575da2339c8",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 32.89,
      "commitsBetweenForRepo": 152,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,31 @@\n   protected void initialize(Configuration conf) throws IOException {\n+    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n+      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n+      if (intervals !\u003d null) {\n+        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n+          intervals);\n+      }\n+    }\n+\n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n     }\n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     }\n     \n     pauseMonitor \u003d new JvmPauseMonitor(conf);\n     pauseMonitor.start();\n \n     startCommonServices(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) \u003d\u003d null) {\n      String intervals \u003d conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);\n      if (intervals !\u003d null) {\n        conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,\n          intervals);\n      }\n    }\n\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    \n    pauseMonitor \u003d new JvmPauseMonitor(conf);\n    pauseMonitor.start();\n\n    startCommonServices(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "d02baff9a0d8cec92bde751777f3e575da2339c8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5536. Implement HTTP policy for Namenode and DataNode. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1547925 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/12/13 1:40 PM",
      "commitName": "d02baff9a0d8cec92bde751777f3e575da2339c8",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "27/11/13 10:20 AM",
      "commitNameOld": "2214871d916fdcae62aa51afbb5fd571f2808745",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 7.14,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,23 @@\n   protected void initialize(Configuration conf) throws IOException {\n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     StartupProgressMetrics.register(startupProgress);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n-      validateConfigurationSettingsOrAbort(conf);\n     }\n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n-    } else {\n-      validateConfigurationSettingsOrAbort(conf);\n     }\n     \n     pauseMonitor \u003d new JvmPauseMonitor(conf);\n     pauseMonitor.start();\n \n     startCommonServices(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n    }\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    }\n    \n    pauseMonitor \u003d new JvmPauseMonitor(conf);\n    pauseMonitor.start();\n\n    startCommonServices(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "1fde8bcc851b5d8f3082f1ed830270874dbf55dd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5336. DataNode should not output StartupProgress metrics. Contributed by Akira Ajisaka.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1533183 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/10/13 11:06 AM",
      "commitName": "1fde8bcc851b5d8f3082f1ed830270874dbf55dd",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "03/09/13 6:49 AM",
      "commitNameOld": "c28c96461210294d4f98b39cf24e18dde1428e2c",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 44.18,
      "commitsBetweenForRepo": 288,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,26 @@\n   protected void initialize(Configuration conf) throws IOException {\n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n+    StartupProgressMetrics.register(startupProgress);\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n       validateConfigurationSettingsOrAbort(conf);\n     }\n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     } else {\n       validateConfigurationSettingsOrAbort(conf);\n     }\n     \n     pauseMonitor \u003d new JvmPauseMonitor(conf);\n     pauseMonitor.start();\n \n     startCommonServices(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    StartupProgressMetrics.register(startupProgress);\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n      validateConfigurationSettingsOrAbort(conf);\n    }\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    } else {\n      validateConfigurationSettingsOrAbort(conf);\n    }\n    \n    pauseMonitor \u003d new JvmPauseMonitor(conf);\n    pauseMonitor.start();\n\n    startCommonServices(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "32076136f7734cb5ca008f10c2088ccd81c2ca99": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-9618.  thread which detects GC pauses (Todd Lipcon via Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1503806 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/07/13 10:48 AM",
      "commitName": "32076136f7734cb5ca008f10c2088ccd81c2ca99",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "15/07/13 11:33 PM",
      "commitNameOld": "afaec5f52d1abf889cfb091d29119fbd191d9a99",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.47,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,25 @@\n   protected void initialize(Configuration conf) throws IOException {\n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n \n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       startHttpServer(conf);\n       validateConfigurationSettingsOrAbort(conf);\n     }\n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n     if (NamenodeRole.NAMENODE \u003d\u003d role) {\n       httpServer.setNameNodeAddress(getNameNodeAddress());\n       httpServer.setFSImage(getFSImage());\n     } else {\n       validateConfigurationSettingsOrAbort(conf);\n     }\n+    \n+    pauseMonitor \u003d new JvmPauseMonitor(conf);\n+    pauseMonitor.start();\n \n     startCommonServices(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n      validateConfigurationSettingsOrAbort(conf);\n    }\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    } else {\n      validateConfigurationSettingsOrAbort(conf);\n    }\n    \n    pauseMonitor \u003d new JvmPauseMonitor(conf);\n    pauseMonitor.start();\n\n    startCommonServices(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "da8e962e39bd41b73b53966826c82e741b08010b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4372. Track NameNode startup progress. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1502120 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/07/13 10:35 PM",
      "commitName": "da8e962e39bd41b73b53966826c82e741b08010b",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "22/04/13 6:18 PM",
      "commitNameOld": "fd24c6e83357d4d3c937e112328a1eb378327eb0",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 79.18,
      "commitsBetweenForRepo": 494,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,22 @@\n   protected void initialize(Configuration conf) throws IOException {\n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n+\n+    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n+      startHttpServer(conf);\n+      validateConfigurationSettingsOrAbort(conf);\n+    }\n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n-    \n-    try {\n-      validateConfigurationSettings(conf);\n-    } catch (IOException e) {\n-      LOG.fatal(e.toString());\n-      throw e;\n+    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n+      httpServer.setNameNodeAddress(getNameNodeAddress());\n+      httpServer.setFSImage(getFSImage());\n+    } else {\n+      validateConfigurationSettingsOrAbort(conf);\n     }\n \n     startCommonServices(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      startHttpServer(conf);\n      validateConfigurationSettingsOrAbort(conf);\n    }\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    if (NamenodeRole.NAMENODE \u003d\u003d role) {\n      httpServer.setNameNodeAddress(getNameNodeAddress());\n      httpServer.setFSImage(getFSImage());\n    } else {\n      validateConfigurationSettingsOrAbort(conf);\n    }\n\n    startCommonServices(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "f00198b16c529bafeb8460427f12de69401941c3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2301. Start/stop appropriate namenode services when transition to active and standby states. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1182080 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/10/11 1:44 PM",
      "commitName": "f00198b16c529bafeb8460427f12de69401941c3",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "06/10/11 4:26 PM",
      "commitNameOld": "8b4f497af85b49519da2e05e8269db6c4e9d621f",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 4.89,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   protected void initialize(Configuration conf) throws IOException {\n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n     \n     try {\n       validateConfigurationSettings(conf);\n     } catch (IOException e) {\n       LOG.fatal(e.toString());\n       throw e;\n     }\n \n-    activate(conf);\n+    startCommonServices(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    \n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n\n    startCommonServices(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "ab0402bc1def44e3d52eea517f4132c460bd5f87": {
      "type": "Ybodychange",
      "commitMessage": "Merging trunk to HDFS-1623 branch\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1177130 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/09/11 5:42 PM",
      "commitName": "ab0402bc1def44e3d52eea517f4132c460bd5f87",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "28/09/11 5:33 PM",
      "commitNameOld": "9992cae54120d2742922745c1f513c6bfbde67a9",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,18 @@\n   protected void initialize(Configuration conf) throws IOException {\n-    initializeGenericKeys(conf);\n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n     \n     try {\n       validateConfigurationSettings(conf);\n     } catch (IOException e) {\n       LOG.fatal(e.toString());\n       throw e;\n     }\n \n     activate(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    \n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n\n    activate(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "9992cae54120d2742922745c1f513c6bfbde67a9": {
      "type": "Ybodychange",
      "commitMessage": "Reverting the previous trunk merge since it added other unintended changes in addition\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1177127 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/09/11 5:33 PM",
      "commitName": "9992cae54120d2742922745c1f513c6bfbde67a9",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "28/09/11 5:09 PM",
      "commitNameOld": "122113922fd398b1a76c1664b58a61661e936e30",
      "commitAuthorOld": "",
      "daysBetweenCommits": 0.02,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,19 @@\n   protected void initialize(Configuration conf) throws IOException {\n+    initializeGenericKeys(conf);\n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n \n     NameNode.initMetrics(conf, this.getRole());\n     loadNamesystem(conf);\n \n     rpcServer \u003d createRpcServer(conf);\n     \n     try {\n       validateConfigurationSettings(conf);\n     } catch (IOException e) {\n       LOG.fatal(e.toString());\n       throw e;\n     }\n \n-    startCommonServices(conf);\n+    activate(conf);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    initializeGenericKeys(conf);\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    \n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n\n    activate(conf);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "b0632df93ae5d00180b21983d960d50a45f8fb7a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2197. Refactor RPC call implementations out of NameNode class. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165463 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/09/11 5:41 PM",
      "commitName": "b0632df93ae5d00180b21983d960d50a45f8fb7a",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "05/09/11 5:34 PM",
      "commitNameOld": "d1438b501dae9efc7aa84de35a57e1b8e6f5645e",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,22 @@\n   protected void initialize(Configuration conf) throws IOException {\n-    InetSocketAddress socAddr \u003d getRpcServerAddress(conf);\n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n-    int handlerCount \u003d \n-      conf.getInt(DFS_DATANODE_HANDLER_COUNT_KEY, \n-                  DFS_DATANODE_HANDLER_COUNT_DEFAULT);\n \n     NameNode.initMetrics(conf, this.getRole());\n     loadNamesystem(conf);\n-    // create rpc server\n-    InetSocketAddress dnSocketAddr \u003d getServiceRpcServerAddress(conf);\n-    if (dnSocketAddr !\u003d null) {\n-      int serviceHandlerCount \u003d\n-        conf.getInt(DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,\n-                    DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);\n-      this.serviceRpcServer \u003d RPC.getServer(NamenodeProtocols.class, this,\n-          dnSocketAddr.getHostName(), dnSocketAddr.getPort(), serviceHandlerCount,\n-          false, conf, namesystem.getDelegationTokenSecretManager());\n-      this.serviceRPCAddress \u003d this.serviceRpcServer.getListenerAddress();\n-      setRpcServiceServerAddress(conf);\n-    }\n-    this.server \u003d RPC.getServer(NamenodeProtocols.class, this,\n-                                socAddr.getHostName(), socAddr.getPort(),\n-                                handlerCount, false, conf, \n-                                namesystem.getDelegationTokenSecretManager());\n \n-    // set service-level authorization security policy\n-    if (serviceAuthEnabled \u003d\n-          conf.getBoolean(\n-            CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION, false)) {\n-      this.server.refreshServiceAcl(conf, new HDFSPolicyProvider());\n-      if (this.serviceRpcServer !\u003d null) {\n-        this.serviceRpcServer.refreshServiceAcl(conf, new HDFSPolicyProvider());\n-      }\n-    }\n-\n-    // The rpc-server port can be ephemeral... ensure we have the correct info\n-    this.rpcAddress \u003d this.server.getListenerAddress(); \n-    setRpcServerAddress(conf);\n+    rpcServer \u003d createRpcServer(conf);\n     \n     try {\n       validateConfigurationSettings(conf);\n     } catch (IOException e) {\n       LOG.fatal(e.toString());\n       throw e;\n     }\n \n     activate(conf);\n-    LOG.info(getRole() + \" up at: \" + rpcAddress);\n-    if (serviceRPCAddress !\u003d null) {\n-      LOG.info(getRole() + \" service server is up at: \" + serviceRPCAddress); \n+    LOG.info(getRole() + \" up at: \" + rpcServer.getRpcAddress());\n+    if (rpcServer.getServiceRpcAddress() !\u003d null) {\n+      LOG.info(getRole() + \" service server is up at: \" + rpcServer.getServiceRpcAddress()); \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n\n    NameNode.initMetrics(conf, this.getRole());\n    loadNamesystem(conf);\n\n    rpcServer \u003d createRpcServer(conf);\n    \n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n\n    activate(conf);\n    LOG.info(getRole() + \" up at: \" + rpcServer.getRpcAddress());\n    if (rpcServer.getServiceRpcAddress() !\u003d null) {\n      LOG.info(getRole() + \" service server is up at: \" + rpcServer.getServiceRpcAddress()); \n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    InetSocketAddress socAddr \u003d getRpcServerAddress(conf);\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n    int handlerCount \u003d \n      conf.getInt(DFS_DATANODE_HANDLER_COUNT_KEY, \n                  DFS_DATANODE_HANDLER_COUNT_DEFAULT);\n\n    NameNode.initMetrics(conf, this.getRole());\n    loadNamesystem(conf);\n    // create rpc server\n    InetSocketAddress dnSocketAddr \u003d getServiceRpcServerAddress(conf);\n    if (dnSocketAddr !\u003d null) {\n      int serviceHandlerCount \u003d\n        conf.getInt(DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,\n                    DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);\n      this.serviceRpcServer \u003d RPC.getServer(NamenodeProtocols.class, this,\n          dnSocketAddr.getHostName(), dnSocketAddr.getPort(), serviceHandlerCount,\n          false, conf, namesystem.getDelegationTokenSecretManager());\n      this.serviceRPCAddress \u003d this.serviceRpcServer.getListenerAddress();\n      setRpcServiceServerAddress(conf);\n    }\n    this.server \u003d RPC.getServer(NamenodeProtocols.class, this,\n                                socAddr.getHostName(), socAddr.getPort(),\n                                handlerCount, false, conf, \n                                namesystem.getDelegationTokenSecretManager());\n\n    // set service-level authorization security policy\n    if (serviceAuthEnabled \u003d\n          conf.getBoolean(\n            CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION, false)) {\n      this.server.refreshServiceAcl(conf, new HDFSPolicyProvider());\n      if (this.serviceRpcServer !\u003d null) {\n        this.serviceRpcServer.refreshServiceAcl(conf, new HDFSPolicyProvider());\n      }\n    }\n\n    // The rpc-server port can be ephemeral... ensure we have the correct info\n    this.rpcAddress \u003d this.server.getListenerAddress(); \n    setRpcServerAddress(conf);\n    \n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n\n    activate(conf);\n    LOG.info(getRole() + \" up at: \" + rpcAddress);\n    if (serviceRPCAddress !\u003d null) {\n      LOG.info(getRole() + \" service server is up at: \" + serviceRPCAddress); \n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    InetSocketAddress socAddr \u003d getRpcServerAddress(conf);\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n    int handlerCount \u003d \n      conf.getInt(DFS_DATANODE_HANDLER_COUNT_KEY, \n                  DFS_DATANODE_HANDLER_COUNT_DEFAULT);\n\n    NameNode.initMetrics(conf, this.getRole());\n    loadNamesystem(conf);\n    // create rpc server\n    InetSocketAddress dnSocketAddr \u003d getServiceRpcServerAddress(conf);\n    if (dnSocketAddr !\u003d null) {\n      int serviceHandlerCount \u003d\n        conf.getInt(DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,\n                    DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);\n      this.serviceRpcServer \u003d RPC.getServer(NamenodeProtocols.class, this,\n          dnSocketAddr.getHostName(), dnSocketAddr.getPort(), serviceHandlerCount,\n          false, conf, namesystem.getDelegationTokenSecretManager());\n      this.serviceRPCAddress \u003d this.serviceRpcServer.getListenerAddress();\n      setRpcServiceServerAddress(conf);\n    }\n    this.server \u003d RPC.getServer(NamenodeProtocols.class, this,\n                                socAddr.getHostName(), socAddr.getPort(),\n                                handlerCount, false, conf, \n                                namesystem.getDelegationTokenSecretManager());\n\n    // set service-level authorization security policy\n    if (serviceAuthEnabled \u003d\n          conf.getBoolean(\n            CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION, false)) {\n      this.server.refreshServiceAcl(conf, new HDFSPolicyProvider());\n      if (this.serviceRpcServer !\u003d null) {\n        this.serviceRpcServer.refreshServiceAcl(conf, new HDFSPolicyProvider());\n      }\n    }\n\n    // The rpc-server port can be ephemeral... ensure we have the correct info\n    this.rpcAddress \u003d this.server.getListenerAddress(); \n    setRpcServerAddress(conf);\n    \n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n\n    activate(conf);\n    LOG.info(getRole() + \" up at: \" + rpcAddress);\n    if (serviceRPCAddress !\u003d null) {\n      LOG.info(getRole() + \" service server is up at: \" + serviceRPCAddress); \n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java"
      }
    },
    "b60772c47ddaefbeffd72bb9dce2a98117538dbc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2198. Remove hardcoded configuration keys. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151501 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/07/11 8:29 AM",
      "commitName": "b60772c47ddaefbeffd72bb9dce2a98117538dbc",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "25/07/11 5:04 PM",
      "commitNameOld": "01cd616d170d5d26a539e51e731e8e73b789b360",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 1.64,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,54 @@\n   protected void initialize(Configuration conf) throws IOException {\n     InetSocketAddress socAddr \u003d getRpcServerAddress(conf);\n     UserGroupInformation.setConfiguration(conf);\n     loginAsNameNodeUser(conf);\n     int handlerCount \u003d \n-      conf.getInt(DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_KEY, \n-                  DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_DEFAULT);\n+      conf.getInt(DFS_DATANODE_HANDLER_COUNT_KEY, \n+                  DFS_DATANODE_HANDLER_COUNT_DEFAULT);\n \n     NameNode.initMetrics(conf, this.getRole());\n     loadNamesystem(conf);\n     // create rpc server\n     InetSocketAddress dnSocketAddr \u003d getServiceRpcServerAddress(conf);\n     if (dnSocketAddr !\u003d null) {\n       int serviceHandlerCount \u003d\n-        conf.getInt(DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,\n-                    DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);\n+        conf.getInt(DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,\n+                    DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);\n       this.serviceRpcServer \u003d RPC.getServer(NamenodeProtocols.class, this,\n           dnSocketAddr.getHostName(), dnSocketAddr.getPort(), serviceHandlerCount,\n           false, conf, namesystem.getDelegationTokenSecretManager());\n       this.serviceRPCAddress \u003d this.serviceRpcServer.getListenerAddress();\n       setRpcServiceServerAddress(conf);\n     }\n     this.server \u003d RPC.getServer(NamenodeProtocols.class, this,\n                                 socAddr.getHostName(), socAddr.getPort(),\n                                 handlerCount, false, conf, \n                                 namesystem.getDelegationTokenSecretManager());\n \n     // set service-level authorization security policy\n     if (serviceAuthEnabled \u003d\n           conf.getBoolean(\n             CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION, false)) {\n       this.server.refreshServiceAcl(conf, new HDFSPolicyProvider());\n       if (this.serviceRpcServer !\u003d null) {\n         this.serviceRpcServer.refreshServiceAcl(conf, new HDFSPolicyProvider());\n       }\n     }\n \n     // The rpc-server port can be ephemeral... ensure we have the correct info\n     this.rpcAddress \u003d this.server.getListenerAddress(); \n     setRpcServerAddress(conf);\n     \n     try {\n       validateConfigurationSettings(conf);\n     } catch (IOException e) {\n       LOG.fatal(e.toString());\n       throw e;\n     }\n \n     activate(conf);\n     LOG.info(getRole() + \" up at: \" + rpcAddress);\n     if (serviceRPCAddress !\u003d null) {\n       LOG.info(getRole() + \" service server is up at: \" + serviceRPCAddress); \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    InetSocketAddress socAddr \u003d getRpcServerAddress(conf);\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n    int handlerCount \u003d \n      conf.getInt(DFS_DATANODE_HANDLER_COUNT_KEY, \n                  DFS_DATANODE_HANDLER_COUNT_DEFAULT);\n\n    NameNode.initMetrics(conf, this.getRole());\n    loadNamesystem(conf);\n    // create rpc server\n    InetSocketAddress dnSocketAddr \u003d getServiceRpcServerAddress(conf);\n    if (dnSocketAddr !\u003d null) {\n      int serviceHandlerCount \u003d\n        conf.getInt(DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,\n                    DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);\n      this.serviceRpcServer \u003d RPC.getServer(NamenodeProtocols.class, this,\n          dnSocketAddr.getHostName(), dnSocketAddr.getPort(), serviceHandlerCount,\n          false, conf, namesystem.getDelegationTokenSecretManager());\n      this.serviceRPCAddress \u003d this.serviceRpcServer.getListenerAddress();\n      setRpcServiceServerAddress(conf);\n    }\n    this.server \u003d RPC.getServer(NamenodeProtocols.class, this,\n                                socAddr.getHostName(), socAddr.getPort(),\n                                handlerCount, false, conf, \n                                namesystem.getDelegationTokenSecretManager());\n\n    // set service-level authorization security policy\n    if (serviceAuthEnabled \u003d\n          conf.getBoolean(\n            CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION, false)) {\n      this.server.refreshServiceAcl(conf, new HDFSPolicyProvider());\n      if (this.serviceRpcServer !\u003d null) {\n        this.serviceRpcServer.refreshServiceAcl(conf, new HDFSPolicyProvider());\n      }\n    }\n\n    // The rpc-server port can be ephemeral... ensure we have the correct info\n    this.rpcAddress \u003d this.server.getListenerAddress(); \n    setRpcServerAddress(conf);\n    \n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n\n    activate(conf);\n    LOG.info(getRole() + \" up at: \" + rpcAddress);\n    if (serviceRPCAddress !\u003d null) {\n      LOG.info(getRole() + \" service server is up at: \" + serviceRPCAddress); \n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "01cd616d170d5d26a539e51e731e8e73b789b360": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2180. Refactor NameNode HTTP server into new class. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1150960 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/07/11 5:04 PM",
      "commitName": "01cd616d170d5d26a539e51e731e8e73b789b360",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "18/07/11 6:32 PM",
      "commitNameOld": "6c0cb4d15179103d4f83fe5b2f8d8f6a05f3a789",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 6.94,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,54 @@\n   protected void initialize(Configuration conf) throws IOException {\n     InetSocketAddress socAddr \u003d getRpcServerAddress(conf);\n     UserGroupInformation.setConfiguration(conf);\n-    SecurityUtil.login(conf, DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY,\n-        DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n+    loginAsNameNodeUser(conf);\n     int handlerCount \u003d \n       conf.getInt(DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_KEY, \n                   DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_DEFAULT);\n \n     NameNode.initMetrics(conf, this.getRole());\n     loadNamesystem(conf);\n     // create rpc server\n     InetSocketAddress dnSocketAddr \u003d getServiceRpcServerAddress(conf);\n     if (dnSocketAddr !\u003d null) {\n       int serviceHandlerCount \u003d\n         conf.getInt(DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,\n                     DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);\n       this.serviceRpcServer \u003d RPC.getServer(NamenodeProtocols.class, this,\n           dnSocketAddr.getHostName(), dnSocketAddr.getPort(), serviceHandlerCount,\n           false, conf, namesystem.getDelegationTokenSecretManager());\n       this.serviceRPCAddress \u003d this.serviceRpcServer.getListenerAddress();\n       setRpcServiceServerAddress(conf);\n     }\n     this.server \u003d RPC.getServer(NamenodeProtocols.class, this,\n                                 socAddr.getHostName(), socAddr.getPort(),\n                                 handlerCount, false, conf, \n                                 namesystem.getDelegationTokenSecretManager());\n \n     // set service-level authorization security policy\n     if (serviceAuthEnabled \u003d\n           conf.getBoolean(\n             CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION, false)) {\n       this.server.refreshServiceAcl(conf, new HDFSPolicyProvider());\n       if (this.serviceRpcServer !\u003d null) {\n         this.serviceRpcServer.refreshServiceAcl(conf, new HDFSPolicyProvider());\n       }\n     }\n \n     // The rpc-server port can be ephemeral... ensure we have the correct info\n     this.rpcAddress \u003d this.server.getListenerAddress(); \n     setRpcServerAddress(conf);\n     \n     try {\n       validateConfigurationSettings(conf);\n     } catch (IOException e) {\n       LOG.fatal(e.toString());\n       throw e;\n     }\n \n     activate(conf);\n     LOG.info(getRole() + \" up at: \" + rpcAddress);\n     if (serviceRPCAddress !\u003d null) {\n       LOG.info(getRole() + \" service server is up at: \" + serviceRPCAddress); \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    InetSocketAddress socAddr \u003d getRpcServerAddress(conf);\n    UserGroupInformation.setConfiguration(conf);\n    loginAsNameNodeUser(conf);\n    int handlerCount \u003d \n      conf.getInt(DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_KEY, \n                  DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_DEFAULT);\n\n    NameNode.initMetrics(conf, this.getRole());\n    loadNamesystem(conf);\n    // create rpc server\n    InetSocketAddress dnSocketAddr \u003d getServiceRpcServerAddress(conf);\n    if (dnSocketAddr !\u003d null) {\n      int serviceHandlerCount \u003d\n        conf.getInt(DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,\n                    DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);\n      this.serviceRpcServer \u003d RPC.getServer(NamenodeProtocols.class, this,\n          dnSocketAddr.getHostName(), dnSocketAddr.getPort(), serviceHandlerCount,\n          false, conf, namesystem.getDelegationTokenSecretManager());\n      this.serviceRPCAddress \u003d this.serviceRpcServer.getListenerAddress();\n      setRpcServiceServerAddress(conf);\n    }\n    this.server \u003d RPC.getServer(NamenodeProtocols.class, this,\n                                socAddr.getHostName(), socAddr.getPort(),\n                                handlerCount, false, conf, \n                                namesystem.getDelegationTokenSecretManager());\n\n    // set service-level authorization security policy\n    if (serviceAuthEnabled \u003d\n          conf.getBoolean(\n            CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION, false)) {\n      this.server.refreshServiceAcl(conf, new HDFSPolicyProvider());\n      if (this.serviceRpcServer !\u003d null) {\n        this.serviceRpcServer.refreshServiceAcl(conf, new HDFSPolicyProvider());\n      }\n    }\n\n    // The rpc-server port can be ephemeral... ensure we have the correct info\n    this.rpcAddress \u003d this.server.getListenerAddress(); \n    setRpcServerAddress(conf);\n    \n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n\n    activate(conf);\n    LOG.info(getRole() + \" up at: \" + rpcAddress);\n    if (serviceRPCAddress !\u003d null) {\n      LOG.info(getRole() + \" service server is up at: \" + serviceRPCAddress); \n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "6894edebd96a669504295d45c39b73e542272401": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1321. If service port and main port are the same, there is no clear log message explaining the issue. (Jim Plush via atm)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139473 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/06/11 5:39 PM",
      "commitName": "6894edebd96a669504295d45c39b73e542272401",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "15/06/11 3:41 PM",
      "commitNameOld": "53268e215592c086983d3a35a24dd176a682c53f",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 9.08,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,55 @@\n   protected void initialize(Configuration conf) throws IOException {\n     InetSocketAddress socAddr \u003d getRpcServerAddress(conf);\n     UserGroupInformation.setConfiguration(conf);\n     SecurityUtil.login(conf, DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY,\n         DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n     int handlerCount \u003d \n       conf.getInt(DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_KEY, \n                   DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_DEFAULT);\n \n     NameNode.initMetrics(conf, this.getRole());\n     loadNamesystem(conf);\n     // create rpc server\n     InetSocketAddress dnSocketAddr \u003d getServiceRpcServerAddress(conf);\n     if (dnSocketAddr !\u003d null) {\n       int serviceHandlerCount \u003d\n         conf.getInt(DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,\n                     DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);\n       this.serviceRpcServer \u003d RPC.getServer(NamenodeProtocols.class, this,\n           dnSocketAddr.getHostName(), dnSocketAddr.getPort(), serviceHandlerCount,\n           false, conf, namesystem.getDelegationTokenSecretManager());\n       this.serviceRPCAddress \u003d this.serviceRpcServer.getListenerAddress();\n       setRpcServiceServerAddress(conf);\n     }\n     this.server \u003d RPC.getServer(NamenodeProtocols.class, this,\n                                 socAddr.getHostName(), socAddr.getPort(),\n                                 handlerCount, false, conf, \n                                 namesystem.getDelegationTokenSecretManager());\n \n     // set service-level authorization security policy\n     if (serviceAuthEnabled \u003d\n           conf.getBoolean(\n             CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION, false)) {\n       this.server.refreshServiceAcl(conf, new HDFSPolicyProvider());\n       if (this.serviceRpcServer !\u003d null) {\n         this.serviceRpcServer.refreshServiceAcl(conf, new HDFSPolicyProvider());\n       }\n     }\n \n     // The rpc-server port can be ephemeral... ensure we have the correct info\n     this.rpcAddress \u003d this.server.getListenerAddress(); \n     setRpcServerAddress(conf);\n+    \n+    try {\n+      validateConfigurationSettings(conf);\n+    } catch (IOException e) {\n+      LOG.fatal(e.toString());\n+      throw e;\n+    }\n \n     activate(conf);\n     LOG.info(getRole() + \" up at: \" + rpcAddress);\n     if (serviceRPCAddress !\u003d null) {\n       LOG.info(getRole() + \" service server is up at: \" + serviceRPCAddress); \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    InetSocketAddress socAddr \u003d getRpcServerAddress(conf);\n    UserGroupInformation.setConfiguration(conf);\n    SecurityUtil.login(conf, DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY,\n        DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n    int handlerCount \u003d \n      conf.getInt(DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_KEY, \n                  DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_DEFAULT);\n\n    NameNode.initMetrics(conf, this.getRole());\n    loadNamesystem(conf);\n    // create rpc server\n    InetSocketAddress dnSocketAddr \u003d getServiceRpcServerAddress(conf);\n    if (dnSocketAddr !\u003d null) {\n      int serviceHandlerCount \u003d\n        conf.getInt(DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,\n                    DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);\n      this.serviceRpcServer \u003d RPC.getServer(NamenodeProtocols.class, this,\n          dnSocketAddr.getHostName(), dnSocketAddr.getPort(), serviceHandlerCount,\n          false, conf, namesystem.getDelegationTokenSecretManager());\n      this.serviceRPCAddress \u003d this.serviceRpcServer.getListenerAddress();\n      setRpcServiceServerAddress(conf);\n    }\n    this.server \u003d RPC.getServer(NamenodeProtocols.class, this,\n                                socAddr.getHostName(), socAddr.getPort(),\n                                handlerCount, false, conf, \n                                namesystem.getDelegationTokenSecretManager());\n\n    // set service-level authorization security policy\n    if (serviceAuthEnabled \u003d\n          conf.getBoolean(\n            CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION, false)) {\n      this.server.refreshServiceAcl(conf, new HDFSPolicyProvider());\n      if (this.serviceRpcServer !\u003d null) {\n        this.serviceRpcServer.refreshServiceAcl(conf, new HDFSPolicyProvider());\n      }\n    }\n\n    // The rpc-server port can be ephemeral... ensure we have the correct info\n    this.rpcAddress \u003d this.server.getListenerAddress(); \n    setRpcServerAddress(conf);\n    \n    try {\n      validateConfigurationSettings(conf);\n    } catch (IOException e) {\n      LOG.fatal(e.toString());\n      throw e;\n    }\n\n    activate(conf);\n    LOG.info(getRole() + \" up at: \" + rpcAddress);\n    if (serviceRPCAddress !\u003d null) {\n      LOG.info(getRole() + \" service server is up at: \" + serviceRPCAddress); \n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,48 @@\n+  protected void initialize(Configuration conf) throws IOException {\n+    InetSocketAddress socAddr \u003d getRpcServerAddress(conf);\n+    UserGroupInformation.setConfiguration(conf);\n+    SecurityUtil.login(conf, DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY,\n+        DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n+    int handlerCount \u003d \n+      conf.getInt(DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_KEY, \n+                  DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_DEFAULT);\n+\n+    NameNode.initMetrics(conf, this.getRole());\n+    loadNamesystem(conf);\n+    // create rpc server\n+    InetSocketAddress dnSocketAddr \u003d getServiceRpcServerAddress(conf);\n+    if (dnSocketAddr !\u003d null) {\n+      int serviceHandlerCount \u003d\n+        conf.getInt(DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,\n+                    DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);\n+      this.serviceRpcServer \u003d RPC.getServer(NamenodeProtocols.class, this,\n+          dnSocketAddr.getHostName(), dnSocketAddr.getPort(), serviceHandlerCount,\n+          false, conf, namesystem.getDelegationTokenSecretManager());\n+      this.serviceRPCAddress \u003d this.serviceRpcServer.getListenerAddress();\n+      setRpcServiceServerAddress(conf);\n+    }\n+    this.server \u003d RPC.getServer(NamenodeProtocols.class, this,\n+                                socAddr.getHostName(), socAddr.getPort(),\n+                                handlerCount, false, conf, \n+                                namesystem.getDelegationTokenSecretManager());\n+\n+    // set service-level authorization security policy\n+    if (serviceAuthEnabled \u003d\n+          conf.getBoolean(\n+            CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION, false)) {\n+      this.server.refreshServiceAcl(conf, new HDFSPolicyProvider());\n+      if (this.serviceRpcServer !\u003d null) {\n+        this.serviceRpcServer.refreshServiceAcl(conf, new HDFSPolicyProvider());\n+      }\n+    }\n+\n+    // The rpc-server port can be ephemeral... ensure we have the correct info\n+    this.rpcAddress \u003d this.server.getListenerAddress(); \n+    setRpcServerAddress(conf);\n+\n+    activate(conf);\n+    LOG.info(getRole() + \" up at: \" + rpcAddress);\n+    if (serviceRPCAddress !\u003d null) {\n+      LOG.info(getRole() + \" service server is up at: \" + serviceRPCAddress); \n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected void initialize(Configuration conf) throws IOException {\n    InetSocketAddress socAddr \u003d getRpcServerAddress(conf);\n    UserGroupInformation.setConfiguration(conf);\n    SecurityUtil.login(conf, DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY,\n        DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY, socAddr.getHostName());\n    int handlerCount \u003d \n      conf.getInt(DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_KEY, \n                  DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_DEFAULT);\n\n    NameNode.initMetrics(conf, this.getRole());\n    loadNamesystem(conf);\n    // create rpc server\n    InetSocketAddress dnSocketAddr \u003d getServiceRpcServerAddress(conf);\n    if (dnSocketAddr !\u003d null) {\n      int serviceHandlerCount \u003d\n        conf.getInt(DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_KEY,\n                    DFSConfigKeys.DFS_NAMENODE_SERVICE_HANDLER_COUNT_DEFAULT);\n      this.serviceRpcServer \u003d RPC.getServer(NamenodeProtocols.class, this,\n          dnSocketAddr.getHostName(), dnSocketAddr.getPort(), serviceHandlerCount,\n          false, conf, namesystem.getDelegationTokenSecretManager());\n      this.serviceRPCAddress \u003d this.serviceRpcServer.getListenerAddress();\n      setRpcServiceServerAddress(conf);\n    }\n    this.server \u003d RPC.getServer(NamenodeProtocols.class, this,\n                                socAddr.getHostName(), socAddr.getPort(),\n                                handlerCount, false, conf, \n                                namesystem.getDelegationTokenSecretManager());\n\n    // set service-level authorization security policy\n    if (serviceAuthEnabled \u003d\n          conf.getBoolean(\n            CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION, false)) {\n      this.server.refreshServiceAcl(conf, new HDFSPolicyProvider());\n      if (this.serviceRpcServer !\u003d null) {\n        this.serviceRpcServer.refreshServiceAcl(conf, new HDFSPolicyProvider());\n      }\n    }\n\n    // The rpc-server port can be ephemeral... ensure we have the correct info\n    this.rpcAddress \u003d this.server.getListenerAddress(); \n    setRpcServerAddress(conf);\n\n    activate(conf);\n    LOG.info(getRole() + \" up at: \" + rpcAddress);\n    if (serviceRPCAddress !\u003d null) {\n      LOG.info(getRole() + \" service server is up at: \" + serviceRPCAddress); \n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java"
    }
  }
}