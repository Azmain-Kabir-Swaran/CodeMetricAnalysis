{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSOutputStream.java",
  "functionName": "close",
  "functionId": "close",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
  "functionStartLine": 847,
  "functionEndLine": 861,
  "numCommitsSeen": 146,
  "timeTaken": 9987,
  "changeHistory": [
    "4fcea8a0c8019d6d9a5e6f315c83659938b93a40",
    "2aa5e2c40364cf1e90e6af7851801f5eda759002",
    "e8bd1ba74b2fc7a6a1b71d068ef01a0fb0bbe294",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "892ade689f9bcce76daae8f66fc00a49bee8548e",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
    "952640fa4cbdc23fe8781e5627c2e8eab565c535",
    "394ba94c5d2801fbc5d95c7872eeeede28eed1eb",
    "36ccf097a95eae0761de7b657752e4808a86c094",
    "463aec11718e47d4aabb86a7a539cb973460aae6",
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc",
    "f2f5cdb5554d294a29ebf465101c5607fd56e244",
    "735046ebecd9e803398be56fbf79dbde5226b4c1",
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
    "0a6806ce8c946b26eceac7d16b467c54c453df84",
    "83cf475050dba27e72b4e399491638c670621175",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "4fcea8a0c8019d6d9a5e6f315c83659938b93a40": "Ybodychange",
    "2aa5e2c40364cf1e90e6af7851801f5eda759002": "Ybodychange",
    "e8bd1ba74b2fc7a6a1b71d068ef01a0fb0bbe294": "Ymultichange(Ymodifierchange,Ybodychange)",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "892ade689f9bcce76daae8f66fc00a49bee8548e": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e": "Ybodychange",
    "952640fa4cbdc23fe8781e5627c2e8eab565c535": "Ybodychange",
    "394ba94c5d2801fbc5d95c7872eeeede28eed1eb": "Ybodychange",
    "36ccf097a95eae0761de7b657752e4808a86c094": "Ybodychange",
    "463aec11718e47d4aabb86a7a539cb973460aae6": "Ybodychange",
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc": "Ybodychange",
    "f2f5cdb5554d294a29ebf465101c5607fd56e244": "Ybodychange",
    "735046ebecd9e803398be56fbf79dbde5226b4c1": "Ybodychange",
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": "Ybodychange",
    "0a6806ce8c946b26eceac7d16b467c54c453df84": "Ybodychange",
    "83cf475050dba27e72b4e399491638c670621175": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "4fcea8a0c8019d6d9a5e6f315c83659938b93a40": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8870. Lease is leaked on write failure. Contributed by Kuhu Shukla.\n",
      "commitDate": "15/11/16 12:47 PM",
      "commitName": "4fcea8a0c8019d6d9a5e6f315c83659938b93a40",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "17/08/16 3:52 PM",
      "commitNameOld": "2aa5e2c40364cf1e90e6af7851801f5eda759002",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 89.91,
      "commitsBetweenForRepo": 673,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,15 @@\n   public void close() throws IOException {\n     final MultipleIOException.Builder b \u003d new MultipleIOException.Builder();\n     synchronized (this) {\n       try (TraceScope ignored \u003d dfsClient.newPathTraceScope(\n           \"DFSOutputStream#close\", src)) {\n         closeImpl();\n       } catch (IOException e) {\n         b.add(e);\n       }\n     }\n-\n-    dfsClient.endFileLease(fileId);\n     final IOException ioe \u003d b.build();\n     if (ioe !\u003d null) {\n       throw ioe;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    final MultipleIOException.Builder b \u003d new MultipleIOException.Builder();\n    synchronized (this) {\n      try (TraceScope ignored \u003d dfsClient.newPathTraceScope(\n          \"DFSOutputStream#close\", src)) {\n        closeImpl();\n      } catch (IOException e) {\n        b.add(e);\n      }\n    }\n    final IOException ioe \u003d b.build();\n    if (ioe !\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "2aa5e2c40364cf1e90e6af7851801f5eda759002": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10549. Correctly revoke file leases when closing files. Contributed by Yiqun Lin.\n",
      "commitDate": "17/08/16 3:52 PM",
      "commitName": "2aa5e2c40364cf1e90e6af7851801f5eda759002",
      "commitAuthor": "Xiao Chen",
      "commitDateOld": "12/08/16 10:52 PM",
      "commitNameOld": "b5af9be72c72734d668f817c99d889031922a951",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 4.71,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,17 @@\n   public void close() throws IOException {\n+    final MultipleIOException.Builder b \u003d new MultipleIOException.Builder();\n     synchronized (this) {\n       try (TraceScope ignored \u003d dfsClient.newPathTraceScope(\n           \"DFSOutputStream#close\", src)) {\n         closeImpl();\n+      } catch (IOException e) {\n+        b.add(e);\n       }\n     }\n+\n     dfsClient.endFileLease(fileId);\n+    final IOException ioe \u003d b.build();\n+    if (ioe !\u003d null) {\n+      throw ioe;\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void close() throws IOException {\n    final MultipleIOException.Builder b \u003d new MultipleIOException.Builder();\n    synchronized (this) {\n      try (TraceScope ignored \u003d dfsClient.newPathTraceScope(\n          \"DFSOutputStream#close\", src)) {\n        closeImpl();\n      } catch (IOException e) {\n        b.add(e);\n      }\n    }\n\n    dfsClient.endFileLease(fileId);\n    final IOException ioe \u003d b.build();\n    if (ioe !\u003d null) {\n      throw ioe;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "e8bd1ba74b2fc7a6a1b71d068ef01a0fb0bbe294": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-9294. DFSClient deadlock when close file and failed to renew lease.  Contributed by Brahma Reddy Battula\n",
      "commitDate": "02/12/15 5:39 PM",
      "commitName": "e8bd1ba74b2fc7a6a1b71d068ef01a0fb0bbe294",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-9294. DFSClient deadlock when close file and failed to renew lease.  Contributed by Brahma Reddy Battula\n",
          "commitDate": "02/12/15 5:39 PM",
          "commitName": "e8bd1ba74b2fc7a6a1b71d068ef01a0fb0bbe294",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "06/10/15 10:22 PM",
          "commitNameOld": "8b7339312cb06b7e021f8f9ea6e3a20ebf009af3",
          "commitAuthorOld": "Uma Mahesh",
          "daysBetweenCommits": 56.85,
          "commitsBetweenForRepo": 444,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,6 +1,9 @@\n-  public synchronized void close() throws IOException {\n-    try (TraceScope ignored \u003d\n-             dfsClient.newPathTraceScope(\"DFSOutputStream#close\", src)) {\n-      closeImpl();\n+  public void close() throws IOException {\n+    synchronized (this) {\n+      try (TraceScope ignored \u003d dfsClient.newPathTraceScope(\n+          \"DFSOutputStream#close\", src)) {\n+        closeImpl();\n+      }\n     }\n+    dfsClient.endFileLease(fileId);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void close() throws IOException {\n    synchronized (this) {\n      try (TraceScope ignored \u003d dfsClient.newPathTraceScope(\n          \"DFSOutputStream#close\", src)) {\n        closeImpl();\n      }\n    }\n    dfsClient.endFileLease(fileId);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9294. DFSClient deadlock when close file and failed to renew lease.  Contributed by Brahma Reddy Battula\n",
          "commitDate": "02/12/15 5:39 PM",
          "commitName": "e8bd1ba74b2fc7a6a1b71d068ef01a0fb0bbe294",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "06/10/15 10:22 PM",
          "commitNameOld": "8b7339312cb06b7e021f8f9ea6e3a20ebf009af3",
          "commitAuthorOld": "Uma Mahesh",
          "daysBetweenCommits": 56.85,
          "commitsBetweenForRepo": 444,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,6 +1,9 @@\n-  public synchronized void close() throws IOException {\n-    try (TraceScope ignored \u003d\n-             dfsClient.newPathTraceScope(\"DFSOutputStream#close\", src)) {\n-      closeImpl();\n+  public void close() throws IOException {\n+    synchronized (this) {\n+      try (TraceScope ignored \u003d dfsClient.newPathTraceScope(\n+          \"DFSOutputStream#close\", src)) {\n+        closeImpl();\n+      }\n     }\n+    dfsClient.endFileLease(fileId);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void close() throws IOException {\n    synchronized (this) {\n      try (TraceScope ignored \u003d dfsClient.newPathTraceScope(\n          \"DFSOutputStream#close\", src)) {\n        closeImpl();\n      }\n    }\n    dfsClient.endFileLease(fileId);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,6 @@\n   public synchronized void close() throws IOException {\n-    TraceScope scope \u003d\n-        dfsClient.newPathTraceScope(\"DFSOutputStream#close\", src);\n-    try {\n+    try (TraceScope ignored \u003d\n+             dfsClient.newPathTraceScope(\"DFSOutputStream#close\", src)) {\n       closeImpl();\n-    } finally {\n-      scope.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    try (TraceScope ignored \u003d\n             dfsClient.newPathTraceScope(\"DFSOutputStream#close\", src)) {\n      closeImpl();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "892ade689f9bcce76daae8f66fc00a49bee8548e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
      "commitDate": "28/09/15 7:42 AM",
      "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "26/09/15 11:08 AM",
      "commitNameOld": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.86,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,9 @@\n   public synchronized void close() throws IOException {\n     TraceScope scope \u003d\n-        dfsClient.getPathTraceScope(\"DFSOutputStream#close\", src);\n+        dfsClient.newPathTraceScope(\"DFSOutputStream#close\", src);\n     try {\n       closeImpl();\n     } finally {\n       scope.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    TraceScope scope \u003d\n        dfsClient.newPathTraceScope(\"DFSOutputStream#close\", src);\n    try {\n      closeImpl();\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized void close() throws IOException {\n    TraceScope scope \u003d\n        dfsClient.getPathTraceScope(\"DFSOutputStream#close\", src);\n    try {\n      closeImpl();\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7054. Make DFSOutputStream tracing more fine-grained (cmccabe)\n",
      "commitDate": "18/03/15 6:14 PM",
      "commitName": "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "16/03/15 9:58 PM",
      "commitNameOld": "046521cd6511b7fc6d9478cb2bed90d8e75fca20",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,9 @@\n   public synchronized void close() throws IOException {\n-    if (isClosed()) {\n-      IOException e \u003d lastException.getAndSet(null);\n-      if (e \u003d\u003d null)\n-        return;\n-      else\n-        throw e;\n-    }\n-\n+    TraceScope scope \u003d\n+        dfsClient.getPathTraceScope(\"DFSOutputStream#close\", src);\n     try {\n-      flushBuffer();       // flush from all upper layers\n-\n-      if (currentPacket !\u003d null) { \n-        waitAndQueueCurrentPacket();\n-      }\n-\n-      if (bytesCurBlock !\u003d 0) {\n-        // send an empty packet to mark the end of the block\n-        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n-        currentPacket.setSyncBlock(shouldSyncBlock);\n-      }\n-\n-      flushInternal();             // flush all data to Datanodes\n-      // get last block before destroying the streamer\n-      ExtendedBlock lastBlock \u003d streamer.getBlock();\n-      closeThreads(false);\n-      completeFile(lastBlock);\n-      dfsClient.endFileLease(fileId);\n-    } catch (ClosedChannelException e) {\n+      closeImpl();\n     } finally {\n-      setClosed();\n+      scope.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    TraceScope scope \u003d\n        dfsClient.getPathTraceScope(\"DFSOutputStream#close\", src);\n    try {\n      closeImpl();\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "952640fa4cbdc23fe8781e5627c2e8eab565c535": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7855. Separate class Packet from DFSOutputStream. Contributed by Li Bo.\n",
      "commitDate": "05/03/15 10:58 AM",
      "commitName": "952640fa4cbdc23fe8781e5627c2e8eab565c535",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "01/03/15 11:03 PM",
      "commitNameOld": "67ed59348d638d56e6752ba2c71fdcd69567546d",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 3.5,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,33 @@\n   public synchronized void close() throws IOException {\n     if (isClosed()) {\n       IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n-        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++);\n-        currentPacket.lastPacketInBlock \u003d true;\n-        currentPacket.syncBlock \u003d shouldSyncBlock;\n+        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n+        currentPacket.setSyncBlock(shouldSyncBlock);\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       setClosed();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (isClosed()) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n        currentPacket.setSyncBlock(shouldSyncBlock);\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "394ba94c5d2801fbc5d95c7872eeeede28eed1eb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7358. Clients may get stuck waiting when using ByteArrayManager.\n",
      "commitDate": "13/11/14 12:28 PM",
      "commitName": "394ba94c5d2801fbc5d95c7872eeeede28eed1eb",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "05/11/14 10:51 AM",
      "commitNameOld": "56257fab1d5a7f66bebd9149c7df0436c0a57adb",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 8.07,
      "commitsBetweenForRepo": 93,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   public synchronized void close() throws IOException {\n-    if (closed) {\n+    if (isClosed()) {\n       IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++);\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n-      closed \u003d true;\n+      setClosed();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (isClosed()) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++);\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "36ccf097a95eae0761de7b657752e4808a86c094": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7276. Limit the number of byte arrays used by DFSOutputStream and provide a mechanism for recycling arrays.\n",
      "commitDate": "01/11/14 11:22 AM",
      "commitName": "36ccf097a95eae0761de7b657752e4808a86c094",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "27/10/14 9:38 AM",
      "commitNameOld": "463aec11718e47d4aabb86a7a539cb973460aae6",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 5.07,
      "commitsBetweenForRepo": 75,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n-        currentPacket \u003d new Packet(0, 0, bytesCurBlock, currentSeqno++, getChecksumSize());\n+        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++);\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++);\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "463aec11718e47d4aabb86a7a539cb973460aae6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6934. Move checksum computation off the hot path when writing to RAM disk. Contributed by Chris Nauroth.\n",
      "commitDate": "27/10/14 9:38 AM",
      "commitName": "463aec11718e47d4aabb86a7a539cb973460aae6",
      "commitAuthor": "cnauroth",
      "commitDateOld": "17/10/14 6:30 PM",
      "commitNameOld": "2e140523d3ccb27809cde4a55e95f7e0006c028f",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 9.63,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,34 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n-        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n-            currentSeqno++, this.checksum.getChecksumSize());\n+        currentPacket \u003d new Packet(0, 0, bytesCurBlock, currentSeqno++, getChecksumSize());\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock, currentSeqno++, getChecksumSize());\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6294. Use INode IDs to avoid conflicts when a file open for write is renamed (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1593634 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/05/14 3:36 PM",
      "commitName": "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "25/03/14 9:11 PM",
      "commitNameOld": "1fbb04e367d7c330e6052207f9f11911f4f5f368",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 44.77,
      "commitsBetweenForRepo": 257,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,35 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n             currentSeqno++, this.checksum.getChecksumSize());\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n-      dfsClient.endFileLease(src);\n+      dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n            currentSeqno++, this.checksum.getChecksumSize());\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "f2f5cdb5554d294a29ebf465101c5607fd56e244": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5335. Hive query failed with possible race in dfs output stream. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1531152 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/10/13 4:58 PM",
      "commitName": "f2f5cdb5554d294a29ebf465101c5607fd56e244",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "22/07/13 11:15 AM",
      "commitNameOld": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 80.24,
      "commitsBetweenForRepo": 499,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,35 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n-      IOException e \u003d lastException;\n+      IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n             currentSeqno++, this.checksum.getChecksumSize());\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.endFileLease(src);\n+    } catch (ClosedChannelException e) {\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n            currentSeqno++, this.checksum.getChecksumSize());\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(src);\n    } catch (ClosedChannelException e) {\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "735046ebecd9e803398be56fbf79dbde5226b4c1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3851. DFSOutputStream class code cleanup. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1377372 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/08/12 9:00 PM",
      "commitName": "735046ebecd9e803398be56fbf79dbde5226b4c1",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "14/08/12 1:59 PM",
      "commitNameOld": "f98d8eb291be364102b5c3011ce72e8f43eab389",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 11.29,
      "commitsBetweenForRepo": 87,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,34 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       IOException e \u003d lastException;\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n-        currentPacket \u003d new Packet(0, 0, bytesCurBlock);\n+        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n+            currentSeqno++, this.checksum.getChecksumSize());\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.endFileLease(src);\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException;\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n            currentSeqno++, this.checksum.getChecksumSize());\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(src);\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3721. hsync support broke wire compatibility. Contributed by Todd Lipcon and Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1371495 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/08/12 2:31 PM",
      "commitName": "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "07/08/12 9:40 AM",
      "commitNameOld": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 2.2,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,33 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       IOException e \u003d lastException;\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n-        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n-            bytesCurBlock);\n+        currentPacket \u003d new Packet(0, 0, bytesCurBlock);\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.endFileLease(src);\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException;\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(src);\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "0a6806ce8c946b26eceac7d16b467c54c453df84": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3646. LeaseRenewer can hold reference to inactive DFSClient instances forever (Kihwal Lee via daryn)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1363170 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/07/12 4:21 PM",
      "commitName": "0a6806ce8c946b26eceac7d16b467c54c453df84",
      "commitAuthor": "Daryn Sharp",
      "commitDateOld": "15/07/12 7:58 PM",
      "commitNameOld": "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 2.85,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       IOException e \u003d lastException;\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n             bytesCurBlock);\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n-      dfsClient.leaserenewer.closeFile(src, dfsClient);\n+      dfsClient.endFileLease(src);\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException;\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(src);\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "83cf475050dba27e72b4e399491638c670621175": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-744. Support hsync in HDFS. Contributed by Lars Hofhans\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1344419 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/05/12 12:10 PM",
      "commitName": "83cf475050dba27e72b4e399491638c670621175",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "29/05/12 12:37 PM",
      "commitNameOld": "47a29c63291f1f9f09b89ce6f3305c0a2ef27b3f",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.98,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,34 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       IOException e \u003d lastException;\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n             bytesCurBlock);\n         currentPacket.lastPacketInBlock \u003d true;\n+        currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.leaserenewer.closeFile(src, dfsClient);\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException;\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.leaserenewer.closeFile(src, dfsClient);\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException;\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.leaserenewer.closeFile(src, dfsClient);\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException;\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.leaserenewer.closeFile(src, dfsClient);\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,33 @@\n+  public synchronized void close() throws IOException {\n+    if (closed) {\n+      IOException e \u003d lastException;\n+      if (e \u003d\u003d null)\n+        return;\n+      else\n+        throw e;\n+    }\n+\n+    try {\n+      flushBuffer();       // flush from all upper layers\n+\n+      if (currentPacket !\u003d null) { \n+        waitAndQueueCurrentPacket();\n+      }\n+\n+      if (bytesCurBlock !\u003d 0) {\n+        // send an empty packet to mark the end of the block\n+        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n+            bytesCurBlock);\n+        currentPacket.lastPacketInBlock \u003d true;\n+      }\n+\n+      flushInternal();             // flush all data to Datanodes\n+      // get last block before destroying the streamer\n+      ExtendedBlock lastBlock \u003d streamer.getBlock();\n+      closeThreads(false);\n+      completeFile(lastBlock);\n+      dfsClient.leaserenewer.closeFile(src, dfsClient);\n+    } finally {\n+      closed \u003d true;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException;\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.leaserenewer.closeFile(src, dfsClient);\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}