{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Command.java",
  "functionName": "populatePathNames",
  "functionId": "populatePathNames___node-DiskBalancerDataNode",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java",
  "functionStartLine": 542,
  "functionEndLine": 564,
  "numCommitsSeen": 33,
  "timeTaken": 1336,
  "changeHistory": [
    "b047bc7270f3461156e4d08423c728ee9c67dba5",
    "9f29f423e426e2d42e650cbed88e46c1c29a2a63",
    "64ccb232ccf204991a28fa0211917fa935ad30c5"
  ],
  "changeHistoryShort": {
    "b047bc7270f3461156e4d08423c728ee9c67dba5": "Ybodychange",
    "9f29f423e426e2d42e650cbed88e46c1c29a2a63": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange)",
    "64ccb232ccf204991a28fa0211917fa935ad30c5": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b047bc7270f3461156e4d08423c728ee9c67dba5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10560. DiskBalancer: Reuse ObjectMapper instance to improve the performance. Contributed by Yiqun Lin.\n",
      "commitDate": "16/08/16 10:20 AM",
      "commitName": "b047bc7270f3461156e4d08423c728ee9c67dba5",
      "commitAuthor": "Anu Engineer",
      "commitDateOld": "15/08/16 9:47 AM",
      "commitNameOld": "9f29f423e426e2d42e650cbed88e46c1c29a2a63",
      "commitAuthorOld": "Anu Engineer",
      "daysBetweenCommits": 1.02,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,23 @@\n   protected void populatePathNames(\n       DiskBalancerDataNode node) throws IOException {\n     // if the cluster is a local file system, there is no need to\n     // invoke rpc call to dataNode.\n     if (getClusterURI().getScheme().startsWith(\"file\")) {\n       return;\n     }\n     String dnAddress \u003d node.getDataNodeIP() + \":\" + node.getDataNodePort();\n     ClientDatanodeProtocol dnClient \u003d getDataNodeProxy(dnAddress);\n     String volumeNameJson \u003d dnClient.getDiskBalancerSetting(\n         DiskBalancerConstants.DISKBALANCER_VOLUME_NAME);\n-    ObjectMapper mapper \u003d new ObjectMapper();\n \n     @SuppressWarnings(\"unchecked\")\n     Map\u003cString, String\u003e volumeMap \u003d\n-        mapper.readValue(volumeNameJson, HashMap.class);\n+        READER.readValue(volumeNameJson);\n     for (DiskBalancerVolumeSet set : node.getVolumeSets().values()) {\n       for (DiskBalancerVolume vol : set.getVolumes()) {\n         if (volumeMap.containsKey(vol.getUuid())) {\n           vol.setPath(volumeMap.get(vol.getUuid()));\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void populatePathNames(\n      DiskBalancerDataNode node) throws IOException {\n    // if the cluster is a local file system, there is no need to\n    // invoke rpc call to dataNode.\n    if (getClusterURI().getScheme().startsWith(\"file\")) {\n      return;\n    }\n    String dnAddress \u003d node.getDataNodeIP() + \":\" + node.getDataNodePort();\n    ClientDatanodeProtocol dnClient \u003d getDataNodeProxy(dnAddress);\n    String volumeNameJson \u003d dnClient.getDiskBalancerSetting(\n        DiskBalancerConstants.DISKBALANCER_VOLUME_NAME);\n\n    @SuppressWarnings(\"unchecked\")\n    Map\u003cString, String\u003e volumeMap \u003d\n        READER.readValue(volumeNameJson);\n    for (DiskBalancerVolumeSet set : node.getVolumeSets().values()) {\n      for (DiskBalancerVolume vol : set.getVolumes()) {\n        if (volumeMap.containsKey(vol.getUuid())) {\n          vol.setPath(volumeMap.get(vol.getUuid()));\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java",
      "extendedDetails": {}
    },
    "9f29f423e426e2d42e650cbed88e46c1c29a2a63": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-10737. disk balancer add volume path to report command. Contributed by Yuanbo Liu.\n",
      "commitDate": "15/08/16 9:47 AM",
      "commitName": "9f29f423e426e2d42e650cbed88e46c1c29a2a63",
      "commitAuthor": "Anu Engineer",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-10737. disk balancer add volume path to report command. Contributed by Yuanbo Liu.\n",
          "commitDate": "15/08/16 9:47 AM",
          "commitName": "9f29f423e426e2d42e650cbed88e46c1c29a2a63",
          "commitAuthor": "Anu Engineer",
          "commitDateOld": "14/08/16 3:01 PM",
          "commitNameOld": "d677b68c2599445fff56db4df26448a8bad0f5dd",
          "commitAuthorOld": "Varun Saxena",
          "daysBetweenCommits": 0.78,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,24 @@\n-  private void populatePathNames(DiskBalancerDataNode node) throws IOException {\n+  protected void populatePathNames(\n+      DiskBalancerDataNode node) throws IOException {\n+    // if the cluster is a local file system, there is no need to\n+    // invoke rpc call to dataNode.\n+    if (getClusterURI().getScheme().startsWith(\"file\")) {\n+      return;\n+    }\n     String dnAddress \u003d node.getDataNodeIP() + \":\" + node.getDataNodePort();\n     ClientDatanodeProtocol dnClient \u003d getDataNodeProxy(dnAddress);\n     String volumeNameJson \u003d dnClient.getDiskBalancerSetting(\n         DiskBalancerConstants.DISKBALANCER_VOLUME_NAME);\n     ObjectMapper mapper \u003d new ObjectMapper();\n \n     @SuppressWarnings(\"unchecked\")\n     Map\u003cString, String\u003e volumeMap \u003d\n         mapper.readValue(volumeNameJson, HashMap.class);\n     for (DiskBalancerVolumeSet set : node.getVolumeSets().values()) {\n       for (DiskBalancerVolume vol : set.getVolumes()) {\n         if (volumeMap.containsKey(vol.getUuid())) {\n           vol.setPath(volumeMap.get(vol.getUuid()));\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void populatePathNames(\n      DiskBalancerDataNode node) throws IOException {\n    // if the cluster is a local file system, there is no need to\n    // invoke rpc call to dataNode.\n    if (getClusterURI().getScheme().startsWith(\"file\")) {\n      return;\n    }\n    String dnAddress \u003d node.getDataNodeIP() + \":\" + node.getDataNodePort();\n    ClientDatanodeProtocol dnClient \u003d getDataNodeProxy(dnAddress);\n    String volumeNameJson \u003d dnClient.getDiskBalancerSetting(\n        DiskBalancerConstants.DISKBALANCER_VOLUME_NAME);\n    ObjectMapper mapper \u003d new ObjectMapper();\n\n    @SuppressWarnings(\"unchecked\")\n    Map\u003cString, String\u003e volumeMap \u003d\n        mapper.readValue(volumeNameJson, HashMap.class);\n    for (DiskBalancerVolumeSet set : node.getVolumeSets().values()) {\n      for (DiskBalancerVolume vol : set.getVolumes()) {\n        if (volumeMap.containsKey(vol.getUuid())) {\n          vol.setPath(volumeMap.get(vol.getUuid()));\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java",
            "oldMethodName": "populatePathNames",
            "newMethodName": "populatePathNames"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-10737. disk balancer add volume path to report command. Contributed by Yuanbo Liu.\n",
          "commitDate": "15/08/16 9:47 AM",
          "commitName": "9f29f423e426e2d42e650cbed88e46c1c29a2a63",
          "commitAuthor": "Anu Engineer",
          "commitDateOld": "14/08/16 3:01 PM",
          "commitNameOld": "d677b68c2599445fff56db4df26448a8bad0f5dd",
          "commitAuthorOld": "Varun Saxena",
          "daysBetweenCommits": 0.78,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,24 @@\n-  private void populatePathNames(DiskBalancerDataNode node) throws IOException {\n+  protected void populatePathNames(\n+      DiskBalancerDataNode node) throws IOException {\n+    // if the cluster is a local file system, there is no need to\n+    // invoke rpc call to dataNode.\n+    if (getClusterURI().getScheme().startsWith(\"file\")) {\n+      return;\n+    }\n     String dnAddress \u003d node.getDataNodeIP() + \":\" + node.getDataNodePort();\n     ClientDatanodeProtocol dnClient \u003d getDataNodeProxy(dnAddress);\n     String volumeNameJson \u003d dnClient.getDiskBalancerSetting(\n         DiskBalancerConstants.DISKBALANCER_VOLUME_NAME);\n     ObjectMapper mapper \u003d new ObjectMapper();\n \n     @SuppressWarnings(\"unchecked\")\n     Map\u003cString, String\u003e volumeMap \u003d\n         mapper.readValue(volumeNameJson, HashMap.class);\n     for (DiskBalancerVolumeSet set : node.getVolumeSets().values()) {\n       for (DiskBalancerVolume vol : set.getVolumes()) {\n         if (volumeMap.containsKey(vol.getUuid())) {\n           vol.setPath(volumeMap.get(vol.getUuid()));\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void populatePathNames(\n      DiskBalancerDataNode node) throws IOException {\n    // if the cluster is a local file system, there is no need to\n    // invoke rpc call to dataNode.\n    if (getClusterURI().getScheme().startsWith(\"file\")) {\n      return;\n    }\n    String dnAddress \u003d node.getDataNodeIP() + \":\" + node.getDataNodePort();\n    ClientDatanodeProtocol dnClient \u003d getDataNodeProxy(dnAddress);\n    String volumeNameJson \u003d dnClient.getDiskBalancerSetting(\n        DiskBalancerConstants.DISKBALANCER_VOLUME_NAME);\n    ObjectMapper mapper \u003d new ObjectMapper();\n\n    @SuppressWarnings(\"unchecked\")\n    Map\u003cString, String\u003e volumeMap \u003d\n        mapper.readValue(volumeNameJson, HashMap.class);\n    for (DiskBalancerVolumeSet set : node.getVolumeSets().values()) {\n      for (DiskBalancerVolume vol : set.getVolumes()) {\n        if (volumeMap.containsKey(vol.getUuid())) {\n          vol.setPath(volumeMap.get(vol.getUuid()));\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[protected]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10737. disk balancer add volume path to report command. Contributed by Yuanbo Liu.\n",
          "commitDate": "15/08/16 9:47 AM",
          "commitName": "9f29f423e426e2d42e650cbed88e46c1c29a2a63",
          "commitAuthor": "Anu Engineer",
          "commitDateOld": "14/08/16 3:01 PM",
          "commitNameOld": "d677b68c2599445fff56db4df26448a8bad0f5dd",
          "commitAuthorOld": "Varun Saxena",
          "daysBetweenCommits": 0.78,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,24 @@\n-  private void populatePathNames(DiskBalancerDataNode node) throws IOException {\n+  protected void populatePathNames(\n+      DiskBalancerDataNode node) throws IOException {\n+    // if the cluster is a local file system, there is no need to\n+    // invoke rpc call to dataNode.\n+    if (getClusterURI().getScheme().startsWith(\"file\")) {\n+      return;\n+    }\n     String dnAddress \u003d node.getDataNodeIP() + \":\" + node.getDataNodePort();\n     ClientDatanodeProtocol dnClient \u003d getDataNodeProxy(dnAddress);\n     String volumeNameJson \u003d dnClient.getDiskBalancerSetting(\n         DiskBalancerConstants.DISKBALANCER_VOLUME_NAME);\n     ObjectMapper mapper \u003d new ObjectMapper();\n \n     @SuppressWarnings(\"unchecked\")\n     Map\u003cString, String\u003e volumeMap \u003d\n         mapper.readValue(volumeNameJson, HashMap.class);\n     for (DiskBalancerVolumeSet set : node.getVolumeSets().values()) {\n       for (DiskBalancerVolume vol : set.getVolumes()) {\n         if (volumeMap.containsKey(vol.getUuid())) {\n           vol.setPath(volumeMap.get(vol.getUuid()));\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void populatePathNames(\n      DiskBalancerDataNode node) throws IOException {\n    // if the cluster is a local file system, there is no need to\n    // invoke rpc call to dataNode.\n    if (getClusterURI().getScheme().startsWith(\"file\")) {\n      return;\n    }\n    String dnAddress \u003d node.getDataNodeIP() + \":\" + node.getDataNodePort();\n    ClientDatanodeProtocol dnClient \u003d getDataNodeProxy(dnAddress);\n    String volumeNameJson \u003d dnClient.getDiskBalancerSetting(\n        DiskBalancerConstants.DISKBALANCER_VOLUME_NAME);\n    ObjectMapper mapper \u003d new ObjectMapper();\n\n    @SuppressWarnings(\"unchecked\")\n    Map\u003cString, String\u003e volumeMap \u003d\n        mapper.readValue(volumeNameJson, HashMap.class);\n    for (DiskBalancerVolumeSet set : node.getVolumeSets().values()) {\n      for (DiskBalancerVolume vol : set.getVolumes()) {\n        if (volumeMap.containsKey(vol.getUuid())) {\n          vol.setPath(volumeMap.get(vol.getUuid()));\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/Command.java",
          "extendedDetails": {}
        }
      ]
    },
    "64ccb232ccf204991a28fa0211917fa935ad30c5": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-10478. DiskBalancer: resolve volume path names. Contributed by Anu Engineer.\n",
      "commitDate": "23/06/16 6:21 PM",
      "commitName": "64ccb232ccf204991a28fa0211917fa935ad30c5",
      "commitAuthor": "Anu Engineer",
      "diff": "@@ -0,0 +1,18 @@\n+  private void populatePathNames(DiskBalancerDataNode node) throws IOException {\n+    String dnAddress \u003d node.getDataNodeIP() + \":\" + node.getDataNodePort();\n+    ClientDatanodeProtocol dnClient \u003d getDataNodeProxy(dnAddress);\n+    String volumeNameJson \u003d dnClient.getDiskBalancerSetting(\n+        DiskBalancerConstants.DISKBALANCER_VOLUME_NAME);\n+    ObjectMapper mapper \u003d new ObjectMapper();\n+\n+    @SuppressWarnings(\"unchecked\")\n+    Map\u003cString, String\u003e volumeMap \u003d\n+        mapper.readValue(volumeNameJson, HashMap.class);\n+    for (DiskBalancerVolumeSet set : node.getVolumeSets().values()) {\n+      for (DiskBalancerVolume vol : set.getVolumes()) {\n+        if (volumeMap.containsKey(vol.getUuid())) {\n+          vol.setPath(volumeMap.get(vol.getUuid()));\n+        }\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void populatePathNames(DiskBalancerDataNode node) throws IOException {\n    String dnAddress \u003d node.getDataNodeIP() + \":\" + node.getDataNodePort();\n    ClientDatanodeProtocol dnClient \u003d getDataNodeProxy(dnAddress);\n    String volumeNameJson \u003d dnClient.getDiskBalancerSetting(\n        DiskBalancerConstants.DISKBALANCER_VOLUME_NAME);\n    ObjectMapper mapper \u003d new ObjectMapper();\n\n    @SuppressWarnings(\"unchecked\")\n    Map\u003cString, String\u003e volumeMap \u003d\n        mapper.readValue(volumeNameJson, HashMap.class);\n    for (DiskBalancerVolumeSet set : node.getVolumeSets().values()) {\n      for (DiskBalancerVolume vol : set.getVolumes()) {\n        if (volumeMap.containsKey(vol.getUuid())) {\n          vol.setPath(volumeMap.get(vol.getUuid()));\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/PlanCommand.java"
    }
  }
}