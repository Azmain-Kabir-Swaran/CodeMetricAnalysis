{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSTopologyNodeImpl.java",
  "functionName": "remove",
  "functionId": "remove___n-Node",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java",
  "functionStartLine": 292,
  "functionEndLine": 365,
  "numCommitsSeen": 5,
  "timeTaken": 2368,
  "changeHistory": [
    "97c2e576c91c2316c2b52bfc948bae9bff8ca49f",
    "615ac09499dc0b391cbb99bb0e9877959a9173a6",
    "9832ae0ed8853d29072c9ea7031cd2373e6b16f9",
    "eeca8b0c4e2804b0fee5b012ea14b58383425ec3"
  ],
  "changeHistoryShort": {
    "97c2e576c91c2316c2b52bfc948bae9bff8ca49f": "Ybodychange",
    "615ac09499dc0b391cbb99bb0e9877959a9173a6": "Ybodychange",
    "9832ae0ed8853d29072c9ea7031cd2373e6b16f9": "Yfilerename",
    "eeca8b0c4e2804b0fee5b012ea14b58383425ec3": "Yintroduced"
  },
  "changeHistoryDetails": {
    "97c2e576c91c2316c2b52bfc948bae9bff8ca49f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11530. Use HDFS specific network topology to choose datanode in BlockPlacementPolicyDefault. Contributed by Yiqun Lin and Chen Liang.\n",
      "commitDate": "04/05/17 8:54 PM",
      "commitName": "97c2e576c91c2316c2b52bfc948bae9bff8ca49f",
      "commitAuthor": "Yiqun Lin",
      "commitDateOld": "15/03/17 12:28 PM",
      "commitNameOld": "615ac09499dc0b391cbb99bb0e9877959a9173a6",
      "commitAuthorOld": "Chen Liang",
      "daysBetweenCommits": 50.35,
      "commitsBetweenForRepo": 290,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,73 +1,74 @@\n   public boolean remove(Node n) {\n+    LOG.debug(\"removing node {}\", n.getName());\n     if (!isAncestor(n)) {\n       throw new IllegalArgumentException(n.getName()\n           + \", which is located at \" + n.getNetworkLocation()\n           + \", is not a descendant of \" + getPath(this));\n     }\n     // In HDFS topology, the leaf node should always be DatanodeDescriptor\n     if (!(n instanceof DatanodeDescriptor)) {\n       throw new IllegalArgumentException(\"Unexpected node type \"\n           + n.getClass().getName());\n     }\n     DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor) n;\n     if (isParent(n)) {\n       // this node is the parent of n; remove n directly\n       if (childrenMap.containsKey(n.getName())) {\n         for (int i\u003d0; i\u003cchildren.size(); i++) {\n           if (children.get(i).getName().equals(n.getName())) {\n             children.remove(i);\n             childrenMap.remove(n.getName());\n             childrenStorageInfo.remove(dnDescriptor.getName());\n             for (StorageType st : dnDescriptor.getStorageTypes()) {\n               decStorageTypeCount(st);\n             }\n             numOfLeaves--;\n             n.setParent(null);\n             return true;\n           }\n         }\n       }\n       return false;\n     } else {\n       // find the next ancestor node: the parent node\n       String parentName \u003d getNextAncestorName(n);\n       DFSTopologyNodeImpl parentNode \u003d\n           (DFSTopologyNodeImpl)childrenMap.get(parentName);\n       if (parentNode \u003d\u003d null) {\n         return false;\n       }\n       // remove n from the parent node\n       boolean isRemoved \u003d parentNode.remove(n);\n       if (isRemoved) {\n         // if the parent node has no children, remove the parent node too\n         EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n             childrenStorageInfo.get(parentNode.getName());\n         EnumSet\u003cStorageType\u003e toRemove \u003d EnumSet.noneOf(StorageType.class);\n         for (StorageType st : dnDescriptor.getStorageTypes()) {\n           int newCount \u003d currentCount.get(st) - 1;\n           if (newCount \u003d\u003d 0) {\n             toRemove.add(st);\n           }\n           currentCount.put(st, newCount);\n         }\n         for (StorageType st : toRemove) {\n           currentCount.remove(st);\n         }\n         for (StorageType st : dnDescriptor.getStorageTypes()) {\n           decStorageTypeCount(st);\n         }\n         if (parentNode.getNumOfChildren() \u003d\u003d 0) {\n           for(int i\u003d0; i \u003c children.size(); i++) {\n             if (children.get(i).getName().equals(parentName)) {\n               children.remove(i);\n               childrenMap.remove(parentName);\n               childrenStorageInfo.remove(parentNode.getName());\n               break;\n             }\n           }\n         }\n         numOfLeaves--;\n       }\n       return isRemoved;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean remove(Node n) {\n    LOG.debug(\"removing node {}\", n.getName());\n    if (!isAncestor(n)) {\n      throw new IllegalArgumentException(n.getName()\n          + \", which is located at \" + n.getNetworkLocation()\n          + \", is not a descendant of \" + getPath(this));\n    }\n    // In HDFS topology, the leaf node should always be DatanodeDescriptor\n    if (!(n instanceof DatanodeDescriptor)) {\n      throw new IllegalArgumentException(\"Unexpected node type \"\n          + n.getClass().getName());\n    }\n    DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor) n;\n    if (isParent(n)) {\n      // this node is the parent of n; remove n directly\n      if (childrenMap.containsKey(n.getName())) {\n        for (int i\u003d0; i\u003cchildren.size(); i++) {\n          if (children.get(i).getName().equals(n.getName())) {\n            children.remove(i);\n            childrenMap.remove(n.getName());\n            childrenStorageInfo.remove(dnDescriptor.getName());\n            for (StorageType st : dnDescriptor.getStorageTypes()) {\n              decStorageTypeCount(st);\n            }\n            numOfLeaves--;\n            n.setParent(null);\n            return true;\n          }\n        }\n      }\n      return false;\n    } else {\n      // find the next ancestor node: the parent node\n      String parentName \u003d getNextAncestorName(n);\n      DFSTopologyNodeImpl parentNode \u003d\n          (DFSTopologyNodeImpl)childrenMap.get(parentName);\n      if (parentNode \u003d\u003d null) {\n        return false;\n      }\n      // remove n from the parent node\n      boolean isRemoved \u003d parentNode.remove(n);\n      if (isRemoved) {\n        // if the parent node has no children, remove the parent node too\n        EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n            childrenStorageInfo.get(parentNode.getName());\n        EnumSet\u003cStorageType\u003e toRemove \u003d EnumSet.noneOf(StorageType.class);\n        for (StorageType st : dnDescriptor.getStorageTypes()) {\n          int newCount \u003d currentCount.get(st) - 1;\n          if (newCount \u003d\u003d 0) {\n            toRemove.add(st);\n          }\n          currentCount.put(st, newCount);\n        }\n        for (StorageType st : toRemove) {\n          currentCount.remove(st);\n        }\n        for (StorageType st : dnDescriptor.getStorageTypes()) {\n          decStorageTypeCount(st);\n        }\n        if (parentNode.getNumOfChildren() \u003d\u003d 0) {\n          for(int i\u003d0; i \u003c children.size(); i++) {\n            if (children.get(i).getName().equals(parentName)) {\n              children.remove(i);\n              childrenMap.remove(parentName);\n              childrenStorageInfo.remove(parentNode.getName());\n              break;\n            }\n          }\n        }\n        numOfLeaves--;\n      }\n      return isRemoved;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java",
      "extendedDetails": {}
    },
    "615ac09499dc0b391cbb99bb0e9877959a9173a6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11419. DFSTopologyNodeImpl#chooseRandom optimizations. Contributed by Chen Liang.\n",
      "commitDate": "15/03/17 12:28 PM",
      "commitName": "615ac09499dc0b391cbb99bb0e9877959a9173a6",
      "commitAuthor": "Chen Liang",
      "commitDateOld": "13/03/17 5:30 PM",
      "commitNameOld": "9832ae0ed8853d29072c9ea7031cd2373e6b16f9",
      "commitAuthorOld": "Chen Liang",
      "daysBetweenCommits": 1.79,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,73 @@\n   public boolean remove(Node n) {\n     if (!isAncestor(n)) {\n       throw new IllegalArgumentException(n.getName()\n           + \", which is located at \" + n.getNetworkLocation()\n           + \", is not a descendant of \" + getPath(this));\n     }\n     // In HDFS topology, the leaf node should always be DatanodeDescriptor\n     if (!(n instanceof DatanodeDescriptor)) {\n       throw new IllegalArgumentException(\"Unexpected node type \"\n           + n.getClass().getName());\n     }\n     DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor) n;\n     if (isParent(n)) {\n       // this node is the parent of n; remove n directly\n       if (childrenMap.containsKey(n.getName())) {\n         for (int i\u003d0; i\u003cchildren.size(); i++) {\n           if (children.get(i).getName().equals(n.getName())) {\n             children.remove(i);\n             childrenMap.remove(n.getName());\n-            synchronized (childrenStorageInfo) {\n-              childrenStorageInfo.remove(dnDescriptor.getName());\n+            childrenStorageInfo.remove(dnDescriptor.getName());\n+            for (StorageType st : dnDescriptor.getStorageTypes()) {\n+              decStorageTypeCount(st);\n             }\n             numOfLeaves--;\n             n.setParent(null);\n             return true;\n           }\n         }\n       }\n       return false;\n     } else {\n       // find the next ancestor node: the parent node\n       String parentName \u003d getNextAncestorName(n);\n       DFSTopologyNodeImpl parentNode \u003d\n           (DFSTopologyNodeImpl)childrenMap.get(parentName);\n       if (parentNode \u003d\u003d null) {\n         return false;\n       }\n       // remove n from the parent node\n       boolean isRemoved \u003d parentNode.remove(n);\n       if (isRemoved) {\n         // if the parent node has no children, remove the parent node too\n-        synchronized (childrenStorageInfo) {\n-          EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n-              childrenStorageInfo.get(parentNode.getName());\n-          EnumSet\u003cStorageType\u003e toRemove \u003d EnumSet.noneOf(StorageType.class);\n-          for (StorageType st : dnDescriptor.getStorageTypes()) {\n-            int newCount \u003d currentCount.get(st) - 1;\n-            if (newCount \u003d\u003d 0) {\n-              toRemove.add(st);\n-            }\n-            currentCount.put(st, newCount);\n+        EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n+            childrenStorageInfo.get(parentNode.getName());\n+        EnumSet\u003cStorageType\u003e toRemove \u003d EnumSet.noneOf(StorageType.class);\n+        for (StorageType st : dnDescriptor.getStorageTypes()) {\n+          int newCount \u003d currentCount.get(st) - 1;\n+          if (newCount \u003d\u003d 0) {\n+            toRemove.add(st);\n           }\n-          for (StorageType st : toRemove) {\n-            currentCount.remove(st);\n-          }\n+          currentCount.put(st, newCount);\n+        }\n+        for (StorageType st : toRemove) {\n+          currentCount.remove(st);\n+        }\n+        for (StorageType st : dnDescriptor.getStorageTypes()) {\n+          decStorageTypeCount(st);\n         }\n         if (parentNode.getNumOfChildren() \u003d\u003d 0) {\n           for(int i\u003d0; i \u003c children.size(); i++) {\n             if (children.get(i).getName().equals(parentName)) {\n               children.remove(i);\n               childrenMap.remove(parentName);\n               childrenStorageInfo.remove(parentNode.getName());\n               break;\n             }\n           }\n         }\n         numOfLeaves--;\n       }\n       return isRemoved;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean remove(Node n) {\n    if (!isAncestor(n)) {\n      throw new IllegalArgumentException(n.getName()\n          + \", which is located at \" + n.getNetworkLocation()\n          + \", is not a descendant of \" + getPath(this));\n    }\n    // In HDFS topology, the leaf node should always be DatanodeDescriptor\n    if (!(n instanceof DatanodeDescriptor)) {\n      throw new IllegalArgumentException(\"Unexpected node type \"\n          + n.getClass().getName());\n    }\n    DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor) n;\n    if (isParent(n)) {\n      // this node is the parent of n; remove n directly\n      if (childrenMap.containsKey(n.getName())) {\n        for (int i\u003d0; i\u003cchildren.size(); i++) {\n          if (children.get(i).getName().equals(n.getName())) {\n            children.remove(i);\n            childrenMap.remove(n.getName());\n            childrenStorageInfo.remove(dnDescriptor.getName());\n            for (StorageType st : dnDescriptor.getStorageTypes()) {\n              decStorageTypeCount(st);\n            }\n            numOfLeaves--;\n            n.setParent(null);\n            return true;\n          }\n        }\n      }\n      return false;\n    } else {\n      // find the next ancestor node: the parent node\n      String parentName \u003d getNextAncestorName(n);\n      DFSTopologyNodeImpl parentNode \u003d\n          (DFSTopologyNodeImpl)childrenMap.get(parentName);\n      if (parentNode \u003d\u003d null) {\n        return false;\n      }\n      // remove n from the parent node\n      boolean isRemoved \u003d parentNode.remove(n);\n      if (isRemoved) {\n        // if the parent node has no children, remove the parent node too\n        EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n            childrenStorageInfo.get(parentNode.getName());\n        EnumSet\u003cStorageType\u003e toRemove \u003d EnumSet.noneOf(StorageType.class);\n        for (StorageType st : dnDescriptor.getStorageTypes()) {\n          int newCount \u003d currentCount.get(st) - 1;\n          if (newCount \u003d\u003d 0) {\n            toRemove.add(st);\n          }\n          currentCount.put(st, newCount);\n        }\n        for (StorageType st : toRemove) {\n          currentCount.remove(st);\n        }\n        for (StorageType st : dnDescriptor.getStorageTypes()) {\n          decStorageTypeCount(st);\n        }\n        if (parentNode.getNumOfChildren() \u003d\u003d 0) {\n          for(int i\u003d0; i \u003c children.size(); i++) {\n            if (children.get(i).getName().equals(parentName)) {\n              children.remove(i);\n              childrenMap.remove(parentName);\n              childrenStorageInfo.remove(parentNode.getName());\n              break;\n            }\n          }\n        }\n        numOfLeaves--;\n      }\n      return isRemoved;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java",
      "extendedDetails": {}
    },
    "9832ae0ed8853d29072c9ea7031cd2373e6b16f9": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-11482. Add storage type demand to into DFSNetworkTopology#chooseRandom. Contributed by Chen Liang.\n",
      "commitDate": "13/03/17 5:30 PM",
      "commitName": "9832ae0ed8853d29072c9ea7031cd2373e6b16f9",
      "commitAuthor": "Chen Liang",
      "commitDateOld": "13/03/17 2:24 PM",
      "commitNameOld": "55796a0946f80a35055701a34379e374399009c5",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.13,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public boolean remove(Node n) {\n    if (!isAncestor(n)) {\n      throw new IllegalArgumentException(n.getName()\n          + \", which is located at \" + n.getNetworkLocation()\n          + \", is not a descendant of \" + getPath(this));\n    }\n    // In HDFS topology, the leaf node should always be DatanodeDescriptor\n    if (!(n instanceof DatanodeDescriptor)) {\n      throw new IllegalArgumentException(\"Unexpected node type \"\n          + n.getClass().getName());\n    }\n    DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor) n;\n    if (isParent(n)) {\n      // this node is the parent of n; remove n directly\n      if (childrenMap.containsKey(n.getName())) {\n        for (int i\u003d0; i\u003cchildren.size(); i++) {\n          if (children.get(i).getName().equals(n.getName())) {\n            children.remove(i);\n            childrenMap.remove(n.getName());\n            synchronized (childrenStorageInfo) {\n              childrenStorageInfo.remove(dnDescriptor.getName());\n            }\n            numOfLeaves--;\n            n.setParent(null);\n            return true;\n          }\n        }\n      }\n      return false;\n    } else {\n      // find the next ancestor node: the parent node\n      String parentName \u003d getNextAncestorName(n);\n      DFSTopologyNodeImpl parentNode \u003d\n          (DFSTopologyNodeImpl)childrenMap.get(parentName);\n      if (parentNode \u003d\u003d null) {\n        return false;\n      }\n      // remove n from the parent node\n      boolean isRemoved \u003d parentNode.remove(n);\n      if (isRemoved) {\n        // if the parent node has no children, remove the parent node too\n        synchronized (childrenStorageInfo) {\n          EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n              childrenStorageInfo.get(parentNode.getName());\n          EnumSet\u003cStorageType\u003e toRemove \u003d EnumSet.noneOf(StorageType.class);\n          for (StorageType st : dnDescriptor.getStorageTypes()) {\n            int newCount \u003d currentCount.get(st) - 1;\n            if (newCount \u003d\u003d 0) {\n              toRemove.add(st);\n            }\n            currentCount.put(st, newCount);\n          }\n          for (StorageType st : toRemove) {\n            currentCount.remove(st);\n          }\n        }\n        if (parentNode.getNumOfChildren() \u003d\u003d 0) {\n          for(int i\u003d0; i \u003c children.size(); i++) {\n            if (children.get(i).getName().equals(parentName)) {\n              children.remove(i);\n              childrenMap.remove(parentName);\n              childrenStorageInfo.remove(parentNode.getName());\n              break;\n            }\n          }\n        }\n        numOfLeaves--;\n      }\n      return isRemoved;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTopologyNodeImpl.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java"
      }
    },
    "eeca8b0c4e2804b0fee5b012ea14b58383425ec3": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11419. HDFS specific network topology classes with storage type info included. Contributed by Chen Liang.\n",
      "commitDate": "02/03/17 9:21 AM",
      "commitName": "eeca8b0c4e2804b0fee5b012ea14b58383425ec3",
      "commitAuthor": "Arpit Agarwal",
      "diff": "@@ -0,0 +1,71 @@\n+  public boolean remove(Node n) {\n+    if (!isAncestor(n)) {\n+      throw new IllegalArgumentException(n.getName()\n+          + \", which is located at \" + n.getNetworkLocation()\n+          + \", is not a descendant of \" + getPath(this));\n+    }\n+    // In HDFS topology, the leaf node should always be DatanodeDescriptor\n+    if (!(n instanceof DatanodeDescriptor)) {\n+      throw new IllegalArgumentException(\"Unexpected node type \"\n+          + n.getClass().getName());\n+    }\n+    DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor) n;\n+    if (isParent(n)) {\n+      // this node is the parent of n; remove n directly\n+      if (childrenMap.containsKey(n.getName())) {\n+        for (int i\u003d0; i\u003cchildren.size(); i++) {\n+          if (children.get(i).getName().equals(n.getName())) {\n+            children.remove(i);\n+            childrenMap.remove(n.getName());\n+            synchronized (childrenStorageInfo) {\n+              childrenStorageInfo.remove(dnDescriptor.getName());\n+            }\n+            numOfLeaves--;\n+            n.setParent(null);\n+            return true;\n+          }\n+        }\n+      }\n+      return false;\n+    } else {\n+      // find the next ancestor node: the parent node\n+      String parentName \u003d getNextAncestorName(n);\n+      DFSTopologyNodeImpl parentNode \u003d\n+          (DFSTopologyNodeImpl)childrenMap.get(parentName);\n+      if (parentNode \u003d\u003d null) {\n+        return false;\n+      }\n+      // remove n from the parent node\n+      boolean isRemoved \u003d parentNode.remove(n);\n+      if (isRemoved) {\n+        // if the parent node has no children, remove the parent node too\n+        synchronized (childrenStorageInfo) {\n+          EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n+              childrenStorageInfo.get(parentNode.getName());\n+          EnumSet\u003cStorageType\u003e toRemove \u003d EnumSet.noneOf(StorageType.class);\n+          for (StorageType st : dnDescriptor.getStorageTypes()) {\n+            int newCount \u003d currentCount.get(st) - 1;\n+            if (newCount \u003d\u003d 0) {\n+              toRemove.add(st);\n+            }\n+            currentCount.put(st, newCount);\n+          }\n+          for (StorageType st : toRemove) {\n+            currentCount.remove(st);\n+          }\n+        }\n+        if (parentNode.getNumOfChildren() \u003d\u003d 0) {\n+          for(int i\u003d0; i \u003c children.size(); i++) {\n+            if (children.get(i).getName().equals(parentName)) {\n+              children.remove(i);\n+              childrenMap.remove(parentName);\n+              childrenStorageInfo.remove(parentNode.getName());\n+              break;\n+            }\n+          }\n+        }\n+        numOfLeaves--;\n+      }\n+      return isRemoved;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean remove(Node n) {\n    if (!isAncestor(n)) {\n      throw new IllegalArgumentException(n.getName()\n          + \", which is located at \" + n.getNetworkLocation()\n          + \", is not a descendant of \" + getPath(this));\n    }\n    // In HDFS topology, the leaf node should always be DatanodeDescriptor\n    if (!(n instanceof DatanodeDescriptor)) {\n      throw new IllegalArgumentException(\"Unexpected node type \"\n          + n.getClass().getName());\n    }\n    DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor) n;\n    if (isParent(n)) {\n      // this node is the parent of n; remove n directly\n      if (childrenMap.containsKey(n.getName())) {\n        for (int i\u003d0; i\u003cchildren.size(); i++) {\n          if (children.get(i).getName().equals(n.getName())) {\n            children.remove(i);\n            childrenMap.remove(n.getName());\n            synchronized (childrenStorageInfo) {\n              childrenStorageInfo.remove(dnDescriptor.getName());\n            }\n            numOfLeaves--;\n            n.setParent(null);\n            return true;\n          }\n        }\n      }\n      return false;\n    } else {\n      // find the next ancestor node: the parent node\n      String parentName \u003d getNextAncestorName(n);\n      DFSTopologyNodeImpl parentNode \u003d\n          (DFSTopologyNodeImpl)childrenMap.get(parentName);\n      if (parentNode \u003d\u003d null) {\n        return false;\n      }\n      // remove n from the parent node\n      boolean isRemoved \u003d parentNode.remove(n);\n      if (isRemoved) {\n        // if the parent node has no children, remove the parent node too\n        synchronized (childrenStorageInfo) {\n          EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n              childrenStorageInfo.get(parentNode.getName());\n          EnumSet\u003cStorageType\u003e toRemove \u003d EnumSet.noneOf(StorageType.class);\n          for (StorageType st : dnDescriptor.getStorageTypes()) {\n            int newCount \u003d currentCount.get(st) - 1;\n            if (newCount \u003d\u003d 0) {\n              toRemove.add(st);\n            }\n            currentCount.put(st, newCount);\n          }\n          for (StorageType st : toRemove) {\n            currentCount.remove(st);\n          }\n        }\n        if (parentNode.getNumOfChildren() \u003d\u003d 0) {\n          for(int i\u003d0; i \u003c children.size(); i++) {\n            if (children.get(i).getName().equals(parentName)) {\n              children.remove(i);\n              childrenMap.remove(parentName);\n              childrenStorageInfo.remove(parentNode.getName());\n              break;\n            }\n          }\n        }\n        numOfLeaves--;\n      }\n      return isRemoved;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTopologyNodeImpl.java"
    }
  }
}