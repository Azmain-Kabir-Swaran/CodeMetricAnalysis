{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSImageFormatPBINode.java",
  "functionName": "loadINodeFile",
  "functionId": "loadINodeFile___n-INodeSection.INode",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
  "functionStartLine": 475,
  "functionEndLine": 545,
  "numCommitsSeen": 63,
  "timeTaken": 13059,
  "changeHistory": [
    "ae8f55b93243560bd891962d6c64320ddc62a7d7",
    "cb672a45a0bbd8950b9b5e304c2e03f516945903",
    "e8bdad7385ab63a122c1e8e8a6a73e0f1100e80b",
    "33a38a534110de454662256545a7f4c075d328c8",
    "5ca6ef0c268b1acb3abf12505b9ead6fe7e38a23",
    "5addacb1e301991a8285a221c726f66330cd6d08",
    "55c07bbed2f475f7b584a86112ee1b6fe0221e98",
    "a8f1c7f542963f66849bcb2a06893c6a99cbe235",
    "a2a5d7b5bca715835d92816e7b267b59f7270708",
    "864f878d5912c82f3204f1582cfb7eb7c9f1a1da",
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "164cbe643988f878f0f4100a4de51783e5b6738e",
    "e535e0f05b5fbd087c93238deb888cc985254b4c",
    "1d37a8812160bb030244a1e6b1c753f962d8d2ed",
    "f4c523b69ba55b1fd35e8995c3011a9f546ac835",
    "4fdd9abd7e43a0fb7b569982954a8f9660b9268b",
    "7e091de1366f4b57b5433bc19d738199dc05313d",
    "c12a974ccf5f52f63e4f825d8b4d2385953cd119",
    "683332b36de1040eb8901d676e666527e8c5f8fe",
    "c17439c2ddd921b63b1635e6f1cba634b8da8557",
    "cdc13efb1af54d931585d25c5ba696a012412828",
    "4928f5473394981829e5ffd4b16ea0801baf5c45",
    "c9103e9cacc67a614940e32fa87c5dbc3daa60de",
    "f05c21285ef23b6a973d69f045b1cb46c5abc039",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
    "0653918dad855b394e8e3b8b3f512f474d872ee9",
    "bb84f1fccb18c6c7373851e05d2451d55e908242",
    "042b33f20b01aadb5cd03da731ae7a3d94026aac",
    "6f41baa6233dad92865af23ec6b7a89733c11ddd",
    "1e89eba47d0f291b33fc26f9406231fc70b63a87",
    "ac23a55547716df29b3e25c98a113399e184d9d1",
    "ea0b21af158016651cb77560778834eb95e6b68d",
    "d03acc756094a332f98167426a39db8faf38f450",
    "5c978a43c3052cc1466b23653c354399186b4e10",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305"
  ],
  "changeHistoryShort": {
    "ae8f55b93243560bd891962d6c64320ddc62a7d7": "Ybodychange",
    "cb672a45a0bbd8950b9b5e304c2e03f516945903": "Ybodychange",
    "e8bdad7385ab63a122c1e8e8a6a73e0f1100e80b": "Ybodychange",
    "33a38a534110de454662256545a7f4c075d328c8": "Ybodychange",
    "5ca6ef0c268b1acb3abf12505b9ead6fe7e38a23": "Ybodychange",
    "5addacb1e301991a8285a221c726f66330cd6d08": "Ybodychange",
    "55c07bbed2f475f7b584a86112ee1b6fe0221e98": "Ybodychange",
    "a8f1c7f542963f66849bcb2a06893c6a99cbe235": "Ybodychange",
    "a2a5d7b5bca715835d92816e7b267b59f7270708": "Ybodychange",
    "864f878d5912c82f3204f1582cfb7eb7c9f1a1da": "Ybodychange",
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ybodychange",
    "164cbe643988f878f0f4100a4de51783e5b6738e": "Ybodychange",
    "e535e0f05b5fbd087c93238deb888cc985254b4c": "Ybodychange",
    "1d37a8812160bb030244a1e6b1c753f962d8d2ed": "Ybodychange",
    "f4c523b69ba55b1fd35e8995c3011a9f546ac835": "Ybodychange",
    "4fdd9abd7e43a0fb7b569982954a8f9660b9268b": "Ybodychange",
    "7e091de1366f4b57b5433bc19d738199dc05313d": "Ybodychange",
    "c12a974ccf5f52f63e4f825d8b4d2385953cd119": "Ybodychange",
    "683332b36de1040eb8901d676e666527e8c5f8fe": "Ybodychange",
    "c17439c2ddd921b63b1635e6f1cba634b8da8557": "Ybodychange",
    "cdc13efb1af54d931585d25c5ba696a012412828": "Ybodychange",
    "4928f5473394981829e5ffd4b16ea0801baf5c45": "Ybodychange",
    "c9103e9cacc67a614940e32fa87c5dbc3daa60de": "Ybodychange",
    "f05c21285ef23b6a973d69f045b1cb46c5abc039": "Ybodychange",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": "Ybodychange",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": "Ybodychange",
    "0653918dad855b394e8e3b8b3f512f474d872ee9": "Ybodychange",
    "bb84f1fccb18c6c7373851e05d2451d55e908242": "Ybodychange",
    "042b33f20b01aadb5cd03da731ae7a3d94026aac": "Ybodychange",
    "6f41baa6233dad92865af23ec6b7a89733c11ddd": "Ybodychange",
    "1e89eba47d0f291b33fc26f9406231fc70b63a87": "Ybodychange",
    "ac23a55547716df29b3e25c98a113399e184d9d1": "Ybodychange",
    "ea0b21af158016651cb77560778834eb95e6b68d": "Ybodychange",
    "d03acc756094a332f98167426a39db8faf38f450": "Ybodychange",
    "5c978a43c3052cc1466b23653c354399186b4e10": "Ybodychange",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": "Yintroduced"
  },
  "changeHistoryDetails": {
    "ae8f55b93243560bd891962d6c64320ddc62a7d7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7859. Erasure Coding: Persist erasure coding policies in NameNode. Contributed by Sammi Chen\n",
      "commitDate": "14/09/17 6:08 PM",
      "commitName": "ae8f55b93243560bd891962d6c64320ddc62a7d7",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "27/04/17 10:18 PM",
      "commitNameOld": "cb672a45a0bbd8950b9b5e304c2e03f516945903",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 139.83,
      "commitsBetweenForRepo": 932,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,71 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n       LoaderContext state \u003d parent.getLoaderContext();\n       boolean isStriped \u003d f.hasErasureCodingPolicyID();\n       assert ((!isStriped) || (isStriped \u0026\u0026 !f.hasReplication()));\n       Short replication \u003d (!isStriped ? (short) f.getReplication() : null);\n+      Byte ecPolicyID \u003d (isStriped ?\n+          (byte) f.getErasureCodingPolicyID() : null);\n       ErasureCodingPolicy ecPolicy \u003d isStriped ?\n-          fsn.getErasureCodingPolicyManager().getByID(\n-              (byte) f.getErasureCodingPolicyID()) : null;\n-      Byte ecPolicyID \u003d (isStriped ? ecPolicy.getId() : null);\n+          fsn.getErasureCodingPolicyManager().getByID(ecPolicyID) : null;\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0; i \u003c bp.size(); ++i) {\n         BlockProto b \u003d bp.get(i);\n         if (isStriped) {\n           Preconditions.checkState(ecPolicy.getId() \u003e 0,\n               \"File with ID \" + n.getId() +\n               \" has an invalid erasure coding policy ID \" + ecPolicy.getId());\n           blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n         } else {\n           blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n               replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, ecPolicyID,\n           f.getPreferredBlockSize(), (byte)f.getStoragePolicyID(), blockType);\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         // update the lease manager\n         fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           final BlockInfo ucBlk;\n           if (isStriped) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n             ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n           } else {\n             ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                 replication);\n           }\n           ucBlk.convertToBlockUnderConstruction(\n               HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n      LoaderContext state \u003d parent.getLoaderContext();\n      boolean isStriped \u003d f.hasErasureCodingPolicyID();\n      assert ((!isStriped) || (isStriped \u0026\u0026 !f.hasReplication()));\n      Short replication \u003d (!isStriped ? (short) f.getReplication() : null);\n      Byte ecPolicyID \u003d (isStriped ?\n          (byte) f.getErasureCodingPolicyID() : null);\n      ErasureCodingPolicy ecPolicy \u003d isStriped ?\n          fsn.getErasureCodingPolicyManager().getByID(ecPolicyID) : null;\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n        BlockProto b \u003d bp.get(i);\n        if (isStriped) {\n          Preconditions.checkState(ecPolicy.getId() \u003e 0,\n              \"File with ID \" + n.getId() +\n              \" has an invalid erasure coding policy ID \" + ecPolicy.getId());\n          blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n        } else {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n              replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, ecPolicyID,\n          f.getPreferredBlockSize(), (byte)f.getStoragePolicyID(), blockType);\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n\n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        // update the lease manager\n        fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (isStriped) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n          } else {\n            ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                replication);\n          }\n          ucBlk.convertToBlockUnderConstruction(\n              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "cb672a45a0bbd8950b9b5e304c2e03f516945903": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11605. Allow user to customize new erasure code policies. Contributed by Huafeng Wang\n",
      "commitDate": "27/04/17 10:18 PM",
      "commitName": "cb672a45a0bbd8950b9b5e304c2e03f516945903",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "07/04/17 4:46 PM",
      "commitNameOld": "e8bdad7385ab63a122c1e8e8a6a73e0f1100e80b",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 20.23,
      "commitsBetweenForRepo": 103,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,71 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n       LoaderContext state \u003d parent.getLoaderContext();\n       boolean isStriped \u003d f.hasErasureCodingPolicyID();\n       assert ((!isStriped) || (isStriped \u0026\u0026 !f.hasReplication()));\n       Short replication \u003d (!isStriped ? (short) f.getReplication() : null);\n       ErasureCodingPolicy ecPolicy \u003d isStriped ?\n-          SystemErasureCodingPolicies.getByID(\n+          fsn.getErasureCodingPolicyManager().getByID(\n               (byte) f.getErasureCodingPolicyID()) : null;\n       Byte ecPolicyID \u003d (isStriped ? ecPolicy.getId() : null);\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0; i \u003c bp.size(); ++i) {\n         BlockProto b \u003d bp.get(i);\n         if (isStriped) {\n           Preconditions.checkState(ecPolicy.getId() \u003e 0,\n               \"File with ID \" + n.getId() +\n               \" has an invalid erasure coding policy ID \" + ecPolicy.getId());\n           blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n         } else {\n           blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n               replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, ecPolicyID,\n           f.getPreferredBlockSize(), (byte)f.getStoragePolicyID(), blockType);\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         // update the lease manager\n         fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           final BlockInfo ucBlk;\n           if (isStriped) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n             ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n           } else {\n             ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                 replication);\n           }\n           ucBlk.convertToBlockUnderConstruction(\n               HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n      LoaderContext state \u003d parent.getLoaderContext();\n      boolean isStriped \u003d f.hasErasureCodingPolicyID();\n      assert ((!isStriped) || (isStriped \u0026\u0026 !f.hasReplication()));\n      Short replication \u003d (!isStriped ? (short) f.getReplication() : null);\n      ErasureCodingPolicy ecPolicy \u003d isStriped ?\n          fsn.getErasureCodingPolicyManager().getByID(\n              (byte) f.getErasureCodingPolicyID()) : null;\n      Byte ecPolicyID \u003d (isStriped ? ecPolicy.getId() : null);\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n        BlockProto b \u003d bp.get(i);\n        if (isStriped) {\n          Preconditions.checkState(ecPolicy.getId() \u003e 0,\n              \"File with ID \" + n.getId() +\n              \" has an invalid erasure coding policy ID \" + ecPolicy.getId());\n          blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n        } else {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n              replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, ecPolicyID,\n          f.getPreferredBlockSize(), (byte)f.getStoragePolicyID(), blockType);\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n\n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        // update the lease manager\n        fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (isStriped) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n          } else {\n            ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                replication);\n          }\n          ucBlk.convertToBlockUnderConstruction(\n              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "e8bdad7385ab63a122c1e8e8a6a73e0f1100e80b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11623. Move system erasure coding policies into hadoop-hdfs-client.\n",
      "commitDate": "07/04/17 4:46 PM",
      "commitName": "e8bdad7385ab63a122c1e8e8a6a73e0f1100e80b",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "08/03/17 4:41 PM",
      "commitNameOld": "33a38a534110de454662256545a7f4c075d328c8",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 29.96,
      "commitsBetweenForRepo": 183,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,71 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n       LoaderContext state \u003d parent.getLoaderContext();\n       boolean isStriped \u003d f.hasErasureCodingPolicyID();\n       assert ((!isStriped) || (isStriped \u0026\u0026 !f.hasReplication()));\n       Short replication \u003d (!isStriped ? (short) f.getReplication() : null);\n       ErasureCodingPolicy ecPolicy \u003d isStriped ?\n-          ErasureCodingPolicyManager.getPolicyByID(\n+          SystemErasureCodingPolicies.getByID(\n               (byte) f.getErasureCodingPolicyID()) : null;\n       Byte ecPolicyID \u003d (isStriped ? ecPolicy.getId() : null);\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0; i \u003c bp.size(); ++i) {\n         BlockProto b \u003d bp.get(i);\n         if (isStriped) {\n           Preconditions.checkState(ecPolicy.getId() \u003e 0,\n               \"File with ID \" + n.getId() +\n               \" has an invalid erasure coding policy ID \" + ecPolicy.getId());\n           blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n         } else {\n           blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n               replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, ecPolicyID,\n           f.getPreferredBlockSize(), (byte)f.getStoragePolicyID(), blockType);\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         // update the lease manager\n         fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           final BlockInfo ucBlk;\n           if (isStriped) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n             ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n           } else {\n             ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                 replication);\n           }\n           ucBlk.convertToBlockUnderConstruction(\n               HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n      LoaderContext state \u003d parent.getLoaderContext();\n      boolean isStriped \u003d f.hasErasureCodingPolicyID();\n      assert ((!isStriped) || (isStriped \u0026\u0026 !f.hasReplication()));\n      Short replication \u003d (!isStriped ? (short) f.getReplication() : null);\n      ErasureCodingPolicy ecPolicy \u003d isStriped ?\n          SystemErasureCodingPolicies.getByID(\n              (byte) f.getErasureCodingPolicyID()) : null;\n      Byte ecPolicyID \u003d (isStriped ? ecPolicy.getId() : null);\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n        BlockProto b \u003d bp.get(i);\n        if (isStriped) {\n          Preconditions.checkState(ecPolicy.getId() \u003e 0,\n              \"File with ID \" + n.getId() +\n              \" has an invalid erasure coding policy ID \" + ecPolicy.getId());\n          blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n        } else {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n              replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, ecPolicyID,\n          f.getPreferredBlockSize(), (byte)f.getStoragePolicyID(), blockType);\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n\n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        // update the lease manager\n        fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (isStriped) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n          } else {\n            ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                replication);\n          }\n          ucBlk.convertToBlockUnderConstruction(\n              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "33a38a534110de454662256545a7f4c075d328c8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11314. Enforce set of enabled EC policies on the NameNode.\n",
      "commitDate": "08/03/17 4:41 PM",
      "commitName": "33a38a534110de454662256545a7f4c075d328c8",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "08/03/17 3:36 PM",
      "commitNameOld": "5ca6ef0c268b1acb3abf12505b9ead6fe7e38a23",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.05,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,71 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n       LoaderContext state \u003d parent.getLoaderContext();\n       boolean isStriped \u003d f.hasErasureCodingPolicyID();\n       assert ((!isStriped) || (isStriped \u0026\u0026 !f.hasReplication()));\n       Short replication \u003d (!isStriped ? (short) f.getReplication() : null);\n       ErasureCodingPolicy ecPolicy \u003d isStriped ?\n-          ErasureCodingPolicyManager.getPolicyByPolicyID(\n+          ErasureCodingPolicyManager.getPolicyByID(\n               (byte) f.getErasureCodingPolicyID()) : null;\n       Byte ecPolicyID \u003d (isStriped ? ecPolicy.getId() : null);\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0; i \u003c bp.size(); ++i) {\n         BlockProto b \u003d bp.get(i);\n         if (isStriped) {\n           Preconditions.checkState(ecPolicy.getId() \u003e 0,\n               \"File with ID \" + n.getId() +\n               \" has an invalid erasure coding policy ID \" + ecPolicy.getId());\n           blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n         } else {\n           blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n               replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, ecPolicyID,\n           f.getPreferredBlockSize(), (byte)f.getStoragePolicyID(), blockType);\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         // update the lease manager\n         fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           final BlockInfo ucBlk;\n           if (isStriped) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n             ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n           } else {\n             ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                 replication);\n           }\n           ucBlk.convertToBlockUnderConstruction(\n               HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n      LoaderContext state \u003d parent.getLoaderContext();\n      boolean isStriped \u003d f.hasErasureCodingPolicyID();\n      assert ((!isStriped) || (isStriped \u0026\u0026 !f.hasReplication()));\n      Short replication \u003d (!isStriped ? (short) f.getReplication() : null);\n      ErasureCodingPolicy ecPolicy \u003d isStriped ?\n          ErasureCodingPolicyManager.getPolicyByID(\n              (byte) f.getErasureCodingPolicyID()) : null;\n      Byte ecPolicyID \u003d (isStriped ? ecPolicy.getId() : null);\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n        BlockProto b \u003d bp.get(i);\n        if (isStriped) {\n          Preconditions.checkState(ecPolicy.getId() \u003e 0,\n              \"File with ID \" + n.getId() +\n              \" has an invalid erasure coding policy ID \" + ecPolicy.getId());\n          blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n        } else {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n              replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, ecPolicyID,\n          f.getPreferredBlockSize(), (byte)f.getStoragePolicyID(), blockType);\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n\n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        // update the lease manager\n        fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (isStriped) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n          } else {\n            ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                replication);\n          }\n          ucBlk.convertToBlockUnderConstruction(\n              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "5ca6ef0c268b1acb3abf12505b9ead6fe7e38a23": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10983. OIV tool should make an EC file explicit. Contributed by Manoj Govindassamy.\n",
      "commitDate": "08/03/17 3:36 PM",
      "commitName": "5ca6ef0c268b1acb3abf12505b9ead6fe7e38a23",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "08/03/17 8:47 AM",
      "commitNameOld": "5addacb1e301991a8285a221c726f66330cd6d08",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.28,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,71 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n       LoaderContext state \u003d parent.getLoaderContext();\n       boolean isStriped \u003d f.hasErasureCodingPolicyID();\n+      assert ((!isStriped) || (isStriped \u0026\u0026 !f.hasReplication()));\n       Short replication \u003d (!isStriped ? (short) f.getReplication() : null);\n       ErasureCodingPolicy ecPolicy \u003d isStriped ?\n           ErasureCodingPolicyManager.getPolicyByPolicyID(\n               (byte) f.getErasureCodingPolicyID()) : null;\n       Byte ecPolicyID \u003d (isStriped ? ecPolicy.getId() : null);\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0; i \u003c bp.size(); ++i) {\n         BlockProto b \u003d bp.get(i);\n         if (isStriped) {\n           Preconditions.checkState(ecPolicy.getId() \u003e 0,\n               \"File with ID \" + n.getId() +\n               \" has an invalid erasure coding policy ID \" + ecPolicy.getId());\n           blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n         } else {\n           blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n               replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, ecPolicyID,\n           f.getPreferredBlockSize(), (byte)f.getStoragePolicyID(), blockType);\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         // update the lease manager\n         fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           final BlockInfo ucBlk;\n           if (isStriped) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n             ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n           } else {\n             ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                 replication);\n           }\n           ucBlk.convertToBlockUnderConstruction(\n               HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n      LoaderContext state \u003d parent.getLoaderContext();\n      boolean isStriped \u003d f.hasErasureCodingPolicyID();\n      assert ((!isStriped) || (isStriped \u0026\u0026 !f.hasReplication()));\n      Short replication \u003d (!isStriped ? (short) f.getReplication() : null);\n      ErasureCodingPolicy ecPolicy \u003d isStriped ?\n          ErasureCodingPolicyManager.getPolicyByPolicyID(\n              (byte) f.getErasureCodingPolicyID()) : null;\n      Byte ecPolicyID \u003d (isStriped ? ecPolicy.getId() : null);\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n        BlockProto b \u003d bp.get(i);\n        if (isStriped) {\n          Preconditions.checkState(ecPolicy.getId() \u003e 0,\n              \"File with ID \" + n.getId() +\n              \" has an invalid erasure coding policy ID \" + ecPolicy.getId());\n          blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n        } else {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n              replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, ecPolicyID,\n          f.getPreferredBlockSize(), (byte)f.getStoragePolicyID(), blockType);\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n\n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        // update the lease manager\n        fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (isStriped) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n          } else {\n            ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                replication);\n          }\n          ucBlk.convertToBlockUnderConstruction(\n              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "5addacb1e301991a8285a221c726f66330cd6d08": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11152. Start erasure coding policy ID number from 1 instead of 0 to void potential unexpected errors. Contributed by SammiChen.\n",
      "commitDate": "08/03/17 8:47 AM",
      "commitName": "5addacb1e301991a8285a221c726f66330cd6d08",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/02/17 5:07 PM",
      "commitNameOld": "55c07bbed2f475f7b584a86112ee1b6fe0221e98",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 8.65,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,70 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n       LoaderContext state \u003d parent.getLoaderContext();\n       boolean isStriped \u003d f.hasErasureCodingPolicyID();\n       Short replication \u003d (!isStriped ? (short) f.getReplication() : null);\n       ErasureCodingPolicy ecPolicy \u003d isStriped ?\n           ErasureCodingPolicyManager.getPolicyByPolicyID(\n               (byte) f.getErasureCodingPolicyID()) : null;\n       Byte ecPolicyID \u003d (isStriped ? ecPolicy.getId() : null);\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0; i \u003c bp.size(); ++i) {\n         BlockProto b \u003d bp.get(i);\n         if (isStriped) {\n+          Preconditions.checkState(ecPolicy.getId() \u003e 0,\n+              \"File with ID \" + n.getId() +\n+              \" has an invalid erasure coding policy ID \" + ecPolicy.getId());\n           blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n         } else {\n           blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n               replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, ecPolicyID,\n           f.getPreferredBlockSize(), (byte)f.getStoragePolicyID(), blockType);\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         // update the lease manager\n         fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           final BlockInfo ucBlk;\n           if (isStriped) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n             ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n           } else {\n             ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                 replication);\n           }\n           ucBlk.convertToBlockUnderConstruction(\n               HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n      LoaderContext state \u003d parent.getLoaderContext();\n      boolean isStriped \u003d f.hasErasureCodingPolicyID();\n      Short replication \u003d (!isStriped ? (short) f.getReplication() : null);\n      ErasureCodingPolicy ecPolicy \u003d isStriped ?\n          ErasureCodingPolicyManager.getPolicyByPolicyID(\n              (byte) f.getErasureCodingPolicyID()) : null;\n      Byte ecPolicyID \u003d (isStriped ? ecPolicy.getId() : null);\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n        BlockProto b \u003d bp.get(i);\n        if (isStriped) {\n          Preconditions.checkState(ecPolicy.getId() \u003e 0,\n              \"File with ID \" + n.getId() +\n              \" has an invalid erasure coding policy ID \" + ecPolicy.getId());\n          blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n        } else {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n              replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, ecPolicyID,\n          f.getPreferredBlockSize(), (byte)f.getStoragePolicyID(), blockType);\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n\n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        // update the lease manager\n        fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (isStriped) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n          } else {\n            ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                replication);\n          }\n          ucBlk.convertToBlockUnderConstruction(\n              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "55c07bbed2f475f7b584a86112ee1b6fe0221e98": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11382. Persist Erasure Coding Policy ID in a new optional field in INodeFile in FSImage. Contributed by Manoj Govindassamy.\n",
      "commitDate": "27/02/17 5:07 PM",
      "commitName": "55c07bbed2f475f7b584a86112ee1b6fe0221e98",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "19/01/17 12:24 AM",
      "commitNameOld": "a8f1c7f542963f66849bcb2a06893c6a99cbe235",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 39.7,
      "commitsBetweenForRepo": 193,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,67 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n-      short replication \u003d (short) f.getReplication();\n       BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n       LoaderContext state \u003d parent.getLoaderContext();\n-      ErasureCodingPolicy ecPolicy \u003d (blockType \u003d\u003d BlockType.STRIPED) ?\n-          ErasureCodingPolicyManager.getPolicyByPolicyID((byte) replication) :\n-          null;\n+      boolean isStriped \u003d f.hasErasureCodingPolicyID();\n+      Short replication \u003d (!isStriped ? (short) f.getReplication() : null);\n+      ErasureCodingPolicy ecPolicy \u003d isStriped ?\n+          ErasureCodingPolicyManager.getPolicyByPolicyID(\n+              (byte) f.getErasureCodingPolicyID()) : null;\n+      Byte ecPolicyID \u003d (isStriped ? ecPolicy.getId() : null);\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0; i \u003c bp.size(); ++i) {\n         BlockProto b \u003d bp.get(i);\n-        if (blockType \u003d\u003d BlockType.STRIPED) {\n+        if (isStriped) {\n           blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n         } else {\n           blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n               replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n-          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n-          (byte)f.getStoragePolicyID(), blockType);\n+          f.getAccessTime(), blocks, replication, ecPolicyID,\n+          f.getPreferredBlockSize(), (byte)f.getStoragePolicyID(), blockType);\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         // update the lease manager\n         fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           final BlockInfo ucBlk;\n-          if (blockType \u003d\u003d BlockType.STRIPED) {\n+          if (isStriped) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n             ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n           } else {\n             ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                 replication);\n           }\n           ucBlk.convertToBlockUnderConstruction(\n               HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n      LoaderContext state \u003d parent.getLoaderContext();\n      boolean isStriped \u003d f.hasErasureCodingPolicyID();\n      Short replication \u003d (!isStriped ? (short) f.getReplication() : null);\n      ErasureCodingPolicy ecPolicy \u003d isStriped ?\n          ErasureCodingPolicyManager.getPolicyByPolicyID(\n              (byte) f.getErasureCodingPolicyID()) : null;\n      Byte ecPolicyID \u003d (isStriped ? ecPolicy.getId() : null);\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n        BlockProto b \u003d bp.get(i);\n        if (isStriped) {\n          blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n        } else {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n              replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, ecPolicyID,\n          f.getPreferredBlockSize(), (byte)f.getStoragePolicyID(), blockType);\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n\n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        // update the lease manager\n        fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (isStriped) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n          } else {\n            ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                replication);\n          }\n          ucBlk.convertToBlockUnderConstruction(\n              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "a8f1c7f542963f66849bcb2a06893c6a99cbe235": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-110268. Correctly reconstruct erasure coding file from FSImage. Contributed by SammiChen.\n",
      "commitDate": "19/01/17 12:24 AM",
      "commitName": "a8f1c7f542963f66849bcb2a06893c6a99cbe235",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "18/01/17 1:31 PM",
      "commitNameOld": "a2a5d7b5bca715835d92816e7b267b59f7270708",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.45,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,65 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n       LoaderContext state \u003d parent.getLoaderContext();\n-      ErasureCodingPolicy ecPolicy \u003d ErasureCodingPolicyManager.getSystemDefaultPolicy();\n+      ErasureCodingPolicy ecPolicy \u003d (blockType \u003d\u003d BlockType.STRIPED) ?\n+          ErasureCodingPolicyManager.getPolicyByPolicyID((byte) replication) :\n+          null;\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0; i \u003c bp.size(); ++i) {\n         BlockProto b \u003d bp.get(i);\n         if (blockType \u003d\u003d BlockType.STRIPED) {\n           blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n         } else {\n           blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n               replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID(), blockType);\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         // update the lease manager\n         fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           final BlockInfo ucBlk;\n           if (blockType \u003d\u003d BlockType.STRIPED) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n             ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n           } else {\n             ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                 replication);\n           }\n           ucBlk.convertToBlockUnderConstruction(\n               HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n      LoaderContext state \u003d parent.getLoaderContext();\n      ErasureCodingPolicy ecPolicy \u003d (blockType \u003d\u003d BlockType.STRIPED) ?\n          ErasureCodingPolicyManager.getPolicyByPolicyID((byte) replication) :\n          null;\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n        BlockProto b \u003d bp.get(i);\n        if (blockType \u003d\u003d BlockType.STRIPED) {\n          blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n        } else {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n              replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID(), blockType);\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n\n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        // update the lease manager\n        fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (blockType \u003d\u003d BlockType.STRIPED) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n          } else {\n            ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                replication);\n          }\n          ucBlk.convertToBlockUnderConstruction(\n              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "a2a5d7b5bca715835d92816e7b267b59f7270708": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10759. Change fsimage bool isStriped from boolean to an enum. Contributed by Ewan Higgs.\n",
      "commitDate": "18/01/17 1:31 PM",
      "commitName": "a2a5d7b5bca715835d92816e7b267b59f7270708",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "15/08/16 3:28 PM",
      "commitNameOld": "864f878d5912c82f3204f1582cfb7eb7c9f1a1da",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 155.96,
      "commitsBetweenForRepo": 1028,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,63 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n-      boolean isStriped \u003d f.getIsStriped();\n+      BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n       LoaderContext state \u003d parent.getLoaderContext();\n       ErasureCodingPolicy ecPolicy \u003d ErasureCodingPolicyManager.getSystemDefaultPolicy();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0; i \u003c bp.size(); ++i) {\n         BlockProto b \u003d bp.get(i);\n-        if (isStriped) {\n+        if (blockType \u003d\u003d BlockType.STRIPED) {\n           blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n         } else {\n           blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n               replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n-          (byte)f.getStoragePolicyID(), isStriped);\n+          (byte)f.getStoragePolicyID(), blockType);\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         // update the lease manager\n         fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           final BlockInfo ucBlk;\n-          if (isStriped) {\n+          if (blockType \u003d\u003d BlockType.STRIPED) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n             ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n           } else {\n             ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                 replication);\n           }\n           ucBlk.convertToBlockUnderConstruction(\n               HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      BlockType blockType \u003d PBHelperClient.convert(f.getBlockType());\n      LoaderContext state \u003d parent.getLoaderContext();\n      ErasureCodingPolicy ecPolicy \u003d ErasureCodingPolicyManager.getSystemDefaultPolicy();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n        BlockProto b \u003d bp.get(i);\n        if (blockType \u003d\u003d BlockType.STRIPED) {\n          blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n        } else {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n              replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID(), blockType);\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n\n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        // update the lease manager\n        fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (blockType \u003d\u003d BlockType.STRIPED) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n          } else {\n            ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                replication);\n          }\n          ucBlk.convertToBlockUnderConstruction(\n              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "864f878d5912c82f3204f1582cfb7eb7c9f1a1da": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10763. Open files can leak permanently due to inconsistent lease update. Contributed by Kihwal Lee.\n",
      "commitDate": "15/08/16 3:28 PM",
      "commitName": "864f878d5912c82f3204f1582cfb7eb7c9f1a1da",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "15/06/16 10:35 PM",
      "commitNameOld": "2449db507d84b1c4fac70a800fb2ad8905cf3db7",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 60.7,
      "commitsBetweenForRepo": 534,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,63 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       boolean isStriped \u003d f.getIsStriped();\n       LoaderContext state \u003d parent.getLoaderContext();\n       ErasureCodingPolicy ecPolicy \u003d ErasureCodingPolicyManager.getSystemDefaultPolicy();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0; i \u003c bp.size(); ++i) {\n         BlockProto b \u003d bp.get(i);\n         if (isStriped) {\n           blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n         } else {\n           blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n               replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID(), isStriped);\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n+        // update the lease manager\n+        fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           final BlockInfo ucBlk;\n           if (isStriped) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n             ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n           } else {\n             ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                 replication);\n           }\n           ucBlk.convertToBlockUnderConstruction(\n               HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      boolean isStriped \u003d f.getIsStriped();\n      LoaderContext state \u003d parent.getLoaderContext();\n      ErasureCodingPolicy ecPolicy \u003d ErasureCodingPolicyManager.getSystemDefaultPolicy();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n        BlockProto b \u003d bp.get(i);\n        if (isStriped) {\n          blocks[i] \u003d new BlockInfoStriped(PBHelperClient.convert(b), ecPolicy);\n        } else {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelperClient.convert(b),\n              replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID(), isStriped);\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n\n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        // update the lease manager\n        fsn.leaseManager.addLease(uc.getClientName(), file.getId());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (isStriped) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n          } else {\n            ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                replication);\n          }\n          ucBlk.convertToBlockUnderConstruction(\n              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "25/08/15 1:16 AM",
      "commitNameOld": "eee0d4563c62647cfaaed6605ee713aaf69add78",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 27.73,
      "commitsBetweenForRepo": 164,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,43 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d\n-            new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n+            new BlockInfoContiguous(PBHelperClient.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           lastBlk.convertToBlockUnderConstruction(\n               HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d\n            new BlockInfoContiguous(PBHelperClient.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          lastBlk.convertToBlockUnderConstruction(\n              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "164cbe643988f878f0f4100a4de51783e5b6738e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8909. Erasure coding: update BlockInfoContiguousUC and BlockInfoStripedUC to use BlockUnderConstructionFeature. Contributed by Jing Zhao.\n",
      "commitDate": "27/08/15 1:02 AM",
      "commitName": "164cbe643988f878f0f4100a4de51783e5b6738e",
      "commitAuthor": "Walter Su",
      "commitDateOld": "24/08/15 12:59 PM",
      "commitNameOld": "6b6a63bbbda920315d3d24b61ed3344a78a981b6",
      "commitAuthorOld": "",
      "daysBetweenCommits": 2.5,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,61 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       boolean isStriped \u003d f.getIsStriped();\n       LoaderContext state \u003d parent.getLoaderContext();\n       ErasureCodingPolicy ecPolicy \u003d ErasureCodingPolicyManager.getSystemDefaultPolicy();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0; i \u003c bp.size(); ++i) {\n         BlockProto b \u003d bp.get(i);\n         if (isStriped) {\n           blocks[i] \u003d new BlockInfoStriped(PBHelper.convert(b), ecPolicy);\n         } else {\n           blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(b),\n               replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID(), isStriped);\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           final BlockInfo ucBlk;\n           if (isStriped) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n-            ucBlk \u003d new BlockInfoStripedUnderConstruction(striped, ecPolicy);\n+            ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n           } else {\n-            ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk,\n+            ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                 replication);\n           }\n+          ucBlk.convertToBlockUnderConstruction(\n+              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      boolean isStriped \u003d f.getIsStriped();\n      LoaderContext state \u003d parent.getLoaderContext();\n      ErasureCodingPolicy ecPolicy \u003d ErasureCodingPolicyManager.getSystemDefaultPolicy();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n        BlockProto b \u003d bp.get(i);\n        if (isStriped) {\n          blocks[i] \u003d new BlockInfoStriped(PBHelper.convert(b), ecPolicy);\n        } else {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(b),\n              replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID(), isStriped);\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n\n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (isStriped) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStriped(striped, ecPolicy);\n          } else {\n            ucBlk \u003d new BlockInfoContiguous(lastBlk,\n                replication);\n          }\n          ucBlk.convertToBlockUnderConstruction(\n              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "e535e0f05b5fbd087c93238deb888cc985254b4c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8801. Convert BlockInfoUnderConstruction as a feature. Contributed by Jing Zhao.\n",
      "commitDate": "17/08/15 11:28 AM",
      "commitName": "e535e0f05b5fbd087c93238deb888cc985254b4c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "06/08/15 10:21 AM",
      "commitNameOld": "f4c523b69ba55b1fd35e8995c3011a9f546ac835",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 11.05,
      "commitsBetweenForRepo": 46,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,43 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d\n             new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n-          // replace the last block of file\n-          file.setBlock(file.numBlocks() - 1, new BlockInfoContiguousUnderConstruction(\n-              lastBlk, replication));\n+          lastBlk.convertToBlockUnderConstruction(\n+              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d\n            new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          lastBlk.convertToBlockUnderConstruction(\n              HdfsServerConstants.BlockUCState.UNDER_CONSTRUCTION, null);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "1d37a8812160bb030244a1e6b1c753f962d8d2ed": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8854. Erasure coding: add ECPolicy to replace schema+cellSize in hadoop-hdfs. Contributed by Walter Su.\n",
      "commitDate": "13/08/15 10:04 AM",
      "commitName": "1d37a8812160bb030244a1e6b1c753f962d8d2ed",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "15/07/15 8:13 PM",
      "commitNameOld": "4fdd9abd7e43a0fb7b569982954a8f9660b9268b",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 28.58,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,59 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       boolean isStriped \u003d f.getIsStriped();\n       LoaderContext state \u003d parent.getLoaderContext();\n-      ECSchema schema \u003d ErasureCodingSchemaManager.getSystemDefaultSchema();\n+      ErasureCodingPolicy ecPolicy \u003d ErasureCodingPolicyManager.getSystemDefaultPolicy();\n \n-      if (isStriped) {\n-        Preconditions.checkState(f.hasStripingCellSize());\n-      }\n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0; i \u003c bp.size(); ++i) {\n         BlockProto b \u003d bp.get(i);\n         if (isStriped) {\n-          blocks[i] \u003d new BlockInfoStriped(PBHelper.convert(b), schema,\n-              (int)f.getStripingCellSize());\n+          blocks[i] \u003d new BlockInfoStriped(PBHelper.convert(b), ecPolicy);\n         } else {\n           blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(b),\n               replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID(), isStriped);\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           final BlockInfo ucBlk;\n           if (isStriped) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n-            ucBlk \u003d new BlockInfoUnderConstructionStriped(striped,\n-                schema, (int)f.getStripingCellSize());\n+            ucBlk \u003d new BlockInfoUnderConstructionStriped(striped, ecPolicy);\n           } else {\n             ucBlk \u003d new BlockInfoUnderConstructionContiguous(lastBlk,\n                 replication);\n           }\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      boolean isStriped \u003d f.getIsStriped();\n      LoaderContext state \u003d parent.getLoaderContext();\n      ErasureCodingPolicy ecPolicy \u003d ErasureCodingPolicyManager.getSystemDefaultPolicy();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n        BlockProto b \u003d bp.get(i);\n        if (isStriped) {\n          blocks[i] \u003d new BlockInfoStriped(PBHelper.convert(b), ecPolicy);\n        } else {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(b),\n              replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID(), isStriped);\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n\n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (isStriped) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoUnderConstructionStriped(striped, ecPolicy);\n          } else {\n            ucBlk \u003d new BlockInfoUnderConstructionContiguous(lastBlk,\n                replication);\n          }\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "f4c523b69ba55b1fd35e8995c3011a9f546ac835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8499. Refactor BlockInfo class hierarchy with static helper class. Contributed by Zhe Zhang.\"\n\nThis reverts commit c17439c2ddd921b63b1635e6f1cba634b8da8557.\n",
      "commitDate": "06/08/15 10:21 AM",
      "commitName": "f4c523b69ba55b1fd35e8995c3011a9f546ac835",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/06/15 11:38 AM",
      "commitNameOld": "c17439c2ddd921b63b1635e6f1cba634b8da8557",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 54.95,
      "commitsBetweenForRepo": 341,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,44 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d\n             new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n-          file.setBlock(file.numBlocks() - 1,\n-              new BlockInfoUnderConstructionContiguous(lastBlk, replication));\n+          file.setBlock(file.numBlocks() - 1, new BlockInfoContiguousUnderConstruction(\n+              lastBlk, replication));\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d\n            new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1, new BlockInfoContiguousUnderConstruction(\n              lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "4fdd9abd7e43a0fb7b569982954a8f9660b9268b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8787. Erasure coding: rename BlockInfoContiguousUC and BlockInfoStripedUC to be consistent with trunk.\n",
      "commitDate": "15/07/15 8:13 PM",
      "commitName": "4fdd9abd7e43a0fb7b569982954a8f9660b9268b",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "15/07/15 9:49 AM",
      "commitNameOld": "7e091de1366f4b57b5433bc19d738199dc05313d",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.43,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,64 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       boolean isStriped \u003d f.getIsStriped();\n       LoaderContext state \u003d parent.getLoaderContext();\n       ECSchema schema \u003d ErasureCodingSchemaManager.getSystemDefaultSchema();\n \n       if (isStriped) {\n         Preconditions.checkState(f.hasStripingCellSize());\n       }\n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0; i \u003c bp.size(); ++i) {\n         BlockProto b \u003d bp.get(i);\n         if (isStriped) {\n           blocks[i] \u003d new BlockInfoStriped(PBHelper.convert(b), schema,\n               (int)f.getStripingCellSize());\n         } else {\n           blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(b),\n               replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID(), isStriped);\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           final BlockInfo ucBlk;\n           if (isStriped) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n-            ucBlk \u003d new BlockInfoStripedUnderConstruction(striped,\n+            ucBlk \u003d new BlockInfoUnderConstructionStriped(striped,\n                 schema, (int)f.getStripingCellSize());\n           } else {\n-            ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk,\n+            ucBlk \u003d new BlockInfoUnderConstructionContiguous(lastBlk,\n                 replication);\n           }\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      boolean isStriped \u003d f.getIsStriped();\n      LoaderContext state \u003d parent.getLoaderContext();\n      ECSchema schema \u003d ErasureCodingSchemaManager.getSystemDefaultSchema();\n\n      if (isStriped) {\n        Preconditions.checkState(f.hasStripingCellSize());\n      }\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n        BlockProto b \u003d bp.get(i);\n        if (isStriped) {\n          blocks[i] \u003d new BlockInfoStriped(PBHelper.convert(b), schema,\n              (int)f.getStripingCellSize());\n        } else {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(b),\n              replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID(), isStriped);\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n\n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (isStriped) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoUnderConstructionStriped(striped,\n                schema, (int)f.getStripingCellSize());\n          } else {\n            ucBlk \u003d new BlockInfoUnderConstructionContiguous(lastBlk,\n                replication);\n          }\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "7e091de1366f4b57b5433bc19d738199dc05313d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8058. Erasure coding: use BlockInfo[] for both striped and contiguous blocks in INodeFile. Contributed by Zhe Zhang and Yi Liu.\n",
      "commitDate": "15/07/15 9:49 AM",
      "commitName": "7e091de1366f4b57b5433bc19d738199dc05313d",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "17/06/15 11:35 AM",
      "commitNameOld": "c12a974ccf5f52f63e4f825d8b4d2385953cd119",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 27.93,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,64 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n+      boolean isStriped \u003d f.getIsStriped();\n       LoaderContext state \u003d parent.getLoaderContext();\n+      ECSchema schema \u003d ErasureCodingSchemaManager.getSystemDefaultSchema();\n \n-      BlockInfoContiguous[] blocks \u003d null;\n-      if (!f.hasStripedBlocks()) {\n-        blocks \u003d new BlockInfoContiguous[bp.size()];\n-        for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n-          blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n+      if (isStriped) {\n+        Preconditions.checkState(f.hasStripingCellSize());\n+      }\n+      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n+      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n+        BlockProto b \u003d bp.get(i);\n+        if (isStriped) {\n+          blocks[i] \u003d new BlockInfoStriped(PBHelper.convert(b), schema,\n+              (int)f.getStripingCellSize());\n+        } else {\n+          blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(b),\n+              replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n-          (byte)f.getStoragePolicyID());\n+          (byte)f.getStoragePolicyID(), isStriped);\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n-      \n+\n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n-      FileWithStripedBlocksFeature stripeFeature \u003d null;\n-      if (f.hasStripedBlocks()) {\n-        // TODO: HDFS-7859\n-        ECSchema schema \u003d ErasureCodingSchemaManager.getSystemDefaultSchema();\n-        stripeFeature \u003d file.addStripedBlocksFeature();\n-        if (bp.size() \u003e 0) {\n-          // if a striped file has block, the cellSize must exist in proto\n-          final int cellSize \u003d f.getStripedBlocks().getCellSize();\n-          for (BlockProto b : bp) {\n-            stripeFeature.addBlock(new BlockInfoStriped(PBHelper.convert(b),\n-                schema, cellSize));\n-          }\n-        }\n-      }\n-\n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n-        BlockInfo lastBlk \u003d file.getLastBlock();\n-        if (lastBlk !\u003d null) {\n+        if (blocks.length \u003e 0) {\n+          BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           final BlockInfo ucBlk;\n-          if (stripeFeature !\u003d null) {\n+          if (isStriped) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n             ucBlk \u003d new BlockInfoStripedUnderConstruction(striped,\n-                striped.getSchema(), striped.getCellSize());\n+                schema, (int)f.getStripingCellSize());\n           } else {\n             ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk,\n                 replication);\n           }\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      boolean isStriped \u003d f.getIsStriped();\n      LoaderContext state \u003d parent.getLoaderContext();\n      ECSchema schema \u003d ErasureCodingSchemaManager.getSystemDefaultSchema();\n\n      if (isStriped) {\n        Preconditions.checkState(f.hasStripingCellSize());\n      }\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0; i \u003c bp.size(); ++i) {\n        BlockProto b \u003d bp.get(i);\n        if (isStriped) {\n          blocks[i] \u003d new BlockInfoStriped(PBHelper.convert(b), schema,\n              (int)f.getStripingCellSize());\n        } else {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(b),\n              replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID(), isStriped);\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n\n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (isStriped) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStripedUnderConstruction(striped,\n                schema, (int)f.getStripingCellSize());\n          } else {\n            ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk,\n                replication);\n          }\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "c12a974ccf5f52f63e4f825d8b4d2385953cd119": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8580. Erasure coding: Persist cellSize in BlockInfoStriped and StripedBlocksFeature. Contributed by Walter Su.\n",
      "commitDate": "17/06/15 11:35 AM",
      "commitName": "c12a974ccf5f52f63e4f825d8b4d2385953cd119",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/06/15 2:48 PM",
      "commitNameOld": "683332b36de1040eb8901d676e666527e8c5f8fe",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 4.87,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,70 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfoContiguous[] blocks \u003d null;\n       if (!f.hasStripedBlocks()) {\n         blocks \u003d new BlockInfoContiguous[bp.size()];\n         for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n           blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n         }\n       }\n \n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       FileWithStripedBlocksFeature stripeFeature \u003d null;\n       if (f.hasStripedBlocks()) {\n         // TODO: HDFS-7859\n         ECSchema schema \u003d ErasureCodingSchemaManager.getSystemDefaultSchema();\n         stripeFeature \u003d file.addStripedBlocksFeature();\n-        for (BlockProto b : bp) {\n-          stripeFeature.addBlock(new BlockInfoStriped(PBHelper.convert(b), schema));\n+        if (bp.size() \u003e 0) {\n+          // if a striped file has block, the cellSize must exist in proto\n+          final int cellSize \u003d f.getStripedBlocks().getCellSize();\n+          for (BlockProto b : bp) {\n+            stripeFeature.addBlock(new BlockInfoStriped(PBHelper.convert(b),\n+                schema, cellSize));\n+          }\n         }\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         BlockInfo lastBlk \u003d file.getLastBlock();\n         if (lastBlk !\u003d null) {\n           // replace the last block of file\n           final BlockInfo ucBlk;\n           if (stripeFeature !\u003d null) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n             ucBlk \u003d new BlockInfoStripedUnderConstruction(striped,\n-                striped.getSchema());\n+                striped.getSchema(), striped.getCellSize());\n           } else {\n             ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk,\n                 replication);\n           }\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfoContiguous[] blocks \u003d null;\n      if (!f.hasStripedBlocks()) {\n        blocks \u003d new BlockInfoContiguous[bp.size()];\n        for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      FileWithStripedBlocksFeature stripeFeature \u003d null;\n      if (f.hasStripedBlocks()) {\n        // TODO: HDFS-7859\n        ECSchema schema \u003d ErasureCodingSchemaManager.getSystemDefaultSchema();\n        stripeFeature \u003d file.addStripedBlocksFeature();\n        if (bp.size() \u003e 0) {\n          // if a striped file has block, the cellSize must exist in proto\n          final int cellSize \u003d f.getStripedBlocks().getCellSize();\n          for (BlockProto b : bp) {\n            stripeFeature.addBlock(new BlockInfoStriped(PBHelper.convert(b),\n                schema, cellSize));\n          }\n        }\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        BlockInfo lastBlk \u003d file.getLastBlock();\n        if (lastBlk !\u003d null) {\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (stripeFeature !\u003d null) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStripedUnderConstruction(striped,\n                striped.getSchema(), striped.getCellSize());\n          } else {\n            ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk,\n                replication);\n          }\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "683332b36de1040eb8901d676e666527e8c5f8fe": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8585. Erasure Coding: Remove dataBlockNum and parityBlockNum from StripedBlockProto. Contributed by Yi Liu.\n",
      "commitDate": "12/06/15 2:48 PM",
      "commitName": "683332b36de1040eb8901d676e666527e8c5f8fe",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 12:02 PM",
      "commitNameOld": "c9103e9cacc67a614940e32fa87c5dbc3daa60de",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 17.12,
      "commitsBetweenForRepo": 42,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,65 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n-      BlockInfoContiguous[] blocks \u003d new BlockInfoContiguous[bp.size()];\n-      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n-        blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n+      BlockInfoContiguous[] blocks \u003d null;\n+      if (!f.hasStripedBlocks()) {\n+        blocks \u003d new BlockInfoContiguous[bp.size()];\n+        for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n+          blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n+        }\n       }\n+\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       FileWithStripedBlocksFeature stripeFeature \u003d null;\n       if (f.hasStripedBlocks()) {\n         // TODO: HDFS-7859\n         ECSchema schema \u003d ErasureCodingSchemaManager.getSystemDefaultSchema();\n-        StripedBlocksFeature sb \u003d f.getStripedBlocks();\n         stripeFeature \u003d file.addStripedBlocksFeature();\n-        for (StripedBlockProto sp : sb.getBlocksList()) {\n-          stripeFeature.addBlock(PBHelper.convert(sp, schema));\n+        for (BlockProto b : bp) {\n+          stripeFeature.addBlock(new BlockInfoStriped(PBHelper.convert(b), schema));\n         }\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         BlockInfo lastBlk \u003d file.getLastBlock();\n         if (lastBlk !\u003d null) {\n           // replace the last block of file\n           final BlockInfo ucBlk;\n           if (stripeFeature !\u003d null) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n             ucBlk \u003d new BlockInfoStripedUnderConstruction(striped,\n                 striped.getSchema());\n           } else {\n             ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk,\n                 replication);\n           }\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfoContiguous[] blocks \u003d null;\n      if (!f.hasStripedBlocks()) {\n        blocks \u003d new BlockInfoContiguous[bp.size()];\n        for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n          blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n        }\n      }\n\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      FileWithStripedBlocksFeature stripeFeature \u003d null;\n      if (f.hasStripedBlocks()) {\n        // TODO: HDFS-7859\n        ECSchema schema \u003d ErasureCodingSchemaManager.getSystemDefaultSchema();\n        stripeFeature \u003d file.addStripedBlocksFeature();\n        for (BlockProto b : bp) {\n          stripeFeature.addBlock(new BlockInfoStriped(PBHelper.convert(b), schema));\n        }\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        BlockInfo lastBlk \u003d file.getLastBlock();\n        if (lastBlk !\u003d null) {\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (stripeFeature !\u003d null) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStripedUnderConstruction(striped,\n                striped.getSchema());\n          } else {\n            ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk,\n                replication);\n          }\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "c17439c2ddd921b63b1635e6f1cba634b8da8557": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8499. Refactor BlockInfo class hierarchy with static helper class. Contributed by Zhe Zhang.\n",
      "commitDate": "12/06/15 11:38 AM",
      "commitName": "c17439c2ddd921b63b1635e6f1cba634b8da8557",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "03/06/15 1:54 AM",
      "commitNameOld": "e965dcec378cb807856372425307598792977604",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 9.41,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,44 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d\n             new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n-          file.setBlock(file.numBlocks() - 1, new BlockInfoContiguousUnderConstruction(\n-              lastBlk, replication));\n+          file.setBlock(file.numBlocks() - 1,\n+              new BlockInfoUnderConstructionContiguous(lastBlk, replication));\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d\n            new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1,\n              new BlockInfoUnderConstructionContiguous(lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "cdc13efb1af54d931585d25c5ba696a012412828": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8489. Subclass BlockInfo to represent contiguous blocks. Contributed by Zhe Zhang.\n",
      "commitDate": "01/06/15 11:42 AM",
      "commitName": "cdc13efb1af54d931585d25c5ba696a012412828",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "27/05/15 3:42 PM",
      "commitNameOld": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.83,
      "commitsBetweenForRepo": 42,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,44 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n-        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n+        blocks[i] \u003d\n+            new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           file.setBlock(file.numBlocks() - 1, new BlockInfoContiguousUnderConstruction(\n               lastBlk, replication));\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d\n            new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1, new BlockInfoContiguousUnderConstruction(\n              lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "4928f5473394981829e5ffd4b16ea0801baf5c45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8482. Rename BlockInfoContiguous to BlockInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "27/05/15 3:42 PM",
      "commitName": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "08/05/15 11:04 PM",
      "commitNameOld": "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 18.69,
      "commitsBetweenForRepo": 146,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,43 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n-      BlockInfoContiguous[] blocks \u003d new BlockInfoContiguous[bp.size()];\n+      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n-        blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n+        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n-          BlockInfoContiguous lastBlk \u003d file.getLastBlock();\n+          BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           file.setBlock(file.numBlocks() - 1, new BlockInfoContiguousUnderConstruction(\n               lastBlk, replication));\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1, new BlockInfoContiguousUnderConstruction(\n              lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "c9103e9cacc67a614940e32fa87c5dbc3daa60de": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8367 BlockInfoStriped uses EC schema. Contributed by Kai Sasaki\n",
      "commitDate": "26/05/15 12:02 PM",
      "commitName": "c9103e9cacc67a614940e32fa87c5dbc3daa60de",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "26/05/15 11:59 AM",
      "commitNameOld": "4c039b0876bb9399c2b4a751ad7b99b36349117b",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 49,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,62 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfoContiguous[] blocks \u003d new BlockInfoContiguous[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       FileWithStripedBlocksFeature stripeFeature \u003d null;\n       if (f.hasStripedBlocks()) {\n+        // TODO: HDFS-7859\n+        ECSchema schema \u003d ErasureCodingSchemaManager.getSystemDefaultSchema();\n         StripedBlocksFeature sb \u003d f.getStripedBlocks();\n         stripeFeature \u003d file.addStripedBlocksFeature();\n         for (StripedBlockProto sp : sb.getBlocksList()) {\n-          stripeFeature.addBlock(PBHelper.convert(sp));\n+          stripeFeature.addBlock(PBHelper.convert(sp, schema));\n         }\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         BlockInfo lastBlk \u003d file.getLastBlock();\n         if (lastBlk !\u003d null) {\n           // replace the last block of file\n           final BlockInfo ucBlk;\n           if (stripeFeature !\u003d null) {\n             BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n             ucBlk \u003d new BlockInfoStripedUnderConstruction(striped,\n-                striped.getDataBlockNum(), striped.getParityBlockNum());\n+                striped.getSchema());\n           } else {\n             ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk,\n                 replication);\n           }\n           file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfoContiguous[] blocks \u003d new BlockInfoContiguous[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      FileWithStripedBlocksFeature stripeFeature \u003d null;\n      if (f.hasStripedBlocks()) {\n        // TODO: HDFS-7859\n        ECSchema schema \u003d ErasureCodingSchemaManager.getSystemDefaultSchema();\n        StripedBlocksFeature sb \u003d f.getStripedBlocks();\n        stripeFeature \u003d file.addStripedBlocksFeature();\n        for (StripedBlockProto sp : sb.getBlocksList()) {\n          stripeFeature.addBlock(PBHelper.convert(sp, schema));\n        }\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        BlockInfo lastBlk \u003d file.getLastBlock();\n        if (lastBlk !\u003d null) {\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (stripeFeature !\u003d null) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStripedUnderConstruction(striped,\n                striped.getSchema());\n          } else {\n            ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk,\n                replication);\n          }\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "f05c21285ef23b6a973d69f045b1cb46c5abc039": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7853. Erasure coding: extend LocatedBlocks to support reading from striped files. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:32 AM",
      "commitName": "f05c21285ef23b6a973d69f045b1cb46c5abc039",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:32 AM",
      "commitNameOld": "1e1e93040748231dc913190aec1e031c379d8271",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,60 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfoContiguous[] blocks \u003d new BlockInfoContiguous[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       FileWithStripedBlocksFeature stripeFeature \u003d null;\n       if (f.hasStripedBlocks()) {\n         StripedBlocksFeature sb \u003d f.getStripedBlocks();\n         stripeFeature \u003d file.addStripedBlocksFeature();\n         for (StripedBlockProto sp : sb.getBlocksList()) {\n           stripeFeature.addBlock(PBHelper.convert(sp));\n         }\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         BlockInfo lastBlk \u003d file.getLastBlock();\n-        // replace the last block of file\n-        final BlockInfo ucBlk;\n-        if (stripeFeature !\u003d null) {\n-          BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n-          ucBlk \u003d new BlockInfoStripedUnderConstruction(striped,\n-              striped.getDataBlockNum(), striped.getParityBlockNum());\n-        } else {\n-          ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk, replication);\n+        if (lastBlk !\u003d null) {\n+          // replace the last block of file\n+          final BlockInfo ucBlk;\n+          if (stripeFeature !\u003d null) {\n+            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n+            ucBlk \u003d new BlockInfoStripedUnderConstruction(striped,\n+                striped.getDataBlockNum(), striped.getParityBlockNum());\n+          } else {\n+            ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk,\n+                replication);\n+          }\n+          file.setBlock(file.numBlocks() - 1, ucBlk);\n         }\n-        file.setBlock(file.numBlocks() - 1, ucBlk);\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfoContiguous[] blocks \u003d new BlockInfoContiguous[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      FileWithStripedBlocksFeature stripeFeature \u003d null;\n      if (f.hasStripedBlocks()) {\n        StripedBlocksFeature sb \u003d f.getStripedBlocks();\n        stripeFeature \u003d file.addStripedBlocksFeature();\n        for (StripedBlockProto sp : sb.getBlocksList()) {\n          stripeFeature.addBlock(PBHelper.convert(sp));\n        }\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        BlockInfo lastBlk \u003d file.getLastBlock();\n        if (lastBlk !\u003d null) {\n          // replace the last block of file\n          final BlockInfo ucBlk;\n          if (stripeFeature !\u003d null) {\n            BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n            ucBlk \u003d new BlockInfoStripedUnderConstruction(striped,\n                striped.getDataBlockNum(), striped.getParityBlockNum());\n          } else {\n            ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk,\n                replication);\n          }\n          file.setBlock(file.numBlocks() - 1, ucBlk);\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7749. Erasure Coding: Add striped block support in INodeFile. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:07 AM",
      "commitName": "9f2f583f401189c3f4a2687795a9e3e0b288322b",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "08/05/15 11:04 PM",
      "commitNameOld": "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 17.5,
      "commitsBetweenForRepo": 144,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,57 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfoContiguous[] blocks \u003d new BlockInfoContiguous[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n+      FileWithStripedBlocksFeature stripeFeature \u003d null;\n+      if (f.hasStripedBlocks()) {\n+        StripedBlocksFeature sb \u003d f.getStripedBlocks();\n+        stripeFeature \u003d file.addStripedBlocksFeature();\n+        for (StripedBlockProto sp : sb.getBlocksList()) {\n+          stripeFeature.addBlock(PBHelper.convert(sp));\n+        }\n+      }\n+\n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n-        if (blocks.length \u003e 0) {\n-          BlockInfoContiguous lastBlk \u003d file.getLastBlock();\n-          // replace the last block of file\n-          file.setBlock(file.numBlocks() - 1, new BlockInfoContiguousUnderConstruction(\n-              lastBlk, replication));\n+        BlockInfo lastBlk \u003d file.getLastBlock();\n+        // replace the last block of file\n+        final BlockInfo ucBlk;\n+        if (stripeFeature !\u003d null) {\n+          BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n+          ucBlk \u003d new BlockInfoStripedUnderConstruction(striped,\n+              striped.getDataBlockNum(), striped.getParityBlockNum());\n+        } else {\n+          ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk, replication);\n         }\n+        file.setBlock(file.numBlocks() - 1, ucBlk);\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfoContiguous[] blocks \u003d new BlockInfoContiguous[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      FileWithStripedBlocksFeature stripeFeature \u003d null;\n      if (f.hasStripedBlocks()) {\n        StripedBlocksFeature sb \u003d f.getStripedBlocks();\n        stripeFeature \u003d file.addStripedBlocksFeature();\n        for (StripedBlockProto sp : sb.getBlocksList()) {\n          stripeFeature.addBlock(PBHelper.convert(sp));\n        }\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        BlockInfo lastBlk \u003d file.getLastBlock();\n        // replace the last block of file\n        final BlockInfo ucBlk;\n        if (stripeFeature !\u003d null) {\n          BlockInfoStriped striped \u003d (BlockInfoStriped) lastBlk;\n          ucBlk \u003d new BlockInfoStripedUnderConstruction(striped,\n              striped.getDataBlockNum(), striped.getParityBlockNum());\n        } else {\n          ucBlk \u003d new BlockInfoContiguousUnderConstruction(lastBlk, replication);\n        }\n        file.setBlock(file.numBlocks() - 1, ucBlk);\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7743. Code cleanup of BlockInfo and rename BlockInfo to BlockInfoContiguous. Contributed by Jing Zhao.\n",
      "commitDate": "08/02/15 11:51 AM",
      "commitName": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "04/12/14 8:49 PM",
      "commitNameOld": "0653918dad855b394e8e3b8b3f512f474d872ee9",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 65.63,
      "commitsBetweenForRepo": 437,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,43 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n-      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n+      BlockInfoContiguous[] blocks \u003d new BlockInfoContiguous[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n-        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n+        blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n         int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n             f.getAcl(), state.getStringTable()));\n         file.addAclFeature(new AclFeature(entries));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n-          BlockInfo lastBlk \u003d file.getLastBlock();\n+          BlockInfoContiguous lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n-          file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n+          file.setBlock(file.numBlocks() - 1, new BlockInfoContiguousUnderConstruction(\n               lastBlk, replication));\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfoContiguous[] blocks \u003d new BlockInfoContiguous[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfoContiguous(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfoContiguous lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1, new BlockInfoContiguousUnderConstruction(\n              lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "0653918dad855b394e8e3b8b3f512f474d872ee9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7454. Reduce memory footprint for AclEntries in NameNode. Contributed by Vinayakumar B.\n",
      "commitDate": "04/12/14 8:49 PM",
      "commitName": "0653918dad855b394e8e3b8b3f512f474d872ee9",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/12/14 2:53 PM",
      "commitNameOld": "185e0c7b4c056b88f606362c71e4a22aae7076e0",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 2.25,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,43 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n           (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n-        file.addAclFeature(new AclFeature(loadAclEntries(f.getAcl(),\n-            state.getStringTable())));\n+        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n+            f.getAcl(), state.getStringTable()));\n+        file.addAclFeature(new AclFeature(entries));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n               lastBlk, replication));\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        int[] entries \u003d AclEntryStatusFormat.toInt(loadAclEntries(\n            f.getAcl(), state.getStringTable()));\n        file.addAclFeature(new AclFeature(entries));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n              lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "bb84f1fccb18c6c7373851e05d2451d55e908242": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7159. Use block storage policy to set lazy persist preference. (Arpit Agarwal)\n",
      "commitDate": "29/09/14 10:27 PM",
      "commitName": "bb84f1fccb18c6c7373851e05d2451d55e908242",
      "commitAuthor": "arp",
      "commitDateOld": "18/09/14 10:26 PM",
      "commitNameOld": "f8bbf80067ac03400acae4655615c9808c538ca8",
      "commitAuthorOld": "",
      "daysBetweenCommits": 11.0,
      "commitsBetweenForRepo": 118,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,42 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n-          f.hasIsLazyPersist() ? f.getIsLazyPersist() : false,\n           (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n         file.addAclFeature(new AclFeature(loadAclEntries(f.getAcl(),\n             state.getStringTable())));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n               lastBlk, replication));\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        file.addAclFeature(new AclFeature(loadAclEntries(f.getAcl(),\n            state.getStringTable())));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n              lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "042b33f20b01aadb5cd03da731ae7a3d94026aac": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6922. Add LazyPersist flag to INodeFile, save it in FsImage and edit logs. (Arpit Agarwal)\n",
      "commitDate": "27/08/14 9:47 PM",
      "commitName": "042b33f20b01aadb5cd03da731ae7a3d94026aac",
      "commitAuthor": "arp",
      "commitDateOld": "22/06/14 12:39 AM",
      "commitNameOld": "1e89eba47d0f291b33fc26f9406231fc70b63a87",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 66.88,
      "commitsBetweenForRepo": 533,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,42 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n-          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize());\n+          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n+          f.hasIsLazyPersist() ? f.getIsLazyPersist() : false);\n \n       if (f.hasAcl()) {\n         file.addAclFeature(new AclFeature(loadAclEntries(f.getAcl(),\n             state.getStringTable())));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n               lastBlk, replication));\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          f.hasIsLazyPersist() ? f.getIsLazyPersist() : false);\n\n      if (f.hasAcl()) {\n        file.addAclFeature(new AclFeature(loadAclEntries(f.getAcl(),\n            state.getStringTable())));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n              lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "6f41baa6233dad92865af23ec6b7a89733c11ddd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6677. Change INodeFile and FSImage to support storage policy ID.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-6584@1610525 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/07/14 1:57 PM",
      "commitName": "6f41baa6233dad92865af23ec6b7a89733c11ddd",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "22/06/14 12:39 AM",
      "commitNameOld": "1e89eba47d0f291b33fc26f9406231fc70b63a87",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 22.55,
      "commitsBetweenForRepo": 129,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,42 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n-          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize());\n+          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n+          (byte)f.getStoragePolicyID());\n \n       if (f.hasAcl()) {\n         file.addAclFeature(new AclFeature(loadAclEntries(f.getAcl(),\n             state.getStringTable())));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n               lastBlk, replication));\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize(),\n          (byte)f.getStoragePolicyID());\n\n      if (f.hasAcl()) {\n        file.addAclFeature(new AclFeature(loadAclEntries(f.getAcl(),\n            state.getStringTable())));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n              lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "1e89eba47d0f291b33fc26f9406231fc70b63a87": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6583. Remove clientNode in FileUnderConstructionFeature. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1604541 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/06/14 12:39 AM",
      "commitName": "1e89eba47d0f291b33fc26f9406231fc70b63a87",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "18/06/14 10:17 PM",
      "commitNameOld": "3f82484218d5694e62ddcb23376d0e4e332aa8b8",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 3.1,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,41 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize());\n \n       if (f.hasAcl()) {\n         file.addAclFeature(new AclFeature(loadAclEntries(f.getAcl(),\n             state.getStringTable())));\n       }\n       \n       if (f.hasXAttrs()) {\n         file.addXAttrFeature(new XAttrFeature(\n             loadXAttrs(f.getXAttrs(), state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n-        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine(),\n-            null);\n+        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n               lastBlk, replication));\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize());\n\n      if (f.hasAcl()) {\n        file.addAclFeature(new AclFeature(loadAclEntries(f.getAcl(),\n            state.getStringTable())));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine());\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n              lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "ac23a55547716df29b3e25c98a113399e184d9d1": {
      "type": "Ybodychange",
      "commitMessage": "Merge HDFS-2006 HDFS XAttrs branch to Trunk\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1596575 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/05/14 6:57 AM",
      "commitName": "ac23a55547716df29b3e25c98a113399e184d9d1",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "20/02/14 8:09 AM",
      "commitNameOld": "b23f6cc1f2dd779a683bd452b5ca014848a9b782",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 89.91,
      "commitsBetweenForRepo": 636,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,42 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize());\n \n       if (f.hasAcl()) {\n         file.addAclFeature(new AclFeature(loadAclEntries(f.getAcl(),\n             state.getStringTable())));\n       }\n+      \n+      if (f.hasXAttrs()) {\n+        file.addXAttrFeature(new XAttrFeature(\n+            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n+      }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine(),\n             null);\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n               lastBlk, replication));\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize());\n\n      if (f.hasAcl()) {\n        file.addAclFeature(new AclFeature(loadAclEntries(f.getAcl(),\n            state.getStringTable())));\n      }\n      \n      if (f.hasXAttrs()) {\n        file.addXAttrFeature(new XAttrFeature(\n            loadXAttrs(f.getXAttrs(), state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine(),\n            null);\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n              lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "ea0b21af158016651cb77560778834eb95e6b68d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5933. Optimize the FSImage layout for ACLs. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4685@1567785 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 2:56 PM",
      "commitName": "ea0b21af158016651cb77560778834eb95e6b68d",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "10/02/14 10:25 PM",
      "commitNameOld": "d03acc756094a332f98167426a39db8faf38f450",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 1.69,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,37 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n       LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize());\n \n-      if (f.hasAclId()) {\n-        file.addAclFeature(new AclFeature(loadAclEntries(f.getAclId(),\n-            state.getExtendedAclTable())));\n+      if (f.hasAcl()) {\n+        file.addAclFeature(new AclFeature(loadAclEntries(f.getAcl(),\n+            state.getStringTable())));\n       }\n \n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine(),\n             null);\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n               lastBlk, replication));\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize());\n\n      if (f.hasAcl()) {\n        file.addAclFeature(new AclFeature(loadAclEntries(f.getAcl(),\n            state.getStringTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine(),\n            null);\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n              lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "d03acc756094a332f98167426a39db8faf38f450": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5914. Incorporate ACLs with the changes from HDFS-5698. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4685@1566991 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/02/14 10:25 PM",
      "commitName": "d03acc756094a332f98167426a39db8faf38f450",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "10/02/14 3:13 PM",
      "commitNameOld": "5c978a43c3052cc1466b23653c354399186b4e10",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 0.3,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,37 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n+      LoaderContext state \u003d parent.getLoaderContext();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n           parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize());\n+\n+      if (f.hasAclId()) {\n+        file.addAclFeature(new AclFeature(loadAclEntries(f.getAclId(),\n+            state.getExtendedAclTable())));\n+      }\n+\n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine(),\n             null);\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n               lastBlk, replication));\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n      LoaderContext state \u003d parent.getLoaderContext();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize());\n\n      if (f.hasAclId()) {\n        file.addAclFeature(new AclFeature(loadAclEntries(f.getAclId(),\n            state.getExtendedAclTable())));\n      }\n\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine(),\n            null);\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n              lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "5c978a43c3052cc1466b23653c354399186b4e10": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5915. Refactor FSImageFormatProtobuf to simplify cross section reads. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566824 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/02/14 3:13 PM",
      "commitName": "5c978a43c3052cc1466b23653c354399186b4e10",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "09/02/14 11:18 AM",
      "commitNameOld": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.16,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,30 @@\n     private INodeFile loadINodeFile(INodeSection.INode n) {\n       assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n       INodeSection.INodeFile f \u003d n.getFile();\n       List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n       short replication \u003d (short) f.getReplication();\n \n       BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n       for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n         blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n       }\n       final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n-          parent.getStringTable());\n+          parent.getLoaderContext().getStringTable());\n \n       final INodeFile file \u003d new INodeFile(n.getId(),\n           n.getName().toByteArray(), permissions, f.getModificationTime(),\n           f.getAccessTime(), blocks, replication, f.getPreferredBlockSize());\n       // under-construction information\n       if (f.hasFileUC()) {\n         INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n         file.toUnderConstruction(uc.getClientName(), uc.getClientMachine(),\n             null);\n         if (blocks.length \u003e 0) {\n           BlockInfo lastBlk \u003d file.getLastBlock();\n           // replace the last block of file\n           file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n               lastBlk, replication));\n         }\n       }\n       return file;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getLoaderContext().getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize());\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine(),\n            null);\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n              lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
      "extendedDetails": {}
    },
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5698. Use protobuf to serialize / deserialize FSImage. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/02/14 11:18 AM",
      "commitName": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,30 @@\n+    private INodeFile loadINodeFile(INodeSection.INode n) {\n+      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n+      INodeSection.INodeFile f \u003d n.getFile();\n+      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n+      short replication \u003d (short) f.getReplication();\n+\n+      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n+      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n+        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n+      }\n+      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n+          parent.getStringTable());\n+\n+      final INodeFile file \u003d new INodeFile(n.getId(),\n+          n.getName().toByteArray(), permissions, f.getModificationTime(),\n+          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize());\n+      // under-construction information\n+      if (f.hasFileUC()) {\n+        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n+        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine(),\n+            null);\n+        if (blocks.length \u003e 0) {\n+          BlockInfo lastBlk \u003d file.getLastBlock();\n+          // replace the last block of file\n+          file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n+              lastBlk, replication));\n+        }\n+      }\n+      return file;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private INodeFile loadINodeFile(INodeSection.INode n) {\n      assert n.getType() \u003d\u003d INodeSection.INode.Type.FILE;\n      INodeSection.INodeFile f \u003d n.getFile();\n      List\u003cBlockProto\u003e bp \u003d f.getBlocksList();\n      short replication \u003d (short) f.getReplication();\n\n      BlockInfo[] blocks \u003d new BlockInfo[bp.size()];\n      for (int i \u003d 0, e \u003d bp.size(); i \u003c e; ++i) {\n        blocks[i] \u003d new BlockInfo(PBHelper.convert(bp.get(i)), replication);\n      }\n      final PermissionStatus permissions \u003d loadPermission(f.getPermission(),\n          parent.getStringTable());\n\n      final INodeFile file \u003d new INodeFile(n.getId(),\n          n.getName().toByteArray(), permissions, f.getModificationTime(),\n          f.getAccessTime(), blocks, replication, f.getPreferredBlockSize());\n      // under-construction information\n      if (f.hasFileUC()) {\n        INodeSection.FileUnderConstructionFeature uc \u003d f.getFileUC();\n        file.toUnderConstruction(uc.getClientName(), uc.getClientMachine(),\n            null);\n        if (blocks.length \u003e 0) {\n          BlockInfo lastBlk \u003d file.getLastBlock();\n          // replace the last block of file\n          file.setBlock(file.numBlocks() - 1, new BlockInfoUnderConstruction(\n              lastBlk, replication));\n        }\n      }\n      return file;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java"
    }
  }
}