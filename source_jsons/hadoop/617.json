{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockReaderFactory.java",
  "functionName": "nextDomainPeer",
  "functionId": "nextDomainPeer",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
  "functionStartLine": 795,
  "functionEndLine": 807,
  "numCommitsSeen": 63,
  "timeTaken": 3482,
  "changeHistory": [
    "f308561f1d885491b88db73ac63003202056d661",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be"
  ],
  "changeHistoryShort": {
    "f308561f1d885491b88db73ac63003202056d661": "Yfilerename",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ybodychange",
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f": "Ybodychange",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f308561f1d885491b88db73ac63003202056d661": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8057 Move BlockReader implementation to the client implementation package.  Contributed by Takanobu Asanuma\n",
      "commitDate": "25/04/16 12:01 PM",
      "commitName": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "25/04/16 9:38 AM",
      "commitNameOld": "10f0f7851a3255caab775777e8fb6c2781d97062",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private BlockReaderPeer nextDomainPeer() {\n    if (remainingCacheTries \u003e 0) {\n      Peer peer \u003d clientContext.getPeerCache().get(datanode, true);\n      if (peer !\u003d null) {\n        LOG.trace(\"nextDomainPeer: reusing existing peer {}\", peer);\n        return new BlockReaderPeer(peer, true);\n      }\n    }\n    DomainSocket sock \u003d clientContext.getDomainSocketFactory().\n        createSocket(pathInfo, conf.getSocketTimeout());\n    if (sock \u003d\u003d null) return null;\n    return new BlockReaderPeer(new DomainPeer(sock), false);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderFactory.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java"
      }
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,13 @@\n   private BlockReaderPeer nextDomainPeer() {\n     if (remainingCacheTries \u003e 0) {\n       Peer peer \u003d clientContext.getPeerCache().get(datanode, true);\n       if (peer !\u003d null) {\n-        if (LOG.isTraceEnabled()) {\n-          LOG.trace(\"nextDomainPeer: reusing existing peer \" + peer);\n-        }\n+        LOG.trace(\"nextDomainPeer: reusing existing peer {}\", peer);\n         return new BlockReaderPeer(peer, true);\n       }\n     }\n     DomainSocket sock \u003d clientContext.getDomainSocketFactory().\n         createSocket(pathInfo, conf.getSocketTimeout());\n     if (sock \u003d\u003d null) return null;\n     return new BlockReaderPeer(new DomainPeer(sock), false);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockReaderPeer nextDomainPeer() {\n    if (remainingCacheTries \u003e 0) {\n      Peer peer \u003d clientContext.getPeerCache().get(datanode, true);\n      if (peer !\u003d null) {\n        LOG.trace(\"nextDomainPeer: reusing existing peer {}\", peer);\n        return new BlockReaderPeer(peer, true);\n      }\n    }\n    DomainSocket sock \u003d clientContext.getDomainSocketFactory().\n        createSocket(pathInfo, conf.getSocketTimeout());\n    if (sock \u003d\u003d null) return null;\n    return new BlockReaderPeer(new DomainPeer(sock), false);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderFactory.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,15 @@\n   private BlockReaderPeer nextDomainPeer() {\n     if (remainingCacheTries \u003e 0) {\n       Peer peer \u003d clientContext.getPeerCache().get(datanode, true);\n       if (peer !\u003d null) {\n-        LOG.trace(\"nextDomainPeer: reusing existing peer {}\", peer);\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(\"nextDomainPeer: reusing existing peer \" + peer);\n+        }\n         return new BlockReaderPeer(peer, true);\n       }\n     }\n     DomainSocket sock \u003d clientContext.getDomainSocketFactory().\n         createSocket(pathInfo, conf.getSocketTimeout());\n     if (sock \u003d\u003d null) return null;\n     return new BlockReaderPeer(new DomainPeer(sock), false);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockReaderPeer nextDomainPeer() {\n    if (remainingCacheTries \u003e 0) {\n      Peer peer \u003d clientContext.getPeerCache().get(datanode, true);\n      if (peer !\u003d null) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"nextDomainPeer: reusing existing peer \" + peer);\n        }\n        return new BlockReaderPeer(peer, true);\n      }\n    }\n    DomainSocket sock \u003d clientContext.getDomainSocketFactory().\n        createSocket(pathInfo, conf.getSocketTimeout());\n    if (sock \u003d\u003d null) return null;\n    return new BlockReaderPeer(new DomainPeer(sock), false);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderFactory.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 1:39 PM",
      "commitNameOld": "e5992ef4df63fbc6a6b8e357b32c647e7837c662",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 1.17,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,13 @@\n   private BlockReaderPeer nextDomainPeer() {\n     if (remainingCacheTries \u003e 0) {\n       Peer peer \u003d clientContext.getPeerCache().get(datanode, true);\n       if (peer !\u003d null) {\n-        if (LOG.isTraceEnabled()) {\n-          LOG.trace(\"nextDomainPeer: reusing existing peer \" + peer);\n-        }\n+        LOG.trace(\"nextDomainPeer: reusing existing peer {}\", peer);\n         return new BlockReaderPeer(peer, true);\n       }\n     }\n     DomainSocket sock \u003d clientContext.getDomainSocketFactory().\n         createSocket(pathInfo, conf.getSocketTimeout());\n     if (sock \u003d\u003d null) return null;\n     return new BlockReaderPeer(new DomainPeer(sock), false);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockReaderPeer nextDomainPeer() {\n    if (remainingCacheTries \u003e 0) {\n      Peer peer \u003d clientContext.getPeerCache().get(datanode, true);\n      if (peer !\u003d null) {\n        LOG.trace(\"nextDomainPeer: reusing existing peer {}\", peer);\n        return new BlockReaderPeer(peer, true);\n      }\n    }\n    DomainSocket sock \u003d clientContext.getDomainSocketFactory().\n        createSocket(pathInfo, conf.getSocketTimeout());\n    if (sock \u003d\u003d null) return null;\n    return new BlockReaderPeer(new DomainPeer(sock), false);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderFactory.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private BlockReaderPeer nextDomainPeer() {\n    if (remainingCacheTries \u003e 0) {\n      Peer peer \u003d clientContext.getPeerCache().get(datanode, true);\n      if (peer !\u003d null) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"nextDomainPeer: reusing existing peer \" + peer);\n        }\n        return new BlockReaderPeer(peer, true);\n      }\n    }\n    DomainSocket sock \u003d clientContext.getDomainSocketFactory().\n        createSocket(pathInfo, conf.getSocketTimeout());\n    if (sock \u003d\u003d null) return null;\n    return new BlockReaderPeer(new DomainPeer(sock), false);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderFactory.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderFactory.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderFactory.java"
      }
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "01/04/15 12:54 PM",
      "commitNameOld": "ed72daa5df97669906234e8ac9a406d78136b206",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 9.08,
      "commitsBetweenForRepo": 88,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   private BlockReaderPeer nextDomainPeer() {\n     if (remainingCacheTries \u003e 0) {\n       Peer peer \u003d clientContext.getPeerCache().get(datanode, true);\n       if (peer !\u003d null) {\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(\"nextDomainPeer: reusing existing peer \" + peer);\n         }\n         return new BlockReaderPeer(peer, true);\n       }\n     }\n     DomainSocket sock \u003d clientContext.getDomainSocketFactory().\n-        createSocket(pathInfo, conf.socketTimeout);\n+        createSocket(pathInfo, conf.getSocketTimeout());\n     if (sock \u003d\u003d null) return null;\n     return new BlockReaderPeer(new DomainPeer(sock), false);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockReaderPeer nextDomainPeer() {\n    if (remainingCacheTries \u003e 0) {\n      Peer peer \u003d clientContext.getPeerCache().get(datanode, true);\n      if (peer !\u003d null) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"nextDomainPeer: reusing existing peer \" + peer);\n        }\n        return new BlockReaderPeer(peer, true);\n      }\n    }\n    DomainSocket sock \u003d clientContext.getDomainSocketFactory().\n        createSocket(pathInfo, conf.getSocketTimeout());\n    if (sock \u003d\u003d null) return null;\n    return new BlockReaderPeer(new DomainPeer(sock), false);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderFactory.java",
      "extendedDetails": {}
    },
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5950. The DFSClient and DataNode should use shared memory segments to communicate short-circuit information (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1573433 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/03/14 7:58 PM",
      "commitName": "dd049a2f6097da189ccce2f5890a2b9bc77fa73f",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "12/02/14 7:10 PM",
      "commitNameOld": "f0d64a078da7e932b9509734f75170e3e525e68c",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 18.03,
      "commitsBetweenForRepo": 129,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,15 @@\n   private BlockReaderPeer nextDomainPeer() {\n     if (remainingCacheTries \u003e 0) {\n       Peer peer \u003d clientContext.getPeerCache().get(datanode, true);\n       if (peer !\u003d null) {\n-        remainingCacheTries--;\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(\"nextDomainPeer: reusing existing peer \" + peer);\n         }\n         return new BlockReaderPeer(peer, true);\n       }\n     }\n     DomainSocket sock \u003d clientContext.getDomainSocketFactory().\n         createSocket(pathInfo, conf.socketTimeout);\n     if (sock \u003d\u003d null) return null;\n     return new BlockReaderPeer(new DomainPeer(sock), false);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockReaderPeer nextDomainPeer() {\n    if (remainingCacheTries \u003e 0) {\n      Peer peer \u003d clientContext.getPeerCache().get(datanode, true);\n      if (peer !\u003d null) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"nextDomainPeer: reusing existing peer \" + peer);\n        }\n        return new BlockReaderPeer(peer, true);\n      }\n    }\n    DomainSocket sock \u003d clientContext.getDomainSocketFactory().\n        createSocket(pathInfo, conf.socketTimeout);\n    if (sock \u003d\u003d null) return null;\n    return new BlockReaderPeer(new DomainPeer(sock), false);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderFactory.java",
      "extendedDetails": {}
    },
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5810. Unify mmap cache and short-circuit file descriptor cache (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567720 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 11:08 AM",
      "commitName": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
      "commitAuthor": "Colin McCabe",
      "diff": "@@ -0,0 +1,16 @@\n+  private BlockReaderPeer nextDomainPeer() {\n+    if (remainingCacheTries \u003e 0) {\n+      Peer peer \u003d clientContext.getPeerCache().get(datanode, true);\n+      if (peer !\u003d null) {\n+        remainingCacheTries--;\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(\"nextDomainPeer: reusing existing peer \" + peer);\n+        }\n+        return new BlockReaderPeer(peer, true);\n+      }\n+    }\n+    DomainSocket sock \u003d clientContext.getDomainSocketFactory().\n+        createSocket(pathInfo, conf.socketTimeout);\n+    if (sock \u003d\u003d null) return null;\n+    return new BlockReaderPeer(new DomainPeer(sock), false);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockReaderPeer nextDomainPeer() {\n    if (remainingCacheTries \u003e 0) {\n      Peer peer \u003d clientContext.getPeerCache().get(datanode, true);\n      if (peer !\u003d null) {\n        remainingCacheTries--;\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"nextDomainPeer: reusing existing peer \" + peer);\n        }\n        return new BlockReaderPeer(peer, true);\n      }\n    }\n    DomainSocket sock \u003d clientContext.getDomainSocketFactory().\n        createSocket(pathInfo, conf.socketTimeout);\n    if (sock \u003d\u003d null) return null;\n    return new BlockReaderPeer(new DomainPeer(sock), false);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderFactory.java"
    }
  }
}