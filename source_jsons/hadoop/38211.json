{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ProportionalCapacityPreemptionPolicy.java",
  "functionName": "containerBasedPreemptOrKill",
  "functionId": "containerBasedPreemptOrKill___root-CSQueue__clusterResources-Resource",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
  "functionStartLine": 433,
  "functionEndLine": 536,
  "numCommitsSeen": 49,
  "timeTaken": 7348,
  "changeHistory": [
    "987d8191ad409298570f7ef981e9bc8fb72ff16c",
    "291194302cc1a875d6d94ea93cf1184a3f1fc2cc",
    "ce832059db077fa95922198b066a737ed4f609fe",
    "bb62e0592566b2fcae7136b30972aad2d3ac55b0",
    "7cb3a3da96e59fc9b6528644dae5fb0ac1e44eac",
    "60e4116bf1d00afed91010e57357fe54057e4e39",
    "ae14e5d07f1b6702a5160637438028bb03d9387e",
    "fa7a43529d529f0006c8033c2003f15b9b93f103",
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
    "a44ce3f14fd940601f984fbf7980aa6fdc8f23b7",
    "150f5ae0343e872ee8bef39c57008c1389f0ba9e",
    "3fe57285635e8058c34aa40a103845b49ca7d6ff",
    "adf260a728df427eb729abe8fb9ad7248991ea54",
    "805a9ed85eb34c8125cfb7d26d07cdfac12b3579",
    "3bba1800513b38a4827f7552f348db87dc47c783",
    "0e4b06690ff51fbde3ab26f68fde8aeb32af69af",
    "d497f6ea2be559aa31ed76f37ae949dbfabe2a51",
    "18741adf97f4fda5f8743318b59c440928e51297",
    "4b130821995a3cfe20c71e38e0f63294085c0491",
    "c6e29a9f069f71bd77fcff2111def4a60676b4ba",
    "85f0efb68f9d1d9ee3466e3939c4fc2f985ccf61"
  ],
  "changeHistoryShort": {
    "987d8191ad409298570f7ef981e9bc8fb72ff16c": "Ybodychange",
    "291194302cc1a875d6d94ea93cf1184a3f1fc2cc": "Ybodychange",
    "ce832059db077fa95922198b066a737ed4f609fe": "Ybodychange",
    "bb62e0592566b2fcae7136b30972aad2d3ac55b0": "Ybodychange",
    "7cb3a3da96e59fc9b6528644dae5fb0ac1e44eac": "Ybodychange",
    "60e4116bf1d00afed91010e57357fe54057e4e39": "Ybodychange",
    "ae14e5d07f1b6702a5160637438028bb03d9387e": "Ybodychange",
    "fa7a43529d529f0006c8033c2003f15b9b93f103": "Ybodychange",
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913": "Ybodychange",
    "a44ce3f14fd940601f984fbf7980aa6fdc8f23b7": "Ybodychange",
    "150f5ae0343e872ee8bef39c57008c1389f0ba9e": "Ybodychange",
    "3fe57285635e8058c34aa40a103845b49ca7d6ff": "Ybodychange",
    "adf260a728df427eb729abe8fb9ad7248991ea54": "Ybodychange",
    "805a9ed85eb34c8125cfb7d26d07cdfac12b3579": "Ybodychange",
    "3bba1800513b38a4827f7552f348db87dc47c783": "Ybodychange",
    "0e4b06690ff51fbde3ab26f68fde8aeb32af69af": "Ybodychange",
    "d497f6ea2be559aa31ed76f37ae949dbfabe2a51": "Ybodychange",
    "18741adf97f4fda5f8743318b59c440928e51297": "Ybodychange",
    "4b130821995a3cfe20c71e38e0f63294085c0491": "Ybodychange",
    "c6e29a9f069f71bd77fcff2111def4a60676b4ba": "Ybodychange",
    "85f0efb68f9d1d9ee3466e3939c4fc2f985ccf61": "Yintroduced"
  },
  "changeHistoryDetails": {
    "987d8191ad409298570f7ef981e9bc8fb72ff16c": {
      "type": "Ybodychange",
      "commitMessage": "YARN-8709: CS preemption monitor always fails since one under-served queue was deleted. Contributed by Tao Yang.\n",
      "commitDate": "10/09/18 12:55 PM",
      "commitName": "987d8191ad409298570f7ef981e9bc8fb72ff16c",
      "commitAuthor": "Eric E Payne",
      "commitDateOld": "28/06/18 10:23 AM",
      "commitNameOld": "291194302cc1a875d6d94ea93cf1184a3f1fc2cc",
      "commitAuthorOld": "Sunil G",
      "daysBetweenCommits": 74.11,
      "commitsBetweenForRepo": 525,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,104 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // Sync killable containers from scheduler when lazy preemption enabled\n     if (lazyPreempionEnabled) {\n       syncKillableContainersFromScheduler();\n     }\n \n     // All partitions to look at\n     Set\u003cString\u003e partitions \u003d new HashSet\u003c\u003e();\n     partitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     partitions.add(RMNodeLabelsManager.NO_LABEL);\n     this.allPartitions \u003d ImmutableSet.copyOf(partitions);\n \n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root, Resources\n                 .clone(nlm.getResourceByLabel(partitionToLookAt, clusterResources)),\n             partitionToLookAt);\n       }\n \n       // Update effective priority of queues\n     }\n \n     this.leafQueueNames \u003d ImmutableSet.copyOf(getLeafQueueNames(\n         getQueueByPartition(CapacitySchedulerConfiguration.ROOT,\n             RMNodeLabelsManager.NO_LABEL)));\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n+    //clear under served queues for every run\n+    partitionToUnderServedQueues.clear();\n+\n     // based on ideal allocation select containers to be preemptionCandidates from each\n     // queue and each application\n     Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         new HashMap\u003c\u003e();\n     Map\u003cPreemptionCandidatesSelector, Map\u003cApplicationAttemptId,\n         Set\u003cRMContainer\u003e\u003e\u003e toPreemptPerSelector \u003d  new HashMap\u003c\u003e();;\n     for (PreemptionCandidatesSelector selector :\n         candidatesSelectionPolicies) {\n       long startTime \u003d 0;\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(MessageFormat\n             .format(\"Trying to use {0} to select preemption candidates\",\n                 selector.getClass().getName()));\n         startTime \u003d clock.getTime();\n       }\n       Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e curCandidates \u003d\n           selector.selectCandidates(toPreempt, clusterResources,\n               totalPreemptionAllowed);\n       toPreemptPerSelector.putIfAbsent(selector, curCandidates);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(MessageFormat\n             .format(\"{0} uses {1} millisecond to run\",\n                 selector.getClass().getName(), clock.getTime() - startTime));\n         int totalSelected \u003d 0;\n         int curSelected \u003d 0;\n         for (Set\u003cRMContainer\u003e set : toPreempt.values()) {\n           totalSelected +\u003d set.size();\n         }\n         for (Set\u003cRMContainer\u003e set : curCandidates.values()) {\n           curSelected +\u003d set.size();\n         }\n         LOG.debug(MessageFormat\n             .format(\"So far, total {0} containers selected to be preempted, {1}\"\n                     + \" containers selected this round\\n\",\n                 totalSelected, curSelected));\n       }\n     }\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // TODO: need consider revert killable containers when no more demandings.\n     // Since we could have several selectors to make decisions concurrently.\n     // So computed ideal-allocation varies between different selectors.\n     //\n     // We may need to \"score\" killable containers and revert the most preferred\n     // containers. The bottom line is, we shouldn\u0027t preempt a queue which is already\n     // below its guaranteed resource.\n \n     long currentTime \u003d clock.getTime();\n \n     pcsMap \u003d toPreemptPerSelector;\n \n     // preempt (or kill) the selected containers\n     preemptOrkillSelectedContainerAfterWait(toPreemptPerSelector, currentTime);\n \n     // cleanup staled preemption candidates\n     cleanupStaledPreemptionCandidates(currentTime);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // Sync killable containers from scheduler when lazy preemption enabled\n    if (lazyPreempionEnabled) {\n      syncKillableContainersFromScheduler();\n    }\n\n    // All partitions to look at\n    Set\u003cString\u003e partitions \u003d new HashSet\u003c\u003e();\n    partitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    partitions.add(RMNodeLabelsManager.NO_LABEL);\n    this.allPartitions \u003d ImmutableSet.copyOf(partitions);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root, Resources\n                .clone(nlm.getResourceByLabel(partitionToLookAt, clusterResources)),\n            partitionToLookAt);\n      }\n\n      // Update effective priority of queues\n    }\n\n    this.leafQueueNames \u003d ImmutableSet.copyOf(getLeafQueueNames(\n        getQueueByPartition(CapacitySchedulerConfiguration.ROOT,\n            RMNodeLabelsManager.NO_LABEL)));\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    //clear under served queues for every run\n    partitionToUnderServedQueues.clear();\n\n    // based on ideal allocation select containers to be preemptionCandidates from each\n    // queue and each application\n    Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        new HashMap\u003c\u003e();\n    Map\u003cPreemptionCandidatesSelector, Map\u003cApplicationAttemptId,\n        Set\u003cRMContainer\u003e\u003e\u003e toPreemptPerSelector \u003d  new HashMap\u003c\u003e();;\n    for (PreemptionCandidatesSelector selector :\n        candidatesSelectionPolicies) {\n      long startTime \u003d 0;\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(MessageFormat\n            .format(\"Trying to use {0} to select preemption candidates\",\n                selector.getClass().getName()));\n        startTime \u003d clock.getTime();\n      }\n      Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e curCandidates \u003d\n          selector.selectCandidates(toPreempt, clusterResources,\n              totalPreemptionAllowed);\n      toPreemptPerSelector.putIfAbsent(selector, curCandidates);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(MessageFormat\n            .format(\"{0} uses {1} millisecond to run\",\n                selector.getClass().getName(), clock.getTime() - startTime));\n        int totalSelected \u003d 0;\n        int curSelected \u003d 0;\n        for (Set\u003cRMContainer\u003e set : toPreempt.values()) {\n          totalSelected +\u003d set.size();\n        }\n        for (Set\u003cRMContainer\u003e set : curCandidates.values()) {\n          curSelected +\u003d set.size();\n        }\n        LOG.debug(MessageFormat\n            .format(\"So far, total {0} containers selected to be preempted, {1}\"\n                    + \" containers selected this round\\n\",\n                totalSelected, curSelected));\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // TODO: need consider revert killable containers when no more demandings.\n    // Since we could have several selectors to make decisions concurrently.\n    // So computed ideal-allocation varies between different selectors.\n    //\n    // We may need to \"score\" killable containers and revert the most preferred\n    // containers. The bottom line is, we shouldn\u0027t preempt a queue which is already\n    // below its guaranteed resource.\n\n    long currentTime \u003d clock.getTime();\n\n    pcsMap \u003d toPreemptPerSelector;\n\n    // preempt (or kill) the selected containers\n    preemptOrkillSelectedContainerAfterWait(toPreemptPerSelector, currentTime);\n\n    // cleanup staled preemption candidates\n    cleanupStaledPreemptionCandidates(currentTime);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "291194302cc1a875d6d94ea93cf1184a3f1fc2cc": {
      "type": "Ybodychange",
      "commitMessage": "YARN-8379. Improve balancing resources in already satisfied queues by using Capacity Scheduler preemption. Contributed by Zian Chen.\n",
      "commitDate": "28/06/18 10:23 AM",
      "commitName": "291194302cc1a875d6d94ea93cf1184a3f1fc2cc",
      "commitAuthor": "Sunil G",
      "commitDateOld": "12/06/18 8:35 AM",
      "commitNameOld": "652bcbb3e4950758e61ce123ecc1798ae2b60a7f",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 16.07,
      "commitsBetweenForRepo": 93,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,101 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // Sync killable containers from scheduler when lazy preemption enabled\n     if (lazyPreempionEnabled) {\n       syncKillableContainersFromScheduler();\n     }\n \n     // All partitions to look at\n     Set\u003cString\u003e partitions \u003d new HashSet\u003c\u003e();\n     partitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     partitions.add(RMNodeLabelsManager.NO_LABEL);\n     this.allPartitions \u003d ImmutableSet.copyOf(partitions);\n \n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root, Resources\n                 .clone(nlm.getResourceByLabel(partitionToLookAt, clusterResources)),\n             partitionToLookAt);\n       }\n \n       // Update effective priority of queues\n     }\n \n     this.leafQueueNames \u003d ImmutableSet.copyOf(getLeafQueueNames(\n         getQueueByPartition(CapacitySchedulerConfiguration.ROOT,\n             RMNodeLabelsManager.NO_LABEL)));\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n     // based on ideal allocation select containers to be preemptionCandidates from each\n     // queue and each application\n     Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         new HashMap\u003c\u003e();\n+    Map\u003cPreemptionCandidatesSelector, Map\u003cApplicationAttemptId,\n+        Set\u003cRMContainer\u003e\u003e\u003e toPreemptPerSelector \u003d  new HashMap\u003c\u003e();;\n     for (PreemptionCandidatesSelector selector :\n         candidatesSelectionPolicies) {\n       long startTime \u003d 0;\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(MessageFormat\n             .format(\"Trying to use {0} to select preemption candidates\",\n                 selector.getClass().getName()));\n         startTime \u003d clock.getTime();\n       }\n-      toPreempt \u003d selector.selectCandidates(toPreempt,\n-          clusterResources, totalPreemptionAllowed);\n+      Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e curCandidates \u003d\n+          selector.selectCandidates(toPreempt, clusterResources,\n+              totalPreemptionAllowed);\n+      toPreemptPerSelector.putIfAbsent(selector, curCandidates);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(MessageFormat\n             .format(\"{0} uses {1} millisecond to run\",\n                 selector.getClass().getName(), clock.getTime() - startTime));\n         int totalSelected \u003d 0;\n+        int curSelected \u003d 0;\n         for (Set\u003cRMContainer\u003e set : toPreempt.values()) {\n           totalSelected +\u003d set.size();\n         }\n+        for (Set\u003cRMContainer\u003e set : curCandidates.values()) {\n+          curSelected +\u003d set.size();\n+        }\n         LOG.debug(MessageFormat\n-            .format(\"So far, total {0} containers selected to be preempted\",\n-                totalSelected));\n+            .format(\"So far, total {0} containers selected to be preempted, {1}\"\n+                    + \" containers selected this round\\n\",\n+                totalSelected, curSelected));\n       }\n     }\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // TODO: need consider revert killable containers when no more demandings.\n     // Since we could have several selectors to make decisions concurrently.\n     // So computed ideal-allocation varies between different selectors.\n     //\n     // We may need to \"score\" killable containers and revert the most preferred\n     // containers. The bottom line is, we shouldn\u0027t preempt a queue which is already\n     // below its guaranteed resource.\n \n     long currentTime \u003d clock.getTime();\n \n+    pcsMap \u003d toPreemptPerSelector;\n+\n     // preempt (or kill) the selected containers\n-    preemptOrkillSelectedContainerAfterWait(toPreempt, currentTime);\n+    preemptOrkillSelectedContainerAfterWait(toPreemptPerSelector, currentTime);\n \n     // cleanup staled preemption candidates\n     cleanupStaledPreemptionCandidates(currentTime);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // Sync killable containers from scheduler when lazy preemption enabled\n    if (lazyPreempionEnabled) {\n      syncKillableContainersFromScheduler();\n    }\n\n    // All partitions to look at\n    Set\u003cString\u003e partitions \u003d new HashSet\u003c\u003e();\n    partitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    partitions.add(RMNodeLabelsManager.NO_LABEL);\n    this.allPartitions \u003d ImmutableSet.copyOf(partitions);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root, Resources\n                .clone(nlm.getResourceByLabel(partitionToLookAt, clusterResources)),\n            partitionToLookAt);\n      }\n\n      // Update effective priority of queues\n    }\n\n    this.leafQueueNames \u003d ImmutableSet.copyOf(getLeafQueueNames(\n        getQueueByPartition(CapacitySchedulerConfiguration.ROOT,\n            RMNodeLabelsManager.NO_LABEL)));\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    // based on ideal allocation select containers to be preemptionCandidates from each\n    // queue and each application\n    Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        new HashMap\u003c\u003e();\n    Map\u003cPreemptionCandidatesSelector, Map\u003cApplicationAttemptId,\n        Set\u003cRMContainer\u003e\u003e\u003e toPreemptPerSelector \u003d  new HashMap\u003c\u003e();;\n    for (PreemptionCandidatesSelector selector :\n        candidatesSelectionPolicies) {\n      long startTime \u003d 0;\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(MessageFormat\n            .format(\"Trying to use {0} to select preemption candidates\",\n                selector.getClass().getName()));\n        startTime \u003d clock.getTime();\n      }\n      Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e curCandidates \u003d\n          selector.selectCandidates(toPreempt, clusterResources,\n              totalPreemptionAllowed);\n      toPreemptPerSelector.putIfAbsent(selector, curCandidates);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(MessageFormat\n            .format(\"{0} uses {1} millisecond to run\",\n                selector.getClass().getName(), clock.getTime() - startTime));\n        int totalSelected \u003d 0;\n        int curSelected \u003d 0;\n        for (Set\u003cRMContainer\u003e set : toPreempt.values()) {\n          totalSelected +\u003d set.size();\n        }\n        for (Set\u003cRMContainer\u003e set : curCandidates.values()) {\n          curSelected +\u003d set.size();\n        }\n        LOG.debug(MessageFormat\n            .format(\"So far, total {0} containers selected to be preempted, {1}\"\n                    + \" containers selected this round\\n\",\n                totalSelected, curSelected));\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // TODO: need consider revert killable containers when no more demandings.\n    // Since we could have several selectors to make decisions concurrently.\n    // So computed ideal-allocation varies between different selectors.\n    //\n    // We may need to \"score\" killable containers and revert the most preferred\n    // containers. The bottom line is, we shouldn\u0027t preempt a queue which is already\n    // below its guaranteed resource.\n\n    long currentTime \u003d clock.getTime();\n\n    pcsMap \u003d toPreemptPerSelector;\n\n    // preempt (or kill) the selected containers\n    preemptOrkillSelectedContainerAfterWait(toPreemptPerSelector, currentTime);\n\n    // cleanup staled preemption candidates\n    cleanupStaledPreemptionCandidates(currentTime);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "ce832059db077fa95922198b066a737ed4f609fe": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5864. Capacity Scheduler - Queue Priorities. (wangda)\n",
      "commitDate": "23/01/17 10:52 AM",
      "commitName": "ce832059db077fa95922198b066a737ed4f609fe",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "11/11/16 3:16 PM",
      "commitNameOld": "fad9609d13e76e9e3a4e01c96f698bb60b03807e",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 72.82,
      "commitsBetweenForRepo": 374,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,73 +1,90 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // Sync killable containers from scheduler when lazy preemption enabled\n     if (lazyPreempionEnabled) {\n       syncKillableContainersFromScheduler();\n     }\n \n     // All partitions to look at\n     Set\u003cString\u003e partitions \u003d new HashSet\u003c\u003e();\n     partitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     partitions.add(RMNodeLabelsManager.NO_LABEL);\n     this.allPartitions \u003d ImmutableSet.copyOf(partitions);\n \n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root, Resources\n                 .clone(nlm.getResourceByLabel(partitionToLookAt, clusterResources)),\n             partitionToLookAt);\n       }\n+\n+      // Update effective priority of queues\n     }\n \n     this.leafQueueNames \u003d ImmutableSet.copyOf(getLeafQueueNames(\n         getQueueByPartition(CapacitySchedulerConfiguration.ROOT,\n             RMNodeLabelsManager.NO_LABEL)));\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n     // based on ideal allocation select containers to be preemptionCandidates from each\n     // queue and each application\n     Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         new HashMap\u003c\u003e();\n     for (PreemptionCandidatesSelector selector :\n         candidatesSelectionPolicies) {\n+      long startTime \u003d 0;\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(MessageFormat\n             .format(\"Trying to use {0} to select preemption candidates\",\n                 selector.getClass().getName()));\n+        startTime \u003d clock.getTime();\n       }\n       toPreempt \u003d selector.selectCandidates(toPreempt,\n           clusterResources, totalPreemptionAllowed);\n+\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(MessageFormat\n+            .format(\"{0} uses {1} millisecond to run\",\n+                selector.getClass().getName(), clock.getTime() - startTime));\n+        int totalSelected \u003d 0;\n+        for (Set\u003cRMContainer\u003e set : toPreempt.values()) {\n+          totalSelected +\u003d set.size();\n+        }\n+        LOG.debug(MessageFormat\n+            .format(\"So far, total {0} containers selected to be preempted\",\n+                totalSelected));\n+      }\n     }\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // TODO: need consider revert killable containers when no more demandings.\n     // Since we could have several selectors to make decisions concurrently.\n     // So computed ideal-allocation varies between different selectors.\n     //\n     // We may need to \"score\" killable containers and revert the most preferred\n     // containers. The bottom line is, we shouldn\u0027t preempt a queue which is already\n     // below its guaranteed resource.\n \n     long currentTime \u003d clock.getTime();\n \n     // preempt (or kill) the selected containers\n     preemptOrkillSelectedContainerAfterWait(toPreempt, currentTime);\n \n     // cleanup staled preemption candidates\n     cleanupStaledPreemptionCandidates(currentTime);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // Sync killable containers from scheduler when lazy preemption enabled\n    if (lazyPreempionEnabled) {\n      syncKillableContainersFromScheduler();\n    }\n\n    // All partitions to look at\n    Set\u003cString\u003e partitions \u003d new HashSet\u003c\u003e();\n    partitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    partitions.add(RMNodeLabelsManager.NO_LABEL);\n    this.allPartitions \u003d ImmutableSet.copyOf(partitions);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root, Resources\n                .clone(nlm.getResourceByLabel(partitionToLookAt, clusterResources)),\n            partitionToLookAt);\n      }\n\n      // Update effective priority of queues\n    }\n\n    this.leafQueueNames \u003d ImmutableSet.copyOf(getLeafQueueNames(\n        getQueueByPartition(CapacitySchedulerConfiguration.ROOT,\n            RMNodeLabelsManager.NO_LABEL)));\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    // based on ideal allocation select containers to be preemptionCandidates from each\n    // queue and each application\n    Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        new HashMap\u003c\u003e();\n    for (PreemptionCandidatesSelector selector :\n        candidatesSelectionPolicies) {\n      long startTime \u003d 0;\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(MessageFormat\n            .format(\"Trying to use {0} to select preemption candidates\",\n                selector.getClass().getName()));\n        startTime \u003d clock.getTime();\n      }\n      toPreempt \u003d selector.selectCandidates(toPreempt,\n          clusterResources, totalPreemptionAllowed);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(MessageFormat\n            .format(\"{0} uses {1} millisecond to run\",\n                selector.getClass().getName(), clock.getTime() - startTime));\n        int totalSelected \u003d 0;\n        for (Set\u003cRMContainer\u003e set : toPreempt.values()) {\n          totalSelected +\u003d set.size();\n        }\n        LOG.debug(MessageFormat\n            .format(\"So far, total {0} containers selected to be preempted\",\n                totalSelected));\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // TODO: need consider revert killable containers when no more demandings.\n    // Since we could have several selectors to make decisions concurrently.\n    // So computed ideal-allocation varies between different selectors.\n    //\n    // We may need to \"score\" killable containers and revert the most preferred\n    // containers. The bottom line is, we shouldn\u0027t preempt a queue which is already\n    // below its guaranteed resource.\n\n    long currentTime \u003d clock.getTime();\n\n    // preempt (or kill) the selected containers\n    preemptOrkillSelectedContainerAfterWait(toPreempt, currentTime);\n\n    // cleanup staled preemption candidates\n    cleanupStaledPreemptionCandidates(currentTime);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "bb62e0592566b2fcae7136b30972aad2d3ac55b0": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4390. Do surgical preemption based on reserved container in CapacityScheduler. Contributed by Wangda Tan\n",
      "commitDate": "05/05/16 12:56 PM",
      "commitName": "bb62e0592566b2fcae7136b30972aad2d3ac55b0",
      "commitAuthor": "Jian He",
      "commitDateOld": "22/04/16 11:40 AM",
      "commitNameOld": "7cb3a3da96e59fc9b6528644dae5fb0ac1e44eac",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 13.05,
      "commitsBetweenForRepo": 72,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,73 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // Sync killable containers from scheduler when lazy preemption enabled\n     if (lazyPreempionEnabled) {\n       syncKillableContainersFromScheduler();\n     }\n \n     // All partitions to look at\n     Set\u003cString\u003e partitions \u003d new HashSet\u003c\u003e();\n     partitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     partitions.add(RMNodeLabelsManager.NO_LABEL);\n     this.allPartitions \u003d ImmutableSet.copyOf(partitions);\n \n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n-        cloneQueues(root,\n-            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n+        cloneQueues(root, Resources\n+                .clone(nlm.getResourceByLabel(partitionToLookAt, clusterResources)),\n             partitionToLookAt);\n       }\n     }\n \n     this.leafQueueNames \u003d ImmutableSet.copyOf(getLeafQueueNames(\n         getQueueByPartition(CapacitySchedulerConfiguration.ROOT,\n             RMNodeLabelsManager.NO_LABEL)));\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n     // based on ideal allocation select containers to be preemptionCandidates from each\n     // queue and each application\n-    Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e toPreempt \u003d null;\n+    Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n+        new HashMap\u003c\u003e();\n     for (PreemptionCandidatesSelector selector :\n         candidatesSelectionPolicies) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(MessageFormat\n+            .format(\"Trying to use {0} to select preemption candidates\",\n+                selector.getClass().getName()));\n+      }\n       toPreempt \u003d selector.selectCandidates(toPreempt,\n           clusterResources, totalPreemptionAllowed);\n     }\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // TODO: need consider revert killable containers when no more demandings.\n     // Since we could have several selectors to make decisions concurrently.\n     // So computed ideal-allocation varies between different selectors.\n     //\n     // We may need to \"score\" killable containers and revert the most preferred\n     // containers. The bottom line is, we shouldn\u0027t preempt a queue which is already\n     // below its guaranteed resource.\n \n     long currentTime \u003d clock.getTime();\n \n     // preempt (or kill) the selected containers\n     preemptOrkillSelectedContainerAfterWait(toPreempt, currentTime);\n \n     // cleanup staled preemption candidates\n     cleanupStaledPreemptionCandidates(currentTime);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // Sync killable containers from scheduler when lazy preemption enabled\n    if (lazyPreempionEnabled) {\n      syncKillableContainersFromScheduler();\n    }\n\n    // All partitions to look at\n    Set\u003cString\u003e partitions \u003d new HashSet\u003c\u003e();\n    partitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    partitions.add(RMNodeLabelsManager.NO_LABEL);\n    this.allPartitions \u003d ImmutableSet.copyOf(partitions);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root, Resources\n                .clone(nlm.getResourceByLabel(partitionToLookAt, clusterResources)),\n            partitionToLookAt);\n      }\n    }\n\n    this.leafQueueNames \u003d ImmutableSet.copyOf(getLeafQueueNames(\n        getQueueByPartition(CapacitySchedulerConfiguration.ROOT,\n            RMNodeLabelsManager.NO_LABEL)));\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    // based on ideal allocation select containers to be preemptionCandidates from each\n    // queue and each application\n    Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        new HashMap\u003c\u003e();\n    for (PreemptionCandidatesSelector selector :\n        candidatesSelectionPolicies) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(MessageFormat\n            .format(\"Trying to use {0} to select preemption candidates\",\n                selector.getClass().getName()));\n      }\n      toPreempt \u003d selector.selectCandidates(toPreempt,\n          clusterResources, totalPreemptionAllowed);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // TODO: need consider revert killable containers when no more demandings.\n    // Since we could have several selectors to make decisions concurrently.\n    // So computed ideal-allocation varies between different selectors.\n    //\n    // We may need to \"score\" killable containers and revert the most preferred\n    // containers. The bottom line is, we shouldn\u0027t preempt a queue which is already\n    // below its guaranteed resource.\n\n    long currentTime \u003d clock.getTime();\n\n    // preempt (or kill) the selected containers\n    preemptOrkillSelectedContainerAfterWait(toPreempt, currentTime);\n\n    // cleanup staled preemption candidates\n    cleanupStaledPreemptionCandidates(currentTime);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "7cb3a3da96e59fc9b6528644dae5fb0ac1e44eac": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4846. Fix random failures for TestCapacitySchedulerPreemption#testPreemptionPolicyShouldRespectAlreadyMarkedKillableContainers. (Bibin A Chundatt via wangda)\n",
      "commitDate": "22/04/16 11:40 AM",
      "commitName": "7cb3a3da96e59fc9b6528644dae5fb0ac1e44eac",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "30/03/16 12:43 PM",
      "commitNameOld": "60e4116bf1d00afed91010e57357fe54057e4e39",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 22.96,
      "commitsBetweenForRepo": 153,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,67 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // Sync killable containers from scheduler when lazy preemption enabled\n     if (lazyPreempionEnabled) {\n       syncKillableContainersFromScheduler();\n     }\n \n     // All partitions to look at\n     Set\u003cString\u003e partitions \u003d new HashSet\u003c\u003e();\n     partitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     partitions.add(RMNodeLabelsManager.NO_LABEL);\n     this.allPartitions \u003d ImmutableSet.copyOf(partitions);\n \n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root,\n             nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n             partitionToLookAt);\n       }\n     }\n \n     this.leafQueueNames \u003d ImmutableSet.copyOf(getLeafQueueNames(\n         getQueueByPartition(CapacitySchedulerConfiguration.ROOT,\n             RMNodeLabelsManager.NO_LABEL)));\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n     // based on ideal allocation select containers to be preemptionCandidates from each\n     // queue and each application\n     Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e toPreempt \u003d null;\n     for (PreemptionCandidatesSelector selector :\n         candidatesSelectionPolicies) {\n       toPreempt \u003d selector.selectCandidates(toPreempt,\n           clusterResources, totalPreemptionAllowed);\n     }\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // TODO: need consider revert killable containers when no more demandings.\n     // Since we could have several selectors to make decisions concurrently.\n     // So computed ideal-allocation varies between different selectors.\n     //\n     // We may need to \"score\" killable containers and revert the most preferred\n     // containers. The bottom line is, we shouldn\u0027t preempt a queue which is already\n     // below its guaranteed resource.\n \n+    long currentTime \u003d clock.getTime();\n+\n     // preempt (or kill) the selected containers\n-    preemptOrkillSelectedContainerAfterWait(toPreempt);\n+    preemptOrkillSelectedContainerAfterWait(toPreempt, currentTime);\n \n     // cleanup staled preemption candidates\n-    cleanupStaledPreemptionCandidates();\n+    cleanupStaledPreemptionCandidates(currentTime);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // Sync killable containers from scheduler when lazy preemption enabled\n    if (lazyPreempionEnabled) {\n      syncKillableContainersFromScheduler();\n    }\n\n    // All partitions to look at\n    Set\u003cString\u003e partitions \u003d new HashSet\u003c\u003e();\n    partitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    partitions.add(RMNodeLabelsManager.NO_LABEL);\n    this.allPartitions \u003d ImmutableSet.copyOf(partitions);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root,\n            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n            partitionToLookAt);\n      }\n    }\n\n    this.leafQueueNames \u003d ImmutableSet.copyOf(getLeafQueueNames(\n        getQueueByPartition(CapacitySchedulerConfiguration.ROOT,\n            RMNodeLabelsManager.NO_LABEL)));\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    // based on ideal allocation select containers to be preemptionCandidates from each\n    // queue and each application\n    Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e toPreempt \u003d null;\n    for (PreemptionCandidatesSelector selector :\n        candidatesSelectionPolicies) {\n      toPreempt \u003d selector.selectCandidates(toPreempt,\n          clusterResources, totalPreemptionAllowed);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // TODO: need consider revert killable containers when no more demandings.\n    // Since we could have several selectors to make decisions concurrently.\n    // So computed ideal-allocation varies between different selectors.\n    //\n    // We may need to \"score\" killable containers and revert the most preferred\n    // containers. The bottom line is, we shouldn\u0027t preempt a queue which is already\n    // below its guaranteed resource.\n\n    long currentTime \u003d clock.getTime();\n\n    // preempt (or kill) the selected containers\n    preemptOrkillSelectedContainerAfterWait(toPreempt, currentTime);\n\n    // cleanup staled preemption candidates\n    cleanupStaledPreemptionCandidates(currentTime);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "60e4116bf1d00afed91010e57357fe54057e4e39": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4822. Refactor existing Preemption Policy of CS for easier adding new approach to select preemption candidates. Contributed by Wangda Tan\n",
      "commitDate": "30/03/16 12:43 PM",
      "commitName": "60e4116bf1d00afed91010e57357fe54057e4e39",
      "commitAuthor": "Jian He",
      "commitDateOld": "16/03/16 5:02 PM",
      "commitNameOld": "ae14e5d07f1b6702a5160637438028bb03d9387e",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 13.82,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,96 +1,65 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n-    // All partitions to look at\n-    Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n-    allPartitions.addAll(scheduler.getRMContext()\n-        .getNodeLabelManager().getClusterNodeLabelNames());\n-    allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n+    // Sync killable containers from scheduler when lazy preemption enabled\n+    if (lazyPreempionEnabled) {\n+      syncKillableContainersFromScheduler();\n+    }\n \n-    syncKillableContainersFromScheduler();\n+    // All partitions to look at\n+    Set\u003cString\u003e partitions \u003d new HashSet\u003c\u003e();\n+    partitions.addAll(scheduler.getRMContext()\n+        .getNodeLabelManager().getClusterNodeLabelNames());\n+    partitions.add(RMNodeLabelsManager.NO_LABEL);\n+    this.allPartitions \u003d ImmutableSet.copyOf(partitions);\n \n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root,\n             nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n             partitionToLookAt);\n       }\n     }\n \n+    this.leafQueueNames \u003d ImmutableSet.copyOf(getLeafQueueNames(\n+        getQueueByPartition(CapacitySchedulerConfiguration.ROOT,\n+            RMNodeLabelsManager.NO_LABEL)));\n+\n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n-    Set\u003cString\u003e leafQueueNames \u003d null;\n-    for (String partition : allPartitions) {\n-      TempQueuePerPartition tRoot \u003d\n-          getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n-      // compute the ideal distribution of resources among queues\n-      // updates cloned queues state accordingly\n-      tRoot.idealAssigned \u003d tRoot.guaranteed;\n-\n-      leafQueueNames \u003d\n-          recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n-    }\n-\n-    // remove containers from killable list when we want to preempt less resources\n-    // from queue.\n-    cleanupStaledKillableContainers(clusterResources, leafQueueNames);\n-\n-    // based on ideal allocation select containers to be preempted from each\n+    // based on ideal allocation select containers to be preemptionCandidates from each\n     // queue and each application\n-    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n-        getContainersToPreempt(leafQueueNames, clusterResources);\n+    Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e toPreempt \u003d null;\n+    for (PreemptionCandidatesSelector selector :\n+        candidatesSelectionPolicies) {\n+      toPreempt \u003d selector.selectCandidates(toPreempt,\n+          clusterResources, totalPreemptionAllowed);\n+    }\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n-    // preempt (or kill) the selected containers\n-    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n-         : toPreempt.entrySet()) {\n-      ApplicationAttemptId appAttemptId \u003d e.getKey();\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n-            + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n-      }\n-      for (RMContainer container : e.getValue()) {\n-        // if we tried to preempt this for more than maxWaitTime\n-        if (preempted.get(container) !\u003d null \u0026\u0026\n-            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n-          // mark container killable\n-          rmContext.getDispatcher().getEventHandler().handle(\n-              new ContainerPreemptEvent(appAttemptId, container,\n-                  SchedulerEventType.MARK_CONTAINER_FOR_KILLABLE));\n-          preempted.remove(container);\n-        } else {\n-          if (preempted.get(container) !\u003d null) {\n-            // We already updated the information to scheduler earlier, we need\n-            // not have to raise another event.\n-            continue;\n-          }\n-          //otherwise just send preemption events\n-          rmContext.getDispatcher().getEventHandler().handle(\n-              new ContainerPreemptEvent(appAttemptId, container,\n-                  SchedulerEventType.MARK_CONTAINER_FOR_PREEMPTION));\n-          preempted.put(container, clock.getTime());\n-        }\n-      }\n-    }\n+    // TODO: need consider revert killable containers when no more demandings.\n+    // Since we could have several selectors to make decisions concurrently.\n+    // So computed ideal-allocation varies between different selectors.\n+    //\n+    // We may need to \"score\" killable containers and revert the most preferred\n+    // containers. The bottom line is, we shouldn\u0027t preempt a queue which is already\n+    // below its guaranteed resource.\n \n-    // Keep the preempted list clean\n-    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n-      RMContainer id \u003d i.next();\n-      // garbage collect containers that are irrelevant for preemption\n-      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n-        i.remove();\n-      }\n-    }\n+    // preempt (or kill) the selected containers\n+    preemptOrkillSelectedContainerAfterWait(toPreempt);\n+\n+    // cleanup staled preemption candidates\n+    cleanupStaledPreemptionCandidates();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // Sync killable containers from scheduler when lazy preemption enabled\n    if (lazyPreempionEnabled) {\n      syncKillableContainersFromScheduler();\n    }\n\n    // All partitions to look at\n    Set\u003cString\u003e partitions \u003d new HashSet\u003c\u003e();\n    partitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    partitions.add(RMNodeLabelsManager.NO_LABEL);\n    this.allPartitions \u003d ImmutableSet.copyOf(partitions);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root,\n            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n            partitionToLookAt);\n      }\n    }\n\n    this.leafQueueNames \u003d ImmutableSet.copyOf(getLeafQueueNames(\n        getQueueByPartition(CapacitySchedulerConfiguration.ROOT,\n            RMNodeLabelsManager.NO_LABEL)));\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    // based on ideal allocation select containers to be preemptionCandidates from each\n    // queue and each application\n    Map\u003cApplicationAttemptId, Set\u003cRMContainer\u003e\u003e toPreempt \u003d null;\n    for (PreemptionCandidatesSelector selector :\n        candidatesSelectionPolicies) {\n      toPreempt \u003d selector.selectCandidates(toPreempt,\n          clusterResources, totalPreemptionAllowed);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // TODO: need consider revert killable containers when no more demandings.\n    // Since we could have several selectors to make decisions concurrently.\n    // So computed ideal-allocation varies between different selectors.\n    //\n    // We may need to \"score\" killable containers and revert the most preferred\n    // containers. The bottom line is, we shouldn\u0027t preempt a queue which is already\n    // below its guaranteed resource.\n\n    // preempt (or kill) the selected containers\n    preemptOrkillSelectedContainerAfterWait(toPreempt);\n\n    // cleanup staled preemption candidates\n    cleanupStaledPreemptionCandidates();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "ae14e5d07f1b6702a5160637438028bb03d9387e": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4108. CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\n\n(cherry picked from commit 7e8c9beb4156dcaeb3a11e60aaa06d2370626913)\n",
      "commitDate": "16/03/16 5:02 PM",
      "commitName": "ae14e5d07f1b6702a5160637438028bb03d9387e",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/03/16 5:02 PM",
      "commitNameOld": "fa7a43529d529f0006c8033c2003f15b9b93f103",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,96 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // All partitions to look at\n     Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n     allPartitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n \n+    syncKillableContainersFromScheduler();\n+\n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root,\n             nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n             partitionToLookAt);\n       }\n     }\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n     Set\u003cString\u003e leafQueueNames \u003d null;\n     for (String partition : allPartitions) {\n       TempQueuePerPartition tRoot \u003d\n           getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n       // compute the ideal distribution of resources among queues\n       // updates cloned queues state accordingly\n       tRoot.idealAssigned \u003d tRoot.guaranteed;\n \n       leafQueueNames \u003d\n           recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n     }\n \n+    // remove containers from killable list when we want to preempt less resources\n+    // from queue.\n+    cleanupStaledKillableContainers(clusterResources, leafQueueNames);\n+\n     // based on ideal allocation select containers to be preempted from each\n     // queue and each application\n     Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         getContainersToPreempt(leafQueueNames, clusterResources);\n \n     if (LOG.isDebugEnabled()) {\n-      logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n+      logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // preempt (or kill) the selected containers\n     for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n          : toPreempt.entrySet()) {\n       ApplicationAttemptId appAttemptId \u003d e.getKey();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n             + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n       }\n       for (RMContainer container : e.getValue()) {\n         // if we tried to preempt this for more than maxWaitTime\n         if (preempted.get(container) !\u003d null \u0026\u0026\n             preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n-          // kill it\n+          // mark container killable\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n-                  SchedulerEventType.KILL_PREEMPTED_CONTAINER));\n+                  SchedulerEventType.MARK_CONTAINER_FOR_KILLABLE));\n           preempted.remove(container);\n         } else {\n           if (preempted.get(container) !\u003d null) {\n             // We already updated the information to scheduler earlier, we need\n             // not have to raise another event.\n             continue;\n           }\n           //otherwise just send preemption events\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n                   SchedulerEventType.MARK_CONTAINER_FOR_PREEMPTION));\n           preempted.put(container, clock.getTime());\n         }\n       }\n     }\n \n     // Keep the preempted list clean\n     for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n       RMContainer id \u003d i.next();\n       // garbage collect containers that are irrelevant for preemption\n       if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n         i.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // All partitions to look at\n    Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n    allPartitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n\n    syncKillableContainersFromScheduler();\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root,\n            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n            partitionToLookAt);\n      }\n    }\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    Set\u003cString\u003e leafQueueNames \u003d null;\n    for (String partition : allPartitions) {\n      TempQueuePerPartition tRoot \u003d\n          getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n      // compute the ideal distribution of resources among queues\n      // updates cloned queues state accordingly\n      tRoot.idealAssigned \u003d tRoot.guaranteed;\n\n      leafQueueNames \u003d\n          recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n    }\n\n    // remove containers from killable list when we want to preempt less resources\n    // from queue.\n    cleanupStaledKillableContainers(clusterResources, leafQueueNames);\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(leafQueueNames, clusterResources);\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      ApplicationAttemptId appAttemptId \u003d e.getKey();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n            + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n      }\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // mark container killable\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.MARK_CONTAINER_FOR_KILLABLE));\n          preempted.remove(container);\n        } else {\n          if (preempted.get(container) !\u003d null) {\n            // We already updated the information to scheduler earlier, we need\n            // not have to raise another event.\n            continue;\n          }\n          //otherwise just send preemption events\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.MARK_CONTAINER_FOR_PREEMPTION));\n          preempted.put(container, clock.getTime());\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "fa7a43529d529f0006c8033c2003f15b9b93f103": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\"\n\nThis reverts commit 7e8c9beb4156dcaeb3a11e60aaa06d2370626913.\n",
      "commitDate": "16/03/16 5:02 PM",
      "commitName": "fa7a43529d529f0006c8033c2003f15b9b93f103",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/03/16 4:59 PM",
      "commitNameOld": "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,96 +1,90 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // All partitions to look at\n     Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n     allPartitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n \n-    syncKillableContainersFromScheduler();\n-\n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root,\n             nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n             partitionToLookAt);\n       }\n     }\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n     Set\u003cString\u003e leafQueueNames \u003d null;\n     for (String partition : allPartitions) {\n       TempQueuePerPartition tRoot \u003d\n           getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n       // compute the ideal distribution of resources among queues\n       // updates cloned queues state accordingly\n       tRoot.idealAssigned \u003d tRoot.guaranteed;\n \n       leafQueueNames \u003d\n           recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n     }\n \n-    // remove containers from killable list when we want to preempt less resources\n-    // from queue.\n-    cleanupStaledKillableContainers(clusterResources, leafQueueNames);\n-\n     // based on ideal allocation select containers to be preempted from each\n     // queue and each application\n     Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         getContainersToPreempt(leafQueueNames, clusterResources);\n \n     if (LOG.isDebugEnabled()) {\n-      logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n+      logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // preempt (or kill) the selected containers\n     for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n          : toPreempt.entrySet()) {\n       ApplicationAttemptId appAttemptId \u003d e.getKey();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n             + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n       }\n       for (RMContainer container : e.getValue()) {\n         // if we tried to preempt this for more than maxWaitTime\n         if (preempted.get(container) !\u003d null \u0026\u0026\n             preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n-          // mark container killable\n+          // kill it\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n-                  SchedulerEventType.MARK_CONTAINER_FOR_KILLABLE));\n+                  SchedulerEventType.KILL_PREEMPTED_CONTAINER));\n           preempted.remove(container);\n         } else {\n           if (preempted.get(container) !\u003d null) {\n             // We already updated the information to scheduler earlier, we need\n             // not have to raise another event.\n             continue;\n           }\n           //otherwise just send preemption events\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n                   SchedulerEventType.MARK_CONTAINER_FOR_PREEMPTION));\n           preempted.put(container, clock.getTime());\n         }\n       }\n     }\n \n     // Keep the preempted list clean\n     for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n       RMContainer id \u003d i.next();\n       // garbage collect containers that are irrelevant for preemption\n       if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n         i.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // All partitions to look at\n    Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n    allPartitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root,\n            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n            partitionToLookAt);\n      }\n    }\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    Set\u003cString\u003e leafQueueNames \u003d null;\n    for (String partition : allPartitions) {\n      TempQueuePerPartition tRoot \u003d\n          getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n      // compute the ideal distribution of resources among queues\n      // updates cloned queues state accordingly\n      tRoot.idealAssigned \u003d tRoot.guaranteed;\n\n      leafQueueNames \u003d\n          recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n    }\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(leafQueueNames, clusterResources);\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      ApplicationAttemptId appAttemptId \u003d e.getKey();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n            + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n      }\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // kill it\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.KILL_PREEMPTED_CONTAINER));\n          preempted.remove(container);\n        } else {\n          if (preempted.get(container) !\u003d null) {\n            // We already updated the information to scheduler earlier, we need\n            // not have to raise another event.\n            continue;\n          }\n          //otherwise just send preemption events\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.MARK_CONTAINER_FOR_PREEMPTION));\n          preempted.put(container, clock.getTime());\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913": {
      "type": "Ybodychange",
      "commitMessage": "CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\n",
      "commitDate": "16/03/16 4:59 PM",
      "commitName": "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "18/01/16 5:30 PM",
      "commitNameOld": "a44ce3f14fd940601f984fbf7980aa6fdc8f23b7",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 57.94,
      "commitsBetweenForRepo": 394,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,96 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // All partitions to look at\n     Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n     allPartitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n \n+    syncKillableContainersFromScheduler();\n+\n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root,\n             nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n             partitionToLookAt);\n       }\n     }\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n     Set\u003cString\u003e leafQueueNames \u003d null;\n     for (String partition : allPartitions) {\n       TempQueuePerPartition tRoot \u003d\n           getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n       // compute the ideal distribution of resources among queues\n       // updates cloned queues state accordingly\n       tRoot.idealAssigned \u003d tRoot.guaranteed;\n \n       leafQueueNames \u003d\n           recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n     }\n \n+    // remove containers from killable list when we want to preempt less resources\n+    // from queue.\n+    cleanupStaledKillableContainers(clusterResources, leafQueueNames);\n+\n     // based on ideal allocation select containers to be preempted from each\n     // queue and each application\n     Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         getContainersToPreempt(leafQueueNames, clusterResources);\n \n     if (LOG.isDebugEnabled()) {\n-      logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n+      logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // preempt (or kill) the selected containers\n     for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n          : toPreempt.entrySet()) {\n       ApplicationAttemptId appAttemptId \u003d e.getKey();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n             + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n       }\n       for (RMContainer container : e.getValue()) {\n         // if we tried to preempt this for more than maxWaitTime\n         if (preempted.get(container) !\u003d null \u0026\u0026\n             preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n-          // kill it\n+          // mark container killable\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n-                  SchedulerEventType.KILL_PREEMPTED_CONTAINER));\n+                  SchedulerEventType.MARK_CONTAINER_FOR_KILLABLE));\n           preempted.remove(container);\n         } else {\n           if (preempted.get(container) !\u003d null) {\n             // We already updated the information to scheduler earlier, we need\n             // not have to raise another event.\n             continue;\n           }\n           //otherwise just send preemption events\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n                   SchedulerEventType.MARK_CONTAINER_FOR_PREEMPTION));\n           preempted.put(container, clock.getTime());\n         }\n       }\n     }\n \n     // Keep the preempted list clean\n     for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n       RMContainer id \u003d i.next();\n       // garbage collect containers that are irrelevant for preemption\n       if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n         i.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // All partitions to look at\n    Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n    allPartitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n\n    syncKillableContainersFromScheduler();\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root,\n            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n            partitionToLookAt);\n      }\n    }\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    Set\u003cString\u003e leafQueueNames \u003d null;\n    for (String partition : allPartitions) {\n      TempQueuePerPartition tRoot \u003d\n          getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n      // compute the ideal distribution of resources among queues\n      // updates cloned queues state accordingly\n      tRoot.idealAssigned \u003d tRoot.guaranteed;\n\n      leafQueueNames \u003d\n          recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n    }\n\n    // remove containers from killable list when we want to preempt less resources\n    // from queue.\n    cleanupStaledKillableContainers(clusterResources, leafQueueNames);\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(leafQueueNames, clusterResources);\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003c\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      ApplicationAttemptId appAttemptId \u003d e.getKey();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n            + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n      }\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // mark container killable\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.MARK_CONTAINER_FOR_KILLABLE));\n          preempted.remove(container);\n        } else {\n          if (preempted.get(container) !\u003d null) {\n            // We already updated the information to scheduler earlier, we need\n            // not have to raise another event.\n            continue;\n          }\n          //otherwise just send preemption events\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.MARK_CONTAINER_FOR_PREEMPTION));\n          preempted.put(container, clock.getTime());\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "a44ce3f14fd940601f984fbf7980aa6fdc8f23b7": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)\n",
      "commitDate": "18/01/16 5:30 PM",
      "commitName": "a44ce3f14fd940601f984fbf7980aa6fdc8f23b7",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "18/01/16 5:27 PM",
      "commitNameOld": "150f5ae0343e872ee8bef39c57008c1389f0ba9e",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,90 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // All partitions to look at\n     Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n     allPartitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n \n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root,\n             nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n             partitionToLookAt);\n       }\n     }\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n     Set\u003cString\u003e leafQueueNames \u003d null;\n     for (String partition : allPartitions) {\n       TempQueuePerPartition tRoot \u003d\n           getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n       // compute the ideal distribution of resources among queues\n       // updates cloned queues state accordingly\n       tRoot.idealAssigned \u003d tRoot.guaranteed;\n \n       leafQueueNames \u003d\n           recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n     }\n \n     // based on ideal allocation select containers to be preempted from each\n     // queue and each application\n     Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         getContainersToPreempt(leafQueueNames, clusterResources);\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // preempt (or kill) the selected containers\n     for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n          : toPreempt.entrySet()) {\n       ApplicationAttemptId appAttemptId \u003d e.getKey();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n             + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n       }\n       for (RMContainer container : e.getValue()) {\n         // if we tried to preempt this for more than maxWaitTime\n         if (preempted.get(container) !\u003d null \u0026\u0026\n             preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n           // kill it\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n-                  SchedulerEventType.KILL_CONTAINER));\n+                  SchedulerEventType.KILL_PREEMPTED_CONTAINER));\n           preempted.remove(container);\n         } else {\n           if (preempted.get(container) !\u003d null) {\n             // We already updated the information to scheduler earlier, we need\n             // not have to raise another event.\n             continue;\n           }\n           //otherwise just send preemption events\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n-                  SchedulerEventType.PREEMPT_CONTAINER));\n+                  SchedulerEventType.MARK_CONTAINER_FOR_PREEMPTION));\n           preempted.put(container, clock.getTime());\n         }\n       }\n     }\n \n     // Keep the preempted list clean\n     for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n       RMContainer id \u003d i.next();\n       // garbage collect containers that are irrelevant for preemption\n       if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n         i.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // All partitions to look at\n    Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n    allPartitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root,\n            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n            partitionToLookAt);\n      }\n    }\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    Set\u003cString\u003e leafQueueNames \u003d null;\n    for (String partition : allPartitions) {\n      TempQueuePerPartition tRoot \u003d\n          getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n      // compute the ideal distribution of resources among queues\n      // updates cloned queues state accordingly\n      tRoot.idealAssigned \u003d tRoot.guaranteed;\n\n      leafQueueNames \u003d\n          recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n    }\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(leafQueueNames, clusterResources);\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      ApplicationAttemptId appAttemptId \u003d e.getKey();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n            + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n      }\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // kill it\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.KILL_PREEMPTED_CONTAINER));\n          preempted.remove(container);\n        } else {\n          if (preempted.get(container) !\u003d null) {\n            // We already updated the information to scheduler earlier, we need\n            // not have to raise another event.\n            continue;\n          }\n          //otherwise just send preemption events\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.MARK_CONTAINER_FOR_PREEMPTION));\n          preempted.put(container, clock.getTime());\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "150f5ae0343e872ee8bef39c57008c1389f0ba9e": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)\"\n\nThis reverts commit 3fe57285635e8058c34aa40a103845b49ca7d6ff.\n\nConflicts:\n\thadoop-yarn-project/CHANGES.txt\n",
      "commitDate": "18/01/16 5:27 PM",
      "commitName": "150f5ae0343e872ee8bef39c57008c1389f0ba9e",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "18/01/16 1:58 AM",
      "commitNameOld": "d40859fab1ad977636457a6cc96b6a4f9b903afc",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 0.65,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,90 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // All partitions to look at\n     Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n     allPartitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n \n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root,\n             nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n             partitionToLookAt);\n       }\n     }\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n     Set\u003cString\u003e leafQueueNames \u003d null;\n     for (String partition : allPartitions) {\n       TempQueuePerPartition tRoot \u003d\n           getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n       // compute the ideal distribution of resources among queues\n       // updates cloned queues state accordingly\n       tRoot.idealAssigned \u003d tRoot.guaranteed;\n \n       leafQueueNames \u003d\n           recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n     }\n \n     // based on ideal allocation select containers to be preempted from each\n     // queue and each application\n     Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         getContainersToPreempt(leafQueueNames, clusterResources);\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // preempt (or kill) the selected containers\n     for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n          : toPreempt.entrySet()) {\n       ApplicationAttemptId appAttemptId \u003d e.getKey();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n             + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n       }\n       for (RMContainer container : e.getValue()) {\n         // if we tried to preempt this for more than maxWaitTime\n         if (preempted.get(container) !\u003d null \u0026\u0026\n             preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n           // kill it\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n-                  SchedulerEventType.KILL_PREEMPTED_CONTAINER));\n+                  SchedulerEventType.KILL_CONTAINER));\n           preempted.remove(container);\n         } else {\n           if (preempted.get(container) !\u003d null) {\n             // We already updated the information to scheduler earlier, we need\n             // not have to raise another event.\n             continue;\n           }\n           //otherwise just send preemption events\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n                   SchedulerEventType.PREEMPT_CONTAINER));\n           preempted.put(container, clock.getTime());\n         }\n       }\n     }\n \n     // Keep the preempted list clean\n     for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n       RMContainer id \u003d i.next();\n       // garbage collect containers that are irrelevant for preemption\n       if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n         i.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // All partitions to look at\n    Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n    allPartitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root,\n            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n            partitionToLookAt);\n      }\n    }\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    Set\u003cString\u003e leafQueueNames \u003d null;\n    for (String partition : allPartitions) {\n      TempQueuePerPartition tRoot \u003d\n          getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n      // compute the ideal distribution of resources among queues\n      // updates cloned queues state accordingly\n      tRoot.idealAssigned \u003d tRoot.guaranteed;\n\n      leafQueueNames \u003d\n          recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n    }\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(leafQueueNames, clusterResources);\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      ApplicationAttemptId appAttemptId \u003d e.getKey();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n            + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n      }\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // kill it\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.KILL_CONTAINER));\n          preempted.remove(container);\n        } else {\n          if (preempted.get(container) !\u003d null) {\n            // We already updated the information to scheduler earlier, we need\n            // not have to raise another event.\n            continue;\n          }\n          //otherwise just send preemption events\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.PREEMPT_CONTAINER));\n          preempted.put(container, clock.getTime());\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "3fe57285635e8058c34aa40a103845b49ca7d6ff": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)\n\n(cherry picked from commit 805a9ed85eb34c8125cfb7d26d07cdfac12b3579)\n",
      "commitDate": "18/01/16 1:06 AM",
      "commitName": "3fe57285635e8058c34aa40a103845b49ca7d6ff",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "18/01/16 12:50 AM",
      "commitNameOld": "adf260a728df427eb729abe8fb9ad7248991ea54",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,90 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // All partitions to look at\n     Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n     allPartitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n \n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root,\n             nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n             partitionToLookAt);\n       }\n     }\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n     Set\u003cString\u003e leafQueueNames \u003d null;\n     for (String partition : allPartitions) {\n       TempQueuePerPartition tRoot \u003d\n           getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n       // compute the ideal distribution of resources among queues\n       // updates cloned queues state accordingly\n       tRoot.idealAssigned \u003d tRoot.guaranteed;\n \n       leafQueueNames \u003d\n           recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n     }\n \n     // based on ideal allocation select containers to be preempted from each\n     // queue and each application\n     Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         getContainersToPreempt(leafQueueNames, clusterResources);\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // preempt (or kill) the selected containers\n     for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n          : toPreempt.entrySet()) {\n       ApplicationAttemptId appAttemptId \u003d e.getKey();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n             + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n       }\n       for (RMContainer container : e.getValue()) {\n         // if we tried to preempt this for more than maxWaitTime\n         if (preempted.get(container) !\u003d null \u0026\u0026\n             preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n           // kill it\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n-                  SchedulerEventType.KILL_CONTAINER));\n+                  SchedulerEventType.KILL_PREEMPTED_CONTAINER));\n           preempted.remove(container);\n         } else {\n           if (preempted.get(container) !\u003d null) {\n             // We already updated the information to scheduler earlier, we need\n             // not have to raise another event.\n             continue;\n           }\n           //otherwise just send preemption events\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n                   SchedulerEventType.PREEMPT_CONTAINER));\n           preempted.put(container, clock.getTime());\n         }\n       }\n     }\n \n     // Keep the preempted list clean\n     for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n       RMContainer id \u003d i.next();\n       // garbage collect containers that are irrelevant for preemption\n       if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n         i.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // All partitions to look at\n    Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n    allPartitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root,\n            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n            partitionToLookAt);\n      }\n    }\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    Set\u003cString\u003e leafQueueNames \u003d null;\n    for (String partition : allPartitions) {\n      TempQueuePerPartition tRoot \u003d\n          getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n      // compute the ideal distribution of resources among queues\n      // updates cloned queues state accordingly\n      tRoot.idealAssigned \u003d tRoot.guaranteed;\n\n      leafQueueNames \u003d\n          recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n    }\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(leafQueueNames, clusterResources);\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      ApplicationAttemptId appAttemptId \u003d e.getKey();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n            + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n      }\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // kill it\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.KILL_PREEMPTED_CONTAINER));\n          preempted.remove(container);\n        } else {\n          if (preempted.get(container) !\u003d null) {\n            // We already updated the information to scheduler earlier, we need\n            // not have to raise another event.\n            continue;\n          }\n          //otherwise just send preemption events\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.PREEMPT_CONTAINER));\n          preempted.put(container, clock.getTime());\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "adf260a728df427eb729abe8fb9ad7248991ea54": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)\"\n\nThis reverts commit 805a9ed85eb34c8125cfb7d26d07cdfac12b3579.\n",
      "commitDate": "18/01/16 12:50 AM",
      "commitName": "adf260a728df427eb729abe8fb9ad7248991ea54",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "17/01/16 7:04 PM",
      "commitNameOld": "805a9ed85eb34c8125cfb7d26d07cdfac12b3579",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.24,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,90 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // All partitions to look at\n     Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n     allPartitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n \n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root,\n             nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n             partitionToLookAt);\n       }\n     }\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n     Set\u003cString\u003e leafQueueNames \u003d null;\n     for (String partition : allPartitions) {\n       TempQueuePerPartition tRoot \u003d\n           getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n       // compute the ideal distribution of resources among queues\n       // updates cloned queues state accordingly\n       tRoot.idealAssigned \u003d tRoot.guaranteed;\n \n       leafQueueNames \u003d\n           recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n     }\n \n     // based on ideal allocation select containers to be preempted from each\n     // queue and each application\n     Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         getContainersToPreempt(leafQueueNames, clusterResources);\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // preempt (or kill) the selected containers\n     for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n          : toPreempt.entrySet()) {\n       ApplicationAttemptId appAttemptId \u003d e.getKey();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n             + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n       }\n       for (RMContainer container : e.getValue()) {\n         // if we tried to preempt this for more than maxWaitTime\n         if (preempted.get(container) !\u003d null \u0026\u0026\n             preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n           // kill it\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n-                  SchedulerEventType.KILL_PREEMPTED_CONTAINER));\n+                  SchedulerEventType.KILL_CONTAINER));\n           preempted.remove(container);\n         } else {\n           if (preempted.get(container) !\u003d null) {\n             // We already updated the information to scheduler earlier, we need\n             // not have to raise another event.\n             continue;\n           }\n           //otherwise just send preemption events\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n                   SchedulerEventType.PREEMPT_CONTAINER));\n           preempted.put(container, clock.getTime());\n         }\n       }\n     }\n \n     // Keep the preempted list clean\n     for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n       RMContainer id \u003d i.next();\n       // garbage collect containers that are irrelevant for preemption\n       if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n         i.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // All partitions to look at\n    Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n    allPartitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root,\n            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n            partitionToLookAt);\n      }\n    }\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    Set\u003cString\u003e leafQueueNames \u003d null;\n    for (String partition : allPartitions) {\n      TempQueuePerPartition tRoot \u003d\n          getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n      // compute the ideal distribution of resources among queues\n      // updates cloned queues state accordingly\n      tRoot.idealAssigned \u003d tRoot.guaranteed;\n\n      leafQueueNames \u003d\n          recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n    }\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(leafQueueNames, clusterResources);\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      ApplicationAttemptId appAttemptId \u003d e.getKey();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n            + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n      }\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // kill it\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.KILL_CONTAINER));\n          preempted.remove(container);\n        } else {\n          if (preempted.get(container) !\u003d null) {\n            // We already updated the information to scheduler earlier, we need\n            // not have to raise another event.\n            continue;\n          }\n          //otherwise just send preemption events\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.PREEMPT_CONTAINER));\n          preempted.put(container, clock.getTime());\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "805a9ed85eb34c8125cfb7d26d07cdfac12b3579": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4502. Fix two AM containers get allocated when AM restart. (Vinod Kumar Vavilapalli via wangda)\n",
      "commitDate": "17/01/16 7:04 PM",
      "commitName": "805a9ed85eb34c8125cfb7d26d07cdfac12b3579",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "20/11/15 3:55 PM",
      "commitNameOld": "2346fa3141bf28f25a90b6a426a1d3a3982e464f",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 58.13,
      "commitsBetweenForRepo": 350,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,90 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // All partitions to look at\n     Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n     allPartitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n \n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root,\n             nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n             partitionToLookAt);\n       }\n     }\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n     Set\u003cString\u003e leafQueueNames \u003d null;\n     for (String partition : allPartitions) {\n       TempQueuePerPartition tRoot \u003d\n           getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n       // compute the ideal distribution of resources among queues\n       // updates cloned queues state accordingly\n       tRoot.idealAssigned \u003d tRoot.guaranteed;\n \n       leafQueueNames \u003d\n           recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n     }\n \n     // based on ideal allocation select containers to be preempted from each\n     // queue and each application\n     Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         getContainersToPreempt(leafQueueNames, clusterResources);\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // preempt (or kill) the selected containers\n     for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n          : toPreempt.entrySet()) {\n       ApplicationAttemptId appAttemptId \u003d e.getKey();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n             + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n       }\n       for (RMContainer container : e.getValue()) {\n         // if we tried to preempt this for more than maxWaitTime\n         if (preempted.get(container) !\u003d null \u0026\u0026\n             preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n           // kill it\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n-                  SchedulerEventType.KILL_CONTAINER));\n+                  SchedulerEventType.KILL_PREEMPTED_CONTAINER));\n           preempted.remove(container);\n         } else {\n           if (preempted.get(container) !\u003d null) {\n             // We already updated the information to scheduler earlier, we need\n             // not have to raise another event.\n             continue;\n           }\n           //otherwise just send preemption events\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n                   SchedulerEventType.PREEMPT_CONTAINER));\n           preempted.put(container, clock.getTime());\n         }\n       }\n     }\n \n     // Keep the preempted list clean\n     for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n       RMContainer id \u003d i.next();\n       // garbage collect containers that are irrelevant for preemption\n       if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n         i.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // All partitions to look at\n    Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n    allPartitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root,\n            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n            partitionToLookAt);\n      }\n    }\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    Set\u003cString\u003e leafQueueNames \u003d null;\n    for (String partition : allPartitions) {\n      TempQueuePerPartition tRoot \u003d\n          getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n      // compute the ideal distribution of resources among queues\n      // updates cloned queues state accordingly\n      tRoot.idealAssigned \u003d tRoot.guaranteed;\n\n      leafQueueNames \u003d\n          recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n    }\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(leafQueueNames, clusterResources);\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      ApplicationAttemptId appAttemptId \u003d e.getKey();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n            + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n      }\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // kill it\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.KILL_PREEMPTED_CONTAINER));\n          preempted.remove(container);\n        } else {\n          if (preempted.get(container) !\u003d null) {\n            // We already updated the information to scheduler earlier, we need\n            // not have to raise another event.\n            continue;\n          }\n          //otherwise just send preemption events\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.PREEMPT_CONTAINER));\n          preempted.put(container, clock.getTime());\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "3bba1800513b38a4827f7552f348db87dc47c783": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3941. Proportional Preemption policy should try to avoid sending duplicate PREEMPT_CONTAINER event to scheduler. (Sunil G via wangda)\n",
      "commitDate": "23/07/15 10:07 AM",
      "commitName": "3bba1800513b38a4827f7552f348db87dc47c783",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/07/15 4:13 PM",
      "commitNameOld": "3540d5fe4b1da942ea80c9e7ca1126b1abb8a68a",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 6.75,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,90 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // All partitions to look at\n     Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n     allPartitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n \n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root,\n             nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n             partitionToLookAt);\n       }\n     }\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n     Set\u003cString\u003e leafQueueNames \u003d null;\n     for (String partition : allPartitions) {\n       TempQueuePerPartition tRoot \u003d\n           getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n       // compute the ideal distribution of resources among queues\n       // updates cloned queues state accordingly\n       tRoot.idealAssigned \u003d tRoot.guaranteed;\n \n       leafQueueNames \u003d\n           recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n     }\n \n     // based on ideal allocation select containers to be preempted from each\n     // queue and each application\n     Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         getContainersToPreempt(leafQueueNames, clusterResources);\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // preempt (or kill) the selected containers\n     for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n          : toPreempt.entrySet()) {\n       ApplicationAttemptId appAttemptId \u003d e.getKey();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n             + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n       }\n       for (RMContainer container : e.getValue()) {\n         // if we tried to preempt this for more than maxWaitTime\n         if (preempted.get(container) !\u003d null \u0026\u0026\n             preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n           // kill it\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n                   SchedulerEventType.KILL_CONTAINER));\n           preempted.remove(container);\n         } else {\n+          if (preempted.get(container) !\u003d null) {\n+            // We already updated the information to scheduler earlier, we need\n+            // not have to raise another event.\n+            continue;\n+          }\n           //otherwise just send preemption events\n           rmContext.getDispatcher().getEventHandler().handle(\n               new ContainerPreemptEvent(appAttemptId, container,\n                   SchedulerEventType.PREEMPT_CONTAINER));\n-          if (preempted.get(container) \u003d\u003d null) {\n-            preempted.put(container, clock.getTime());\n-          }\n+          preempted.put(container, clock.getTime());\n         }\n       }\n     }\n \n     // Keep the preempted list clean\n     for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n       RMContainer id \u003d i.next();\n       // garbage collect containers that are irrelevant for preemption\n       if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n         i.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // All partitions to look at\n    Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n    allPartitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root,\n            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n            partitionToLookAt);\n      }\n    }\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    Set\u003cString\u003e leafQueueNames \u003d null;\n    for (String partition : allPartitions) {\n      TempQueuePerPartition tRoot \u003d\n          getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n      // compute the ideal distribution of resources among queues\n      // updates cloned queues state accordingly\n      tRoot.idealAssigned \u003d tRoot.guaranteed;\n\n      leafQueueNames \u003d\n          recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n    }\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(leafQueueNames, clusterResources);\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      ApplicationAttemptId appAttemptId \u003d e.getKey();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n            + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n      }\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // kill it\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.KILL_CONTAINER));\n          preempted.remove(container);\n        } else {\n          if (preempted.get(container) !\u003d null) {\n            // We already updated the information to scheduler earlier, we need\n            // not have to raise another event.\n            continue;\n          }\n          //otherwise just send preemption events\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.PREEMPT_CONTAINER));\n          preempted.put(container, clock.getTime());\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "0e4b06690ff51fbde3ab26f68fde8aeb32af69af": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3508. Prevent processing preemption events on the main RM dispatcher. (Varun Saxena via wangda)\n",
      "commitDate": "01/07/15 5:32 PM",
      "commitName": "0e4b06690ff51fbde3ab26f68fde8aeb32af69af",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "24/04/15 5:03 PM",
      "commitNameOld": "d497f6ea2be559aa31ed76f37ae949dbfabe2a51",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 68.02,
      "commitsBetweenForRepo": 597,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,87 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n     // All partitions to look at\n     Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n     allPartitions.addAll(scheduler.getRMContext()\n         .getNodeLabelManager().getClusterNodeLabelNames());\n     allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n \n     // extract a summary of the queues from scheduler\n     synchronized (scheduler) {\n       queueToPartitions.clear();\n \n       for (String partitionToLookAt : allPartitions) {\n         cloneQueues(root,\n             nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n             partitionToLookAt);\n       }\n     }\n \n     // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n \n     Set\u003cString\u003e leafQueueNames \u003d null;\n     for (String partition : allPartitions) {\n       TempQueuePerPartition tRoot \u003d\n           getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n       // compute the ideal distribution of resources among queues\n       // updates cloned queues state accordingly\n       tRoot.idealAssigned \u003d tRoot.guaranteed;\n \n       leafQueueNames \u003d\n           recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n     }\n \n     // based on ideal allocation select containers to be preempted from each\n     // queue and each application\n     Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         getContainersToPreempt(leafQueueNames, clusterResources);\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // preempt (or kill) the selected containers\n     for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n          : toPreempt.entrySet()) {\n+      ApplicationAttemptId appAttemptId \u003d e.getKey();\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Send to scheduler: in app\u003d\" + e.getKey()\n+        LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n             + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n       }\n       for (RMContainer container : e.getValue()) {\n         // if we tried to preempt this for more than maxWaitTime\n         if (preempted.get(container) !\u003d null \u0026\u0026\n             preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n           // kill it\n-          dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n-                ContainerPreemptEventType.KILL_CONTAINER));\n+          rmContext.getDispatcher().getEventHandler().handle(\n+              new ContainerPreemptEvent(appAttemptId, container,\n+                  SchedulerEventType.KILL_CONTAINER));\n           preempted.remove(container);\n         } else {\n           //otherwise just send preemption events\n-          dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n-                ContainerPreemptEventType.PREEMPT_CONTAINER));\n+          rmContext.getDispatcher().getEventHandler().handle(\n+              new ContainerPreemptEvent(appAttemptId, container,\n+                  SchedulerEventType.PREEMPT_CONTAINER));\n           if (preempted.get(container) \u003d\u003d null) {\n             preempted.put(container, clock.getTime());\n           }\n         }\n       }\n     }\n \n     // Keep the preempted list clean\n     for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n       RMContainer id \u003d i.next();\n       // garbage collect containers that are irrelevant for preemption\n       if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n         i.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // All partitions to look at\n    Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n    allPartitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root,\n            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n            partitionToLookAt);\n      }\n    }\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    Set\u003cString\u003e leafQueueNames \u003d null;\n    for (String partition : allPartitions) {\n      TempQueuePerPartition tRoot \u003d\n          getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n      // compute the ideal distribution of resources among queues\n      // updates cloned queues state accordingly\n      tRoot.idealAssigned \u003d tRoot.guaranteed;\n\n      leafQueueNames \u003d\n          recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n    }\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(leafQueueNames, clusterResources);\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      ApplicationAttemptId appAttemptId \u003d e.getKey();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Send to scheduler: in app\u003d\" + appAttemptId\n            + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n      }\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // kill it\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.KILL_CONTAINER));\n          preempted.remove(container);\n        } else {\n          //otherwise just send preemption events\n          rmContext.getDispatcher().getEventHandler().handle(\n              new ContainerPreemptEvent(appAttemptId, container,\n                  SchedulerEventType.PREEMPT_CONTAINER));\n          if (preempted.get(container) \u003d\u003d null) {\n            preempted.put(container, clock.getTime());\n          }\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "d497f6ea2be559aa31ed76f37ae949dbfabe2a51": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2498. Respect labels in preemption policy of capacity scheduler for inter-queue preemption. Contributed by Wangda Tan\n",
      "commitDate": "24/04/15 5:03 PM",
      "commitName": "d497f6ea2be559aa31ed76f37ae949dbfabe2a51",
      "commitAuthor": "Jian He",
      "commitDateOld": "20/04/15 5:12 PM",
      "commitNameOld": "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 3.99,
      "commitsBetweenForRepo": 60,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,84 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n+    // All partitions to look at\n+    Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n+    allPartitions.addAll(scheduler.getRMContext()\n+        .getNodeLabelManager().getClusterNodeLabelNames());\n+    allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n \n     // extract a summary of the queues from scheduler\n-    TempQueue tRoot;\n     synchronized (scheduler) {\n-      tRoot \u003d cloneQueues(root, clusterResources);\n+      queueToPartitions.clear();\n+\n+      for (String partitionToLookAt : allPartitions) {\n+        cloneQueues(root,\n+            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n+            partitionToLookAt);\n+      }\n     }\n \n-    // compute the ideal distribution of resources among queues\n-    // updates cloned queues state accordingly\n-    tRoot.idealAssigned \u003d tRoot.guaranteed;\n+    // compute total preemption allowed\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n-    List\u003cTempQueue\u003e queues \u003d\n-      recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n+\n+    Set\u003cString\u003e leafQueueNames \u003d null;\n+    for (String partition : allPartitions) {\n+      TempQueuePerPartition tRoot \u003d\n+          getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n+      // compute the ideal distribution of resources among queues\n+      // updates cloned queues state accordingly\n+      tRoot.idealAssigned \u003d tRoot.guaranteed;\n+\n+      leafQueueNames \u003d\n+          recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n+    }\n \n     // based on ideal allocation select containers to be preempted from each\n     // queue and each application\n     Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n-        getContainersToPreempt(queues, clusterResources);\n+        getContainersToPreempt(leafQueueNames, clusterResources);\n \n     if (LOG.isDebugEnabled()) {\n-      logToCSV(queues);\n+      logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // preempt (or kill) the selected containers\n     for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n          : toPreempt.entrySet()) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Send to scheduler: in app\u003d\" + e.getKey()\n+            + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n+      }\n       for (RMContainer container : e.getValue()) {\n         // if we tried to preempt this for more than maxWaitTime\n         if (preempted.get(container) !\u003d null \u0026\u0026\n             preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n           // kill it\n           dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                 ContainerPreemptEventType.KILL_CONTAINER));\n           preempted.remove(container);\n         } else {\n           //otherwise just send preemption events\n           dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                 ContainerPreemptEventType.PREEMPT_CONTAINER));\n           if (preempted.get(container) \u003d\u003d null) {\n             preempted.put(container, clock.getTime());\n           }\n         }\n       }\n     }\n \n     // Keep the preempted list clean\n     for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n       RMContainer id \u003d i.next();\n       // garbage collect containers that are irrelevant for preemption\n       if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n         i.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n    // All partitions to look at\n    Set\u003cString\u003e allPartitions \u003d new HashSet\u003c\u003e();\n    allPartitions.addAll(scheduler.getRMContext()\n        .getNodeLabelManager().getClusterNodeLabelNames());\n    allPartitions.add(RMNodeLabelsManager.NO_LABEL);\n\n    // extract a summary of the queues from scheduler\n    synchronized (scheduler) {\n      queueToPartitions.clear();\n\n      for (String partitionToLookAt : allPartitions) {\n        cloneQueues(root,\n            nlm.getResourceByLabel(partitionToLookAt, clusterResources),\n            partitionToLookAt);\n      }\n    }\n\n    // compute total preemption allowed\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n\n    Set\u003cString\u003e leafQueueNames \u003d null;\n    for (String partition : allPartitions) {\n      TempQueuePerPartition tRoot \u003d\n          getQueueByPartition(CapacitySchedulerConfiguration.ROOT, partition);\n      // compute the ideal distribution of resources among queues\n      // updates cloned queues state accordingly\n      tRoot.idealAssigned \u003d tRoot.guaranteed;\n\n      leafQueueNames \u003d\n          recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n    }\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(leafQueueNames, clusterResources);\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(new ArrayList\u003cString\u003e(leafQueueNames));\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Send to scheduler: in app\u003d\" + e.getKey()\n            + \" #containers-to-be-preempted\u003d\" + e.getValue().size());\n      }\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // kill it\n          dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                ContainerPreemptEventType.KILL_CONTAINER));\n          preempted.remove(container);\n        } else {\n          //otherwise just send preemption events\n          dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                ContainerPreemptEventType.PREEMPT_CONTAINER));\n          if (preempted.get(container) \u003d\u003d null) {\n            preempted.put(container, clock.getTime());\n          }\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "18741adf97f4fda5f8743318b59c440928e51297": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2932. Add entry for preemptable status (enabled/disabled) to scheduler web UI and queue initialize/refresh logging. (Eric Payne via wangda)\n",
      "commitDate": "27/01/15 3:36 PM",
      "commitName": "18741adf97f4fda5f8743318b59c440928e51297",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "19/01/15 4:48 PM",
      "commitNameOld": "0a2d3e717d9c42090a32ff177991a222a1e34132",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 7.95,
      "commitsBetweenForRepo": 65,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,62 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n \n     // extract a summary of the queues from scheduler\n     TempQueue tRoot;\n     synchronized (scheduler) {\n-      tRoot \u003d cloneQueues(root, clusterResources, false);\n+      tRoot \u003d cloneQueues(root, clusterResources);\n     }\n \n     // compute the ideal distribution of resources among queues\n     // updates cloned queues state accordingly\n     tRoot.idealAssigned \u003d tRoot.guaranteed;\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n     List\u003cTempQueue\u003e queues \u003d\n       recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n \n     // based on ideal allocation select containers to be preempted from each\n     // queue and each application\n     Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         getContainersToPreempt(queues, clusterResources);\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(queues);\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // preempt (or kill) the selected containers\n     for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n          : toPreempt.entrySet()) {\n       for (RMContainer container : e.getValue()) {\n         // if we tried to preempt this for more than maxWaitTime\n         if (preempted.get(container) !\u003d null \u0026\u0026\n             preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n           // kill it\n           dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                 ContainerPreemptEventType.KILL_CONTAINER));\n           preempted.remove(container);\n         } else {\n           //otherwise just send preemption events\n           dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                 ContainerPreemptEventType.PREEMPT_CONTAINER));\n           if (preempted.get(container) \u003d\u003d null) {\n             preempted.put(container, clock.getTime());\n           }\n         }\n       }\n     }\n \n     // Keep the preempted list clean\n     for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n       RMContainer id \u003d i.next();\n       // garbage collect containers that are irrelevant for preemption\n       if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n         i.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n\n    // extract a summary of the queues from scheduler\n    TempQueue tRoot;\n    synchronized (scheduler) {\n      tRoot \u003d cloneQueues(root, clusterResources);\n    }\n\n    // compute the ideal distribution of resources among queues\n    // updates cloned queues state accordingly\n    tRoot.idealAssigned \u003d tRoot.guaranteed;\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n    List\u003cTempQueue\u003e queues \u003d\n      recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(queues, clusterResources);\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(queues);\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // kill it\n          dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                ContainerPreemptEventType.KILL_CONTAINER));\n          preempted.remove(container);\n        } else {\n          //otherwise just send preemption events\n          dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                ContainerPreemptEventType.PREEMPT_CONTAINER));\n          if (preempted.get(container) \u003d\u003d null) {\n            preempted.put(container, clock.getTime());\n          }\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "4b130821995a3cfe20c71e38e0f63294085c0491": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2056. Disable preemption at Queue level. Contributed by Eric Payne\n",
      "commitDate": "05/12/14 1:06 PM",
      "commitName": "4b130821995a3cfe20c71e38e0f63294085c0491",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "12/09/14 10:33 AM",
      "commitNameOld": "3122daa80261b466e309e88d88d1e2c030525e3f",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 84.15,
      "commitsBetweenForRepo": 775,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,62 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n \n     // extract a summary of the queues from scheduler\n     TempQueue tRoot;\n     synchronized (scheduler) {\n-      tRoot \u003d cloneQueues(root, clusterResources);\n+      tRoot \u003d cloneQueues(root, clusterResources, false);\n     }\n \n     // compute the ideal distribution of resources among queues\n     // updates cloned queues state accordingly\n     tRoot.idealAssigned \u003d tRoot.guaranteed;\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n     List\u003cTempQueue\u003e queues \u003d\n       recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n \n     // based on ideal allocation select containers to be preempted from each\n     // queue and each application\n     Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         getContainersToPreempt(queues, clusterResources);\n \n     if (LOG.isDebugEnabled()) {\n       logToCSV(queues);\n     }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // preempt (or kill) the selected containers\n     for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n          : toPreempt.entrySet()) {\n       for (RMContainer container : e.getValue()) {\n         // if we tried to preempt this for more than maxWaitTime\n         if (preempted.get(container) !\u003d null \u0026\u0026\n             preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n           // kill it\n           dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                 ContainerPreemptEventType.KILL_CONTAINER));\n           preempted.remove(container);\n         } else {\n           //otherwise just send preemption events\n           dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                 ContainerPreemptEventType.PREEMPT_CONTAINER));\n           if (preempted.get(container) \u003d\u003d null) {\n             preempted.put(container, clock.getTime());\n           }\n         }\n       }\n     }\n \n     // Keep the preempted list clean\n     for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n       RMContainer id \u003d i.next();\n       // garbage collect containers that are irrelevant for preemption\n       if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n         i.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n\n    // extract a summary of the queues from scheduler\n    TempQueue tRoot;\n    synchronized (scheduler) {\n      tRoot \u003d cloneQueues(root, clusterResources, false);\n    }\n\n    // compute the ideal distribution of resources among queues\n    // updates cloned queues state accordingly\n    tRoot.idealAssigned \u003d tRoot.guaranteed;\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n    List\u003cTempQueue\u003e queues \u003d\n      recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(queues, clusterResources);\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(queues);\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // kill it\n          dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                ContainerPreemptEventType.KILL_CONTAINER));\n          preempted.remove(container);\n        } else {\n          //otherwise just send preemption events\n          dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                ContainerPreemptEventType.PREEMPT_CONTAINER));\n          if (preempted.get(container) \u003d\u003d null) {\n            preempted.put(container, clock.getTime());\n          }\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "c6e29a9f069f71bd77fcff2111def4a60676b4ba": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2125. Changed ProportionalCapacityPreemptionPolicy to log CSV in debug level. Contributed by Wangda Tan\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601980 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/06/14 11:31 AM",
      "commitName": "c6e29a9f069f71bd77fcff2111def4a60676b4ba",
      "commitAuthor": "Jian He",
      "commitDateOld": "11/06/14 10:30 AM",
      "commitNameOld": "710a8693e57235d283e704c983722079c8fd6f38",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,62 @@\n   private void containerBasedPreemptOrKill(CSQueue root,\n       Resource clusterResources) {\n \n     // extract a summary of the queues from scheduler\n     TempQueue tRoot;\n     synchronized (scheduler) {\n       tRoot \u003d cloneQueues(root, clusterResources);\n     }\n \n     // compute the ideal distribution of resources among queues\n     // updates cloned queues state accordingly\n     tRoot.idealAssigned \u003d tRoot.guaranteed;\n     Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n         percentageClusterPreemptionAllowed);\n     List\u003cTempQueue\u003e queues \u003d\n       recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n \n     // based on ideal allocation select containers to be preempted from each\n     // queue and each application\n     Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n         getContainersToPreempt(queues, clusterResources);\n \n-    logToCSV(queues);\n+    if (LOG.isDebugEnabled()) {\n+      logToCSV(queues);\n+    }\n \n     // if we are in observeOnly mode return before any action is taken\n     if (observeOnly) {\n       return;\n     }\n \n     // preempt (or kill) the selected containers\n     for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n          : toPreempt.entrySet()) {\n       for (RMContainer container : e.getValue()) {\n         // if we tried to preempt this for more than maxWaitTime\n         if (preempted.get(container) !\u003d null \u0026\u0026\n             preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n           // kill it\n           dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                 ContainerPreemptEventType.KILL_CONTAINER));\n           preempted.remove(container);\n         } else {\n           //otherwise just send preemption events\n           dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                 ContainerPreemptEventType.PREEMPT_CONTAINER));\n           if (preempted.get(container) \u003d\u003d null) {\n             preempted.put(container, clock.getTime());\n           }\n         }\n       }\n     }\n \n     // Keep the preempted list clean\n     for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n       RMContainer id \u003d i.next();\n       // garbage collect containers that are irrelevant for preemption\n       if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n         i.remove();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n\n    // extract a summary of the queues from scheduler\n    TempQueue tRoot;\n    synchronized (scheduler) {\n      tRoot \u003d cloneQueues(root, clusterResources);\n    }\n\n    // compute the ideal distribution of resources among queues\n    // updates cloned queues state accordingly\n    tRoot.idealAssigned \u003d tRoot.guaranteed;\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n    List\u003cTempQueue\u003e queues \u003d\n      recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(queues, clusterResources);\n\n    if (LOG.isDebugEnabled()) {\n      logToCSV(queues);\n    }\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // kill it\n          dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                ContainerPreemptEventType.KILL_CONTAINER));\n          preempted.remove(container);\n        } else {\n          //otherwise just send preemption events\n          dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                ContainerPreemptEventType.PREEMPT_CONTAINER));\n          if (preempted.get(container) \u003d\u003d null) {\n            preempted.put(container, clock.getTime());\n          }\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "85f0efb68f9d1d9ee3466e3939c4fc2f985ccf61": {
      "type": "Yintroduced",
      "commitMessage": "YARN-569. Add support for requesting and enforcing preemption requests via\na capacity monitor. Contributed by Carlo Curino, Chris Douglas\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1502083 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/07/13 6:20 PM",
      "commitName": "85f0efb68f9d1d9ee3466e3939c4fc2f985ccf61",
      "commitAuthor": "Christopher Douglas",
      "diff": "@@ -0,0 +1,60 @@\n+  private void containerBasedPreemptOrKill(CSQueue root,\n+      Resource clusterResources) {\n+\n+    // extract a summary of the queues from scheduler\n+    TempQueue tRoot;\n+    synchronized (scheduler) {\n+      tRoot \u003d cloneQueues(root, clusterResources);\n+    }\n+\n+    // compute the ideal distribution of resources among queues\n+    // updates cloned queues state accordingly\n+    tRoot.idealAssigned \u003d tRoot.guaranteed;\n+    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n+        percentageClusterPreemptionAllowed);\n+    List\u003cTempQueue\u003e queues \u003d\n+      recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n+\n+    // based on ideal allocation select containers to be preempted from each\n+    // queue and each application\n+    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n+        getContainersToPreempt(queues, clusterResources);\n+\n+    logToCSV(queues);\n+\n+    // if we are in observeOnly mode return before any action is taken\n+    if (observeOnly) {\n+      return;\n+    }\n+\n+    // preempt (or kill) the selected containers\n+    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n+         : toPreempt.entrySet()) {\n+      for (RMContainer container : e.getValue()) {\n+        // if we tried to preempt this for more than maxWaitTime\n+        if (preempted.get(container) !\u003d null \u0026\u0026\n+            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n+          // kill it\n+          dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n+                ContainerPreemptEventType.KILL_CONTAINER));\n+          preempted.remove(container);\n+        } else {\n+          //otherwise just send preemption events\n+          dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n+                ContainerPreemptEventType.PREEMPT_CONTAINER));\n+          if (preempted.get(container) \u003d\u003d null) {\n+            preempted.put(container, clock.getTime());\n+          }\n+        }\n+      }\n+    }\n+\n+    // Keep the preempted list clean\n+    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n+      RMContainer id \u003d i.next();\n+      // garbage collect containers that are irrelevant for preemption\n+      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n+        i.remove();\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void containerBasedPreemptOrKill(CSQueue root,\n      Resource clusterResources) {\n\n    // extract a summary of the queues from scheduler\n    TempQueue tRoot;\n    synchronized (scheduler) {\n      tRoot \u003d cloneQueues(root, clusterResources);\n    }\n\n    // compute the ideal distribution of resources among queues\n    // updates cloned queues state accordingly\n    tRoot.idealAssigned \u003d tRoot.guaranteed;\n    Resource totalPreemptionAllowed \u003d Resources.multiply(clusterResources,\n        percentageClusterPreemptionAllowed);\n    List\u003cTempQueue\u003e queues \u003d\n      recursivelyComputeIdealAssignment(tRoot, totalPreemptionAllowed);\n\n    // based on ideal allocation select containers to be preempted from each\n    // queue and each application\n    Map\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e toPreempt \u003d\n        getContainersToPreempt(queues, clusterResources);\n\n    logToCSV(queues);\n\n    // if we are in observeOnly mode return before any action is taken\n    if (observeOnly) {\n      return;\n    }\n\n    // preempt (or kill) the selected containers\n    for (Map.Entry\u003cApplicationAttemptId,Set\u003cRMContainer\u003e\u003e e\n         : toPreempt.entrySet()) {\n      for (RMContainer container : e.getValue()) {\n        // if we tried to preempt this for more than maxWaitTime\n        if (preempted.get(container) !\u003d null \u0026\u0026\n            preempted.get(container) + maxWaitTime \u003c clock.getTime()) {\n          // kill it\n          dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                ContainerPreemptEventType.KILL_CONTAINER));\n          preempted.remove(container);\n        } else {\n          //otherwise just send preemption events\n          dispatcher.handle(new ContainerPreemptEvent(e.getKey(), container,\n                ContainerPreemptEventType.PREEMPT_CONTAINER));\n          if (preempted.get(container) \u003d\u003d null) {\n            preempted.put(container, clock.getTime());\n          }\n        }\n      }\n    }\n\n    // Keep the preempted list clean\n    for (Iterator\u003cRMContainer\u003e i \u003d preempted.keySet().iterator(); i.hasNext();){\n      RMContainer id \u003d i.next();\n      // garbage collect containers that are irrelevant for preemption\n      if (preempted.get(id) + 2 * maxWaitTime \u003c clock.getTime()) {\n        i.remove();\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java"
    }
  }
}