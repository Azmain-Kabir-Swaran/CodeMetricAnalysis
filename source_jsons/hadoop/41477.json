{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FairScheduler.java",
  "functionName": "attemptScheduling",
  "functionId": "attemptScheduling___node-FSSchedulerNode",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
  "functionStartLine": 1107,
  "functionEndLine": 1166,
  "numCommitsSeen": 226,
  "timeTaken": 9862,
  "changeHistory": [
    "2064ca015d1584263aac0cc20c60b925a3aff612",
    "ea2c6c8c9a55813a19b3dbd0d29747d6a7739030",
    "0cd145a44390bc1a01113dce4be4e629637c3e8a",
    "f48fec83d0f2d1a781a141ad7216463c5526321f",
    "3de47ab5ea5cb75805a03010dc72e11b0cf6c173",
    "c3375175d616e0380560f89d491b6b9753a8f3e1",
    "31f8da22d0b8d2dcce5fbc8e45d832f40acf056f",
    "04ded558b03ee0fbf68a611cf1f25508b4447e44",
    "20d389ce61eaacb5ddfb329015f50e96ad894f8d",
    "bd69ea408f8fdd8293836ce1089fe9b01616f2f7",
    "4513761869c732cf2f462763043067ebf8749df7",
    "b6466deac6d5d6344f693144290b46e2bef83a02",
    "1a47f890ba3cb22b6262f47c1f1af2990559bb89",
    "485c96e3cb9b0b05d6e490b4773506da83ebc61d",
    "486e718fc1f5befd231494e2ec06bb360484f191",
    "29c102cad01b8a91cbf5173ca49d2e1ed8a45eee",
    "82f3454f5ac1f1c457e668e2cee12b4dcc800ee1",
    "1ab2f5a916a8e942e01b08685caa6c79ad3e1107",
    "ae05623a75803d4e12a902ac4a24187540f56699"
  ],
  "changeHistoryShort": {
    "2064ca015d1584263aac0cc20c60b925a3aff612": "Ybodychange",
    "ea2c6c8c9a55813a19b3dbd0d29747d6a7739030": "Ybodychange",
    "0cd145a44390bc1a01113dce4be4e629637c3e8a": "Ybodychange",
    "f48fec83d0f2d1a781a141ad7216463c5526321f": "Ybodychange",
    "3de47ab5ea5cb75805a03010dc72e11b0cf6c173": "Ybodychange",
    "c3375175d616e0380560f89d491b6b9753a8f3e1": "Ybodychange",
    "31f8da22d0b8d2dcce5fbc8e45d832f40acf056f": "Ymultichange(Ymodifierchange,Ybodychange)",
    "04ded558b03ee0fbf68a611cf1f25508b4447e44": "Ybodychange",
    "20d389ce61eaacb5ddfb329015f50e96ad894f8d": "Ybodychange",
    "bd69ea408f8fdd8293836ce1089fe9b01616f2f7": "Ybodychange",
    "4513761869c732cf2f462763043067ebf8749df7": "Ymultichange(Ymodifierchange,Ybodychange)",
    "b6466deac6d5d6344f693144290b46e2bef83a02": "Ybodychange",
    "1a47f890ba3cb22b6262f47c1f1af2990559bb89": "Ybodychange",
    "485c96e3cb9b0b05d6e490b4773506da83ebc61d": "Ybodychange",
    "486e718fc1f5befd231494e2ec06bb360484f191": "Ybodychange",
    "29c102cad01b8a91cbf5173ca49d2e1ed8a45eee": "Ybodychange",
    "82f3454f5ac1f1c457e668e2cee12b4dcc800ee1": "Ybodychange",
    "1ab2f5a916a8e942e01b08685caa6c79ad3e1107": "Ybodychange",
    "ae05623a75803d4e12a902ac4a24187540f56699": "Yintroduced"
  },
  "changeHistoryDetails": {
    "2064ca015d1584263aac0cc20c60b925a3aff612": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9349.  Changed logging to use slf4j api.\n            Contributed by Prabhu Joseph\n",
      "commitDate": "15/03/19 4:20 PM",
      "commitName": "2064ca015d1584263aac0cc20c60b925a3aff612",
      "commitAuthor": "Eric Yang",
      "commitDateOld": "04/03/19 9:10 PM",
      "commitNameOld": "e40e2d6ad5cbe782c3a067229270738b501ed27e",
      "commitAuthorOld": "Prabhu Joseph",
      "daysBetweenCommits": 10.76,
      "commitsBetweenForRepo": 108,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,60 @@\n   void attemptScheduling(FSSchedulerNode node) {\n     writeLock.lock();\n     try {\n       if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n           .isSchedulerReadyForAllocatingContainers()) {\n         return;\n       }\n \n       final NodeId nodeID \u003d (node !\u003d null ? node.getNodeID() : null);\n       if (!nodeTracker.exists(nodeID)) {\n         // The node might have just been removed while this thread was waiting\n         // on the synchronized lock before it entered this synchronized method\n         LOG.info(\n             \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n         return;\n       }\n \n       // Assign new containers...\n       // 1. Ensure containers are assigned to the apps that preempted\n       // 2. Check for reserved applications\n       // 3. Schedule if there are no reservations\n \n       // Apps may wait for preempted containers\n       // We have to satisfy these first to avoid cases, when we preempt\n       // a container for A from B and C gets the preempted containers,\n       // when C does not qualify for preemption itself.\n       assignPreemptedContainers(node);\n       FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n       boolean validReservation \u003d false;\n       if (reservedAppSchedulable !\u003d null) {\n         validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n       }\n       if (!validReservation) {\n         // No reservation, schedule at queue which is farthest below fair share\n         int assignedContainers \u003d 0;\n         Resource assignedResource \u003d Resources.clone(Resources.none());\n         Resource maxResourcesToAssign \u003d Resources.multiply(\n             node.getUnallocatedResource(), 0.5f);\n \n         while (node.getReservedContainer() \u003d\u003d null) {\n           Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n \n           if (assignment.equals(Resources.none())) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"No container is allocated on node \" + node);\n-            }\n+            LOG.debug(\"No container is allocated on node {}\", node);\n             break;\n           }\n \n           assignedContainers++;\n           Resources.addTo(assignedResource, assignment);\n           if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n               assignedResource)) {\n             break;\n           }\n         }\n       }\n       updateRootQueueMetrics();\n     } finally {\n       writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void attemptScheduling(FSSchedulerNode node) {\n    writeLock.lock();\n    try {\n      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n          .isSchedulerReadyForAllocatingContainers()) {\n        return;\n      }\n\n      final NodeId nodeID \u003d (node !\u003d null ? node.getNodeID() : null);\n      if (!nodeTracker.exists(nodeID)) {\n        // The node might have just been removed while this thread was waiting\n        // on the synchronized lock before it entered this synchronized method\n        LOG.info(\n            \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n        return;\n      }\n\n      // Assign new containers...\n      // 1. Ensure containers are assigned to the apps that preempted\n      // 2. Check for reserved applications\n      // 3. Schedule if there are no reservations\n\n      // Apps may wait for preempted containers\n      // We have to satisfy these first to avoid cases, when we preempt\n      // a container for A from B and C gets the preempted containers,\n      // when C does not qualify for preemption itself.\n      assignPreemptedContainers(node);\n      FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n      boolean validReservation \u003d false;\n      if (reservedAppSchedulable !\u003d null) {\n        validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n      }\n      if (!validReservation) {\n        // No reservation, schedule at queue which is farthest below fair share\n        int assignedContainers \u003d 0;\n        Resource assignedResource \u003d Resources.clone(Resources.none());\n        Resource maxResourcesToAssign \u003d Resources.multiply(\n            node.getUnallocatedResource(), 0.5f);\n\n        while (node.getReservedContainer() \u003d\u003d null) {\n          Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n\n          if (assignment.equals(Resources.none())) {\n            LOG.debug(\"No container is allocated on node {}\", node);\n            break;\n          }\n\n          assignedContainers++;\n          Resources.addTo(assignedResource, assignment);\n          if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n              assignedResource)) {\n            break;\n          }\n        }\n      }\n      updateRootQueueMetrics();\n    } finally {\n      writeLock.unlock();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "ea2c6c8c9a55813a19b3dbd0d29747d6a7739030": {
      "type": "Ybodychange",
      "commitMessage": "YARN-7133. Clean up lock-try order in fair scheduler. (Szilard Nemeth via Haibo Chen)\n",
      "commitDate": "24/07/18 12:46 PM",
      "commitName": "ea2c6c8c9a55813a19b3dbd0d29747d6a7739030",
      "commitAuthor": "Haibo Chen",
      "commitDateOld": "11/06/18 11:12 AM",
      "commitNameOld": "c190ac2be88e574b3322cdc73a7c0af0cef708b2",
      "commitAuthorOld": "Haibo Chen",
      "daysBetweenCommits": 43.07,
      "commitsBetweenForRepo": 266,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,62 @@\n   void attemptScheduling(FSSchedulerNode node) {\n+    writeLock.lock();\n     try {\n-      writeLock.lock();\n       if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n           .isSchedulerReadyForAllocatingContainers()) {\n         return;\n       }\n \n       final NodeId nodeID \u003d (node !\u003d null ? node.getNodeID() : null);\n       if (!nodeTracker.exists(nodeID)) {\n         // The node might have just been removed while this thread was waiting\n         // on the synchronized lock before it entered this synchronized method\n         LOG.info(\n             \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n         return;\n       }\n \n       // Assign new containers...\n       // 1. Ensure containers are assigned to the apps that preempted\n       // 2. Check for reserved applications\n       // 3. Schedule if there are no reservations\n \n       // Apps may wait for preempted containers\n       // We have to satisfy these first to avoid cases, when we preempt\n       // a container for A from B and C gets the preempted containers,\n       // when C does not qualify for preemption itself.\n       assignPreemptedContainers(node);\n       FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n       boolean validReservation \u003d false;\n       if (reservedAppSchedulable !\u003d null) {\n         validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n       }\n       if (!validReservation) {\n         // No reservation, schedule at queue which is farthest below fair share\n         int assignedContainers \u003d 0;\n         Resource assignedResource \u003d Resources.clone(Resources.none());\n         Resource maxResourcesToAssign \u003d Resources.multiply(\n             node.getUnallocatedResource(), 0.5f);\n \n         while (node.getReservedContainer() \u003d\u003d null) {\n           Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n \n           if (assignment.equals(Resources.none())) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"No container is allocated on node \" + node);\n             }\n             break;\n           }\n \n           assignedContainers++;\n           Resources.addTo(assignedResource, assignment);\n           if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n               assignedResource)) {\n             break;\n           }\n         }\n       }\n       updateRootQueueMetrics();\n     } finally {\n       writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void attemptScheduling(FSSchedulerNode node) {\n    writeLock.lock();\n    try {\n      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n          .isSchedulerReadyForAllocatingContainers()) {\n        return;\n      }\n\n      final NodeId nodeID \u003d (node !\u003d null ? node.getNodeID() : null);\n      if (!nodeTracker.exists(nodeID)) {\n        // The node might have just been removed while this thread was waiting\n        // on the synchronized lock before it entered this synchronized method\n        LOG.info(\n            \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n        return;\n      }\n\n      // Assign new containers...\n      // 1. Ensure containers are assigned to the apps that preempted\n      // 2. Check for reserved applications\n      // 3. Schedule if there are no reservations\n\n      // Apps may wait for preempted containers\n      // We have to satisfy these first to avoid cases, when we preempt\n      // a container for A from B and C gets the preempted containers,\n      // when C does not qualify for preemption itself.\n      assignPreemptedContainers(node);\n      FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n      boolean validReservation \u003d false;\n      if (reservedAppSchedulable !\u003d null) {\n        validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n      }\n      if (!validReservation) {\n        // No reservation, schedule at queue which is farthest below fair share\n        int assignedContainers \u003d 0;\n        Resource assignedResource \u003d Resources.clone(Resources.none());\n        Resource maxResourcesToAssign \u003d Resources.multiply(\n            node.getUnallocatedResource(), 0.5f);\n\n        while (node.getReservedContainer() \u003d\u003d null) {\n          Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n\n          if (assignment.equals(Resources.none())) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"No container is allocated on node \" + node);\n            }\n            break;\n          }\n\n          assignedContainers++;\n          Resources.addTo(assignedResource, assignment);\n          if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n              assignedResource)) {\n            break;\n          }\n        }\n      }\n      updateRootQueueMetrics();\n    } finally {\n      writeLock.unlock();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "0cd145a44390bc1a01113dce4be4e629637c3e8a": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4677. RMNodeResourceUpdateEvent update from scheduler can lead to race condition (wilfreds and gphillips via rkanter)\n",
      "commitDate": "04/06/18 3:32 PM",
      "commitName": "0cd145a44390bc1a01113dce4be4e629637c3e8a",
      "commitAuthor": "Robert Kanter",
      "commitDateOld": "04/06/18 9:15 AM",
      "commitNameOld": "ba12f87dcb0e406da57cdd1ad17677ac2367f425",
      "commitAuthorOld": "Haibo Chen",
      "daysBetweenCommits": 0.26,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,62 @@\n   void attemptScheduling(FSSchedulerNode node) {\n     try {\n       writeLock.lock();\n       if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n           .isSchedulerReadyForAllocatingContainers()) {\n         return;\n       }\n \n-      final NodeId nodeID \u003d node.getNodeID();\n+      final NodeId nodeID \u003d (node !\u003d null ? node.getNodeID() : null);\n       if (!nodeTracker.exists(nodeID)) {\n         // The node might have just been removed while this thread was waiting\n         // on the synchronized lock before it entered this synchronized method\n         LOG.info(\n             \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n         return;\n       }\n \n       // Assign new containers...\n       // 1. Ensure containers are assigned to the apps that preempted\n       // 2. Check for reserved applications\n       // 3. Schedule if there are no reservations\n \n       // Apps may wait for preempted containers\n       // We have to satisfy these first to avoid cases, when we preempt\n       // a container for A from B and C gets the preempted containers,\n       // when C does not qualify for preemption itself.\n       assignPreemptedContainers(node);\n       FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n       boolean validReservation \u003d false;\n       if (reservedAppSchedulable !\u003d null) {\n         validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n       }\n       if (!validReservation) {\n         // No reservation, schedule at queue which is farthest below fair share\n         int assignedContainers \u003d 0;\n         Resource assignedResource \u003d Resources.clone(Resources.none());\n         Resource maxResourcesToAssign \u003d Resources.multiply(\n             node.getUnallocatedResource(), 0.5f);\n \n         while (node.getReservedContainer() \u003d\u003d null) {\n           Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n \n           if (assignment.equals(Resources.none())) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"No container is allocated on node \" + node);\n             }\n             break;\n           }\n \n           assignedContainers++;\n           Resources.addTo(assignedResource, assignment);\n           if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n               assignedResource)) {\n             break;\n           }\n         }\n       }\n       updateRootQueueMetrics();\n     } finally {\n       writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void attemptScheduling(FSSchedulerNode node) {\n    try {\n      writeLock.lock();\n      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n          .isSchedulerReadyForAllocatingContainers()) {\n        return;\n      }\n\n      final NodeId nodeID \u003d (node !\u003d null ? node.getNodeID() : null);\n      if (!nodeTracker.exists(nodeID)) {\n        // The node might have just been removed while this thread was waiting\n        // on the synchronized lock before it entered this synchronized method\n        LOG.info(\n            \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n        return;\n      }\n\n      // Assign new containers...\n      // 1. Ensure containers are assigned to the apps that preempted\n      // 2. Check for reserved applications\n      // 3. Schedule if there are no reservations\n\n      // Apps may wait for preempted containers\n      // We have to satisfy these first to avoid cases, when we preempt\n      // a container for A from B and C gets the preempted containers,\n      // when C does not qualify for preemption itself.\n      assignPreemptedContainers(node);\n      FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n      boolean validReservation \u003d false;\n      if (reservedAppSchedulable !\u003d null) {\n        validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n      }\n      if (!validReservation) {\n        // No reservation, schedule at queue which is farthest below fair share\n        int assignedContainers \u003d 0;\n        Resource assignedResource \u003d Resources.clone(Resources.none());\n        Resource maxResourcesToAssign \u003d Resources.multiply(\n            node.getUnallocatedResource(), 0.5f);\n\n        while (node.getReservedContainer() \u003d\u003d null) {\n          Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n\n          if (assignment.equals(Resources.none())) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"No container is allocated on node \" + node);\n            }\n            break;\n          }\n\n          assignedContainers++;\n          Resources.addTo(assignedResource, assignment);\n          if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n              assignedResource)) {\n            break;\n          }\n        }\n      }\n      updateRootQueueMetrics();\n    } finally {\n      writeLock.unlock();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "f48fec83d0f2d1a781a141ad7216463c5526321f": {
      "type": "Ybodychange",
      "commitMessage": "YARN-8248. Job hangs when a job requests a resource that its queue does not have. (Szilard Nemeth via Haibo Chen)\n",
      "commitDate": "21/05/18 8:10 AM",
      "commitName": "f48fec83d0f2d1a781a141ad7216463c5526321f",
      "commitAuthor": "Haibo Chen",
      "commitDateOld": "31/01/18 1:30 AM",
      "commitNameOld": "38af23796971193fa529c3d08ffde8fcd6e607b6",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 110.24,
      "commitsBetweenForRepo": 1383,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,62 @@\n   void attemptScheduling(FSSchedulerNode node) {\n     try {\n       writeLock.lock();\n       if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n           .isSchedulerReadyForAllocatingContainers()) {\n         return;\n       }\n \n       final NodeId nodeID \u003d node.getNodeID();\n       if (!nodeTracker.exists(nodeID)) {\n         // The node might have just been removed while this thread was waiting\n         // on the synchronized lock before it entered this synchronized method\n         LOG.info(\n             \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n         return;\n       }\n \n       // Assign new containers...\n       // 1. Ensure containers are assigned to the apps that preempted\n       // 2. Check for reserved applications\n       // 3. Schedule if there are no reservations\n \n       // Apps may wait for preempted containers\n       // We have to satisfy these first to avoid cases, when we preempt\n       // a container for A from B and C gets the preempted containers,\n       // when C does not qualify for preemption itself.\n       assignPreemptedContainers(node);\n       FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n       boolean validReservation \u003d false;\n       if (reservedAppSchedulable !\u003d null) {\n         validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n       }\n       if (!validReservation) {\n         // No reservation, schedule at queue which is farthest below fair share\n         int assignedContainers \u003d 0;\n         Resource assignedResource \u003d Resources.clone(Resources.none());\n         Resource maxResourcesToAssign \u003d Resources.multiply(\n             node.getUnallocatedResource(), 0.5f);\n+\n         while (node.getReservedContainer() \u003d\u003d null) {\n           Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n+\n           if (assignment.equals(Resources.none())) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"No container is allocated on node \" + node);\n+            }\n             break;\n           }\n \n           assignedContainers++;\n           Resources.addTo(assignedResource, assignment);\n           if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n               assignedResource)) {\n             break;\n           }\n         }\n       }\n       updateRootQueueMetrics();\n     } finally {\n       writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void attemptScheduling(FSSchedulerNode node) {\n    try {\n      writeLock.lock();\n      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n          .isSchedulerReadyForAllocatingContainers()) {\n        return;\n      }\n\n      final NodeId nodeID \u003d node.getNodeID();\n      if (!nodeTracker.exists(nodeID)) {\n        // The node might have just been removed while this thread was waiting\n        // on the synchronized lock before it entered this synchronized method\n        LOG.info(\n            \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n        return;\n      }\n\n      // Assign new containers...\n      // 1. Ensure containers are assigned to the apps that preempted\n      // 2. Check for reserved applications\n      // 3. Schedule if there are no reservations\n\n      // Apps may wait for preempted containers\n      // We have to satisfy these first to avoid cases, when we preempt\n      // a container for A from B and C gets the preempted containers,\n      // when C does not qualify for preemption itself.\n      assignPreemptedContainers(node);\n      FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n      boolean validReservation \u003d false;\n      if (reservedAppSchedulable !\u003d null) {\n        validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n      }\n      if (!validReservation) {\n        // No reservation, schedule at queue which is farthest below fair share\n        int assignedContainers \u003d 0;\n        Resource assignedResource \u003d Resources.clone(Resources.none());\n        Resource maxResourcesToAssign \u003d Resources.multiply(\n            node.getUnallocatedResource(), 0.5f);\n\n        while (node.getReservedContainer() \u003d\u003d null) {\n          Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n\n          if (assignment.equals(Resources.none())) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"No container is allocated on node \" + node);\n            }\n            break;\n          }\n\n          assignedContainers++;\n          Resources.addTo(assignedResource, assignment);\n          if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n              assignedResource)) {\n            break;\n          }\n        }\n      }\n      updateRootQueueMetrics();\n    } finally {\n      writeLock.unlock();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "3de47ab5ea5cb75805a03010dc72e11b0cf6c173": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6764. Simplify the logic in FairScheduler#attemptScheduling. Contributed by Yufei Gu.\n",
      "commitDate": "09/07/17 4:09 PM",
      "commitName": "3de47ab5ea5cb75805a03010dc72e11b0cf6c173",
      "commitAuthor": "Yufei Gu",
      "commitDateOld": "31/05/17 3:48 PM",
      "commitNameOld": "d5b71e4175c13679d451710be150fc461a661263",
      "commitAuthorOld": "Daniel Templeton",
      "daysBetweenCommits": 39.01,
      "commitsBetweenForRepo": 177,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,57 @@\n   void attemptScheduling(FSSchedulerNode node) {\n     try {\n       writeLock.lock();\n       if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n           .isSchedulerReadyForAllocatingContainers()) {\n         return;\n       }\n \n       final NodeId nodeID \u003d node.getNodeID();\n       if (!nodeTracker.exists(nodeID)) {\n         // The node might have just been removed while this thread was waiting\n         // on the synchronized lock before it entered this synchronized method\n         LOG.info(\n             \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n         return;\n       }\n \n       // Assign new containers...\n       // 1. Ensure containers are assigned to the apps that preempted\n       // 2. Check for reserved applications\n       // 3. Schedule if there are no reservations\n \n       // Apps may wait for preempted containers\n       // We have to satisfy these first to avoid cases, when we preempt\n       // a container for A from B and C gets the preempted containers,\n       // when C does not qualify for preemption itself.\n       assignPreemptedContainers(node);\n       FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n       boolean validReservation \u003d false;\n       if (reservedAppSchedulable !\u003d null) {\n         validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n       }\n       if (!validReservation) {\n         // No reservation, schedule at queue which is farthest below fair share\n         int assignedContainers \u003d 0;\n         Resource assignedResource \u003d Resources.clone(Resources.none());\n         Resource maxResourcesToAssign \u003d Resources.multiply(\n             node.getUnallocatedResource(), 0.5f);\n         while (node.getReservedContainer() \u003d\u003d null) {\n-          boolean assignedContainer \u003d false;\n           Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n-          if (!assignment.equals(Resources.none())) {\n-            assignedContainers++;\n-            assignedContainer \u003d true;\n-            Resources.addTo(assignedResource, assignment);\n-          }\n-          if (!assignedContainer) {\n+          if (assignment.equals(Resources.none())) {\n             break;\n           }\n+\n+          assignedContainers++;\n+          Resources.addTo(assignedResource, assignment);\n           if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n               assignedResource)) {\n             break;\n           }\n         }\n       }\n       updateRootQueueMetrics();\n     } finally {\n       writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void attemptScheduling(FSSchedulerNode node) {\n    try {\n      writeLock.lock();\n      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n          .isSchedulerReadyForAllocatingContainers()) {\n        return;\n      }\n\n      final NodeId nodeID \u003d node.getNodeID();\n      if (!nodeTracker.exists(nodeID)) {\n        // The node might have just been removed while this thread was waiting\n        // on the synchronized lock before it entered this synchronized method\n        LOG.info(\n            \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n        return;\n      }\n\n      // Assign new containers...\n      // 1. Ensure containers are assigned to the apps that preempted\n      // 2. Check for reserved applications\n      // 3. Schedule if there are no reservations\n\n      // Apps may wait for preempted containers\n      // We have to satisfy these first to avoid cases, when we preempt\n      // a container for A from B and C gets the preempted containers,\n      // when C does not qualify for preemption itself.\n      assignPreemptedContainers(node);\n      FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n      boolean validReservation \u003d false;\n      if (reservedAppSchedulable !\u003d null) {\n        validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n      }\n      if (!validReservation) {\n        // No reservation, schedule at queue which is farthest below fair share\n        int assignedContainers \u003d 0;\n        Resource assignedResource \u003d Resources.clone(Resources.none());\n        Resource maxResourcesToAssign \u003d Resources.multiply(\n            node.getUnallocatedResource(), 0.5f);\n        while (node.getReservedContainer() \u003d\u003d null) {\n          Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n          if (assignment.equals(Resources.none())) {\n            break;\n          }\n\n          assignedContainers++;\n          Resources.addTo(assignedResource, assignment);\n          if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n              assignedResource)) {\n            break;\n          }\n        }\n      }\n      updateRootQueueMetrics();\n    } finally {\n      writeLock.unlock();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "c3375175d616e0380560f89d491b6b9753a8f3e1": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6432. FairScheduler: Reserve preempted resources for corresponding applications. (Miklos Szegedi via kasha)\n",
      "commitDate": "12/04/17 2:21 PM",
      "commitName": "c3375175d616e0380560f89d491b6b9753a8f3e1",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "05/04/17 3:43 PM",
      "commitNameOld": "b4c4f365948d36b36942f912ef994c1c21ba59e3",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 6.94,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,60 @@\n   void attemptScheduling(FSSchedulerNode node) {\n     try {\n       writeLock.lock();\n       if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n           .isSchedulerReadyForAllocatingContainers()) {\n         return;\n       }\n \n       final NodeId nodeID \u003d node.getNodeID();\n       if (!nodeTracker.exists(nodeID)) {\n         // The node might have just been removed while this thread was waiting\n         // on the synchronized lock before it entered this synchronized method\n         LOG.info(\n             \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n         return;\n       }\n \n       // Assign new containers...\n-      // 1. Check for reserved applications\n-      // 2. Schedule if there are no reservations\n+      // 1. Ensure containers are assigned to the apps that preempted\n+      // 2. Check for reserved applications\n+      // 3. Schedule if there are no reservations\n \n-      boolean validReservation \u003d false;\n+      // Apps may wait for preempted containers\n+      // We have to satisfy these first to avoid cases, when we preempt\n+      // a container for A from B and C gets the preempted containers,\n+      // when C does not qualify for preemption itself.\n+      assignPreemptedContainers(node);\n       FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n+      boolean validReservation \u003d false;\n       if (reservedAppSchedulable !\u003d null) {\n         validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n       }\n       if (!validReservation) {\n         // No reservation, schedule at queue which is farthest below fair share\n         int assignedContainers \u003d 0;\n         Resource assignedResource \u003d Resources.clone(Resources.none());\n         Resource maxResourcesToAssign \u003d Resources.multiply(\n             node.getUnallocatedResource(), 0.5f);\n         while (node.getReservedContainer() \u003d\u003d null) {\n           boolean assignedContainer \u003d false;\n           Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n           if (!assignment.equals(Resources.none())) {\n             assignedContainers++;\n             assignedContainer \u003d true;\n             Resources.addTo(assignedResource, assignment);\n           }\n           if (!assignedContainer) {\n             break;\n           }\n           if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n               assignedResource)) {\n             break;\n           }\n         }\n       }\n       updateRootQueueMetrics();\n     } finally {\n       writeLock.unlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void attemptScheduling(FSSchedulerNode node) {\n    try {\n      writeLock.lock();\n      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n          .isSchedulerReadyForAllocatingContainers()) {\n        return;\n      }\n\n      final NodeId nodeID \u003d node.getNodeID();\n      if (!nodeTracker.exists(nodeID)) {\n        // The node might have just been removed while this thread was waiting\n        // on the synchronized lock before it entered this synchronized method\n        LOG.info(\n            \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n        return;\n      }\n\n      // Assign new containers...\n      // 1. Ensure containers are assigned to the apps that preempted\n      // 2. Check for reserved applications\n      // 3. Schedule if there are no reservations\n\n      // Apps may wait for preempted containers\n      // We have to satisfy these first to avoid cases, when we preempt\n      // a container for A from B and C gets the preempted containers,\n      // when C does not qualify for preemption itself.\n      assignPreemptedContainers(node);\n      FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n      boolean validReservation \u003d false;\n      if (reservedAppSchedulable !\u003d null) {\n        validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n      }\n      if (!validReservation) {\n        // No reservation, schedule at queue which is farthest below fair share\n        int assignedContainers \u003d 0;\n        Resource assignedResource \u003d Resources.clone(Resources.none());\n        Resource maxResourcesToAssign \u003d Resources.multiply(\n            node.getUnallocatedResource(), 0.5f);\n        while (node.getReservedContainer() \u003d\u003d null) {\n          boolean assignedContainer \u003d false;\n          Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n          if (!assignment.equals(Resources.none())) {\n            assignedContainers++;\n            assignedContainer \u003d true;\n            Resources.addTo(assignedResource, assignment);\n          }\n          if (!assignedContainer) {\n            break;\n          }\n          if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n              assignedResource)) {\n            break;\n          }\n        }\n      }\n      updateRootQueueMetrics();\n    } finally {\n      writeLock.unlock();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "31f8da22d0b8d2dcce5fbc8e45d832f40acf056f": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "YARN-3139. Improve locks in AbstractYarnScheduler/CapacityScheduler/FairScheduler. Contributed by Wangda Tan\n",
      "commitDate": "04/10/16 5:23 PM",
      "commitName": "31f8da22d0b8d2dcce5fbc8e45d832f40acf056f",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "YARN-3139. Improve locks in AbstractYarnScheduler/CapacityScheduler/FairScheduler. Contributed by Wangda Tan\n",
          "commitDate": "04/10/16 5:23 PM",
          "commitName": "31f8da22d0b8d2dcce5fbc8e45d832f40acf056f",
          "commitAuthor": "Jian He",
          "commitDateOld": "03/10/16 6:03 AM",
          "commitNameOld": "0da54e8848764c71a31473516d23ada582013f8c",
          "commitAuthorOld": "Karthik Kambatla",
          "daysBetweenCommits": 1.47,
          "commitsBetweenForRepo": 14,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,54 @@\n-  synchronized void attemptScheduling(FSSchedulerNode node) {\n-    if (rmContext.isWorkPreservingRecoveryEnabled()\n-        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n-      return;\n-    }\n+  void attemptScheduling(FSSchedulerNode node) {\n+    try {\n+      writeLock.lock();\n+      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n+          .isSchedulerReadyForAllocatingContainers()) {\n+        return;\n+      }\n \n-    final NodeId nodeID \u003d node.getNodeID();\n-    if (!nodeTracker.exists(nodeID)) {\n-      // The node might have just been removed while this thread was waiting\n-      // on the synchronized lock before it entered this synchronized method\n-      LOG.info(\"Skipping scheduling as the node \" + nodeID +\n-          \" has been removed\");\n-      return;\n-    }\n+      final NodeId nodeID \u003d node.getNodeID();\n+      if (!nodeTracker.exists(nodeID)) {\n+        // The node might have just been removed while this thread was waiting\n+        // on the synchronized lock before it entered this synchronized method\n+        LOG.info(\n+            \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n+        return;\n+      }\n \n-    // Assign new containers...\n-    // 1. Check for reserved applications\n-    // 2. Schedule if there are no reservations\n+      // Assign new containers...\n+      // 1. Check for reserved applications\n+      // 2. Schedule if there are no reservations\n \n-    boolean validReservation \u003d false;\n-    FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n-    if (reservedAppSchedulable !\u003d null) {\n-      validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n-    }\n-    if (!validReservation) {\n-      // No reservation, schedule at queue which is farthest below fair share\n-      int assignedContainers \u003d 0;\n-      Resource assignedResource \u003d Resources.clone(Resources.none());\n-      Resource maxResourcesToAssign \u003d\n-          Resources.multiply(node.getUnallocatedResource(), 0.5f);\n-      while (node.getReservedContainer() \u003d\u003d null) {\n-        boolean assignedContainer \u003d false;\n-        Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n-        if (!assignment.equals(Resources.none())) {\n-          assignedContainers++;\n-          assignedContainer \u003d true;\n-          Resources.addTo(assignedResource, assignment);\n-        }\n-        if (!assignedContainer) { break; }\n-        if (!shouldContinueAssigning(assignedContainers,\n-            maxResourcesToAssign, assignedResource)) {\n-          break;\n+      boolean validReservation \u003d false;\n+      FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n+      if (reservedAppSchedulable !\u003d null) {\n+        validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n+      }\n+      if (!validReservation) {\n+        // No reservation, schedule at queue which is farthest below fair share\n+        int assignedContainers \u003d 0;\n+        Resource assignedResource \u003d Resources.clone(Resources.none());\n+        Resource maxResourcesToAssign \u003d Resources.multiply(\n+            node.getUnallocatedResource(), 0.5f);\n+        while (node.getReservedContainer() \u003d\u003d null) {\n+          boolean assignedContainer \u003d false;\n+          Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n+          if (!assignment.equals(Resources.none())) {\n+            assignedContainers++;\n+            assignedContainer \u003d true;\n+            Resources.addTo(assignedResource, assignment);\n+          }\n+          if (!assignedContainer) {\n+            break;\n+          }\n+          if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n+              assignedResource)) {\n+            break;\n+          }\n         }\n       }\n+      updateRootQueueMetrics();\n+    } finally {\n+      writeLock.unlock();\n     }\n-    updateRootQueueMetrics();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void attemptScheduling(FSSchedulerNode node) {\n    try {\n      writeLock.lock();\n      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n          .isSchedulerReadyForAllocatingContainers()) {\n        return;\n      }\n\n      final NodeId nodeID \u003d node.getNodeID();\n      if (!nodeTracker.exists(nodeID)) {\n        // The node might have just been removed while this thread was waiting\n        // on the synchronized lock before it entered this synchronized method\n        LOG.info(\n            \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n        return;\n      }\n\n      // Assign new containers...\n      // 1. Check for reserved applications\n      // 2. Schedule if there are no reservations\n\n      boolean validReservation \u003d false;\n      FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n      if (reservedAppSchedulable !\u003d null) {\n        validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n      }\n      if (!validReservation) {\n        // No reservation, schedule at queue which is farthest below fair share\n        int assignedContainers \u003d 0;\n        Resource assignedResource \u003d Resources.clone(Resources.none());\n        Resource maxResourcesToAssign \u003d Resources.multiply(\n            node.getUnallocatedResource(), 0.5f);\n        while (node.getReservedContainer() \u003d\u003d null) {\n          boolean assignedContainer \u003d false;\n          Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n          if (!assignment.equals(Resources.none())) {\n            assignedContainers++;\n            assignedContainer \u003d true;\n            Resources.addTo(assignedResource, assignment);\n          }\n          if (!assignedContainer) {\n            break;\n          }\n          if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n              assignedResource)) {\n            break;\n          }\n        }\n      }\n      updateRootQueueMetrics();\n    } finally {\n      writeLock.unlock();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
          "extendedDetails": {
            "oldValue": "[synchronized]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3139. Improve locks in AbstractYarnScheduler/CapacityScheduler/FairScheduler. Contributed by Wangda Tan\n",
          "commitDate": "04/10/16 5:23 PM",
          "commitName": "31f8da22d0b8d2dcce5fbc8e45d832f40acf056f",
          "commitAuthor": "Jian He",
          "commitDateOld": "03/10/16 6:03 AM",
          "commitNameOld": "0da54e8848764c71a31473516d23ada582013f8c",
          "commitAuthorOld": "Karthik Kambatla",
          "daysBetweenCommits": 1.47,
          "commitsBetweenForRepo": 14,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,54 @@\n-  synchronized void attemptScheduling(FSSchedulerNode node) {\n-    if (rmContext.isWorkPreservingRecoveryEnabled()\n-        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n-      return;\n-    }\n+  void attemptScheduling(FSSchedulerNode node) {\n+    try {\n+      writeLock.lock();\n+      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n+          .isSchedulerReadyForAllocatingContainers()) {\n+        return;\n+      }\n \n-    final NodeId nodeID \u003d node.getNodeID();\n-    if (!nodeTracker.exists(nodeID)) {\n-      // The node might have just been removed while this thread was waiting\n-      // on the synchronized lock before it entered this synchronized method\n-      LOG.info(\"Skipping scheduling as the node \" + nodeID +\n-          \" has been removed\");\n-      return;\n-    }\n+      final NodeId nodeID \u003d node.getNodeID();\n+      if (!nodeTracker.exists(nodeID)) {\n+        // The node might have just been removed while this thread was waiting\n+        // on the synchronized lock before it entered this synchronized method\n+        LOG.info(\n+            \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n+        return;\n+      }\n \n-    // Assign new containers...\n-    // 1. Check for reserved applications\n-    // 2. Schedule if there are no reservations\n+      // Assign new containers...\n+      // 1. Check for reserved applications\n+      // 2. Schedule if there are no reservations\n \n-    boolean validReservation \u003d false;\n-    FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n-    if (reservedAppSchedulable !\u003d null) {\n-      validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n-    }\n-    if (!validReservation) {\n-      // No reservation, schedule at queue which is farthest below fair share\n-      int assignedContainers \u003d 0;\n-      Resource assignedResource \u003d Resources.clone(Resources.none());\n-      Resource maxResourcesToAssign \u003d\n-          Resources.multiply(node.getUnallocatedResource(), 0.5f);\n-      while (node.getReservedContainer() \u003d\u003d null) {\n-        boolean assignedContainer \u003d false;\n-        Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n-        if (!assignment.equals(Resources.none())) {\n-          assignedContainers++;\n-          assignedContainer \u003d true;\n-          Resources.addTo(assignedResource, assignment);\n-        }\n-        if (!assignedContainer) { break; }\n-        if (!shouldContinueAssigning(assignedContainers,\n-            maxResourcesToAssign, assignedResource)) {\n-          break;\n+      boolean validReservation \u003d false;\n+      FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n+      if (reservedAppSchedulable !\u003d null) {\n+        validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n+      }\n+      if (!validReservation) {\n+        // No reservation, schedule at queue which is farthest below fair share\n+        int assignedContainers \u003d 0;\n+        Resource assignedResource \u003d Resources.clone(Resources.none());\n+        Resource maxResourcesToAssign \u003d Resources.multiply(\n+            node.getUnallocatedResource(), 0.5f);\n+        while (node.getReservedContainer() \u003d\u003d null) {\n+          boolean assignedContainer \u003d false;\n+          Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n+          if (!assignment.equals(Resources.none())) {\n+            assignedContainers++;\n+            assignedContainer \u003d true;\n+            Resources.addTo(assignedResource, assignment);\n+          }\n+          if (!assignedContainer) {\n+            break;\n+          }\n+          if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n+              assignedResource)) {\n+            break;\n+          }\n         }\n       }\n+      updateRootQueueMetrics();\n+    } finally {\n+      writeLock.unlock();\n     }\n-    updateRootQueueMetrics();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void attemptScheduling(FSSchedulerNode node) {\n    try {\n      writeLock.lock();\n      if (rmContext.isWorkPreservingRecoveryEnabled() \u0026\u0026 !rmContext\n          .isSchedulerReadyForAllocatingContainers()) {\n        return;\n      }\n\n      final NodeId nodeID \u003d node.getNodeID();\n      if (!nodeTracker.exists(nodeID)) {\n        // The node might have just been removed while this thread was waiting\n        // on the synchronized lock before it entered this synchronized method\n        LOG.info(\n            \"Skipping scheduling as the node \" + nodeID + \" has been removed\");\n        return;\n      }\n\n      // Assign new containers...\n      // 1. Check for reserved applications\n      // 2. Schedule if there are no reservations\n\n      boolean validReservation \u003d false;\n      FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n      if (reservedAppSchedulable !\u003d null) {\n        validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n      }\n      if (!validReservation) {\n        // No reservation, schedule at queue which is farthest below fair share\n        int assignedContainers \u003d 0;\n        Resource assignedResource \u003d Resources.clone(Resources.none());\n        Resource maxResourcesToAssign \u003d Resources.multiply(\n            node.getUnallocatedResource(), 0.5f);\n        while (node.getReservedContainer() \u003d\u003d null) {\n          boolean assignedContainer \u003d false;\n          Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n          if (!assignment.equals(Resources.none())) {\n            assignedContainers++;\n            assignedContainer \u003d true;\n            Resources.addTo(assignedResource, assignment);\n          }\n          if (!assignedContainer) {\n            break;\n          }\n          if (!shouldContinueAssigning(assignedContainers, maxResourcesToAssign,\n              assignedResource)) {\n            break;\n          }\n        }\n      }\n      updateRootQueueMetrics();\n    } finally {\n      writeLock.unlock();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
          "extendedDetails": {}
        }
      ]
    },
    "04ded558b03ee0fbf68a611cf1f25508b4447e44": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5035. FairScheduler: Adjust maxAssign dynamically when assignMultiple is turned on. (kasha)\n",
      "commitDate": "26/05/16 2:41 PM",
      "commitName": "04ded558b03ee0fbf68a611cf1f25508b4447e44",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "05/05/16 12:56 PM",
      "commitNameOld": "bb62e0592566b2fcae7136b30972aad2d3ac55b0",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 21.07,
      "commitsBetweenForRepo": 155,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,47 @@\n   synchronized void attemptScheduling(FSSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n \n     final NodeId nodeID \u003d node.getNodeID();\n     if (!nodeTracker.exists(nodeID)) {\n       // The node might have just been removed while this thread was waiting\n       // on the synchronized lock before it entered this synchronized method\n       LOG.info(\"Skipping scheduling as the node \" + nodeID +\n           \" has been removed\");\n       return;\n     }\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     boolean validReservation \u003d false;\n     FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n     if (reservedAppSchedulable !\u003d null) {\n       validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n     }\n     if (!validReservation) {\n       // No reservation, schedule at queue which is farthest below fair share\n       int assignedContainers \u003d 0;\n+      Resource assignedResource \u003d Resources.clone(Resources.none());\n+      Resource maxResourcesToAssign \u003d\n+          Resources.multiply(node.getUnallocatedResource(), 0.5f);\n       while (node.getReservedContainer() \u003d\u003d null) {\n         boolean assignedContainer \u003d false;\n-        if (!queueMgr.getRootQueue().assignContainer(node).equals(\n-            Resources.none())) {\n+        Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n+        if (!assignment.equals(Resources.none())) {\n           assignedContainers++;\n           assignedContainer \u003d true;\n+          Resources.addTo(assignedResource, assignment);\n         }\n         if (!assignedContainer) { break; }\n-        if (!assignMultiple) { break; }\n-        if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n+        if (!shouldContinueAssigning(assignedContainers,\n+            maxResourcesToAssign, assignedResource)) {\n+          break;\n+        }\n       }\n     }\n     updateRootQueueMetrics();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void attemptScheduling(FSSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    final NodeId nodeID \u003d node.getNodeID();\n    if (!nodeTracker.exists(nodeID)) {\n      // The node might have just been removed while this thread was waiting\n      // on the synchronized lock before it entered this synchronized method\n      LOG.info(\"Skipping scheduling as the node \" + nodeID +\n          \" has been removed\");\n      return;\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    boolean validReservation \u003d false;\n    FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n    if (reservedAppSchedulable !\u003d null) {\n      validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n    }\n    if (!validReservation) {\n      // No reservation, schedule at queue which is farthest below fair share\n      int assignedContainers \u003d 0;\n      Resource assignedResource \u003d Resources.clone(Resources.none());\n      Resource maxResourcesToAssign \u003d\n          Resources.multiply(node.getUnallocatedResource(), 0.5f);\n      while (node.getReservedContainer() \u003d\u003d null) {\n        boolean assignedContainer \u003d false;\n        Resource assignment \u003d queueMgr.getRootQueue().assignContainer(node);\n        if (!assignment.equals(Resources.none())) {\n          assignedContainers++;\n          assignedContainer \u003d true;\n          Resources.addTo(assignedResource, assignment);\n        }\n        if (!assignedContainer) { break; }\n        if (!shouldContinueAssigning(assignedContainers,\n            maxResourcesToAssign, assignedResource)) {\n          break;\n        }\n      }\n    }\n    updateRootQueueMetrics();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "20d389ce61eaacb5ddfb329015f50e96ad894f8d": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4719. Add a helper library to maintain node state and allows common queries. (kasha)\n",
      "commitDate": "14/03/16 2:19 PM",
      "commitName": "20d389ce61eaacb5ddfb329015f50e96ad894f8d",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "06/03/16 7:46 PM",
      "commitNameOld": "e1ccc9622b2f1fbefea1862fa74d1fb56d8eb264",
      "commitAuthorOld": "Zhihai Xu",
      "daysBetweenCommits": 7.73,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   synchronized void attemptScheduling(FSSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n \n     final NodeId nodeID \u003d node.getNodeID();\n-    if (!nodes.containsKey(nodeID)) {\n+    if (!nodeTracker.exists(nodeID)) {\n       // The node might have just been removed while this thread was waiting\n       // on the synchronized lock before it entered this synchronized method\n       LOG.info(\"Skipping scheduling as the node \" + nodeID +\n           \" has been removed\");\n       return;\n     }\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     boolean validReservation \u003d false;\n     FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n     if (reservedAppSchedulable !\u003d null) {\n       validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n     }\n     if (!validReservation) {\n       // No reservation, schedule at queue which is farthest below fair share\n       int assignedContainers \u003d 0;\n       while (node.getReservedContainer() \u003d\u003d null) {\n         boolean assignedContainer \u003d false;\n         if (!queueMgr.getRootQueue().assignContainer(node).equals(\n             Resources.none())) {\n           assignedContainers++;\n           assignedContainer \u003d true;\n         }\n         if (!assignedContainer) { break; }\n         if (!assignMultiple) { break; }\n         if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n       }\n     }\n     updateRootQueueMetrics();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void attemptScheduling(FSSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    final NodeId nodeID \u003d node.getNodeID();\n    if (!nodeTracker.exists(nodeID)) {\n      // The node might have just been removed while this thread was waiting\n      // on the synchronized lock before it entered this synchronized method\n      LOG.info(\"Skipping scheduling as the node \" + nodeID +\n          \" has been removed\");\n      return;\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    boolean validReservation \u003d false;\n    FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n    if (reservedAppSchedulable !\u003d null) {\n      validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n    }\n    if (!validReservation) {\n      // No reservation, schedule at queue which is farthest below fair share\n      int assignedContainers \u003d 0;\n      while (node.getReservedContainer() \u003d\u003d null) {\n        boolean assignedContainer \u003d false;\n        if (!queueMgr.getRootQueue().assignContainer(node).equals(\n            Resources.none())) {\n          assignedContainers++;\n          assignedContainer \u003d true;\n        }\n        if (!assignedContainer) { break; }\n        if (!assignMultiple) { break; }\n        if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n      }\n    }\n    updateRootQueueMetrics();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "bd69ea408f8fdd8293836ce1089fe9b01616f2f7": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3655. FairScheduler: potential livelock due to maxAMShare limitation and container reservation. (Zhihai Xu via kasha)\n",
      "commitDate": "07/06/15 11:37 AM",
      "commitName": "bd69ea408f8fdd8293836ce1089fe9b01616f2f7",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "05/06/15 9:39 AM",
      "commitNameOld": "75885852cc19dd6de12e62498b112d5d70ce87f4",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 2.08,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,41 @@\n   synchronized void attemptScheduling(FSSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n \n     final NodeId nodeID \u003d node.getNodeID();\n     if (!nodes.containsKey(nodeID)) {\n       // The node might have just been removed while this thread was waiting\n       // on the synchronized lock before it entered this synchronized method\n       LOG.info(\"Skipping scheduling as the node \" + nodeID +\n           \" has been removed\");\n       return;\n     }\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n+    boolean validReservation \u003d false;\n     FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n     if (reservedAppSchedulable !\u003d null) {\n-      Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n-      FSQueue queue \u003d reservedAppSchedulable.getQueue();\n-\n-      if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)\n-          || !fitsInMaxShare(queue,\n-          node.getReservedContainer().getReservedResource())) {\n-        // Don\u0027t hold the reservation if app can no longer use it\n-        LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n-            + reservedAppSchedulable.getApplicationAttemptId()\n-            + \" on node \" + node);\n-        reservedAppSchedulable.unreserve(reservedPriority, node);\n-        reservedAppSchedulable \u003d null;\n-      } else {\n-        // Reservation exists; try to fulfill the reservation\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Trying to fulfill reservation for application \"\n-              + reservedAppSchedulable.getApplicationAttemptId()\n-              + \" on node: \" + node);\n-        }\n-        node.getReservedAppSchedulable().assignReservedContainer(node);\n-      }\n+      validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n     }\n-    if (reservedAppSchedulable \u003d\u003d null) {\n+    if (!validReservation) {\n       // No reservation, schedule at queue which is farthest below fair share\n       int assignedContainers \u003d 0;\n       while (node.getReservedContainer() \u003d\u003d null) {\n         boolean assignedContainer \u003d false;\n         if (!queueMgr.getRootQueue().assignContainer(node).equals(\n             Resources.none())) {\n           assignedContainers++;\n           assignedContainer \u003d true;\n         }\n         if (!assignedContainer) { break; }\n         if (!assignMultiple) { break; }\n         if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n       }\n     }\n     updateRootQueueMetrics();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void attemptScheduling(FSSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    final NodeId nodeID \u003d node.getNodeID();\n    if (!nodes.containsKey(nodeID)) {\n      // The node might have just been removed while this thread was waiting\n      // on the synchronized lock before it entered this synchronized method\n      LOG.info(\"Skipping scheduling as the node \" + nodeID +\n          \" has been removed\");\n      return;\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    boolean validReservation \u003d false;\n    FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n    if (reservedAppSchedulable !\u003d null) {\n      validReservation \u003d reservedAppSchedulable.assignReservedContainer(node);\n    }\n    if (!validReservation) {\n      // No reservation, schedule at queue which is farthest below fair share\n      int assignedContainers \u003d 0;\n      while (node.getReservedContainer() \u003d\u003d null) {\n        boolean assignedContainer \u003d false;\n        if (!queueMgr.getRootQueue().assignContainer(node).equals(\n            Resources.none())) {\n          assignedContainers++;\n          assignedContainer \u003d true;\n        }\n        if (!assignedContainer) { break; }\n        if (!assignMultiple) { break; }\n        if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n      }\n    }\n    updateRootQueueMetrics();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "4513761869c732cf2f462763043067ebf8749df7": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "YARN-3675. FairScheduler: RM quits when node removal races with continuous-scheduling on the same node. (Anubhav Dhoot via kasha)\n",
      "commitDate": "21/05/15 1:44 PM",
      "commitName": "4513761869c732cf2f462763043067ebf8749df7",
      "commitAuthor": "Karthik Kambatla",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "YARN-3675. FairScheduler: RM quits when node removal races with continuous-scheduling on the same node. (Anubhav Dhoot via kasha)\n",
          "commitDate": "21/05/15 1:44 PM",
          "commitName": "4513761869c732cf2f462763043067ebf8749df7",
          "commitAuthor": "Karthik Kambatla",
          "commitDateOld": "21/04/15 1:47 PM",
          "commitNameOld": "105afd54779852c518b978101f23526143e234a5",
          "commitAuthorOld": "Tsuyoshi Ozawa",
          "daysBetweenCommits": 30.0,
          "commitsBetweenForRepo": 358,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,60 @@\n-  private synchronized void attemptScheduling(FSSchedulerNode node) {\n+  synchronized void attemptScheduling(FSSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n \n+    final NodeId nodeID \u003d node.getNodeID();\n+    if (!nodes.containsKey(nodeID)) {\n+      // The node might have just been removed while this thread was waiting\n+      // on the synchronized lock before it entered this synchronized method\n+      LOG.info(\"Skipping scheduling as the node \" + nodeID +\n+          \" has been removed\");\n+      return;\n+    }\n+\n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n     if (reservedAppSchedulable !\u003d null) {\n       Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n       FSQueue queue \u003d reservedAppSchedulable.getQueue();\n \n       if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)\n           || !fitsInMaxShare(queue,\n           node.getReservedContainer().getReservedResource())) {\n         // Don\u0027t hold the reservation if app can no longer use it\n         LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n             + reservedAppSchedulable.getApplicationAttemptId()\n             + \" on node \" + node);\n         reservedAppSchedulable.unreserve(reservedPriority, node);\n         reservedAppSchedulable \u003d null;\n       } else {\n         // Reservation exists; try to fulfill the reservation\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to fulfill reservation for application \"\n               + reservedAppSchedulable.getApplicationAttemptId()\n               + \" on node: \" + node);\n         }\n         node.getReservedAppSchedulable().assignReservedContainer(node);\n       }\n     }\n     if (reservedAppSchedulable \u003d\u003d null) {\n       // No reservation, schedule at queue which is farthest below fair share\n       int assignedContainers \u003d 0;\n       while (node.getReservedContainer() \u003d\u003d null) {\n         boolean assignedContainer \u003d false;\n         if (!queueMgr.getRootQueue().assignContainer(node).equals(\n             Resources.none())) {\n           assignedContainers++;\n           assignedContainer \u003d true;\n         }\n         if (!assignedContainer) { break; }\n         if (!assignMultiple) { break; }\n         if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n       }\n     }\n     updateRootQueueMetrics();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized void attemptScheduling(FSSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    final NodeId nodeID \u003d node.getNodeID();\n    if (!nodes.containsKey(nodeID)) {\n      // The node might have just been removed while this thread was waiting\n      // on the synchronized lock before it entered this synchronized method\n      LOG.info(\"Skipping scheduling as the node \" + nodeID +\n          \" has been removed\");\n      return;\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n    if (reservedAppSchedulable !\u003d null) {\n      Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n      FSQueue queue \u003d reservedAppSchedulable.getQueue();\n\n      if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)\n          || !fitsInMaxShare(queue,\n          node.getReservedContainer().getReservedResource())) {\n        // Don\u0027t hold the reservation if app can no longer use it\n        LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n            + reservedAppSchedulable.getApplicationAttemptId()\n            + \" on node \" + node);\n        reservedAppSchedulable.unreserve(reservedPriority, node);\n        reservedAppSchedulable \u003d null;\n      } else {\n        // Reservation exists; try to fulfill the reservation\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to fulfill reservation for application \"\n              + reservedAppSchedulable.getApplicationAttemptId()\n              + \" on node: \" + node);\n        }\n        node.getReservedAppSchedulable().assignReservedContainer(node);\n      }\n    }\n    if (reservedAppSchedulable \u003d\u003d null) {\n      // No reservation, schedule at queue which is farthest below fair share\n      int assignedContainers \u003d 0;\n      while (node.getReservedContainer() \u003d\u003d null) {\n        boolean assignedContainer \u003d false;\n        if (!queueMgr.getRootQueue().assignContainer(node).equals(\n            Resources.none())) {\n          assignedContainers++;\n          assignedContainer \u003d true;\n        }\n        if (!assignedContainer) { break; }\n        if (!assignMultiple) { break; }\n        if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n      }\n    }\n    updateRootQueueMetrics();\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
          "extendedDetails": {
            "oldValue": "[private, synchronized]",
            "newValue": "[synchronized]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3675. FairScheduler: RM quits when node removal races with continuous-scheduling on the same node. (Anubhav Dhoot via kasha)\n",
          "commitDate": "21/05/15 1:44 PM",
          "commitName": "4513761869c732cf2f462763043067ebf8749df7",
          "commitAuthor": "Karthik Kambatla",
          "commitDateOld": "21/04/15 1:47 PM",
          "commitNameOld": "105afd54779852c518b978101f23526143e234a5",
          "commitAuthorOld": "Tsuyoshi Ozawa",
          "daysBetweenCommits": 30.0,
          "commitsBetweenForRepo": 358,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,60 @@\n-  private synchronized void attemptScheduling(FSSchedulerNode node) {\n+  synchronized void attemptScheduling(FSSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n \n+    final NodeId nodeID \u003d node.getNodeID();\n+    if (!nodes.containsKey(nodeID)) {\n+      // The node might have just been removed while this thread was waiting\n+      // on the synchronized lock before it entered this synchronized method\n+      LOG.info(\"Skipping scheduling as the node \" + nodeID +\n+          \" has been removed\");\n+      return;\n+    }\n+\n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n     if (reservedAppSchedulable !\u003d null) {\n       Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n       FSQueue queue \u003d reservedAppSchedulable.getQueue();\n \n       if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)\n           || !fitsInMaxShare(queue,\n           node.getReservedContainer().getReservedResource())) {\n         // Don\u0027t hold the reservation if app can no longer use it\n         LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n             + reservedAppSchedulable.getApplicationAttemptId()\n             + \" on node \" + node);\n         reservedAppSchedulable.unreserve(reservedPriority, node);\n         reservedAppSchedulable \u003d null;\n       } else {\n         // Reservation exists; try to fulfill the reservation\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to fulfill reservation for application \"\n               + reservedAppSchedulable.getApplicationAttemptId()\n               + \" on node: \" + node);\n         }\n         node.getReservedAppSchedulable().assignReservedContainer(node);\n       }\n     }\n     if (reservedAppSchedulable \u003d\u003d null) {\n       // No reservation, schedule at queue which is farthest below fair share\n       int assignedContainers \u003d 0;\n       while (node.getReservedContainer() \u003d\u003d null) {\n         boolean assignedContainer \u003d false;\n         if (!queueMgr.getRootQueue().assignContainer(node).equals(\n             Resources.none())) {\n           assignedContainers++;\n           assignedContainer \u003d true;\n         }\n         if (!assignedContainer) { break; }\n         if (!assignMultiple) { break; }\n         if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n       }\n     }\n     updateRootQueueMetrics();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized void attemptScheduling(FSSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    final NodeId nodeID \u003d node.getNodeID();\n    if (!nodes.containsKey(nodeID)) {\n      // The node might have just been removed while this thread was waiting\n      // on the synchronized lock before it entered this synchronized method\n      LOG.info(\"Skipping scheduling as the node \" + nodeID +\n          \" has been removed\");\n      return;\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n    if (reservedAppSchedulable !\u003d null) {\n      Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n      FSQueue queue \u003d reservedAppSchedulable.getQueue();\n\n      if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)\n          || !fitsInMaxShare(queue,\n          node.getReservedContainer().getReservedResource())) {\n        // Don\u0027t hold the reservation if app can no longer use it\n        LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n            + reservedAppSchedulable.getApplicationAttemptId()\n            + \" on node \" + node);\n        reservedAppSchedulable.unreserve(reservedPriority, node);\n        reservedAppSchedulable \u003d null;\n      } else {\n        // Reservation exists; try to fulfill the reservation\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to fulfill reservation for application \"\n              + reservedAppSchedulable.getApplicationAttemptId()\n              + \" on node: \" + node);\n        }\n        node.getReservedAppSchedulable().assignReservedContainer(node);\n      }\n    }\n    if (reservedAppSchedulable \u003d\u003d null) {\n      // No reservation, schedule at queue which is farthest below fair share\n      int assignedContainers \u003d 0;\n      while (node.getReservedContainer() \u003d\u003d null) {\n        boolean assignedContainer \u003d false;\n        if (!queueMgr.getRootQueue().assignContainer(node).equals(\n            Resources.none())) {\n          assignedContainers++;\n          assignedContainer \u003d true;\n        }\n        if (!assignedContainer) { break; }\n        if (!assignMultiple) { break; }\n        if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n      }\n    }\n    updateRootQueueMetrics();\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
          "extendedDetails": {}
        }
      ]
    },
    "b6466deac6d5d6344f693144290b46e2bef83a02": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3101. In Fair Scheduler, fix canceling of reservations for exceeding max share (Anubhav Dhoot via Sandy Ryza)\n",
      "commitDate": "05/02/15 9:39 AM",
      "commitName": "b6466deac6d5d6344f693144290b46e2bef83a02",
      "commitAuthor": "Sandy Ryza",
      "commitDateOld": "12/01/15 5:51 PM",
      "commitNameOld": "51881535e659940b1b332d0c5952ee1f9958cc7f",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 23.66,
      "commitsBetweenForRepo": 190,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,51 @@\n   private synchronized void attemptScheduling(FSSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n     if (reservedAppSchedulable !\u003d null) {\n       Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n       FSQueue queue \u003d reservedAppSchedulable.getQueue();\n \n       if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)\n-          || !fitInMaxShare(queue)) {\n+          || !fitsInMaxShare(queue,\n+          node.getReservedContainer().getReservedResource())) {\n         // Don\u0027t hold the reservation if app can no longer use it\n         LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n             + reservedAppSchedulable.getApplicationAttemptId()\n             + \" on node \" + node);\n         reservedAppSchedulable.unreserve(reservedPriority, node);\n         reservedAppSchedulable \u003d null;\n       } else {\n         // Reservation exists; try to fulfill the reservation\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to fulfill reservation for application \"\n               + reservedAppSchedulable.getApplicationAttemptId()\n               + \" on node: \" + node);\n         }\n         node.getReservedAppSchedulable().assignReservedContainer(node);\n       }\n     }\n     if (reservedAppSchedulable \u003d\u003d null) {\n       // No reservation, schedule at queue which is farthest below fair share\n       int assignedContainers \u003d 0;\n       while (node.getReservedContainer() \u003d\u003d null) {\n         boolean assignedContainer \u003d false;\n         if (!queueMgr.getRootQueue().assignContainer(node).equals(\n             Resources.none())) {\n           assignedContainers++;\n           assignedContainer \u003d true;\n         }\n         if (!assignedContainer) { break; }\n         if (!assignMultiple) { break; }\n         if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n       }\n     }\n     updateRootQueueMetrics();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void attemptScheduling(FSSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n    if (reservedAppSchedulable !\u003d null) {\n      Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n      FSQueue queue \u003d reservedAppSchedulable.getQueue();\n\n      if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)\n          || !fitsInMaxShare(queue,\n          node.getReservedContainer().getReservedResource())) {\n        // Don\u0027t hold the reservation if app can no longer use it\n        LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n            + reservedAppSchedulable.getApplicationAttemptId()\n            + \" on node \" + node);\n        reservedAppSchedulable.unreserve(reservedPriority, node);\n        reservedAppSchedulable \u003d null;\n      } else {\n        // Reservation exists; try to fulfill the reservation\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to fulfill reservation for application \"\n              + reservedAppSchedulable.getApplicationAttemptId()\n              + \" on node: \" + node);\n        }\n        node.getReservedAppSchedulable().assignReservedContainer(node);\n      }\n    }\n    if (reservedAppSchedulable \u003d\u003d null) {\n      // No reservation, schedule at queue which is farthest below fair share\n      int assignedContainers \u003d 0;\n      while (node.getReservedContainer() \u003d\u003d null) {\n        boolean assignedContainer \u003d false;\n        if (!queueMgr.getRootQueue().assignContainer(node).equals(\n            Resources.none())) {\n          assignedContainers++;\n          assignedContainer \u003d true;\n        }\n        if (!assignedContainer) { break; }\n        if (!assignMultiple) { break; }\n        if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n      }\n    }\n    updateRootQueueMetrics();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "1a47f890ba3cb22b6262f47c1f1af2990559bb89": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2811. In Fair Scheduler, reservation fulfillments shouldn\u0027t ignore max share (Siqi Li via Sandy Ryza)\n",
      "commitDate": "14/11/14 3:18 PM",
      "commitName": "1a47f890ba3cb22b6262f47c1f1af2990559bb89",
      "commitAuthor": "Sandy Ryza",
      "commitDateOld": "30/10/14 12:29 AM",
      "commitNameOld": "179cab81e0bde1af0cba6131ccccf16ff127358a",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 15.66,
      "commitsBetweenForRepo": 173,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,50 @@\n   private synchronized void attemptScheduling(FSSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n \n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n     if (reservedAppSchedulable !\u003d null) {\n       Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n-      if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)) {\n+      FSQueue queue \u003d reservedAppSchedulable.getQueue();\n+\n+      if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)\n+          || !fitInMaxShare(queue)) {\n         // Don\u0027t hold the reservation if app can no longer use it\n         LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n             + reservedAppSchedulable.getApplicationAttemptId()\n             + \" on node \" + node);\n         reservedAppSchedulable.unreserve(reservedPriority, node);\n         reservedAppSchedulable \u003d null;\n       } else {\n         // Reservation exists; try to fulfill the reservation\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to fulfill reservation for application \"\n               + reservedAppSchedulable.getApplicationAttemptId()\n               + \" on node: \" + node);\n         }\n-        \n         node.getReservedAppSchedulable().assignReservedContainer(node);\n       }\n     }\n     if (reservedAppSchedulable \u003d\u003d null) {\n       // No reservation, schedule at queue which is farthest below fair share\n       int assignedContainers \u003d 0;\n       while (node.getReservedContainer() \u003d\u003d null) {\n         boolean assignedContainer \u003d false;\n         if (!queueMgr.getRootQueue().assignContainer(node).equals(\n             Resources.none())) {\n           assignedContainers++;\n           assignedContainer \u003d true;\n         }\n         if (!assignedContainer) { break; }\n         if (!assignMultiple) { break; }\n         if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n       }\n     }\n     updateRootQueueMetrics();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void attemptScheduling(FSSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n    if (reservedAppSchedulable !\u003d null) {\n      Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n      FSQueue queue \u003d reservedAppSchedulable.getQueue();\n\n      if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)\n          || !fitInMaxShare(queue)) {\n        // Don\u0027t hold the reservation if app can no longer use it\n        LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n            + reservedAppSchedulable.getApplicationAttemptId()\n            + \" on node \" + node);\n        reservedAppSchedulable.unreserve(reservedPriority, node);\n        reservedAppSchedulable \u003d null;\n      } else {\n        // Reservation exists; try to fulfill the reservation\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to fulfill reservation for application \"\n              + reservedAppSchedulable.getApplicationAttemptId()\n              + \" on node: \" + node);\n        }\n        node.getReservedAppSchedulable().assignReservedContainer(node);\n      }\n    }\n    if (reservedAppSchedulable \u003d\u003d null) {\n      // No reservation, schedule at queue which is farthest below fair share\n      int assignedContainers \u003d 0;\n      while (node.getReservedContainer() \u003d\u003d null) {\n        boolean assignedContainer \u003d false;\n        if (!queueMgr.getRootQueue().assignContainer(node).equals(\n            Resources.none())) {\n          assignedContainers++;\n          assignedContainer \u003d true;\n        }\n        if (!assignedContainer) { break; }\n        if (!assignMultiple) { break; }\n        if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n      }\n    }\n    updateRootQueueMetrics();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "485c96e3cb9b0b05d6e490b4773506da83ebc61d": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2001. Added a time threshold for RM to wait before starting container allocations after restart/failover. Contributed by Jian He.\n",
      "commitDate": "18/09/14 11:03 AM",
      "commitName": "485c96e3cb9b0b05d6e490b4773506da83ebc61d",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "10/09/14 10:15 AM",
      "commitNameOld": "b67d5ba7842cc10695d987f217027848a5a8c3d8",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 8.03,
      "commitsBetweenForRepo": 79,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,48 @@\n   private synchronized void attemptScheduling(FSSchedulerNode node) {\n+    if (rmContext.isWorkPreservingRecoveryEnabled()\n+        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n+      return;\n+    }\n+\n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n     if (reservedAppSchedulable !\u003d null) {\n       Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n       if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)) {\n         // Don\u0027t hold the reservation if app can no longer use it\n         LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n             + reservedAppSchedulable.getApplicationAttemptId()\n             + \" on node \" + node);\n         reservedAppSchedulable.unreserve(reservedPriority, node);\n         reservedAppSchedulable \u003d null;\n       } else {\n         // Reservation exists; try to fulfill the reservation\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to fulfill reservation for application \"\n               + reservedAppSchedulable.getApplicationAttemptId()\n               + \" on node: \" + node);\n         }\n         \n         node.getReservedAppSchedulable().assignReservedContainer(node);\n       }\n     }\n     if (reservedAppSchedulable \u003d\u003d null) {\n       // No reservation, schedule at queue which is farthest below fair share\n       int assignedContainers \u003d 0;\n       while (node.getReservedContainer() \u003d\u003d null) {\n         boolean assignedContainer \u003d false;\n         if (!queueMgr.getRootQueue().assignContainer(node).equals(\n             Resources.none())) {\n           assignedContainers++;\n           assignedContainer \u003d true;\n         }\n         if (!assignedContainer) { break; }\n         if (!assignMultiple) { break; }\n         if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n       }\n     }\n     updateRootQueueMetrics();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void attemptScheduling(FSSchedulerNode node) {\n    if (rmContext.isWorkPreservingRecoveryEnabled()\n        \u0026\u0026 !rmContext.isSchedulerReadyForAllocatingContainers()) {\n      return;\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n    if (reservedAppSchedulable !\u003d null) {\n      Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n      if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)) {\n        // Don\u0027t hold the reservation if app can no longer use it\n        LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n            + reservedAppSchedulable.getApplicationAttemptId()\n            + \" on node \" + node);\n        reservedAppSchedulable.unreserve(reservedPriority, node);\n        reservedAppSchedulable \u003d null;\n      } else {\n        // Reservation exists; try to fulfill the reservation\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to fulfill reservation for application \"\n              + reservedAppSchedulable.getApplicationAttemptId()\n              + \" on node: \" + node);\n        }\n        \n        node.getReservedAppSchedulable().assignReservedContainer(node);\n      }\n    }\n    if (reservedAppSchedulable \u003d\u003d null) {\n      // No reservation, schedule at queue which is farthest below fair share\n      int assignedContainers \u003d 0;\n      while (node.getReservedContainer() \u003d\u003d null) {\n        boolean assignedContainer \u003d false;\n        if (!queueMgr.getRootQueue().assignContainer(node).equals(\n            Resources.none())) {\n          assignedContainers++;\n          assignedContainer \u003d true;\n        }\n        if (!assignedContainer) { break; }\n        if (!assignMultiple) { break; }\n        if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n      }\n    }\n    updateRootQueueMetrics();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "486e718fc1f5befd231494e2ec06bb360484f191": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2399. FairScheduler: Merge AppSchedulable and FSSchedulerApp into FSAppAttempt. (kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1617600 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/08/14 2:43 PM",
      "commitName": "486e718fc1f5befd231494e2ec06bb360484f191",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "08/08/14 7:17 AM",
      "commitNameOld": "14864e9c7c879c15b5fa2d1776614ec83152918f",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 4.31,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,43 @@\n   private synchronized void attemptScheduling(FSSchedulerNode node) {\n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n-    AppSchedulable reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n+    FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n     if (reservedAppSchedulable !\u003d null) {\n       Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n       if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)) {\n         // Don\u0027t hold the reservation if app can no longer use it\n         LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n-            + reservedAppSchedulable.getApp().getApplicationAttemptId()\n+            + reservedAppSchedulable.getApplicationAttemptId()\n             + \" on node \" + node);\n         reservedAppSchedulable.unreserve(reservedPriority, node);\n         reservedAppSchedulable \u003d null;\n       } else {\n         // Reservation exists; try to fulfill the reservation\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to fulfill reservation for application \"\n-              + reservedAppSchedulable.getApp().getApplicationAttemptId()\n+              + reservedAppSchedulable.getApplicationAttemptId()\n               + \" on node: \" + node);\n         }\n         \n         node.getReservedAppSchedulable().assignReservedContainer(node);\n       }\n     }\n     if (reservedAppSchedulable \u003d\u003d null) {\n       // No reservation, schedule at queue which is farthest below fair share\n       int assignedContainers \u003d 0;\n       while (node.getReservedContainer() \u003d\u003d null) {\n         boolean assignedContainer \u003d false;\n         if (!queueMgr.getRootQueue().assignContainer(node).equals(\n             Resources.none())) {\n           assignedContainers++;\n           assignedContainer \u003d true;\n         }\n         if (!assignedContainer) { break; }\n         if (!assignMultiple) { break; }\n         if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n       }\n     }\n     updateRootQueueMetrics();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void attemptScheduling(FSSchedulerNode node) {\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    FSAppAttempt reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n    if (reservedAppSchedulable !\u003d null) {\n      Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n      if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)) {\n        // Don\u0027t hold the reservation if app can no longer use it\n        LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n            + reservedAppSchedulable.getApplicationAttemptId()\n            + \" on node \" + node);\n        reservedAppSchedulable.unreserve(reservedPriority, node);\n        reservedAppSchedulable \u003d null;\n      } else {\n        // Reservation exists; try to fulfill the reservation\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to fulfill reservation for application \"\n              + reservedAppSchedulable.getApplicationAttemptId()\n              + \" on node: \" + node);\n        }\n        \n        node.getReservedAppSchedulable().assignReservedContainer(node);\n      }\n    }\n    if (reservedAppSchedulable \u003d\u003d null) {\n      // No reservation, schedule at queue which is farthest below fair share\n      int assignedContainers \u003d 0;\n      while (node.getReservedContainer() \u003d\u003d null) {\n        boolean assignedContainer \u003d false;\n        if (!queueMgr.getRootQueue().assignContainer(node).equals(\n            Resources.none())) {\n          assignedContainers++;\n          assignedContainer \u003d true;\n        }\n        if (!assignedContainer) { break; }\n        if (!assignMultiple) { break; }\n        if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n      }\n    }\n    updateRootQueueMetrics();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "29c102cad01b8a91cbf5173ca49d2e1ed8a45eee": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2111. In FairScheduler.attemptScheduling, we don\u0027t count containers as assigned if they have 0 memory but non-zero cores (Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1605113 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/06/14 8:40 AM",
      "commitName": "29c102cad01b8a91cbf5173ca49d2e1ed8a45eee",
      "commitAuthor": "Sanford Ryza",
      "commitDateOld": "12/06/14 2:23 PM",
      "commitNameOld": "4bc91b44c990c9eb060293853573a363b25ccea3",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 11.76,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,43 @@\n   private synchronized void attemptScheduling(FSSchedulerNode node) {\n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     AppSchedulable reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n     if (reservedAppSchedulable !\u003d null) {\n       Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n       if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)) {\n         // Don\u0027t hold the reservation if app can no longer use it\n         LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n             + reservedAppSchedulable.getApp().getApplicationAttemptId()\n             + \" on node \" + node);\n         reservedAppSchedulable.unreserve(reservedPriority, node);\n         reservedAppSchedulable \u003d null;\n       } else {\n         // Reservation exists; try to fulfill the reservation\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to fulfill reservation for application \"\n               + reservedAppSchedulable.getApp().getApplicationAttemptId()\n               + \" on node: \" + node);\n         }\n         \n         node.getReservedAppSchedulable().assignReservedContainer(node);\n       }\n     }\n     if (reservedAppSchedulable \u003d\u003d null) {\n       // No reservation, schedule at queue which is farthest below fair share\n       int assignedContainers \u003d 0;\n       while (node.getReservedContainer() \u003d\u003d null) {\n         boolean assignedContainer \u003d false;\n-        if (Resources.greaterThan(RESOURCE_CALCULATOR, clusterResource,\n-              queueMgr.getRootQueue().assignContainer(node),\n-              Resources.none())) {\n+        if (!queueMgr.getRootQueue().assignContainer(node).equals(\n+            Resources.none())) {\n           assignedContainers++;\n           assignedContainer \u003d true;\n         }\n         if (!assignedContainer) { break; }\n         if (!assignMultiple) { break; }\n         if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n       }\n     }\n     updateRootQueueMetrics();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void attemptScheduling(FSSchedulerNode node) {\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    AppSchedulable reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n    if (reservedAppSchedulable !\u003d null) {\n      Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n      if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)) {\n        // Don\u0027t hold the reservation if app can no longer use it\n        LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n            + reservedAppSchedulable.getApp().getApplicationAttemptId()\n            + \" on node \" + node);\n        reservedAppSchedulable.unreserve(reservedPriority, node);\n        reservedAppSchedulable \u003d null;\n      } else {\n        // Reservation exists; try to fulfill the reservation\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to fulfill reservation for application \"\n              + reservedAppSchedulable.getApp().getApplicationAttemptId()\n              + \" on node: \" + node);\n        }\n        \n        node.getReservedAppSchedulable().assignReservedContainer(node);\n      }\n    }\n    if (reservedAppSchedulable \u003d\u003d null) {\n      // No reservation, schedule at queue which is farthest below fair share\n      int assignedContainers \u003d 0;\n      while (node.getReservedContainer() \u003d\u003d null) {\n        boolean assignedContainer \u003d false;\n        if (!queueMgr.getRootQueue().assignContainer(node).equals(\n            Resources.none())) {\n          assignedContainers++;\n          assignedContainer \u003d true;\n        }\n        if (!assignedContainer) { break; }\n        if (!assignMultiple) { break; }\n        if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n      }\n    }\n    updateRootQueueMetrics();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "82f3454f5ac1f1c457e668e2cee12b4dcc800ee1": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2017. Merged some of the common scheduler code. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1596753 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/05/14 10:32 PM",
      "commitName": "82f3454f5ac1f1c457e668e2cee12b4dcc800ee1",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "31/03/14 4:40 PM",
      "commitNameOld": "7bd62b8da03642612fae8349e967b9c0aa290843",
      "commitAuthorOld": "Sanford Ryza",
      "daysBetweenCommits": 51.24,
      "commitsBetweenForRepo": 299,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,44 @@\n   private synchronized void attemptScheduling(FSSchedulerNode node) {\n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     AppSchedulable reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n     if (reservedAppSchedulable !\u003d null) {\n       Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n       if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)) {\n         // Don\u0027t hold the reservation if app can no longer use it\n         LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n             + reservedAppSchedulable.getApp().getApplicationAttemptId()\n             + \" on node \" + node);\n         reservedAppSchedulable.unreserve(reservedPriority, node);\n         reservedAppSchedulable \u003d null;\n       } else {\n         // Reservation exists; try to fulfill the reservation\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Trying to fulfill reservation for application \"\n               + reservedAppSchedulable.getApp().getApplicationAttemptId()\n               + \" on node: \" + node);\n         }\n         \n         node.getReservedAppSchedulable().assignReservedContainer(node);\n       }\n     }\n     if (reservedAppSchedulable \u003d\u003d null) {\n       // No reservation, schedule at queue which is farthest below fair share\n       int assignedContainers \u003d 0;\n       while (node.getReservedContainer() \u003d\u003d null) {\n         boolean assignedContainer \u003d false;\n-        if (Resources.greaterThan(RESOURCE_CALCULATOR, clusterCapacity,\n+        if (Resources.greaterThan(RESOURCE_CALCULATOR, clusterResource,\n               queueMgr.getRootQueue().assignContainer(node),\n               Resources.none())) {\n           assignedContainers++;\n           assignedContainer \u003d true;\n         }\n         if (!assignedContainer) { break; }\n         if (!assignMultiple) { break; }\n         if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n       }\n     }\n     updateRootQueueMetrics();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void attemptScheduling(FSSchedulerNode node) {\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    AppSchedulable reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n    if (reservedAppSchedulable !\u003d null) {\n      Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n      if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)) {\n        // Don\u0027t hold the reservation if app can no longer use it\n        LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n            + reservedAppSchedulable.getApp().getApplicationAttemptId()\n            + \" on node \" + node);\n        reservedAppSchedulable.unreserve(reservedPriority, node);\n        reservedAppSchedulable \u003d null;\n      } else {\n        // Reservation exists; try to fulfill the reservation\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to fulfill reservation for application \"\n              + reservedAppSchedulable.getApp().getApplicationAttemptId()\n              + \" on node: \" + node);\n        }\n        \n        node.getReservedAppSchedulable().assignReservedContainer(node);\n      }\n    }\n    if (reservedAppSchedulable \u003d\u003d null) {\n      // No reservation, schedule at queue which is farthest below fair share\n      int assignedContainers \u003d 0;\n      while (node.getReservedContainer() \u003d\u003d null) {\n        boolean assignedContainer \u003d false;\n        if (Resources.greaterThan(RESOURCE_CALCULATOR, clusterResource,\n              queueMgr.getRootQueue().assignContainer(node),\n              Resources.none())) {\n          assignedContainers++;\n          assignedContainer \u003d true;\n        }\n        if (!assignedContainer) { break; }\n        if (!assignMultiple) { break; }\n        if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n      }\n    }\n    updateRootQueueMetrics();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "1ab2f5a916a8e942e01b08685caa6c79ad3e1107": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1678. Fair scheduler gabs incessantly about reservations (Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1571468 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/02/14 2:35 PM",
      "commitName": "1ab2f5a916a8e942e01b08685caa6c79ad3e1107",
      "commitAuthor": "Sanford Ryza",
      "commitDateOld": "19/02/14 3:39 PM",
      "commitNameOld": "5fd5c9900cfd299428acbc8dff767273e44647c0",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 4.96,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,44 @@\n   private synchronized void attemptScheduling(FSSchedulerNode node) {\n     // Assign new containers...\n     // 1. Check for reserved applications\n     // 2. Schedule if there are no reservations\n \n     AppSchedulable reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n     if (reservedAppSchedulable !\u003d null) {\n       Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n       if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)) {\n         // Don\u0027t hold the reservation if app can no longer use it\n         LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n             + reservedAppSchedulable.getApp().getApplicationAttemptId()\n             + \" on node \" + node);\n         reservedAppSchedulable.unreserve(reservedPriority, node);\n         reservedAppSchedulable \u003d null;\n       } else {\n         // Reservation exists; try to fulfill the reservation\n-        LOG.info(\"Trying to fulfill reservation for application \"\n-            + reservedAppSchedulable.getApp().getApplicationAttemptId()\n-            + \" on node: \" + node);\n-\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Trying to fulfill reservation for application \"\n+              + reservedAppSchedulable.getApp().getApplicationAttemptId()\n+              + \" on node: \" + node);\n+        }\n+        \n         node.getReservedAppSchedulable().assignReservedContainer(node);\n       }\n     }\n     if (reservedAppSchedulable \u003d\u003d null) {\n       // No reservation, schedule at queue which is farthest below fair share\n       int assignedContainers \u003d 0;\n       while (node.getReservedContainer() \u003d\u003d null) {\n         boolean assignedContainer \u003d false;\n         if (Resources.greaterThan(RESOURCE_CALCULATOR, clusterCapacity,\n               queueMgr.getRootQueue().assignContainer(node),\n               Resources.none())) {\n           assignedContainers++;\n           assignedContainer \u003d true;\n         }\n         if (!assignedContainer) { break; }\n         if (!assignMultiple) { break; }\n         if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n       }\n     }\n     updateRootQueueMetrics();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void attemptScheduling(FSSchedulerNode node) {\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    AppSchedulable reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n    if (reservedAppSchedulable !\u003d null) {\n      Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n      if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)) {\n        // Don\u0027t hold the reservation if app can no longer use it\n        LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n            + reservedAppSchedulable.getApp().getApplicationAttemptId()\n            + \" on node \" + node);\n        reservedAppSchedulable.unreserve(reservedPriority, node);\n        reservedAppSchedulable \u003d null;\n      } else {\n        // Reservation exists; try to fulfill the reservation\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trying to fulfill reservation for application \"\n              + reservedAppSchedulable.getApp().getApplicationAttemptId()\n              + \" on node: \" + node);\n        }\n        \n        node.getReservedAppSchedulable().assignReservedContainer(node);\n      }\n    }\n    if (reservedAppSchedulable \u003d\u003d null) {\n      // No reservation, schedule at queue which is farthest below fair share\n      int assignedContainers \u003d 0;\n      while (node.getReservedContainer() \u003d\u003d null) {\n        boolean assignedContainer \u003d false;\n        if (Resources.greaterThan(RESOURCE_CALCULATOR, clusterCapacity,\n              queueMgr.getRootQueue().assignContainer(node),\n              Resources.none())) {\n          assignedContainers++;\n          assignedContainer \u003d true;\n        }\n        if (!assignedContainer) { break; }\n        if (!assignMultiple) { break; }\n        if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n      }\n    }\n    updateRootQueueMetrics();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "extendedDetails": {}
    },
    "ae05623a75803d4e12a902ac4a24187540f56699": {
      "type": "Yintroduced",
      "commitMessage": "YARN-1010. FairScheduler: decouple container scheduling from nodemanager heartbeats. (Wei Yan via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1528192 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/10/13 12:54 PM",
      "commitName": "ae05623a75803d4e12a902ac4a24187540f56699",
      "commitAuthor": "Sanford Ryza",
      "diff": "@@ -0,0 +1,42 @@\n+  private synchronized void attemptScheduling(FSSchedulerNode node) {\n+    // Assign new containers...\n+    // 1. Check for reserved applications\n+    // 2. Schedule if there are no reservations\n+\n+    AppSchedulable reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n+    if (reservedAppSchedulable !\u003d null) {\n+      Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n+      if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)) {\n+        // Don\u0027t hold the reservation if app can no longer use it\n+        LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n+            + reservedAppSchedulable.getApp().getApplicationAttemptId()\n+            + \" on node \" + node);\n+        reservedAppSchedulable.unreserve(reservedPriority, node);\n+        reservedAppSchedulable \u003d null;\n+      } else {\n+        // Reservation exists; try to fulfill the reservation\n+        LOG.info(\"Trying to fulfill reservation for application \"\n+            + reservedAppSchedulable.getApp().getApplicationAttemptId()\n+            + \" on node: \" + node);\n+\n+        node.getReservedAppSchedulable().assignReservedContainer(node);\n+      }\n+    }\n+    if (reservedAppSchedulable \u003d\u003d null) {\n+      // No reservation, schedule at queue which is farthest below fair share\n+      int assignedContainers \u003d 0;\n+      while (node.getReservedContainer() \u003d\u003d null) {\n+        boolean assignedContainer \u003d false;\n+        if (Resources.greaterThan(RESOURCE_CALCULATOR, clusterCapacity,\n+              queueMgr.getRootQueue().assignContainer(node),\n+              Resources.none())) {\n+          assignedContainers++;\n+          assignedContainer \u003d true;\n+        }\n+        if (!assignedContainer) { break; }\n+        if (!assignMultiple) { break; }\n+        if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n+      }\n+    }\n+    updateRootQueueMetrics();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void attemptScheduling(FSSchedulerNode node) {\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    AppSchedulable reservedAppSchedulable \u003d node.getReservedAppSchedulable();\n    if (reservedAppSchedulable !\u003d null) {\n      Priority reservedPriority \u003d node.getReservedContainer().getReservedPriority();\n      if (!reservedAppSchedulable.hasContainerForNode(reservedPriority, node)) {\n        // Don\u0027t hold the reservation if app can no longer use it\n        LOG.info(\"Releasing reservation that cannot be satisfied for application \"\n            + reservedAppSchedulable.getApp().getApplicationAttemptId()\n            + \" on node \" + node);\n        reservedAppSchedulable.unreserve(reservedPriority, node);\n        reservedAppSchedulable \u003d null;\n      } else {\n        // Reservation exists; try to fulfill the reservation\n        LOG.info(\"Trying to fulfill reservation for application \"\n            + reservedAppSchedulable.getApp().getApplicationAttemptId()\n            + \" on node: \" + node);\n\n        node.getReservedAppSchedulable().assignReservedContainer(node);\n      }\n    }\n    if (reservedAppSchedulable \u003d\u003d null) {\n      // No reservation, schedule at queue which is farthest below fair share\n      int assignedContainers \u003d 0;\n      while (node.getReservedContainer() \u003d\u003d null) {\n        boolean assignedContainer \u003d false;\n        if (Resources.greaterThan(RESOURCE_CALCULATOR, clusterCapacity,\n              queueMgr.getRootQueue().assignContainer(node),\n              Resources.none())) {\n          assignedContainers++;\n          assignedContainer \u003d true;\n        }\n        if (!assignedContainer) { break; }\n        if (!assignMultiple) { break; }\n        if ((assignedContainers \u003e\u003d maxAssign) \u0026\u0026 (maxAssign \u003e 0)) { break; }\n      }\n    }\n    updateRootQueueMetrics();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java"
    }
  }
}