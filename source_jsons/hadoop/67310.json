{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Folder.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/Folder.java",
  "functionStartLine": 220,
  "functionEndLine": 499,
  "numCommitsSeen": 6,
  "timeTaken": 4725,
  "changeHistory": [
    "10325d97329c214bb3899c8535df5a366bc86d2f",
    "a238f931ea7dce0ca620d1798156c84ff77097ff",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "10325d97329c214bb3899c8535df5a366bc86d2f": "Yfilerename",
    "a238f931ea7dce0ca620d1798156c84ff77097ff": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "10325d97329c214bb3899c8535df5a366bc86d2f": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-3582. Move successfully passing MR1 tests to MR2 maven tree.(ahmed via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1233090 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/01/12 2:10 PM",
      "commitName": "10325d97329c214bb3899c8535df5a366bc86d2f",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "18/01/12 10:20 AM",
      "commitNameOld": "8b2f6909ec7df5cffb5ef417f5c9cffdee43e38a",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.16,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public int run() throws IOException {\n    class JobEntryComparator implements\n        Comparator\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e {\n      public int compare(Pair\u003cLoggedJob, JobTraceReader\u003e p1,\n          Pair\u003cLoggedJob, JobTraceReader\u003e p2) {\n        LoggedJob j1 \u003d p1.first();\n        LoggedJob j2 \u003d p2.first();\n\n        return (j1.getSubmitTime() \u003c j2.getSubmitTime()) ? -1 : (j1\n            .getSubmitTime() \u003d\u003d j2.getSubmitTime()) ? 0 : 1;\n      }\n    }\n\n    // we initialize an empty heap so if we take an error before establishing\n    // a real one the finally code goes through\n    Queue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e heap \u003d\n        new PriorityQueue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e();\n\n    try {\n      LoggedJob job \u003d reader.nextJob();\n\n      if (job \u003d\u003d null) {\n        LOG.error(\"The job trace is empty\");\n\n        return EMPTY_JOB_TRACE;\n      }\n      \n      // If starts-after time is specified, skip the number of jobs till we reach\n      // the starting time limit.\n      if (startsAfter \u003e 0) {\n        LOG.info(\"starts-after time is specified. Initial job submit time : \" \n                 + job.getSubmitTime());\n\n        long approximateTime \u003d job.getSubmitTime() + startsAfter;\n        job \u003d reader.nextJob();\n        long skippedCount \u003d 0;\n        while (job !\u003d null \u0026\u0026 job.getSubmitTime() \u003c approximateTime) {\n          job \u003d reader.nextJob();\n          skippedCount++;\n        }\n\n        LOG.debug(\"Considering jobs with submit time greater than \" \n                  + startsAfter + \" ms. Skipped \" + skippedCount + \" jobs.\");\n\n        if (job \u003d\u003d null) {\n          LOG.error(\"No more jobs to process in the trace with \u0027starts-after\u0027\"+\n                    \" set to \" + startsAfter + \"ms.\");\n          return EMPTY_JOB_TRACE;\n        }\n        LOG.info(\"The first job has a submit time of \" + job.getSubmitTime());\n      }\n\n      firstJobSubmitTime \u003d job.getSubmitTime();\n      long lastJobSubmitTime \u003d firstJobSubmitTime;\n\n      int numberJobs \u003d 0;\n\n      long currentIntervalEnd \u003d Long.MIN_VALUE;\n\n      Path nextSegment \u003d null;\n      Outputter\u003cLoggedJob\u003e tempGen \u003d null;\n\n      if (debug) {\n        LOG.debug(\"The first job has a submit time of \" + firstJobSubmitTime);\n      }\n\n      final Configuration conf \u003d getConf();\n\n      try {\n        // At the top of this loop, skewBuffer has at most\n        // skewBufferLength entries.\n        while (job !\u003d null) {\n          final Random tempNameGenerator \u003d new Random();\n\n          lastJobSubmitTime \u003d job.getSubmitTime();\n\n          ++numberJobs;\n\n          if (job.getSubmitTime() \u003e\u003d currentIntervalEnd) {\n            if (tempGen !\u003d null) {\n              tempGen.close();\n            }\n            \n            nextSegment \u003d null;\n            for (int i \u003d 0; i \u003c 3 \u0026\u0026 nextSegment \u003d\u003d null; ++i) {\n              try {\n                nextSegment \u003d\n                    new Path(tempDir, \"segment-\" + tempNameGenerator.nextLong()\n                        + \".json.gz\");\n\n                if (debug) {\n                  LOG.debug(\"The next segment name is \" + nextSegment);\n                }\n\n                FileSystem fs \u003d nextSegment.getFileSystem(conf);\n\n                try {\n                  if (!fs.exists(nextSegment)) {\n                    break;\n                  }\n\n                  continue;\n                } catch (IOException e) {\n                  // no code -- file did not already exist\n                }\n              } catch (IOException e) {\n                // no code -- file exists now, or directory bad. We try three\n                // times.\n              }\n            }\n\n            if (nextSegment \u003d\u003d null) {\n              throw new RuntimeException(\"Failed to create a new file!\");\n            }\n            \n            if (debug) {\n              LOG.debug(\"Creating \" + nextSegment\n                  + \" for a job with a submit time of \" + job.getSubmitTime());\n            }\n\n            deletees.add(nextSegment);\n\n            tempPaths.add(nextSegment);\n\n            tempGen \u003d new DefaultOutputter\u003cLoggedJob\u003e();\n            tempGen.init(nextSegment, conf);\n\n            long currentIntervalNumber \u003d\n                (job.getSubmitTime() - firstJobSubmitTime) / inputCycle;\n\n            currentIntervalEnd \u003d\n                firstJobSubmitTime + ((currentIntervalNumber + 1) * inputCycle);\n          }\n\n          // the temp files contain UDadjusted times, but each temp file\u0027s\n          // content is in the same input cycle interval.\n          if (tempGen !\u003d null) {\n            tempGen.output(job);\n          }\n\n          job \u003d reader.nextJob();\n        }\n      } catch (DeskewedJobTraceReader.OutOfOrderException e) {\n        return OUT_OF_ORDER_JOBS;\n      } finally {\n        if (tempGen !\u003d null) {\n          tempGen.close();\n        }\n      }\n\n      if (lastJobSubmitTime \u003c\u003d firstJobSubmitTime) {\n        LOG.error(\"All of your job[s] have the same submit time.\"\n            + \"  Please just use your input file.\");\n\n        return ALL_JOBS_SIMULTANEOUS;\n      }\n\n      double submitTimeSpan \u003d lastJobSubmitTime - firstJobSubmitTime;\n\n      LOG.warn(\"Your input trace spans \"\n          + (lastJobSubmitTime - firstJobSubmitTime) + \" ticks.\");\n\n      double foldingRatio \u003d\n          submitTimeSpan * (numberJobs + 1) / numberJobs / inputCycle;\n\n      if (debug) {\n        LOG.warn(\"run: submitTimeSpan \u003d \" + submitTimeSpan + \", numberJobs \u003d \"\n            + numberJobs + \", inputCycle \u003d \" + inputCycle);\n      }\n\n      if (reader.neededSkewBufferSize() \u003e 0) {\n        LOG.warn(\"You needed a -skew-buffer-length of \"\n            + reader.neededSkewBufferSize() + \" but no more, for this input.\");\n      }\n\n      double tProbability \u003d timeDilation * concentration / foldingRatio;\n\n      if (debug) {\n        LOG.warn(\"run: timeDilation \u003d \" + timeDilation + \", concentration \u003d \"\n            + concentration + \", foldingRatio \u003d \" + foldingRatio);\n        LOG.warn(\"The transcription probability is \" + tProbability);\n      }\n\n      transcriptionRateInteger \u003d (int) Math.floor(tProbability);\n      transcriptionRateFraction \u003d tProbability - Math.floor(tProbability);\n\n      // Now read all the inputs in parallel\n      heap \u003d\n          new PriorityQueue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e(tempPaths.size(),\n              new JobEntryComparator());\n\n      for (Path tempPath : tempPaths) {\n        JobTraceReader thisReader \u003d new JobTraceReader(tempPath, conf);\n\n        closees.add(thisReader);\n\n        LoggedJob streamFirstJob \u003d thisReader.getNext();\n\n        long thisIndex \u003d\n            (streamFirstJob.getSubmitTime() - firstJobSubmitTime) / inputCycle;\n\n        if (debug) {\n          LOG.debug(\"A job with submit time of \"\n              + streamFirstJob.getSubmitTime() + \" is in interval # \"\n              + thisIndex);\n        }\n\n        adjustJobTimes(streamFirstJob);\n\n        if (debug) {\n          LOG.debug(\"That job\u0027s submit time is adjusted to \"\n              + streamFirstJob.getSubmitTime());\n        }\n\n        heap\n            .add(new Pair\u003cLoggedJob, JobTraceReader\u003e(streamFirstJob, thisReader));\n      }\n\n      Pair\u003cLoggedJob, JobTraceReader\u003e next \u003d heap.poll();\n\n      while (next !\u003d null) {\n        maybeOutput(next.first());\n\n        if (debug) {\n          LOG.debug(\"The most recent job has an adjusted submit time of \"\n              + next.first().getSubmitTime());\n          LOG.debug(\" Its replacement in the heap will come from input engine \"\n              + next.second());\n        }\n\n        LoggedJob replacement \u003d next.second().getNext();\n\n        if (replacement \u003d\u003d null) {\n          next.second().close();\n\n          if (debug) {\n            LOG.debug(\"That input engine is depleted.\");\n          }\n        } else {\n          adjustJobTimes(replacement);\n\n          if (debug) {\n            LOG.debug(\"The replacement has an adjusted submit time of \"\n                + replacement.getSubmitTime());\n          }\n\n          heap.add(new Pair\u003cLoggedJob, JobTraceReader\u003e(replacement, next\n              .second()));\n        }\n\n        next \u003d heap.poll();\n      }\n    } finally {\n      IOUtils.cleanup(null, reader);\n      if (outGen !\u003d null) {\n        outGen.close();\n      }\n      for (Pair\u003cLoggedJob, JobTraceReader\u003e heapEntry : heap) {\n        heapEntry.second().close();\n      }\n      for (Closeable closee : closees) {\n        closee.close();\n      }\n      if (!debug) {\n        Configuration conf \u003d getConf();\n\n        for (Path deletee : deletees) {\n          FileSystem fs \u003d deletee.getFileSystem(conf);\n\n          try {\n            fs.delete(deletee, false);\n          } catch (IOException e) {\n            // no code\n          }\n        }\n      }\n    }\n\n    return 0;\n  }",
      "path": "hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/Folder.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Folder.java",
        "newPath": "hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/Folder.java"
      }
    },
    "a238f931ea7dce0ca620d1798156c84ff77097ff": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-778. Rumen Anonymizer. (Amar Kamat and Chris Douglas via amarrk)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1215141 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/12/11 6:20 AM",
      "commitName": "a238f931ea7dce0ca620d1798156c84ff77097ff",
      "commitAuthor": "Amar Kamat",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 113.59,
      "commitsBetweenForRepo": 787,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,293 +1,280 @@\n   public int run() throws IOException {\n     class JobEntryComparator implements\n         Comparator\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e {\n       public int compare(Pair\u003cLoggedJob, JobTraceReader\u003e p1,\n           Pair\u003cLoggedJob, JobTraceReader\u003e p2) {\n         LoggedJob j1 \u003d p1.first();\n         LoggedJob j2 \u003d p2.first();\n \n         return (j1.getSubmitTime() \u003c j2.getSubmitTime()) ? -1 : (j1\n             .getSubmitTime() \u003d\u003d j2.getSubmitTime()) ? 0 : 1;\n       }\n     }\n \n-    ObjectMapper outMapper \u003d new ObjectMapper();\n-    outMapper.configure(\n-        SerializationConfig.Feature.CAN_OVERRIDE_ACCESS_MODIFIERS, true);\n-    JsonFactory outFactory \u003d outMapper.getJsonFactory();\n-\n     // we initialize an empty heap so if we take an error before establishing\n     // a real one the finally code goes through\n     Queue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e heap \u003d\n         new PriorityQueue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e();\n \n     try {\n       LoggedJob job \u003d reader.nextJob();\n \n       if (job \u003d\u003d null) {\n         LOG.error(\"The job trace is empty\");\n \n         return EMPTY_JOB_TRACE;\n       }\n       \n       // If starts-after time is specified, skip the number of jobs till we reach\n       // the starting time limit.\n       if (startsAfter \u003e 0) {\n         LOG.info(\"starts-after time is specified. Initial job submit time : \" \n                  + job.getSubmitTime());\n \n         long approximateTime \u003d job.getSubmitTime() + startsAfter;\n         job \u003d reader.nextJob();\n         long skippedCount \u003d 0;\n         while (job !\u003d null \u0026\u0026 job.getSubmitTime() \u003c approximateTime) {\n           job \u003d reader.nextJob();\n           skippedCount++;\n         }\n \n         LOG.debug(\"Considering jobs with submit time greater than \" \n                   + startsAfter + \" ms. Skipped \" + skippedCount + \" jobs.\");\n \n         if (job \u003d\u003d null) {\n           LOG.error(\"No more jobs to process in the trace with \u0027starts-after\u0027\"+\n                     \" set to \" + startsAfter + \"ms.\");\n           return EMPTY_JOB_TRACE;\n         }\n         LOG.info(\"The first job has a submit time of \" + job.getSubmitTime());\n       }\n \n       firstJobSubmitTime \u003d job.getSubmitTime();\n       long lastJobSubmitTime \u003d firstJobSubmitTime;\n \n       int numberJobs \u003d 0;\n \n       long currentIntervalEnd \u003d Long.MIN_VALUE;\n \n       Path nextSegment \u003d null;\n-      OutputStream tempUncompOut \u003d null;\n-      JsonGenerator tempGen \u003d null;\n+      Outputter\u003cLoggedJob\u003e tempGen \u003d null;\n \n       if (debug) {\n         LOG.debug(\"The first job has a submit time of \" + firstJobSubmitTime);\n       }\n \n       final Configuration conf \u003d getConf();\n \n       try {\n         // At the top of this loop, skewBuffer has at most\n         // skewBufferLength entries.\n         while (job !\u003d null) {\n           final Random tempNameGenerator \u003d new Random();\n \n           lastJobSubmitTime \u003d job.getSubmitTime();\n \n           ++numberJobs;\n \n           if (job.getSubmitTime() \u003e\u003d currentIntervalEnd) {\n             if (tempGen !\u003d null) {\n               tempGen.close();\n             }\n-            for (int i \u003d 0; i \u003c 3 \u0026\u0026 tempUncompOut \u003d\u003d null; ++i) {\n+            \n+            nextSegment \u003d null;\n+            for (int i \u003d 0; i \u003c 3 \u0026\u0026 nextSegment \u003d\u003d null; ++i) {\n               try {\n                 nextSegment \u003d\n                     new Path(tempDir, \"segment-\" + tempNameGenerator.nextLong()\n                         + \".json.gz\");\n \n                 if (debug) {\n                   LOG.debug(\"The next segment name is \" + nextSegment);\n                 }\n \n                 FileSystem fs \u003d nextSegment.getFileSystem(conf);\n \n                 try {\n                   if (!fs.exists(nextSegment)) {\n-                    tempUncompOut \u003d fs.create(nextSegment, false);\n+                    break;\n                   }\n \n                   continue;\n                 } catch (IOException e) {\n                   // no code -- file did not already exist\n                 }\n               } catch (IOException e) {\n                 // no code -- file exists now, or directory bad. We try three\n                 // times.\n               }\n             }\n \n+            if (nextSegment \u003d\u003d null) {\n+              throw new RuntimeException(\"Failed to create a new file!\");\n+            }\n+            \n             if (debug) {\n               LOG.debug(\"Creating \" + nextSegment\n                   + \" for a job with a submit time of \" + job.getSubmitTime());\n             }\n \n             deletees.add(nextSegment);\n \n             tempPaths.add(nextSegment);\n \n-            CompressionCodec codec \u003d\n-                new CompressionCodecFactory(conf).getCodec(nextSegment);\n-            OutputStream output;\n-            Compressor compressor \u003d null;\n-            if (codec !\u003d null) {\n-              compressor \u003d CodecPool.getCompressor(codec);\n-              output \u003d codec.createOutputStream(tempUncompOut, compressor);\n-            } else {\n-              output \u003d tempUncompOut;\n-            }\n-\n-            tempUncompOut \u003d null;\n-\n-            tempGen \u003d outFactory.createJsonGenerator(output, JsonEncoding.UTF8);\n-            if (debug) {\n-              tempGen.useDefaultPrettyPrinter();\n-            }\n+            tempGen \u003d new DefaultOutputter\u003cLoggedJob\u003e();\n+            tempGen.init(nextSegment, conf);\n \n             long currentIntervalNumber \u003d\n                 (job.getSubmitTime() - firstJobSubmitTime) / inputCycle;\n \n             currentIntervalEnd \u003d\n                 firstJobSubmitTime + ((currentIntervalNumber + 1) * inputCycle);\n           }\n \n           // the temp files contain UDadjusted times, but each temp file\u0027s\n           // content is in the same input cycle interval.\n-          tempGen.writeObject(job);\n+          if (tempGen !\u003d null) {\n+            tempGen.output(job);\n+          }\n \n           job \u003d reader.nextJob();\n         }\n       } catch (DeskewedJobTraceReader.OutOfOrderException e) {\n         return OUT_OF_ORDER_JOBS;\n       } finally {\n         if (tempGen !\u003d null) {\n           tempGen.close();\n         }\n       }\n \n       if (lastJobSubmitTime \u003c\u003d firstJobSubmitTime) {\n         LOG.error(\"All of your job[s] have the same submit time.\"\n             + \"  Please just use your input file.\");\n \n         return ALL_JOBS_SIMULTANEOUS;\n       }\n \n       double submitTimeSpan \u003d lastJobSubmitTime - firstJobSubmitTime;\n \n       LOG.warn(\"Your input trace spans \"\n           + (lastJobSubmitTime - firstJobSubmitTime) + \" ticks.\");\n \n       double foldingRatio \u003d\n           submitTimeSpan * (numberJobs + 1) / numberJobs / inputCycle;\n \n       if (debug) {\n         LOG.warn(\"run: submitTimeSpan \u003d \" + submitTimeSpan + \", numberJobs \u003d \"\n             + numberJobs + \", inputCycle \u003d \" + inputCycle);\n       }\n \n       if (reader.neededSkewBufferSize() \u003e 0) {\n         LOG.warn(\"You needed a -skew-buffer-length of \"\n             + reader.neededSkewBufferSize() + \" but no more, for this input.\");\n       }\n \n       double tProbability \u003d timeDilation * concentration / foldingRatio;\n \n       if (debug) {\n         LOG.warn(\"run: timeDilation \u003d \" + timeDilation + \", concentration \u003d \"\n             + concentration + \", foldingRatio \u003d \" + foldingRatio);\n         LOG.warn(\"The transcription probability is \" + tProbability);\n       }\n \n       transcriptionRateInteger \u003d (int) Math.floor(tProbability);\n       transcriptionRateFraction \u003d tProbability - Math.floor(tProbability);\n \n       // Now read all the inputs in parallel\n       heap \u003d\n           new PriorityQueue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e(tempPaths.size(),\n               new JobEntryComparator());\n \n       for (Path tempPath : tempPaths) {\n         JobTraceReader thisReader \u003d new JobTraceReader(tempPath, conf);\n \n         closees.add(thisReader);\n \n         LoggedJob streamFirstJob \u003d thisReader.getNext();\n \n         long thisIndex \u003d\n             (streamFirstJob.getSubmitTime() - firstJobSubmitTime) / inputCycle;\n \n         if (debug) {\n           LOG.debug(\"A job with submit time of \"\n               + streamFirstJob.getSubmitTime() + \" is in interval # \"\n               + thisIndex);\n         }\n \n         adjustJobTimes(streamFirstJob);\n \n         if (debug) {\n           LOG.debug(\"That job\u0027s submit time is adjusted to \"\n               + streamFirstJob.getSubmitTime());\n         }\n \n         heap\n             .add(new Pair\u003cLoggedJob, JobTraceReader\u003e(streamFirstJob, thisReader));\n       }\n \n       Pair\u003cLoggedJob, JobTraceReader\u003e next \u003d heap.poll();\n \n       while (next !\u003d null) {\n         maybeOutput(next.first());\n \n         if (debug) {\n           LOG.debug(\"The most recent job has an adjusted submit time of \"\n               + next.first().getSubmitTime());\n           LOG.debug(\" Its replacement in the heap will come from input engine \"\n               + next.second());\n         }\n \n         LoggedJob replacement \u003d next.second().getNext();\n \n         if (replacement \u003d\u003d null) {\n           next.second().close();\n \n           if (debug) {\n             LOG.debug(\"That input engine is depleted.\");\n           }\n         } else {\n           adjustJobTimes(replacement);\n \n           if (debug) {\n             LOG.debug(\"The replacement has an adjusted submit time of \"\n                 + replacement.getSubmitTime());\n           }\n \n           heap.add(new Pair\u003cLoggedJob, JobTraceReader\u003e(replacement, next\n               .second()));\n         }\n \n         next \u003d heap.poll();\n       }\n     } finally {\n       IOUtils.cleanup(null, reader);\n       if (outGen !\u003d null) {\n         outGen.close();\n       }\n       for (Pair\u003cLoggedJob, JobTraceReader\u003e heapEntry : heap) {\n         heapEntry.second().close();\n       }\n       for (Closeable closee : closees) {\n         closee.close();\n       }\n       if (!debug) {\n         Configuration conf \u003d getConf();\n \n         for (Path deletee : deletees) {\n           FileSystem fs \u003d deletee.getFileSystem(conf);\n \n           try {\n             fs.delete(deletee, false);\n           } catch (IOException e) {\n             // no code\n           }\n         }\n       }\n     }\n \n     return 0;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public int run() throws IOException {\n    class JobEntryComparator implements\n        Comparator\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e {\n      public int compare(Pair\u003cLoggedJob, JobTraceReader\u003e p1,\n          Pair\u003cLoggedJob, JobTraceReader\u003e p2) {\n        LoggedJob j1 \u003d p1.first();\n        LoggedJob j2 \u003d p2.first();\n\n        return (j1.getSubmitTime() \u003c j2.getSubmitTime()) ? -1 : (j1\n            .getSubmitTime() \u003d\u003d j2.getSubmitTime()) ? 0 : 1;\n      }\n    }\n\n    // we initialize an empty heap so if we take an error before establishing\n    // a real one the finally code goes through\n    Queue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e heap \u003d\n        new PriorityQueue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e();\n\n    try {\n      LoggedJob job \u003d reader.nextJob();\n\n      if (job \u003d\u003d null) {\n        LOG.error(\"The job trace is empty\");\n\n        return EMPTY_JOB_TRACE;\n      }\n      \n      // If starts-after time is specified, skip the number of jobs till we reach\n      // the starting time limit.\n      if (startsAfter \u003e 0) {\n        LOG.info(\"starts-after time is specified. Initial job submit time : \" \n                 + job.getSubmitTime());\n\n        long approximateTime \u003d job.getSubmitTime() + startsAfter;\n        job \u003d reader.nextJob();\n        long skippedCount \u003d 0;\n        while (job !\u003d null \u0026\u0026 job.getSubmitTime() \u003c approximateTime) {\n          job \u003d reader.nextJob();\n          skippedCount++;\n        }\n\n        LOG.debug(\"Considering jobs with submit time greater than \" \n                  + startsAfter + \" ms. Skipped \" + skippedCount + \" jobs.\");\n\n        if (job \u003d\u003d null) {\n          LOG.error(\"No more jobs to process in the trace with \u0027starts-after\u0027\"+\n                    \" set to \" + startsAfter + \"ms.\");\n          return EMPTY_JOB_TRACE;\n        }\n        LOG.info(\"The first job has a submit time of \" + job.getSubmitTime());\n      }\n\n      firstJobSubmitTime \u003d job.getSubmitTime();\n      long lastJobSubmitTime \u003d firstJobSubmitTime;\n\n      int numberJobs \u003d 0;\n\n      long currentIntervalEnd \u003d Long.MIN_VALUE;\n\n      Path nextSegment \u003d null;\n      Outputter\u003cLoggedJob\u003e tempGen \u003d null;\n\n      if (debug) {\n        LOG.debug(\"The first job has a submit time of \" + firstJobSubmitTime);\n      }\n\n      final Configuration conf \u003d getConf();\n\n      try {\n        // At the top of this loop, skewBuffer has at most\n        // skewBufferLength entries.\n        while (job !\u003d null) {\n          final Random tempNameGenerator \u003d new Random();\n\n          lastJobSubmitTime \u003d job.getSubmitTime();\n\n          ++numberJobs;\n\n          if (job.getSubmitTime() \u003e\u003d currentIntervalEnd) {\n            if (tempGen !\u003d null) {\n              tempGen.close();\n            }\n            \n            nextSegment \u003d null;\n            for (int i \u003d 0; i \u003c 3 \u0026\u0026 nextSegment \u003d\u003d null; ++i) {\n              try {\n                nextSegment \u003d\n                    new Path(tempDir, \"segment-\" + tempNameGenerator.nextLong()\n                        + \".json.gz\");\n\n                if (debug) {\n                  LOG.debug(\"The next segment name is \" + nextSegment);\n                }\n\n                FileSystem fs \u003d nextSegment.getFileSystem(conf);\n\n                try {\n                  if (!fs.exists(nextSegment)) {\n                    break;\n                  }\n\n                  continue;\n                } catch (IOException e) {\n                  // no code -- file did not already exist\n                }\n              } catch (IOException e) {\n                // no code -- file exists now, or directory bad. We try three\n                // times.\n              }\n            }\n\n            if (nextSegment \u003d\u003d null) {\n              throw new RuntimeException(\"Failed to create a new file!\");\n            }\n            \n            if (debug) {\n              LOG.debug(\"Creating \" + nextSegment\n                  + \" for a job with a submit time of \" + job.getSubmitTime());\n            }\n\n            deletees.add(nextSegment);\n\n            tempPaths.add(nextSegment);\n\n            tempGen \u003d new DefaultOutputter\u003cLoggedJob\u003e();\n            tempGen.init(nextSegment, conf);\n\n            long currentIntervalNumber \u003d\n                (job.getSubmitTime() - firstJobSubmitTime) / inputCycle;\n\n            currentIntervalEnd \u003d\n                firstJobSubmitTime + ((currentIntervalNumber + 1) * inputCycle);\n          }\n\n          // the temp files contain UDadjusted times, but each temp file\u0027s\n          // content is in the same input cycle interval.\n          if (tempGen !\u003d null) {\n            tempGen.output(job);\n          }\n\n          job \u003d reader.nextJob();\n        }\n      } catch (DeskewedJobTraceReader.OutOfOrderException e) {\n        return OUT_OF_ORDER_JOBS;\n      } finally {\n        if (tempGen !\u003d null) {\n          tempGen.close();\n        }\n      }\n\n      if (lastJobSubmitTime \u003c\u003d firstJobSubmitTime) {\n        LOG.error(\"All of your job[s] have the same submit time.\"\n            + \"  Please just use your input file.\");\n\n        return ALL_JOBS_SIMULTANEOUS;\n      }\n\n      double submitTimeSpan \u003d lastJobSubmitTime - firstJobSubmitTime;\n\n      LOG.warn(\"Your input trace spans \"\n          + (lastJobSubmitTime - firstJobSubmitTime) + \" ticks.\");\n\n      double foldingRatio \u003d\n          submitTimeSpan * (numberJobs + 1) / numberJobs / inputCycle;\n\n      if (debug) {\n        LOG.warn(\"run: submitTimeSpan \u003d \" + submitTimeSpan + \", numberJobs \u003d \"\n            + numberJobs + \", inputCycle \u003d \" + inputCycle);\n      }\n\n      if (reader.neededSkewBufferSize() \u003e 0) {\n        LOG.warn(\"You needed a -skew-buffer-length of \"\n            + reader.neededSkewBufferSize() + \" but no more, for this input.\");\n      }\n\n      double tProbability \u003d timeDilation * concentration / foldingRatio;\n\n      if (debug) {\n        LOG.warn(\"run: timeDilation \u003d \" + timeDilation + \", concentration \u003d \"\n            + concentration + \", foldingRatio \u003d \" + foldingRatio);\n        LOG.warn(\"The transcription probability is \" + tProbability);\n      }\n\n      transcriptionRateInteger \u003d (int) Math.floor(tProbability);\n      transcriptionRateFraction \u003d tProbability - Math.floor(tProbability);\n\n      // Now read all the inputs in parallel\n      heap \u003d\n          new PriorityQueue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e(tempPaths.size(),\n              new JobEntryComparator());\n\n      for (Path tempPath : tempPaths) {\n        JobTraceReader thisReader \u003d new JobTraceReader(tempPath, conf);\n\n        closees.add(thisReader);\n\n        LoggedJob streamFirstJob \u003d thisReader.getNext();\n\n        long thisIndex \u003d\n            (streamFirstJob.getSubmitTime() - firstJobSubmitTime) / inputCycle;\n\n        if (debug) {\n          LOG.debug(\"A job with submit time of \"\n              + streamFirstJob.getSubmitTime() + \" is in interval # \"\n              + thisIndex);\n        }\n\n        adjustJobTimes(streamFirstJob);\n\n        if (debug) {\n          LOG.debug(\"That job\u0027s submit time is adjusted to \"\n              + streamFirstJob.getSubmitTime());\n        }\n\n        heap\n            .add(new Pair\u003cLoggedJob, JobTraceReader\u003e(streamFirstJob, thisReader));\n      }\n\n      Pair\u003cLoggedJob, JobTraceReader\u003e next \u003d heap.poll();\n\n      while (next !\u003d null) {\n        maybeOutput(next.first());\n\n        if (debug) {\n          LOG.debug(\"The most recent job has an adjusted submit time of \"\n              + next.first().getSubmitTime());\n          LOG.debug(\" Its replacement in the heap will come from input engine \"\n              + next.second());\n        }\n\n        LoggedJob replacement \u003d next.second().getNext();\n\n        if (replacement \u003d\u003d null) {\n          next.second().close();\n\n          if (debug) {\n            LOG.debug(\"That input engine is depleted.\");\n          }\n        } else {\n          adjustJobTimes(replacement);\n\n          if (debug) {\n            LOG.debug(\"The replacement has an adjusted submit time of \"\n                + replacement.getSubmitTime());\n          }\n\n          heap.add(new Pair\u003cLoggedJob, JobTraceReader\u003e(replacement, next\n              .second()));\n        }\n\n        next \u003d heap.poll();\n      }\n    } finally {\n      IOUtils.cleanup(null, reader);\n      if (outGen !\u003d null) {\n        outGen.close();\n      }\n      for (Pair\u003cLoggedJob, JobTraceReader\u003e heapEntry : heap) {\n        heapEntry.second().close();\n      }\n      for (Closeable closee : closees) {\n        closee.close();\n      }\n      if (!debug) {\n        Configuration conf \u003d getConf();\n\n        for (Path deletee : deletees) {\n          FileSystem fs \u003d deletee.getFileSystem(conf);\n\n          try {\n            fs.delete(deletee, false);\n          } catch (IOException e) {\n            // no code\n          }\n        }\n      }\n    }\n\n    return 0;\n  }",
      "path": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Folder.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public int run() throws IOException {\n    class JobEntryComparator implements\n        Comparator\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e {\n      public int compare(Pair\u003cLoggedJob, JobTraceReader\u003e p1,\n          Pair\u003cLoggedJob, JobTraceReader\u003e p2) {\n        LoggedJob j1 \u003d p1.first();\n        LoggedJob j2 \u003d p2.first();\n\n        return (j1.getSubmitTime() \u003c j2.getSubmitTime()) ? -1 : (j1\n            .getSubmitTime() \u003d\u003d j2.getSubmitTime()) ? 0 : 1;\n      }\n    }\n\n    ObjectMapper outMapper \u003d new ObjectMapper();\n    outMapper.configure(\n        SerializationConfig.Feature.CAN_OVERRIDE_ACCESS_MODIFIERS, true);\n    JsonFactory outFactory \u003d outMapper.getJsonFactory();\n\n    // we initialize an empty heap so if we take an error before establishing\n    // a real one the finally code goes through\n    Queue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e heap \u003d\n        new PriorityQueue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e();\n\n    try {\n      LoggedJob job \u003d reader.nextJob();\n\n      if (job \u003d\u003d null) {\n        LOG.error(\"The job trace is empty\");\n\n        return EMPTY_JOB_TRACE;\n      }\n      \n      // If starts-after time is specified, skip the number of jobs till we reach\n      // the starting time limit.\n      if (startsAfter \u003e 0) {\n        LOG.info(\"starts-after time is specified. Initial job submit time : \" \n                 + job.getSubmitTime());\n\n        long approximateTime \u003d job.getSubmitTime() + startsAfter;\n        job \u003d reader.nextJob();\n        long skippedCount \u003d 0;\n        while (job !\u003d null \u0026\u0026 job.getSubmitTime() \u003c approximateTime) {\n          job \u003d reader.nextJob();\n          skippedCount++;\n        }\n\n        LOG.debug(\"Considering jobs with submit time greater than \" \n                  + startsAfter + \" ms. Skipped \" + skippedCount + \" jobs.\");\n\n        if (job \u003d\u003d null) {\n          LOG.error(\"No more jobs to process in the trace with \u0027starts-after\u0027\"+\n                    \" set to \" + startsAfter + \"ms.\");\n          return EMPTY_JOB_TRACE;\n        }\n        LOG.info(\"The first job has a submit time of \" + job.getSubmitTime());\n      }\n\n      firstJobSubmitTime \u003d job.getSubmitTime();\n      long lastJobSubmitTime \u003d firstJobSubmitTime;\n\n      int numberJobs \u003d 0;\n\n      long currentIntervalEnd \u003d Long.MIN_VALUE;\n\n      Path nextSegment \u003d null;\n      OutputStream tempUncompOut \u003d null;\n      JsonGenerator tempGen \u003d null;\n\n      if (debug) {\n        LOG.debug(\"The first job has a submit time of \" + firstJobSubmitTime);\n      }\n\n      final Configuration conf \u003d getConf();\n\n      try {\n        // At the top of this loop, skewBuffer has at most\n        // skewBufferLength entries.\n        while (job !\u003d null) {\n          final Random tempNameGenerator \u003d new Random();\n\n          lastJobSubmitTime \u003d job.getSubmitTime();\n\n          ++numberJobs;\n\n          if (job.getSubmitTime() \u003e\u003d currentIntervalEnd) {\n            if (tempGen !\u003d null) {\n              tempGen.close();\n            }\n            for (int i \u003d 0; i \u003c 3 \u0026\u0026 tempUncompOut \u003d\u003d null; ++i) {\n              try {\n                nextSegment \u003d\n                    new Path(tempDir, \"segment-\" + tempNameGenerator.nextLong()\n                        + \".json.gz\");\n\n                if (debug) {\n                  LOG.debug(\"The next segment name is \" + nextSegment);\n                }\n\n                FileSystem fs \u003d nextSegment.getFileSystem(conf);\n\n                try {\n                  if (!fs.exists(nextSegment)) {\n                    tempUncompOut \u003d fs.create(nextSegment, false);\n                  }\n\n                  continue;\n                } catch (IOException e) {\n                  // no code -- file did not already exist\n                }\n              } catch (IOException e) {\n                // no code -- file exists now, or directory bad. We try three\n                // times.\n              }\n            }\n\n            if (debug) {\n              LOG.debug(\"Creating \" + nextSegment\n                  + \" for a job with a submit time of \" + job.getSubmitTime());\n            }\n\n            deletees.add(nextSegment);\n\n            tempPaths.add(nextSegment);\n\n            CompressionCodec codec \u003d\n                new CompressionCodecFactory(conf).getCodec(nextSegment);\n            OutputStream output;\n            Compressor compressor \u003d null;\n            if (codec !\u003d null) {\n              compressor \u003d CodecPool.getCompressor(codec);\n              output \u003d codec.createOutputStream(tempUncompOut, compressor);\n            } else {\n              output \u003d tempUncompOut;\n            }\n\n            tempUncompOut \u003d null;\n\n            tempGen \u003d outFactory.createJsonGenerator(output, JsonEncoding.UTF8);\n            if (debug) {\n              tempGen.useDefaultPrettyPrinter();\n            }\n\n            long currentIntervalNumber \u003d\n                (job.getSubmitTime() - firstJobSubmitTime) / inputCycle;\n\n            currentIntervalEnd \u003d\n                firstJobSubmitTime + ((currentIntervalNumber + 1) * inputCycle);\n          }\n\n          // the temp files contain UDadjusted times, but each temp file\u0027s\n          // content is in the same input cycle interval.\n          tempGen.writeObject(job);\n\n          job \u003d reader.nextJob();\n        }\n      } catch (DeskewedJobTraceReader.OutOfOrderException e) {\n        return OUT_OF_ORDER_JOBS;\n      } finally {\n        if (tempGen !\u003d null) {\n          tempGen.close();\n        }\n      }\n\n      if (lastJobSubmitTime \u003c\u003d firstJobSubmitTime) {\n        LOG.error(\"All of your job[s] have the same submit time.\"\n            + \"  Please just use your input file.\");\n\n        return ALL_JOBS_SIMULTANEOUS;\n      }\n\n      double submitTimeSpan \u003d lastJobSubmitTime - firstJobSubmitTime;\n\n      LOG.warn(\"Your input trace spans \"\n          + (lastJobSubmitTime - firstJobSubmitTime) + \" ticks.\");\n\n      double foldingRatio \u003d\n          submitTimeSpan * (numberJobs + 1) / numberJobs / inputCycle;\n\n      if (debug) {\n        LOG.warn(\"run: submitTimeSpan \u003d \" + submitTimeSpan + \", numberJobs \u003d \"\n            + numberJobs + \", inputCycle \u003d \" + inputCycle);\n      }\n\n      if (reader.neededSkewBufferSize() \u003e 0) {\n        LOG.warn(\"You needed a -skew-buffer-length of \"\n            + reader.neededSkewBufferSize() + \" but no more, for this input.\");\n      }\n\n      double tProbability \u003d timeDilation * concentration / foldingRatio;\n\n      if (debug) {\n        LOG.warn(\"run: timeDilation \u003d \" + timeDilation + \", concentration \u003d \"\n            + concentration + \", foldingRatio \u003d \" + foldingRatio);\n        LOG.warn(\"The transcription probability is \" + tProbability);\n      }\n\n      transcriptionRateInteger \u003d (int) Math.floor(tProbability);\n      transcriptionRateFraction \u003d tProbability - Math.floor(tProbability);\n\n      // Now read all the inputs in parallel\n      heap \u003d\n          new PriorityQueue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e(tempPaths.size(),\n              new JobEntryComparator());\n\n      for (Path tempPath : tempPaths) {\n        JobTraceReader thisReader \u003d new JobTraceReader(tempPath, conf);\n\n        closees.add(thisReader);\n\n        LoggedJob streamFirstJob \u003d thisReader.getNext();\n\n        long thisIndex \u003d\n            (streamFirstJob.getSubmitTime() - firstJobSubmitTime) / inputCycle;\n\n        if (debug) {\n          LOG.debug(\"A job with submit time of \"\n              + streamFirstJob.getSubmitTime() + \" is in interval # \"\n              + thisIndex);\n        }\n\n        adjustJobTimes(streamFirstJob);\n\n        if (debug) {\n          LOG.debug(\"That job\u0027s submit time is adjusted to \"\n              + streamFirstJob.getSubmitTime());\n        }\n\n        heap\n            .add(new Pair\u003cLoggedJob, JobTraceReader\u003e(streamFirstJob, thisReader));\n      }\n\n      Pair\u003cLoggedJob, JobTraceReader\u003e next \u003d heap.poll();\n\n      while (next !\u003d null) {\n        maybeOutput(next.first());\n\n        if (debug) {\n          LOG.debug(\"The most recent job has an adjusted submit time of \"\n              + next.first().getSubmitTime());\n          LOG.debug(\" Its replacement in the heap will come from input engine \"\n              + next.second());\n        }\n\n        LoggedJob replacement \u003d next.second().getNext();\n\n        if (replacement \u003d\u003d null) {\n          next.second().close();\n\n          if (debug) {\n            LOG.debug(\"That input engine is depleted.\");\n          }\n        } else {\n          adjustJobTimes(replacement);\n\n          if (debug) {\n            LOG.debug(\"The replacement has an adjusted submit time of \"\n                + replacement.getSubmitTime());\n          }\n\n          heap.add(new Pair\u003cLoggedJob, JobTraceReader\u003e(replacement, next\n              .second()));\n        }\n\n        next \u003d heap.poll();\n      }\n    } finally {\n      IOUtils.cleanup(null, reader);\n      if (outGen !\u003d null) {\n        outGen.close();\n      }\n      for (Pair\u003cLoggedJob, JobTraceReader\u003e heapEntry : heap) {\n        heapEntry.second().close();\n      }\n      for (Closeable closee : closees) {\n        closee.close();\n      }\n      if (!debug) {\n        Configuration conf \u003d getConf();\n\n        for (Path deletee : deletees) {\n          FileSystem fs \u003d deletee.getFileSystem(conf);\n\n          try {\n            fs.delete(deletee, false);\n          } catch (IOException e) {\n            // no code\n          }\n        }\n      }\n    }\n\n    return 0;\n  }",
      "path": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Folder.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/rumen/Folder.java",
        "newPath": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/Folder.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public int run() throws IOException {\n    class JobEntryComparator implements\n        Comparator\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e {\n      public int compare(Pair\u003cLoggedJob, JobTraceReader\u003e p1,\n          Pair\u003cLoggedJob, JobTraceReader\u003e p2) {\n        LoggedJob j1 \u003d p1.first();\n        LoggedJob j2 \u003d p2.first();\n\n        return (j1.getSubmitTime() \u003c j2.getSubmitTime()) ? -1 : (j1\n            .getSubmitTime() \u003d\u003d j2.getSubmitTime()) ? 0 : 1;\n      }\n    }\n\n    ObjectMapper outMapper \u003d new ObjectMapper();\n    outMapper.configure(\n        SerializationConfig.Feature.CAN_OVERRIDE_ACCESS_MODIFIERS, true);\n    JsonFactory outFactory \u003d outMapper.getJsonFactory();\n\n    // we initialize an empty heap so if we take an error before establishing\n    // a real one the finally code goes through\n    Queue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e heap \u003d\n        new PriorityQueue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e();\n\n    try {\n      LoggedJob job \u003d reader.nextJob();\n\n      if (job \u003d\u003d null) {\n        LOG.error(\"The job trace is empty\");\n\n        return EMPTY_JOB_TRACE;\n      }\n      \n      // If starts-after time is specified, skip the number of jobs till we reach\n      // the starting time limit.\n      if (startsAfter \u003e 0) {\n        LOG.info(\"starts-after time is specified. Initial job submit time : \" \n                 + job.getSubmitTime());\n\n        long approximateTime \u003d job.getSubmitTime() + startsAfter;\n        job \u003d reader.nextJob();\n        long skippedCount \u003d 0;\n        while (job !\u003d null \u0026\u0026 job.getSubmitTime() \u003c approximateTime) {\n          job \u003d reader.nextJob();\n          skippedCount++;\n        }\n\n        LOG.debug(\"Considering jobs with submit time greater than \" \n                  + startsAfter + \" ms. Skipped \" + skippedCount + \" jobs.\");\n\n        if (job \u003d\u003d null) {\n          LOG.error(\"No more jobs to process in the trace with \u0027starts-after\u0027\"+\n                    \" set to \" + startsAfter + \"ms.\");\n          return EMPTY_JOB_TRACE;\n        }\n        LOG.info(\"The first job has a submit time of \" + job.getSubmitTime());\n      }\n\n      firstJobSubmitTime \u003d job.getSubmitTime();\n      long lastJobSubmitTime \u003d firstJobSubmitTime;\n\n      int numberJobs \u003d 0;\n\n      long currentIntervalEnd \u003d Long.MIN_VALUE;\n\n      Path nextSegment \u003d null;\n      OutputStream tempUncompOut \u003d null;\n      JsonGenerator tempGen \u003d null;\n\n      if (debug) {\n        LOG.debug(\"The first job has a submit time of \" + firstJobSubmitTime);\n      }\n\n      final Configuration conf \u003d getConf();\n\n      try {\n        // At the top of this loop, skewBuffer has at most\n        // skewBufferLength entries.\n        while (job !\u003d null) {\n          final Random tempNameGenerator \u003d new Random();\n\n          lastJobSubmitTime \u003d job.getSubmitTime();\n\n          ++numberJobs;\n\n          if (job.getSubmitTime() \u003e\u003d currentIntervalEnd) {\n            if (tempGen !\u003d null) {\n              tempGen.close();\n            }\n            for (int i \u003d 0; i \u003c 3 \u0026\u0026 tempUncompOut \u003d\u003d null; ++i) {\n              try {\n                nextSegment \u003d\n                    new Path(tempDir, \"segment-\" + tempNameGenerator.nextLong()\n                        + \".json.gz\");\n\n                if (debug) {\n                  LOG.debug(\"The next segment name is \" + nextSegment);\n                }\n\n                FileSystem fs \u003d nextSegment.getFileSystem(conf);\n\n                try {\n                  if (!fs.exists(nextSegment)) {\n                    tempUncompOut \u003d fs.create(nextSegment, false);\n                  }\n\n                  continue;\n                } catch (IOException e) {\n                  // no code -- file did not already exist\n                }\n              } catch (IOException e) {\n                // no code -- file exists now, or directory bad. We try three\n                // times.\n              }\n            }\n\n            if (debug) {\n              LOG.debug(\"Creating \" + nextSegment\n                  + \" for a job with a submit time of \" + job.getSubmitTime());\n            }\n\n            deletees.add(nextSegment);\n\n            tempPaths.add(nextSegment);\n\n            CompressionCodec codec \u003d\n                new CompressionCodecFactory(conf).getCodec(nextSegment);\n            OutputStream output;\n            Compressor compressor \u003d null;\n            if (codec !\u003d null) {\n              compressor \u003d CodecPool.getCompressor(codec);\n              output \u003d codec.createOutputStream(tempUncompOut, compressor);\n            } else {\n              output \u003d tempUncompOut;\n            }\n\n            tempUncompOut \u003d null;\n\n            tempGen \u003d outFactory.createJsonGenerator(output, JsonEncoding.UTF8);\n            if (debug) {\n              tempGen.useDefaultPrettyPrinter();\n            }\n\n            long currentIntervalNumber \u003d\n                (job.getSubmitTime() - firstJobSubmitTime) / inputCycle;\n\n            currentIntervalEnd \u003d\n                firstJobSubmitTime + ((currentIntervalNumber + 1) * inputCycle);\n          }\n\n          // the temp files contain UDadjusted times, but each temp file\u0027s\n          // content is in the same input cycle interval.\n          tempGen.writeObject(job);\n\n          job \u003d reader.nextJob();\n        }\n      } catch (DeskewedJobTraceReader.OutOfOrderException e) {\n        return OUT_OF_ORDER_JOBS;\n      } finally {\n        if (tempGen !\u003d null) {\n          tempGen.close();\n        }\n      }\n\n      if (lastJobSubmitTime \u003c\u003d firstJobSubmitTime) {\n        LOG.error(\"All of your job[s] have the same submit time.\"\n            + \"  Please just use your input file.\");\n\n        return ALL_JOBS_SIMULTANEOUS;\n      }\n\n      double submitTimeSpan \u003d lastJobSubmitTime - firstJobSubmitTime;\n\n      LOG.warn(\"Your input trace spans \"\n          + (lastJobSubmitTime - firstJobSubmitTime) + \" ticks.\");\n\n      double foldingRatio \u003d\n          submitTimeSpan * (numberJobs + 1) / numberJobs / inputCycle;\n\n      if (debug) {\n        LOG.warn(\"run: submitTimeSpan \u003d \" + submitTimeSpan + \", numberJobs \u003d \"\n            + numberJobs + \", inputCycle \u003d \" + inputCycle);\n      }\n\n      if (reader.neededSkewBufferSize() \u003e 0) {\n        LOG.warn(\"You needed a -skew-buffer-length of \"\n            + reader.neededSkewBufferSize() + \" but no more, for this input.\");\n      }\n\n      double tProbability \u003d timeDilation * concentration / foldingRatio;\n\n      if (debug) {\n        LOG.warn(\"run: timeDilation \u003d \" + timeDilation + \", concentration \u003d \"\n            + concentration + \", foldingRatio \u003d \" + foldingRatio);\n        LOG.warn(\"The transcription probability is \" + tProbability);\n      }\n\n      transcriptionRateInteger \u003d (int) Math.floor(tProbability);\n      transcriptionRateFraction \u003d tProbability - Math.floor(tProbability);\n\n      // Now read all the inputs in parallel\n      heap \u003d\n          new PriorityQueue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e(tempPaths.size(),\n              new JobEntryComparator());\n\n      for (Path tempPath : tempPaths) {\n        JobTraceReader thisReader \u003d new JobTraceReader(tempPath, conf);\n\n        closees.add(thisReader);\n\n        LoggedJob streamFirstJob \u003d thisReader.getNext();\n\n        long thisIndex \u003d\n            (streamFirstJob.getSubmitTime() - firstJobSubmitTime) / inputCycle;\n\n        if (debug) {\n          LOG.debug(\"A job with submit time of \"\n              + streamFirstJob.getSubmitTime() + \" is in interval # \"\n              + thisIndex);\n        }\n\n        adjustJobTimes(streamFirstJob);\n\n        if (debug) {\n          LOG.debug(\"That job\u0027s submit time is adjusted to \"\n              + streamFirstJob.getSubmitTime());\n        }\n\n        heap\n            .add(new Pair\u003cLoggedJob, JobTraceReader\u003e(streamFirstJob, thisReader));\n      }\n\n      Pair\u003cLoggedJob, JobTraceReader\u003e next \u003d heap.poll();\n\n      while (next !\u003d null) {\n        maybeOutput(next.first());\n\n        if (debug) {\n          LOG.debug(\"The most recent job has an adjusted submit time of \"\n              + next.first().getSubmitTime());\n          LOG.debug(\" Its replacement in the heap will come from input engine \"\n              + next.second());\n        }\n\n        LoggedJob replacement \u003d next.second().getNext();\n\n        if (replacement \u003d\u003d null) {\n          next.second().close();\n\n          if (debug) {\n            LOG.debug(\"That input engine is depleted.\");\n          }\n        } else {\n          adjustJobTimes(replacement);\n\n          if (debug) {\n            LOG.debug(\"The replacement has an adjusted submit time of \"\n                + replacement.getSubmitTime());\n          }\n\n          heap.add(new Pair\u003cLoggedJob, JobTraceReader\u003e(replacement, next\n              .second()));\n        }\n\n        next \u003d heap.poll();\n      }\n    } finally {\n      IOUtils.cleanup(null, reader);\n      if (outGen !\u003d null) {\n        outGen.close();\n      }\n      for (Pair\u003cLoggedJob, JobTraceReader\u003e heapEntry : heap) {\n        heapEntry.second().close();\n      }\n      for (Closeable closee : closees) {\n        closee.close();\n      }\n      if (!debug) {\n        Configuration conf \u003d getConf();\n\n        for (Path deletee : deletees) {\n          FileSystem fs \u003d deletee.getFileSystem(conf);\n\n          try {\n            fs.delete(deletee, false);\n          } catch (IOException e) {\n            // no code\n          }\n        }\n      }\n    }\n\n    return 0;\n  }",
      "path": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/rumen/Folder.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/tools/org/apache/hadoop/tools/rumen/Folder.java",
        "newPath": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/rumen/Folder.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,293 @@\n+  public int run() throws IOException {\n+    class JobEntryComparator implements\n+        Comparator\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e {\n+      public int compare(Pair\u003cLoggedJob, JobTraceReader\u003e p1,\n+          Pair\u003cLoggedJob, JobTraceReader\u003e p2) {\n+        LoggedJob j1 \u003d p1.first();\n+        LoggedJob j2 \u003d p2.first();\n+\n+        return (j1.getSubmitTime() \u003c j2.getSubmitTime()) ? -1 : (j1\n+            .getSubmitTime() \u003d\u003d j2.getSubmitTime()) ? 0 : 1;\n+      }\n+    }\n+\n+    ObjectMapper outMapper \u003d new ObjectMapper();\n+    outMapper.configure(\n+        SerializationConfig.Feature.CAN_OVERRIDE_ACCESS_MODIFIERS, true);\n+    JsonFactory outFactory \u003d outMapper.getJsonFactory();\n+\n+    // we initialize an empty heap so if we take an error before establishing\n+    // a real one the finally code goes through\n+    Queue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e heap \u003d\n+        new PriorityQueue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e();\n+\n+    try {\n+      LoggedJob job \u003d reader.nextJob();\n+\n+      if (job \u003d\u003d null) {\n+        LOG.error(\"The job trace is empty\");\n+\n+        return EMPTY_JOB_TRACE;\n+      }\n+      \n+      // If starts-after time is specified, skip the number of jobs till we reach\n+      // the starting time limit.\n+      if (startsAfter \u003e 0) {\n+        LOG.info(\"starts-after time is specified. Initial job submit time : \" \n+                 + job.getSubmitTime());\n+\n+        long approximateTime \u003d job.getSubmitTime() + startsAfter;\n+        job \u003d reader.nextJob();\n+        long skippedCount \u003d 0;\n+        while (job !\u003d null \u0026\u0026 job.getSubmitTime() \u003c approximateTime) {\n+          job \u003d reader.nextJob();\n+          skippedCount++;\n+        }\n+\n+        LOG.debug(\"Considering jobs with submit time greater than \" \n+                  + startsAfter + \" ms. Skipped \" + skippedCount + \" jobs.\");\n+\n+        if (job \u003d\u003d null) {\n+          LOG.error(\"No more jobs to process in the trace with \u0027starts-after\u0027\"+\n+                    \" set to \" + startsAfter + \"ms.\");\n+          return EMPTY_JOB_TRACE;\n+        }\n+        LOG.info(\"The first job has a submit time of \" + job.getSubmitTime());\n+      }\n+\n+      firstJobSubmitTime \u003d job.getSubmitTime();\n+      long lastJobSubmitTime \u003d firstJobSubmitTime;\n+\n+      int numberJobs \u003d 0;\n+\n+      long currentIntervalEnd \u003d Long.MIN_VALUE;\n+\n+      Path nextSegment \u003d null;\n+      OutputStream tempUncompOut \u003d null;\n+      JsonGenerator tempGen \u003d null;\n+\n+      if (debug) {\n+        LOG.debug(\"The first job has a submit time of \" + firstJobSubmitTime);\n+      }\n+\n+      final Configuration conf \u003d getConf();\n+\n+      try {\n+        // At the top of this loop, skewBuffer has at most\n+        // skewBufferLength entries.\n+        while (job !\u003d null) {\n+          final Random tempNameGenerator \u003d new Random();\n+\n+          lastJobSubmitTime \u003d job.getSubmitTime();\n+\n+          ++numberJobs;\n+\n+          if (job.getSubmitTime() \u003e\u003d currentIntervalEnd) {\n+            if (tempGen !\u003d null) {\n+              tempGen.close();\n+            }\n+            for (int i \u003d 0; i \u003c 3 \u0026\u0026 tempUncompOut \u003d\u003d null; ++i) {\n+              try {\n+                nextSegment \u003d\n+                    new Path(tempDir, \"segment-\" + tempNameGenerator.nextLong()\n+                        + \".json.gz\");\n+\n+                if (debug) {\n+                  LOG.debug(\"The next segment name is \" + nextSegment);\n+                }\n+\n+                FileSystem fs \u003d nextSegment.getFileSystem(conf);\n+\n+                try {\n+                  if (!fs.exists(nextSegment)) {\n+                    tempUncompOut \u003d fs.create(nextSegment, false);\n+                  }\n+\n+                  continue;\n+                } catch (IOException e) {\n+                  // no code -- file did not already exist\n+                }\n+              } catch (IOException e) {\n+                // no code -- file exists now, or directory bad. We try three\n+                // times.\n+              }\n+            }\n+\n+            if (debug) {\n+              LOG.debug(\"Creating \" + nextSegment\n+                  + \" for a job with a submit time of \" + job.getSubmitTime());\n+            }\n+\n+            deletees.add(nextSegment);\n+\n+            tempPaths.add(nextSegment);\n+\n+            CompressionCodec codec \u003d\n+                new CompressionCodecFactory(conf).getCodec(nextSegment);\n+            OutputStream output;\n+            Compressor compressor \u003d null;\n+            if (codec !\u003d null) {\n+              compressor \u003d CodecPool.getCompressor(codec);\n+              output \u003d codec.createOutputStream(tempUncompOut, compressor);\n+            } else {\n+              output \u003d tempUncompOut;\n+            }\n+\n+            tempUncompOut \u003d null;\n+\n+            tempGen \u003d outFactory.createJsonGenerator(output, JsonEncoding.UTF8);\n+            if (debug) {\n+              tempGen.useDefaultPrettyPrinter();\n+            }\n+\n+            long currentIntervalNumber \u003d\n+                (job.getSubmitTime() - firstJobSubmitTime) / inputCycle;\n+\n+            currentIntervalEnd \u003d\n+                firstJobSubmitTime + ((currentIntervalNumber + 1) * inputCycle);\n+          }\n+\n+          // the temp files contain UDadjusted times, but each temp file\u0027s\n+          // content is in the same input cycle interval.\n+          tempGen.writeObject(job);\n+\n+          job \u003d reader.nextJob();\n+        }\n+      } catch (DeskewedJobTraceReader.OutOfOrderException e) {\n+        return OUT_OF_ORDER_JOBS;\n+      } finally {\n+        if (tempGen !\u003d null) {\n+          tempGen.close();\n+        }\n+      }\n+\n+      if (lastJobSubmitTime \u003c\u003d firstJobSubmitTime) {\n+        LOG.error(\"All of your job[s] have the same submit time.\"\n+            + \"  Please just use your input file.\");\n+\n+        return ALL_JOBS_SIMULTANEOUS;\n+      }\n+\n+      double submitTimeSpan \u003d lastJobSubmitTime - firstJobSubmitTime;\n+\n+      LOG.warn(\"Your input trace spans \"\n+          + (lastJobSubmitTime - firstJobSubmitTime) + \" ticks.\");\n+\n+      double foldingRatio \u003d\n+          submitTimeSpan * (numberJobs + 1) / numberJobs / inputCycle;\n+\n+      if (debug) {\n+        LOG.warn(\"run: submitTimeSpan \u003d \" + submitTimeSpan + \", numberJobs \u003d \"\n+            + numberJobs + \", inputCycle \u003d \" + inputCycle);\n+      }\n+\n+      if (reader.neededSkewBufferSize() \u003e 0) {\n+        LOG.warn(\"You needed a -skew-buffer-length of \"\n+            + reader.neededSkewBufferSize() + \" but no more, for this input.\");\n+      }\n+\n+      double tProbability \u003d timeDilation * concentration / foldingRatio;\n+\n+      if (debug) {\n+        LOG.warn(\"run: timeDilation \u003d \" + timeDilation + \", concentration \u003d \"\n+            + concentration + \", foldingRatio \u003d \" + foldingRatio);\n+        LOG.warn(\"The transcription probability is \" + tProbability);\n+      }\n+\n+      transcriptionRateInteger \u003d (int) Math.floor(tProbability);\n+      transcriptionRateFraction \u003d tProbability - Math.floor(tProbability);\n+\n+      // Now read all the inputs in parallel\n+      heap \u003d\n+          new PriorityQueue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e(tempPaths.size(),\n+              new JobEntryComparator());\n+\n+      for (Path tempPath : tempPaths) {\n+        JobTraceReader thisReader \u003d new JobTraceReader(tempPath, conf);\n+\n+        closees.add(thisReader);\n+\n+        LoggedJob streamFirstJob \u003d thisReader.getNext();\n+\n+        long thisIndex \u003d\n+            (streamFirstJob.getSubmitTime() - firstJobSubmitTime) / inputCycle;\n+\n+        if (debug) {\n+          LOG.debug(\"A job with submit time of \"\n+              + streamFirstJob.getSubmitTime() + \" is in interval # \"\n+              + thisIndex);\n+        }\n+\n+        adjustJobTimes(streamFirstJob);\n+\n+        if (debug) {\n+          LOG.debug(\"That job\u0027s submit time is adjusted to \"\n+              + streamFirstJob.getSubmitTime());\n+        }\n+\n+        heap\n+            .add(new Pair\u003cLoggedJob, JobTraceReader\u003e(streamFirstJob, thisReader));\n+      }\n+\n+      Pair\u003cLoggedJob, JobTraceReader\u003e next \u003d heap.poll();\n+\n+      while (next !\u003d null) {\n+        maybeOutput(next.first());\n+\n+        if (debug) {\n+          LOG.debug(\"The most recent job has an adjusted submit time of \"\n+              + next.first().getSubmitTime());\n+          LOG.debug(\" Its replacement in the heap will come from input engine \"\n+              + next.second());\n+        }\n+\n+        LoggedJob replacement \u003d next.second().getNext();\n+\n+        if (replacement \u003d\u003d null) {\n+          next.second().close();\n+\n+          if (debug) {\n+            LOG.debug(\"That input engine is depleted.\");\n+          }\n+        } else {\n+          adjustJobTimes(replacement);\n+\n+          if (debug) {\n+            LOG.debug(\"The replacement has an adjusted submit time of \"\n+                + replacement.getSubmitTime());\n+          }\n+\n+          heap.add(new Pair\u003cLoggedJob, JobTraceReader\u003e(replacement, next\n+              .second()));\n+        }\n+\n+        next \u003d heap.poll();\n+      }\n+    } finally {\n+      IOUtils.cleanup(null, reader);\n+      if (outGen !\u003d null) {\n+        outGen.close();\n+      }\n+      for (Pair\u003cLoggedJob, JobTraceReader\u003e heapEntry : heap) {\n+        heapEntry.second().close();\n+      }\n+      for (Closeable closee : closees) {\n+        closee.close();\n+      }\n+      if (!debug) {\n+        Configuration conf \u003d getConf();\n+\n+        for (Path deletee : deletees) {\n+          FileSystem fs \u003d deletee.getFileSystem(conf);\n+\n+          try {\n+            fs.delete(deletee, false);\n+          } catch (IOException e) {\n+            // no code\n+          }\n+        }\n+      }\n+    }\n+\n+    return 0;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public int run() throws IOException {\n    class JobEntryComparator implements\n        Comparator\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e {\n      public int compare(Pair\u003cLoggedJob, JobTraceReader\u003e p1,\n          Pair\u003cLoggedJob, JobTraceReader\u003e p2) {\n        LoggedJob j1 \u003d p1.first();\n        LoggedJob j2 \u003d p2.first();\n\n        return (j1.getSubmitTime() \u003c j2.getSubmitTime()) ? -1 : (j1\n            .getSubmitTime() \u003d\u003d j2.getSubmitTime()) ? 0 : 1;\n      }\n    }\n\n    ObjectMapper outMapper \u003d new ObjectMapper();\n    outMapper.configure(\n        SerializationConfig.Feature.CAN_OVERRIDE_ACCESS_MODIFIERS, true);\n    JsonFactory outFactory \u003d outMapper.getJsonFactory();\n\n    // we initialize an empty heap so if we take an error before establishing\n    // a real one the finally code goes through\n    Queue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e heap \u003d\n        new PriorityQueue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e();\n\n    try {\n      LoggedJob job \u003d reader.nextJob();\n\n      if (job \u003d\u003d null) {\n        LOG.error(\"The job trace is empty\");\n\n        return EMPTY_JOB_TRACE;\n      }\n      \n      // If starts-after time is specified, skip the number of jobs till we reach\n      // the starting time limit.\n      if (startsAfter \u003e 0) {\n        LOG.info(\"starts-after time is specified. Initial job submit time : \" \n                 + job.getSubmitTime());\n\n        long approximateTime \u003d job.getSubmitTime() + startsAfter;\n        job \u003d reader.nextJob();\n        long skippedCount \u003d 0;\n        while (job !\u003d null \u0026\u0026 job.getSubmitTime() \u003c approximateTime) {\n          job \u003d reader.nextJob();\n          skippedCount++;\n        }\n\n        LOG.debug(\"Considering jobs with submit time greater than \" \n                  + startsAfter + \" ms. Skipped \" + skippedCount + \" jobs.\");\n\n        if (job \u003d\u003d null) {\n          LOG.error(\"No more jobs to process in the trace with \u0027starts-after\u0027\"+\n                    \" set to \" + startsAfter + \"ms.\");\n          return EMPTY_JOB_TRACE;\n        }\n        LOG.info(\"The first job has a submit time of \" + job.getSubmitTime());\n      }\n\n      firstJobSubmitTime \u003d job.getSubmitTime();\n      long lastJobSubmitTime \u003d firstJobSubmitTime;\n\n      int numberJobs \u003d 0;\n\n      long currentIntervalEnd \u003d Long.MIN_VALUE;\n\n      Path nextSegment \u003d null;\n      OutputStream tempUncompOut \u003d null;\n      JsonGenerator tempGen \u003d null;\n\n      if (debug) {\n        LOG.debug(\"The first job has a submit time of \" + firstJobSubmitTime);\n      }\n\n      final Configuration conf \u003d getConf();\n\n      try {\n        // At the top of this loop, skewBuffer has at most\n        // skewBufferLength entries.\n        while (job !\u003d null) {\n          final Random tempNameGenerator \u003d new Random();\n\n          lastJobSubmitTime \u003d job.getSubmitTime();\n\n          ++numberJobs;\n\n          if (job.getSubmitTime() \u003e\u003d currentIntervalEnd) {\n            if (tempGen !\u003d null) {\n              tempGen.close();\n            }\n            for (int i \u003d 0; i \u003c 3 \u0026\u0026 tempUncompOut \u003d\u003d null; ++i) {\n              try {\n                nextSegment \u003d\n                    new Path(tempDir, \"segment-\" + tempNameGenerator.nextLong()\n                        + \".json.gz\");\n\n                if (debug) {\n                  LOG.debug(\"The next segment name is \" + nextSegment);\n                }\n\n                FileSystem fs \u003d nextSegment.getFileSystem(conf);\n\n                try {\n                  if (!fs.exists(nextSegment)) {\n                    tempUncompOut \u003d fs.create(nextSegment, false);\n                  }\n\n                  continue;\n                } catch (IOException e) {\n                  // no code -- file did not already exist\n                }\n              } catch (IOException e) {\n                // no code -- file exists now, or directory bad. We try three\n                // times.\n              }\n            }\n\n            if (debug) {\n              LOG.debug(\"Creating \" + nextSegment\n                  + \" for a job with a submit time of \" + job.getSubmitTime());\n            }\n\n            deletees.add(nextSegment);\n\n            tempPaths.add(nextSegment);\n\n            CompressionCodec codec \u003d\n                new CompressionCodecFactory(conf).getCodec(nextSegment);\n            OutputStream output;\n            Compressor compressor \u003d null;\n            if (codec !\u003d null) {\n              compressor \u003d CodecPool.getCompressor(codec);\n              output \u003d codec.createOutputStream(tempUncompOut, compressor);\n            } else {\n              output \u003d tempUncompOut;\n            }\n\n            tempUncompOut \u003d null;\n\n            tempGen \u003d outFactory.createJsonGenerator(output, JsonEncoding.UTF8);\n            if (debug) {\n              tempGen.useDefaultPrettyPrinter();\n            }\n\n            long currentIntervalNumber \u003d\n                (job.getSubmitTime() - firstJobSubmitTime) / inputCycle;\n\n            currentIntervalEnd \u003d\n                firstJobSubmitTime + ((currentIntervalNumber + 1) * inputCycle);\n          }\n\n          // the temp files contain UDadjusted times, but each temp file\u0027s\n          // content is in the same input cycle interval.\n          tempGen.writeObject(job);\n\n          job \u003d reader.nextJob();\n        }\n      } catch (DeskewedJobTraceReader.OutOfOrderException e) {\n        return OUT_OF_ORDER_JOBS;\n      } finally {\n        if (tempGen !\u003d null) {\n          tempGen.close();\n        }\n      }\n\n      if (lastJobSubmitTime \u003c\u003d firstJobSubmitTime) {\n        LOG.error(\"All of your job[s] have the same submit time.\"\n            + \"  Please just use your input file.\");\n\n        return ALL_JOBS_SIMULTANEOUS;\n      }\n\n      double submitTimeSpan \u003d lastJobSubmitTime - firstJobSubmitTime;\n\n      LOG.warn(\"Your input trace spans \"\n          + (lastJobSubmitTime - firstJobSubmitTime) + \" ticks.\");\n\n      double foldingRatio \u003d\n          submitTimeSpan * (numberJobs + 1) / numberJobs / inputCycle;\n\n      if (debug) {\n        LOG.warn(\"run: submitTimeSpan \u003d \" + submitTimeSpan + \", numberJobs \u003d \"\n            + numberJobs + \", inputCycle \u003d \" + inputCycle);\n      }\n\n      if (reader.neededSkewBufferSize() \u003e 0) {\n        LOG.warn(\"You needed a -skew-buffer-length of \"\n            + reader.neededSkewBufferSize() + \" but no more, for this input.\");\n      }\n\n      double tProbability \u003d timeDilation * concentration / foldingRatio;\n\n      if (debug) {\n        LOG.warn(\"run: timeDilation \u003d \" + timeDilation + \", concentration \u003d \"\n            + concentration + \", foldingRatio \u003d \" + foldingRatio);\n        LOG.warn(\"The transcription probability is \" + tProbability);\n      }\n\n      transcriptionRateInteger \u003d (int) Math.floor(tProbability);\n      transcriptionRateFraction \u003d tProbability - Math.floor(tProbability);\n\n      // Now read all the inputs in parallel\n      heap \u003d\n          new PriorityQueue\u003cPair\u003cLoggedJob, JobTraceReader\u003e\u003e(tempPaths.size(),\n              new JobEntryComparator());\n\n      for (Path tempPath : tempPaths) {\n        JobTraceReader thisReader \u003d new JobTraceReader(tempPath, conf);\n\n        closees.add(thisReader);\n\n        LoggedJob streamFirstJob \u003d thisReader.getNext();\n\n        long thisIndex \u003d\n            (streamFirstJob.getSubmitTime() - firstJobSubmitTime) / inputCycle;\n\n        if (debug) {\n          LOG.debug(\"A job with submit time of \"\n              + streamFirstJob.getSubmitTime() + \" is in interval # \"\n              + thisIndex);\n        }\n\n        adjustJobTimes(streamFirstJob);\n\n        if (debug) {\n          LOG.debug(\"That job\u0027s submit time is adjusted to \"\n              + streamFirstJob.getSubmitTime());\n        }\n\n        heap\n            .add(new Pair\u003cLoggedJob, JobTraceReader\u003e(streamFirstJob, thisReader));\n      }\n\n      Pair\u003cLoggedJob, JobTraceReader\u003e next \u003d heap.poll();\n\n      while (next !\u003d null) {\n        maybeOutput(next.first());\n\n        if (debug) {\n          LOG.debug(\"The most recent job has an adjusted submit time of \"\n              + next.first().getSubmitTime());\n          LOG.debug(\" Its replacement in the heap will come from input engine \"\n              + next.second());\n        }\n\n        LoggedJob replacement \u003d next.second().getNext();\n\n        if (replacement \u003d\u003d null) {\n          next.second().close();\n\n          if (debug) {\n            LOG.debug(\"That input engine is depleted.\");\n          }\n        } else {\n          adjustJobTimes(replacement);\n\n          if (debug) {\n            LOG.debug(\"The replacement has an adjusted submit time of \"\n                + replacement.getSubmitTime());\n          }\n\n          heap.add(new Pair\u003cLoggedJob, JobTraceReader\u003e(replacement, next\n              .second()));\n        }\n\n        next \u003d heap.poll();\n      }\n    } finally {\n      IOUtils.cleanup(null, reader);\n      if (outGen !\u003d null) {\n        outGen.close();\n      }\n      for (Pair\u003cLoggedJob, JobTraceReader\u003e heapEntry : heap) {\n        heapEntry.second().close();\n      }\n      for (Closeable closee : closees) {\n        closee.close();\n      }\n      if (!debug) {\n        Configuration conf \u003d getConf();\n\n        for (Path deletee : deletees) {\n          FileSystem fs \u003d deletee.getFileSystem(conf);\n\n          try {\n            fs.delete(deletee, false);\n          } catch (IOException e) {\n            // no code\n          }\n        }\n      }\n    }\n\n    return 0;\n  }",
      "path": "mapreduce/src/tools/org/apache/hadoop/tools/rumen/Folder.java"
    }
  }
}