{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSNamesystem.java",
  "functionName": "metaSave",
  "functionId": "metaSave___filename-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
  "functionStartLine": 1878,
  "functionEndLine": 1898,
  "numCommitsSeen": 899,
  "timeTaken": 56322,
  "changeHistory": [
    "1824aee9da4056de0fb638906b2172e486bbebe7",
    "33c62f8f4e94442825fe286c2b18518925d980e6",
    "7a3188d054481b9bd563e337901e93476303ce7f",
    "1bea785020a538115b3e08f41ff88167033d2775",
    "9d3e4cccf9cd0ffb60ee0e7c65cea5ae3c8015c2",
    "ff0b99eafeda035ebe0dc82cfe689808047a8893",
    "e2efe52ac89e9f8298be139873e7a0d24392ef11",
    "3bf09c51501a23b7fa28fd0a0c4c0965858d026c",
    "cdb292f44caff9763631d9e9bcd69c375a7cddea",
    "6449f524552f8c24d20b314ad21f6c579fa08e85",
    "31c91706f7d17da006ef2d6c541f8dd092fae077",
    "ab0402bc1def44e3d52eea517f4132c460bd5f87",
    "9992cae54120d2742922745c1f513c6bfbde67a9",
    "96f9fc91993b04166f30fdf2dc5145ac91dbf1df",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "371f4a59059322000a40eb4bdf5386b96b626ece",
    "969a263188f7015261719fe45fa1505121ebb80e",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "1824aee9da4056de0fb638906b2172e486bbebe7": "Ybodychange",
    "33c62f8f4e94442825fe286c2b18518925d980e6": "Ybodychange",
    "7a3188d054481b9bd563e337901e93476303ce7f": "Ybodychange",
    "1bea785020a538115b3e08f41ff88167033d2775": "Ybodychange",
    "9d3e4cccf9cd0ffb60ee0e7c65cea5ae3c8015c2": "Ybodychange",
    "ff0b99eafeda035ebe0dc82cfe689808047a8893": "Ybodychange",
    "e2efe52ac89e9f8298be139873e7a0d24392ef11": "Ybodychange",
    "3bf09c51501a23b7fa28fd0a0c4c0965858d026c": "Ybodychange",
    "cdb292f44caff9763631d9e9bcd69c375a7cddea": "Ybodychange",
    "6449f524552f8c24d20b314ad21f6c579fa08e85": "Ybodychange",
    "31c91706f7d17da006ef2d6c541f8dd092fae077": "Ybodychange",
    "ab0402bc1def44e3d52eea517f4132c460bd5f87": "Ybodychange",
    "9992cae54120d2742922745c1f513c6bfbde67a9": "Ybodychange",
    "96f9fc91993b04166f30fdf2dc5145ac91dbf1df": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "371f4a59059322000a40eb4bdf5386b96b626ece": "Ybodychange",
    "969a263188f7015261719fe45fa1505121ebb80e": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "1824aee9da4056de0fb638906b2172e486bbebe7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15217 Add more information to longest write/read lock held log\n\n",
      "commitDate": "18/04/20 1:52 PM",
      "commitName": "1824aee9da4056de0fb638906b2172e486bbebe7",
      "commitAuthor": "Toshihiro Suzuki",
      "commitDateOld": "25/03/20 10:28 AM",
      "commitNameOld": "a700803a18fb957d2799001a2ce1dcb70f75c080",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 24.14,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   void metaSave(String filename) throws IOException {\n     String operationName \u003d \"metaSave\";\n     checkSuperuserPrivilege(operationName);\n     checkOperation(OperationCategory.READ);\n     readLock();\n     try {\n       checkOperation(OperationCategory.READ);\n       synchronized(metaSaveLock) {\n         File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n         PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n                 new OutputStreamWriter(Files.newOutputStream(file.toPath()),\n                         Charsets.UTF_8)));\n         metaSave(out);\n         out.flush();\n         out.close();\n       }\n     } finally {\n-      readUnlock(operationName);\n+      readUnlock(operationName, getLockReportInfoSupplier(null));\n     }\n     logAuditEvent(true, operationName, null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    String operationName \u003d \"metaSave\";\n    checkSuperuserPrivilege(operationName);\n    checkOperation(OperationCategory.READ);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      synchronized(metaSaveLock) {\n        File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n        PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n                new OutputStreamWriter(Files.newOutputStream(file.toPath()),\n                        Charsets.UTF_8)));\n        metaSave(out);\n        out.flush();\n        out.close();\n      }\n    } finally {\n      readUnlock(operationName, getLockReportInfoSupplier(null));\n    }\n    logAuditEvent(true, operationName, null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "33c62f8f4e94442825fe286c2b18518925d980e6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14497. Write lock held by metasave impact following RPC processing. Contributed by He Xiaoqiao.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "30/05/19 1:30 PM",
      "commitName": "33c62f8f4e94442825fe286c2b18518925d980e6",
      "commitAuthor": "He Xiaoqiao",
      "commitDateOld": "02/05/19 12:58 PM",
      "commitNameOld": "7a3188d054481b9bd563e337901e93476303ce7f",
      "commitAuthorOld": "Giovanni Matteo Fumarola",
      "daysBetweenCommits": 28.02,
      "commitsBetweenForRepo": 154,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,21 @@\n   void metaSave(String filename) throws IOException {\n     String operationName \u003d \"metaSave\";\n     checkSuperuserPrivilege(operationName);\n     checkOperation(OperationCategory.READ);\n-    writeLock();\n+    readLock();\n     try {\n       checkOperation(OperationCategory.READ);\n-      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n-      PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n-          new OutputStreamWriter(Files.newOutputStream(file.toPath()),\n-              Charsets.UTF_8)));\n-      metaSave(out);\n-      out.flush();\n-      out.close();\n+      synchronized(metaSaveLock) {\n+        File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n+        PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n+                new OutputStreamWriter(Files.newOutputStream(file.toPath()),\n+                        Charsets.UTF_8)));\n+        metaSave(out);\n+        out.flush();\n+        out.close();\n+      }\n     } finally {\n-      writeUnlock(operationName);\n+      readUnlock(operationName);\n     }\n     logAuditEvent(true, operationName, null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    String operationName \u003d \"metaSave\";\n    checkSuperuserPrivilege(operationName);\n    checkOperation(OperationCategory.READ);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      synchronized(metaSaveLock) {\n        File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n        PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n                new OutputStreamWriter(Files.newOutputStream(file.toPath()),\n                        Charsets.UTF_8)));\n        metaSave(out);\n        out.flush();\n        out.close();\n      }\n    } finally {\n      readUnlock(operationName);\n    }\n    logAuditEvent(true, operationName, null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "7a3188d054481b9bd563e337901e93476303ce7f": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16282. Avoid FileStream to improve performance. Contributed by Ayush Saxena.\n",
      "commitDate": "02/05/19 12:58 PM",
      "commitName": "7a3188d054481b9bd563e337901e93476303ce7f",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "16/04/19 10:34 AM",
      "commitNameOld": "be6c8014e66be919388269b70cb2966c35b8c578",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 16.1,
      "commitsBetweenForRepo": 81,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,19 @@\n   void metaSave(String filename) throws IOException {\n     String operationName \u003d \"metaSave\";\n     checkSuperuserPrivilege(operationName);\n     checkOperation(OperationCategory.READ);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.READ);\n       File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n-          new OutputStreamWriter(new FileOutputStream(file), Charsets.UTF_8)));\n+          new OutputStreamWriter(Files.newOutputStream(file.toPath()),\n+              Charsets.UTF_8)));\n       metaSave(out);\n       out.flush();\n       out.close();\n     } finally {\n       writeUnlock(operationName);\n     }\n     logAuditEvent(true, operationName, null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    String operationName \u003d \"metaSave\";\n    checkSuperuserPrivilege(operationName);\n    checkOperation(OperationCategory.READ);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n          new OutputStreamWriter(Files.newOutputStream(file.toPath()),\n              Charsets.UTF_8)));\n      metaSave(out);\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock(operationName);\n    }\n    logAuditEvent(true, operationName, null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "1bea785020a538115b3e08f41ff88167033d2775": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14081. hdfs dfsadmin -metasave metasave_test results NPE. Contributed by Shweta Yakkali.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "20/02/19 2:28 PM",
      "commitName": "1bea785020a538115b3e08f41ff88167033d2775",
      "commitAuthor": "Shweta Yakkali",
      "commitDateOld": "14/02/19 8:43 AM",
      "commitNameOld": "0d7a5ac5f526801367a9ec963e6d72783b637d55",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 6.24,
      "commitsBetweenForRepo": 55,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   void metaSave(String filename) throws IOException {\n     String operationName \u003d \"metaSave\";\n     checkSuperuserPrivilege(operationName);\n-    checkOperation(OperationCategory.UNCHECKED);\n+    checkOperation(OperationCategory.READ);\n     writeLock();\n     try {\n-      checkOperation(OperationCategory.UNCHECKED);\n+      checkOperation(OperationCategory.READ);\n       File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n           new OutputStreamWriter(new FileOutputStream(file), Charsets.UTF_8)));\n       metaSave(out);\n       out.flush();\n       out.close();\n     } finally {\n       writeUnlock(operationName);\n     }\n     logAuditEvent(true, operationName, null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    String operationName \u003d \"metaSave\";\n    checkSuperuserPrivilege(operationName);\n    checkOperation(OperationCategory.READ);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n          new OutputStreamWriter(new FileOutputStream(file), Charsets.UTF_8)));\n      metaSave(out);\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock(operationName);\n    }\n    logAuditEvent(true, operationName, null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "9d3e4cccf9cd0ffb60ee0e7c65cea5ae3c8015c2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5040.Audit log for admin commands/ logging output of all DFS admin commands. Contributed by Kuhu Shukla.\n",
      "commitDate": "26/09/17 9:29 AM",
      "commitName": "9d3e4cccf9cd0ffb60ee0e7c65cea5ae3c8015c2",
      "commitAuthor": "Brahma Reddy Battula",
      "commitDateOld": "24/09/17 9:03 PM",
      "commitNameOld": "d0b2c5850b523a3888b2fadcfcdf6edbed33f221",
      "commitAuthorOld": "Anu Engineer",
      "daysBetweenCommits": 1.52,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,18 @@\n   void metaSave(String filename) throws IOException {\n-    checkSuperuserPrivilege();\n+    String operationName \u003d \"metaSave\";\n+    checkSuperuserPrivilege(operationName);\n     checkOperation(OperationCategory.UNCHECKED);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.UNCHECKED);\n       File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n           new OutputStreamWriter(new FileOutputStream(file), Charsets.UTF_8)));\n       metaSave(out);\n       out.flush();\n       out.close();\n     } finally {\n-      writeUnlock(\"metaSave\");\n+      writeUnlock(operationName);\n     }\n+    logAuditEvent(true, operationName, null);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    String operationName \u003d \"metaSave\";\n    checkSuperuserPrivilege(operationName);\n    checkOperation(OperationCategory.UNCHECKED);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n          new OutputStreamWriter(new FileOutputStream(file), Charsets.UTF_8)));\n      metaSave(out);\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock(operationName);\n    }\n    logAuditEvent(true, operationName, null);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "ff0b99eafeda035ebe0dc82cfe689808047a8893": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10872. Add MutableRate metrics for FSNamesystemLock operations. Contributed by Erik Krogen.\n",
      "commitDate": "14/11/16 11:05 AM",
      "commitName": "ff0b99eafeda035ebe0dc82cfe689808047a8893",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "08/11/16 6:17 PM",
      "commitNameOld": "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 5.7,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   void metaSave(String filename) throws IOException {\n     checkSuperuserPrivilege();\n     checkOperation(OperationCategory.UNCHECKED);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.UNCHECKED);\n       File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n           new OutputStreamWriter(new FileOutputStream(file), Charsets.UTF_8)));\n       metaSave(out);\n       out.flush();\n       out.close();\n     } finally {\n-      writeUnlock();\n+      writeUnlock(\"metaSave\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    checkSuperuserPrivilege();\n    checkOperation(OperationCategory.UNCHECKED);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n          new OutputStreamWriter(new FileOutputStream(file), Charsets.UTF_8)));\n      metaSave(out);\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock(\"metaSave\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "e2efe52ac89e9f8298be139873e7a0d24392ef11": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4996. ClientProtocol#metaSave can be made idempotent by overwriting the output file instead of appending to it. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1504679 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/07/13 2:57 PM",
      "commitName": "e2efe52ac89e9f8298be139873e7a0d24392ef11",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "12/07/13 10:44 AM",
      "commitNameOld": "b47df4e3b9089ad84185dd62693ae363ac994358",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 6.18,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   void metaSave(String filename) throws IOException {\n     checkSuperuserPrivilege();\n     checkOperation(OperationCategory.UNCHECKED);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.UNCHECKED);\n       File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n-          new OutputStreamWriter(new FileOutputStream(file, true), Charsets.UTF_8)));\n+          new OutputStreamWriter(new FileOutputStream(file), Charsets.UTF_8)));\n       metaSave(out);\n       out.flush();\n       out.close();\n     } finally {\n       writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    checkSuperuserPrivilege();\n    checkOperation(OperationCategory.UNCHECKED);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n          new OutputStreamWriter(new FileOutputStream(file), Charsets.UTF_8)));\n      metaSave(out);\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "3bf09c51501a23b7fa28fd0a0c4c0965858d026c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4591. HA clients can fail to fail over while Standby NN is performing long checkpoint. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1456107 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/03/13 12:51 PM",
      "commitName": "3bf09c51501a23b7fa28fd0a0c4c0965858d026c",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "12/03/13 7:32 PM",
      "commitNameOld": "86a940f7adc5bd9c9eaea2283df5e014e5079ab6",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 0.72,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,16 @@\n   void metaSave(String filename) throws IOException {\n     checkSuperuserPrivilege();\n+    checkOperation(OperationCategory.UNCHECKED);\n     writeLock();\n     try {\n+      checkOperation(OperationCategory.UNCHECKED);\n       File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n           new OutputStreamWriter(new FileOutputStream(file, true), Charsets.UTF_8)));\n       metaSave(out);\n       out.flush();\n       out.close();\n     } finally {\n       writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    checkSuperuserPrivilege();\n    checkOperation(OperationCategory.UNCHECKED);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n          new OutputStreamWriter(new FileOutputStream(file, true), Charsets.UTF_8)));\n      metaSave(out);\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "cdb292f44caff9763631d9e9bcd69c375a7cddea": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4222. NN is unresponsive and loses heartbeats from DNs when configured to use LDAP and LDAP has issues. Contributed by Xiaobo Peng and Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1448801 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/02/13 1:02 PM",
      "commitName": "cdb292f44caff9763631d9e9bcd69c375a7cddea",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "11/02/13 4:50 PM",
      "commitNameOld": "969e84decbc976bd98f1050aead695d15a024ab6",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 9.84,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   void metaSave(String filename) throws IOException {\n+    checkSuperuserPrivilege();\n     writeLock();\n     try {\n-      checkSuperuserPrivilege();\n       File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n           new OutputStreamWriter(new FileOutputStream(file, true), Charsets.UTF_8)));\n       metaSave(out);\n       out.flush();\n       out.close();\n     } finally {\n       writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    checkSuperuserPrivilege();\n    writeLock();\n    try {\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n          new OutputStreamWriter(new FileOutputStream(file, true), Charsets.UTF_8)));\n      metaSave(out);\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "6449f524552f8c24d20b314ad21f6c579fa08e85": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4032. Specify the charset explicitly rather than rely on the default. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1431179 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 6:30 PM",
      "commitName": "6449f524552f8c24d20b314ad21f6c579fa08e85",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "08/01/13 1:05 PM",
      "commitNameOld": "5cdb7e5ce7f0c3129749be8f29e2f11c0e0f2269",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 1.23,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   void metaSave(String filename) throws IOException {\n     writeLock();\n     try {\n       checkSuperuserPrivilege();\n       File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n-      PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n-          true)));\n+      PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n+          new OutputStreamWriter(new FileOutputStream(file, true), Charsets.UTF_8)));\n       metaSave(out);\n       out.flush();\n       out.close();\n     } finally {\n       writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    writeLock();\n    try {\n      checkSuperuserPrivilege();\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(\n          new OutputStreamWriter(new FileOutputStream(file, true), Charsets.UTF_8)));\n      metaSave(out);\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "31c91706f7d17da006ef2d6c541f8dd092fae077": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1972. Fencing mechanism for block invalidations and replications. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1221608 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/11 8:32 PM",
      "commitName": "31c91706f7d17da006ef2d6c541f8dd092fae077",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "20/12/11 7:03 PM",
      "commitNameOld": "36d1c49486587c2dbb193e8538b1d4510c462fa6",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,14 @@\n   void metaSave(String filename) throws IOException {\n     writeLock();\n     try {\n       checkSuperuserPrivilege();\n       File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n           true)));\n-  \n-      long totalInodes \u003d this.dir.totalInodes();\n-      long totalBlocks \u003d this.getBlocksTotal();\n-      out.println(totalInodes + \" files and directories, \" + totalBlocks\n-          + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\");\n-\n-      blockManager.metaSave(out);\n-\n+      metaSave(out);\n       out.flush();\n       out.close();\n     } finally {\n       writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    writeLock();\n    try {\n      checkSuperuserPrivilege();\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n          true)));\n      metaSave(out);\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "ab0402bc1def44e3d52eea517f4132c460bd5f87": {
      "type": "Ybodychange",
      "commitMessage": "Merging trunk to HDFS-1623 branch\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1177130 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/09/11 5:42 PM",
      "commitName": "ab0402bc1def44e3d52eea517f4132c460bd5f87",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "28/09/11 5:33 PM",
      "commitNameOld": "9992cae54120d2742922745c1f513c6bfbde67a9",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,21 @@\n   void metaSave(String filename) throws IOException {\n     writeLock();\n     try {\n       checkSuperuserPrivilege();\n       File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n           true)));\n   \n       long totalInodes \u003d this.dir.totalInodes();\n       long totalBlocks \u003d this.getBlocksTotal();\n       out.println(totalInodes + \" files and directories, \" + totalBlocks\n           + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\");\n \n-      final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n-      final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n-      blockManager.getDatanodeManager().fetchDatanodes(live, dead, false);\n-      out.println(\"Live Datanodes: \"+live.size());\n-      out.println(\"Dead Datanodes: \"+dead.size());\n       blockManager.metaSave(out);\n \n       out.flush();\n       out.close();\n     } finally {\n       writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    writeLock();\n    try {\n      checkSuperuserPrivilege();\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n          true)));\n  \n      long totalInodes \u003d this.dir.totalInodes();\n      long totalBlocks \u003d this.getBlocksTotal();\n      out.println(totalInodes + \" files and directories, \" + totalBlocks\n          + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\");\n\n      blockManager.metaSave(out);\n\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "9992cae54120d2742922745c1f513c6bfbde67a9": {
      "type": "Ybodychange",
      "commitMessage": "Reverting the previous trunk merge since it added other unintended changes in addition\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1177127 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/09/11 5:33 PM",
      "commitName": "9992cae54120d2742922745c1f513c6bfbde67a9",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "28/09/11 5:09 PM",
      "commitNameOld": "122113922fd398b1a76c1664b58a61661e936e30",
      "commitAuthorOld": "",
      "daysBetweenCommits": 0.02,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,26 @@\n   void metaSave(String filename) throws IOException {\n     writeLock();\n     try {\n       checkSuperuserPrivilege();\n       File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n           true)));\n   \n       long totalInodes \u003d this.dir.totalInodes();\n       long totalBlocks \u003d this.getBlocksTotal();\n       out.println(totalInodes + \" files and directories, \" + totalBlocks\n           + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\");\n \n+      final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n+      final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n+      blockManager.getDatanodeManager().fetchDatanodes(live, dead, false);\n+      out.println(\"Live Datanodes: \"+live.size());\n+      out.println(\"Dead Datanodes: \"+dead.size());\n       blockManager.metaSave(out);\n \n       out.flush();\n       out.close();\n     } finally {\n       writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    writeLock();\n    try {\n      checkSuperuserPrivilege();\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n          true)));\n  \n      long totalInodes \u003d this.dir.totalInodes();\n      long totalBlocks \u003d this.getBlocksTotal();\n      out.println(totalInodes + \" files and directories, \" + totalBlocks\n          + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\");\n\n      final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n      final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n      blockManager.getDatanodeManager().fetchDatanodes(live, dead, false);\n      out.println(\"Live Datanodes: \"+live.size());\n      out.println(\"Dead Datanodes: \"+dead.size());\n      blockManager.metaSave(out);\n\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "96f9fc91993b04166f30fdf2dc5145ac91dbf1df": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2363. Move datanodes size printing from FSNamesystem.metasave(..) to BlockManager.  Contributed by Uma Maheswara Rao G\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1176733 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/11 10:49 PM",
      "commitName": "96f9fc91993b04166f30fdf2dc5145ac91dbf1df",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "07/09/11 4:23 PM",
      "commitNameOld": "06e84a1bca19bd01568a3095e33944d4d6387fd3",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 20.27,
      "commitsBetweenForRepo": 140,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,21 @@\n   void metaSave(String filename) throws IOException {\n     writeLock();\n     try {\n       checkSuperuserPrivilege();\n       File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n           true)));\n   \n       long totalInodes \u003d this.dir.totalInodes();\n       long totalBlocks \u003d this.getBlocksTotal();\n       out.println(totalInodes + \" files and directories, \" + totalBlocks\n           + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\");\n \n-      final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n-      final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n-      blockManager.getDatanodeManager().fetchDatanodes(live, dead, false);\n-      out.println(\"Live Datanodes: \"+live.size());\n-      out.println(\"Dead Datanodes: \"+dead.size());\n       blockManager.metaSave(out);\n \n       out.flush();\n       out.close();\n     } finally {\n       writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    writeLock();\n    try {\n      checkSuperuserPrivilege();\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n          true)));\n  \n      long totalInodes \u003d this.dir.totalInodes();\n      long totalBlocks \u003d this.getBlocksTotal();\n      out.println(totalInodes + \" files and directories, \" + totalBlocks\n          + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\");\n\n      blockManager.metaSave(out);\n\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    writeLock();\n    try {\n      checkSuperuserPrivilege();\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n          true)));\n  \n      long totalInodes \u003d this.dir.totalInodes();\n      long totalBlocks \u003d this.getBlocksTotal();\n      out.println(totalInodes + \" files and directories, \" + totalBlocks\n          + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\");\n\n      final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n      final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n      blockManager.getDatanodeManager().fetchDatanodes(live, dead, false);\n      out.println(\"Live Datanodes: \"+live.size());\n      out.println(\"Dead Datanodes: \"+dead.size());\n      blockManager.metaSave(out);\n\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    writeLock();\n    try {\n      checkSuperuserPrivilege();\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n          true)));\n  \n      long totalInodes \u003d this.dir.totalInodes();\n      long totalBlocks \u003d this.getBlocksTotal();\n      out.println(totalInodes + \" files and directories, \" + totalBlocks\n          + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\");\n\n      final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n      final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n      blockManager.getDatanodeManager().fetchDatanodes(live, dead, false);\n      out.println(\"Live Datanodes: \"+live.size());\n      out.println(\"Dead Datanodes: \"+dead.size());\n      blockManager.metaSave(out);\n\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "371f4a59059322000a40eb4bdf5386b96b626ece": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2228. Move block and datanode code from FSNamesystem to BlockManager and DatanodeManager.  (szetszwo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154899 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/08/11 3:06 AM",
      "commitName": "371f4a59059322000a40eb4bdf5386b96b626ece",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "04/08/11 3:55 PM",
      "commitNameOld": "7fac946ac983e31613fd62836c8ac9c4a579210a",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 3.47,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,26 @@\n   void metaSave(String filename) throws IOException {\n     writeLock();\n     try {\n       checkSuperuserPrivilege();\n       File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n           true)));\n   \n       long totalInodes \u003d this.dir.totalInodes();\n       long totalBlocks \u003d this.getBlocksTotal();\n-  \n-      ArrayList\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n-      ArrayList\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n-      this.DFSNodesStatus(live, dead);\n-      \n-      String str \u003d totalInodes + \" files and directories, \" + totalBlocks\n-          + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\";\n-      out.println(str);\n+      out.println(totalInodes + \" files and directories, \" + totalBlocks\n+          + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\");\n+\n+      final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n+      final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n+      blockManager.getDatanodeManager().fetchDatanodes(live, dead, false);\n       out.println(\"Live Datanodes: \"+live.size());\n       out.println(\"Dead Datanodes: \"+dead.size());\n       blockManager.metaSave(out);\n \n       out.flush();\n       out.close();\n     } finally {\n       writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    writeLock();\n    try {\n      checkSuperuserPrivilege();\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n          true)));\n  \n      long totalInodes \u003d this.dir.totalInodes();\n      long totalBlocks \u003d this.getBlocksTotal();\n      out.println(totalInodes + \" files and directories, \" + totalBlocks\n          + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\");\n\n      final List\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n      final List\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n      blockManager.getDatanodeManager().fetchDatanodes(live, dead, false);\n      out.println(\"Live Datanodes: \"+live.size());\n      out.println(\"Dead Datanodes: \"+dead.size());\n      blockManager.metaSave(out);\n\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock();\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "969a263188f7015261719fe45fa1505121ebb80e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2191.  Move datanodeMap from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151339 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/11 10:46 PM",
      "commitName": "969a263188f7015261719fe45fa1505121ebb80e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "22/07/11 6:01 PM",
      "commitNameOld": "89537b7710b23db7abcd2a77f03818c06a5f5fa7",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.2,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,28 @@\n   void metaSave(String filename) throws IOException {\n     writeLock();\n     try {\n       checkSuperuserPrivilege();\n       File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n           true)));\n   \n       long totalInodes \u003d this.dir.totalInodes();\n       long totalBlocks \u003d this.getBlocksTotal();\n   \n       ArrayList\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n       ArrayList\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n       this.DFSNodesStatus(live, dead);\n       \n       String str \u003d totalInodes + \" files and directories, \" + totalBlocks\n           + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\";\n       out.println(str);\n       out.println(\"Live Datanodes: \"+live.size());\n       out.println(\"Dead Datanodes: \"+dead.size());\n       blockManager.metaSave(out);\n-  \n-      //\n-      // Dump all datanodes\n-      //\n-      datanodeDump(out);\n-  \n+\n       out.flush();\n       out.close();\n     } finally {\n       writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    writeLock();\n    try {\n      checkSuperuserPrivilege();\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n          true)));\n  \n      long totalInodes \u003d this.dir.totalInodes();\n      long totalBlocks \u003d this.getBlocksTotal();\n  \n      ArrayList\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n      ArrayList\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n      this.DFSNodesStatus(live, dead);\n      \n      String str \u003d totalInodes + \" files and directories, \" + totalBlocks\n          + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\";\n      out.println(str);\n      out.println(\"Live Datanodes: \"+live.size());\n      out.println(\"Dead Datanodes: \"+dead.size());\n      blockManager.metaSave(out);\n\n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock();\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,33 @@\n+  void metaSave(String filename) throws IOException {\n+    writeLock();\n+    try {\n+      checkSuperuserPrivilege();\n+      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n+      PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n+          true)));\n+  \n+      long totalInodes \u003d this.dir.totalInodes();\n+      long totalBlocks \u003d this.getBlocksTotal();\n+  \n+      ArrayList\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n+      ArrayList\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n+      this.DFSNodesStatus(live, dead);\n+      \n+      String str \u003d totalInodes + \" files and directories, \" + totalBlocks\n+          + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\";\n+      out.println(str);\n+      out.println(\"Live Datanodes: \"+live.size());\n+      out.println(\"Dead Datanodes: \"+dead.size());\n+      blockManager.metaSave(out);\n+  \n+      //\n+      // Dump all datanodes\n+      //\n+      datanodeDump(out);\n+  \n+      out.flush();\n+      out.close();\n+    } finally {\n+      writeUnlock();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void metaSave(String filename) throws IOException {\n    writeLock();\n    try {\n      checkSuperuserPrivilege();\n      File file \u003d new File(System.getProperty(\"hadoop.log.dir\"), filename);\n      PrintWriter out \u003d new PrintWriter(new BufferedWriter(new FileWriter(file,\n          true)));\n  \n      long totalInodes \u003d this.dir.totalInodes();\n      long totalBlocks \u003d this.getBlocksTotal();\n  \n      ArrayList\u003cDatanodeDescriptor\u003e live \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n      ArrayList\u003cDatanodeDescriptor\u003e dead \u003d new ArrayList\u003cDatanodeDescriptor\u003e();\n      this.DFSNodesStatus(live, dead);\n      \n      String str \u003d totalInodes + \" files and directories, \" + totalBlocks\n          + \" blocks \u003d \" + (totalInodes + totalBlocks) + \" total\";\n      out.println(str);\n      out.println(\"Live Datanodes: \"+live.size());\n      out.println(\"Dead Datanodes: \"+dead.size());\n      blockManager.metaSave(out);\n  \n      //\n      // Dump all datanodes\n      //\n      datanodeDump(out);\n  \n      out.flush();\n      out.close();\n    } finally {\n      writeUnlock();\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
    }
  }
}