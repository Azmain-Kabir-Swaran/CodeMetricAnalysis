{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RegularContainerAllocator.java",
  "functionName": "assignContainer",
  "functionId": "assignContainer___clusterResource-Resource__node-FiCaSchedulerNode__schedulerKey-SchedulerRequestKey__pendingAsk-PendingAsk__type-NodeType__rmContainer-RMContainer__schedulingMode-SchedulingMode__currentResoureLimits-ResourceLimits",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
  "functionStartLine": 508,
  "functionEndLine": 668,
  "numCommitsSeen": 566,
  "timeTaken": 24902,
  "changeHistory": [
    "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8",
    "09763925025a3709e6098186348e1afd80cb9f71",
    "12b7059ddc8d8f67dd7131565f03a0e09cb92ca7",
    "2064ca015d1584263aac0cc20c60b925a3aff612",
    "0712537e799bc03855d548d1f4bd690dd478b871",
    "e81596d06d226f1cfa44b2390ce3095ed4dee621",
    "2977bc6a141041ef7579efc416e93fc55e0c2a1a",
    "e0d131f055ee126052ad4d0f7b0d192e6c730188",
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
    "42f90ab885d9693fcc1e52f9637f7de4111110ae",
    "ae14e5d07f1b6702a5160637438028bb03d9387e",
    "fa7a43529d529f0006c8033c2003f15b9b93f103",
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
    "f9692770a58af0ab082eb7f15da9cbdcd177605b",
    "89cab1ba5f0671f8ef30dbe7432079c18362b434",
    "e5003be907acef87c2770e3f2914953f62017b0e",
    "ba2313d6145a1234777938a747187373f4cd58d9",
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
    "189a63a719c63b67a1783a280bfc2f72dcb55277",
    "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5",
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
    "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
    "487374b7fe0c92fc7eb1406c568952722b5d5b15",
    "e17e5ba9d7e2bd45ba6884f59f8045817594b284",
    "86358221fc85a7743052a0b4c1647353508bf308",
    "fdf042dfffa4d2474e3cac86cfb8fe9ee4648beb",
    "f2ea555ac6c06a3f2f6559731f48711fff05d3f1",
    "9c22065109a77681bc2534063eabe8692fbcb3cd",
    "424fd9494f144c035fdef8c533be51e2027ad8d9",
    "44b6261bfacddea88a3cf02d406f970bbbb98d04",
    "d0a5e43de73119e57d12f2ec89a9d1a192cde204",
    "1e513bfc68c8de2976e3340cb83b6763c5d16813",
    "942e2ebaa54306ffc5b0ffb403e552764a40d58c",
    "a2c42330047bf955a6a585dcddf798920d4c8640",
    "ca8024673178fa1c80224b390dfba932921693d9",
    "453926397182078c65a4428eb5de5a90d6af6448",
    "90ba993bc72e374f99c44d0770f55aeaa8342f2d",
    "e1fdf62123625e4ba399af02f8aad500637d29d1",
    "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76",
    "126dd6adefeb00e4ba81ea137d63a8a76b75c3bd",
    "ffdf980b2056b2a1b31ccb19746f23c31f7d08ef",
    "f24dcb3449c77da665058427bc7fa480cad507fc",
    "1e6dfa7472ad78a252d05c8ebffe086d938b61fa",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517"
  ],
  "changeHistoryShort": {
    "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8": "Ybodychange",
    "09763925025a3709e6098186348e1afd80cb9f71": "Ybodychange",
    "12b7059ddc8d8f67dd7131565f03a0e09cb92ca7": "Ybodychange",
    "2064ca015d1584263aac0cc20c60b925a3aff612": "Ybodychange",
    "0712537e799bc03855d548d1f4bd690dd478b871": "Ybodychange",
    "e81596d06d226f1cfa44b2390ce3095ed4dee621": "Ybodychange",
    "2977bc6a141041ef7579efc416e93fc55e0c2a1a": "Ymultichange(Yparameterchange,Ybodychange)",
    "e0d131f055ee126052ad4d0f7b0d192e6c730188": "Ybodychange",
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828": "Ymultichange(Yparameterchange,Ybodychange)",
    "42f90ab885d9693fcc1e52f9637f7de4111110ae": "Ybodychange",
    "ae14e5d07f1b6702a5160637438028bb03d9387e": "Ybodychange",
    "fa7a43529d529f0006c8033c2003f15b9b93f103": "Ybodychange",
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913": "Ybodychange",
    "f9692770a58af0ab082eb7f15da9cbdcd177605b": "Ybodychange",
    "89cab1ba5f0671f8ef30dbe7432079c18362b434": "Ybodychange",
    "e5003be907acef87c2770e3f2914953f62017b0e": "Ybodychange",
    "ba2313d6145a1234777938a747187373f4cd58d9": "Ymultichange(Ymovefromfile,Yreturntypechange,Ybodychange,Yparameterchange)",
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b": "Ymultichange(Ymovefromfile,Ybodychange,Yparameterchange)",
    "189a63a719c63b67a1783a280bfc2f72dcb55277": "Ymultichange(Yparameterchange,Ybodychange)",
    "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5": "Ybodychange",
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5": "Ymultichange(Yparameterchange,Ybodychange)",
    "afa5d4715a3aea2a6e93380b014c7bb8f0880383": "Ymultichange(Yreturntypechange,Ybodychange)",
    "487374b7fe0c92fc7eb1406c568952722b5d5b15": "Ymultichange(Yparameterchange,Ybodychange)",
    "e17e5ba9d7e2bd45ba6884f59f8045817594b284": "Ymultichange(Yparameterchange,Ybodychange)",
    "86358221fc85a7743052a0b4c1647353508bf308": "Ybodychange",
    "fdf042dfffa4d2474e3cac86cfb8fe9ee4648beb": "Ybodychange",
    "f2ea555ac6c06a3f2f6559731f48711fff05d3f1": "Ybodychange",
    "9c22065109a77681bc2534063eabe8692fbcb3cd": "Ymultichange(Yparameterchange,Ybodychange)",
    "424fd9494f144c035fdef8c533be51e2027ad8d9": "Ybodychange",
    "44b6261bfacddea88a3cf02d406f970bbbb98d04": "Ybodychange",
    "d0a5e43de73119e57d12f2ec89a9d1a192cde204": "Ybodychange",
    "1e513bfc68c8de2976e3340cb83b6763c5d16813": "Ybodychange",
    "942e2ebaa54306ffc5b0ffb403e552764a40d58c": "Ybodychange",
    "a2c42330047bf955a6a585dcddf798920d4c8640": "Ybodychange",
    "ca8024673178fa1c80224b390dfba932921693d9": "Ybodychange",
    "453926397182078c65a4428eb5de5a90d6af6448": "Ybodychange",
    "90ba993bc72e374f99c44d0770f55aeaa8342f2d": "Ybodychange",
    "e1fdf62123625e4ba399af02f8aad500637d29d1": "Yfilerename",
    "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76": "Yparameterchange",
    "126dd6adefeb00e4ba81ea137d63a8a76b75c3bd": "Ybodychange",
    "ffdf980b2056b2a1b31ccb19746f23c31f7d08ef": "Ybodychange",
    "f24dcb3449c77da665058427bc7fa480cad507fc": "Ybodychange",
    "1e6dfa7472ad78a252d05c8ebffe086d938b61fa": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yintroduced"
  },
  "changeHistoryDetails": {
    "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9664. Improve response of scheduler/app activities for better understanding. Contributed by Tao Yang.\n",
      "commitDate": "29/08/19 3:14 AM",
      "commitName": "8c0759d02a9a530cfdd25e0a8f410cd74a8ac4c8",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "06/06/19 6:59 AM",
      "commitNameOld": "09763925025a3709e6098186348e1afd80cb9f71",
      "commitAuthorOld": "Weiwei Yang",
      "daysBetweenCommits": 83.84,
      "commitsBetweenForRepo": 744,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,155 +1,161 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n       SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" application\u003d\" + application.getApplicationId()\n           + \" priority\u003d\" + schedulerKey.getPriority()\n           + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n     }\n \n     Resource capability \u003d pendingAsk.getPerAllocationResource();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(rc, capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for ask : \" + pendingAsk\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n           activitiesManager, node, application, schedulerKey,\n-          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n-              + getResourceDiagnostics(capability, totalResource));\n+          ActivityDiagnosticConstant.\n+              NODE_TOTAL_RESOURCE_INSUFFICIENT_FOR_REQUEST\n+              + getResourceDiagnostics(capability, totalResource),\n+          ActivityLevel.NODE);\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         schedulerKey, capability);\n \n     // Can we allocate a container on this node?\n     long availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n     // available resource for diagnostics collector\n     Resource availableForDC \u003d available;\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         rc.isAnyMajorResourceAboveZero(resourceNeedToUnReserve);\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     // Check if we need to kill some containers to allocate this one\n     List\u003cRMContainer\u003e toKillContainers \u003d null;\n     if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n       Resource availableAndKillable \u003d Resources.clone(available);\n       for (RMContainer killableContainer : node\n           .getKillableContainers().values()) {\n         if (null \u003d\u003d toKillContainers) {\n           toKillContainers \u003d new ArrayList\u003c\u003e();\n         }\n         toKillContainers.add(killableContainer);\n         Resources.addTo(availableAndKillable,\n                         killableContainer.getAllocatedResource());\n         if (Resources.fitsIn(rc, capability, availableAndKillable)) {\n           // Stop if we find enough spaces\n           availableContainers \u003d 1;\n           break;\n         }\n       }\n       availableForDC \u003d availableAndKillable;\n     }\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d application.findNodeToUnreserve(node,\n                   schedulerKey, resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                 activitiesManager, node, application, schedulerKey,\n                 ActivityDiagnosticConstant.\n-                    NODE_CAN_NOT_FIND_CONTAINER_TO_BE_UNRESERVED_WHEN_NEEDED);\n+                    NODE_CAN_NOT_FIND_CONTAINER_TO_BE_UNRESERVED_WHEN_NEEDED,\n+                ActivityLevel.NODE);\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n           pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             LOG.debug(\"we needed to unreserve to be able to allocate\");\n \n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                 activitiesManager, node, application, schedulerKey,\n-                ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n-                    + getResourceDiagnostics(capability, availableForDC));\n+                ActivityDiagnosticConstant.NODE_DO_NOT_HAVE_SUFFICIENT_RESOURCE\n+                    + getResourceDiagnostics(capability, availableForDC),\n+                ActivityLevel.NODE);\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ActivitiesLogger.APP.recordAppActivityWithoutAllocation(\n             activitiesManager, node, application, schedulerKey,\n-            ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n+            ActivityDiagnosticConstant.NODE_DO_NOT_HAVE_SUFFICIENT_RESOURCE\n                 + getResourceDiagnostics(capability, availableForDC),\n             rmContainer \u003d\u003d null ?\n-                ActivityState.RESERVED : ActivityState.RE_RESERVED);\n+                ActivityState.RESERVED : ActivityState.RE_RESERVED,\n+            ActivityLevel.NODE);\n         ContainerAllocation result \u003d new ContainerAllocation(null,\n             pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n           activitiesManager, node, application, schedulerKey,\n-          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n-              + getResourceDiagnostics(capability, availableForDC));\n+          ActivityDiagnosticConstant.NODE_DO_NOT_HAVE_SUFFICIENT_RESOURCE\n+              + getResourceDiagnostics(capability, availableForDC),\n+          ActivityLevel.NODE);\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n      SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" application\u003d\" + application.getApplicationId()\n          + \" priority\u003d\" + schedulerKey.getPriority()\n          + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n    }\n\n    Resource capability \u003d pendingAsk.getPerAllocationResource();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(rc, capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for ask : \" + pendingAsk\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, schedulerKey,\n          ActivityDiagnosticConstant.\n              NODE_TOTAL_RESOURCE_INSUFFICIENT_FOR_REQUEST\n              + getResourceDiagnostics(capability, totalResource),\n          ActivityLevel.NODE);\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        schedulerKey, capability);\n\n    // Can we allocate a container on this node?\n    long availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n    // available resource for diagnostics collector\n    Resource availableForDC \u003d available;\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        rc.isAnyMajorResourceAboveZero(resourceNeedToUnReserve);\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    // Check if we need to kill some containers to allocate this one\n    List\u003cRMContainer\u003e toKillContainers \u003d null;\n    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n      Resource availableAndKillable \u003d Resources.clone(available);\n      for (RMContainer killableContainer : node\n          .getKillableContainers().values()) {\n        if (null \u003d\u003d toKillContainers) {\n          toKillContainers \u003d new ArrayList\u003c\u003e();\n        }\n        toKillContainers.add(killableContainer);\n        Resources.addTo(availableAndKillable,\n                        killableContainer.getAllocatedResource());\n        if (Resources.fitsIn(rc, capability, availableAndKillable)) {\n          // Stop if we find enough spaces\n          availableContainers \u003d 1;\n          break;\n        }\n      }\n      availableForDC \u003d availableAndKillable;\n    }\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d application.findNodeToUnreserve(node,\n                  schedulerKey, resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, schedulerKey,\n                ActivityDiagnosticConstant.\n                    NODE_CAN_NOT_FIND_CONTAINER_TO_BE_UNRESERVED_WHEN_NEEDED,\n                ActivityLevel.NODE);\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n          pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      result.setToKillContainers(toKillContainers);\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            LOG.debug(\"we needed to unreserve to be able to allocate\");\n\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, schedulerKey,\n                ActivityDiagnosticConstant.NODE_DO_NOT_HAVE_SUFFICIENT_RESOURCE\n                    + getResourceDiagnostics(capability, availableForDC),\n                ActivityLevel.NODE);\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ActivitiesLogger.APP.recordAppActivityWithoutAllocation(\n            activitiesManager, node, application, schedulerKey,\n            ActivityDiagnosticConstant.NODE_DO_NOT_HAVE_SUFFICIENT_RESOURCE\n                + getResourceDiagnostics(capability, availableForDC),\n            rmContainer \u003d\u003d null ?\n                ActivityState.RESERVED : ActivityState.RE_RESERVED,\n            ActivityLevel.NODE);\n        ContainerAllocation result \u003d new ContainerAllocation(null,\n            pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        result.setToKillContainers(null);\n        return result;\n      }\n      // Skip the locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, schedulerKey,\n          ActivityDiagnosticConstant.NODE_DO_NOT_HAVE_SUFFICIENT_RESOURCE\n              + getResourceDiagnostics(capability, availableForDC),\n          ActivityLevel.NODE);\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "09763925025a3709e6098186348e1afd80cb9f71": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9590. Correct incompatible, incomplete and redundant activities. Contributed by Tao Yang.\n",
      "commitDate": "06/06/19 6:59 AM",
      "commitName": "09763925025a3709e6098186348e1afd80cb9f71",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "06/05/19 5:00 AM",
      "commitNameOld": "12b7059ddc8d8f67dd7131565f03a0e09cb92ca7",
      "commitAuthorOld": "Weiwei Yang",
      "daysBetweenCommits": 31.08,
      "commitsBetweenForRepo": 197,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,149 +1,155 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n       SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" application\u003d\" + application.getApplicationId()\n           + \" priority\u003d\" + schedulerKey.getPriority()\n           + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n     }\n \n     Resource capability \u003d pendingAsk.getPerAllocationResource();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(rc, capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for ask : \" + pendingAsk\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n           activitiesManager, node, application, schedulerKey,\n           ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n               + getResourceDiagnostics(capability, totalResource));\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         schedulerKey, capability);\n \n     // Can we allocate a container on this node?\n     long availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n     // available resource for diagnostics collector\n     Resource availableForDC \u003d available;\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         rc.isAnyMajorResourceAboveZero(resourceNeedToUnReserve);\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     // Check if we need to kill some containers to allocate this one\n     List\u003cRMContainer\u003e toKillContainers \u003d null;\n     if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n       Resource availableAndKillable \u003d Resources.clone(available);\n       for (RMContainer killableContainer : node\n           .getKillableContainers().values()) {\n         if (null \u003d\u003d toKillContainers) {\n           toKillContainers \u003d new ArrayList\u003c\u003e();\n         }\n         toKillContainers.add(killableContainer);\n         Resources.addTo(availableAndKillable,\n                         killableContainer.getAllocatedResource());\n         if (Resources.fitsIn(rc, capability, availableAndKillable)) {\n           // Stop if we find enough spaces\n           availableContainers \u003d 1;\n           break;\n         }\n       }\n       availableForDC \u003d availableAndKillable;\n     }\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d application.findNodeToUnreserve(node,\n                   schedulerKey, resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                 activitiesManager, node, application, schedulerKey,\n                 ActivityDiagnosticConstant.\n                     NODE_CAN_NOT_FIND_CONTAINER_TO_BE_UNRESERVED_WHEN_NEEDED);\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n           pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             LOG.debug(\"we needed to unreserve to be able to allocate\");\n \n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                 activitiesManager, node, application, schedulerKey,\n                 ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n                     + getResourceDiagnostics(capability, availableForDC));\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n+        ActivitiesLogger.APP.recordAppActivityWithoutAllocation(\n+            activitiesManager, node, application, schedulerKey,\n+            ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n+                + getResourceDiagnostics(capability, availableForDC),\n+            rmContainer \u003d\u003d null ?\n+                ActivityState.RESERVED : ActivityState.RE_RESERVED);\n         ContainerAllocation result \u003d new ContainerAllocation(null,\n             pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n           activitiesManager, node, application, schedulerKey,\n           ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n               + getResourceDiagnostics(capability, availableForDC));\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n      SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" application\u003d\" + application.getApplicationId()\n          + \" priority\u003d\" + schedulerKey.getPriority()\n          + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n    }\n\n    Resource capability \u003d pendingAsk.getPerAllocationResource();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(rc, capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for ask : \" + pendingAsk\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, schedulerKey,\n          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n              + getResourceDiagnostics(capability, totalResource));\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        schedulerKey, capability);\n\n    // Can we allocate a container on this node?\n    long availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n    // available resource for diagnostics collector\n    Resource availableForDC \u003d available;\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        rc.isAnyMajorResourceAboveZero(resourceNeedToUnReserve);\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    // Check if we need to kill some containers to allocate this one\n    List\u003cRMContainer\u003e toKillContainers \u003d null;\n    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n      Resource availableAndKillable \u003d Resources.clone(available);\n      for (RMContainer killableContainer : node\n          .getKillableContainers().values()) {\n        if (null \u003d\u003d toKillContainers) {\n          toKillContainers \u003d new ArrayList\u003c\u003e();\n        }\n        toKillContainers.add(killableContainer);\n        Resources.addTo(availableAndKillable,\n                        killableContainer.getAllocatedResource());\n        if (Resources.fitsIn(rc, capability, availableAndKillable)) {\n          // Stop if we find enough spaces\n          availableContainers \u003d 1;\n          break;\n        }\n      }\n      availableForDC \u003d availableAndKillable;\n    }\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d application.findNodeToUnreserve(node,\n                  schedulerKey, resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, schedulerKey,\n                ActivityDiagnosticConstant.\n                    NODE_CAN_NOT_FIND_CONTAINER_TO_BE_UNRESERVED_WHEN_NEEDED);\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n          pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      result.setToKillContainers(toKillContainers);\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            LOG.debug(\"we needed to unreserve to be able to allocate\");\n\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, schedulerKey,\n                ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n                    + getResourceDiagnostics(capability, availableForDC));\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ActivitiesLogger.APP.recordAppActivityWithoutAllocation(\n            activitiesManager, node, application, schedulerKey,\n            ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n                + getResourceDiagnostics(capability, availableForDC),\n            rmContainer \u003d\u003d null ?\n                ActivityState.RESERVED : ActivityState.RE_RESERVED);\n        ContainerAllocation result \u003d new ContainerAllocation(null,\n            pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        result.setToKillContainers(null);\n        return result;\n      }\n      // Skip the locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, schedulerKey,\n          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n              + getResourceDiagnostics(capability, availableForDC));\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "12b7059ddc8d8f67dd7131565f03a0e09cb92ca7": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9440. Improve diagnostics for scheduler and app activities. Contributed by Tao Yang.\n",
      "commitDate": "06/05/19 5:00 AM",
      "commitName": "12b7059ddc8d8f67dd7131565f03a0e09cb92ca7",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "15/03/19 4:20 PM",
      "commitNameOld": "2064ca015d1584263aac0cc20c60b925a3aff612",
      "commitAuthorOld": "Eric Yang",
      "daysBetweenCommits": 51.53,
      "commitsBetweenForRepo": 314,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,144 +1,149 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n       SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n-    Priority priority \u003d schedulerKey.getPriority();\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" application\u003d\" + application.getApplicationId()\n           + \" priority\u003d\" + schedulerKey.getPriority()\n           + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n     }\n \n     Resource capability \u003d pendingAsk.getPerAllocationResource();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n-    if (!Resources.lessThanOrEqual(rc, clusterResource,\n-        capability, totalResource)) {\n+    if (!Resources.fitsIn(rc, capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for ask : \" + pendingAsk\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n-          activitiesManager, node, application, priority,\n-          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE);\n+          activitiesManager, node, application, schedulerKey,\n+          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n+              + getResourceDiagnostics(capability, totalResource));\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         schedulerKey, capability);\n \n     // Can we allocate a container on this node?\n     long availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n+    // available resource for diagnostics collector\n+    Resource availableForDC \u003d available;\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         rc.isAnyMajorResourceAboveZero(resourceNeedToUnReserve);\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     // Check if we need to kill some containers to allocate this one\n     List\u003cRMContainer\u003e toKillContainers \u003d null;\n     if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n       Resource availableAndKillable \u003d Resources.clone(available);\n       for (RMContainer killableContainer : node\n           .getKillableContainers().values()) {\n         if (null \u003d\u003d toKillContainers) {\n           toKillContainers \u003d new ArrayList\u003c\u003e();\n         }\n         toKillContainers.add(killableContainer);\n         Resources.addTo(availableAndKillable,\n                         killableContainer.getAllocatedResource());\n         if (Resources.fitsIn(rc, capability, availableAndKillable)) {\n           // Stop if we find enough spaces\n           availableContainers \u003d 1;\n           break;\n         }\n       }\n+      availableForDC \u003d availableAndKillable;\n     }\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d application.findNodeToUnreserve(node,\n                   schedulerKey, resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n-                activitiesManager, node, application, priority,\n-                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n+                activitiesManager, node, application, schedulerKey,\n+                ActivityDiagnosticConstant.\n+                    NODE_CAN_NOT_FIND_CONTAINER_TO_BE_UNRESERVED_WHEN_NEEDED);\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n           pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             LOG.debug(\"we needed to unreserve to be able to allocate\");\n \n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n-                activitiesManager, node, application, priority,\n-                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n+                activitiesManager, node, application, schedulerKey,\n+                ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n+                    + getResourceDiagnostics(capability, availableForDC));\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ContainerAllocation result \u003d new ContainerAllocation(null,\n             pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n-          activitiesManager, node, application, priority,\n-          ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n+          activitiesManager, node, application, schedulerKey,\n+          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n+              + getResourceDiagnostics(capability, availableForDC));\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n      SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" application\u003d\" + application.getApplicationId()\n          + \" priority\u003d\" + schedulerKey.getPriority()\n          + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n    }\n\n    Resource capability \u003d pendingAsk.getPerAllocationResource();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(rc, capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for ask : \" + pendingAsk\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, schedulerKey,\n          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n              + getResourceDiagnostics(capability, totalResource));\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        schedulerKey, capability);\n\n    // Can we allocate a container on this node?\n    long availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n    // available resource for diagnostics collector\n    Resource availableForDC \u003d available;\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        rc.isAnyMajorResourceAboveZero(resourceNeedToUnReserve);\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    // Check if we need to kill some containers to allocate this one\n    List\u003cRMContainer\u003e toKillContainers \u003d null;\n    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n      Resource availableAndKillable \u003d Resources.clone(available);\n      for (RMContainer killableContainer : node\n          .getKillableContainers().values()) {\n        if (null \u003d\u003d toKillContainers) {\n          toKillContainers \u003d new ArrayList\u003c\u003e();\n        }\n        toKillContainers.add(killableContainer);\n        Resources.addTo(availableAndKillable,\n                        killableContainer.getAllocatedResource());\n        if (Resources.fitsIn(rc, capability, availableAndKillable)) {\n          // Stop if we find enough spaces\n          availableContainers \u003d 1;\n          break;\n        }\n      }\n      availableForDC \u003d availableAndKillable;\n    }\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d application.findNodeToUnreserve(node,\n                  schedulerKey, resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, schedulerKey,\n                ActivityDiagnosticConstant.\n                    NODE_CAN_NOT_FIND_CONTAINER_TO_BE_UNRESERVED_WHEN_NEEDED);\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n          pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      result.setToKillContainers(toKillContainers);\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            LOG.debug(\"we needed to unreserve to be able to allocate\");\n\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, schedulerKey,\n                ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n                    + getResourceDiagnostics(capability, availableForDC));\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d new ContainerAllocation(null,\n            pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        result.setToKillContainers(null);\n        return result;\n      }\n      // Skip the locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, schedulerKey,\n          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE\n              + getResourceDiagnostics(capability, availableForDC));\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "2064ca015d1584263aac0cc20c60b925a3aff612": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9349.  Changed logging to use slf4j api.\n            Contributed by Prabhu Joseph\n",
      "commitDate": "15/03/19 4:20 PM",
      "commitName": "2064ca015d1584263aac0cc20c60b925a3aff612",
      "commitAuthor": "Eric Yang",
      "commitDateOld": "04/03/19 9:10 PM",
      "commitNameOld": "e40e2d6ad5cbe782c3a067229270738b501ed27e",
      "commitAuthorOld": "Prabhu Joseph",
      "daysBetweenCommits": 10.76,
      "commitsBetweenForRepo": 108,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,145 +1,144 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n       SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n     Priority priority \u003d schedulerKey.getPriority();\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" application\u003d\" + application.getApplicationId()\n           + \" priority\u003d\" + schedulerKey.getPriority()\n           + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n     }\n \n     Resource capability \u003d pendingAsk.getPerAllocationResource();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for ask : \" + pendingAsk\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n           activitiesManager, node, application, priority,\n           ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE);\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         schedulerKey, capability);\n \n     // Can we allocate a container on this node?\n     long availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         rc.isAnyMajorResourceAboveZero(resourceNeedToUnReserve);\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     // Check if we need to kill some containers to allocate this one\n     List\u003cRMContainer\u003e toKillContainers \u003d null;\n     if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n       Resource availableAndKillable \u003d Resources.clone(available);\n       for (RMContainer killableContainer : node\n           .getKillableContainers().values()) {\n         if (null \u003d\u003d toKillContainers) {\n           toKillContainers \u003d new ArrayList\u003c\u003e();\n         }\n         toKillContainers.add(killableContainer);\n         Resources.addTo(availableAndKillable,\n                         killableContainer.getAllocatedResource());\n         if (Resources.fitsIn(rc, capability, availableAndKillable)) {\n           // Stop if we find enough spaces\n           availableContainers \u003d 1;\n           break;\n         }\n       }\n     }\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d application.findNodeToUnreserve(node,\n                   schedulerKey, resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                 activitiesManager, node, application, priority,\n                 ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n           pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"we needed to unreserve to be able to allocate\");\n-            }\n+            LOG.debug(\"we needed to unreserve to be able to allocate\");\n+\n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                 activitiesManager, node, application, priority,\n                 ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ContainerAllocation result \u003d new ContainerAllocation(null,\n             pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n           activitiesManager, node, application, priority,\n           ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n      SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n    Priority priority \u003d schedulerKey.getPriority();\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" application\u003d\" + application.getApplicationId()\n          + \" priority\u003d\" + schedulerKey.getPriority()\n          + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n    }\n\n    Resource capability \u003d pendingAsk.getPerAllocationResource();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for ask : \" + pendingAsk\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, priority,\n          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE);\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        schedulerKey, capability);\n\n    // Can we allocate a container on this node?\n    long availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        rc.isAnyMajorResourceAboveZero(resourceNeedToUnReserve);\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    // Check if we need to kill some containers to allocate this one\n    List\u003cRMContainer\u003e toKillContainers \u003d null;\n    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n      Resource availableAndKillable \u003d Resources.clone(available);\n      for (RMContainer killableContainer : node\n          .getKillableContainers().values()) {\n        if (null \u003d\u003d toKillContainers) {\n          toKillContainers \u003d new ArrayList\u003c\u003e();\n        }\n        toKillContainers.add(killableContainer);\n        Resources.addTo(availableAndKillable,\n                        killableContainer.getAllocatedResource());\n        if (Resources.fitsIn(rc, capability, availableAndKillable)) {\n          // Stop if we find enough spaces\n          availableContainers \u003d 1;\n          break;\n        }\n      }\n    }\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d application.findNodeToUnreserve(node,\n                  schedulerKey, resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, priority,\n                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n          pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      result.setToKillContainers(toKillContainers);\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            LOG.debug(\"we needed to unreserve to be able to allocate\");\n\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, priority,\n                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d new ContainerAllocation(null,\n            pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        result.setToKillContainers(null);\n        return result;\n      }\n      // Skip the locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, priority,\n          ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "0712537e799bc03855d548d1f4bd690dd478b871": {
      "type": "Ybodychange",
      "commitMessage": "YARN-8771. CapacityScheduler fails to unreserve when cluster resource contains empty resource type. Contributed by Tao Yang.\n",
      "commitDate": "19/09/18 4:31 AM",
      "commitName": "0712537e799bc03855d548d1f4bd690dd478b871",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "21/08/18 7:42 AM",
      "commitNameOld": "9c3fc3ef2865164aa5f121793ac914cfeb21a181",
      "commitAuthorOld": "Weiwei Yang",
      "daysBetweenCommits": 28.87,
      "commitsBetweenForRepo": 251,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,146 +1,145 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n       SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n     Priority priority \u003d schedulerKey.getPriority();\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" application\u003d\" + application.getApplicationId()\n           + \" priority\u003d\" + schedulerKey.getPriority()\n           + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n     }\n \n     Resource capability \u003d pendingAsk.getPerAllocationResource();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for ask : \" + pendingAsk\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n           activitiesManager, node, application, priority,\n           ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE);\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         schedulerKey, capability);\n \n     // Can we allocate a container on this node?\n     long availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n-        Resources.greaterThan(rc, clusterResource,\n-            resourceNeedToUnReserve, Resources.none());\n+        rc.isAnyMajorResourceAboveZero(resourceNeedToUnReserve);\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     // Check if we need to kill some containers to allocate this one\n     List\u003cRMContainer\u003e toKillContainers \u003d null;\n     if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n       Resource availableAndKillable \u003d Resources.clone(available);\n       for (RMContainer killableContainer : node\n           .getKillableContainers().values()) {\n         if (null \u003d\u003d toKillContainers) {\n           toKillContainers \u003d new ArrayList\u003c\u003e();\n         }\n         toKillContainers.add(killableContainer);\n         Resources.addTo(availableAndKillable,\n                         killableContainer.getAllocatedResource());\n         if (Resources.fitsIn(rc, capability, availableAndKillable)) {\n           // Stop if we find enough spaces\n           availableContainers \u003d 1;\n           break;\n         }\n       }\n     }\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d application.findNodeToUnreserve(node,\n                   schedulerKey, resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                 activitiesManager, node, application, priority,\n                 ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n           pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                 activitiesManager, node, application, priority,\n                 ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ContainerAllocation result \u003d new ContainerAllocation(null,\n             pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n           activitiesManager, node, application, priority,\n           ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n      SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n    Priority priority \u003d schedulerKey.getPriority();\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" application\u003d\" + application.getApplicationId()\n          + \" priority\u003d\" + schedulerKey.getPriority()\n          + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n    }\n\n    Resource capability \u003d pendingAsk.getPerAllocationResource();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for ask : \" + pendingAsk\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, priority,\n          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE);\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        schedulerKey, capability);\n\n    // Can we allocate a container on this node?\n    long availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        rc.isAnyMajorResourceAboveZero(resourceNeedToUnReserve);\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    // Check if we need to kill some containers to allocate this one\n    List\u003cRMContainer\u003e toKillContainers \u003d null;\n    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n      Resource availableAndKillable \u003d Resources.clone(available);\n      for (RMContainer killableContainer : node\n          .getKillableContainers().values()) {\n        if (null \u003d\u003d toKillContainers) {\n          toKillContainers \u003d new ArrayList\u003c\u003e();\n        }\n        toKillContainers.add(killableContainer);\n        Resources.addTo(availableAndKillable,\n                        killableContainer.getAllocatedResource());\n        if (Resources.fitsIn(rc, capability, availableAndKillable)) {\n          // Stop if we find enough spaces\n          availableContainers \u003d 1;\n          break;\n        }\n      }\n    }\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d application.findNodeToUnreserve(node,\n                  schedulerKey, resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, priority,\n                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n          pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      result.setToKillContainers(toKillContainers);\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, priority,\n                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d new ContainerAllocation(null,\n            pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        result.setToKillContainers(null);\n        return result;\n      }\n      // Skip the locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, priority,\n          ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "e81596d06d226f1cfa44b2390ce3095ed4dee621": {
      "type": "Ybodychange",
      "commitMessage": "YARN-7172. ResourceCalculator.fitsIn() should not take a cluster resource parameter. (Sen Zhao via wangda)\n\nChange-Id: Icc3670c9381ce7591ca69ec12da5aa52d3612d34\n",
      "commitDate": "17/09/17 9:20 PM",
      "commitName": "e81596d06d226f1cfa44b2390ce3095ed4dee621",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "10/04/17 3:34 PM",
      "commitNameOld": "7999318af12a75b35815461c601d4c25750e8340",
      "commitAuthorOld": "Konstantinos Karanasos",
      "daysBetweenCommits": 160.24,
      "commitsBetweenForRepo": 1042,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,150 +1,146 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n       SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n     Priority priority \u003d schedulerKey.getPriority();\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" application\u003d\" + application.getApplicationId()\n           + \" priority\u003d\" + schedulerKey.getPriority()\n           + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n     }\n \n     Resource capability \u003d pendingAsk.getPerAllocationResource();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for ask : \" + pendingAsk\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n           activitiesManager, node, application, priority,\n           ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE);\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         schedulerKey, capability);\n \n     // Can we allocate a container on this node?\n     long availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     // Check if we need to kill some containers to allocate this one\n     List\u003cRMContainer\u003e toKillContainers \u003d null;\n     if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n       Resource availableAndKillable \u003d Resources.clone(available);\n       for (RMContainer killableContainer : node\n           .getKillableContainers().values()) {\n         if (null \u003d\u003d toKillContainers) {\n           toKillContainers \u003d new ArrayList\u003c\u003e();\n         }\n         toKillContainers.add(killableContainer);\n         Resources.addTo(availableAndKillable,\n                         killableContainer.getAllocatedResource());\n-        if (Resources.fitsIn(rc,\n-                             clusterResource,\n-                             capability,\n-                             availableAndKillable)) {\n+        if (Resources.fitsIn(rc, capability, availableAndKillable)) {\n           // Stop if we find enough spaces\n           availableContainers \u003d 1;\n           break;\n         }\n       }\n     }\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n-          unreservedContainer \u003d\n-              application.findNodeToUnreserve(clusterResource, node,\n+          unreservedContainer \u003d application.findNodeToUnreserve(node,\n                   schedulerKey, resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                 activitiesManager, node, application, priority,\n                 ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n           pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                 activitiesManager, node, application, priority,\n                 ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ContainerAllocation result \u003d new ContainerAllocation(null,\n             pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n           activitiesManager, node, application, priority,\n           ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n      SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n    Priority priority \u003d schedulerKey.getPriority();\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" application\u003d\" + application.getApplicationId()\n          + \" priority\u003d\" + schedulerKey.getPriority()\n          + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n    }\n\n    Resource capability \u003d pendingAsk.getPerAllocationResource();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for ask : \" + pendingAsk\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, priority,\n          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE);\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        schedulerKey, capability);\n\n    // Can we allocate a container on this node?\n    long availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    // Check if we need to kill some containers to allocate this one\n    List\u003cRMContainer\u003e toKillContainers \u003d null;\n    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n      Resource availableAndKillable \u003d Resources.clone(available);\n      for (RMContainer killableContainer : node\n          .getKillableContainers().values()) {\n        if (null \u003d\u003d toKillContainers) {\n          toKillContainers \u003d new ArrayList\u003c\u003e();\n        }\n        toKillContainers.add(killableContainer);\n        Resources.addTo(availableAndKillable,\n                        killableContainer.getAllocatedResource());\n        if (Resources.fitsIn(rc, capability, availableAndKillable)) {\n          // Stop if we find enough spaces\n          availableContainers \u003d 1;\n          break;\n        }\n      }\n    }\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d application.findNodeToUnreserve(node,\n                  schedulerKey, resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, priority,\n                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n          pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      result.setToKillContainers(toKillContainers);\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, priority,\n                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d new ContainerAllocation(null,\n            pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        result.setToKillContainers(null);\n        return result;\n      }\n      // Skip the locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, priority,\n          ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "2977bc6a141041ef7579efc416e93fc55e0c2a1a": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-6040. Introduce api independent PendingAsk to replace usage of ResourceRequest within Scheduler classes. (Wangda Tan via asuresh)\n",
      "commitDate": "06/01/17 9:59 AM",
      "commitName": "2977bc6a141041ef7579efc416e93fc55e0c2a1a",
      "commitAuthor": "Arun Suresh",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-6040. Introduce api independent PendingAsk to replace usage of ResourceRequest within Scheduler classes. (Wangda Tan via asuresh)\n",
          "commitDate": "06/01/17 9:59 AM",
          "commitName": "2977bc6a141041ef7579efc416e93fc55e0c2a1a",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "05/01/17 10:31 AM",
          "commitNameOld": "0a55bd841ec0f2eb89a0383f4c589526e8b138d4",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 0.98,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,168 +1,150 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n-      ResourceRequest request, NodeType type, RMContainer rmContainer,\n+      PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n       SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n     Priority priority \u003d schedulerKey.getPriority();\n-    lastResourceRequest \u003d request;\n-    \n+\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" application\u003d\" + application.getApplicationId()\n           + \" priority\u003d\" + schedulerKey.getPriority()\n-          + \" request\u003d\" + request + \" type\u003d\" + type);\n+          + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n     }\n \n-    // check if the resource request can access the label\n-    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n-        request.getNodeLabelExpression(), node.getPartition(),\n-        schedulingMode)) {\n-      // this is a reserved container, but we cannot allocate it now according\n-      // to label not match. This can be caused by node label changed\n-      // We should un-reserve this container.\n-      ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n-          node, application, priority,\n-          ActivityDiagnosticConstant.REQUEST_CAN_NOT_ACCESS_NODE_LABEL,\n-          ActivityState.REJECTED);\n-      return new ContainerAllocation(rmContainer, null,\n-          AllocationState.LOCALITY_SKIPPED);\n-    }\n-\n-    Resource capability \u003d request.getCapability();\n+    Resource capability \u003d pendingAsk.getPerAllocationResource();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n-          + \" does not have sufficient resource for request : \" + request\n+          + \" does not have sufficient resource for ask : \" + pendingAsk\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n           activitiesManager, node, application, priority,\n           ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE);\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         schedulerKey, capability);\n \n     // Can we allocate a container on this node?\n     long availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     // Check if we need to kill some containers to allocate this one\n     List\u003cRMContainer\u003e toKillContainers \u003d null;\n     if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n       Resource availableAndKillable \u003d Resources.clone(available);\n       for (RMContainer killableContainer : node\n           .getKillableContainers().values()) {\n         if (null \u003d\u003d toKillContainers) {\n           toKillContainers \u003d new ArrayList\u003c\u003e();\n         }\n         toKillContainers.add(killableContainer);\n         Resources.addTo(availableAndKillable,\n                         killableContainer.getAllocatedResource());\n         if (Resources.fitsIn(rc,\n                              clusterResource,\n                              capability,\n                              availableAndKillable)) {\n           // Stop if we find enough spaces\n           availableContainers \u003d 1;\n           break;\n         }\n       }\n     }\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n               application.findNodeToUnreserve(clusterResource, node,\n                   schedulerKey, resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                 activitiesManager, node, application, priority,\n                 ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n-      ContainerAllocation result \u003d\n-          new ContainerAllocation(unreservedContainer, request.getCapability(),\n-              AllocationState.ALLOCATED);\n+      ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n+          pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                 activitiesManager, node, application, priority,\n                 ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n-        ContainerAllocation result \u003d\n-            new ContainerAllocation(null, request.getCapability(),\n-                AllocationState.RESERVED);\n+        ContainerAllocation result \u003d new ContainerAllocation(null,\n+            pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n           activitiesManager, node, application, priority,\n           ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n      SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n    Priority priority \u003d schedulerKey.getPriority();\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" application\u003d\" + application.getApplicationId()\n          + \" priority\u003d\" + schedulerKey.getPriority()\n          + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n    }\n\n    Resource capability \u003d pendingAsk.getPerAllocationResource();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for ask : \" + pendingAsk\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, priority,\n          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE);\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        schedulerKey, capability);\n\n    // Can we allocate a container on this node?\n    long availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    // Check if we need to kill some containers to allocate this one\n    List\u003cRMContainer\u003e toKillContainers \u003d null;\n    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n      Resource availableAndKillable \u003d Resources.clone(available);\n      for (RMContainer killableContainer : node\n          .getKillableContainers().values()) {\n        if (null \u003d\u003d toKillContainers) {\n          toKillContainers \u003d new ArrayList\u003c\u003e();\n        }\n        toKillContainers.add(killableContainer);\n        Resources.addTo(availableAndKillable,\n                        killableContainer.getAllocatedResource());\n        if (Resources.fitsIn(rc,\n                             clusterResource,\n                             capability,\n                             availableAndKillable)) {\n          // Stop if we find enough spaces\n          availableContainers \u003d 1;\n          break;\n        }\n      }\n    }\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node,\n                  schedulerKey, resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, priority,\n                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n          pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      result.setToKillContainers(toKillContainers);\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, priority,\n                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d new ContainerAllocation(null,\n            pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        result.setToKillContainers(null);\n        return result;\n      }\n      // Skip the locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, priority,\n          ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, schedulerKey-SchedulerRequestKey, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, schedulingMode-SchedulingMode, currentResoureLimits-ResourceLimits]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, schedulerKey-SchedulerRequestKey, pendingAsk-PendingAsk, type-NodeType, rmContainer-RMContainer, schedulingMode-SchedulingMode, currentResoureLimits-ResourceLimits]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-6040. Introduce api independent PendingAsk to replace usage of ResourceRequest within Scheduler classes. (Wangda Tan via asuresh)\n",
          "commitDate": "06/01/17 9:59 AM",
          "commitName": "2977bc6a141041ef7579efc416e93fc55e0c2a1a",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "05/01/17 10:31 AM",
          "commitNameOld": "0a55bd841ec0f2eb89a0383f4c589526e8b138d4",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 0.98,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,168 +1,150 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n-      ResourceRequest request, NodeType type, RMContainer rmContainer,\n+      PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n       SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n     Priority priority \u003d schedulerKey.getPriority();\n-    lastResourceRequest \u003d request;\n-    \n+\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" application\u003d\" + application.getApplicationId()\n           + \" priority\u003d\" + schedulerKey.getPriority()\n-          + \" request\u003d\" + request + \" type\u003d\" + type);\n+          + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n     }\n \n-    // check if the resource request can access the label\n-    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n-        request.getNodeLabelExpression(), node.getPartition(),\n-        schedulingMode)) {\n-      // this is a reserved container, but we cannot allocate it now according\n-      // to label not match. This can be caused by node label changed\n-      // We should un-reserve this container.\n-      ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n-          node, application, priority,\n-          ActivityDiagnosticConstant.REQUEST_CAN_NOT_ACCESS_NODE_LABEL,\n-          ActivityState.REJECTED);\n-      return new ContainerAllocation(rmContainer, null,\n-          AllocationState.LOCALITY_SKIPPED);\n-    }\n-\n-    Resource capability \u003d request.getCapability();\n+    Resource capability \u003d pendingAsk.getPerAllocationResource();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n-          + \" does not have sufficient resource for request : \" + request\n+          + \" does not have sufficient resource for ask : \" + pendingAsk\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n           activitiesManager, node, application, priority,\n           ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE);\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         schedulerKey, capability);\n \n     // Can we allocate a container on this node?\n     long availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     // Check if we need to kill some containers to allocate this one\n     List\u003cRMContainer\u003e toKillContainers \u003d null;\n     if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n       Resource availableAndKillable \u003d Resources.clone(available);\n       for (RMContainer killableContainer : node\n           .getKillableContainers().values()) {\n         if (null \u003d\u003d toKillContainers) {\n           toKillContainers \u003d new ArrayList\u003c\u003e();\n         }\n         toKillContainers.add(killableContainer);\n         Resources.addTo(availableAndKillable,\n                         killableContainer.getAllocatedResource());\n         if (Resources.fitsIn(rc,\n                              clusterResource,\n                              capability,\n                              availableAndKillable)) {\n           // Stop if we find enough spaces\n           availableContainers \u003d 1;\n           break;\n         }\n       }\n     }\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n               application.findNodeToUnreserve(clusterResource, node,\n                   schedulerKey, resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                 activitiesManager, node, application, priority,\n                 ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n-      ContainerAllocation result \u003d\n-          new ContainerAllocation(unreservedContainer, request.getCapability(),\n-              AllocationState.ALLOCATED);\n+      ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n+          pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             // Skip the locality request\n             ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                 activitiesManager, node, application, priority,\n                 ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n-        ContainerAllocation result \u003d\n-            new ContainerAllocation(null, request.getCapability(),\n-                AllocationState.RESERVED);\n+        ContainerAllocation result \u003d new ContainerAllocation(null,\n+            pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n       ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n           activitiesManager, node, application, priority,\n           ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      PendingAsk pendingAsk, NodeType type, RMContainer rmContainer,\n      SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n    Priority priority \u003d schedulerKey.getPriority();\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" application\u003d\" + application.getApplicationId()\n          + \" priority\u003d\" + schedulerKey.getPriority()\n          + \" pendingAsk\u003d\" + pendingAsk + \" type\u003d\" + type);\n    }\n\n    Resource capability \u003d pendingAsk.getPerAllocationResource();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for ask : \" + pendingAsk\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, priority,\n          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE);\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        schedulerKey, capability);\n\n    // Can we allocate a container on this node?\n    long availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    // Check if we need to kill some containers to allocate this one\n    List\u003cRMContainer\u003e toKillContainers \u003d null;\n    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n      Resource availableAndKillable \u003d Resources.clone(available);\n      for (RMContainer killableContainer : node\n          .getKillableContainers().values()) {\n        if (null \u003d\u003d toKillContainers) {\n          toKillContainers \u003d new ArrayList\u003c\u003e();\n        }\n        toKillContainers.add(killableContainer);\n        Resources.addTo(availableAndKillable,\n                        killableContainer.getAllocatedResource());\n        if (Resources.fitsIn(rc,\n                             clusterResource,\n                             capability,\n                             availableAndKillable)) {\n          // Stop if we find enough spaces\n          availableContainers \u003d 1;\n          break;\n        }\n      }\n    }\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node,\n                  schedulerKey, resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, priority,\n                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d new ContainerAllocation(unreservedContainer,\n          pendingAsk.getPerAllocationResource(), AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      result.setToKillContainers(toKillContainers);\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, priority,\n                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d new ContainerAllocation(null,\n            pendingAsk.getPerAllocationResource(), AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        result.setToKillContainers(null);\n        return result;\n      }\n      // Skip the locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, priority,\n          ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
          "extendedDetails": {}
        }
      ]
    },
    "e0d131f055ee126052ad4d0f7b0d192e6c730188": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4091. Add REST API to retrieve scheduler activity. (Chen Ge and Sunil G via wangda)\n",
      "commitDate": "05/08/16 10:27 AM",
      "commitName": "e0d131f055ee126052ad4d0f7b0d192e6c730188",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "26/07/16 6:14 PM",
      "commitNameOld": "49969b16cdba0f251b9f8bf3d8df9906e38b5c61",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 9.68,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,151 +1,168 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n+    Priority priority \u003d schedulerKey.getPriority();\n     lastResourceRequest \u003d request;\n     \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n           + \" application\u003d\" + application.getApplicationId()\n           + \" priority\u003d\" + schedulerKey.getPriority()\n           + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n         request.getNodeLabelExpression(), node.getPartition(),\n         schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n+      ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n+          node, application, priority,\n+          ActivityDiagnosticConstant.REQUEST_CAN_NOT_ACCESS_NODE_LABEL,\n+          ActivityState.REJECTED);\n       return new ContainerAllocation(rmContainer, null,\n           AllocationState.LOCALITY_SKIPPED);\n     }\n \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n+      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n+          activitiesManager, node, application, priority,\n+          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE);\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         schedulerKey, capability);\n \n     // Can we allocate a container on this node?\n     long availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     // Check if we need to kill some containers to allocate this one\n     List\u003cRMContainer\u003e toKillContainers \u003d null;\n     if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n       Resource availableAndKillable \u003d Resources.clone(available);\n       for (RMContainer killableContainer : node\n           .getKillableContainers().values()) {\n         if (null \u003d\u003d toKillContainers) {\n           toKillContainers \u003d new ArrayList\u003c\u003e();\n         }\n         toKillContainers.add(killableContainer);\n         Resources.addTo(availableAndKillable,\n                         killableContainer.getAllocatedResource());\n         if (Resources.fitsIn(rc,\n                              clusterResource,\n                              capability,\n                              availableAndKillable)) {\n           // Stop if we find enough spaces\n           availableContainers \u003d 1;\n           break;\n         }\n       }\n     }\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n               application.findNodeToUnreserve(clusterResource, node,\n                   schedulerKey, resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n+            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n+                activitiesManager, node, application, priority,\n+                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d\n           new ContainerAllocation(unreservedContainer, request.getCapability(),\n               AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             // Skip the locality request\n+            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n+                activitiesManager, node, application, priority,\n+                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ContainerAllocation result \u003d\n             new ContainerAllocation(null, request.getCapability(),\n                 AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n+      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n+          activitiesManager, node, application, priority,\n+          ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n    Priority priority \u003d schedulerKey.getPriority();\n    lastResourceRequest \u003d request;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" application\u003d\" + application.getApplicationId()\n          + \" priority\u003d\" + schedulerKey.getPriority()\n          + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n        request.getNodeLabelExpression(), node.getPartition(),\n        schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,\n          node, application, priority,\n          ActivityDiagnosticConstant.REQUEST_CAN_NOT_ACCESS_NODE_LABEL,\n          ActivityState.REJECTED);\n      return new ContainerAllocation(rmContainer, null,\n          AllocationState.LOCALITY_SKIPPED);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, priority,\n          ActivityDiagnosticConstant.NOT_SUFFICIENT_RESOURCE);\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        schedulerKey, capability);\n\n    // Can we allocate a container on this node?\n    long availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    // Check if we need to kill some containers to allocate this one\n    List\u003cRMContainer\u003e toKillContainers \u003d null;\n    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n      Resource availableAndKillable \u003d Resources.clone(available);\n      for (RMContainer killableContainer : node\n          .getKillableContainers().values()) {\n        if (null \u003d\u003d toKillContainers) {\n          toKillContainers \u003d new ArrayList\u003c\u003e();\n        }\n        toKillContainers.add(killableContainer);\n        Resources.addTo(availableAndKillable,\n                        killableContainer.getAllocatedResource());\n        if (Resources.fitsIn(rc,\n                             clusterResource,\n                             capability,\n                             availableAndKillable)) {\n          // Stop if we find enough spaces\n          availableContainers \u003d 1;\n          break;\n        }\n      }\n    }\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node,\n                  schedulerKey, resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, priority,\n                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d\n          new ContainerAllocation(unreservedContainer, request.getCapability(),\n              AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      result.setToKillContainers(toKillContainers);\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            // Skip the locality request\n            ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n                activitiesManager, node, application, priority,\n                ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d\n            new ContainerAllocation(null, request.getCapability(),\n                AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        result.setToKillContainers(null);\n        return result;\n      }\n      // Skip the locality request\n      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(\n          activitiesManager, node, application, priority,\n          ActivityDiagnosticConstant.LOCALITY_SKIPPED);\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "5aace38b748ba71aaadd2c4d64eba8dc1f816828": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-5392. Replace use of Priority in the Scheduling infrastructure with an opaque ShedulerRequestKey. (asuresh and subru)\n",
      "commitDate": "26/07/16 2:54 PM",
      "commitName": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
      "commitAuthor": "Arun Suresh",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-5392. Replace use of Priority in the Scheduling infrastructure with an opaque ShedulerRequestKey. (asuresh and subru)\n",
          "commitDate": "26/07/16 2:54 PM",
          "commitName": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "07/06/16 3:06 PM",
          "commitNameOld": "620325e81696fca140195b74929ed9eda2d5eb16",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 48.99,
          "commitsBetweenForRepo": 441,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,150 +1,151 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n-      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n-      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n-      ResourceLimits currentResoureLimits) {\n+      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n+      ResourceRequest request, NodeType type, RMContainer rmContainer,\n+      SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n     lastResourceRequest \u003d request;\n     \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + application.getApplicationId()\n-        + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type);\n+          + \" application\u003d\" + application.getApplicationId()\n+          + \" priority\u003d\" + schedulerKey.getPriority()\n+          + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n-        request.getNodeLabelExpression(), node.getPartition(), schedulingMode)) {\n+        request.getNodeLabelExpression(), node.getPartition(),\n+        schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       return new ContainerAllocation(rmContainer, null,\n           AllocationState.LOCALITY_SKIPPED);\n     }\n \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n-        priority, capability);\n+        schedulerKey, capability);\n \n     // Can we allocate a container on this node?\n     long availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     // Check if we need to kill some containers to allocate this one\n     List\u003cRMContainer\u003e toKillContainers \u003d null;\n     if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n       Resource availableAndKillable \u003d Resources.clone(available);\n       for (RMContainer killableContainer : node\n           .getKillableContainers().values()) {\n         if (null \u003d\u003d toKillContainers) {\n           toKillContainers \u003d new ArrayList\u003c\u003e();\n         }\n         toKillContainers.add(killableContainer);\n         Resources.addTo(availableAndKillable,\n                         killableContainer.getAllocatedResource());\n         if (Resources.fitsIn(rc,\n                              clusterResource,\n                              capability,\n                              availableAndKillable)) {\n           // Stop if we find enough spaces\n           availableContainers \u003d 1;\n           break;\n         }\n       }\n     }\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n-              application.findNodeToUnreserve(clusterResource, node, priority,\n-                  resourceNeedToUnReserve);\n+              application.findNodeToUnreserve(clusterResource, node,\n+                  schedulerKey, resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d\n           new ContainerAllocation(unreservedContainer, request.getCapability(),\n               AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ContainerAllocation result \u003d\n             new ContainerAllocation(null, request.getCapability(),\n                 AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n    lastResourceRequest \u003d request;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" application\u003d\" + application.getApplicationId()\n          + \" priority\u003d\" + schedulerKey.getPriority()\n          + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n        request.getNodeLabelExpression(), node.getPartition(),\n        schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      return new ContainerAllocation(rmContainer, null,\n          AllocationState.LOCALITY_SKIPPED);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        schedulerKey, capability);\n\n    // Can we allocate a container on this node?\n    long availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    // Check if we need to kill some containers to allocate this one\n    List\u003cRMContainer\u003e toKillContainers \u003d null;\n    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n      Resource availableAndKillable \u003d Resources.clone(available);\n      for (RMContainer killableContainer : node\n          .getKillableContainers().values()) {\n        if (null \u003d\u003d toKillContainers) {\n          toKillContainers \u003d new ArrayList\u003c\u003e();\n        }\n        toKillContainers.add(killableContainer);\n        Resources.addTo(availableAndKillable,\n                        killableContainer.getAllocatedResource());\n        if (Resources.fitsIn(rc,\n                             clusterResource,\n                             capability,\n                             availableAndKillable)) {\n          // Stop if we find enough spaces\n          availableContainers \u003d 1;\n          break;\n        }\n      }\n    }\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node,\n                  schedulerKey, resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d\n          new ContainerAllocation(unreservedContainer, request.getCapability(),\n              AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      result.setToKillContainers(toKillContainers);\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d\n            new ContainerAllocation(null, request.getCapability(),\n                AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        result.setToKillContainers(null);\n        return result;\n      }\n      // Skip the locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, schedulingMode-SchedulingMode, currentResoureLimits-ResourceLimits]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, schedulerKey-SchedulerRequestKey, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, schedulingMode-SchedulingMode, currentResoureLimits-ResourceLimits]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-5392. Replace use of Priority in the Scheduling infrastructure with an opaque ShedulerRequestKey. (asuresh and subru)\n",
          "commitDate": "26/07/16 2:54 PM",
          "commitName": "5aace38b748ba71aaadd2c4d64eba8dc1f816828",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "07/06/16 3:06 PM",
          "commitNameOld": "620325e81696fca140195b74929ed9eda2d5eb16",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 48.99,
          "commitsBetweenForRepo": 441,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,150 +1,151 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n-      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n-      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n-      ResourceLimits currentResoureLimits) {\n+      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n+      ResourceRequest request, NodeType type, RMContainer rmContainer,\n+      SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n     lastResourceRequest \u003d request;\n     \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + application.getApplicationId()\n-        + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type);\n+          + \" application\u003d\" + application.getApplicationId()\n+          + \" priority\u003d\" + schedulerKey.getPriority()\n+          + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n-        request.getNodeLabelExpression(), node.getPartition(), schedulingMode)) {\n+        request.getNodeLabelExpression(), node.getPartition(),\n+        schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       return new ContainerAllocation(rmContainer, null,\n           AllocationState.LOCALITY_SKIPPED);\n     }\n \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n-        priority, capability);\n+        schedulerKey, capability);\n \n     // Can we allocate a container on this node?\n     long availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     // Check if we need to kill some containers to allocate this one\n     List\u003cRMContainer\u003e toKillContainers \u003d null;\n     if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n       Resource availableAndKillable \u003d Resources.clone(available);\n       for (RMContainer killableContainer : node\n           .getKillableContainers().values()) {\n         if (null \u003d\u003d toKillContainers) {\n           toKillContainers \u003d new ArrayList\u003c\u003e();\n         }\n         toKillContainers.add(killableContainer);\n         Resources.addTo(availableAndKillable,\n                         killableContainer.getAllocatedResource());\n         if (Resources.fitsIn(rc,\n                              clusterResource,\n                              capability,\n                              availableAndKillable)) {\n           // Stop if we find enough spaces\n           availableContainers \u003d 1;\n           break;\n         }\n       }\n     }\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n-              application.findNodeToUnreserve(clusterResource, node, priority,\n-                  resourceNeedToUnReserve);\n+              application.findNodeToUnreserve(clusterResource, node,\n+                  schedulerKey, resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d\n           new ContainerAllocation(unreservedContainer, request.getCapability(),\n               AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ContainerAllocation result \u003d\n             new ContainerAllocation(null, request.getCapability(),\n                 AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, SchedulerRequestKey schedulerKey,\n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      SchedulingMode schedulingMode, ResourceLimits currentResoureLimits) {\n    lastResourceRequest \u003d request;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n          + \" application\u003d\" + application.getApplicationId()\n          + \" priority\u003d\" + schedulerKey.getPriority()\n          + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n        request.getNodeLabelExpression(), node.getPartition(),\n        schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      return new ContainerAllocation(rmContainer, null,\n          AllocationState.LOCALITY_SKIPPED);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        schedulerKey, capability);\n\n    // Can we allocate a container on this node?\n    long availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    // Check if we need to kill some containers to allocate this one\n    List\u003cRMContainer\u003e toKillContainers \u003d null;\n    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n      Resource availableAndKillable \u003d Resources.clone(available);\n      for (RMContainer killableContainer : node\n          .getKillableContainers().values()) {\n        if (null \u003d\u003d toKillContainers) {\n          toKillContainers \u003d new ArrayList\u003c\u003e();\n        }\n        toKillContainers.add(killableContainer);\n        Resources.addTo(availableAndKillable,\n                        killableContainer.getAllocatedResource());\n        if (Resources.fitsIn(rc,\n                             clusterResource,\n                             capability,\n                             availableAndKillable)) {\n          // Stop if we find enough spaces\n          availableContainers \u003d 1;\n          break;\n        }\n      }\n    }\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node,\n                  schedulerKey, resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d\n          new ContainerAllocation(unreservedContainer, request.getCapability(),\n              AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      result.setToKillContainers(toKillContainers);\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d\n            new ContainerAllocation(null, request.getCapability(),\n                AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        result.setToKillContainers(null);\n        return result;\n      }\n      // Skip the locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
          "extendedDetails": {}
        }
      ]
    },
    "42f90ab885d9693fcc1e52f9637f7de4111110ae": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4844. Add getMemorySize/getVirtualCoresSize to o.a.h.y.api.records.Resource. Contributed by Wangda Tan.\n",
      "commitDate": "29/05/16 8:54 AM",
      "commitName": "42f90ab885d9693fcc1e52f9637f7de4111110ae",
      "commitAuthor": "Varun Vasudev",
      "commitDateOld": "16/03/16 5:02 PM",
      "commitNameOld": "ae14e5d07f1b6702a5160637438028bb03d9387e",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 73.66,
      "commitsBetweenForRepo": 467,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,150 +1,150 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n       NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n       ResourceLimits currentResoureLimits) {\n     lastResourceRequest \u003d request;\n     \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n         request.getNodeLabelExpression(), node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       return new ContainerAllocation(rmContainer, null,\n           AllocationState.LOCALITY_SKIPPED);\n     }\n \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         priority, capability);\n \n     // Can we allocate a container on this node?\n-    int availableContainers \u003d\n+    long availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     // Check if we need to kill some containers to allocate this one\n     List\u003cRMContainer\u003e toKillContainers \u003d null;\n     if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n       Resource availableAndKillable \u003d Resources.clone(available);\n       for (RMContainer killableContainer : node\n           .getKillableContainers().values()) {\n         if (null \u003d\u003d toKillContainers) {\n           toKillContainers \u003d new ArrayList\u003c\u003e();\n         }\n         toKillContainers.add(killableContainer);\n         Resources.addTo(availableAndKillable,\n                         killableContainer.getAllocatedResource());\n         if (Resources.fitsIn(rc,\n                              clusterResource,\n                              capability,\n                              availableAndKillable)) {\n           // Stop if we find enough spaces\n           availableContainers \u003d 1;\n           break;\n         }\n       }\n     }\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n               application.findNodeToUnreserve(clusterResource, node, priority,\n                   resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d\n           new ContainerAllocation(unreservedContainer, request.getCapability(),\n               AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ContainerAllocation result \u003d\n             new ContainerAllocation(null, request.getCapability(),\n                 AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    lastResourceRequest \u003d request;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n        request.getNodeLabelExpression(), node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      return new ContainerAllocation(rmContainer, null,\n          AllocationState.LOCALITY_SKIPPED);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        priority, capability);\n\n    // Can we allocate a container on this node?\n    long availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    // Check if we need to kill some containers to allocate this one\n    List\u003cRMContainer\u003e toKillContainers \u003d null;\n    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n      Resource availableAndKillable \u003d Resources.clone(available);\n      for (RMContainer killableContainer : node\n          .getKillableContainers().values()) {\n        if (null \u003d\u003d toKillContainers) {\n          toKillContainers \u003d new ArrayList\u003c\u003e();\n        }\n        toKillContainers.add(killableContainer);\n        Resources.addTo(availableAndKillable,\n                        killableContainer.getAllocatedResource());\n        if (Resources.fitsIn(rc,\n                             clusterResource,\n                             capability,\n                             availableAndKillable)) {\n          // Stop if we find enough spaces\n          availableContainers \u003d 1;\n          break;\n        }\n      }\n    }\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node, priority,\n                  resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d\n          new ContainerAllocation(unreservedContainer, request.getCapability(),\n              AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      result.setToKillContainers(toKillContainers);\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d\n            new ContainerAllocation(null, request.getCapability(),\n                AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        result.setToKillContainers(null);\n        return result;\n      }\n      // Skip the locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "ae14e5d07f1b6702a5160637438028bb03d9387e": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4108. CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\n\n(cherry picked from commit 7e8c9beb4156dcaeb3a11e60aaa06d2370626913)\n",
      "commitDate": "16/03/16 5:02 PM",
      "commitName": "ae14e5d07f1b6702a5160637438028bb03d9387e",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/03/16 5:02 PM",
      "commitNameOld": "fa7a43529d529f0006c8033c2003f15b9b93f103",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,129 +1,150 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n       NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n       ResourceLimits currentResoureLimits) {\n     lastResourceRequest \u003d request;\n     \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n         request.getNodeLabelExpression(), node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       return new ContainerAllocation(rmContainer, null,\n           AllocationState.LOCALITY_SKIPPED);\n     }\n \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n-    assert Resources.greaterThan(\n-        rc, clusterResource, available, Resources.none());\n-\n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n+    // Check if we need to kill some containers to allocate this one\n+    List\u003cRMContainer\u003e toKillContainers \u003d null;\n+    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n+      Resource availableAndKillable \u003d Resources.clone(available);\n+      for (RMContainer killableContainer : node\n+          .getKillableContainers().values()) {\n+        if (null \u003d\u003d toKillContainers) {\n+          toKillContainers \u003d new ArrayList\u003c\u003e();\n+        }\n+        toKillContainers.add(killableContainer);\n+        Resources.addTo(availableAndKillable,\n+                        killableContainer.getAllocatedResource());\n+        if (Resources.fitsIn(rc,\n+                             clusterResource,\n+                             capability,\n+                             availableAndKillable)) {\n+          // Stop if we find enough spaces\n+          availableContainers \u003d 1;\n+          break;\n+        }\n+      }\n+    }\n+\n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n               application.findNodeToUnreserve(clusterResource, node, priority,\n                   resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d\n           new ContainerAllocation(unreservedContainer, request.getCapability(),\n               AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n+      result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n-\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ContainerAllocation result \u003d\n             new ContainerAllocation(null, request.getCapability(),\n                 AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n+        result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    lastResourceRequest \u003d request;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n        request.getNodeLabelExpression(), node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      return new ContainerAllocation(rmContainer, null,\n          AllocationState.LOCALITY_SKIPPED);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    // Check if we need to kill some containers to allocate this one\n    List\u003cRMContainer\u003e toKillContainers \u003d null;\n    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n      Resource availableAndKillable \u003d Resources.clone(available);\n      for (RMContainer killableContainer : node\n          .getKillableContainers().values()) {\n        if (null \u003d\u003d toKillContainers) {\n          toKillContainers \u003d new ArrayList\u003c\u003e();\n        }\n        toKillContainers.add(killableContainer);\n        Resources.addTo(availableAndKillable,\n                        killableContainer.getAllocatedResource());\n        if (Resources.fitsIn(rc,\n                             clusterResource,\n                             capability,\n                             availableAndKillable)) {\n          // Stop if we find enough spaces\n          availableContainers \u003d 1;\n          break;\n        }\n      }\n    }\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node, priority,\n                  resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d\n          new ContainerAllocation(unreservedContainer, request.getCapability(),\n              AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      result.setToKillContainers(toKillContainers);\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d\n            new ContainerAllocation(null, request.getCapability(),\n                AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        result.setToKillContainers(null);\n        return result;\n      }\n      // Skip the locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "fa7a43529d529f0006c8033c2003f15b9b93f103": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\"\n\nThis reverts commit 7e8c9beb4156dcaeb3a11e60aaa06d2370626913.\n",
      "commitDate": "16/03/16 5:02 PM",
      "commitName": "fa7a43529d529f0006c8033c2003f15b9b93f103",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/03/16 4:59 PM",
      "commitNameOld": "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,150 +1,129 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n       NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n       ResourceLimits currentResoureLimits) {\n     lastResourceRequest \u003d request;\n     \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n         request.getNodeLabelExpression(), node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       return new ContainerAllocation(rmContainer, null,\n           AllocationState.LOCALITY_SKIPPED);\n     }\n \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n+    assert Resources.greaterThan(\n+        rc, clusterResource, available, Resources.none());\n+\n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n-    // Check if we need to kill some containers to allocate this one\n-    List\u003cRMContainer\u003e toKillContainers \u003d null;\n-    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n-      Resource availableAndKillable \u003d Resources.clone(available);\n-      for (RMContainer killableContainer : node\n-          .getKillableContainers().values()) {\n-        if (null \u003d\u003d toKillContainers) {\n-          toKillContainers \u003d new ArrayList\u003c\u003e();\n-        }\n-        toKillContainers.add(killableContainer);\n-        Resources.addTo(availableAndKillable,\n-                        killableContainer.getAllocatedResource());\n-        if (Resources.fitsIn(rc,\n-                             clusterResource,\n-                             capability,\n-                             availableAndKillable)) {\n-          // Stop if we find enough spaces\n-          availableContainers \u003d 1;\n-          break;\n-        }\n-      }\n-    }\n-\n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n               application.findNodeToUnreserve(clusterResource, node, priority,\n                   resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d\n           new ContainerAllocation(unreservedContainer, request.getCapability(),\n               AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n-      result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n+\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ContainerAllocation result \u003d\n             new ContainerAllocation(null, request.getCapability(),\n                 AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n-        result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    lastResourceRequest \u003d request;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n        request.getNodeLabelExpression(), node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      return new ContainerAllocation(rmContainer, null,\n          AllocationState.LOCALITY_SKIPPED);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    assert Resources.greaterThan(\n        rc, clusterResource, available, Resources.none());\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node, priority,\n                  resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d\n          new ContainerAllocation(unreservedContainer, request.getCapability(),\n              AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d\n            new ContainerAllocation(null, request.getCapability(),\n                AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        return result;\n      }\n      // Skip the locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913": {
      "type": "Ybodychange",
      "commitMessage": "CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\n",
      "commitDate": "16/03/16 4:59 PM",
      "commitName": "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "28/02/16 9:35 AM",
      "commitNameOld": "f9692770a58af0ab082eb7f15da9cbdcd177605b",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 17.27,
      "commitsBetweenForRepo": 107,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,129 +1,150 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n       NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n       ResourceLimits currentResoureLimits) {\n     lastResourceRequest \u003d request;\n     \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n         request.getNodeLabelExpression(), node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       return new ContainerAllocation(rmContainer, null,\n           AllocationState.LOCALITY_SKIPPED);\n     }\n \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n-    assert Resources.greaterThan(\n-        rc, clusterResource, available, Resources.none());\n-\n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n+    // Check if we need to kill some containers to allocate this one\n+    List\u003cRMContainer\u003e toKillContainers \u003d null;\n+    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n+      Resource availableAndKillable \u003d Resources.clone(available);\n+      for (RMContainer killableContainer : node\n+          .getKillableContainers().values()) {\n+        if (null \u003d\u003d toKillContainers) {\n+          toKillContainers \u003d new ArrayList\u003c\u003e();\n+        }\n+        toKillContainers.add(killableContainer);\n+        Resources.addTo(availableAndKillable,\n+                        killableContainer.getAllocatedResource());\n+        if (Resources.fitsIn(rc,\n+                             clusterResource,\n+                             capability,\n+                             availableAndKillable)) {\n+          // Stop if we find enough spaces\n+          availableContainers \u003d 1;\n+          break;\n+        }\n+      }\n+    }\n+\n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n               application.findNodeToUnreserve(clusterResource, node, priority,\n                   resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d\n           new ContainerAllocation(unreservedContainer, request.getCapability(),\n               AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n+      result.setToKillContainers(toKillContainers);\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n-\n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ContainerAllocation result \u003d\n             new ContainerAllocation(null, request.getCapability(),\n                 AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n+        result.setToKillContainers(null);\n         return result;\n       }\n       // Skip the locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    lastResourceRequest \u003d request;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n        request.getNodeLabelExpression(), node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      return new ContainerAllocation(rmContainer, null,\n          AllocationState.LOCALITY_SKIPPED);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    // Check if we need to kill some containers to allocate this one\n    List\u003cRMContainer\u003e toKillContainers \u003d null;\n    if (availableContainers \u003d\u003d 0 \u0026\u0026 currentResoureLimits.isAllowPreemption()) {\n      Resource availableAndKillable \u003d Resources.clone(available);\n      for (RMContainer killableContainer : node\n          .getKillableContainers().values()) {\n        if (null \u003d\u003d toKillContainers) {\n          toKillContainers \u003d new ArrayList\u003c\u003e();\n        }\n        toKillContainers.add(killableContainer);\n        Resources.addTo(availableAndKillable,\n                        killableContainer.getAllocatedResource());\n        if (Resources.fitsIn(rc,\n                             clusterResource,\n                             capability,\n                             availableAndKillable)) {\n          // Stop if we find enough spaces\n          availableContainers \u003d 1;\n          break;\n        }\n      }\n    }\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node, priority,\n                  resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d\n          new ContainerAllocation(unreservedContainer, request.getCapability(),\n              AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      result.setToKillContainers(toKillContainers);\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d\n            new ContainerAllocation(null, request.getCapability(),\n                AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        result.setToKillContainers(null);\n        return result;\n      }\n      // Skip the locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "f9692770a58af0ab082eb7f15da9cbdcd177605b": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4718. Rename variables in SchedulerNode to reduce ambiguity post YARN-1011. (Inigo Goiri via kasha)\n",
      "commitDate": "28/02/16 9:35 AM",
      "commitName": "f9692770a58af0ab082eb7f15da9cbdcd177605b",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "14/12/15 10:52 AM",
      "commitNameOld": "6cb0af3c39a5d49cb2f7911ee21363a9542ca2d7",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 75.95,
      "commitsBetweenForRepo": 496,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,129 +1,129 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n       NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n       ResourceLimits currentResoureLimits) {\n     lastResourceRequest \u003d request;\n     \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n         request.getNodeLabelExpression(), node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       return new ContainerAllocation(rmContainer, null,\n           AllocationState.LOCALITY_SKIPPED);\n     }\n \n     Resource capability \u003d request.getCapability();\n-    Resource available \u003d node.getAvailableResource();\n+    Resource available \u003d node.getUnallocatedResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     assert Resources.greaterThan(\n         rc, clusterResource, available, Resources.none());\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n               application.findNodeToUnreserve(clusterResource, node, priority,\n                   resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d\n           new ContainerAllocation(unreservedContainer, request.getCapability(),\n               AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ContainerAllocation result \u003d\n             new ContainerAllocation(null, request.getCapability(),\n                 AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         return result;\n       }\n       // Skip the locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    lastResourceRequest \u003d request;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n        request.getNodeLabelExpression(), node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      return new ContainerAllocation(rmContainer, null,\n          AllocationState.LOCALITY_SKIPPED);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getUnallocatedResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    assert Resources.greaterThan(\n        rc, clusterResource, available, Resources.none());\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node, priority,\n                  resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d\n          new ContainerAllocation(unreservedContainer, request.getCapability(),\n              AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d\n            new ContainerAllocation(null, request.getCapability(),\n                AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        return result;\n      }\n      // Skip the locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "89cab1ba5f0671f8ef30dbe7432079c18362b434": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1651. CapacityScheduler side changes to support container resize. Contributed by Wangda Tan\n",
      "commitDate": "23/09/15 1:29 PM",
      "commitName": "89cab1ba5f0671f8ef30dbe7432079c18362b434",
      "commitAuthor": "Jian He",
      "commitDateOld": "12/08/15 3:07 PM",
      "commitNameOld": "e5003be907acef87c2770e3f2914953f62017b0e",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 41.93,
      "commitsBetweenForRepo": 260,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,129 +1,129 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n       NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n       ResourceLimits currentResoureLimits) {\n     lastResourceRequest \u003d request;\n     \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n \n     // check if the resource request can access the label\n-    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n-        node.getPartition(), schedulingMode)) {\n+    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n+        request.getNodeLabelExpression(), node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       return new ContainerAllocation(rmContainer, null,\n           AllocationState.LOCALITY_SKIPPED);\n     }\n \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       // Skip this locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     assert Resources.greaterThan(\n         rc, clusterResource, available, Resources.none());\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n               application.findNodeToUnreserve(clusterResource, node, priority,\n                   resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d\n           new ContainerAllocation(unreservedContainer, request.getCapability(),\n               AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n       // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n           // unreserve one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             // Skip the locality request\n             return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ContainerAllocation result \u003d\n             new ContainerAllocation(null, request.getCapability(),\n                 AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         return result;\n       }\n       // Skip the locality request\n       return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    lastResourceRequest \u003d request;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(\n        request.getNodeLabelExpression(), node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      return new ContainerAllocation(rmContainer, null,\n          AllocationState.LOCALITY_SKIPPED);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    assert Resources.greaterThan(\n        rc, clusterResource, available, Resources.none());\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node, priority,\n                  resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d\n          new ContainerAllocation(unreservedContainer, request.getCapability(),\n              AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d\n            new ContainerAllocation(null, request.getCapability(),\n                AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        return result;\n      }\n      // Skip the locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "e5003be907acef87c2770e3f2914953f62017b0e": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4026. Refactored ContainerAllocator to accept a list of priorites rather than a single priority. Contributed by Wangda Tan\n",
      "commitDate": "12/08/15 3:07 PM",
      "commitName": "e5003be907acef87c2770e3f2914953f62017b0e",
      "commitAuthor": "Jian He",
      "commitDateOld": "05/08/15 1:47 PM",
      "commitNameOld": "ba2313d6145a1234777938a747187373f4cd58d9",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 7.06,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,125 +1,129 @@\n   private ContainerAllocation assignContainer(Resource clusterResource,\n       FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n       NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n       ResourceLimits currentResoureLimits) {\n     lastResourceRequest \u003d request;\n     \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n         node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       return new ContainerAllocation(rmContainer, null,\n-          AllocationState.QUEUE_SKIPPED);\n+          AllocationState.LOCALITY_SKIPPED);\n     }\n \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n-      return ContainerAllocation.QUEUE_SKIPPED;\n+      // Skip this locality request\n+      return ContainerAllocation.LOCALITY_SKIPPED;\n     }\n \n     assert Resources.greaterThan(\n         rc, clusterResource, available, Resources.none());\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n         application.getCSLeafQueue().getReservationContinueLooking();\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n       // We will only do continuous reservation when this is not allocated from\n       // reserved container\n       if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n           \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027\n         // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n               application.findNodeToUnreserve(clusterResource, node, priority,\n                   resourceNeedToUnReserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n           // new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t\n           // continue.\n           if (null \u003d\u003d unreservedContainer) {\n-            return ContainerAllocation.QUEUE_SKIPPED;\n+            // Skip the locality request\n+            return ContainerAllocation.LOCALITY_SKIPPED;\n           }\n         }\n       }\n \n       ContainerAllocation result \u003d\n           new ContainerAllocation(unreservedContainer, request.getCapability(),\n               AllocationState.ALLOCATED);\n       result.containerNodeType \u003d type;\n       return result;\n     } else {\n-      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n-      // if this was an already a reserved container, reserve it again\n+      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n+      // it or if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n-          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n-          // one.\n+          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n+          // unreserve one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n-            return ContainerAllocation.QUEUE_SKIPPED;\n+            // Skip the locality request\n+            return ContainerAllocation.LOCALITY_SKIPPED;          \n           }\n         }\n \n         ContainerAllocation result \u003d\n             new ContainerAllocation(null, request.getCapability(),\n                 AllocationState.RESERVED);\n         result.containerNodeType \u003d type;\n         return result;\n       }\n-      return ContainerAllocation.QUEUE_SKIPPED;\n+      // Skip the locality request\n+      return ContainerAllocation.LOCALITY_SKIPPED;    \n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    lastResourceRequest \u003d request;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      return new ContainerAllocation(rmContainer, null,\n          AllocationState.LOCALITY_SKIPPED);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      // Skip this locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;\n    }\n\n    assert Resources.greaterThan(\n        rc, clusterResource, available, Resources.none());\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node, priority,\n                  resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d\n          new ContainerAllocation(unreservedContainer, request.getCapability(),\n              AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve\n      // it or if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to\n          // unreserve one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            // Skip the locality request\n            return ContainerAllocation.LOCALITY_SKIPPED;          \n          }\n        }\n\n        ContainerAllocation result \u003d\n            new ContainerAllocation(null, request.getCapability(),\n                AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        return result;\n      }\n      // Skip the locality request\n      return ContainerAllocation.LOCALITY_SKIPPED;    \n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
      "extendedDetails": {}
    },
    "ba2313d6145a1234777938a747187373f4cd58d9": {
      "type": "Ymultichange(Ymovefromfile,Yreturntypechange,Ybodychange,Yparameterchange)",
      "commitMessage": "YARN-3983. Refactored CapacityScheduleri#FiCaSchedulerApp to easier extend container allocation logic. Contributed by Wangda Tan\n",
      "commitDate": "05/08/15 1:47 PM",
      "commitName": "ba2313d6145a1234777938a747187373f4cd58d9",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "YARN-3983. Refactored CapacityScheduleri#FiCaSchedulerApp to easier extend container allocation logic. Contributed by Wangda Tan\n",
          "commitDate": "05/08/15 1:47 PM",
          "commitName": "ba2313d6145a1234777938a747187373f4cd58d9",
          "commitAuthor": "Jian He",
          "commitDateOld": "05/08/15 12:57 PM",
          "commitNameOld": "f271d377357ad680924d19f07e6c8315e7c89bae",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,175 +1,125 @@\n-  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n-      Priority priority,\n-      ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer, SchedulingMode schedulingMode,\n+  private ContainerAllocation assignContainer(Resource clusterResource,\n+      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n+      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n       ResourceLimits currentResoureLimits) {\n+    lastResourceRequest \u003d request;\n+    \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + getApplicationId()\n+        + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n         node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n-      if (rmContainer !\u003d null) {\n-        unreserve(priority, node, rmContainer);\n-      }\n-      return new CSAssignment(Resources.none(), type);\n+      return new ContainerAllocation(rmContainer, null,\n+          AllocationState.QUEUE_SKIPPED);\n     }\n \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n-      return new CSAssignment(Resources.none(), type);\n+      return ContainerAllocation.QUEUE_SKIPPED;\n     }\n \n     assert Resources.greaterThan(\n         rc, clusterResource, available, Resources.none());\n \n-    // Create the container if necessary\n-    Container container \u003d\n-        getContainer(rmContainer, node, capability, priority);\n-\n-    // something went wrong getting/creating the container\n-    if (container \u003d\u003d null) {\n-      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n-      return new CSAssignment(Resources.none(), type);\n-    }\n-\n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n-        getCSLeafQueue().getReservationContinueLooking();\n+        application.getCSLeafQueue().getReservationContinueLooking();\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n-\n-      // Did we previously reserve containers at this \u0027priority\u0027?\n-      if (rmContainer !\u003d null) {\n-        unreserve(priority, node, rmContainer);\n-      } else if (reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n+      // We will only do continuous reservation when this is not allocated from\n+      // reserved container\n+      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n+          \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n-        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n+        // some containers to meet this queue, its parents\u0027, or the users\u0027\n+        // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n-              findNodeToUnreserve(clusterResource, node, priority,\n+              application.findNodeToUnreserve(clusterResource, node, priority,\n                   resourceNeedToUnReserve);\n-          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved\n+          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n+          // new/reserved\n           // container (That means we *have to* unreserve some resource to\n-          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n+          // continue)). If we failed to unreserve some resource, we can\u0027t\n+          // continue.\n           if (null \u003d\u003d unreservedContainer) {\n-            return new CSAssignment(Resources.none(), type);\n+            return ContainerAllocation.QUEUE_SKIPPED;\n           }\n         }\n       }\n \n-      // Inform the application\n-      RMContainer allocatedContainer \u003d\n-          allocate(type, node, priority, request, container);\n-\n-      // Does the application need this resource?\n-      if (allocatedContainer \u003d\u003d null) {\n-        CSAssignment csAssignment \u003d  new CSAssignment(Resources.none(), type);\n-        csAssignment.setApplication(this);\n-        csAssignment.setExcessReservation(unreservedContainer);\n-        return csAssignment;\n-      }\n-\n-      // Inform the node\n-      node.allocateContainer(allocatedContainer);\n-\n-      // Inform the ordering policy\n-      getCSLeafQueue().getOrderingPolicy().containerAllocated(this,\n-          allocatedContainer);\n-\n-      LOG.info(\"assignedContainer\" +\n-          \" application attempt\u003d\" + getApplicationAttemptId() +\n-          \" container\u003d\" + container +\n-          \" queue\u003d\" + this +\n-          \" clusterResource\u003d\" + clusterResource);\n-      createdContainer.setValue(allocatedContainer);\n-      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n-      assignment.getAssignmentInformation().addAllocationDetails(\n-        container.getId(), getCSLeafQueue().getQueuePath());\n-      assignment.getAssignmentInformation().incrAllocations();\n-      assignment.setApplication(this);\n-      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-        container.getResource());\n-\n-      assignment.setExcessReservation(unreservedContainer);\n-      return assignment;\n+      ContainerAllocation result \u003d\n+          new ContainerAllocation(unreservedContainer, request.getCapability(),\n+              AllocationState.ALLOCATED);\n+      result.containerNodeType \u003d type;\n+      return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n           // one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n-            return new CSAssignment(Resources.none(), type);\n+            return ContainerAllocation.QUEUE_SKIPPED;\n           }\n         }\n \n-        // Reserve by \u0027charging\u0027 in advance...\n-        reserve(priority, node, rmContainer, container);\n-\n-        LOG.info(\"Reserved container \" +\n-            \" application\u003d\" + getApplicationId() +\n-            \" resource\u003d\" + request.getCapability() +\n-            \" queue\u003d\" + this.toString() +\n-            \" cluster\u003d\" + clusterResource);\n-        CSAssignment assignment \u003d\n-            new CSAssignment(request.getCapability(), type);\n-        assignment.getAssignmentInformation().addReservationDetails(\n-          container.getId(), getCSLeafQueue().getQueuePath());\n-        assignment.getAssignmentInformation().incrReservations();\n-        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n-          request.getCapability());\n-        return assignment;\n+        ContainerAllocation result \u003d\n+            new ContainerAllocation(null, request.getCapability(),\n+                AllocationState.RESERVED);\n+        result.containerNodeType \u003d type;\n+        return result;\n       }\n-      return new CSAssignment(Resources.none(), type);\n+      return ContainerAllocation.QUEUE_SKIPPED;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    lastResourceRequest \u003d request;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      return new ContainerAllocation(rmContainer, null,\n          AllocationState.QUEUE_SKIPPED);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return ContainerAllocation.QUEUE_SKIPPED;\n    }\n\n    assert Resources.greaterThan(\n        rc, clusterResource, available, Resources.none());\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node, priority,\n                  resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            return ContainerAllocation.QUEUE_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d\n          new ContainerAllocation(unreservedContainer, request.getCapability(),\n              AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n          // one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            return ContainerAllocation.QUEUE_SKIPPED;\n          }\n        }\n\n        ContainerAllocation result \u003d\n            new ContainerAllocation(null, request.getCapability(),\n                AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        return result;\n      }\n      return ContainerAllocation.QUEUE_SKIPPED;\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
          "extendedDetails": {
            "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
            "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
            "oldMethodName": "assignContainer",
            "newMethodName": "assignContainer"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "YARN-3983. Refactored CapacityScheduleri#FiCaSchedulerApp to easier extend container allocation logic. Contributed by Wangda Tan\n",
          "commitDate": "05/08/15 1:47 PM",
          "commitName": "ba2313d6145a1234777938a747187373f4cd58d9",
          "commitAuthor": "Jian He",
          "commitDateOld": "05/08/15 12:57 PM",
          "commitNameOld": "f271d377357ad680924d19f07e6c8315e7c89bae",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,175 +1,125 @@\n-  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n-      Priority priority,\n-      ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer, SchedulingMode schedulingMode,\n+  private ContainerAllocation assignContainer(Resource clusterResource,\n+      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n+      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n       ResourceLimits currentResoureLimits) {\n+    lastResourceRequest \u003d request;\n+    \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + getApplicationId()\n+        + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n         node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n-      if (rmContainer !\u003d null) {\n-        unreserve(priority, node, rmContainer);\n-      }\n-      return new CSAssignment(Resources.none(), type);\n+      return new ContainerAllocation(rmContainer, null,\n+          AllocationState.QUEUE_SKIPPED);\n     }\n \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n-      return new CSAssignment(Resources.none(), type);\n+      return ContainerAllocation.QUEUE_SKIPPED;\n     }\n \n     assert Resources.greaterThan(\n         rc, clusterResource, available, Resources.none());\n \n-    // Create the container if necessary\n-    Container container \u003d\n-        getContainer(rmContainer, node, capability, priority);\n-\n-    // something went wrong getting/creating the container\n-    if (container \u003d\u003d null) {\n-      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n-      return new CSAssignment(Resources.none(), type);\n-    }\n-\n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n-        getCSLeafQueue().getReservationContinueLooking();\n+        application.getCSLeafQueue().getReservationContinueLooking();\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n-\n-      // Did we previously reserve containers at this \u0027priority\u0027?\n-      if (rmContainer !\u003d null) {\n-        unreserve(priority, node, rmContainer);\n-      } else if (reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n+      // We will only do continuous reservation when this is not allocated from\n+      // reserved container\n+      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n+          \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n-        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n+        // some containers to meet this queue, its parents\u0027, or the users\u0027\n+        // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n-              findNodeToUnreserve(clusterResource, node, priority,\n+              application.findNodeToUnreserve(clusterResource, node, priority,\n                   resourceNeedToUnReserve);\n-          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved\n+          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n+          // new/reserved\n           // container (That means we *have to* unreserve some resource to\n-          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n+          // continue)). If we failed to unreserve some resource, we can\u0027t\n+          // continue.\n           if (null \u003d\u003d unreservedContainer) {\n-            return new CSAssignment(Resources.none(), type);\n+            return ContainerAllocation.QUEUE_SKIPPED;\n           }\n         }\n       }\n \n-      // Inform the application\n-      RMContainer allocatedContainer \u003d\n-          allocate(type, node, priority, request, container);\n-\n-      // Does the application need this resource?\n-      if (allocatedContainer \u003d\u003d null) {\n-        CSAssignment csAssignment \u003d  new CSAssignment(Resources.none(), type);\n-        csAssignment.setApplication(this);\n-        csAssignment.setExcessReservation(unreservedContainer);\n-        return csAssignment;\n-      }\n-\n-      // Inform the node\n-      node.allocateContainer(allocatedContainer);\n-\n-      // Inform the ordering policy\n-      getCSLeafQueue().getOrderingPolicy().containerAllocated(this,\n-          allocatedContainer);\n-\n-      LOG.info(\"assignedContainer\" +\n-          \" application attempt\u003d\" + getApplicationAttemptId() +\n-          \" container\u003d\" + container +\n-          \" queue\u003d\" + this +\n-          \" clusterResource\u003d\" + clusterResource);\n-      createdContainer.setValue(allocatedContainer);\n-      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n-      assignment.getAssignmentInformation().addAllocationDetails(\n-        container.getId(), getCSLeafQueue().getQueuePath());\n-      assignment.getAssignmentInformation().incrAllocations();\n-      assignment.setApplication(this);\n-      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-        container.getResource());\n-\n-      assignment.setExcessReservation(unreservedContainer);\n-      return assignment;\n+      ContainerAllocation result \u003d\n+          new ContainerAllocation(unreservedContainer, request.getCapability(),\n+              AllocationState.ALLOCATED);\n+      result.containerNodeType \u003d type;\n+      return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n           // one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n-            return new CSAssignment(Resources.none(), type);\n+            return ContainerAllocation.QUEUE_SKIPPED;\n           }\n         }\n \n-        // Reserve by \u0027charging\u0027 in advance...\n-        reserve(priority, node, rmContainer, container);\n-\n-        LOG.info(\"Reserved container \" +\n-            \" application\u003d\" + getApplicationId() +\n-            \" resource\u003d\" + request.getCapability() +\n-            \" queue\u003d\" + this.toString() +\n-            \" cluster\u003d\" + clusterResource);\n-        CSAssignment assignment \u003d\n-            new CSAssignment(request.getCapability(), type);\n-        assignment.getAssignmentInformation().addReservationDetails(\n-          container.getId(), getCSLeafQueue().getQueuePath());\n-        assignment.getAssignmentInformation().incrReservations();\n-        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n-          request.getCapability());\n-        return assignment;\n+        ContainerAllocation result \u003d\n+            new ContainerAllocation(null, request.getCapability(),\n+                AllocationState.RESERVED);\n+        result.containerNodeType \u003d type;\n+        return result;\n       }\n-      return new CSAssignment(Resources.none(), type);\n+      return ContainerAllocation.QUEUE_SKIPPED;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    lastResourceRequest \u003d request;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      return new ContainerAllocation(rmContainer, null,\n          AllocationState.QUEUE_SKIPPED);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return ContainerAllocation.QUEUE_SKIPPED;\n    }\n\n    assert Resources.greaterThan(\n        rc, clusterResource, available, Resources.none());\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node, priority,\n                  resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            return ContainerAllocation.QUEUE_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d\n          new ContainerAllocation(unreservedContainer, request.getCapability(),\n              AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n          // one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            return ContainerAllocation.QUEUE_SKIPPED;\n          }\n        }\n\n        ContainerAllocation result \u003d\n            new ContainerAllocation(null, request.getCapability(),\n                AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        return result;\n      }\n      return ContainerAllocation.QUEUE_SKIPPED;\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "CSAssignment",
            "newValue": "ContainerAllocation"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3983. Refactored CapacityScheduleri#FiCaSchedulerApp to easier extend container allocation logic. Contributed by Wangda Tan\n",
          "commitDate": "05/08/15 1:47 PM",
          "commitName": "ba2313d6145a1234777938a747187373f4cd58d9",
          "commitAuthor": "Jian He",
          "commitDateOld": "05/08/15 12:57 PM",
          "commitNameOld": "f271d377357ad680924d19f07e6c8315e7c89bae",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,175 +1,125 @@\n-  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n-      Priority priority,\n-      ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer, SchedulingMode schedulingMode,\n+  private ContainerAllocation assignContainer(Resource clusterResource,\n+      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n+      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n       ResourceLimits currentResoureLimits) {\n+    lastResourceRequest \u003d request;\n+    \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + getApplicationId()\n+        + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n         node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n-      if (rmContainer !\u003d null) {\n-        unreserve(priority, node, rmContainer);\n-      }\n-      return new CSAssignment(Resources.none(), type);\n+      return new ContainerAllocation(rmContainer, null,\n+          AllocationState.QUEUE_SKIPPED);\n     }\n \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n-      return new CSAssignment(Resources.none(), type);\n+      return ContainerAllocation.QUEUE_SKIPPED;\n     }\n \n     assert Resources.greaterThan(\n         rc, clusterResource, available, Resources.none());\n \n-    // Create the container if necessary\n-    Container container \u003d\n-        getContainer(rmContainer, node, capability, priority);\n-\n-    // something went wrong getting/creating the container\n-    if (container \u003d\u003d null) {\n-      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n-      return new CSAssignment(Resources.none(), type);\n-    }\n-\n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n-        getCSLeafQueue().getReservationContinueLooking();\n+        application.getCSLeafQueue().getReservationContinueLooking();\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n-\n-      // Did we previously reserve containers at this \u0027priority\u0027?\n-      if (rmContainer !\u003d null) {\n-        unreserve(priority, node, rmContainer);\n-      } else if (reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n+      // We will only do continuous reservation when this is not allocated from\n+      // reserved container\n+      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n+          \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n-        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n+        // some containers to meet this queue, its parents\u0027, or the users\u0027\n+        // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n-              findNodeToUnreserve(clusterResource, node, priority,\n+              application.findNodeToUnreserve(clusterResource, node, priority,\n                   resourceNeedToUnReserve);\n-          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved\n+          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n+          // new/reserved\n           // container (That means we *have to* unreserve some resource to\n-          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n+          // continue)). If we failed to unreserve some resource, we can\u0027t\n+          // continue.\n           if (null \u003d\u003d unreservedContainer) {\n-            return new CSAssignment(Resources.none(), type);\n+            return ContainerAllocation.QUEUE_SKIPPED;\n           }\n         }\n       }\n \n-      // Inform the application\n-      RMContainer allocatedContainer \u003d\n-          allocate(type, node, priority, request, container);\n-\n-      // Does the application need this resource?\n-      if (allocatedContainer \u003d\u003d null) {\n-        CSAssignment csAssignment \u003d  new CSAssignment(Resources.none(), type);\n-        csAssignment.setApplication(this);\n-        csAssignment.setExcessReservation(unreservedContainer);\n-        return csAssignment;\n-      }\n-\n-      // Inform the node\n-      node.allocateContainer(allocatedContainer);\n-\n-      // Inform the ordering policy\n-      getCSLeafQueue().getOrderingPolicy().containerAllocated(this,\n-          allocatedContainer);\n-\n-      LOG.info(\"assignedContainer\" +\n-          \" application attempt\u003d\" + getApplicationAttemptId() +\n-          \" container\u003d\" + container +\n-          \" queue\u003d\" + this +\n-          \" clusterResource\u003d\" + clusterResource);\n-      createdContainer.setValue(allocatedContainer);\n-      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n-      assignment.getAssignmentInformation().addAllocationDetails(\n-        container.getId(), getCSLeafQueue().getQueuePath());\n-      assignment.getAssignmentInformation().incrAllocations();\n-      assignment.setApplication(this);\n-      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-        container.getResource());\n-\n-      assignment.setExcessReservation(unreservedContainer);\n-      return assignment;\n+      ContainerAllocation result \u003d\n+          new ContainerAllocation(unreservedContainer, request.getCapability(),\n+              AllocationState.ALLOCATED);\n+      result.containerNodeType \u003d type;\n+      return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n           // one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n-            return new CSAssignment(Resources.none(), type);\n+            return ContainerAllocation.QUEUE_SKIPPED;\n           }\n         }\n \n-        // Reserve by \u0027charging\u0027 in advance...\n-        reserve(priority, node, rmContainer, container);\n-\n-        LOG.info(\"Reserved container \" +\n-            \" application\u003d\" + getApplicationId() +\n-            \" resource\u003d\" + request.getCapability() +\n-            \" queue\u003d\" + this.toString() +\n-            \" cluster\u003d\" + clusterResource);\n-        CSAssignment assignment \u003d\n-            new CSAssignment(request.getCapability(), type);\n-        assignment.getAssignmentInformation().addReservationDetails(\n-          container.getId(), getCSLeafQueue().getQueuePath());\n-        assignment.getAssignmentInformation().incrReservations();\n-        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n-          request.getCapability());\n-        return assignment;\n+        ContainerAllocation result \u003d\n+            new ContainerAllocation(null, request.getCapability(),\n+                AllocationState.RESERVED);\n+        result.containerNodeType \u003d type;\n+        return result;\n       }\n-      return new CSAssignment(Resources.none(), type);\n+      return ContainerAllocation.QUEUE_SKIPPED;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    lastResourceRequest \u003d request;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      return new ContainerAllocation(rmContainer, null,\n          AllocationState.QUEUE_SKIPPED);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return ContainerAllocation.QUEUE_SKIPPED;\n    }\n\n    assert Resources.greaterThan(\n        rc, clusterResource, available, Resources.none());\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node, priority,\n                  resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            return ContainerAllocation.QUEUE_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d\n          new ContainerAllocation(unreservedContainer, request.getCapability(),\n              AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n          // one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            return ContainerAllocation.QUEUE_SKIPPED;\n          }\n        }\n\n        ContainerAllocation result \u003d\n            new ContainerAllocation(null, request.getCapability(),\n                AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        return result;\n      }\n      return ContainerAllocation.QUEUE_SKIPPED;\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3983. Refactored CapacityScheduleri#FiCaSchedulerApp to easier extend container allocation logic. Contributed by Wangda Tan\n",
          "commitDate": "05/08/15 1:47 PM",
          "commitName": "ba2313d6145a1234777938a747187373f4cd58d9",
          "commitAuthor": "Jian He",
          "commitDateOld": "05/08/15 12:57 PM",
          "commitNameOld": "f271d377357ad680924d19f07e6c8315e7c89bae",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,175 +1,125 @@\n-  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n-      Priority priority,\n-      ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer, SchedulingMode schedulingMode,\n+  private ContainerAllocation assignContainer(Resource clusterResource,\n+      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n+      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n       ResourceLimits currentResoureLimits) {\n+    lastResourceRequest \u003d request;\n+    \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + getApplicationId()\n+        + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n         node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n-      if (rmContainer !\u003d null) {\n-        unreserve(priority, node, rmContainer);\n-      }\n-      return new CSAssignment(Resources.none(), type);\n+      return new ContainerAllocation(rmContainer, null,\n+          AllocationState.QUEUE_SKIPPED);\n     }\n \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n-      return new CSAssignment(Resources.none(), type);\n+      return ContainerAllocation.QUEUE_SKIPPED;\n     }\n \n     assert Resources.greaterThan(\n         rc, clusterResource, available, Resources.none());\n \n-    // Create the container if necessary\n-    Container container \u003d\n-        getContainer(rmContainer, node, capability, priority);\n-\n-    // something went wrong getting/creating the container\n-    if (container \u003d\u003d null) {\n-      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n-      return new CSAssignment(Resources.none(), type);\n-    }\n-\n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d\n         rc.computeAvailableContainers(available, capability);\n \n     // How much need to unreserve equals to:\n     // max(required - headroom, amountNeedUnreserve)\n     Resource resourceNeedToUnReserve \u003d\n         Resources.max(rc, clusterResource,\n             Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n             currentResoureLimits.getAmountNeededUnreserve());\n \n     boolean needToUnreserve \u003d\n         Resources.greaterThan(rc, clusterResource,\n             resourceNeedToUnReserve, Resources.none());\n \n     RMContainer unreservedContainer \u003d null;\n     boolean reservationsContinueLooking \u003d\n-        getCSLeafQueue().getReservationContinueLooking();\n+        application.getCSLeafQueue().getReservationContinueLooking();\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n-\n-      // Did we previously reserve containers at this \u0027priority\u0027?\n-      if (rmContainer !\u003d null) {\n-        unreserve(priority, node, rmContainer);\n-      } else if (reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n+      // We will only do continuous reservation when this is not allocated from\n+      // reserved container\n+      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n+          \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n-        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n+        // some containers to meet this queue, its parents\u0027, or the users\u0027\n+        // resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n           if (!needToUnreserve) {\n             // If we shouldn\u0027t allocate/reserve new container then we should\n             // unreserve one the same size we are asking for since the\n             // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n             // the limit was hit then use the amount we need to unreserve to be\n             // under the limit.\n             resourceNeedToUnReserve \u003d capability;\n           }\n           unreservedContainer \u003d\n-              findNodeToUnreserve(clusterResource, node, priority,\n+              application.findNodeToUnreserve(clusterResource, node, priority,\n                   resourceNeedToUnReserve);\n-          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved\n+          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n+          // new/reserved\n           // container (That means we *have to* unreserve some resource to\n-          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n+          // continue)). If we failed to unreserve some resource, we can\u0027t\n+          // continue.\n           if (null \u003d\u003d unreservedContainer) {\n-            return new CSAssignment(Resources.none(), type);\n+            return ContainerAllocation.QUEUE_SKIPPED;\n           }\n         }\n       }\n \n-      // Inform the application\n-      RMContainer allocatedContainer \u003d\n-          allocate(type, node, priority, request, container);\n-\n-      // Does the application need this resource?\n-      if (allocatedContainer \u003d\u003d null) {\n-        CSAssignment csAssignment \u003d  new CSAssignment(Resources.none(), type);\n-        csAssignment.setApplication(this);\n-        csAssignment.setExcessReservation(unreservedContainer);\n-        return csAssignment;\n-      }\n-\n-      // Inform the node\n-      node.allocateContainer(allocatedContainer);\n-\n-      // Inform the ordering policy\n-      getCSLeafQueue().getOrderingPolicy().containerAllocated(this,\n-          allocatedContainer);\n-\n-      LOG.info(\"assignedContainer\" +\n-          \" application attempt\u003d\" + getApplicationAttemptId() +\n-          \" container\u003d\" + container +\n-          \" queue\u003d\" + this +\n-          \" clusterResource\u003d\" + clusterResource);\n-      createdContainer.setValue(allocatedContainer);\n-      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n-      assignment.getAssignmentInformation().addAllocationDetails(\n-        container.getId(), getCSLeafQueue().getQueuePath());\n-      assignment.getAssignmentInformation().incrAllocations();\n-      assignment.setApplication(this);\n-      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n-        container.getResource());\n-\n-      assignment.setExcessReservation(unreservedContainer);\n-      return assignment;\n+      ContainerAllocation result \u003d\n+          new ContainerAllocation(unreservedContainer, request.getCapability(),\n+              AllocationState.ALLOCATED);\n+      result.containerNodeType \u003d type;\n+      return result;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n           // one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n-            return new CSAssignment(Resources.none(), type);\n+            return ContainerAllocation.QUEUE_SKIPPED;\n           }\n         }\n \n-        // Reserve by \u0027charging\u0027 in advance...\n-        reserve(priority, node, rmContainer, container);\n-\n-        LOG.info(\"Reserved container \" +\n-            \" application\u003d\" + getApplicationId() +\n-            \" resource\u003d\" + request.getCapability() +\n-            \" queue\u003d\" + this.toString() +\n-            \" cluster\u003d\" + clusterResource);\n-        CSAssignment assignment \u003d\n-            new CSAssignment(request.getCapability(), type);\n-        assignment.getAssignmentInformation().addReservationDetails(\n-          container.getId(), getCSLeafQueue().getQueuePath());\n-        assignment.getAssignmentInformation().incrReservations();\n-        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n-          request.getCapability());\n-        return assignment;\n+        ContainerAllocation result \u003d\n+            new ContainerAllocation(null, request.getCapability(),\n+                AllocationState.RESERVED);\n+        result.containerNodeType \u003d type;\n+        return result;\n       }\n-      return new CSAssignment(Resources.none(), type);\n+      return ContainerAllocation.QUEUE_SKIPPED;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private ContainerAllocation assignContainer(Resource clusterResource,\n      FiCaSchedulerNode node, Priority priority, ResourceRequest request,\n      NodeType type, RMContainer rmContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    lastResourceRequest \u003d request;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      return new ContainerAllocation(rmContainer, null,\n          AllocationState.QUEUE_SKIPPED);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return ContainerAllocation.QUEUE_SKIPPED;\n    }\n\n    assert Resources.greaterThan(\n        rc, clusterResource, available, Resources.none());\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        application.getCSLeafQueue().getReservationContinueLooking();\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n      // We will only do continuous reservation when this is not allocated from\n      // reserved container\n      if (rmContainer \u003d\u003d null \u0026\u0026 reservationsContinueLooking\n          \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027\n        // resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              application.findNodeToUnreserve(clusterResource, node, priority,\n                  resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate\n          // new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t\n          // continue.\n          if (null \u003d\u003d unreservedContainer) {\n            return ContainerAllocation.QUEUE_SKIPPED;\n          }\n        }\n      }\n\n      ContainerAllocation result \u003d\n          new ContainerAllocation(unreservedContainer, request.getCapability(),\n              AllocationState.ALLOCATED);\n      result.containerNodeType \u003d type;\n      return result;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n          // one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            return ContainerAllocation.QUEUE_SKIPPED;\n          }\n        }\n\n        ContainerAllocation result \u003d\n            new ContainerAllocation(null, request.getCapability(),\n                AllocationState.RESERVED);\n        result.containerNodeType \u003d type;\n        return result;\n      }\n      return ContainerAllocation.QUEUE_SKIPPED;\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/RegularContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, createdContainer-MutableObject, schedulingMode-SchedulingMode, currentResoureLimits-ResourceLimits]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, schedulingMode-SchedulingMode, currentResoureLimits-ResourceLimits]"
          }
        }
      ]
    },
    "83fe34ac0896cee0918bbfad7bd51231e4aec39b": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange,Yparameterchange)",
      "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
      "commitDate": "24/07/15 2:00 PM",
      "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
          "commitDate": "24/07/15 2:00 PM",
          "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
          "commitAuthor": "Jian He",
          "commitDateOld": "24/07/15 1:38 PM",
          "commitNameOld": "fc42fa8ae3bc9d6d055090a7bb5e6f0c5972fcff",
          "commitAuthorOld": "carlo curino",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,159 +1,175 @@\n   private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n-      FiCaSchedulerApp application, Priority priority, \n+      Priority priority,\n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       MutableObject createdContainer, SchedulingMode schedulingMode,\n       ResourceLimits currentResoureLimits) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + application.getApplicationId()\n+        + \" application\u003d\" + getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n-    \n+\n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n         node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n+        unreserve(priority, node, rmContainer);\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n-    \n+\n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n-    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n+    if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n-        resourceCalculator, clusterResource, available, Resources.none());\n+        rc, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n-    Container container \u003d \n-        getContainer(rmContainer, application, node, capability, priority);\n-  \n-    // something went wrong getting/creating the container \n+    Container container \u003d\n+        getContainer(rmContainer, node, capability, priority);\n+\n+    // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n-        application, priority, capability);\n+        priority, capability);\n \n     // Can we allocate a container on this node?\n-    int availableContainers \u003d \n-        resourceCalculator.computeAvailableContainers(available, capability);\n+    int availableContainers \u003d\n+        rc.computeAvailableContainers(available, capability);\n \n-    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n-        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n+    // How much need to unreserve equals to:\n+    // max(required - headroom, amountNeedUnreserve)\n+    Resource resourceNeedToUnReserve \u003d\n+        Resources.max(rc, clusterResource,\n+            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n+            currentResoureLimits.getAmountNeededUnreserve());\n+\n+    boolean needToUnreserve \u003d\n+        Resources.greaterThan(rc, clusterResource,\n+            resourceNeedToUnReserve, Resources.none());\n+\n+    RMContainer unreservedContainer \u003d null;\n+    boolean reservationsContinueLooking \u003d\n+        getCSLeafQueue().getReservationContinueLooking();\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n-      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n+        unreserve(priority, node, rmContainer);\n+      } else if (reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n-          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n-          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n-          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n-          // under the limit.\n-          Resource amountToUnreserve \u003d capability;\n-          if (needToUnreserve) {\n-            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n+          if (!needToUnreserve) {\n+            // If we shouldn\u0027t allocate/reserve new container then we should\n+            // unreserve one the same size we are asking for since the\n+            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n+            // the limit was hit then use the amount we need to unreserve to be\n+            // under the limit.\n+            resourceNeedToUnReserve \u003d capability;\n           }\n-          boolean containerUnreserved \u003d\n-              findNodeToUnreserve(clusterResource, node, application, priority,\n-                  amountToUnreserve);\n-          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n+          unreservedContainer \u003d\n+              findNodeToUnreserve(clusterResource, node, priority,\n+                  resourceNeedToUnReserve);\n+          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n-          if (!containerUnreserved) {\n+          if (null \u003d\u003d unreservedContainer) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n-      RMContainer allocatedContainer \u003d \n-          application.allocate(type, node, priority, request, container);\n+      RMContainer allocatedContainer \u003d\n+          allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n-        return new CSAssignment(Resources.none(), type);\n+        CSAssignment csAssignment \u003d  new CSAssignment(Resources.none(), type);\n+        csAssignment.setApplication(this);\n+        csAssignment.setExcessReservation(unreservedContainer);\n+        return csAssignment;\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n-            \n+\n       // Inform the ordering policy\n-      orderingPolicy.containerAllocated(application, allocatedContainer);\n+      getCSLeafQueue().getOrderingPolicy().containerAllocated(this,\n+          allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n-          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n-          \" container\u003d\" + container + \n-          \" queue\u003d\" + this + \n+          \" application attempt\u003d\" + getApplicationAttemptId() +\n+          \" container\u003d\" + container +\n+          \" queue\u003d\" + this +\n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n       assignment.getAssignmentInformation().addAllocationDetails(\n-        container.getId(), getQueuePath());\n+        container.getId(), getCSLeafQueue().getQueuePath());\n       assignment.getAssignmentInformation().incrAllocations();\n+      assignment.setApplication(this);\n       Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n         container.getResource());\n+\n+      assignment.setExcessReservation(unreservedContainer);\n       return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n           // one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n-        reserve(application, priority, node, rmContainer, container);\n+        reserve(priority, node, rmContainer, container);\n \n-        LOG.info(\"Reserved container \" + \n-            \" application\u003d\" + application.getApplicationId() + \n-            \" resource\u003d\" + request.getCapability() + \n-            \" queue\u003d\" + this.toString() + \n-            \" usedCapacity\u003d\" + getUsedCapacity() + \n-            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n-            \" used\u003d\" + queueUsage.getUsed() +\n+        LOG.info(\"Reserved container \" +\n+            \" application\u003d\" + getApplicationId() +\n+            \" resource\u003d\" + request.getCapability() +\n+            \" queue\u003d\" + this.toString() +\n             \" cluster\u003d\" + clusterResource);\n         CSAssignment assignment \u003d\n             new CSAssignment(request.getCapability(), type);\n         assignment.getAssignmentInformation().addReservationDetails(\n-          container.getId(), getQueuePath());\n+          container.getId(), getCSLeafQueue().getQueuePath());\n         assignment.getAssignmentInformation().incrReservations();\n         Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n           request.getCapability());\n         return assignment;\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      Priority priority,\n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        rc, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(rmContainer, node, capability, priority);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        getCSLeafQueue().getReservationContinueLooking();\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(priority, node, rmContainer);\n      } else if (reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              findNodeToUnreserve(clusterResource, node, priority,\n                  resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n          if (null \u003d\u003d unreservedContainer) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d\n          allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        CSAssignment csAssignment \u003d  new CSAssignment(Resources.none(), type);\n        csAssignment.setApplication(this);\n        csAssignment.setExcessReservation(unreservedContainer);\n        return csAssignment;\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      // Inform the ordering policy\n      getCSLeafQueue().getOrderingPolicy().containerAllocated(this,\n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + getApplicationAttemptId() +\n          \" container\u003d\" + container +\n          \" queue\u003d\" + this +\n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getCSLeafQueue().getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      assignment.setApplication(this);\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n\n      assignment.setExcessReservation(unreservedContainer);\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n          // one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" +\n            \" application\u003d\" + getApplicationId() +\n            \" resource\u003d\" + request.getCapability() +\n            \" queue\u003d\" + this.toString() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getCSLeafQueue().getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
            "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
            "oldMethodName": "assignContainer",
            "newMethodName": "assignContainer"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
          "commitDate": "24/07/15 2:00 PM",
          "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
          "commitAuthor": "Jian He",
          "commitDateOld": "24/07/15 1:38 PM",
          "commitNameOld": "fc42fa8ae3bc9d6d055090a7bb5e6f0c5972fcff",
          "commitAuthorOld": "carlo curino",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,159 +1,175 @@\n   private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n-      FiCaSchedulerApp application, Priority priority, \n+      Priority priority,\n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       MutableObject createdContainer, SchedulingMode schedulingMode,\n       ResourceLimits currentResoureLimits) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + application.getApplicationId()\n+        + \" application\u003d\" + getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n-    \n+\n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n         node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n+        unreserve(priority, node, rmContainer);\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n-    \n+\n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n-    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n+    if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n-        resourceCalculator, clusterResource, available, Resources.none());\n+        rc, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n-    Container container \u003d \n-        getContainer(rmContainer, application, node, capability, priority);\n-  \n-    // something went wrong getting/creating the container \n+    Container container \u003d\n+        getContainer(rmContainer, node, capability, priority);\n+\n+    // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n-        application, priority, capability);\n+        priority, capability);\n \n     // Can we allocate a container on this node?\n-    int availableContainers \u003d \n-        resourceCalculator.computeAvailableContainers(available, capability);\n+    int availableContainers \u003d\n+        rc.computeAvailableContainers(available, capability);\n \n-    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n-        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n+    // How much need to unreserve equals to:\n+    // max(required - headroom, amountNeedUnreserve)\n+    Resource resourceNeedToUnReserve \u003d\n+        Resources.max(rc, clusterResource,\n+            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n+            currentResoureLimits.getAmountNeededUnreserve());\n+\n+    boolean needToUnreserve \u003d\n+        Resources.greaterThan(rc, clusterResource,\n+            resourceNeedToUnReserve, Resources.none());\n+\n+    RMContainer unreservedContainer \u003d null;\n+    boolean reservationsContinueLooking \u003d\n+        getCSLeafQueue().getReservationContinueLooking();\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n-      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n+        unreserve(priority, node, rmContainer);\n+      } else if (reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n-          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n-          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n-          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n-          // under the limit.\n-          Resource amountToUnreserve \u003d capability;\n-          if (needToUnreserve) {\n-            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n+          if (!needToUnreserve) {\n+            // If we shouldn\u0027t allocate/reserve new container then we should\n+            // unreserve one the same size we are asking for since the\n+            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n+            // the limit was hit then use the amount we need to unreserve to be\n+            // under the limit.\n+            resourceNeedToUnReserve \u003d capability;\n           }\n-          boolean containerUnreserved \u003d\n-              findNodeToUnreserve(clusterResource, node, application, priority,\n-                  amountToUnreserve);\n-          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n+          unreservedContainer \u003d\n+              findNodeToUnreserve(clusterResource, node, priority,\n+                  resourceNeedToUnReserve);\n+          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n-          if (!containerUnreserved) {\n+          if (null \u003d\u003d unreservedContainer) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n-      RMContainer allocatedContainer \u003d \n-          application.allocate(type, node, priority, request, container);\n+      RMContainer allocatedContainer \u003d\n+          allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n-        return new CSAssignment(Resources.none(), type);\n+        CSAssignment csAssignment \u003d  new CSAssignment(Resources.none(), type);\n+        csAssignment.setApplication(this);\n+        csAssignment.setExcessReservation(unreservedContainer);\n+        return csAssignment;\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n-            \n+\n       // Inform the ordering policy\n-      orderingPolicy.containerAllocated(application, allocatedContainer);\n+      getCSLeafQueue().getOrderingPolicy().containerAllocated(this,\n+          allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n-          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n-          \" container\u003d\" + container + \n-          \" queue\u003d\" + this + \n+          \" application attempt\u003d\" + getApplicationAttemptId() +\n+          \" container\u003d\" + container +\n+          \" queue\u003d\" + this +\n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n       assignment.getAssignmentInformation().addAllocationDetails(\n-        container.getId(), getQueuePath());\n+        container.getId(), getCSLeafQueue().getQueuePath());\n       assignment.getAssignmentInformation().incrAllocations();\n+      assignment.setApplication(this);\n       Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n         container.getResource());\n+\n+      assignment.setExcessReservation(unreservedContainer);\n       return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n           // one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n-        reserve(application, priority, node, rmContainer, container);\n+        reserve(priority, node, rmContainer, container);\n \n-        LOG.info(\"Reserved container \" + \n-            \" application\u003d\" + application.getApplicationId() + \n-            \" resource\u003d\" + request.getCapability() + \n-            \" queue\u003d\" + this.toString() + \n-            \" usedCapacity\u003d\" + getUsedCapacity() + \n-            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n-            \" used\u003d\" + queueUsage.getUsed() +\n+        LOG.info(\"Reserved container \" +\n+            \" application\u003d\" + getApplicationId() +\n+            \" resource\u003d\" + request.getCapability() +\n+            \" queue\u003d\" + this.toString() +\n             \" cluster\u003d\" + clusterResource);\n         CSAssignment assignment \u003d\n             new CSAssignment(request.getCapability(), type);\n         assignment.getAssignmentInformation().addReservationDetails(\n-          container.getId(), getQueuePath());\n+          container.getId(), getCSLeafQueue().getQueuePath());\n         assignment.getAssignmentInformation().incrReservations();\n         Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n           request.getCapability());\n         return assignment;\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      Priority priority,\n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        rc, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(rmContainer, node, capability, priority);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        getCSLeafQueue().getReservationContinueLooking();\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(priority, node, rmContainer);\n      } else if (reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              findNodeToUnreserve(clusterResource, node, priority,\n                  resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n          if (null \u003d\u003d unreservedContainer) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d\n          allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        CSAssignment csAssignment \u003d  new CSAssignment(Resources.none(), type);\n        csAssignment.setApplication(this);\n        csAssignment.setExcessReservation(unreservedContainer);\n        return csAssignment;\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      // Inform the ordering policy\n      getCSLeafQueue().getOrderingPolicy().containerAllocated(this,\n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + getApplicationAttemptId() +\n          \" container\u003d\" + container +\n          \" queue\u003d\" + this +\n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getCSLeafQueue().getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      assignment.setApplication(this);\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n\n      assignment.setExcessReservation(unreservedContainer);\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n          // one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" +\n            \" application\u003d\" + getApplicationId() +\n            \" resource\u003d\" + request.getCapability() +\n            \" queue\u003d\" + this.toString() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getCSLeafQueue().getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3026. Move application-specific container allocation logic from LeafQueue to FiCaSchedulerApp. Contributed by Wangda Tan\n",
          "commitDate": "24/07/15 2:00 PM",
          "commitName": "83fe34ac0896cee0918bbfad7bd51231e4aec39b",
          "commitAuthor": "Jian He",
          "commitDateOld": "24/07/15 1:38 PM",
          "commitNameOld": "fc42fa8ae3bc9d6d055090a7bb5e6f0c5972fcff",
          "commitAuthorOld": "carlo curino",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,159 +1,175 @@\n   private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n-      FiCaSchedulerApp application, Priority priority, \n+      Priority priority,\n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       MutableObject createdContainer, SchedulingMode schedulingMode,\n       ResourceLimits currentResoureLimits) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + application.getApplicationId()\n+        + \" application\u003d\" + getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n-    \n+\n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n         node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n+        unreserve(priority, node, rmContainer);\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n-    \n+\n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n-    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n+    if (!Resources.lessThanOrEqual(rc, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n-        resourceCalculator, clusterResource, available, Resources.none());\n+        rc, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n-    Container container \u003d \n-        getContainer(rmContainer, application, node, capability, priority);\n-  \n-    // something went wrong getting/creating the container \n+    Container container \u003d\n+        getContainer(rmContainer, node, capability, priority);\n+\n+    // something went wrong getting/creating the container\n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n-        application, priority, capability);\n+        priority, capability);\n \n     // Can we allocate a container on this node?\n-    int availableContainers \u003d \n-        resourceCalculator.computeAvailableContainers(available, capability);\n+    int availableContainers \u003d\n+        rc.computeAvailableContainers(available, capability);\n \n-    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n-        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n+    // How much need to unreserve equals to:\n+    // max(required - headroom, amountNeedUnreserve)\n+    Resource resourceNeedToUnReserve \u003d\n+        Resources.max(rc, clusterResource,\n+            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n+            currentResoureLimits.getAmountNeededUnreserve());\n+\n+    boolean needToUnreserve \u003d\n+        Resources.greaterThan(rc, clusterResource,\n+            resourceNeedToUnReserve, Resources.none());\n+\n+    RMContainer unreservedContainer \u003d null;\n+    boolean reservationsContinueLooking \u003d\n+        getCSLeafQueue().getReservationContinueLooking();\n \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n-        unreserve(application, priority, node, rmContainer);\n-      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n+        unreserve(priority, node, rmContainer);\n+      } else if (reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n-          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n-          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n-          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n-          // under the limit.\n-          Resource amountToUnreserve \u003d capability;\n-          if (needToUnreserve) {\n-            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n+          if (!needToUnreserve) {\n+            // If we shouldn\u0027t allocate/reserve new container then we should\n+            // unreserve one the same size we are asking for since the\n+            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n+            // the limit was hit then use the amount we need to unreserve to be\n+            // under the limit.\n+            resourceNeedToUnReserve \u003d capability;\n           }\n-          boolean containerUnreserved \u003d\n-              findNodeToUnreserve(clusterResource, node, application, priority,\n-                  amountToUnreserve);\n-          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n+          unreservedContainer \u003d\n+              findNodeToUnreserve(clusterResource, node, priority,\n+                  resourceNeedToUnReserve);\n+          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved\n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n-          if (!containerUnreserved) {\n+          if (null \u003d\u003d unreservedContainer) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n-      RMContainer allocatedContainer \u003d \n-          application.allocate(type, node, priority, request, container);\n+      RMContainer allocatedContainer \u003d\n+          allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n-        return new CSAssignment(Resources.none(), type);\n+        CSAssignment csAssignment \u003d  new CSAssignment(Resources.none(), type);\n+        csAssignment.setApplication(this);\n+        csAssignment.setExcessReservation(unreservedContainer);\n+        return csAssignment;\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n-            \n+\n       // Inform the ordering policy\n-      orderingPolicy.containerAllocated(application, allocatedContainer);\n+      getCSLeafQueue().getOrderingPolicy().containerAllocated(this,\n+          allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n-          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n-          \" container\u003d\" + container + \n-          \" queue\u003d\" + this + \n+          \" application attempt\u003d\" + getApplicationAttemptId() +\n+          \" container\u003d\" + container +\n+          \" queue\u003d\" + this +\n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n       assignment.getAssignmentInformation().addAllocationDetails(\n-        container.getId(), getQueuePath());\n+        container.getId(), getCSLeafQueue().getQueuePath());\n       assignment.getAssignmentInformation().incrAllocations();\n+      assignment.setApplication(this);\n       Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n         container.getResource());\n+\n+      assignment.setExcessReservation(unreservedContainer);\n       return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring queue capacity or user limits when\n           // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n           // one.\n           if (needToUnreserve) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"we needed to unreserve to be able to allocate\");\n             }\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n-        reserve(application, priority, node, rmContainer, container);\n+        reserve(priority, node, rmContainer, container);\n \n-        LOG.info(\"Reserved container \" + \n-            \" application\u003d\" + application.getApplicationId() + \n-            \" resource\u003d\" + request.getCapability() + \n-            \" queue\u003d\" + this.toString() + \n-            \" usedCapacity\u003d\" + getUsedCapacity() + \n-            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n-            \" used\u003d\" + queueUsage.getUsed() +\n+        LOG.info(\"Reserved container \" +\n+            \" application\u003d\" + getApplicationId() +\n+            \" resource\u003d\" + request.getCapability() +\n+            \" queue\u003d\" + this.toString() +\n             \" cluster\u003d\" + clusterResource);\n         CSAssignment assignment \u003d\n             new CSAssignment(request.getCapability(), type);\n         assignment.getAssignmentInformation().addReservationDetails(\n-          container.getId(), getQueuePath());\n+          container.getId(), getCSLeafQueue().getQueuePath());\n         assignment.getAssignmentInformation().incrReservations();\n         Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n           request.getCapability());\n         return assignment;\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      Priority priority,\n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n\n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(rc, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        rc, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d\n        getContainer(rmContainer, node, capability, priority);\n\n    // something went wrong getting/creating the container\n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d\n        rc.computeAvailableContainers(available, capability);\n\n    // How much need to unreserve equals to:\n    // max(required - headroom, amountNeedUnreserve)\n    Resource resourceNeedToUnReserve \u003d\n        Resources.max(rc, clusterResource,\n            Resources.subtract(capability, currentResoureLimits.getHeadroom()),\n            currentResoureLimits.getAmountNeededUnreserve());\n\n    boolean needToUnreserve \u003d\n        Resources.greaterThan(rc, clusterResource,\n            resourceNeedToUnReserve, Resources.none());\n\n    RMContainer unreservedContainer \u003d null;\n    boolean reservationsContinueLooking \u003d\n        getCSLeafQueue().getReservationContinueLooking();\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(priority, node, rmContainer);\n      } else if (reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          if (!needToUnreserve) {\n            // If we shouldn\u0027t allocate/reserve new container then we should\n            // unreserve one the same size we are asking for since the\n            // currentResoureLimits.getAmountNeededUnreserve could be zero. If\n            // the limit was hit then use the amount we need to unreserve to be\n            // under the limit.\n            resourceNeedToUnReserve \u003d capability;\n          }\n          unreservedContainer \u003d\n              findNodeToUnreserve(clusterResource, node, priority,\n                  resourceNeedToUnReserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved\n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n          if (null \u003d\u003d unreservedContainer) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d\n          allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        CSAssignment csAssignment \u003d  new CSAssignment(Resources.none(), type);\n        csAssignment.setApplication(this);\n        csAssignment.setExcessReservation(unreservedContainer);\n        return csAssignment;\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      // Inform the ordering policy\n      getCSLeafQueue().getOrderingPolicy().containerAllocated(this,\n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + getApplicationAttemptId() +\n          \" container\u003d\" + container +\n          \" queue\u003d\" + this +\n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getCSLeafQueue().getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      assignment.setApplication(this);\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n\n      assignment.setExcessReservation(unreservedContainer);\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n          // one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" +\n            \" application\u003d\" + getApplicationId() +\n            \" resource\u003d\" + request.getCapability() +\n            \" queue\u003d\" + this.toString() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getCSLeafQueue().getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, createdContainer-MutableObject, schedulingMode-SchedulingMode, currentResoureLimits-ResourceLimits]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, createdContainer-MutableObject, schedulingMode-SchedulingMode, currentResoureLimits-ResourceLimits]"
          }
        }
      ]
    },
    "189a63a719c63b67a1783a280bfc2f72dcb55277": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-3434. Interaction between reservations and userlimit can result in significant ULF violation\n",
      "commitDate": "23/04/15 7:39 AM",
      "commitName": "189a63a719c63b67a1783a280bfc2f72dcb55277",
      "commitAuthor": "tgraves",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3434. Interaction between reservations and userlimit can result in significant ULF violation\n",
          "commitDate": "23/04/15 7:39 AM",
          "commitName": "189a63a719c63b67a1783a280bfc2f72dcb55277",
          "commitAuthor": "tgraves",
          "commitDateOld": "21/04/15 8:06 PM",
          "commitNameOld": "bdd90110e6904b59746812d9a093924a65e72280",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.48,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,150 +1,159 @@\n   private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer, SchedulingMode schedulingMode) {\n+      MutableObject createdContainer, SchedulingMode schedulingMode,\n+      ResourceLimits currentResoureLimits) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n         node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return new CSAssignment(Resources.none(), type);\n     }\n-    \n+\n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n+\n+    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n+        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n+\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n-        // some containers to meet this queue and its parents\u0027 resource limits\n+        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n-        Resource minimumUnreservedResource \u003d\n-            getMinimumResourceNeedUnreserved(capability);\n-        if (!shouldAllocOrReserveNewContainer\n-            || Resources.greaterThan(resourceCalculator, clusterResource,\n-                minimumUnreservedResource, Resources.none())) {\n+        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n+          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n+          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n+          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n+          // under the limit.\n+          Resource amountToUnreserve \u003d capability;\n+          if (needToUnreserve) {\n+            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n+          }\n           boolean containerUnreserved \u003d\n               findNodeToUnreserve(clusterResource, node, application, priority,\n-                  capability, minimumUnreservedResource);\n+                  amountToUnreserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n           // container (That means we *have to* unreserve some resource to\n-          // continue)). If we failed to unreserve some resource,\n+          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n           if (!containerUnreserved) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return new CSAssignment(Resources.none(), type);\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n             \n       // Inform the ordering policy\n       orderingPolicy.containerAllocated(application, allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n       assignment.getAssignmentInformation().addAllocationDetails(\n         container.getId(), getQueuePath());\n       assignment.getAssignmentInformation().incrAllocations();\n       Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n         container.getResource());\n       return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n-          // we could possibly ignoring parent queue capacity limits when\n-          // reservationsContinueLooking is set.\n-          // If we\u0027re trying to reserve a container here, not container will be\n-          // unreserved for reserving the new one. Check limits again before\n-          // reserve the new container\n-          if (!checkLimitsToReserve(clusterResource,\n-              application, capability, node.getPartition(), schedulingMode)) {\n+          // we could possibly ignoring queue capacity or user limits when\n+          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n+          // one.\n+          if (needToUnreserve) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"we needed to unreserve to be able to allocate\");\n+            }\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n         CSAssignment assignment \u003d\n             new CSAssignment(request.getCapability(), type);\n         assignment.getAssignmentInformation().addReservationDetails(\n           container.getId(), getQueuePath());\n         assignment.getAssignmentInformation().incrReservations();\n         Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n           request.getCapability());\n         return assignment;\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n\n    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n          // under the limit.\n          Resource amountToUnreserve \u003d capability;\n          if (needToUnreserve) {\n            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n          }\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  amountToUnreserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n          if (!containerUnreserved) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return new CSAssignment(Resources.none(), type);\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n            \n      // Inform the ordering policy\n      orderingPolicy.containerAllocated(application, allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n          // one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, createdContainer-MutableObject, schedulingMode-SchedulingMode]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, createdContainer-MutableObject, schedulingMode-SchedulingMode, currentResoureLimits-ResourceLimits]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3434. Interaction between reservations and userlimit can result in significant ULF violation\n",
          "commitDate": "23/04/15 7:39 AM",
          "commitName": "189a63a719c63b67a1783a280bfc2f72dcb55277",
          "commitAuthor": "tgraves",
          "commitDateOld": "21/04/15 8:06 PM",
          "commitNameOld": "bdd90110e6904b59746812d9a093924a65e72280",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.48,
          "commitsBetweenForRepo": 17,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,150 +1,159 @@\n   private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer, SchedulingMode schedulingMode) {\n+      MutableObject createdContainer, SchedulingMode schedulingMode,\n+      ResourceLimits currentResoureLimits) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n         node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return new CSAssignment(Resources.none(), type);\n     }\n-    \n+\n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n+\n+    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n+        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n+\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n-        // some containers to meet this queue and its parents\u0027 resource limits\n+        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n-        Resource minimumUnreservedResource \u003d\n-            getMinimumResourceNeedUnreserved(capability);\n-        if (!shouldAllocOrReserveNewContainer\n-            || Resources.greaterThan(resourceCalculator, clusterResource,\n-                minimumUnreservedResource, Resources.none())) {\n+        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n+          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n+          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n+          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n+          // under the limit.\n+          Resource amountToUnreserve \u003d capability;\n+          if (needToUnreserve) {\n+            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n+          }\n           boolean containerUnreserved \u003d\n               findNodeToUnreserve(clusterResource, node, application, priority,\n-                  capability, minimumUnreservedResource);\n+                  amountToUnreserve);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n           // container (That means we *have to* unreserve some resource to\n-          // continue)). If we failed to unreserve some resource,\n+          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n           if (!containerUnreserved) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return new CSAssignment(Resources.none(), type);\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n             \n       // Inform the ordering policy\n       orderingPolicy.containerAllocated(application, allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n       assignment.getAssignmentInformation().addAllocationDetails(\n         container.getId(), getQueuePath());\n       assignment.getAssignmentInformation().incrAllocations();\n       Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n         container.getResource());\n       return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n-          // we could possibly ignoring parent queue capacity limits when\n-          // reservationsContinueLooking is set.\n-          // If we\u0027re trying to reserve a container here, not container will be\n-          // unreserved for reserving the new one. Check limits again before\n-          // reserve the new container\n-          if (!checkLimitsToReserve(clusterResource,\n-              application, capability, node.getPartition(), schedulingMode)) {\n+          // we could possibly ignoring queue capacity or user limits when\n+          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n+          // one.\n+          if (needToUnreserve) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"we needed to unreserve to be able to allocate\");\n+            }\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n         CSAssignment assignment \u003d\n             new CSAssignment(request.getCapability(), type);\n         assignment.getAssignmentInformation().addReservationDetails(\n           container.getId(), getQueuePath());\n         assignment.getAssignmentInformation().incrReservations();\n         Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n           request.getCapability());\n         return assignment;\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, SchedulingMode schedulingMode,\n      ResourceLimits currentResoureLimits) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n\n    boolean needToUnreserve \u003d Resources.greaterThan(resourceCalculator,clusterResource,\n        currentResoureLimits.getAmountNeededUnreserve(), Resources.none());\n\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue, its parents\u0027, or the users\u0027 resource limits.\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        if (!shouldAllocOrReserveNewContainer || needToUnreserve) {\n          // If we shouldn\u0027t allocate/reserve new container then we should unreserve one the same\n          // size we are asking for since the currentResoureLimits.getAmountNeededUnreserve\n          // could be zero. If the limit was hit then use the amount we need to unreserve to be\n          // under the limit.\n          Resource amountToUnreserve \u003d capability;\n          if (needToUnreserve) {\n            amountToUnreserve \u003d currentResoureLimits.getAmountNeededUnreserve();\n          }\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  amountToUnreserve);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource, we can\u0027t continue.\n          if (!containerUnreserved) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return new CSAssignment(Resources.none(), type);\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n            \n      // Inform the ordering policy\n      orderingPolicy.containerAllocated(application, allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring queue capacity or user limits when\n          // reservationsContinueLooking is set. Make sure we didn\u0027t need to unreserve\n          // one.\n          if (needToUnreserve) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"we needed to unreserve to be able to allocate\");\n            }\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3463. Integrate OrderingPolicy Framework with CapacityScheduler. (Craig Welch via wangda)\n",
      "commitDate": "20/04/15 5:12 PM",
      "commitName": "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "17/04/15 1:36 PM",
      "commitNameOld": "d573f09fb93dbb711d504620af5d73840ea063a6",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 3.15,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,147 +1,150 @@\n   private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       MutableObject createdContainer, SchedulingMode schedulingMode) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n         node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue and its parents\u0027 resource limits\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         Resource minimumUnreservedResource \u003d\n             getMinimumResourceNeedUnreserved(capability);\n         if (!shouldAllocOrReserveNewContainer\n             || Resources.greaterThan(resourceCalculator, clusterResource,\n                 minimumUnreservedResource, Resources.none())) {\n           boolean containerUnreserved \u003d\n               findNodeToUnreserve(clusterResource, node, application, priority,\n                   capability, minimumUnreservedResource);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource,\n           if (!containerUnreserved) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return new CSAssignment(Resources.none(), type);\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n+            \n+      // Inform the ordering policy\n+      orderingPolicy.containerAllocated(application, allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n       assignment.getAssignmentInformation().addAllocationDetails(\n         container.getId(), getQueuePath());\n       assignment.getAssignmentInformation().incrAllocations();\n       Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n         container.getResource());\n       return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring parent queue capacity limits when\n           // reservationsContinueLooking is set.\n           // If we\u0027re trying to reserve a container here, not container will be\n           // unreserved for reserving the new one. Check limits again before\n           // reserve the new container\n           if (!checkLimitsToReserve(clusterResource,\n               application, capability, node.getPartition(), schedulingMode)) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n         CSAssignment assignment \u003d\n             new CSAssignment(request.getCapability(), type);\n         assignment.getAssignmentInformation().addReservationDetails(\n           container.getId(), getQueuePath());\n         assignment.getAssignmentInformation().incrReservations();\n         Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n           request.getCapability());\n         return assignment;\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, SchedulingMode schedulingMode) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue and its parents\u0027 resource limits\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        Resource minimumUnreservedResource \u003d\n            getMinimumResourceNeedUnreserved(capability);\n        if (!shouldAllocOrReserveNewContainer\n            || Resources.greaterThan(resourceCalculator, clusterResource,\n                minimumUnreservedResource, Resources.none())) {\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  capability, minimumUnreservedResource);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource,\n          if (!containerUnreserved) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return new CSAssignment(Resources.none(), type);\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n            \n      // Inform the ordering policy\n      orderingPolicy.containerAllocated(application, allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring parent queue capacity limits when\n          // reservationsContinueLooking is set.\n          // If we\u0027re trying to reserve a container here, not container will be\n          // unreserved for reserving the new one. Check limits again before\n          // reserve the new container\n          if (!checkLimitsToReserve(clusterResource,\n              application, capability, node.getPartition(), schedulingMode)) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "0fefda645bca935b87b6bb8ca63e6f18340d59f5": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-3361. CapacityScheduler side changes to support non-exclusive node labels. Contributed by Wangda Tan\n",
      "commitDate": "14/04/15 11:45 AM",
      "commitName": "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3361. CapacityScheduler side changes to support non-exclusive node labels. Contributed by Wangda Tan\n",
          "commitDate": "14/04/15 11:45 AM",
          "commitName": "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
          "commitAuthor": "Jian He",
          "commitDateOld": "09/04/15 11:38 PM",
          "commitNameOld": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
          "commitAuthorOld": "Xuan",
          "daysBetweenCommits": 4.51,
          "commitsBetweenForRepo": 30,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,148 +1,147 @@\n   private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer) {\n+      MutableObject createdContainer, SchedulingMode schedulingMode) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n-    if (!SchedulerUtils.checkNodeLabelExpression(\n-        node.getLabels(),\n-        request.getNodeLabelExpression())) {\n+    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n+        node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue and its parents\u0027 resource limits\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         Resource minimumUnreservedResource \u003d\n             getMinimumResourceNeedUnreserved(capability);\n         if (!shouldAllocOrReserveNewContainer\n             || Resources.greaterThan(resourceCalculator, clusterResource,\n                 minimumUnreservedResource, Resources.none())) {\n           boolean containerUnreserved \u003d\n               findNodeToUnreserve(clusterResource, node, application, priority,\n                   capability, minimumUnreservedResource);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource,\n           if (!containerUnreserved) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return new CSAssignment(Resources.none(), type);\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n       assignment.getAssignmentInformation().addAllocationDetails(\n         container.getId(), getQueuePath());\n       assignment.getAssignmentInformation().incrAllocations();\n       Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n         container.getResource());\n       return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring parent queue capacity limits when\n           // reservationsContinueLooking is set.\n           // If we\u0027re trying to reserve a container here, not container will be\n           // unreserved for reserving the new one. Check limits again before\n           // reserve the new container\n-          if (!checkLimitsToReserve(clusterResource, \n-              application, capability)) {\n+          if (!checkLimitsToReserve(clusterResource,\n+              application, capability, node.getPartition(), schedulingMode)) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n         CSAssignment assignment \u003d\n             new CSAssignment(request.getCapability(), type);\n         assignment.getAssignmentInformation().addReservationDetails(\n           container.getId(), getQueuePath());\n         assignment.getAssignmentInformation().incrReservations();\n         Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n           request.getCapability());\n         return assignment;\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, SchedulingMode schedulingMode) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue and its parents\u0027 resource limits\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        Resource minimumUnreservedResource \u003d\n            getMinimumResourceNeedUnreserved(capability);\n        if (!shouldAllocOrReserveNewContainer\n            || Resources.greaterThan(resourceCalculator, clusterResource,\n                minimumUnreservedResource, Resources.none())) {\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  capability, minimumUnreservedResource);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource,\n          if (!containerUnreserved) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return new CSAssignment(Resources.none(), type);\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring parent queue capacity limits when\n          // reservationsContinueLooking is set.\n          // If we\u0027re trying to reserve a container here, not container will be\n          // unreserved for reserving the new one. Check limits again before\n          // reserve the new container\n          if (!checkLimitsToReserve(clusterResource,\n              application, capability, node.getPartition(), schedulingMode)) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, createdContainer-MutableObject]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, createdContainer-MutableObject, schedulingMode-SchedulingMode]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3361. CapacityScheduler side changes to support non-exclusive node labels. Contributed by Wangda Tan\n",
          "commitDate": "14/04/15 11:45 AM",
          "commitName": "0fefda645bca935b87b6bb8ca63e6f18340d59f5",
          "commitAuthor": "Jian He",
          "commitDateOld": "09/04/15 11:38 PM",
          "commitNameOld": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
          "commitAuthorOld": "Xuan",
          "daysBetweenCommits": 4.51,
          "commitsBetweenForRepo": 30,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,148 +1,147 @@\n   private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      MutableObject createdContainer) {\n+      MutableObject createdContainer, SchedulingMode schedulingMode) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n-    if (!SchedulerUtils.checkNodeLabelExpression(\n-        node.getLabels(),\n-        request.getNodeLabelExpression())) {\n+    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n+        node.getPartition(), schedulingMode)) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return new CSAssignment(Resources.none(), type);\n     }\n     \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue and its parents\u0027 resource limits\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         Resource minimumUnreservedResource \u003d\n             getMinimumResourceNeedUnreserved(capability);\n         if (!shouldAllocOrReserveNewContainer\n             || Resources.greaterThan(resourceCalculator, clusterResource,\n                 minimumUnreservedResource, Resources.none())) {\n           boolean containerUnreserved \u003d\n               findNodeToUnreserve(clusterResource, node, application, priority,\n                   capability, minimumUnreservedResource);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource,\n           if (!containerUnreserved) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return new CSAssignment(Resources.none(), type);\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n       assignment.getAssignmentInformation().addAllocationDetails(\n         container.getId(), getQueuePath());\n       assignment.getAssignmentInformation().incrAllocations();\n       Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n         container.getResource());\n       return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring parent queue capacity limits when\n           // reservationsContinueLooking is set.\n           // If we\u0027re trying to reserve a container here, not container will be\n           // unreserved for reserving the new one. Check limits again before\n           // reserve the new container\n-          if (!checkLimitsToReserve(clusterResource, \n-              application, capability)) {\n+          if (!checkLimitsToReserve(clusterResource,\n+              application, capability, node.getPartition(), schedulingMode)) {\n             return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n         CSAssignment assignment \u003d\n             new CSAssignment(request.getCapability(), type);\n         assignment.getAssignmentInformation().addReservationDetails(\n           container.getId(), getQueuePath());\n         assignment.getAssignmentInformation().incrReservations();\n         Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n           request.getCapability());\n         return assignment;\n       }\n       return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer, SchedulingMode schedulingMode) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkResourceRequestMatchingNodePartition(request,\n        node.getPartition(), schedulingMode)) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue and its parents\u0027 resource limits\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        Resource minimumUnreservedResource \u003d\n            getMinimumResourceNeedUnreserved(capability);\n        if (!shouldAllocOrReserveNewContainer\n            || Resources.greaterThan(resourceCalculator, clusterResource,\n                minimumUnreservedResource, Resources.none())) {\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  capability, minimumUnreservedResource);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource,\n          if (!containerUnreserved) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return new CSAssignment(Resources.none(), type);\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring parent queue capacity limits when\n          // reservationsContinueLooking is set.\n          // If we\u0027re trying to reserve a container here, not container will be\n          // unreserved for reserving the new one. Check limits again before\n          // reserve the new container\n          if (!checkLimitsToReserve(clusterResource,\n              application, capability, node.getPartition(), schedulingMode)) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "afa5d4715a3aea2a6e93380b014c7bb8f0880383": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "YARN-3293. Track and display capacity scheduler health metrics in web\nUI. Contributed by Varun Vasudev\n",
      "commitDate": "09/04/15 11:38 PM",
      "commitName": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
      "commitAuthor": "Xuan",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "YARN-3293. Track and display capacity scheduler health metrics in web\nUI. Contributed by Varun Vasudev\n",
          "commitDate": "09/04/15 11:38 PM",
          "commitName": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
          "commitAuthor": "Xuan",
          "commitDateOld": "20/03/15 1:54 PM",
          "commitNameOld": "586348e4cbf197188057d6b843a6701cfffdaff3",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 20.41,
          "commitsBetweenForRepo": 184,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,136 +1,148 @@\n-  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n+  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       MutableObject createdContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n         node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n     \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue and its parents\u0027 resource limits\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         Resource minimumUnreservedResource \u003d\n             getMinimumResourceNeedUnreserved(capability);\n         if (!shouldAllocOrReserveNewContainer\n             || Resources.greaterThan(resourceCalculator, clusterResource,\n                 minimumUnreservedResource, Resources.none())) {\n           boolean containerUnreserved \u003d\n               findNodeToUnreserve(clusterResource, node, application, priority,\n                   capability, minimumUnreservedResource);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource,\n           if (!containerUnreserved) {\n-            return Resources.none();\n+            return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n-        return Resources.none();\n+        return new CSAssignment(Resources.none(), type);\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n-      return container.getResource();\n+      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n+      assignment.getAssignmentInformation().addAllocationDetails(\n+        container.getId(), getQueuePath());\n+      assignment.getAssignmentInformation().incrAllocations();\n+      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n+        container.getResource());\n+      return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring parent queue capacity limits when\n           // reservationsContinueLooking is set.\n           // If we\u0027re trying to reserve a container here, not container will be\n           // unreserved for reserving the new one. Check limits again before\n           // reserve the new container\n           if (!checkLimitsToReserve(clusterResource, \n               application, capability)) {\n-            return Resources.none();\n+            return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n-\n-        return request.getCapability();\n+        CSAssignment assignment \u003d\n+            new CSAssignment(request.getCapability(), type);\n+        assignment.getAssignmentInformation().addReservationDetails(\n+          container.getId(), getQueuePath());\n+        assignment.getAssignmentInformation().incrReservations();\n+        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n+          request.getCapability());\n+        return assignment;\n       }\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue and its parents\u0027 resource limits\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        Resource minimumUnreservedResource \u003d\n            getMinimumResourceNeedUnreserved(capability);\n        if (!shouldAllocOrReserveNewContainer\n            || Resources.greaterThan(resourceCalculator, clusterResource,\n                minimumUnreservedResource, Resources.none())) {\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  capability, minimumUnreservedResource);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource,\n          if (!containerUnreserved) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return new CSAssignment(Resources.none(), type);\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring parent queue capacity limits when\n          // reservationsContinueLooking is set.\n          // If we\u0027re trying to reserve a container here, not container will be\n          // unreserved for reserving the new one. Check limits again before\n          // reserve the new container\n          if (!checkLimitsToReserve(clusterResource, \n              application, capability)) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "Resource",
            "newValue": "CSAssignment"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3293. Track and display capacity scheduler health metrics in web\nUI. Contributed by Varun Vasudev\n",
          "commitDate": "09/04/15 11:38 PM",
          "commitName": "afa5d4715a3aea2a6e93380b014c7bb8f0880383",
          "commitAuthor": "Xuan",
          "commitDateOld": "20/03/15 1:54 PM",
          "commitNameOld": "586348e4cbf197188057d6b843a6701cfffdaff3",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 20.41,
          "commitsBetweenForRepo": 184,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,136 +1,148 @@\n-  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n+  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       MutableObject createdContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n         node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n         capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n     \n     boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n         application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n         // when reservationsContinueLooking is set, we may need to unreserve\n         // some containers to meet this queue and its parents\u0027 resource limits\n         // TODO, need change here when we want to support continuous reservation\n         // looking for labeled partitions.\n         Resource minimumUnreservedResource \u003d\n             getMinimumResourceNeedUnreserved(capability);\n         if (!shouldAllocOrReserveNewContainer\n             || Resources.greaterThan(resourceCalculator, clusterResource,\n                 minimumUnreservedResource, Resources.none())) {\n           boolean containerUnreserved \u003d\n               findNodeToUnreserve(clusterResource, node, application, priority,\n                   capability, minimumUnreservedResource);\n           // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n           // container (That means we *have to* unreserve some resource to\n           // continue)). If we failed to unreserve some resource,\n           if (!containerUnreserved) {\n-            return Resources.none();\n+            return new CSAssignment(Resources.none(), type);\n           }\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n-        return Resources.none();\n+        return new CSAssignment(Resources.none(), type);\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n-      return container.getResource();\n+      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n+      assignment.getAssignmentInformation().addAllocationDetails(\n+        container.getId(), getQueuePath());\n+      assignment.getAssignmentInformation().incrAllocations();\n+      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n+        container.getResource());\n+      return assignment;\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n         if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n           // we could possibly ignoring parent queue capacity limits when\n           // reservationsContinueLooking is set.\n           // If we\u0027re trying to reserve a container here, not container will be\n           // unreserved for reserving the new one. Check limits again before\n           // reserve the new container\n           if (!checkLimitsToReserve(clusterResource, \n               application, capability)) {\n-            return Resources.none();\n+            return new CSAssignment(Resources.none(), type);\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n-\n-        return request.getCapability();\n+        CSAssignment assignment \u003d\n+            new CSAssignment(request.getCapability(), type);\n+        assignment.getAssignmentInformation().addReservationDetails(\n+          container.getId(), getQueuePath());\n+        assignment.getAssignmentInformation().incrReservations();\n+        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n+          request.getCapability());\n+        return assignment;\n       }\n-      return Resources.none();\n+      return new CSAssignment(Resources.none(), type);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private CSAssignment assignContainer(Resource clusterResource, FiCaSchedulerNode node,\n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return new CSAssignment(Resources.none(), type);\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return new CSAssignment(Resources.none(), type);\n    }\n    \n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue and its parents\u0027 resource limits\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        Resource minimumUnreservedResource \u003d\n            getMinimumResourceNeedUnreserved(capability);\n        if (!shouldAllocOrReserveNewContainer\n            || Resources.greaterThan(resourceCalculator, clusterResource,\n                minimumUnreservedResource, Resources.none())) {\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  capability, minimumUnreservedResource);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource,\n          if (!containerUnreserved) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return new CSAssignment(Resources.none(), type);\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      CSAssignment assignment \u003d new CSAssignment(container.getResource(), type);\n      assignment.getAssignmentInformation().addAllocationDetails(\n        container.getId(), getQueuePath());\n      assignment.getAssignmentInformation().incrAllocations();\n      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),\n        container.getResource());\n      return assignment;\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring parent queue capacity limits when\n          // reservationsContinueLooking is set.\n          // If we\u0027re trying to reserve a container here, not container will be\n          // unreserved for reserving the new one. Check limits again before\n          // reserve the new container\n          if (!checkLimitsToReserve(clusterResource, \n              application, capability)) {\n            return new CSAssignment(Resources.none(), type);\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n        CSAssignment assignment \u003d\n            new CSAssignment(request.getCapability(), type);\n        assignment.getAssignmentInformation().addReservationDetails(\n          container.getId(), getQueuePath());\n        assignment.getAssignmentInformation().incrReservations();\n        Resources.addTo(assignment.getAssignmentInformation().getReserved(),\n          request.getCapability());\n        return assignment;\n      }\n      return new CSAssignment(Resources.none(), type);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "487374b7fe0c92fc7eb1406c568952722b5d5b15": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-3243. CapacityScheduler should pass headroom from parent to children to make sure ParentQueue obey its capacity limits. Contributed by Wangda Tan.\n",
      "commitDate": "17/03/15 10:24 AM",
      "commitName": "487374b7fe0c92fc7eb1406c568952722b5d5b15",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3243. CapacityScheduler should pass headroom from parent to children to make sure ParentQueue obey its capacity limits. Contributed by Wangda Tan.\n",
          "commitDate": "17/03/15 10:24 AM",
          "commitName": "487374b7fe0c92fc7eb1406c568952722b5d5b15",
          "commitAuthor": "Jian He",
          "commitDateOld": "03/03/15 11:49 AM",
          "commitNameOld": "e17e5ba9d7e2bd45ba6884f59f8045817594b284",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 13.9,
          "commitsBetweenForRepo": 109,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,145 +1,136 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      boolean needToUnreserve, MutableObject createdContainer) {\n+      MutableObject createdContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type\n-        + \" needToUnreserve\u003d \" + needToUnreserve);\n+        + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n         node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return Resources.none();\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n-    if (!Resources.fitsIn(capability, totalResource)) {\n+    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n+        capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n+\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n-\n-    // default to true since if reservation continue look feature isn\u0027t on\n-    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n-    boolean canAllocContainer \u003d true;\n-    if (this.reservationsContinueLooking) {\n-      // based on reservations can we allocate/reserve more or do we need\n-      // to unreserve one first\n-      canAllocContainer \u003d needContainers(application, priority, capability);\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n-      }\n-    }\n+    \n+    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n+        application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n-      } else if (this.reservationsContinueLooking\n-          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n-        // need to unreserve some other container first\n-        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n-            priority, capability);\n-        if (!res) {\n-          return Resources.none();\n-        }\n-      } else {\n-        // we got here by possibly ignoring queue capacity limits. If the\n-        // parameter needToUnreserve is true it means we ignored one of those\n-        // limits in the chance we could unreserve. If we are here we aren\u0027t\n-        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n-        // limit.\n-        if (needToUnreserve) {\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n+      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n+        // when reservationsContinueLooking is set, we may need to unreserve\n+        // some containers to meet this queue and its parents\u0027 resource limits\n+        // TODO, need change here when we want to support continuous reservation\n+        // looking for labeled partitions.\n+        Resource minimumUnreservedResource \u003d\n+            getMinimumResourceNeedUnreserved(capability);\n+        if (!shouldAllocOrReserveNewContainer\n+            || Resources.greaterThan(resourceCalculator, clusterResource,\n+                minimumUnreservedResource, Resources.none())) {\n+          boolean containerUnreserved \u003d\n+              findNodeToUnreserve(clusterResource, node, application, priority,\n+                  capability, minimumUnreservedResource);\n+          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n+          // container (That means we *have to* unreserve some resource to\n+          // continue)). If we failed to unreserve some resource,\n+          if (!containerUnreserved) {\n+            return Resources.none();\n           }\n-          return Resources.none();\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       return container.getResource();\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n-      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n+      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n-        if (reservationsContinueLooking) {\n-          // we got here by possibly ignoring parent queue capacity limits. If\n-          // the parameter needToUnreserve is true it means we ignored one of\n-          // those limits in the chance we could unreserve. If we are here\n-          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n-          // anymore due to that parent limit\n-          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n-              needToUnreserve);\n-          if (!res) {\n+        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n+          // we could possibly ignoring parent queue capacity limits when\n+          // reservationsContinueLooking is set.\n+          // If we\u0027re trying to reserve a container here, not container will be\n+          // unreserved for reserving the new one. Check limits again before\n+          // reserve the new container\n+          if (!checkLimitsToReserve(clusterResource, \n+              application, capability)) {\n             return Resources.none();\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n \n         return request.getCapability();\n       }\n       return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return Resources.none();\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n    \n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue and its parents\u0027 resource limits\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        Resource minimumUnreservedResource \u003d\n            getMinimumResourceNeedUnreserved(capability);\n        if (!shouldAllocOrReserveNewContainer\n            || Resources.greaterThan(resourceCalculator, clusterResource,\n                minimumUnreservedResource, Resources.none())) {\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  capability, minimumUnreservedResource);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource,\n          if (!containerUnreserved) {\n            return Resources.none();\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring parent queue capacity limits when\n          // reservationsContinueLooking is set.\n          // If we\u0027re trying to reserve a container here, not container will be\n          // unreserved for reserving the new one. Check limits again before\n          // reserve the new container\n          if (!checkLimitsToReserve(clusterResource, \n              application, capability)) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, needToUnreserve-boolean, createdContainer-MutableObject]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, createdContainer-MutableObject]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3243. CapacityScheduler should pass headroom from parent to children to make sure ParentQueue obey its capacity limits. Contributed by Wangda Tan.\n",
          "commitDate": "17/03/15 10:24 AM",
          "commitName": "487374b7fe0c92fc7eb1406c568952722b5d5b15",
          "commitAuthor": "Jian He",
          "commitDateOld": "03/03/15 11:49 AM",
          "commitNameOld": "e17e5ba9d7e2bd45ba6884f59f8045817594b284",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 13.9,
          "commitsBetweenForRepo": 109,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,145 +1,136 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      boolean needToUnreserve, MutableObject createdContainer) {\n+      MutableObject createdContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type\n-        + \" needToUnreserve\u003d \" + needToUnreserve);\n+        + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n         node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return Resources.none();\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n-    if (!Resources.fitsIn(capability, totalResource)) {\n+    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n+        capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n+\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n-\n-    // default to true since if reservation continue look feature isn\u0027t on\n-    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n-    boolean canAllocContainer \u003d true;\n-    if (this.reservationsContinueLooking) {\n-      // based on reservations can we allocate/reserve more or do we need\n-      // to unreserve one first\n-      canAllocContainer \u003d needContainers(application, priority, capability);\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n-      }\n-    }\n+    \n+    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n+        application, priority, capability);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n-      } else if (this.reservationsContinueLooking\n-          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n-        // need to unreserve some other container first\n-        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n-            priority, capability);\n-        if (!res) {\n-          return Resources.none();\n-        }\n-      } else {\n-        // we got here by possibly ignoring queue capacity limits. If the\n-        // parameter needToUnreserve is true it means we ignored one of those\n-        // limits in the chance we could unreserve. If we are here we aren\u0027t\n-        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n-        // limit.\n-        if (needToUnreserve) {\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n+      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n+        // when reservationsContinueLooking is set, we may need to unreserve\n+        // some containers to meet this queue and its parents\u0027 resource limits\n+        // TODO, need change here when we want to support continuous reservation\n+        // looking for labeled partitions.\n+        Resource minimumUnreservedResource \u003d\n+            getMinimumResourceNeedUnreserved(capability);\n+        if (!shouldAllocOrReserveNewContainer\n+            || Resources.greaterThan(resourceCalculator, clusterResource,\n+                minimumUnreservedResource, Resources.none())) {\n+          boolean containerUnreserved \u003d\n+              findNodeToUnreserve(clusterResource, node, application, priority,\n+                  capability, minimumUnreservedResource);\n+          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n+          // container (That means we *have to* unreserve some resource to\n+          // continue)). If we failed to unreserve some resource,\n+          if (!containerUnreserved) {\n+            return Resources.none();\n           }\n-          return Resources.none();\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n       createdContainer.setValue(allocatedContainer);\n       return container.getResource();\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n-      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n+      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n \n-        if (reservationsContinueLooking) {\n-          // we got here by possibly ignoring parent queue capacity limits. If\n-          // the parameter needToUnreserve is true it means we ignored one of\n-          // those limits in the chance we could unreserve. If we are here\n-          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n-          // anymore due to that parent limit\n-          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n-              needToUnreserve);\n-          if (!res) {\n+        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n+          // we could possibly ignoring parent queue capacity limits when\n+          // reservationsContinueLooking is set.\n+          // If we\u0027re trying to reserve a container here, not container will be\n+          // unreserved for reserving the new one. Check limits again before\n+          // reserve the new container\n+          if (!checkLimitsToReserve(clusterResource, \n+              application, capability)) {\n             return Resources.none();\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n \n         return request.getCapability();\n       }\n       return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      MutableObject createdContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return Resources.none();\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.lessThanOrEqual(resourceCalculator, clusterResource,\n        capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n    \n    boolean shouldAllocOrReserveNewContainer \u003d shouldAllocOrReserveNewContainer(\n        application, priority, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking \u0026\u0026 node.getLabels().isEmpty()) {\n        // when reservationsContinueLooking is set, we may need to unreserve\n        // some containers to meet this queue and its parents\u0027 resource limits\n        // TODO, need change here when we want to support continuous reservation\n        // looking for labeled partitions.\n        Resource minimumUnreservedResource \u003d\n            getMinimumResourceNeedUnreserved(capability);\n        if (!shouldAllocOrReserveNewContainer\n            || Resources.greaterThan(resourceCalculator, clusterResource,\n                minimumUnreservedResource, Resources.none())) {\n          boolean containerUnreserved \u003d\n              findNodeToUnreserve(clusterResource, node, application, priority,\n                  capability, minimumUnreservedResource);\n          // When (minimum-unreserved-resource \u003e 0 OR we cannot allocate new/reserved \n          // container (That means we *have to* unreserve some resource to\n          // continue)). If we failed to unreserve some resource,\n          if (!containerUnreserved) {\n            return Resources.none();\n          }\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if (shouldAllocOrReserveNewContainer || rmContainer !\u003d null) {\n\n        if (reservationsContinueLooking \u0026\u0026 rmContainer \u003d\u003d null) {\n          // we could possibly ignoring parent queue capacity limits when\n          // reservationsContinueLooking is set.\n          // If we\u0027re trying to reserve a container here, not container will be\n          // unreserved for reserving the new one. Check limits again before\n          // reserve the new container\n          if (!checkLimitsToReserve(clusterResource, \n              application, capability)) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "e17e5ba9d7e2bd45ba6884f59f8045817594b284": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-3272. Surface container locality info in RM web UI (Jian He via wangda)\n",
      "commitDate": "03/03/15 11:49 AM",
      "commitName": "e17e5ba9d7e2bd45ba6884f59f8045817594b284",
      "commitAuthor": "Wangda Tan",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-3272. Surface container locality info in RM web UI (Jian He via wangda)\n",
          "commitDate": "03/03/15 11:49 AM",
          "commitName": "e17e5ba9d7e2bd45ba6884f59f8045817594b284",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "02/03/15 5:52 PM",
          "commitNameOld": "14dd647c556016d351f425ee956ccf800ccb9ce2",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 0.75,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,145 +1,145 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      boolean needToUnreserve) {\n+      boolean needToUnreserve, MutableObject createdContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type\n         + \" needToUnreserve\u003d \" + needToUnreserve);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n         node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return Resources.none();\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // default to true since if reservation continue look feature isn\u0027t on\n     // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n     boolean canAllocContainer \u003d true;\n     if (this.reservationsContinueLooking) {\n       // based on reservations can we allocate/reserve more or do we need\n       // to unreserve one first\n       canAllocContainer \u003d needContainers(application, priority, capability);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"can alloc container is: \" + canAllocContainer);\n       }\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking\n           \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n         // need to unreserve some other container first\n         boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n             priority, capability);\n         if (!res) {\n           return Resources.none();\n         }\n       } else {\n         // we got here by possibly ignoring queue capacity limits. If the\n         // parameter needToUnreserve is true it means we ignored one of those\n         // limits in the chance we could unreserve. If we are here we aren\u0027t\n         // trying to unreserve so we can\u0027t allocate anymore due to that parent\n         // limit.\n         if (needToUnreserve) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n           }\n           return Resources.none();\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n-\n+      createdContainer.setValue(allocatedContainer);\n       return container.getResource();\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if ((canAllocContainer) || (rmContainer !\u003d null)) {\n \n         if (reservationsContinueLooking) {\n           // we got here by possibly ignoring parent queue capacity limits. If\n           // the parameter needToUnreserve is true it means we ignored one of\n           // those limits in the chance we could unreserve. If we are here\n           // we aren\u0027t trying to unreserve so we can\u0027t allocate\n           // anymore due to that parent limit\n           boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n               needToUnreserve);\n           if (!res) {\n             return Resources.none();\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n \n         return request.getCapability();\n       }\n       return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      boolean needToUnreserve, MutableObject createdContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type\n        + \" needToUnreserve\u003d \" + needToUnreserve);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return Resources.none();\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // default to true since if reservation continue look feature isn\u0027t on\n    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n    boolean canAllocContainer \u003d true;\n    if (this.reservationsContinueLooking) {\n      // based on reservations can we allocate/reserve more or do we need\n      // to unreserve one first\n      canAllocContainer \u003d needContainers(application, priority, capability);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n      }\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking\n          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n        // need to unreserve some other container first\n        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n            priority, capability);\n        if (!res) {\n          return Resources.none();\n        }\n      } else {\n        // we got here by possibly ignoring queue capacity limits. If the\n        // parameter needToUnreserve is true it means we ignored one of those\n        // limits in the chance we could unreserve. If we are here we aren\u0027t\n        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n        // limit.\n        if (needToUnreserve) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n          }\n          return Resources.none();\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n\n        if (reservationsContinueLooking) {\n          // we got here by possibly ignoring parent queue capacity limits. If\n          // the parameter needToUnreserve is true it means we ignored one of\n          // those limits in the chance we could unreserve. If we are here\n          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n          // anymore due to that parent limit\n          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n              needToUnreserve);\n          if (!res) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, needToUnreserve-boolean]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, needToUnreserve-boolean, createdContainer-MutableObject]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-3272. Surface container locality info in RM web UI (Jian He via wangda)\n",
          "commitDate": "03/03/15 11:49 AM",
          "commitName": "e17e5ba9d7e2bd45ba6884f59f8045817594b284",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "02/03/15 5:52 PM",
          "commitNameOld": "14dd647c556016d351f425ee956ccf800ccb9ce2",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 0.75,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,145 +1,145 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n-      boolean needToUnreserve) {\n+      boolean needToUnreserve, MutableObject createdContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type\n         + \" needToUnreserve\u003d \" + needToUnreserve);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n         node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return Resources.none();\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // default to true since if reservation continue look feature isn\u0027t on\n     // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n     boolean canAllocContainer \u003d true;\n     if (this.reservationsContinueLooking) {\n       // based on reservations can we allocate/reserve more or do we need\n       // to unreserve one first\n       canAllocContainer \u003d needContainers(application, priority, capability);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"can alloc container is: \" + canAllocContainer);\n       }\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking\n           \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n         // need to unreserve some other container first\n         boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n             priority, capability);\n         if (!res) {\n           return Resources.none();\n         }\n       } else {\n         // we got here by possibly ignoring queue capacity limits. If the\n         // parameter needToUnreserve is true it means we ignored one of those\n         // limits in the chance we could unreserve. If we are here we aren\u0027t\n         // trying to unreserve so we can\u0027t allocate anymore due to that parent\n         // limit.\n         if (needToUnreserve) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n           }\n           return Resources.none();\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n-\n+      createdContainer.setValue(allocatedContainer);\n       return container.getResource();\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if ((canAllocContainer) || (rmContainer !\u003d null)) {\n \n         if (reservationsContinueLooking) {\n           // we got here by possibly ignoring parent queue capacity limits. If\n           // the parameter needToUnreserve is true it means we ignored one of\n           // those limits in the chance we could unreserve. If we are here\n           // we aren\u0027t trying to unreserve so we can\u0027t allocate\n           // anymore due to that parent limit\n           boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n               needToUnreserve);\n           if (!res) {\n             return Resources.none();\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n \n         return request.getCapability();\n       }\n       return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      boolean needToUnreserve, MutableObject createdContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type\n        + \" needToUnreserve\u003d \" + needToUnreserve);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return Resources.none();\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // default to true since if reservation continue look feature isn\u0027t on\n    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n    boolean canAllocContainer \u003d true;\n    if (this.reservationsContinueLooking) {\n      // based on reservations can we allocate/reserve more or do we need\n      // to unreserve one first\n      canAllocContainer \u003d needContainers(application, priority, capability);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n      }\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking\n          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n        // need to unreserve some other container first\n        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n            priority, capability);\n        if (!res) {\n          return Resources.none();\n        }\n      } else {\n        // we got here by possibly ignoring queue capacity limits. If the\n        // parameter needToUnreserve is true it means we ignored one of those\n        // limits in the chance we could unreserve. If we are here we aren\u0027t\n        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n        // limit.\n        if (needToUnreserve) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n          }\n          return Resources.none();\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n      createdContainer.setValue(allocatedContainer);\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n\n        if (reservationsContinueLooking) {\n          // we got here by possibly ignoring parent queue capacity limits. If\n          // the parameter needToUnreserve is true it means we ignored one of\n          // those limits in the chance we could unreserve. If we are here\n          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n          // anymore due to that parent limit\n          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n              needToUnreserve);\n          if (!res) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "86358221fc85a7743052a0b4c1647353508bf308": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3099. Capacity Scheduler LeafQueue/ParentQueue should use ResourceUsage to track used-resources-by-label. Contributed by Wangda Tan\n",
      "commitDate": "30/01/15 3:15 PM",
      "commitName": "86358221fc85a7743052a0b4c1647353508bf308",
      "commitAuthor": "Jian He",
      "commitDateOld": "27/01/15 3:36 PM",
      "commitNameOld": "18741adf97f4fda5f8743318b59c440928e51297",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 2.99,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,145 +1,145 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       boolean needToUnreserve) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type\n         + \" needToUnreserve\u003d \" + needToUnreserve);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n         node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return Resources.none();\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // default to true since if reservation continue look feature isn\u0027t on\n     // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n     boolean canAllocContainer \u003d true;\n     if (this.reservationsContinueLooking) {\n       // based on reservations can we allocate/reserve more or do we need\n       // to unreserve one first\n       canAllocContainer \u003d needContainers(application, priority, capability);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"can alloc container is: \" + canAllocContainer);\n       }\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking\n           \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n         // need to unreserve some other container first\n         boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n             priority, capability);\n         if (!res) {\n           return Resources.none();\n         }\n       } else {\n         // we got here by possibly ignoring queue capacity limits. If the\n         // parameter needToUnreserve is true it means we ignored one of those\n         // limits in the chance we could unreserve. If we are here we aren\u0027t\n         // trying to unreserve so we can\u0027t allocate anymore due to that parent\n         // limit.\n         if (needToUnreserve) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n           }\n           return Resources.none();\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if ((canAllocContainer) || (rmContainer !\u003d null)) {\n \n         if (reservationsContinueLooking) {\n           // we got here by possibly ignoring parent queue capacity limits. If\n           // the parameter needToUnreserve is true it means we ignored one of\n           // those limits in the chance we could unreserve. If we are here\n           // we aren\u0027t trying to unreserve so we can\u0027t allocate\n           // anymore due to that parent limit\n           boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n               needToUnreserve);\n           if (!res) {\n             return Resources.none();\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n-            \" used\u003d\" + usedResources +\n+            \" used\u003d\" + queueUsage.getUsed() +\n             \" cluster\u003d\" + clusterResource);\n \n         return request.getCapability();\n       }\n       return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      boolean needToUnreserve) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type\n        + \" needToUnreserve\u003d \" + needToUnreserve);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return Resources.none();\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // default to true since if reservation continue look feature isn\u0027t on\n    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n    boolean canAllocContainer \u003d true;\n    if (this.reservationsContinueLooking) {\n      // based on reservations can we allocate/reserve more or do we need\n      // to unreserve one first\n      canAllocContainer \u003d needContainers(application, priority, capability);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n      }\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking\n          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n        // need to unreserve some other container first\n        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n            priority, capability);\n        if (!res) {\n          return Resources.none();\n        }\n      } else {\n        // we got here by possibly ignoring queue capacity limits. If the\n        // parameter needToUnreserve is true it means we ignored one of those\n        // limits in the chance we could unreserve. If we are here we aren\u0027t\n        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n        // limit.\n        if (needToUnreserve) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n          }\n          return Resources.none();\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n\n        if (reservationsContinueLooking) {\n          // we got here by possibly ignoring parent queue capacity limits. If\n          // the parameter needToUnreserve is true it means we ignored one of\n          // those limits in the chance we could unreserve. If we are here\n          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n          // anymore due to that parent limit\n          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n              needToUnreserve);\n          if (!res) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + queueUsage.getUsed() +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "fdf042dfffa4d2474e3cac86cfb8fe9ee4648beb": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2920. Changed CapacityScheduler to kill containers on nodes where node labels are changed. Contributed by  Wangda Tan\n",
      "commitDate": "22/12/14 4:51 PM",
      "commitName": "fdf042dfffa4d2474e3cac86cfb8fe9ee4648beb",
      "commitAuthor": "Jian He",
      "commitDateOld": "15/10/14 6:33 PM",
      "commitNameOld": "f2ea555ac6c06a3f2f6559731f48711fff05d3f1",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 67.97,
      "commitsBetweenForRepo": 558,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,145 +1,145 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       boolean needToUnreserve) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type\n         + \" needToUnreserve\u003d \" + needToUnreserve);\n     }\n     \n     // check if the resource request can access the label\n     if (!SchedulerUtils.checkNodeLabelExpression(\n-        labelManager.getLabelsOnNode(node.getNodeID()),\n+        node.getLabels(),\n         request.getNodeLabelExpression())) {\n       // this is a reserved container, but we cannot allocate it now according\n       // to label not match. This can be caused by node label changed\n       // We should un-reserve this container.\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       }\n       return Resources.none();\n     }\n     \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // default to true since if reservation continue look feature isn\u0027t on\n     // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n     boolean canAllocContainer \u003d true;\n     if (this.reservationsContinueLooking) {\n       // based on reservations can we allocate/reserve more or do we need\n       // to unreserve one first\n       canAllocContainer \u003d needContainers(application, priority, capability);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"can alloc container is: \" + canAllocContainer);\n       }\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking\n           \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n         // need to unreserve some other container first\n         boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n             priority, capability);\n         if (!res) {\n           return Resources.none();\n         }\n       } else {\n         // we got here by possibly ignoring queue capacity limits. If the\n         // parameter needToUnreserve is true it means we ignored one of those\n         // limits in the chance we could unreserve. If we are here we aren\u0027t\n         // trying to unreserve so we can\u0027t allocate anymore due to that parent\n         // limit.\n         if (needToUnreserve) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n           }\n           return Resources.none();\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if ((canAllocContainer) || (rmContainer !\u003d null)) {\n \n         if (reservationsContinueLooking) {\n           // we got here by possibly ignoring parent queue capacity limits. If\n           // the parameter needToUnreserve is true it means we ignored one of\n           // those limits in the chance we could unreserve. If we are here\n           // we aren\u0027t trying to unreserve so we can\u0027t allocate\n           // anymore due to that parent limit\n           boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n               needToUnreserve);\n           if (!res) {\n             return Resources.none();\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + usedResources +\n             \" cluster\u003d\" + clusterResource);\n \n         return request.getCapability();\n       }\n       return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      boolean needToUnreserve) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type\n        + \" needToUnreserve\u003d \" + needToUnreserve);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        node.getLabels(),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return Resources.none();\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // default to true since if reservation continue look feature isn\u0027t on\n    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n    boolean canAllocContainer \u003d true;\n    if (this.reservationsContinueLooking) {\n      // based on reservations can we allocate/reserve more or do we need\n      // to unreserve one first\n      canAllocContainer \u003d needContainers(application, priority, capability);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n      }\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking\n          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n        // need to unreserve some other container first\n        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n            priority, capability);\n        if (!res) {\n          return Resources.none();\n        }\n      } else {\n        // we got here by possibly ignoring queue capacity limits. If the\n        // parameter needToUnreserve is true it means we ignored one of those\n        // limits in the chance we could unreserve. If we are here we aren\u0027t\n        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n        // limit.\n        if (needToUnreserve) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n          }\n          return Resources.none();\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n\n        if (reservationsContinueLooking) {\n          // we got here by possibly ignoring parent queue capacity limits. If\n          // the parameter needToUnreserve is true it means we ignored one of\n          // those limits in the chance we could unreserve. If we are here\n          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n          // anymore due to that parent limit\n          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n              needToUnreserve);\n          if (!res) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + usedResources +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "f2ea555ac6c06a3f2f6559731f48711fff05d3f1": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2496. Enhanced Capacity Scheduler to have basic support for allocating resources based on node-labels. Contributed by Wangda Tan.\nYARN-2500. Ehnaced ResourceManager to support schedulers allocating resources based on node-labels. Contributed by Wangda Tan.\n",
      "commitDate": "15/10/14 6:33 PM",
      "commitName": "f2ea555ac6c06a3f2f6559731f48711fff05d3f1",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "07/10/14 1:45 PM",
      "commitNameOld": "30d56fdbb40d06c4e267d6c314c8c767a7adc6a3",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 8.2,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,131 +1,145 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer,\n       boolean needToUnreserve) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type\n         + \" needToUnreserve\u003d \" + needToUnreserve);\n     }\n+    \n+    // check if the resource request can access the label\n+    if (!SchedulerUtils.checkNodeLabelExpression(\n+        labelManager.getLabelsOnNode(node.getNodeID()),\n+        request.getNodeLabelExpression())) {\n+      // this is a reserved container, but we cannot allocate it now according\n+      // to label not match. This can be caused by node label changed\n+      // We should un-reserve this container.\n+      if (rmContainer !\u003d null) {\n+        unreserve(application, priority, node, rmContainer);\n+      }\n+      return Resources.none();\n+    }\n+    \n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // default to true since if reservation continue look feature isn\u0027t on\n     // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n     boolean canAllocContainer \u003d true;\n     if (this.reservationsContinueLooking) {\n       // based on reservations can we allocate/reserve more or do we need\n       // to unreserve one first\n       canAllocContainer \u003d needContainers(application, priority, capability);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"can alloc container is: \" + canAllocContainer);\n       }\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n       } else if (this.reservationsContinueLooking\n           \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n         // need to unreserve some other container first\n         boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n             priority, capability);\n         if (!res) {\n           return Resources.none();\n         }\n       } else {\n         // we got here by possibly ignoring queue capacity limits. If the\n         // parameter needToUnreserve is true it means we ignored one of those\n         // limits in the chance we could unreserve. If we are here we aren\u0027t\n         // trying to unreserve so we can\u0027t allocate anymore due to that parent\n         // limit.\n         if (needToUnreserve) {\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n           }\n           return Resources.none();\n         }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n       // if this was an already a reserved container, reserve it again\n       if ((canAllocContainer) || (rmContainer !\u003d null)) {\n \n         if (reservationsContinueLooking) {\n           // we got here by possibly ignoring parent queue capacity limits. If\n           // the parameter needToUnreserve is true it means we ignored one of\n           // those limits in the chance we could unreserve. If we are here\n           // we aren\u0027t trying to unreserve so we can\u0027t allocate\n           // anymore due to that parent limit\n           boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n               needToUnreserve);\n           if (!res) {\n             return Resources.none();\n           }\n         }\n \n         // Reserve by \u0027charging\u0027 in advance...\n         reserve(application, priority, node, rmContainer, container);\n \n         LOG.info(\"Reserved container \" + \n             \" application\u003d\" + application.getApplicationId() + \n             \" resource\u003d\" + request.getCapability() + \n             \" queue\u003d\" + this.toString() + \n             \" usedCapacity\u003d\" + getUsedCapacity() + \n             \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n             \" used\u003d\" + usedResources +\n             \" cluster\u003d\" + clusterResource);\n \n         return request.getCapability();\n       }\n       return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      boolean needToUnreserve) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type\n        + \" needToUnreserve\u003d \" + needToUnreserve);\n    }\n    \n    // check if the resource request can access the label\n    if (!SchedulerUtils.checkNodeLabelExpression(\n        labelManager.getLabelsOnNode(node.getNodeID()),\n        request.getNodeLabelExpression())) {\n      // this is a reserved container, but we cannot allocate it now according\n      // to label not match. This can be caused by node label changed\n      // We should un-reserve this container.\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      }\n      return Resources.none();\n    }\n    \n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // default to true since if reservation continue look feature isn\u0027t on\n    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n    boolean canAllocContainer \u003d true;\n    if (this.reservationsContinueLooking) {\n      // based on reservations can we allocate/reserve more or do we need\n      // to unreserve one first\n      canAllocContainer \u003d needContainers(application, priority, capability);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n      }\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking\n          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n        // need to unreserve some other container first\n        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n            priority, capability);\n        if (!res) {\n          return Resources.none();\n        }\n      } else {\n        // we got here by possibly ignoring queue capacity limits. If the\n        // parameter needToUnreserve is true it means we ignored one of those\n        // limits in the chance we could unreserve. If we are here we aren\u0027t\n        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n        // limit.\n        if (needToUnreserve) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n          }\n          return Resources.none();\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n\n        if (reservationsContinueLooking) {\n          // we got here by possibly ignoring parent queue capacity limits. If\n          // the parameter needToUnreserve is true it means we ignored one of\n          // those limits in the chance we could unreserve. If we are here\n          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n          // anymore due to that parent limit\n          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n              needToUnreserve);\n          if (!res) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + usedResources +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "9c22065109a77681bc2534063eabe8692fbcb3cd": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-1769. CapacityScheduler: Improve reservations. Contributed by Thomas Graves\n",
      "commitDate": "29/09/14 7:12 AM",
      "commitName": "9c22065109a77681bc2534063eabe8692fbcb3cd",
      "commitAuthor": "Jason Lowe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-1769. CapacityScheduler: Improve reservations. Contributed by Thomas Graves\n",
          "commitDate": "29/09/14 7:12 AM",
          "commitName": "9c22065109a77681bc2534063eabe8692fbcb3cd",
          "commitAuthor": "Jason Lowe",
          "commitDateOld": "14/08/14 11:00 PM",
          "commitNameOld": "7360cec692be5dcc3377ae5082fe22870caac96b",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 45.34,
          "commitsBetweenForRepo": 409,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,76 +1,131 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n-      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n+      ResourceRequest request, NodeType type, RMContainer rmContainer,\n+      boolean needToUnreserve) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type);\n+        + \" request\u003d\" + request + \" type\u003d\" + type\n+        + \" needToUnreserve\u003d \" + needToUnreserve);\n     }\n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n+    // default to true since if reservation continue look feature isn\u0027t on\n+    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n+    boolean canAllocContainer \u003d true;\n+    if (this.reservationsContinueLooking) {\n+      // based on reservations can we allocate/reserve more or do we need\n+      // to unreserve one first\n+      canAllocContainer \u003d needContainers(application, priority, capability);\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n+      }\n+    }\n+\n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n-      if (rmContainer !\u003d null){\n+      if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n+      } else if (this.reservationsContinueLooking\n+          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n+        // need to unreserve some other container first\n+        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n+            priority, capability);\n+        if (!res) {\n+          return Resources.none();\n+        }\n+      } else {\n+        // we got here by possibly ignoring queue capacity limits. If the\n+        // parameter needToUnreserve is true it means we ignored one of those\n+        // limits in the chance we could unreserve. If we are here we aren\u0027t\n+        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n+        // limit.\n+        if (needToUnreserve) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n+          }\n+          return Resources.none();\n+        }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n-      // Reserve by \u0027charging\u0027 in advance...\n-      reserve(application, priority, node, rmContainer, container);\n+      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n+      // if this was an already a reserved container, reserve it again\n+      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n \n-      LOG.info(\"Reserved container \" + \n-          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n-          \" resource\u003d\" + request.getCapability() + \n-          \" queue\u003d\" + this.toString() + \n-          \" node\u003d\" + node +\n-          \" clusterResource\u003d\" + clusterResource);\n+        if (reservationsContinueLooking) {\n+          // we got here by possibly ignoring parent queue capacity limits. If\n+          // the parameter needToUnreserve is true it means we ignored one of\n+          // those limits in the chance we could unreserve. If we are here\n+          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n+          // anymore due to that parent limit\n+          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n+              needToUnreserve);\n+          if (!res) {\n+            return Resources.none();\n+          }\n+        }\n \n-      return request.getCapability();\n+        // Reserve by \u0027charging\u0027 in advance...\n+        reserve(application, priority, node, rmContainer, container);\n+\n+        LOG.info(\"Reserved container \" + \n+            \" application\u003d\" + application.getApplicationId() + \n+            \" resource\u003d\" + request.getCapability() + \n+            \" queue\u003d\" + this.toString() + \n+            \" usedCapacity\u003d\" + getUsedCapacity() + \n+            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n+            \" used\u003d\" + usedResources +\n+            \" cluster\u003d\" + clusterResource);\n+\n+        return request.getCapability();\n+      }\n+      return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      boolean needToUnreserve) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type\n        + \" needToUnreserve\u003d \" + needToUnreserve);\n    }\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // default to true since if reservation continue look feature isn\u0027t on\n    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n    boolean canAllocContainer \u003d true;\n    if (this.reservationsContinueLooking) {\n      // based on reservations can we allocate/reserve more or do we need\n      // to unreserve one first\n      canAllocContainer \u003d needContainers(application, priority, capability);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n      }\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking\n          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n        // need to unreserve some other container first\n        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n            priority, capability);\n        if (!res) {\n          return Resources.none();\n        }\n      } else {\n        // we got here by possibly ignoring queue capacity limits. If the\n        // parameter needToUnreserve is true it means we ignored one of those\n        // limits in the chance we could unreserve. If we are here we aren\u0027t\n        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n        // limit.\n        if (needToUnreserve) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n          }\n          return Resources.none();\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n\n        if (reservationsContinueLooking) {\n          // we got here by possibly ignoring parent queue capacity limits. If\n          // the parameter needToUnreserve is true it means we ignored one of\n          // those limits in the chance we could unreserve. If we are here\n          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n          // anymore due to that parent limit\n          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n              needToUnreserve);\n          if (!res) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + usedResources +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {
            "oldValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer]",
            "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer, needToUnreserve-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-1769. CapacityScheduler: Improve reservations. Contributed by Thomas Graves\n",
          "commitDate": "29/09/14 7:12 AM",
          "commitName": "9c22065109a77681bc2534063eabe8692fbcb3cd",
          "commitAuthor": "Jason Lowe",
          "commitDateOld": "14/08/14 11:00 PM",
          "commitNameOld": "7360cec692be5dcc3377ae5082fe22870caac96b",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 45.34,
          "commitsBetweenForRepo": 409,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,76 +1,131 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n-      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n+      ResourceRequest request, NodeType type, RMContainer rmContainer,\n+      boolean needToUnreserve) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n-        + \" request\u003d\" + request + \" type\u003d\" + type);\n+        + \" request\u003d\" + request + \" type\u003d\" + type\n+        + \" needToUnreserve\u003d \" + needToUnreserve);\n     }\n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n+    // default to true since if reservation continue look feature isn\u0027t on\n+    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n+    boolean canAllocContainer \u003d true;\n+    if (this.reservationsContinueLooking) {\n+      // based on reservations can we allocate/reserve more or do we need\n+      // to unreserve one first\n+      canAllocContainer \u003d needContainers(application, priority, capability);\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n+      }\n+    }\n+\n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n-      if (rmContainer !\u003d null){\n+      if (rmContainer !\u003d null) {\n         unreserve(application, priority, node, rmContainer);\n+      } else if (this.reservationsContinueLooking\n+          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n+        // need to unreserve some other container first\n+        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n+            priority, capability);\n+        if (!res) {\n+          return Resources.none();\n+        }\n+      } else {\n+        // we got here by possibly ignoring queue capacity limits. If the\n+        // parameter needToUnreserve is true it means we ignored one of those\n+        // limits in the chance we could unreserve. If we are here we aren\u0027t\n+        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n+        // limit.\n+        if (needToUnreserve) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n+          }\n+          return Resources.none();\n+        }\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n-      // Reserve by \u0027charging\u0027 in advance...\n-      reserve(application, priority, node, rmContainer, container);\n+      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n+      // if this was an already a reserved container, reserve it again\n+      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n \n-      LOG.info(\"Reserved container \" + \n-          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n-          \" resource\u003d\" + request.getCapability() + \n-          \" queue\u003d\" + this.toString() + \n-          \" node\u003d\" + node +\n-          \" clusterResource\u003d\" + clusterResource);\n+        if (reservationsContinueLooking) {\n+          // we got here by possibly ignoring parent queue capacity limits. If\n+          // the parameter needToUnreserve is true it means we ignored one of\n+          // those limits in the chance we could unreserve. If we are here\n+          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n+          // anymore due to that parent limit\n+          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n+              needToUnreserve);\n+          if (!res) {\n+            return Resources.none();\n+          }\n+        }\n \n-      return request.getCapability();\n+        // Reserve by \u0027charging\u0027 in advance...\n+        reserve(application, priority, node, rmContainer, container);\n+\n+        LOG.info(\"Reserved container \" + \n+            \" application\u003d\" + application.getApplicationId() + \n+            \" resource\u003d\" + request.getCapability() + \n+            \" queue\u003d\" + this.toString() + \n+            \" usedCapacity\u003d\" + getUsedCapacity() + \n+            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n+            \" used\u003d\" + usedResources +\n+            \" cluster\u003d\" + clusterResource);\n+\n+        return request.getCapability();\n+      }\n+      return Resources.none();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer,\n      boolean needToUnreserve) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type\n        + \" needToUnreserve\u003d \" + needToUnreserve);\n    }\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // default to true since if reservation continue look feature isn\u0027t on\n    // needContainers is checked earlier and we wouldn\u0027t have gotten this far\n    boolean canAllocContainer \u003d true;\n    if (this.reservationsContinueLooking) {\n      // based on reservations can we allocate/reserve more or do we need\n      // to unreserve one first\n      canAllocContainer \u003d needContainers(application, priority, capability);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"can alloc container is: \" + canAllocContainer);\n      }\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null) {\n        unreserve(application, priority, node, rmContainer);\n      } else if (this.reservationsContinueLooking\n          \u0026\u0026 (!canAllocContainer || needToUnreserve)) {\n        // need to unreserve some other container first\n        boolean res \u003d findNodeToUnreserve(clusterResource, node, application,\n            priority, capability);\n        if (!res) {\n          return Resources.none();\n        }\n      } else {\n        // we got here by possibly ignoring queue capacity limits. If the\n        // parameter needToUnreserve is true it means we ignored one of those\n        // limits in the chance we could unreserve. If we are here we aren\u0027t\n        // trying to unreserve so we can\u0027t allocate anymore due to that parent\n        // limit.\n        if (needToUnreserve) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"we needed to unreserve to be able to allocate, skipping\");\n          }\n          return Resources.none();\n        }\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // if we are allowed to allocate but this node doesn\u0027t have space, reserve it or\n      // if this was an already a reserved container, reserve it again\n      if ((canAllocContainer) || (rmContainer !\u003d null)) {\n\n        if (reservationsContinueLooking) {\n          // we got here by possibly ignoring parent queue capacity limits. If\n          // the parameter needToUnreserve is true it means we ignored one of\n          // those limits in the chance we could unreserve. If we are here\n          // we aren\u0027t trying to unreserve so we can\u0027t allocate\n          // anymore due to that parent limit\n          boolean res \u003d checkLimitsToReserve(clusterResource, application, capability, \n              needToUnreserve);\n          if (!res) {\n            return Resources.none();\n          }\n        }\n\n        // Reserve by \u0027charging\u0027 in advance...\n        reserve(application, priority, node, rmContainer, container);\n\n        LOG.info(\"Reserved container \" + \n            \" application\u003d\" + application.getApplicationId() + \n            \" resource\u003d\" + request.getCapability() + \n            \" queue\u003d\" + this.toString() + \n            \" usedCapacity\u003d\" + getUsedCapacity() + \n            \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() + \n            \" used\u003d\" + usedResources +\n            \" cluster\u003d\" + clusterResource);\n\n        return request.getCapability();\n      }\n      return Resources.none();\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
          "extendedDetails": {}
        }
      ]
    },
    "424fd9494f144c035fdef8c533be51e2027ad8d9": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1368. Added core functionality of recovering container state into schedulers after ResourceManager Restart so as to preserve running work in the cluster. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601303 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/06/14 8:09 PM",
      "commitName": "424fd9494f144c035fdef8c533be51e2027ad8d9",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "21/05/14 10:32 PM",
      "commitNameOld": "82f3454f5ac1f1c457e668e2cee12b4dcc800ee1",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 17.9,
      "commitsBetweenForRepo": 83,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,76 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n-      node.allocateContainer(application.getApplicationId(), \n-          allocatedContainer);\n+      node.allocateContainer(allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n           \" queue\u003d\" + this + \n           \" clusterResource\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" node\u003d\" + node +\n           \" clusterResource\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" node\u003d\" + node +\n          \" clusterResource\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "44b6261bfacddea88a3cf02d406f970bbbb98d04": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1892. Improved some logs in the scheduler. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1587717 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/04/14 1:37 PM",
      "commitName": "44b6261bfacddea88a3cf02d406f970bbbb98d04",
      "commitAuthor": "Zhijie Shen",
      "commitDateOld": "12/03/14 7:36 AM",
      "commitNameOld": "4ce0e4bf2e91278bbc33f4a1c44c7929627b5d6e",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 34.25,
      "commitsBetweenForRepo": 252,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,77 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n-        + \" application\u003d\" + application.getApplicationId().getId()\n+        + \" application\u003d\" + application.getApplicationId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n-          \" application\u003d\" + application.getApplicationId() +\n+          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" container\u003d\" + container + \n-          \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n-          \" usedCapacity\u003d\" + getUsedCapacity() +\n-          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n-          \" used\u003d\" + usedResources + \n-          \" cluster\u003d\" + clusterResource);\n+          \" clusterResource\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n-          \" application\u003d\" + application.getApplicationId() +\n+          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n-          \" usedCapacity\u003d\" + getUsedCapacity() +\n-          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n-          \" used\u003d\" + usedResources + \n-          \" cluster\u003d\" + clusterResource);\n+          \" node\u003d\" + node +\n+          \" clusterResource\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" container\u003d\" + container + \n          \" queue\u003d\" + this + \n          \" clusterResource\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application attempt\u003d\" + application.getApplicationAttemptId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" node\u003d\" + node +\n          \" clusterResource\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "d0a5e43de73119e57d12f2ec89a9d1a192cde204": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1417. Modified RM to generate container-tokens not at creation time, but at allocation time so as to prevent RM\nfrom shelling out containers with expired tokens. Contributed by Omkar Vinit Joshi and Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1568060 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/02/14 2:02 PM",
      "commitName": "d0a5e43de73119e57d12f2ec89a9d1a192cde204",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "10/01/14 5:15 PM",
      "commitNameOld": "f677175f35f68bde9df72e648dffacbd31cfd620",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 33.87,
      "commitsBetweenForRepo": 224,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,91 +1,83 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n     Resource available \u003d node.getAvailableResource();\n     Resource totalResource \u003d node.getTotalResource();\n \n     if (!Resources.fitsIn(capability, totalResource)) {\n       LOG.warn(\"Node : \" + node.getNodeID()\n           + \" does not have sufficient resource for request : \" + request\n           + \" node total capability : \" + node.getTotalResource());\n       return Resources.none();\n     }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n-      Token containerToken \u003d\n-          createContainerToken(application, container);\n-      if (containerToken \u003d\u003d null) {\n-        // Something went wrong...\n-        return Resources.none();\n-      }\n-      container.setContainerToken(containerToken);\n-      \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "1e513bfc68c8de2976e3340cb83b6763c5d16813": {
      "type": "Ybodychange",
      "commitMessage": "YARN-957. Fixed a bug in CapacityScheduler because of which requests that need more than a node\u0027s total capability were incorrectly allocated on that node causing apps to hang. Contributed by Omkar Vinit Joshi.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1520187 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/13 6:20 PM",
      "commitName": "1e513bfc68c8de2976e3340cb83b6763c5d16813",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "26/08/13 8:39 AM",
      "commitNameOld": "942e2ebaa54306ffc5b0ffb403e552764a40d58c",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 9.4,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,91 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n-\n     Resource available \u003d node.getAvailableResource();\n+    Resource totalResource \u003d node.getTotalResource();\n \n+    if (!Resources.fitsIn(capability, totalResource)) {\n+      LOG.warn(\"Node : \" + node.getNodeID()\n+          + \" does not have sufficient resource for request : \" + request\n+          + \" node total capability : \" + node.getTotalResource());\n+      return Resources.none();\n+    }\n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       Token containerToken \u003d\n           createContainerToken(application, container);\n       if (containerToken \u003d\u003d null) {\n         // Something went wrong...\n         return Resources.none();\n       }\n       container.setContainerToken(containerToken);\n       \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n    Resource available \u003d node.getAvailableResource();\n    Resource totalResource \u003d node.getTotalResource();\n\n    if (!Resources.fitsIn(capability, totalResource)) {\n      LOG.warn(\"Node : \" + node.getNodeID()\n          + \" does not have sufficient resource for request : \" + request\n          + \" node total capability : \" + node.getTotalResource());\n      return Resources.none();\n    }\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      Token containerToken \u003d\n          createContainerToken(application, container);\n      if (containerToken \u003d\u003d null) {\n        // Something went wrong...\n        return Resources.none();\n      }\n      container.setContainerToken(containerToken);\n      \n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "942e2ebaa54306ffc5b0ffb403e552764a40d58c": {
      "type": "Ybodychange",
      "commitMessage": "YARN-1008. MiniYARNCluster with multiple nodemanagers, all nodes have same key for allocations. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1517563 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/08/13 8:39 AM",
      "commitName": "942e2ebaa54306ffc5b0ffb403e552764a40d58c",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "22/07/13 4:49 PM",
      "commitNameOld": "5b3bb05fbeb7ed4671f4d3a59677788f7fda43d0",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 34.66,
      "commitsBetweenForRepo": 214,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,85 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n+      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       Token containerToken \u003d\n           createContainerToken(application, container);\n       if (containerToken \u003d\u003d null) {\n         // Something went wrong...\n         return Resources.none();\n       }\n       container.setContainerToken(containerToken);\n       \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getNodeName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      Token containerToken \u003d\n          createContainerToken(application, container);\n      if (containerToken \u003d\u003d null) {\n        // Something went wrong...\n        return Resources.none();\n      }\n      container.setContainerToken(containerToken);\n      \n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "a2c42330047bf955a6a585dcddf798920d4c8640": {
      "type": "Ybodychange",
      "commitMessage": "YARN-717. Put object creation factories for Token in the class itself and remove useless derivations for specific tokens. Contributed by Jian He.\nMAPREDUCE-5289. Updated MR App to use Token directly after YARN-717. Contributed by Jian He.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1488616 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/06/13 2:43 PM",
      "commitName": "a2c42330047bf955a6a585dcddf798920d4c8640",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "29/05/13 9:59 PM",
      "commitNameOld": "b16c5638b5190c56f9d854d873589cb5c11c8b32",
      "commitAuthorOld": "Siddharth Seth",
      "daysBetweenCommits": 2.7,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,85 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n-      ContainerToken containerToken \u003d\n+      Token containerToken \u003d\n           createContainerToken(application, container);\n       if (containerToken \u003d\u003d null) {\n         // Something went wrong...\n         return Resources.none();\n       }\n       container.setContainerToken(containerToken);\n       \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      Token containerToken \u003d\n          createContainerToken(application, container);\n      if (containerToken \u003d\u003d null) {\n        // Something went wrong...\n        return Resources.none();\n      }\n      container.setContainerToken(containerToken);\n      \n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "ca8024673178fa1c80224b390dfba932921693d9": {
      "type": "Ybodychange",
      "commitMessage": "YARN-617. Made ContainerTokens to be used for validation at NodeManager also in unsecure mode to prevent AMs from faking resource requirements in unsecure mode. Contributed by Omkar Vinit Joshi.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1483667 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/05/13 11:36 PM",
      "commitName": "ca8024673178fa1c80224b390dfba932921693d9",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "25/04/13 8:50 PM",
      "commitNameOld": "fbb55784d93e1a819daf55d936e864d344579cbf",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 21.12,
      "commitsBetweenForRepo": 137,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,88 +1,85 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert Resources.greaterThan(\n         resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n-      // Create container tokens in secure-mode\n-      if (UserGroupInformation.isSecurityEnabled()) {\n-        ContainerToken containerToken \u003d \n-            createContainerToken(application, container);\n-        if (containerToken \u003d\u003d null) {\n-          // Something went wrong...\n-          return Resources.none();\n-        }\n-        container.setContainerToken(containerToken);\n+      ContainerToken containerToken \u003d\n+          createContainerToken(application, container);\n+      if (containerToken \u003d\u003d null) {\n+        // Something went wrong...\n+        return Resources.none();\n       }\n+      container.setContainerToken(containerToken);\n       \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n \n       // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      ContainerToken containerToken \u003d\n          createContainerToken(application, container);\n      if (containerToken \u003d\u003d null) {\n        // Something went wrong...\n        return Resources.none();\n      }\n      container.setContainerToken(containerToken);\n      \n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "453926397182078c65a4428eb5de5a90d6af6448": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2. Enhanced CapacityScheduler to account for CPU alongwith memory for multi-dimensional resource scheduling. Contributed by Arun C. Murthy.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430682 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 9:08 PM",
      "commitName": "453926397182078c65a4428eb5de5a90d6af6448",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "07/11/12 1:56 PM",
      "commitNameOld": "fb5b96dfc324f999e8b3698288c110a1c3b71c30",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 62.3,
      "commitsBetweenForRepo": 257,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,88 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n-    assert (available.getMemory() \u003e  0);\n+    assert Resources.greaterThan(\n+        resourceCalculator, clusterResource, available, Resources.none());\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n+      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n-        available.getMemory() / capability.getMemory();         \n+        resourceCalculator.computeAvailableContainers(available, capability);\n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Create container tokens in secure-mode\n       if (UserGroupInformation.isSecurityEnabled()) {\n         ContainerToken containerToken \u003d \n             createContainerToken(application, container);\n         if (containerToken \u003d\u003d null) {\n           // Something went wrong...\n           return Resources.none();\n         }\n         container.setContainerToken(containerToken);\n       }\n       \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n+\n+      // Does the application need this resource?\n       if (allocatedContainer \u003d\u003d null) {\n-        // Did the application need this resource?\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert Resources.greaterThan(\n        resourceCalculator, clusterResource, available, Resources.none());\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      LOG.warn(\"Couldn\u0027t get container for allocation!\");\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        resourceCalculator.computeAvailableContainers(available, capability);\n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Create container tokens in secure-mode\n      if (UserGroupInformation.isSecurityEnabled()) {\n        ContainerToken containerToken \u003d \n            createContainerToken(application, container);\n        if (containerToken \u003d\u003d null) {\n          // Something went wrong...\n          return Resources.none();\n        }\n        container.setContainerToken(containerToken);\n      }\n      \n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n\n      // Does the application need this resource?\n      if (allocatedContainer \u003d\u003d null) {\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "90ba993bc72e374f99c44d0770f55aeaa8342f2d": {
      "type": "Ybodychange",
      "commitMessage": "YARN-180. Capacity scheduler - containers that get reserved create container token to early (acmurthy and bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1401703 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/10/12 7:16 AM",
      "commitName": "90ba993bc72e374f99c44d0770f55aeaa8342f2d",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "24/10/12 6:21 AM",
      "commitNameOld": "cc523683cfa76c1255667a3aedc48b08e5daabc7",
      "commitAuthorOld": "Thomas Graves",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,85 @@\n   private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n       FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert (available.getMemory() \u003e  0);\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         available.getMemory() / capability.getMemory();         \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n+      // Create container tokens in secure-mode\n+      if (UserGroupInformation.isSecurityEnabled()) {\n+        ContainerToken containerToken \u003d \n+            createContainerToken(application, container);\n+        if (containerToken \u003d\u003d null) {\n+          // Something went wrong...\n+          return Resources.none();\n+        }\n+        container.setContainerToken(containerToken);\n+      }\n+      \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n       if (allocatedContainer \u003d\u003d null) {\n         // Did the application need this resource?\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Create container tokens in secure-mode\n      if (UserGroupInformation.isSecurityEnabled()) {\n        ContainerToken containerToken \u003d \n            createContainerToken(application, container);\n        if (containerToken \u003d\u003d null) {\n          // Something went wrong...\n          return Resources.none();\n        }\n        container.setContainerToken(containerToken);\n      }\n      \n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "e1fdf62123625e4ba399af02f8aad500637d29d1": {
      "type": "Yfilerename",
      "commitMessage": "YARN-1. Promote YARN to be a sub-project of Apache Hadoop.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370666 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 10:22 PM",
      "commitName": "e1fdf62123625e4ba399af02f8aad500637d29d1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "07/08/12 7:53 PM",
      "commitNameOld": "34554d1e11ee1d5b564d7d9ed3e6d55931d72749",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java"
      }
    },
    "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76": {
      "type": "Yparameterchange",
      "commitMessage": "MAPREDUCE-4440. Changed SchedulerApp and SchedulerNode to be a minimal interface to allow schedulers to maintain their own.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1362332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/07/12 6:43 PM",
      "commitName": "7f2b1eadc1b0807ec1302a0c3488bf6e7a59bc76",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "10/07/12 2:26 PM",
      "commitNameOld": "3bfb26ad3b5ac46f992a632541c97ca2bc897638",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 6.18,
      "commitsBetweenForRepo": 55,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,74 @@\n-  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n-      SchedulerApp application, Priority priority, \n+  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n+      FiCaSchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert (available.getMemory() \u003e  0);\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n   \n     // something went wrong getting/creating the container \n     if (container \u003d\u003d null) {\n       return Resources.none();\n     }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         available.getMemory() / capability.getMemory();         \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n       if (allocatedContainer \u003d\u003d null) {\n         // Did the application need this resource?\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, FiCaSchedulerNode node, \n      FiCaSchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {
        "oldValue": "[clusterResource-Resource, node-SchedulerNode, application-SchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer]",
        "newValue": "[clusterResource-Resource, node-FiCaSchedulerNode, application-FiCaSchedulerApp, priority-Priority, request-ResourceRequest, type-NodeType, rmContainer-RMContainer]"
      }
    },
    "126dd6adefeb00e4ba81ea137d63a8a76b75c3bd": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4295. RM crashes due to DNS issue (tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1352638 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/06/12 11:14 AM",
      "commitName": "126dd6adefeb00e4ba81ea137d63a8a76b75c3bd",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "21/05/12 12:15 PM",
      "commitNameOld": "d74bec2f883b562d377cc564ca86473c498a618a",
      "commitAuthorOld": "Thomas Graves",
      "daysBetweenCommits": 30.96,
      "commitsBetweenForRepo": 129,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,74 @@\n   private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n       SchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert (available.getMemory() \u003e  0);\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n+  \n+    // something went wrong getting/creating the container \n+    if (container \u003d\u003d null) {\n+      return Resources.none();\n+    }\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         available.getMemory() / capability.getMemory();         \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n       if (allocatedContainer \u003d\u003d null) {\n         // Did the application need this resource?\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" usedCapacity\u003d\" + getUsedCapacity() +\n           \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n  \n    // something went wrong getting/creating the container \n    if (container \u003d\u003d null) {\n      return Resources.none();\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "ffdf980b2056b2a1b31ccb19746f23c31f7d08ef": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3816 capacity scheduler web ui bar graphs for used capacity wrong (tgraves via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1294808 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/02/12 12:06 PM",
      "commitName": "ffdf980b2056b2a1b31ccb19746f23c31f7d08ef",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "25/02/12 10:49 PM",
      "commitNameOld": "f3cc8911485385713395a04a5b292ae375ff83a3",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 2.55,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,69 @@\n   private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n       SchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n         + \" application\u003d\" + application.getApplicationId().getId()\n         + \" priority\u003d\" + priority.getPriority()\n         + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert (available.getMemory() \u003e  0);\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         available.getMemory() / capability.getMemory();         \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n       if (allocatedContainer \u003d\u003d null) {\n         // Did the application need this resource?\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n-          \" util\u003d\" + getUtilization() + \n+          \" usedCapacity\u003d\" + getUsedCapacity() +\n+          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n-          \" util\u003d\" + getUtilization() + \n+          \" usedCapacity\u003d\" + getUsedCapacity() +\n+          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" usedCapacity\u003d\" + getUsedCapacity() +\n          \" absoluteUsedCapacity\u003d\" + getAbsoluteUsedCapacity() +\n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "f24dcb3449c77da665058427bc7fa480cad507fc": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2907. Changed log level for various messages in ResourceManager from INFO to DEBUG. Contributed by Ravi Prakash.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179178 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/10/11 4:56 AM",
      "commitName": "f24dcb3449c77da665058427bc7fa480cad507fc",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "03/10/11 4:21 PM",
      "commitNameOld": "12743d2169f5a24a9b3be07c9e9dcc3f2f1001f0",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 1.52,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,67 @@\n   private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n       SchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n-      LOG.info(\"DEBUG --- assignContainers:\" +\n-          \" node\u003d\" + node.getHostName() + \n-          \" application\u003d\" + application.getApplicationId().getId() + \n-          \" priority\u003d\" + priority.getPriority() + \n-          \" request\u003d\" + request + \" type\u003d\" + type);\n+      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n+        + \" application\u003d\" + application.getApplicationId().getId()\n+        + \" priority\u003d\" + priority.getPriority()\n+        + \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert (available.getMemory() \u003e  0);\n \n     // Create the container if necessary\n     Container container \u003d \n         getContainer(rmContainer, application, node, capability, priority);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         available.getMemory() / capability.getMemory();         \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n       if (allocatedContainer \u003d\u003d null) {\n         // Did the application need this resource?\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" util\u003d\" + getUtilization() + \n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" util\u003d\" + getUtilization() + \n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"assignContainers: node\u003d\" + node.getHostName()\n        + \" application\u003d\" + application.getApplicationId().getId()\n        + \" priority\u003d\" + priority.getPriority()\n        + \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "1e6dfa7472ad78a252d05c8ebffe086d938b61fa": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2646. Fixed AMRMProtocol to return containers based on priority. Contributed by Sharad Agarwal and Arun C Murthy.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1175859 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/09/11 6:25 AM",
      "commitName": "1e6dfa7472ad78a252d05c8ebffe086d938b61fa",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "20/09/11 6:14 PM",
      "commitNameOld": "339b85b88ead760c6d4dc0f63a72780d6d5df8c2",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 5.51,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,68 @@\n   private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n       SchedulerApp application, Priority priority, \n       ResourceRequest request, NodeType type, RMContainer rmContainer) {\n     if (LOG.isDebugEnabled()) {\n       LOG.info(\"DEBUG --- assignContainers:\" +\n           \" node\u003d\" + node.getHostName() + \n           \" application\u003d\" + application.getApplicationId().getId() + \n           \" priority\u003d\" + priority.getPriority() + \n           \" request\u003d\" + request + \" type\u003d\" + type);\n     }\n     Resource capability \u003d request.getCapability();\n \n     Resource available \u003d node.getAvailableResource();\n \n     assert (available.getMemory() \u003e  0);\n \n     // Create the container if necessary\n     Container container \u003d \n-        getContainer(rmContainer, application, node, capability);\n+        getContainer(rmContainer, application, node, capability, priority);\n \n     // Can we allocate a container on this node?\n     int availableContainers \u003d \n         available.getMemory() / capability.getMemory();         \n     if (availableContainers \u003e 0) {\n       // Allocate...\n \n       // Did we previously reserve containers at this \u0027priority\u0027?\n       if (rmContainer !\u003d null){\n         unreserve(application, priority, node, rmContainer);\n       }\n \n       // Inform the application\n       RMContainer allocatedContainer \u003d \n           application.allocate(type, node, priority, request, container);\n       if (allocatedContainer \u003d\u003d null) {\n         // Did the application need this resource?\n         return Resources.none();\n       }\n \n       // Inform the node\n       node.allocateContainer(application.getApplicationId(), \n           allocatedContainer);\n \n       LOG.info(\"assignedContainer\" +\n           \" application\u003d\" + application.getApplicationId() +\n           \" container\u003d\" + container + \n           \" containerId\u003d\" + container.getId() + \n           \" queue\u003d\" + this + \n           \" util\u003d\" + getUtilization() + \n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return container.getResource();\n     } else {\n       // Reserve by \u0027charging\u0027 in advance...\n       reserve(application, priority, node, rmContainer, container);\n \n       LOG.info(\"Reserved container \" + \n           \" application\u003d\" + application.getApplicationId() +\n           \" resource\u003d\" + request.getCapability() + \n           \" queue\u003d\" + this.toString() + \n           \" util\u003d\" + getUtilization() + \n           \" used\u003d\" + usedResources + \n           \" cluster\u003d\" + clusterResource);\n \n       return request.getCapability();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.info(\"DEBUG --- assignContainers:\" +\n          \" node\u003d\" + node.getHostName() + \n          \" application\u003d\" + application.getApplicationId().getId() + \n          \" priority\u003d\" + priority.getPriority() + \n          \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability, priority);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.info(\"DEBUG --- assignContainers:\" +\n          \" node\u003d\" + node.getHostName() + \n          \" application\u003d\" + application.getApplicationId().getId() + \n          \" priority\u003d\" + priority.getPriority() + \n          \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
        "newPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "diff": "@@ -0,0 +1,68 @@\n+  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n+      SchedulerApp application, Priority priority, \n+      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n+    if (LOG.isDebugEnabled()) {\n+      LOG.info(\"DEBUG --- assignContainers:\" +\n+          \" node\u003d\" + node.getHostName() + \n+          \" application\u003d\" + application.getApplicationId().getId() + \n+          \" priority\u003d\" + priority.getPriority() + \n+          \" request\u003d\" + request + \" type\u003d\" + type);\n+    }\n+    Resource capability \u003d request.getCapability();\n+\n+    Resource available \u003d node.getAvailableResource();\n+\n+    assert (available.getMemory() \u003e  0);\n+\n+    // Create the container if necessary\n+    Container container \u003d \n+        getContainer(rmContainer, application, node, capability);\n+\n+    // Can we allocate a container on this node?\n+    int availableContainers \u003d \n+        available.getMemory() / capability.getMemory();         \n+    if (availableContainers \u003e 0) {\n+      // Allocate...\n+\n+      // Did we previously reserve containers at this \u0027priority\u0027?\n+      if (rmContainer !\u003d null){\n+        unreserve(application, priority, node, rmContainer);\n+      }\n+\n+      // Inform the application\n+      RMContainer allocatedContainer \u003d \n+          application.allocate(type, node, priority, request, container);\n+      if (allocatedContainer \u003d\u003d null) {\n+        // Did the application need this resource?\n+        return Resources.none();\n+      }\n+\n+      // Inform the node\n+      node.allocateContainer(application.getApplicationId(), \n+          allocatedContainer);\n+\n+      LOG.info(\"assignedContainer\" +\n+          \" application\u003d\" + application.getApplicationId() +\n+          \" container\u003d\" + container + \n+          \" containerId\u003d\" + container.getId() + \n+          \" queue\u003d\" + this + \n+          \" util\u003d\" + getUtilization() + \n+          \" used\u003d\" + usedResources + \n+          \" cluster\u003d\" + clusterResource);\n+\n+      return container.getResource();\n+    } else {\n+      // Reserve by \u0027charging\u0027 in advance...\n+      reserve(application, priority, node, rmContainer, container);\n+\n+      LOG.info(\"Reserved container \" + \n+          \" application\u003d\" + application.getApplicationId() +\n+          \" resource\u003d\" + request.getCapability() + \n+          \" queue\u003d\" + this.toString() + \n+          \" util\u003d\" + getUtilization() + \n+          \" used\u003d\" + usedResources + \n+          \" cluster\u003d\" + clusterResource);\n+\n+      return request.getCapability();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private Resource assignContainer(Resource clusterResource, SchedulerNode node, \n      SchedulerApp application, Priority priority, \n      ResourceRequest request, NodeType type, RMContainer rmContainer) {\n    if (LOG.isDebugEnabled()) {\n      LOG.info(\"DEBUG --- assignContainers:\" +\n          \" node\u003d\" + node.getHostName() + \n          \" application\u003d\" + application.getApplicationId().getId() + \n          \" priority\u003d\" + priority.getPriority() + \n          \" request\u003d\" + request + \" type\u003d\" + type);\n    }\n    Resource capability \u003d request.getCapability();\n\n    Resource available \u003d node.getAvailableResource();\n\n    assert (available.getMemory() \u003e  0);\n\n    // Create the container if necessary\n    Container container \u003d \n        getContainer(rmContainer, application, node, capability);\n\n    // Can we allocate a container on this node?\n    int availableContainers \u003d \n        available.getMemory() / capability.getMemory();         \n    if (availableContainers \u003e 0) {\n      // Allocate...\n\n      // Did we previously reserve containers at this \u0027priority\u0027?\n      if (rmContainer !\u003d null){\n        unreserve(application, priority, node, rmContainer);\n      }\n\n      // Inform the application\n      RMContainer allocatedContainer \u003d \n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer \u003d\u003d null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(), \n          allocatedContainer);\n\n      LOG.info(\"assignedContainer\" +\n          \" application\u003d\" + application.getApplicationId() +\n          \" container\u003d\" + container + \n          \" containerId\u003d\" + container.getId() + \n          \" queue\u003d\" + this + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return container.getResource();\n    } else {\n      // Reserve by \u0027charging\u0027 in advance...\n      reserve(application, priority, node, rmContainer, container);\n\n      LOG.info(\"Reserved container \" + \n          \" application\u003d\" + application.getApplicationId() +\n          \" resource\u003d\" + request.getCapability() + \n          \" queue\u003d\" + this.toString() + \n          \" util\u003d\" + getUtilization() + \n          \" used\u003d\" + usedResources + \n          \" cluster\u003d\" + clusterResource);\n\n      return request.getCapability();\n    }\n  }",
      "path": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java"
    }
  }
}