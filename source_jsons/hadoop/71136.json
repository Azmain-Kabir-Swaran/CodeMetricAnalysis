{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Client.java",
  "functionName": "init",
  "functionId": "init___args-String[]",
  "sourceFilePath": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/Client.java",
  "functionStartLine": 394,
  "functionEndLine": 509,
  "numCommitsSeen": 7,
  "timeTaken": 1423,
  "changeHistory": [
    "477505ccfc480f2605a7b65de95ea6f6ff5ce090",
    "9637097ef9b213fcbeffa2538ccb7e0aaabde9c4",
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91"
  ],
  "changeHistoryShort": {
    "477505ccfc480f2605a7b65de95ea6f6ff5ce090": "Ybodychange",
    "9637097ef9b213fcbeffa2538ccb7e0aaabde9c4": "Ybodychange",
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91": "Yintroduced"
  },
  "changeHistoryDetails": {
    "477505ccfc480f2605a7b65de95ea6f6ff5ce090": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14824. [Dynamometer] Dynamometer in org.apache.hadoop.tools does not output the benchmark results. (#1685)\n\n\r\n",
      "commitDate": "01/11/19 9:32 AM",
      "commitName": "477505ccfc480f2605a7b65de95ea6f6ff5ce090",
      "commitAuthor": "Takanobu Asanuma",
      "commitDateOld": "06/09/19 10:24 AM",
      "commitNameOld": "9637097ef9b213fcbeffa2538ccb7e0aaabde9c4",
      "commitAuthorOld": "Erik Krogen",
      "daysBetweenCommits": 55.96,
      "commitsBetweenForRepo": 364,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,115 +1,116 @@\n   public boolean init(String[] args) throws ParseException, IOException {\n \n     List\u003cString\u003e list \u003d Arrays.asList(args);\n     if (list.contains(\"-h\") || list.contains(\"--help\")) {\n       printUsage();\n       return false;\n     }\n \n     CommandLineParser parser \u003d new GnuParser();\n     CommandLine commandLine \u003d parser.parse(opts, args);\n \n     yarnClient \u003d YarnClient.createYarnClient();\n     yarnClient.init(getConf());\n \n     LOG.info(\"Starting with arguments: [\\\"{}\\\"]\",\n         Joiner.on(\"\\\" \\\"\").join(args));\n \n     Path fsImageDir \u003d new Path(commandLine.getOptionValue(FS_IMAGE_DIR_ARG,\n         \"\"));\n     versionFilePath \u003d new Path(fsImageDir, \"VERSION\").toString();\n     if (commandLine.hasOption(NAMENODE_SERVICERPC_ADDR_ARG)) {\n       launchNameNode \u003d false;\n       remoteNameNodeRpcAddress \u003d\n           commandLine.getOptionValue(NAMENODE_SERVICERPC_ADDR_ARG);\n     } else {\n       launchNameNode \u003d true;\n       FileSystem localFS \u003d FileSystem.getLocal(getConf());\n       fsImageDir \u003d fsImageDir.makeQualified(localFS.getUri(),\n           localFS.getWorkingDirectory());\n       FileSystem fsImageFS \u003d fsImageDir.getFileSystem(getConf());\n       FileStatus[] fsImageFiles \u003d fsImageFS.listStatus(fsImageDir,\n           (path) -\u003e path.getName().matches(\"^fsimage_(\\\\d)+$\"));\n       if (fsImageFiles.length !\u003d 1) {\n         throw new IllegalArgumentException(\n             \"Must be exactly one fsimage file present in fs_image_dir\");\n       }\n       fsImagePath \u003d fsImageFiles[0].getPath().toString();\n       fsImageMD5Path \u003d fsImageFiles[0].getPath().suffix(\".md5\").toString();\n     }\n \n     if (amMemory \u003c 0) {\n       throw new IllegalArgumentException(\"Invalid memory specified for \"\n           + \"application master, exiting. Specified memory\u003d\" + amMemory);\n     }\n     if (amVCores \u003c 0) {\n       throw new IllegalArgumentException(\"Invalid virtual cores specified for \"\n           + \"application master, exiting. Specified virtual cores\u003d\" + amVCores);\n     }\n \n     this.appName \u003d commandLine.getOptionValue(APPNAME_ARG, APPNAME_DEFAULT);\n     this.amQueue \u003d commandLine.getOptionValue(QUEUE_ARG, QUEUE_DEFAULT);\n     this.amMemory \u003d Integer.parseInt(commandLine\n         .getOptionValue(MASTER_MEMORY_MB_ARG, MASTER_MEMORY_MB_DEFAULT));\n     this.amVCores \u003d Integer.parseInt(\n         commandLine.getOptionValue(MASTER_VCORES_ARG, MASTER_VCORES_DEFAULT));\n     this.confPath \u003d commandLine.getOptionValue(CONF_PATH_ARG);\n     this.blockListPath \u003d commandLine.getOptionValue(BLOCK_LIST_PATH_ARG);\n     if (commandLine.hasOption(HADOOP_BINARY_PATH_ARG)) {\n       this.hadoopBinary \u003d commandLine.getOptionValue(HADOOP_BINARY_PATH_ARG);\n     } else {\n       this.hadoopBinary \u003d DynoInfraUtils.fetchHadoopTarball(\n           new File(\".\").getAbsoluteFile(),\n           commandLine.getOptionValue(HADOOP_VERSION_ARG), getConf(), LOG)\n           .toString();\n     }\n     this.amOptions \u003d AMOptions.initFromParser(commandLine);\n     this.clientTimeout \u003d Integer\n         .parseInt(commandLine.getOptionValue(TIMEOUT_ARG, TIMEOUT_DEFAULT));\n     this.tokenFileLocation \u003d commandLine.\n         getOptionValue(TOKEN_FILE_LOCATION_ARG);\n \n     amOptions.verify();\n \n     Path blockPath \u003d new Path(blockListPath);\n     FileSystem blockListFS \u003d blockPath.getFileSystem(getConf());\n     if (blockListFS.getUri().equals(FileSystem.getLocal(getConf()).getUri())\n         || !blockListFS.exists(blockPath)) {\n       throw new IllegalArgumentException(\n           \"block list path must already exist on remote fs!\");\n     }\n     numTotalDataNodes \u003d blockListFS.listStatus(blockPath,\n         DynoConstants.BLOCK_LIST_FILE_FILTER).length;\n \n     if (commandLine.hasOption(WORKLOAD_REPLAY_ENABLE_ARG)) {\n       if (!commandLine.hasOption(WORKLOAD_INPUT_PATH_ARG)\n           || !commandLine.hasOption(WORKLOAD_START_DELAY_ARG)) {\n         throw new IllegalArgumentException(\"workload_replay_enable was \"\n             + \"specified; must include all required workload_ parameters.\");\n       }\n       launchWorkloadJob \u003d true;\n       workloadInputPath \u003d commandLine.getOptionValue(WORKLOAD_INPUT_PATH_ARG);\n+      workloadOutputPath \u003d commandLine.getOptionValue(WORKLOAD_OUTPUT_PATH_ARG);\n       workloadThreadsPerMapper \u003d Integer\n           .parseInt(commandLine.getOptionValue(WORKLOAD_THREADS_PER_MAPPER_ARG,\n               String.valueOf(AuditReplayMapper.NUM_THREADS_DEFAULT)));\n       workloadRateFactor \u003d Double.parseDouble(commandLine.getOptionValue(\n           WORKLOAD_RATE_FACTOR_ARG, WORKLOAD_RATE_FACTOR_DEFAULT));\n       workloadExtraConfigs \u003d new HashMap\u003c\u003e();\n       if (commandLine.getOptionValues(WORKLOAD_CONFIG_ARG) !\u003d null) {\n         for (String opt : commandLine.getOptionValues(WORKLOAD_CONFIG_ARG)) {\n           Iterator\u003cString\u003e kvPair \u003d\n               Splitter.on(\"\u003d\").trimResults().split(opt).iterator();\n           workloadExtraConfigs.put(kvPair.next(), kvPair.next());\n         }\n       }\n       String delayString \u003d commandLine.getOptionValue(WORKLOAD_START_DELAY_ARG,\n           WorkloadDriver.START_TIME_OFFSET_DEFAULT);\n       // Store a temporary config to leverage Configuration\u0027s time duration\n       // parsing.\n       getConf().set(\"___temp___\", delayString);\n       workloadStartDelayMs \u003d getConf().getTimeDuration(\"___temp___\", 0,\n           TimeUnit.MILLISECONDS);\n     }\n \n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean init(String[] args) throws ParseException, IOException {\n\n    List\u003cString\u003e list \u003d Arrays.asList(args);\n    if (list.contains(\"-h\") || list.contains(\"--help\")) {\n      printUsage();\n      return false;\n    }\n\n    CommandLineParser parser \u003d new GnuParser();\n    CommandLine commandLine \u003d parser.parse(opts, args);\n\n    yarnClient \u003d YarnClient.createYarnClient();\n    yarnClient.init(getConf());\n\n    LOG.info(\"Starting with arguments: [\\\"{}\\\"]\",\n        Joiner.on(\"\\\" \\\"\").join(args));\n\n    Path fsImageDir \u003d new Path(commandLine.getOptionValue(FS_IMAGE_DIR_ARG,\n        \"\"));\n    versionFilePath \u003d new Path(fsImageDir, \"VERSION\").toString();\n    if (commandLine.hasOption(NAMENODE_SERVICERPC_ADDR_ARG)) {\n      launchNameNode \u003d false;\n      remoteNameNodeRpcAddress \u003d\n          commandLine.getOptionValue(NAMENODE_SERVICERPC_ADDR_ARG);\n    } else {\n      launchNameNode \u003d true;\n      FileSystem localFS \u003d FileSystem.getLocal(getConf());\n      fsImageDir \u003d fsImageDir.makeQualified(localFS.getUri(),\n          localFS.getWorkingDirectory());\n      FileSystem fsImageFS \u003d fsImageDir.getFileSystem(getConf());\n      FileStatus[] fsImageFiles \u003d fsImageFS.listStatus(fsImageDir,\n          (path) -\u003e path.getName().matches(\"^fsimage_(\\\\d)+$\"));\n      if (fsImageFiles.length !\u003d 1) {\n        throw new IllegalArgumentException(\n            \"Must be exactly one fsimage file present in fs_image_dir\");\n      }\n      fsImagePath \u003d fsImageFiles[0].getPath().toString();\n      fsImageMD5Path \u003d fsImageFiles[0].getPath().suffix(\".md5\").toString();\n    }\n\n    if (amMemory \u003c 0) {\n      throw new IllegalArgumentException(\"Invalid memory specified for \"\n          + \"application master, exiting. Specified memory\u003d\" + amMemory);\n    }\n    if (amVCores \u003c 0) {\n      throw new IllegalArgumentException(\"Invalid virtual cores specified for \"\n          + \"application master, exiting. Specified virtual cores\u003d\" + amVCores);\n    }\n\n    this.appName \u003d commandLine.getOptionValue(APPNAME_ARG, APPNAME_DEFAULT);\n    this.amQueue \u003d commandLine.getOptionValue(QUEUE_ARG, QUEUE_DEFAULT);\n    this.amMemory \u003d Integer.parseInt(commandLine\n        .getOptionValue(MASTER_MEMORY_MB_ARG, MASTER_MEMORY_MB_DEFAULT));\n    this.amVCores \u003d Integer.parseInt(\n        commandLine.getOptionValue(MASTER_VCORES_ARG, MASTER_VCORES_DEFAULT));\n    this.confPath \u003d commandLine.getOptionValue(CONF_PATH_ARG);\n    this.blockListPath \u003d commandLine.getOptionValue(BLOCK_LIST_PATH_ARG);\n    if (commandLine.hasOption(HADOOP_BINARY_PATH_ARG)) {\n      this.hadoopBinary \u003d commandLine.getOptionValue(HADOOP_BINARY_PATH_ARG);\n    } else {\n      this.hadoopBinary \u003d DynoInfraUtils.fetchHadoopTarball(\n          new File(\".\").getAbsoluteFile(),\n          commandLine.getOptionValue(HADOOP_VERSION_ARG), getConf(), LOG)\n          .toString();\n    }\n    this.amOptions \u003d AMOptions.initFromParser(commandLine);\n    this.clientTimeout \u003d Integer\n        .parseInt(commandLine.getOptionValue(TIMEOUT_ARG, TIMEOUT_DEFAULT));\n    this.tokenFileLocation \u003d commandLine.\n        getOptionValue(TOKEN_FILE_LOCATION_ARG);\n\n    amOptions.verify();\n\n    Path blockPath \u003d new Path(blockListPath);\n    FileSystem blockListFS \u003d blockPath.getFileSystem(getConf());\n    if (blockListFS.getUri().equals(FileSystem.getLocal(getConf()).getUri())\n        || !blockListFS.exists(blockPath)) {\n      throw new IllegalArgumentException(\n          \"block list path must already exist on remote fs!\");\n    }\n    numTotalDataNodes \u003d blockListFS.listStatus(blockPath,\n        DynoConstants.BLOCK_LIST_FILE_FILTER).length;\n\n    if (commandLine.hasOption(WORKLOAD_REPLAY_ENABLE_ARG)) {\n      if (!commandLine.hasOption(WORKLOAD_INPUT_PATH_ARG)\n          || !commandLine.hasOption(WORKLOAD_START_DELAY_ARG)) {\n        throw new IllegalArgumentException(\"workload_replay_enable was \"\n            + \"specified; must include all required workload_ parameters.\");\n      }\n      launchWorkloadJob \u003d true;\n      workloadInputPath \u003d commandLine.getOptionValue(WORKLOAD_INPUT_PATH_ARG);\n      workloadOutputPath \u003d commandLine.getOptionValue(WORKLOAD_OUTPUT_PATH_ARG);\n      workloadThreadsPerMapper \u003d Integer\n          .parseInt(commandLine.getOptionValue(WORKLOAD_THREADS_PER_MAPPER_ARG,\n              String.valueOf(AuditReplayMapper.NUM_THREADS_DEFAULT)));\n      workloadRateFactor \u003d Double.parseDouble(commandLine.getOptionValue(\n          WORKLOAD_RATE_FACTOR_ARG, WORKLOAD_RATE_FACTOR_DEFAULT));\n      workloadExtraConfigs \u003d new HashMap\u003c\u003e();\n      if (commandLine.getOptionValues(WORKLOAD_CONFIG_ARG) !\u003d null) {\n        for (String opt : commandLine.getOptionValues(WORKLOAD_CONFIG_ARG)) {\n          Iterator\u003cString\u003e kvPair \u003d\n              Splitter.on(\"\u003d\").trimResults().split(opt).iterator();\n          workloadExtraConfigs.put(kvPair.next(), kvPair.next());\n        }\n      }\n      String delayString \u003d commandLine.getOptionValue(WORKLOAD_START_DELAY_ARG,\n          WorkloadDriver.START_TIME_OFFSET_DEFAULT);\n      // Store a temporary config to leverage Configuration\u0027s time duration\n      // parsing.\n      getConf().set(\"___temp___\", delayString);\n      workloadStartDelayMs \u003d getConf().getTimeDuration(\"___temp___\", 0,\n          TimeUnit.MILLISECONDS);\n    }\n\n    return true;\n  }",
      "path": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/Client.java",
      "extendedDetails": {}
    },
    "9637097ef9b213fcbeffa2538ccb7e0aaabde9c4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14817. [Dynamometer] Fix start script options parsing which incorrectly interpret options starting with h as a help argument. Contributed by Soya Miyoshi.\n",
      "commitDate": "06/09/19 10:24 AM",
      "commitName": "9637097ef9b213fcbeffa2538ccb7e0aaabde9c4",
      "commitAuthor": "Erik Krogen",
      "commitDateOld": "13/08/19 8:52 AM",
      "commitNameOld": "274966e675d03875d4522440d1e4d0ab1ba04f23",
      "commitAuthorOld": "Erik Krogen",
      "daysBetweenCommits": 24.06,
      "commitsBetweenForRepo": 243,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,115 @@\n   public boolean init(String[] args) throws ParseException, IOException {\n \n-    CommandLineParser parser \u003d new GnuParser();\n-    if (parser.parse(\n-        new Options().addOption(\"h\", \"help\", false, \"Shows this message.\"),\n-        args, true).hasOption(\"h\")) {\n+    List\u003cString\u003e list \u003d Arrays.asList(args);\n+    if (list.contains(\"-h\") || list.contains(\"--help\")) {\n       printUsage();\n       return false;\n     }\n \n-    CommandLine cliParser \u003d parser.parse(opts, args);\n+    CommandLineParser parser \u003d new GnuParser();\n+    CommandLine commandLine \u003d parser.parse(opts, args);\n \n     yarnClient \u003d YarnClient.createYarnClient();\n     yarnClient.init(getConf());\n \n     LOG.info(\"Starting with arguments: [\\\"{}\\\"]\",\n         Joiner.on(\"\\\" \\\"\").join(args));\n \n-    Path fsImageDir \u003d new Path(cliParser.getOptionValue(FS_IMAGE_DIR_ARG, \"\"));\n+    Path fsImageDir \u003d new Path(commandLine.getOptionValue(FS_IMAGE_DIR_ARG,\n+        \"\"));\n     versionFilePath \u003d new Path(fsImageDir, \"VERSION\").toString();\n-    if (cliParser.hasOption(NAMENODE_SERVICERPC_ADDR_ARG)) {\n+    if (commandLine.hasOption(NAMENODE_SERVICERPC_ADDR_ARG)) {\n       launchNameNode \u003d false;\n       remoteNameNodeRpcAddress \u003d\n-          cliParser.getOptionValue(NAMENODE_SERVICERPC_ADDR_ARG);\n+          commandLine.getOptionValue(NAMENODE_SERVICERPC_ADDR_ARG);\n     } else {\n       launchNameNode \u003d true;\n       FileSystem localFS \u003d FileSystem.getLocal(getConf());\n       fsImageDir \u003d fsImageDir.makeQualified(localFS.getUri(),\n           localFS.getWorkingDirectory());\n       FileSystem fsImageFS \u003d fsImageDir.getFileSystem(getConf());\n       FileStatus[] fsImageFiles \u003d fsImageFS.listStatus(fsImageDir,\n           (path) -\u003e path.getName().matches(\"^fsimage_(\\\\d)+$\"));\n       if (fsImageFiles.length !\u003d 1) {\n         throw new IllegalArgumentException(\n             \"Must be exactly one fsimage file present in fs_image_dir\");\n       }\n       fsImagePath \u003d fsImageFiles[0].getPath().toString();\n       fsImageMD5Path \u003d fsImageFiles[0].getPath().suffix(\".md5\").toString();\n     }\n \n     if (amMemory \u003c 0) {\n       throw new IllegalArgumentException(\"Invalid memory specified for \"\n           + \"application master, exiting. Specified memory\u003d\" + amMemory);\n     }\n     if (amVCores \u003c 0) {\n       throw new IllegalArgumentException(\"Invalid virtual cores specified for \"\n           + \"application master, exiting. Specified virtual cores\u003d\" + amVCores);\n     }\n \n-    this.appName \u003d cliParser.getOptionValue(APPNAME_ARG, APPNAME_DEFAULT);\n-    this.amQueue \u003d cliParser.getOptionValue(QUEUE_ARG, QUEUE_DEFAULT);\n-    this.amMemory \u003d Integer.parseInt(cliParser\n+    this.appName \u003d commandLine.getOptionValue(APPNAME_ARG, APPNAME_DEFAULT);\n+    this.amQueue \u003d commandLine.getOptionValue(QUEUE_ARG, QUEUE_DEFAULT);\n+    this.amMemory \u003d Integer.parseInt(commandLine\n         .getOptionValue(MASTER_MEMORY_MB_ARG, MASTER_MEMORY_MB_DEFAULT));\n     this.amVCores \u003d Integer.parseInt(\n-        cliParser.getOptionValue(MASTER_VCORES_ARG, MASTER_VCORES_DEFAULT));\n-    this.confPath \u003d cliParser.getOptionValue(CONF_PATH_ARG);\n-    this.blockListPath \u003d cliParser.getOptionValue(BLOCK_LIST_PATH_ARG);\n-    if (cliParser.hasOption(HADOOP_BINARY_PATH_ARG)) {\n-      this.hadoopBinary \u003d cliParser.getOptionValue(HADOOP_BINARY_PATH_ARG);\n+        commandLine.getOptionValue(MASTER_VCORES_ARG, MASTER_VCORES_DEFAULT));\n+    this.confPath \u003d commandLine.getOptionValue(CONF_PATH_ARG);\n+    this.blockListPath \u003d commandLine.getOptionValue(BLOCK_LIST_PATH_ARG);\n+    if (commandLine.hasOption(HADOOP_BINARY_PATH_ARG)) {\n+      this.hadoopBinary \u003d commandLine.getOptionValue(HADOOP_BINARY_PATH_ARG);\n     } else {\n       this.hadoopBinary \u003d DynoInfraUtils.fetchHadoopTarball(\n           new File(\".\").getAbsoluteFile(),\n-          cliParser.getOptionValue(HADOOP_VERSION_ARG), getConf(), LOG)\n+          commandLine.getOptionValue(HADOOP_VERSION_ARG), getConf(), LOG)\n           .toString();\n     }\n-    this.amOptions \u003d AMOptions.initFromParser(cliParser);\n+    this.amOptions \u003d AMOptions.initFromParser(commandLine);\n     this.clientTimeout \u003d Integer\n-        .parseInt(cliParser.getOptionValue(TIMEOUT_ARG, TIMEOUT_DEFAULT));\n-    this.tokenFileLocation \u003d cliParser.getOptionValue(TOKEN_FILE_LOCATION_ARG);\n+        .parseInt(commandLine.getOptionValue(TIMEOUT_ARG, TIMEOUT_DEFAULT));\n+    this.tokenFileLocation \u003d commandLine.\n+        getOptionValue(TOKEN_FILE_LOCATION_ARG);\n \n     amOptions.verify();\n \n     Path blockPath \u003d new Path(blockListPath);\n     FileSystem blockListFS \u003d blockPath.getFileSystem(getConf());\n     if (blockListFS.getUri().equals(FileSystem.getLocal(getConf()).getUri())\n         || !blockListFS.exists(blockPath)) {\n       throw new IllegalArgumentException(\n           \"block list path must already exist on remote fs!\");\n     }\n     numTotalDataNodes \u003d blockListFS.listStatus(blockPath,\n         DynoConstants.BLOCK_LIST_FILE_FILTER).length;\n \n-    if (cliParser.hasOption(WORKLOAD_REPLAY_ENABLE_ARG)) {\n-      if (!cliParser.hasOption(WORKLOAD_INPUT_PATH_ARG)\n-          || !cliParser.hasOption(WORKLOAD_START_DELAY_ARG)) {\n+    if (commandLine.hasOption(WORKLOAD_REPLAY_ENABLE_ARG)) {\n+      if (!commandLine.hasOption(WORKLOAD_INPUT_PATH_ARG)\n+          || !commandLine.hasOption(WORKLOAD_START_DELAY_ARG)) {\n         throw new IllegalArgumentException(\"workload_replay_enable was \"\n             + \"specified; must include all required workload_ parameters.\");\n       }\n       launchWorkloadJob \u003d true;\n-      workloadInputPath \u003d cliParser.getOptionValue(WORKLOAD_INPUT_PATH_ARG);\n+      workloadInputPath \u003d commandLine.getOptionValue(WORKLOAD_INPUT_PATH_ARG);\n       workloadThreadsPerMapper \u003d Integer\n-          .parseInt(cliParser.getOptionValue(WORKLOAD_THREADS_PER_MAPPER_ARG,\n+          .parseInt(commandLine.getOptionValue(WORKLOAD_THREADS_PER_MAPPER_ARG,\n               String.valueOf(AuditReplayMapper.NUM_THREADS_DEFAULT)));\n-      workloadRateFactor \u003d Double.parseDouble(cliParser.getOptionValue(\n+      workloadRateFactor \u003d Double.parseDouble(commandLine.getOptionValue(\n           WORKLOAD_RATE_FACTOR_ARG, WORKLOAD_RATE_FACTOR_DEFAULT));\n       workloadExtraConfigs \u003d new HashMap\u003c\u003e();\n-      if (cliParser.getOptionValues(WORKLOAD_CONFIG_ARG) !\u003d null) {\n-        for (String opt : cliParser.getOptionValues(WORKLOAD_CONFIG_ARG)) {\n+      if (commandLine.getOptionValues(WORKLOAD_CONFIG_ARG) !\u003d null) {\n+        for (String opt : commandLine.getOptionValues(WORKLOAD_CONFIG_ARG)) {\n           Iterator\u003cString\u003e kvPair \u003d\n               Splitter.on(\"\u003d\").trimResults().split(opt).iterator();\n           workloadExtraConfigs.put(kvPair.next(), kvPair.next());\n         }\n       }\n-      String delayString \u003d cliParser.getOptionValue(WORKLOAD_START_DELAY_ARG,\n+      String delayString \u003d commandLine.getOptionValue(WORKLOAD_START_DELAY_ARG,\n           WorkloadDriver.START_TIME_OFFSET_DEFAULT);\n       // Store a temporary config to leverage Configuration\u0027s time duration\n       // parsing.\n       getConf().set(\"___temp___\", delayString);\n       workloadStartDelayMs \u003d getConf().getTimeDuration(\"___temp___\", 0,\n           TimeUnit.MILLISECONDS);\n     }\n \n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean init(String[] args) throws ParseException, IOException {\n\n    List\u003cString\u003e list \u003d Arrays.asList(args);\n    if (list.contains(\"-h\") || list.contains(\"--help\")) {\n      printUsage();\n      return false;\n    }\n\n    CommandLineParser parser \u003d new GnuParser();\n    CommandLine commandLine \u003d parser.parse(opts, args);\n\n    yarnClient \u003d YarnClient.createYarnClient();\n    yarnClient.init(getConf());\n\n    LOG.info(\"Starting with arguments: [\\\"{}\\\"]\",\n        Joiner.on(\"\\\" \\\"\").join(args));\n\n    Path fsImageDir \u003d new Path(commandLine.getOptionValue(FS_IMAGE_DIR_ARG,\n        \"\"));\n    versionFilePath \u003d new Path(fsImageDir, \"VERSION\").toString();\n    if (commandLine.hasOption(NAMENODE_SERVICERPC_ADDR_ARG)) {\n      launchNameNode \u003d false;\n      remoteNameNodeRpcAddress \u003d\n          commandLine.getOptionValue(NAMENODE_SERVICERPC_ADDR_ARG);\n    } else {\n      launchNameNode \u003d true;\n      FileSystem localFS \u003d FileSystem.getLocal(getConf());\n      fsImageDir \u003d fsImageDir.makeQualified(localFS.getUri(),\n          localFS.getWorkingDirectory());\n      FileSystem fsImageFS \u003d fsImageDir.getFileSystem(getConf());\n      FileStatus[] fsImageFiles \u003d fsImageFS.listStatus(fsImageDir,\n          (path) -\u003e path.getName().matches(\"^fsimage_(\\\\d)+$\"));\n      if (fsImageFiles.length !\u003d 1) {\n        throw new IllegalArgumentException(\n            \"Must be exactly one fsimage file present in fs_image_dir\");\n      }\n      fsImagePath \u003d fsImageFiles[0].getPath().toString();\n      fsImageMD5Path \u003d fsImageFiles[0].getPath().suffix(\".md5\").toString();\n    }\n\n    if (amMemory \u003c 0) {\n      throw new IllegalArgumentException(\"Invalid memory specified for \"\n          + \"application master, exiting. Specified memory\u003d\" + amMemory);\n    }\n    if (amVCores \u003c 0) {\n      throw new IllegalArgumentException(\"Invalid virtual cores specified for \"\n          + \"application master, exiting. Specified virtual cores\u003d\" + amVCores);\n    }\n\n    this.appName \u003d commandLine.getOptionValue(APPNAME_ARG, APPNAME_DEFAULT);\n    this.amQueue \u003d commandLine.getOptionValue(QUEUE_ARG, QUEUE_DEFAULT);\n    this.amMemory \u003d Integer.parseInt(commandLine\n        .getOptionValue(MASTER_MEMORY_MB_ARG, MASTER_MEMORY_MB_DEFAULT));\n    this.amVCores \u003d Integer.parseInt(\n        commandLine.getOptionValue(MASTER_VCORES_ARG, MASTER_VCORES_DEFAULT));\n    this.confPath \u003d commandLine.getOptionValue(CONF_PATH_ARG);\n    this.blockListPath \u003d commandLine.getOptionValue(BLOCK_LIST_PATH_ARG);\n    if (commandLine.hasOption(HADOOP_BINARY_PATH_ARG)) {\n      this.hadoopBinary \u003d commandLine.getOptionValue(HADOOP_BINARY_PATH_ARG);\n    } else {\n      this.hadoopBinary \u003d DynoInfraUtils.fetchHadoopTarball(\n          new File(\".\").getAbsoluteFile(),\n          commandLine.getOptionValue(HADOOP_VERSION_ARG), getConf(), LOG)\n          .toString();\n    }\n    this.amOptions \u003d AMOptions.initFromParser(commandLine);\n    this.clientTimeout \u003d Integer\n        .parseInt(commandLine.getOptionValue(TIMEOUT_ARG, TIMEOUT_DEFAULT));\n    this.tokenFileLocation \u003d commandLine.\n        getOptionValue(TOKEN_FILE_LOCATION_ARG);\n\n    amOptions.verify();\n\n    Path blockPath \u003d new Path(blockListPath);\n    FileSystem blockListFS \u003d blockPath.getFileSystem(getConf());\n    if (blockListFS.getUri().equals(FileSystem.getLocal(getConf()).getUri())\n        || !blockListFS.exists(blockPath)) {\n      throw new IllegalArgumentException(\n          \"block list path must already exist on remote fs!\");\n    }\n    numTotalDataNodes \u003d blockListFS.listStatus(blockPath,\n        DynoConstants.BLOCK_LIST_FILE_FILTER).length;\n\n    if (commandLine.hasOption(WORKLOAD_REPLAY_ENABLE_ARG)) {\n      if (!commandLine.hasOption(WORKLOAD_INPUT_PATH_ARG)\n          || !commandLine.hasOption(WORKLOAD_START_DELAY_ARG)) {\n        throw new IllegalArgumentException(\"workload_replay_enable was \"\n            + \"specified; must include all required workload_ parameters.\");\n      }\n      launchWorkloadJob \u003d true;\n      workloadInputPath \u003d commandLine.getOptionValue(WORKLOAD_INPUT_PATH_ARG);\n      workloadThreadsPerMapper \u003d Integer\n          .parseInt(commandLine.getOptionValue(WORKLOAD_THREADS_PER_MAPPER_ARG,\n              String.valueOf(AuditReplayMapper.NUM_THREADS_DEFAULT)));\n      workloadRateFactor \u003d Double.parseDouble(commandLine.getOptionValue(\n          WORKLOAD_RATE_FACTOR_ARG, WORKLOAD_RATE_FACTOR_DEFAULT));\n      workloadExtraConfigs \u003d new HashMap\u003c\u003e();\n      if (commandLine.getOptionValues(WORKLOAD_CONFIG_ARG) !\u003d null) {\n        for (String opt : commandLine.getOptionValues(WORKLOAD_CONFIG_ARG)) {\n          Iterator\u003cString\u003e kvPair \u003d\n              Splitter.on(\"\u003d\").trimResults().split(opt).iterator();\n          workloadExtraConfigs.put(kvPair.next(), kvPair.next());\n        }\n      }\n      String delayString \u003d commandLine.getOptionValue(WORKLOAD_START_DELAY_ARG,\n          WorkloadDriver.START_TIME_OFFSET_DEFAULT);\n      // Store a temporary config to leverage Configuration\u0027s time duration\n      // parsing.\n      getConf().set(\"___temp___\", delayString);\n      workloadStartDelayMs \u003d getConf().getTimeDuration(\"___temp___\", 0,\n          TimeUnit.MILLISECONDS);\n    }\n\n    return true;\n  }",
      "path": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/Client.java",
      "extendedDetails": {}
    },
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-12345 Add Dynamometer to hadoop-tools, a tool for scale testing the HDFS NameNode with real metadata and workloads. Contributed by Erik Krogen.\n",
      "commitDate": "25/06/19 8:07 AM",
      "commitName": "ab0b180ddb5d0775a2452d5eeb7badd252aadb91",
      "commitAuthor": "Erik Krogen",
      "diff": "@@ -0,0 +1,114 @@\n+  public boolean init(String[] args) throws ParseException, IOException {\n+\n+    CommandLineParser parser \u003d new GnuParser();\n+    if (parser.parse(\n+        new Options().addOption(\"h\", \"help\", false, \"Shows this message.\"),\n+        args, true).hasOption(\"h\")) {\n+      printUsage();\n+      return false;\n+    }\n+\n+    CommandLine cliParser \u003d parser.parse(opts, args);\n+\n+    yarnClient \u003d YarnClient.createYarnClient();\n+    yarnClient.init(getConf());\n+\n+    LOG.info(\"Starting with arguments: [\\\"{}\\\"]\",\n+        Joiner.on(\"\\\" \\\"\").join(args));\n+\n+    Path fsImageDir \u003d new Path(cliParser.getOptionValue(FS_IMAGE_DIR_ARG, \"\"));\n+    versionFilePath \u003d new Path(fsImageDir, \"VERSION\").toString();\n+    if (cliParser.hasOption(NAMENODE_SERVICERPC_ADDR_ARG)) {\n+      launchNameNode \u003d false;\n+      remoteNameNodeRpcAddress \u003d\n+          cliParser.getOptionValue(NAMENODE_SERVICERPC_ADDR_ARG);\n+    } else {\n+      launchNameNode \u003d true;\n+      FileSystem localFS \u003d FileSystem.getLocal(getConf());\n+      fsImageDir \u003d fsImageDir.makeQualified(localFS.getUri(),\n+          localFS.getWorkingDirectory());\n+      FileSystem fsImageFS \u003d fsImageDir.getFileSystem(getConf());\n+      FileStatus[] fsImageFiles \u003d fsImageFS.listStatus(fsImageDir,\n+          (path) -\u003e path.getName().matches(\"^fsimage_(\\\\d)+$\"));\n+      if (fsImageFiles.length !\u003d 1) {\n+        throw new IllegalArgumentException(\n+            \"Must be exactly one fsimage file present in fs_image_dir\");\n+      }\n+      fsImagePath \u003d fsImageFiles[0].getPath().toString();\n+      fsImageMD5Path \u003d fsImageFiles[0].getPath().suffix(\".md5\").toString();\n+    }\n+\n+    if (amMemory \u003c 0) {\n+      throw new IllegalArgumentException(\"Invalid memory specified for \"\n+          + \"application master, exiting. Specified memory\u003d\" + amMemory);\n+    }\n+    if (amVCores \u003c 0) {\n+      throw new IllegalArgumentException(\"Invalid virtual cores specified for \"\n+          + \"application master, exiting. Specified virtual cores\u003d\" + amVCores);\n+    }\n+\n+    this.appName \u003d cliParser.getOptionValue(APPNAME_ARG, APPNAME_DEFAULT);\n+    this.amQueue \u003d cliParser.getOptionValue(QUEUE_ARG, QUEUE_DEFAULT);\n+    this.amMemory \u003d Integer.parseInt(cliParser\n+        .getOptionValue(MASTER_MEMORY_MB_ARG, MASTER_MEMORY_MB_DEFAULT));\n+    this.amVCores \u003d Integer.parseInt(\n+        cliParser.getOptionValue(MASTER_VCORES_ARG, MASTER_VCORES_DEFAULT));\n+    this.confPath \u003d cliParser.getOptionValue(CONF_PATH_ARG);\n+    this.blockListPath \u003d cliParser.getOptionValue(BLOCK_LIST_PATH_ARG);\n+    if (cliParser.hasOption(HADOOP_BINARY_PATH_ARG)) {\n+      this.hadoopBinary \u003d cliParser.getOptionValue(HADOOP_BINARY_PATH_ARG);\n+    } else {\n+      this.hadoopBinary \u003d DynoInfraUtils.fetchHadoopTarball(\n+          new File(\".\").getAbsoluteFile(),\n+          cliParser.getOptionValue(HADOOP_VERSION_ARG), getConf(), LOG)\n+          .toString();\n+    }\n+    this.amOptions \u003d AMOptions.initFromParser(cliParser);\n+    this.clientTimeout \u003d Integer\n+        .parseInt(cliParser.getOptionValue(TIMEOUT_ARG, TIMEOUT_DEFAULT));\n+    this.tokenFileLocation \u003d cliParser.getOptionValue(TOKEN_FILE_LOCATION_ARG);\n+\n+    amOptions.verify();\n+\n+    Path blockPath \u003d new Path(blockListPath);\n+    FileSystem blockListFS \u003d blockPath.getFileSystem(getConf());\n+    if (blockListFS.getUri().equals(FileSystem.getLocal(getConf()).getUri())\n+        || !blockListFS.exists(blockPath)) {\n+      throw new IllegalArgumentException(\n+          \"block list path must already exist on remote fs!\");\n+    }\n+    numTotalDataNodes \u003d blockListFS.listStatus(blockPath,\n+        DynoConstants.BLOCK_LIST_FILE_FILTER).length;\n+\n+    if (cliParser.hasOption(WORKLOAD_REPLAY_ENABLE_ARG)) {\n+      if (!cliParser.hasOption(WORKLOAD_INPUT_PATH_ARG)\n+          || !cliParser.hasOption(WORKLOAD_START_DELAY_ARG)) {\n+        throw new IllegalArgumentException(\"workload_replay_enable was \"\n+            + \"specified; must include all required workload_ parameters.\");\n+      }\n+      launchWorkloadJob \u003d true;\n+      workloadInputPath \u003d cliParser.getOptionValue(WORKLOAD_INPUT_PATH_ARG);\n+      workloadThreadsPerMapper \u003d Integer\n+          .parseInt(cliParser.getOptionValue(WORKLOAD_THREADS_PER_MAPPER_ARG,\n+              String.valueOf(AuditReplayMapper.NUM_THREADS_DEFAULT)));\n+      workloadRateFactor \u003d Double.parseDouble(cliParser.getOptionValue(\n+          WORKLOAD_RATE_FACTOR_ARG, WORKLOAD_RATE_FACTOR_DEFAULT));\n+      workloadExtraConfigs \u003d new HashMap\u003c\u003e();\n+      if (cliParser.getOptionValues(WORKLOAD_CONFIG_ARG) !\u003d null) {\n+        for (String opt : cliParser.getOptionValues(WORKLOAD_CONFIG_ARG)) {\n+          Iterator\u003cString\u003e kvPair \u003d\n+              Splitter.on(\"\u003d\").trimResults().split(opt).iterator();\n+          workloadExtraConfigs.put(kvPair.next(), kvPair.next());\n+        }\n+      }\n+      String delayString \u003d cliParser.getOptionValue(WORKLOAD_START_DELAY_ARG,\n+          WorkloadDriver.START_TIME_OFFSET_DEFAULT);\n+      // Store a temporary config to leverage Configuration\u0027s time duration\n+      // parsing.\n+      getConf().set(\"___temp___\", delayString);\n+      workloadStartDelayMs \u003d getConf().getTimeDuration(\"___temp___\", 0,\n+          TimeUnit.MILLISECONDS);\n+    }\n+\n+    return true;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean init(String[] args) throws ParseException, IOException {\n\n    CommandLineParser parser \u003d new GnuParser();\n    if (parser.parse(\n        new Options().addOption(\"h\", \"help\", false, \"Shows this message.\"),\n        args, true).hasOption(\"h\")) {\n      printUsage();\n      return false;\n    }\n\n    CommandLine cliParser \u003d parser.parse(opts, args);\n\n    yarnClient \u003d YarnClient.createYarnClient();\n    yarnClient.init(getConf());\n\n    LOG.info(\"Starting with arguments: [\\\"{}\\\"]\",\n        Joiner.on(\"\\\" \\\"\").join(args));\n\n    Path fsImageDir \u003d new Path(cliParser.getOptionValue(FS_IMAGE_DIR_ARG, \"\"));\n    versionFilePath \u003d new Path(fsImageDir, \"VERSION\").toString();\n    if (cliParser.hasOption(NAMENODE_SERVICERPC_ADDR_ARG)) {\n      launchNameNode \u003d false;\n      remoteNameNodeRpcAddress \u003d\n          cliParser.getOptionValue(NAMENODE_SERVICERPC_ADDR_ARG);\n    } else {\n      launchNameNode \u003d true;\n      FileSystem localFS \u003d FileSystem.getLocal(getConf());\n      fsImageDir \u003d fsImageDir.makeQualified(localFS.getUri(),\n          localFS.getWorkingDirectory());\n      FileSystem fsImageFS \u003d fsImageDir.getFileSystem(getConf());\n      FileStatus[] fsImageFiles \u003d fsImageFS.listStatus(fsImageDir,\n          (path) -\u003e path.getName().matches(\"^fsimage_(\\\\d)+$\"));\n      if (fsImageFiles.length !\u003d 1) {\n        throw new IllegalArgumentException(\n            \"Must be exactly one fsimage file present in fs_image_dir\");\n      }\n      fsImagePath \u003d fsImageFiles[0].getPath().toString();\n      fsImageMD5Path \u003d fsImageFiles[0].getPath().suffix(\".md5\").toString();\n    }\n\n    if (amMemory \u003c 0) {\n      throw new IllegalArgumentException(\"Invalid memory specified for \"\n          + \"application master, exiting. Specified memory\u003d\" + amMemory);\n    }\n    if (amVCores \u003c 0) {\n      throw new IllegalArgumentException(\"Invalid virtual cores specified for \"\n          + \"application master, exiting. Specified virtual cores\u003d\" + amVCores);\n    }\n\n    this.appName \u003d cliParser.getOptionValue(APPNAME_ARG, APPNAME_DEFAULT);\n    this.amQueue \u003d cliParser.getOptionValue(QUEUE_ARG, QUEUE_DEFAULT);\n    this.amMemory \u003d Integer.parseInt(cliParser\n        .getOptionValue(MASTER_MEMORY_MB_ARG, MASTER_MEMORY_MB_DEFAULT));\n    this.amVCores \u003d Integer.parseInt(\n        cliParser.getOptionValue(MASTER_VCORES_ARG, MASTER_VCORES_DEFAULT));\n    this.confPath \u003d cliParser.getOptionValue(CONF_PATH_ARG);\n    this.blockListPath \u003d cliParser.getOptionValue(BLOCK_LIST_PATH_ARG);\n    if (cliParser.hasOption(HADOOP_BINARY_PATH_ARG)) {\n      this.hadoopBinary \u003d cliParser.getOptionValue(HADOOP_BINARY_PATH_ARG);\n    } else {\n      this.hadoopBinary \u003d DynoInfraUtils.fetchHadoopTarball(\n          new File(\".\").getAbsoluteFile(),\n          cliParser.getOptionValue(HADOOP_VERSION_ARG), getConf(), LOG)\n          .toString();\n    }\n    this.amOptions \u003d AMOptions.initFromParser(cliParser);\n    this.clientTimeout \u003d Integer\n        .parseInt(cliParser.getOptionValue(TIMEOUT_ARG, TIMEOUT_DEFAULT));\n    this.tokenFileLocation \u003d cliParser.getOptionValue(TOKEN_FILE_LOCATION_ARG);\n\n    amOptions.verify();\n\n    Path blockPath \u003d new Path(blockListPath);\n    FileSystem blockListFS \u003d blockPath.getFileSystem(getConf());\n    if (blockListFS.getUri().equals(FileSystem.getLocal(getConf()).getUri())\n        || !blockListFS.exists(blockPath)) {\n      throw new IllegalArgumentException(\n          \"block list path must already exist on remote fs!\");\n    }\n    numTotalDataNodes \u003d blockListFS.listStatus(blockPath,\n        DynoConstants.BLOCK_LIST_FILE_FILTER).length;\n\n    if (cliParser.hasOption(WORKLOAD_REPLAY_ENABLE_ARG)) {\n      if (!cliParser.hasOption(WORKLOAD_INPUT_PATH_ARG)\n          || !cliParser.hasOption(WORKLOAD_START_DELAY_ARG)) {\n        throw new IllegalArgumentException(\"workload_replay_enable was \"\n            + \"specified; must include all required workload_ parameters.\");\n      }\n      launchWorkloadJob \u003d true;\n      workloadInputPath \u003d cliParser.getOptionValue(WORKLOAD_INPUT_PATH_ARG);\n      workloadThreadsPerMapper \u003d Integer\n          .parseInt(cliParser.getOptionValue(WORKLOAD_THREADS_PER_MAPPER_ARG,\n              String.valueOf(AuditReplayMapper.NUM_THREADS_DEFAULT)));\n      workloadRateFactor \u003d Double.parseDouble(cliParser.getOptionValue(\n          WORKLOAD_RATE_FACTOR_ARG, WORKLOAD_RATE_FACTOR_DEFAULT));\n      workloadExtraConfigs \u003d new HashMap\u003c\u003e();\n      if (cliParser.getOptionValues(WORKLOAD_CONFIG_ARG) !\u003d null) {\n        for (String opt : cliParser.getOptionValues(WORKLOAD_CONFIG_ARG)) {\n          Iterator\u003cString\u003e kvPair \u003d\n              Splitter.on(\"\u003d\").trimResults().split(opt).iterator();\n          workloadExtraConfigs.put(kvPair.next(), kvPair.next());\n        }\n      }\n      String delayString \u003d cliParser.getOptionValue(WORKLOAD_START_DELAY_ARG,\n          WorkloadDriver.START_TIME_OFFSET_DEFAULT);\n      // Store a temporary config to leverage Configuration\u0027s time duration\n      // parsing.\n      getConf().set(\"___temp___\", delayString);\n      workloadStartDelayMs \u003d getConf().getTimeDuration(\"___temp___\", 0,\n          TimeUnit.MILLISECONDS);\n    }\n\n    return true;\n  }",
      "path": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-infra/src/main/java/org/apache/hadoop/tools/dynamometer/Client.java"
    }
  }
}