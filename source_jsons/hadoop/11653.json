{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetCache.java",
  "functionName": "uncacheBlock",
  "functionId": "uncacheBlock___bpid-String__blockId-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
  "functionStartLine": 291,
  "functionEndLine": 337,
  "numCommitsSeen": 33,
  "timeTaken": 4198,
  "changeHistory": [
    "659c88801d008bb352d10a1cb3bd0e401486cc9b",
    "ed72daa5df97669906234e8ac9a406d78136b206",
    "cad14aa9168112ef1ceae80b94d9aae3ba293578",
    "93e23a99157c30b51752fc49748c3c210745a187",
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f",
    "f0d64a078da7e932b9509734f75170e3e525e68c",
    "9673baa7e8b43fa6300080f72ebce0189ea775e5",
    "97199baea1c41a66bd2a88bda31742ef6ddcb5dc",
    "15d08c4778350a86d7bae0174aeb48f8d8f61cce",
    "40eb94ade3161d93e7a762a839004748f6d0ae89",
    "b992219fa13ccee2b417d91222fd0c3e8c3ffe11"
  ],
  "changeHistoryShort": {
    "659c88801d008bb352d10a1cb3bd0e401486cc9b": "Ybodychange",
    "ed72daa5df97669906234e8ac9a406d78136b206": "Ybodychange",
    "cad14aa9168112ef1ceae80b94d9aae3ba293578": "Ybodychange",
    "93e23a99157c30b51752fc49748c3c210745a187": "Ybodychange",
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f": "Ybodychange",
    "f0d64a078da7e932b9509734f75170e3e525e68c": "Ybodychange",
    "9673baa7e8b43fa6300080f72ebce0189ea775e5": "Ybodychange",
    "97199baea1c41a66bd2a88bda31742ef6ddcb5dc": "Ymultichange(Ymodifierchange,Ybodychange)",
    "15d08c4778350a86d7bae0174aeb48f8d8f61cce": "Ymultichange(Yparameterchange,Ybodychange)",
    "40eb94ade3161d93e7a762a839004748f6d0ae89": "Ybodychange",
    "b992219fa13ccee2b417d91222fd0c3e8c3ffe11": "Yintroduced"
  },
  "changeHistoryDetails": {
    "659c88801d008bb352d10a1cb3bd0e401486cc9b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14818. Check native pmdk lib by \u0027hadoop checknative\u0027 command. Contributed by Feilong He.\n",
      "commitDate": "22/09/19 9:32 AM",
      "commitName": "659c88801d008bb352d10a1cb3bd0e401486cc9b",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "15/07/19 12:32 AM",
      "commitNameOld": "e98adb00b7da8fa913b86ecf2049444b1d8617d4",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 69.38,
      "commitsBetweenForRepo": 622,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,47 @@\n   synchronized void uncacheBlock(String bpid, long blockId) {\n     ExtendedBlockId key \u003d new ExtendedBlockId(blockId, bpid);\n     Value prevValue \u003d mappableBlockMap.get(key);\n     boolean deferred \u003d false;\n \n-    if (!dataset.datanode.getShortCircuitRegistry().\n-            processBlockMunlockRequest(key)) {\n+    if (cacheLoader.isTransientCache() \u0026\u0026 !dataset.datanode.\n+        getShortCircuitRegistry().processBlockMunlockRequest(key)) {\n       deferred \u003d true;\n     }\n     if (prevValue \u003d\u003d null) {\n       LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n           + \"because it is not currently in the mappableBlockMap.\", blockId,\n           bpid);\n       numBlocksFailedToUncache.incrementAndGet();\n       return;\n     }\n     switch (prevValue.state) {\n     case CACHING:\n       LOG.debug(\"Cancelling caching for block with id {}, pool {}.\", blockId,\n           bpid);\n       mappableBlockMap.put(key,\n           new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n       break;\n     case CACHED:\n       mappableBlockMap.put(key,\n           new Value(prevValue.mappableBlock, State.UNCACHING));\n       if (deferred) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"{} is anchored, and can\u0027t be uncached now.  Scheduling it \" +\n                   \"for uncaching in {} \",\n               key, DurationFormatUtils.formatDurationHMS(revocationPollingMs));\n         }\n         deferredUncachingExecutor.schedule(\n             new UncachingTask(key, revocationMs),\n             revocationPollingMs, TimeUnit.MILLISECONDS);\n       } else {\n         LOG.debug(\"{} has been scheduled for immediate uncaching.\", key);\n         uncachingExecutor.execute(new UncachingTask(key, 0));\n       }\n       break;\n     default:\n       LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n           + \"because it is in state {}.\", blockId, bpid, prevValue.state);\n       numBlocksFailedToUncache.incrementAndGet();\n       break;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void uncacheBlock(String bpid, long blockId) {\n    ExtendedBlockId key \u003d new ExtendedBlockId(blockId, bpid);\n    Value prevValue \u003d mappableBlockMap.get(key);\n    boolean deferred \u003d false;\n\n    if (cacheLoader.isTransientCache() \u0026\u0026 !dataset.datanode.\n        getShortCircuitRegistry().processBlockMunlockRequest(key)) {\n      deferred \u003d true;\n    }\n    if (prevValue \u003d\u003d null) {\n      LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n          + \"because it is not currently in the mappableBlockMap.\", blockId,\n          bpid);\n      numBlocksFailedToUncache.incrementAndGet();\n      return;\n    }\n    switch (prevValue.state) {\n    case CACHING:\n      LOG.debug(\"Cancelling caching for block with id {}, pool {}.\", blockId,\n          bpid);\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n      break;\n    case CACHED:\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.UNCACHING));\n      if (deferred) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"{} is anchored, and can\u0027t be uncached now.  Scheduling it \" +\n                  \"for uncaching in {} \",\n              key, DurationFormatUtils.formatDurationHMS(revocationPollingMs));\n        }\n        deferredUncachingExecutor.schedule(\n            new UncachingTask(key, revocationMs),\n            revocationPollingMs, TimeUnit.MILLISECONDS);\n      } else {\n        LOG.debug(\"{} has been scheduled for immediate uncaching.\", key);\n        uncachingExecutor.execute(new UncachingTask(key, 0));\n      }\n      break;\n    default:\n      LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n          + \"because it is in state {}.\", blockId, bpid, prevValue.state);\n      numBlocksFailedToUncache.incrementAndGet();\n      break;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "ed72daa5df97669906234e8ac9a406d78136b206": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7978. Add LOG.isDebugEnabled() guard for some LOG.debug(..). Contributed by Walter Su.\n",
      "commitDate": "01/04/15 12:54 PM",
      "commitName": "ed72daa5df97669906234e8ac9a406d78136b206",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "09/11/14 5:55 PM",
      "commitNameOld": "4ddc5cad0a4175f7f5ef9504a7365601dc7e63b4",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 142.75,
      "commitsBetweenForRepo": 1108,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,47 @@\n   synchronized void uncacheBlock(String bpid, long blockId) {\n     ExtendedBlockId key \u003d new ExtendedBlockId(blockId, bpid);\n     Value prevValue \u003d mappableBlockMap.get(key);\n     boolean deferred \u003d false;\n \n     if (!dataset.datanode.getShortCircuitRegistry().\n             processBlockMunlockRequest(key)) {\n       deferred \u003d true;\n     }\n     if (prevValue \u003d\u003d null) {\n       LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n           + \"because it is not currently in the mappableBlockMap.\", blockId,\n           bpid);\n       numBlocksFailedToUncache.incrementAndGet();\n       return;\n     }\n     switch (prevValue.state) {\n     case CACHING:\n       LOG.debug(\"Cancelling caching for block with id {}, pool {}.\", blockId,\n           bpid);\n       mappableBlockMap.put(key,\n           new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n       break;\n     case CACHED:\n       mappableBlockMap.put(key,\n           new Value(prevValue.mappableBlock, State.UNCACHING));\n       if (deferred) {\n-        LOG.debug(\"{} is anchored, and can\u0027t be uncached now.  Scheduling it \" +\n-            \"for uncaching in {} \",\n-            key, DurationFormatUtils.formatDurationHMS(revocationPollingMs));\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"{} is anchored, and can\u0027t be uncached now.  Scheduling it \" +\n+                  \"for uncaching in {} \",\n+              key, DurationFormatUtils.formatDurationHMS(revocationPollingMs));\n+        }\n         deferredUncachingExecutor.schedule(\n             new UncachingTask(key, revocationMs),\n             revocationPollingMs, TimeUnit.MILLISECONDS);\n       } else {\n         LOG.debug(\"{} has been scheduled for immediate uncaching.\", key);\n         uncachingExecutor.execute(new UncachingTask(key, 0));\n       }\n       break;\n     default:\n       LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n           + \"because it is in state {}.\", blockId, bpid, prevValue.state);\n       numBlocksFailedToUncache.incrementAndGet();\n       break;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void uncacheBlock(String bpid, long blockId) {\n    ExtendedBlockId key \u003d new ExtendedBlockId(blockId, bpid);\n    Value prevValue \u003d mappableBlockMap.get(key);\n    boolean deferred \u003d false;\n\n    if (!dataset.datanode.getShortCircuitRegistry().\n            processBlockMunlockRequest(key)) {\n      deferred \u003d true;\n    }\n    if (prevValue \u003d\u003d null) {\n      LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n          + \"because it is not currently in the mappableBlockMap.\", blockId,\n          bpid);\n      numBlocksFailedToUncache.incrementAndGet();\n      return;\n    }\n    switch (prevValue.state) {\n    case CACHING:\n      LOG.debug(\"Cancelling caching for block with id {}, pool {}.\", blockId,\n          bpid);\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n      break;\n    case CACHED:\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.UNCACHING));\n      if (deferred) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"{} is anchored, and can\u0027t be uncached now.  Scheduling it \" +\n                  \"for uncaching in {} \",\n              key, DurationFormatUtils.formatDurationHMS(revocationPollingMs));\n        }\n        deferredUncachingExecutor.schedule(\n            new UncachingTask(key, revocationMs),\n            revocationPollingMs, TimeUnit.MILLISECONDS);\n      } else {\n        LOG.debug(\"{} has been scheduled for immediate uncaching.\", key);\n        uncachingExecutor.execute(new UncachingTask(key, 0));\n      }\n      break;\n    default:\n      LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n          + \"because it is in state {}.\", blockId, bpid, prevValue.state);\n      numBlocksFailedToUncache.incrementAndGet();\n      break;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "cad14aa9168112ef1ceae80b94d9aae3ba293578": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6036. Forcibly timeout misbehaving DFSClients that try to do no-checksum reads that extend too long.  (cmccabe)\n",
      "commitDate": "08/09/14 12:51 PM",
      "commitName": "cad14aa9168112ef1ceae80b94d9aae3ba293578",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "03/07/14 10:13 AM",
      "commitNameOld": "93e23a99157c30b51752fc49748c3c210745a187",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 67.11,
      "commitsBetweenForRepo": 528,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,45 @@\n   synchronized void uncacheBlock(String bpid, long blockId) {\n     ExtendedBlockId key \u003d new ExtendedBlockId(blockId, bpid);\n     Value prevValue \u003d mappableBlockMap.get(key);\n+    boolean deferred \u003d false;\n \n     if (!dataset.datanode.getShortCircuitRegistry().\n             processBlockMunlockRequest(key)) {\n-      // TODO: we probably want to forcibly uncache the block (and close the \n-      // shm) after a certain timeout has elapsed.\n-      LOG.debug(\"{} is anchored, and can\u0027t be uncached now.\", key);\n-      return;\n+      deferred \u003d true;\n     }\n     if (prevValue \u003d\u003d null) {\n       LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n           + \"because it is not currently in the mappableBlockMap.\", blockId,\n           bpid);\n       numBlocksFailedToUncache.incrementAndGet();\n       return;\n     }\n     switch (prevValue.state) {\n     case CACHING:\n       LOG.debug(\"Cancelling caching for block with id {}, pool {}.\", blockId,\n           bpid);\n       mappableBlockMap.put(key,\n           new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n       break;\n     case CACHED:\n-      LOG.debug(\n-          \"Block with id {}, pool {} has been scheduled for uncaching\" + \".\",\n-          blockId, bpid);\n       mappableBlockMap.put(key,\n           new Value(prevValue.mappableBlock, State.UNCACHING));\n-      uncachingExecutor.execute(new UncachingTask(key));\n+      if (deferred) {\n+        LOG.debug(\"{} is anchored, and can\u0027t be uncached now.  Scheduling it \" +\n+            \"for uncaching in {} \",\n+            key, DurationFormatUtils.formatDurationHMS(revocationPollingMs));\n+        deferredUncachingExecutor.schedule(\n+            new UncachingTask(key, revocationMs),\n+            revocationPollingMs, TimeUnit.MILLISECONDS);\n+      } else {\n+        LOG.debug(\"{} has been scheduled for immediate uncaching.\", key);\n+        uncachingExecutor.execute(new UncachingTask(key, 0));\n+      }\n       break;\n     default:\n       LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n           + \"because it is in state {}.\", blockId, bpid, prevValue.state);\n       numBlocksFailedToUncache.incrementAndGet();\n       break;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void uncacheBlock(String bpid, long blockId) {\n    ExtendedBlockId key \u003d new ExtendedBlockId(blockId, bpid);\n    Value prevValue \u003d mappableBlockMap.get(key);\n    boolean deferred \u003d false;\n\n    if (!dataset.datanode.getShortCircuitRegistry().\n            processBlockMunlockRequest(key)) {\n      deferred \u003d true;\n    }\n    if (prevValue \u003d\u003d null) {\n      LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n          + \"because it is not currently in the mappableBlockMap.\", blockId,\n          bpid);\n      numBlocksFailedToUncache.incrementAndGet();\n      return;\n    }\n    switch (prevValue.state) {\n    case CACHING:\n      LOG.debug(\"Cancelling caching for block with id {}, pool {}.\", blockId,\n          bpid);\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n      break;\n    case CACHED:\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.UNCACHING));\n      if (deferred) {\n        LOG.debug(\"{} is anchored, and can\u0027t be uncached now.  Scheduling it \" +\n            \"for uncaching in {} \",\n            key, DurationFormatUtils.formatDurationHMS(revocationPollingMs));\n        deferredUncachingExecutor.schedule(\n            new UncachingTask(key, revocationMs),\n            revocationPollingMs, TimeUnit.MILLISECONDS);\n      } else {\n        LOG.debug(\"{} has been scheduled for immediate uncaching.\", key);\n        uncachingExecutor.execute(new UncachingTask(key, 0));\n      }\n      break;\n    default:\n      LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n          + \"because it is in state {}.\", blockId, bpid, prevValue.state);\n      numBlocksFailedToUncache.incrementAndGet();\n      break;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "93e23a99157c30b51752fc49748c3c210745a187": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6613. Improve logging in caching classes. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1607697 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/07/14 10:13 AM",
      "commitName": "93e23a99157c30b51752fc49748c3c210745a187",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "09/04/14 2:45 PM",
      "commitNameOld": "5c48f379ab359ea7a7c2421df998080f3792a1d9",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 84.81,
      "commitsBetweenForRepo": 513,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,40 @@\n   synchronized void uncacheBlock(String bpid, long blockId) {\n     ExtendedBlockId key \u003d new ExtendedBlockId(blockId, bpid);\n     Value prevValue \u003d mappableBlockMap.get(key);\n \n     if (!dataset.datanode.getShortCircuitRegistry().\n             processBlockMunlockRequest(key)) {\n       // TODO: we probably want to forcibly uncache the block (and close the \n       // shm) after a certain timeout has elapsed.\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(key + \" is anchored, and can\u0027t be uncached now.\");\n-      }\n+      LOG.debug(\"{} is anchored, and can\u0027t be uncached now.\", key);\n       return;\n     }\n     if (prevValue \u003d\u003d null) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n-            \"does not need to be uncached, because it is not currently \" +\n-            \"in the mappableBlockMap.\");\n-      }\n+      LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n+          + \"because it is not currently in the mappableBlockMap.\", blockId,\n+          bpid);\n       numBlocksFailedToUncache.incrementAndGet();\n       return;\n     }\n     switch (prevValue.state) {\n     case CACHING:\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Cancelling caching for block with id \" + blockId +\n-            \", pool \" + bpid + \".\");\n-      }\n+      LOG.debug(\"Cancelling caching for block with id {}, pool {}.\", blockId,\n+          bpid);\n       mappableBlockMap.put(key,\n           new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n       break;\n     case CACHED:\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n-            \"has been scheduled for uncaching.\");\n-      }\n+      LOG.debug(\n+          \"Block with id {}, pool {} has been scheduled for uncaching\" + \".\",\n+          blockId, bpid);\n       mappableBlockMap.put(key,\n           new Value(prevValue.mappableBlock, State.UNCACHING));\n       uncachingExecutor.execute(new UncachingTask(key));\n       break;\n     default:\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n-            \"does not need to be uncached, because it is \" +\n-            \"in state \" + prevValue.state + \".\");\n-      }\n+      LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n+          + \"because it is in state {}.\", blockId, bpid, prevValue.state);\n       numBlocksFailedToUncache.incrementAndGet();\n       break;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void uncacheBlock(String bpid, long blockId) {\n    ExtendedBlockId key \u003d new ExtendedBlockId(blockId, bpid);\n    Value prevValue \u003d mappableBlockMap.get(key);\n\n    if (!dataset.datanode.getShortCircuitRegistry().\n            processBlockMunlockRequest(key)) {\n      // TODO: we probably want to forcibly uncache the block (and close the \n      // shm) after a certain timeout has elapsed.\n      LOG.debug(\"{} is anchored, and can\u0027t be uncached now.\", key);\n      return;\n    }\n    if (prevValue \u003d\u003d null) {\n      LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n          + \"because it is not currently in the mappableBlockMap.\", blockId,\n          bpid);\n      numBlocksFailedToUncache.incrementAndGet();\n      return;\n    }\n    switch (prevValue.state) {\n    case CACHING:\n      LOG.debug(\"Cancelling caching for block with id {}, pool {}.\", blockId,\n          bpid);\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n      break;\n    case CACHED:\n      LOG.debug(\n          \"Block with id {}, pool {} has been scheduled for uncaching\" + \".\",\n          blockId, bpid);\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.UNCACHING));\n      uncachingExecutor.execute(new UncachingTask(key));\n      break;\n    default:\n      LOG.debug(\"Block with id {}, pool {} does not need to be uncached, \"\n          + \"because it is in state {}.\", blockId, bpid, prevValue.state);\n      numBlocksFailedToUncache.incrementAndGet();\n      break;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5950. The DFSClient and DataNode should use shared memory segments to communicate short-circuit information (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1573433 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/03/14 7:58 PM",
      "commitName": "dd049a2f6097da189ccce2f5890a2b9bc77fa73f",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "12/02/14 7:10 PM",
      "commitNameOld": "f0d64a078da7e932b9509734f75170e3e525e68c",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 18.03,
      "commitsBetweenForRepo": 129,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,50 @@\n   synchronized void uncacheBlock(String bpid, long blockId) {\n     ExtendedBlockId key \u003d new ExtendedBlockId(blockId, bpid);\n     Value prevValue \u003d mappableBlockMap.get(key);\n \n+    if (!dataset.datanode.getShortCircuitRegistry().\n+            processBlockMunlockRequest(key)) {\n+      // TODO: we probably want to forcibly uncache the block (and close the \n+      // shm) after a certain timeout has elapsed.\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(key + \" is anchored, and can\u0027t be uncached now.\");\n+      }\n+      return;\n+    }\n     if (prevValue \u003d\u003d null) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n             \"does not need to be uncached, because it is not currently \" +\n             \"in the mappableBlockMap.\");\n       }\n       numBlocksFailedToUncache.incrementAndGet();\n       return;\n     }\n     switch (prevValue.state) {\n     case CACHING:\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Cancelling caching for block with id \" + blockId +\n             \", pool \" + bpid + \".\");\n       }\n       mappableBlockMap.put(key,\n           new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n       break;\n     case CACHED:\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n             \"has been scheduled for uncaching.\");\n       }\n       mappableBlockMap.put(key,\n           new Value(prevValue.mappableBlock, State.UNCACHING));\n       uncachingExecutor.execute(new UncachingTask(key));\n       break;\n     default:\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n             \"does not need to be uncached, because it is \" +\n             \"in state \" + prevValue.state + \".\");\n       }\n       numBlocksFailedToUncache.incrementAndGet();\n       break;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void uncacheBlock(String bpid, long blockId) {\n    ExtendedBlockId key \u003d new ExtendedBlockId(blockId, bpid);\n    Value prevValue \u003d mappableBlockMap.get(key);\n\n    if (!dataset.datanode.getShortCircuitRegistry().\n            processBlockMunlockRequest(key)) {\n      // TODO: we probably want to forcibly uncache the block (and close the \n      // shm) after a certain timeout has elapsed.\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(key + \" is anchored, and can\u0027t be uncached now.\");\n      }\n      return;\n    }\n    if (prevValue \u003d\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"does not need to be uncached, because it is not currently \" +\n            \"in the mappableBlockMap.\");\n      }\n      numBlocksFailedToUncache.incrementAndGet();\n      return;\n    }\n    switch (prevValue.state) {\n    case CACHING:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Cancelling caching for block with id \" + blockId +\n            \", pool \" + bpid + \".\");\n      }\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n      break;\n    case CACHED:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"has been scheduled for uncaching.\");\n      }\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.UNCACHING));\n      uncachingExecutor.execute(new UncachingTask(key));\n      break;\n    default:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"does not need to be uncached, because it is \" +\n            \"in state \" + prevValue.state + \".\");\n      }\n      numBlocksFailedToUncache.incrementAndGet();\n      break;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "f0d64a078da7e932b9509734f75170e3e525e68c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5940.  Minor cleanups to ShortCircuitReplica, FsDatasetCache, and DomainSocketWatcher (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567835 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 7:10 PM",
      "commitName": "f0d64a078da7e932b9509734f75170e3e525e68c",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "27/11/13 9:55 AM",
      "commitNameOld": "13edb391d06c479720202eb5ac81f1c71fe64748",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 77.39,
      "commitsBetweenForRepo": 431,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   synchronized void uncacheBlock(String bpid, long blockId) {\n-    Key key \u003d new Key(blockId, bpid);\n+    ExtendedBlockId key \u003d new ExtendedBlockId(blockId, bpid);\n     Value prevValue \u003d mappableBlockMap.get(key);\n \n     if (prevValue \u003d\u003d null) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n             \"does not need to be uncached, because it is not currently \" +\n             \"in the mappableBlockMap.\");\n       }\n       numBlocksFailedToUncache.incrementAndGet();\n       return;\n     }\n     switch (prevValue.state) {\n     case CACHING:\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Cancelling caching for block with id \" + blockId +\n             \", pool \" + bpid + \".\");\n       }\n       mappableBlockMap.put(key,\n           new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n       break;\n     case CACHED:\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n             \"has been scheduled for uncaching.\");\n       }\n       mappableBlockMap.put(key,\n           new Value(prevValue.mappableBlock, State.UNCACHING));\n       uncachingExecutor.execute(new UncachingTask(key));\n       break;\n     default:\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n             \"does not need to be uncached, because it is \" +\n             \"in state \" + prevValue.state + \".\");\n       }\n       numBlocksFailedToUncache.incrementAndGet();\n       break;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void uncacheBlock(String bpid, long blockId) {\n    ExtendedBlockId key \u003d new ExtendedBlockId(blockId, bpid);\n    Value prevValue \u003d mappableBlockMap.get(key);\n\n    if (prevValue \u003d\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"does not need to be uncached, because it is not currently \" +\n            \"in the mappableBlockMap.\");\n      }\n      numBlocksFailedToUncache.incrementAndGet();\n      return;\n    }\n    switch (prevValue.state) {\n    case CACHING:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Cancelling caching for block with id \" + blockId +\n            \", pool \" + bpid + \".\");\n      }\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n      break;\n    case CACHED:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"has been scheduled for uncaching.\");\n      }\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.UNCACHING));\n      uncachingExecutor.execute(new UncachingTask(key));\n      break;\n    default:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"does not need to be uncached, because it is \" +\n            \"in state \" + prevValue.state + \".\");\n      }\n      numBlocksFailedToUncache.incrementAndGet();\n      break;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "9673baa7e8b43fa6300080f72ebce0189ea775e5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5320. Add datanode caching metrics. Contributed by Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1540796 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/11/13 10:30 AM",
      "commitName": "9673baa7e8b43fa6300080f72ebce0189ea775e5",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "07/11/13 7:00 PM",
      "commitNameOld": "97199baea1c41a66bd2a88bda31742ef6ddcb5dc",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 3.65,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,41 @@\n   synchronized void uncacheBlock(String bpid, long blockId) {\n     Key key \u003d new Key(blockId, bpid);\n     Value prevValue \u003d mappableBlockMap.get(key);\n \n     if (prevValue \u003d\u003d null) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n             \"does not need to be uncached, because it is not currently \" +\n             \"in the mappableBlockMap.\");\n       }\n+      numBlocksFailedToUncache.incrementAndGet();\n       return;\n     }\n     switch (prevValue.state) {\n     case CACHING:\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Cancelling caching for block with id \" + blockId +\n             \", pool \" + bpid + \".\");\n       }\n       mappableBlockMap.put(key,\n           new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n       break;\n     case CACHED:\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n             \"has been scheduled for uncaching.\");\n       }\n       mappableBlockMap.put(key,\n           new Value(prevValue.mappableBlock, State.UNCACHING));\n       uncachingExecutor.execute(new UncachingTask(key));\n       break;\n     default:\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n             \"does not need to be uncached, because it is \" +\n             \"in state \" + prevValue.state + \".\");\n       }\n+      numBlocksFailedToUncache.incrementAndGet();\n       break;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized void uncacheBlock(String bpid, long blockId) {\n    Key key \u003d new Key(blockId, bpid);\n    Value prevValue \u003d mappableBlockMap.get(key);\n\n    if (prevValue \u003d\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"does not need to be uncached, because it is not currently \" +\n            \"in the mappableBlockMap.\");\n      }\n      numBlocksFailedToUncache.incrementAndGet();\n      return;\n    }\n    switch (prevValue.state) {\n    case CACHING:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Cancelling caching for block with id \" + blockId +\n            \", pool \" + bpid + \".\");\n      }\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n      break;\n    case CACHED:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"has been scheduled for uncaching.\");\n      }\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.UNCACHING));\n      uncachingExecutor.execute(new UncachingTask(key));\n      break;\n    default:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"does not need to be uncached, because it is \" +\n            \"in state \" + prevValue.state + \".\");\n      }\n      numBlocksFailedToUncache.incrementAndGet();\n      break;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "97199baea1c41a66bd2a88bda31742ef6ddcb5dc": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-5394: Fix race conditions in DN caching and uncaching (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1539909 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/11/13 7:00 PM",
      "commitName": "97199baea1c41a66bd2a88bda31742ef6ddcb5dc",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5394: Fix race conditions in DN caching and uncaching (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1539909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/11/13 7:00 PM",
          "commitName": "97199baea1c41a66bd2a88bda31742ef6ddcb5dc",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "21/10/13 12:29 PM",
          "commitNameOld": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 17.31,
          "commitsBetweenForRepo": 77,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,39 @@\n-  void uncacheBlock(String bpid, long blockId) {\n-    MappableBlock mapBlock \u003d cachedBlocks.get(blockId);\n-    if (mapBlock !\u003d null \u0026\u0026\n-        mapBlock.getBlockPoolId().equals(bpid) \u0026\u0026\n-        mapBlock.getBlock().getBlockId() \u003d\u003d blockId) {\n-      mapBlock.close();\n-      cachedBlocks.remove(blockId);\n-      long bytes \u003d mapBlock.getNumBytes();\n-      long used \u003d usedBytes.get();\n-      while (!usedBytes.compareAndSet(used, used - bytes)) {\n-        used \u003d usedBytes.get();\n+  synchronized void uncacheBlock(String bpid, long blockId) {\n+    Key key \u003d new Key(blockId, bpid);\n+    Value prevValue \u003d mappableBlockMap.get(key);\n+\n+    if (prevValue \u003d\u003d null) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n+            \"does not need to be uncached, because it is not currently \" +\n+            \"in the mappableBlockMap.\");\n       }\n-      LOG.info(\"Successfully uncached block \" + blockId);\n-    } else {\n-      LOG.info(\"Could not uncache block \" + blockId + \": unknown block.\");\n+      return;\n+    }\n+    switch (prevValue.state) {\n+    case CACHING:\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Cancelling caching for block with id \" + blockId +\n+            \", pool \" + bpid + \".\");\n+      }\n+      mappableBlockMap.put(key,\n+          new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n+      break;\n+    case CACHED:\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n+            \"has been scheduled for uncaching.\");\n+      }\n+      mappableBlockMap.put(key,\n+          new Value(prevValue.mappableBlock, State.UNCACHING));\n+      uncachingExecutor.execute(new UncachingTask(key));\n+      break;\n+    default:\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n+            \"does not need to be uncached, because it is \" +\n+            \"in state \" + prevValue.state + \".\");\n+      }\n+      break;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized void uncacheBlock(String bpid, long blockId) {\n    Key key \u003d new Key(blockId, bpid);\n    Value prevValue \u003d mappableBlockMap.get(key);\n\n    if (prevValue \u003d\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"does not need to be uncached, because it is not currently \" +\n            \"in the mappableBlockMap.\");\n      }\n      return;\n    }\n    switch (prevValue.state) {\n    case CACHING:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Cancelling caching for block with id \" + blockId +\n            \", pool \" + bpid + \".\");\n      }\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n      break;\n    case CACHED:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"has been scheduled for uncaching.\");\n      }\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.UNCACHING));\n      uncachingExecutor.execute(new UncachingTask(key));\n      break;\n    default:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"does not need to be uncached, because it is \" +\n            \"in state \" + prevValue.state + \".\");\n      }\n      break;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[synchronized]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5394: Fix race conditions in DN caching and uncaching (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1539909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/11/13 7:00 PM",
          "commitName": "97199baea1c41a66bd2a88bda31742ef6ddcb5dc",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "21/10/13 12:29 PM",
          "commitNameOld": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 17.31,
          "commitsBetweenForRepo": 77,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,39 @@\n-  void uncacheBlock(String bpid, long blockId) {\n-    MappableBlock mapBlock \u003d cachedBlocks.get(blockId);\n-    if (mapBlock !\u003d null \u0026\u0026\n-        mapBlock.getBlockPoolId().equals(bpid) \u0026\u0026\n-        mapBlock.getBlock().getBlockId() \u003d\u003d blockId) {\n-      mapBlock.close();\n-      cachedBlocks.remove(blockId);\n-      long bytes \u003d mapBlock.getNumBytes();\n-      long used \u003d usedBytes.get();\n-      while (!usedBytes.compareAndSet(used, used - bytes)) {\n-        used \u003d usedBytes.get();\n+  synchronized void uncacheBlock(String bpid, long blockId) {\n+    Key key \u003d new Key(blockId, bpid);\n+    Value prevValue \u003d mappableBlockMap.get(key);\n+\n+    if (prevValue \u003d\u003d null) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n+            \"does not need to be uncached, because it is not currently \" +\n+            \"in the mappableBlockMap.\");\n       }\n-      LOG.info(\"Successfully uncached block \" + blockId);\n-    } else {\n-      LOG.info(\"Could not uncache block \" + blockId + \": unknown block.\");\n+      return;\n+    }\n+    switch (prevValue.state) {\n+    case CACHING:\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Cancelling caching for block with id \" + blockId +\n+            \", pool \" + bpid + \".\");\n+      }\n+      mappableBlockMap.put(key,\n+          new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n+      break;\n+    case CACHED:\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n+            \"has been scheduled for uncaching.\");\n+      }\n+      mappableBlockMap.put(key,\n+          new Value(prevValue.mappableBlock, State.UNCACHING));\n+      uncachingExecutor.execute(new UncachingTask(key));\n+      break;\n+    default:\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n+            \"does not need to be uncached, because it is \" +\n+            \"in state \" + prevValue.state + \".\");\n+      }\n+      break;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  synchronized void uncacheBlock(String bpid, long blockId) {\n    Key key \u003d new Key(blockId, bpid);\n    Value prevValue \u003d mappableBlockMap.get(key);\n\n    if (prevValue \u003d\u003d null) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"does not need to be uncached, because it is not currently \" +\n            \"in the mappableBlockMap.\");\n      }\n      return;\n    }\n    switch (prevValue.state) {\n    case CACHING:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Cancelling caching for block with id \" + blockId +\n            \", pool \" + bpid + \".\");\n      }\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.CACHING_CANCELLED));\n      break;\n    case CACHED:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"has been scheduled for uncaching.\");\n      }\n      mappableBlockMap.put(key,\n          new Value(prevValue.mappableBlock, State.UNCACHING));\n      uncachingExecutor.execute(new UncachingTask(key));\n      break;\n    default:\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Block with id \" + blockId + \", pool \" + bpid + \" \" +\n            \"does not need to be uncached, because it is \" +\n            \"in state \" + prevValue.state + \".\");\n      }\n      break;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
          "extendedDetails": {}
        }
      ]
    },
    "15d08c4778350a86d7bae0174aeb48f8d8f61cce": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5349. DNA_CACHE and DNA_UNCACHE should be by blockId only (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532116 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/10/13 3:19 PM",
      "commitName": "15d08c4778350a86d7bae0174aeb48f8d8f61cce",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5349. DNA_CACHE and DNA_UNCACHE should be by blockId only (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532116 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/10/13 3:19 PM",
          "commitName": "15d08c4778350a86d7bae0174aeb48f8d8f61cce",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "13/09/13 4:27 PM",
          "commitNameOld": "40eb94ade3161d93e7a762a839004748f6d0ae89",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 30.95,
          "commitsBetweenForRepo": 18,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,17 @@\n-  void uncacheBlock(String bpid, Block block) {\n-    MappableBlock mapBlock \u003d cachedBlocks.get(block.getBlockId());\n+  void uncacheBlock(String bpid, long blockId) {\n+    MappableBlock mapBlock \u003d cachedBlocks.get(blockId);\n     if (mapBlock !\u003d null \u0026\u0026\n         mapBlock.getBlockPoolId().equals(bpid) \u0026\u0026\n-        mapBlock.getBlock().equals(block)) {\n+        mapBlock.getBlock().getBlockId() \u003d\u003d blockId) {\n       mapBlock.close();\n-      cachedBlocks.remove(block.getBlockId());\n+      cachedBlocks.remove(blockId);\n       long bytes \u003d mapBlock.getNumBytes();\n       long used \u003d usedBytes.get();\n       while (!usedBytes.compareAndSet(used, used - bytes)) {\n         used \u003d usedBytes.get();\n       }\n-      LOG.info(\"Successfully uncached block \" + block);\n+      LOG.info(\"Successfully uncached block \" + blockId);\n     } else {\n-      LOG.info(\"Could not uncache block \" + block + \": unknown block.\");\n+      LOG.info(\"Could not uncache block \" + blockId + \": unknown block.\");\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void uncacheBlock(String bpid, long blockId) {\n    MappableBlock mapBlock \u003d cachedBlocks.get(blockId);\n    if (mapBlock !\u003d null \u0026\u0026\n        mapBlock.getBlockPoolId().equals(bpid) \u0026\u0026\n        mapBlock.getBlock().getBlockId() \u003d\u003d blockId) {\n      mapBlock.close();\n      cachedBlocks.remove(blockId);\n      long bytes \u003d mapBlock.getNumBytes();\n      long used \u003d usedBytes.get();\n      while (!usedBytes.compareAndSet(used, used - bytes)) {\n        used \u003d usedBytes.get();\n      }\n      LOG.info(\"Successfully uncached block \" + blockId);\n    } else {\n      LOG.info(\"Could not uncache block \" + blockId + \": unknown block.\");\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
          "extendedDetails": {
            "oldValue": "[bpid-String, block-Block]",
            "newValue": "[bpid-String, blockId-long]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5349. DNA_CACHE and DNA_UNCACHE should be by blockId only (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532116 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "14/10/13 3:19 PM",
          "commitName": "15d08c4778350a86d7bae0174aeb48f8d8f61cce",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "13/09/13 4:27 PM",
          "commitNameOld": "40eb94ade3161d93e7a762a839004748f6d0ae89",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 30.95,
          "commitsBetweenForRepo": 18,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,17 @@\n-  void uncacheBlock(String bpid, Block block) {\n-    MappableBlock mapBlock \u003d cachedBlocks.get(block.getBlockId());\n+  void uncacheBlock(String bpid, long blockId) {\n+    MappableBlock mapBlock \u003d cachedBlocks.get(blockId);\n     if (mapBlock !\u003d null \u0026\u0026\n         mapBlock.getBlockPoolId().equals(bpid) \u0026\u0026\n-        mapBlock.getBlock().equals(block)) {\n+        mapBlock.getBlock().getBlockId() \u003d\u003d blockId) {\n       mapBlock.close();\n-      cachedBlocks.remove(block.getBlockId());\n+      cachedBlocks.remove(blockId);\n       long bytes \u003d mapBlock.getNumBytes();\n       long used \u003d usedBytes.get();\n       while (!usedBytes.compareAndSet(used, used - bytes)) {\n         used \u003d usedBytes.get();\n       }\n-      LOG.info(\"Successfully uncached block \" + block);\n+      LOG.info(\"Successfully uncached block \" + blockId);\n     } else {\n-      LOG.info(\"Could not uncache block \" + block + \": unknown block.\");\n+      LOG.info(\"Could not uncache block \" + blockId + \": unknown block.\");\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void uncacheBlock(String bpid, long blockId) {\n    MappableBlock mapBlock \u003d cachedBlocks.get(blockId);\n    if (mapBlock !\u003d null \u0026\u0026\n        mapBlock.getBlockPoolId().equals(bpid) \u0026\u0026\n        mapBlock.getBlock().getBlockId() \u003d\u003d blockId) {\n      mapBlock.close();\n      cachedBlocks.remove(blockId);\n      long bytes \u003d mapBlock.getNumBytes();\n      long used \u003d usedBytes.get();\n      while (!usedBytes.compareAndSet(used, used - bytes)) {\n        used \u003d usedBytes.get();\n      }\n      LOG.info(\"Successfully uncached block \" + blockId);\n    } else {\n      LOG.info(\"Could not uncache block \" + blockId + \": unknown block.\");\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
          "extendedDetails": {}
        }
      ]
    },
    "40eb94ade3161d93e7a762a839004748f6d0ae89": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5053. NameNode should invoke DataNode APIs to coordinate caching. (Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1523145 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/09/13 4:27 PM",
      "commitName": "40eb94ade3161d93e7a762a839004748f6d0ae89",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "23/08/13 8:41 PM",
      "commitNameOld": "b992219fa13ccee2b417d91222fd0c3e8c3ffe11",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 20.82,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,17 @@\n   void uncacheBlock(String bpid, Block block) {\n     MappableBlock mapBlock \u003d cachedBlocks.get(block.getBlockId());\n     if (mapBlock !\u003d null \u0026\u0026\n         mapBlock.getBlockPoolId().equals(bpid) \u0026\u0026\n         mapBlock.getBlock().equals(block)) {\n       mapBlock.close();\n-      cachedBlocks.remove(mapBlock);\n+      cachedBlocks.remove(block.getBlockId());\n       long bytes \u003d mapBlock.getNumBytes();\n       long used \u003d usedBytes.get();\n       while (!usedBytes.compareAndSet(used, used - bytes)) {\n         used \u003d usedBytes.get();\n       }\n+      LOG.info(\"Successfully uncached block \" + block);\n+    } else {\n+      LOG.info(\"Could not uncache block \" + block + \": unknown block.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void uncacheBlock(String bpid, Block block) {\n    MappableBlock mapBlock \u003d cachedBlocks.get(block.getBlockId());\n    if (mapBlock !\u003d null \u0026\u0026\n        mapBlock.getBlockPoolId().equals(bpid) \u0026\u0026\n        mapBlock.getBlock().equals(block)) {\n      mapBlock.close();\n      cachedBlocks.remove(block.getBlockId());\n      long bytes \u003d mapBlock.getNumBytes();\n      long used \u003d usedBytes.get();\n      while (!usedBytes.compareAndSet(used, used - bytes)) {\n        used \u003d usedBytes.get();\n      }\n      LOG.info(\"Successfully uncached block \" + block);\n    } else {\n      LOG.info(\"Could not uncache block \" + block + \": unknown block.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
      "extendedDetails": {}
    },
    "b992219fa13ccee2b417d91222fd0c3e8c3ffe11": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5050.  Add DataNode support for mlock and munlock  (contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1517106 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/08/13 8:41 PM",
      "commitName": "b992219fa13ccee2b417d91222fd0c3e8c3ffe11",
      "commitAuthor": "Colin McCabe",
      "diff": "@@ -0,0 +1,14 @@\n+  void uncacheBlock(String bpid, Block block) {\n+    MappableBlock mapBlock \u003d cachedBlocks.get(block.getBlockId());\n+    if (mapBlock !\u003d null \u0026\u0026\n+        mapBlock.getBlockPoolId().equals(bpid) \u0026\u0026\n+        mapBlock.getBlock().equals(block)) {\n+      mapBlock.close();\n+      cachedBlocks.remove(mapBlock);\n+      long bytes \u003d mapBlock.getNumBytes();\n+      long used \u003d usedBytes.get();\n+      while (!usedBytes.compareAndSet(used, used - bytes)) {\n+        used \u003d usedBytes.get();\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void uncacheBlock(String bpid, Block block) {\n    MappableBlock mapBlock \u003d cachedBlocks.get(block.getBlockId());\n    if (mapBlock !\u003d null \u0026\u0026\n        mapBlock.getBlockPoolId().equals(bpid) \u0026\u0026\n        mapBlock.getBlock().equals(block)) {\n      mapBlock.close();\n      cachedBlocks.remove(mapBlock);\n      long bytes \u003d mapBlock.getNumBytes();\n      long used \u003d usedBytes.get();\n      while (!usedBytes.compareAndSet(used, used - bytes)) {\n        used \u003d usedBytes.get();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java"
    }
  }
}