{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FileDistributionCalculator.java",
  "functionName": "run",
  "functionId": "run___in-InputStream",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FileDistributionCalculator.java",
  "functionStartLine": 118,
  "functionEndLine": 151,
  "numCommitsSeen": 9,
  "timeTaken": 2031,
  "changeHistory": [
    "204a2055b1b9270ae13ea03b7aeac62b65166efd",
    "fec1e2eed95e34209bde1da7efd8fdda8951b85d",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305"
  ],
  "changeHistoryShort": {
    "204a2055b1b9270ae13ea03b7aeac62b65166efd": "Ybodychange",
    "fec1e2eed95e34209bde1da7efd8fdda8951b85d": "Ybodychange",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": "Yintroduced"
  },
  "changeHistoryDetails": {
    "204a2055b1b9270ae13ea03b7aeac62b65166efd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10691. FileDistribution fails in hdfs oiv command due to ArrayIndexOutOfBoundsException. Contributed by Yiqun Lin.\n",
      "commitDate": "28/07/16 11:39 PM",
      "commitName": "204a2055b1b9270ae13ea03b7aeac62b65166efd",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "11/12/14 12:36 PM",
      "commitNameOld": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 595.42,
      "commitsBetweenForRepo": 4558,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,34 @@\n   private void run(InputStream in) throws IOException {\n     INodeSection s \u003d INodeSection.parseDelimitedFrom(in);\n     for (int i \u003d 0; i \u003c s.getNumInodes(); ++i) {\n       INodeSection.INode p \u003d INodeSection.INode.parseDelimitedFrom(in);\n       if (p.getType() \u003d\u003d INodeSection.INode.Type.FILE) {\n         ++totalFiles;\n         INodeSection.INodeFile f \u003d p.getFile();\n         totalBlocks +\u003d f.getBlocksCount();\n         long fileSize \u003d 0;\n         for (BlockProto b : f.getBlocksList()) {\n           fileSize +\u003d b.getNumBytes();\n         }\n         maxFileSize \u003d Math.max(fileSize, maxFileSize);\n         totalSpace +\u003d fileSize * f.getReplication();\n \n         int bucket \u003d fileSize \u003e maxSize ? distribution.length - 1 : (int) Math\n             .ceil((double)fileSize / steps);\n+        // Compare the bucket value with distribution\u0027s length again,\n+        // because sometimes the bucket value will be equal to\n+        // the length when maxSize can\u0027t be divided completely by step.\n+        if (bucket \u003e\u003d distribution.length) {\n+          bucket \u003d distribution.length - 1;\n+        }\n         ++distribution[bucket];\n \n       } else if (p.getType() \u003d\u003d INodeSection.INode.Type.DIRECTORY) {\n         ++totalDirectories;\n       }\n \n       if (i % (1 \u003c\u003c 20) \u003d\u003d 0) {\n         out.println(\"Processed \" + i + \" inodes.\");\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void run(InputStream in) throws IOException {\n    INodeSection s \u003d INodeSection.parseDelimitedFrom(in);\n    for (int i \u003d 0; i \u003c s.getNumInodes(); ++i) {\n      INodeSection.INode p \u003d INodeSection.INode.parseDelimitedFrom(in);\n      if (p.getType() \u003d\u003d INodeSection.INode.Type.FILE) {\n        ++totalFiles;\n        INodeSection.INodeFile f \u003d p.getFile();\n        totalBlocks +\u003d f.getBlocksCount();\n        long fileSize \u003d 0;\n        for (BlockProto b : f.getBlocksList()) {\n          fileSize +\u003d b.getNumBytes();\n        }\n        maxFileSize \u003d Math.max(fileSize, maxFileSize);\n        totalSpace +\u003d fileSize * f.getReplication();\n\n        int bucket \u003d fileSize \u003e maxSize ? distribution.length - 1 : (int) Math\n            .ceil((double)fileSize / steps);\n        // Compare the bucket value with distribution\u0027s length again,\n        // because sometimes the bucket value will be equal to\n        // the length when maxSize can\u0027t be divided completely by step.\n        if (bucket \u003e\u003d distribution.length) {\n          bucket \u003d distribution.length - 1;\n        }\n        ++distribution[bucket];\n\n      } else if (p.getType() \u003d\u003d INodeSection.INode.Type.DIRECTORY) {\n        ++totalDirectories;\n      }\n\n      if (i % (1 \u003c\u003c 20) \u003d\u003d 0) {\n        out.println(\"Processed \" + i + \" inodes.\");\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FileDistributionCalculator.java",
      "extendedDetails": {}
    },
    "fec1e2eed95e34209bde1da7efd8fdda8951b85d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5956. A file size is multiplied by the replication factor in \u0027hdfs oiv -p FileDistribution\u0027 option. Contributed by Akira Ajisaka.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1573078 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/02/14 1:07 PM",
      "commitName": "fec1e2eed95e34209bde1da7efd8fdda8951b85d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "09/02/14 11:18 AM",
      "commitNameOld": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 19.08,
      "commitsBetweenForRepo": 167,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   private void run(InputStream in) throws IOException {\n     INodeSection s \u003d INodeSection.parseDelimitedFrom(in);\n     for (int i \u003d 0; i \u003c s.getNumInodes(); ++i) {\n       INodeSection.INode p \u003d INodeSection.INode.parseDelimitedFrom(in);\n       if (p.getType() \u003d\u003d INodeSection.INode.Type.FILE) {\n         ++totalFiles;\n         INodeSection.INodeFile f \u003d p.getFile();\n         totalBlocks +\u003d f.getBlocksCount();\n         long fileSize \u003d 0;\n         for (BlockProto b : f.getBlocksList()) {\n-          fileSize +\u003d b.getNumBytes() * f.getReplication();\n+          fileSize +\u003d b.getNumBytes();\n         }\n         maxFileSize \u003d Math.max(fileSize, maxFileSize);\n-        totalSpace +\u003d fileSize;\n+        totalSpace +\u003d fileSize * f.getReplication();\n \n         int bucket \u003d fileSize \u003e maxSize ? distribution.length - 1 : (int) Math\n             .ceil((double)fileSize / steps);\n         ++distribution[bucket];\n \n       } else if (p.getType() \u003d\u003d INodeSection.INode.Type.DIRECTORY) {\n         ++totalDirectories;\n       }\n \n       if (i % (1 \u003c\u003c 20) \u003d\u003d 0) {\n         out.println(\"Processed \" + i + \" inodes.\");\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void run(InputStream in) throws IOException {\n    INodeSection s \u003d INodeSection.parseDelimitedFrom(in);\n    for (int i \u003d 0; i \u003c s.getNumInodes(); ++i) {\n      INodeSection.INode p \u003d INodeSection.INode.parseDelimitedFrom(in);\n      if (p.getType() \u003d\u003d INodeSection.INode.Type.FILE) {\n        ++totalFiles;\n        INodeSection.INodeFile f \u003d p.getFile();\n        totalBlocks +\u003d f.getBlocksCount();\n        long fileSize \u003d 0;\n        for (BlockProto b : f.getBlocksList()) {\n          fileSize +\u003d b.getNumBytes();\n        }\n        maxFileSize \u003d Math.max(fileSize, maxFileSize);\n        totalSpace +\u003d fileSize * f.getReplication();\n\n        int bucket \u003d fileSize \u003e maxSize ? distribution.length - 1 : (int) Math\n            .ceil((double)fileSize / steps);\n        ++distribution[bucket];\n\n      } else if (p.getType() \u003d\u003d INodeSection.INode.Type.DIRECTORY) {\n        ++totalDirectories;\n      }\n\n      if (i % (1 \u003c\u003c 20) \u003d\u003d 0) {\n        out.println(\"Processed \" + i + \" inodes.\");\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FileDistributionCalculator.java",
      "extendedDetails": {}
    },
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5698. Use protobuf to serialize / deserialize FSImage. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/02/14 11:18 AM",
      "commitName": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,28 @@\n+  private void run(InputStream in) throws IOException {\n+    INodeSection s \u003d INodeSection.parseDelimitedFrom(in);\n+    for (int i \u003d 0; i \u003c s.getNumInodes(); ++i) {\n+      INodeSection.INode p \u003d INodeSection.INode.parseDelimitedFrom(in);\n+      if (p.getType() \u003d\u003d INodeSection.INode.Type.FILE) {\n+        ++totalFiles;\n+        INodeSection.INodeFile f \u003d p.getFile();\n+        totalBlocks +\u003d f.getBlocksCount();\n+        long fileSize \u003d 0;\n+        for (BlockProto b : f.getBlocksList()) {\n+          fileSize +\u003d b.getNumBytes() * f.getReplication();\n+        }\n+        maxFileSize \u003d Math.max(fileSize, maxFileSize);\n+        totalSpace +\u003d fileSize;\n+\n+        int bucket \u003d fileSize \u003e maxSize ? distribution.length - 1 : (int) Math\n+            .ceil((double)fileSize / steps);\n+        ++distribution[bucket];\n+\n+      } else if (p.getType() \u003d\u003d INodeSection.INode.Type.DIRECTORY) {\n+        ++totalDirectories;\n+      }\n+\n+      if (i % (1 \u003c\u003c 20) \u003d\u003d 0) {\n+        out.println(\"Processed \" + i + \" inodes.\");\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void run(InputStream in) throws IOException {\n    INodeSection s \u003d INodeSection.parseDelimitedFrom(in);\n    for (int i \u003d 0; i \u003c s.getNumInodes(); ++i) {\n      INodeSection.INode p \u003d INodeSection.INode.parseDelimitedFrom(in);\n      if (p.getType() \u003d\u003d INodeSection.INode.Type.FILE) {\n        ++totalFiles;\n        INodeSection.INodeFile f \u003d p.getFile();\n        totalBlocks +\u003d f.getBlocksCount();\n        long fileSize \u003d 0;\n        for (BlockProto b : f.getBlocksList()) {\n          fileSize +\u003d b.getNumBytes() * f.getReplication();\n        }\n        maxFileSize \u003d Math.max(fileSize, maxFileSize);\n        totalSpace +\u003d fileSize;\n\n        int bucket \u003d fileSize \u003e maxSize ? distribution.length - 1 : (int) Math\n            .ceil((double)fileSize / steps);\n        ++distribution[bucket];\n\n      } else if (p.getType() \u003d\u003d INodeSection.INode.Type.DIRECTORY) {\n        ++totalDirectories;\n      }\n\n      if (i % (1 \u003c\u003c 20) \u003d\u003d 0) {\n        out.println(\"Processed \" + i + \" inodes.\");\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FileDistributionCalculator.java"
    }
  }
}