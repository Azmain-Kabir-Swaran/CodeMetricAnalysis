{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ReplicationWork.java",
  "functionName": "addTaskToDatanode",
  "functionId": "addTaskToDatanode___numberReplicas-NumberReplicas",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ReplicationWork.java",
  "functionStartLine": 63,
  "functionEndLine": 65,
  "numCommitsSeen": 18,
  "timeTaken": 1902,
  "changeHistory": [
    "743a99f2dbc9a27e19f92ff3551937d90dba2e89",
    "e54cc2931262bf49682a8323da9811976218c03b"
  ],
  "changeHistoryShort": {
    "743a99f2dbc9a27e19f92ff3551937d90dba2e89": "Yparameterchange",
    "e54cc2931262bf49682a8323da9811976218c03b": "Yintroduced"
  },
  "changeHistoryDetails": {
    "743a99f2dbc9a27e19f92ff3551937d90dba2e89": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-8786. Erasure coding: use simple replication for internal blocks on decommissioning datanodes. Contributed by Rakesh R.\n",
      "commitDate": "08/03/16 10:24 AM",
      "commitName": "743a99f2dbc9a27e19f92ff3551937d90dba2e89",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "19/02/16 7:02 PM",
      "commitNameOld": "e54cc2931262bf49682a8323da9811976218c03b",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 17.64,
      "commitsBetweenForRepo": 114,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,3 +1,3 @@\n-  void addTaskToDatanode() {\n+  void addTaskToDatanode(NumberReplicas numberReplicas) {\n     getSrcNodes()[0].addBlockToBeReplicated(getBlock(), getTargets());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void addTaskToDatanode(NumberReplicas numberReplicas) {\n    getSrcNodes()[0].addBlockToBeReplicated(getBlock(), getTargets());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ReplicationWork.java",
      "extendedDetails": {
        "oldValue": "[]",
        "newValue": "[numberReplicas-NumberReplicas]"
      }
    },
    "e54cc2931262bf49682a8323da9811976218c03b": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9818. Correctly handle EC reconstruction work caused by not enough racks. Contributed by Jing Zhao.\n",
      "commitDate": "19/02/16 7:02 PM",
      "commitName": "e54cc2931262bf49682a8323da9811976218c03b",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,3 @@\n+  void addTaskToDatanode() {\n+    getSrcNodes()[0].addBlockToBeReplicated(getBlock(), getTargets());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void addTaskToDatanode() {\n    getSrcNodes()[0].addBlockToBeReplicated(getBlock(), getTargets());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ReplicationWork.java"
    }
  }
}