{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStreamer.java",
  "functionName": "addDatanode2ExistingPipeline",
  "functionId": "addDatanode2ExistingPipeline",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
  "functionStartLine": 1322,
  "functionEndLine": 1424,
  "numCommitsSeen": 156,
  "timeTaken": 10525,
  "changeHistory": [
    "685cb83e4c3f433c5147e35217ce79ea520a0da5",
    "fda1221c55101d97ac62e1ee4e3ddf9a915d5363",
    "a3954ccab148bddc290cb96528e63ff19799bcc9",
    "627da6f7178e18aa41996969c408b6f344e297d1",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "4c9497cbf02ecc82532a4e79e18912d8e0eb4731",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
    "552b4fb9f9a76b18605322c0b0e8072613d67773",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54",
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc",
    "3e3fea7cd433a77ab2b92a5035ffc4bdea02c6cf",
    "abf09f090f77a7e54e331b7a07354e7926b60dc9",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "685cb83e4c3f433c5147e35217ce79ea520a0da5": "Ybodychange",
    "fda1221c55101d97ac62e1ee4e3ddf9a915d5363": "Ybodychange",
    "a3954ccab148bddc290cb96528e63ff19799bcc9": "Ybodychange",
    "627da6f7178e18aa41996969c408b6f344e297d1": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "4c9497cbf02ecc82532a4e79e18912d8e0eb4731": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": "Ymultichange(Ymovefromfile,Ybodychange)",
    "552b4fb9f9a76b18605322c0b0e8072613d67773": "Ybodychange",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": "Ybodychange",
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc": "Ybodychange",
    "3e3fea7cd433a77ab2b92a5035ffc4bdea02c6cf": "Ybodychange",
    "abf09f090f77a7e54e331b7a07354e7926b60dc9": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "685cb83e4c3f433c5147e35217ce79ea520a0da5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14433. Remove the extra empty space in the DataStreamer logging. Contributed by Yishuang Lu. (#747)\n\n",
      "commitDate": "17/04/19 10:38 AM",
      "commitName": "685cb83e4c3f433c5147e35217ce79ea520a0da5",
      "commitAuthor": "lys0716",
      "commitDateOld": "28/03/19 11:16 AM",
      "commitNameOld": "49b02d4a9bf9beac19f716488348ea4e30563ff4",
      "commitAuthorOld": "Giovanni Matteo Fumarola",
      "daysBetweenCommits": 19.97,
      "commitsBetweenForRepo": 157,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,103 +1,103 @@\n   private void addDatanode2ExistingPipeline() throws IOException {\n     DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n       /*\n        * Is data transfer necessary?  We have the following cases.\n        *\n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n        *    + If there are data written, transfer RBW. This case may happens\n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n        *\n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n     if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n         \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n       //no data have been written\n       return;\n     } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n         || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n       //pipeline is closing\n       return;\n     }\n \n     int tried \u003d 0;\n     final DatanodeInfo[] original \u003d nodes;\n     final StorageType[] originalTypes \u003d storageTypes;\n     final String[] originalIDs \u003d storageIDs;\n     IOException caughtException \u003d null;\n     ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003c\u003e(failed);\n     while (tried \u003c 3) {\n       LocatedBlock lb;\n       //get a new datanode\n       lb \u003d dfsClient.namenode.getAdditionalDatanode(\n           src, stat.getFileId(), block.getCurrentBlock(), nodes, storageIDs,\n           exclude.toArray(new DatanodeInfo[exclude.size()]),\n           1, dfsClient.clientName);\n       // a new node was allocated by the namenode. Update nodes.\n       setPipeline(lb);\n \n       //find the new datanode\n       final int d;\n       try {\n         d \u003d findNewDatanode(original);\n       } catch (IOException ioe) {\n         // check the minimal number of nodes available to decide whether to\n         // continue the write.\n \n         //if live block location datanodes is greater than or equal to\n         // HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.\n         // MIN_REPLICATION threshold value, continue writing to the\n         // remaining nodes. Otherwise throw exception.\n         //\n         // If HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.\n         // MIN_REPLICATION is set to 0 or less than zero, an exception will be\n         // thrown if a replacement could not be found.\n \n         if (dfsClient.dtpReplaceDatanodeOnFailureReplication \u003e 0 \u0026\u0026 nodes.length\n             \u003e\u003d dfsClient.dtpReplaceDatanodeOnFailureReplication) {\n           DFSClient.LOG.warn(\n-              \"Failed to find a new datanode to add to the write pipeline, \"\n+              \"Failed to find a new datanode to add to the write pipeline,\"\n                   + \" continue to write to the pipeline with \" + nodes.length\n                   + \" nodes since it\u0027s no less than minimum replication: \"\n                   + dfsClient.dtpReplaceDatanodeOnFailureReplication\n                   + \" configured by \"\n                   + BlockWrite.ReplaceDatanodeOnFailure.MIN_REPLICATION\n                   + \".\", ioe);\n           return;\n         }\n         throw ioe;\n       }\n       //transfer replica. pick a source from the original nodes\n       final DatanodeInfo src \u003d original[tried % original.length];\n       final DatanodeInfo[] targets \u003d {nodes[d]};\n       final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n       final String[] targetStorageIDs \u003d {storageIDs[d]};\n \n       try {\n         transfer(src, targets, targetStorageTypes, targetStorageIDs,\n             lb.getBlockToken());\n       } catch (IOException ioe) {\n         DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n             nodes[d] + \": \" + ioe.getMessage());\n         caughtException \u003d ioe;\n         // add the allocated node to the exclude list.\n         exclude.add(nodes[d]);\n         setPipeline(original, originalTypes, originalIDs);\n         tried++;\n         continue;\n       }\n       return; // finished successfully\n     }\n     // All retries failed\n     throw (caughtException !\u003d null) ? caughtException :\n         new IOException(\"Failed to add a node\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addDatanode2ExistingPipeline() throws IOException {\n    DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       *\n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens\n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       *\n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n    if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n        \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n      //no data have been written\n      return;\n    } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n        || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n      //pipeline is closing\n      return;\n    }\n\n    int tried \u003d 0;\n    final DatanodeInfo[] original \u003d nodes;\n    final StorageType[] originalTypes \u003d storageTypes;\n    final String[] originalIDs \u003d storageIDs;\n    IOException caughtException \u003d null;\n    ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003c\u003e(failed);\n    while (tried \u003c 3) {\n      LocatedBlock lb;\n      //get a new datanode\n      lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, stat.getFileId(), block.getCurrentBlock(), nodes, storageIDs,\n          exclude.toArray(new DatanodeInfo[exclude.size()]),\n          1, dfsClient.clientName);\n      // a new node was allocated by the namenode. Update nodes.\n      setPipeline(lb);\n\n      //find the new datanode\n      final int d;\n      try {\n        d \u003d findNewDatanode(original);\n      } catch (IOException ioe) {\n        // check the minimal number of nodes available to decide whether to\n        // continue the write.\n\n        //if live block location datanodes is greater than or equal to\n        // HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.\n        // MIN_REPLICATION threshold value, continue writing to the\n        // remaining nodes. Otherwise throw exception.\n        //\n        // If HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.\n        // MIN_REPLICATION is set to 0 or less than zero, an exception will be\n        // thrown if a replacement could not be found.\n\n        if (dfsClient.dtpReplaceDatanodeOnFailureReplication \u003e 0 \u0026\u0026 nodes.length\n            \u003e\u003d dfsClient.dtpReplaceDatanodeOnFailureReplication) {\n          DFSClient.LOG.warn(\n              \"Failed to find a new datanode to add to the write pipeline,\"\n                  + \" continue to write to the pipeline with \" + nodes.length\n                  + \" nodes since it\u0027s no less than minimum replication: \"\n                  + dfsClient.dtpReplaceDatanodeOnFailureReplication\n                  + \" configured by \"\n                  + BlockWrite.ReplaceDatanodeOnFailure.MIN_REPLICATION\n                  + \".\", ioe);\n          return;\n        }\n        throw ioe;\n      }\n      //transfer replica. pick a source from the original nodes\n      final DatanodeInfo src \u003d original[tried % original.length];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n      final String[] targetStorageIDs \u003d {storageIDs[d]};\n\n      try {\n        transfer(src, targets, targetStorageTypes, targetStorageIDs,\n            lb.getBlockToken());\n      } catch (IOException ioe) {\n        DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n            nodes[d] + \": \" + ioe.getMessage());\n        caughtException \u003d ioe;\n        // add the allocated node to the exclude list.\n        exclude.add(nodes[d]);\n        setPipeline(original, originalTypes, originalIDs);\n        tried++;\n        continue;\n      }\n      return; // finished successfully\n    }\n    // All retries failed\n    throw (caughtException !\u003d null) ? caughtException :\n        new IOException(\"Failed to add a node\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "fda1221c55101d97ac62e1ee4e3ddf9a915d5363": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11799. Introduce a config to allow setting up write pipeline with fewer nodes than replication factor. Contributed by Brahma Reddy Battula\n",
      "commitDate": "18/09/17 10:55 PM",
      "commitName": "fda1221c55101d97ac62e1ee4e3ddf9a915d5363",
      "commitAuthor": "Brahma Reddy Battula",
      "commitDateOld": "31/08/17 11:18 PM",
      "commitNameOld": "36f33a1efb35e9f6986516499b54fdfa38fac2a1",
      "commitAuthorOld": "Mingliang Liu",
      "daysBetweenCommits": 17.98,
      "commitsBetweenForRepo": 186,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,103 @@\n   private void addDatanode2ExistingPipeline() throws IOException {\n     DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n       /*\n        * Is data transfer necessary?  We have the following cases.\n        *\n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n        *    + If there are data written, transfer RBW. This case may happens\n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n        *\n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n     if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n         \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n       //no data have been written\n       return;\n     } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n         || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n       //pipeline is closing\n       return;\n     }\n \n     int tried \u003d 0;\n     final DatanodeInfo[] original \u003d nodes;\n     final StorageType[] originalTypes \u003d storageTypes;\n     final String[] originalIDs \u003d storageIDs;\n     IOException caughtException \u003d null;\n     ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003c\u003e(failed);\n     while (tried \u003c 3) {\n       LocatedBlock lb;\n       //get a new datanode\n       lb \u003d dfsClient.namenode.getAdditionalDatanode(\n           src, stat.getFileId(), block.getCurrentBlock(), nodes, storageIDs,\n           exclude.toArray(new DatanodeInfo[exclude.size()]),\n           1, dfsClient.clientName);\n       // a new node was allocated by the namenode. Update nodes.\n       setPipeline(lb);\n \n       //find the new datanode\n-      final int d \u003d findNewDatanode(original);\n+      final int d;\n+      try {\n+        d \u003d findNewDatanode(original);\n+      } catch (IOException ioe) {\n+        // check the minimal number of nodes available to decide whether to\n+        // continue the write.\n+\n+        //if live block location datanodes is greater than or equal to\n+        // HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.\n+        // MIN_REPLICATION threshold value, continue writing to the\n+        // remaining nodes. Otherwise throw exception.\n+        //\n+        // If HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.\n+        // MIN_REPLICATION is set to 0 or less than zero, an exception will be\n+        // thrown if a replacement could not be found.\n+\n+        if (dfsClient.dtpReplaceDatanodeOnFailureReplication \u003e 0 \u0026\u0026 nodes.length\n+            \u003e\u003d dfsClient.dtpReplaceDatanodeOnFailureReplication) {\n+          DFSClient.LOG.warn(\n+              \"Failed to find a new datanode to add to the write pipeline, \"\n+                  + \" continue to write to the pipeline with \" + nodes.length\n+                  + \" nodes since it\u0027s no less than minimum replication: \"\n+                  + dfsClient.dtpReplaceDatanodeOnFailureReplication\n+                  + \" configured by \"\n+                  + BlockWrite.ReplaceDatanodeOnFailure.MIN_REPLICATION\n+                  + \".\", ioe);\n+          return;\n+        }\n+        throw ioe;\n+      }\n       //transfer replica. pick a source from the original nodes\n       final DatanodeInfo src \u003d original[tried % original.length];\n       final DatanodeInfo[] targets \u003d {nodes[d]};\n       final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n       final String[] targetStorageIDs \u003d {storageIDs[d]};\n \n       try {\n         transfer(src, targets, targetStorageTypes, targetStorageIDs,\n             lb.getBlockToken());\n       } catch (IOException ioe) {\n         DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n             nodes[d] + \": \" + ioe.getMessage());\n         caughtException \u003d ioe;\n         // add the allocated node to the exclude list.\n         exclude.add(nodes[d]);\n         setPipeline(original, originalTypes, originalIDs);\n         tried++;\n         continue;\n       }\n       return; // finished successfully\n     }\n     // All retries failed\n     throw (caughtException !\u003d null) ? caughtException :\n         new IOException(\"Failed to add a node\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addDatanode2ExistingPipeline() throws IOException {\n    DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       *\n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens\n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       *\n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n    if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n        \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n      //no data have been written\n      return;\n    } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n        || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n      //pipeline is closing\n      return;\n    }\n\n    int tried \u003d 0;\n    final DatanodeInfo[] original \u003d nodes;\n    final StorageType[] originalTypes \u003d storageTypes;\n    final String[] originalIDs \u003d storageIDs;\n    IOException caughtException \u003d null;\n    ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003c\u003e(failed);\n    while (tried \u003c 3) {\n      LocatedBlock lb;\n      //get a new datanode\n      lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, stat.getFileId(), block.getCurrentBlock(), nodes, storageIDs,\n          exclude.toArray(new DatanodeInfo[exclude.size()]),\n          1, dfsClient.clientName);\n      // a new node was allocated by the namenode. Update nodes.\n      setPipeline(lb);\n\n      //find the new datanode\n      final int d;\n      try {\n        d \u003d findNewDatanode(original);\n      } catch (IOException ioe) {\n        // check the minimal number of nodes available to decide whether to\n        // continue the write.\n\n        //if live block location datanodes is greater than or equal to\n        // HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.\n        // MIN_REPLICATION threshold value, continue writing to the\n        // remaining nodes. Otherwise throw exception.\n        //\n        // If HdfsClientConfigKeys.BlockWrite.ReplaceDatanodeOnFailure.\n        // MIN_REPLICATION is set to 0 or less than zero, an exception will be\n        // thrown if a replacement could not be found.\n\n        if (dfsClient.dtpReplaceDatanodeOnFailureReplication \u003e 0 \u0026\u0026 nodes.length\n            \u003e\u003d dfsClient.dtpReplaceDatanodeOnFailureReplication) {\n          DFSClient.LOG.warn(\n              \"Failed to find a new datanode to add to the write pipeline, \"\n                  + \" continue to write to the pipeline with \" + nodes.length\n                  + \" nodes since it\u0027s no less than minimum replication: \"\n                  + dfsClient.dtpReplaceDatanodeOnFailureReplication\n                  + \" configured by \"\n                  + BlockWrite.ReplaceDatanodeOnFailure.MIN_REPLICATION\n                  + \".\", ioe);\n          return;\n        }\n        throw ioe;\n      }\n      //transfer replica. pick a source from the original nodes\n      final DatanodeInfo src \u003d original[tried % original.length];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n      final String[] targetStorageIDs \u003d {storageIDs[d]};\n\n      try {\n        transfer(src, targets, targetStorageTypes, targetStorageIDs,\n            lb.getBlockToken());\n      } catch (IOException ioe) {\n        DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n            nodes[d] + \": \" + ioe.getMessage());\n        caughtException \u003d ioe;\n        // add the allocated node to the exclude list.\n        exclude.add(nodes[d]);\n        setPipeline(original, originalTypes, originalIDs);\n        tried++;\n        continue;\n      }\n      return; // finished successfully\n    }\n    // All retries failed\n    throw (caughtException !\u003d null) ? caughtException :\n        new IOException(\"Failed to add a node\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "a3954ccab148bddc290cb96528e63ff19799bcc9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
      "commitDate": "05/05/17 12:01 PM",
      "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "15/02/17 10:44 AM",
      "commitNameOld": "627da6f7178e18aa41996969c408b6f344e297d1",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 79.01,
      "commitsBetweenForRepo": 465,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,72 +1,74 @@\n   private void addDatanode2ExistingPipeline() throws IOException {\n     DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n       /*\n        * Is data transfer necessary?  We have the following cases.\n        *\n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n        *    + If there are data written, transfer RBW. This case may happens\n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n        *\n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n     if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n         \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n       //no data have been written\n       return;\n     } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n         || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n       //pipeline is closing\n       return;\n     }\n \n     int tried \u003d 0;\n     final DatanodeInfo[] original \u003d nodes;\n     final StorageType[] originalTypes \u003d storageTypes;\n     final String[] originalIDs \u003d storageIDs;\n     IOException caughtException \u003d null;\n     ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003c\u003e(failed);\n     while (tried \u003c 3) {\n       LocatedBlock lb;\n       //get a new datanode\n       lb \u003d dfsClient.namenode.getAdditionalDatanode(\n           src, stat.getFileId(), block.getCurrentBlock(), nodes, storageIDs,\n           exclude.toArray(new DatanodeInfo[exclude.size()]),\n           1, dfsClient.clientName);\n       // a new node was allocated by the namenode. Update nodes.\n       setPipeline(lb);\n \n       //find the new datanode\n       final int d \u003d findNewDatanode(original);\n       //transfer replica. pick a source from the original nodes\n       final DatanodeInfo src \u003d original[tried % original.length];\n       final DatanodeInfo[] targets \u003d {nodes[d]};\n       final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n+      final String[] targetStorageIDs \u003d {storageIDs[d]};\n \n       try {\n-        transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n+        transfer(src, targets, targetStorageTypes, targetStorageIDs,\n+            lb.getBlockToken());\n       } catch (IOException ioe) {\n         DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n             nodes[d] + \": \" + ioe.getMessage());\n         caughtException \u003d ioe;\n         // add the allocated node to the exclude list.\n         exclude.add(nodes[d]);\n         setPipeline(original, originalTypes, originalIDs);\n         tried++;\n         continue;\n       }\n       return; // finished successfully\n     }\n     // All retries failed\n     throw (caughtException !\u003d null) ? caughtException :\n         new IOException(\"Failed to add a node\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addDatanode2ExistingPipeline() throws IOException {\n    DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       *\n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens\n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       *\n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n    if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n        \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n      //no data have been written\n      return;\n    } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n        || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n      //pipeline is closing\n      return;\n    }\n\n    int tried \u003d 0;\n    final DatanodeInfo[] original \u003d nodes;\n    final StorageType[] originalTypes \u003d storageTypes;\n    final String[] originalIDs \u003d storageIDs;\n    IOException caughtException \u003d null;\n    ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003c\u003e(failed);\n    while (tried \u003c 3) {\n      LocatedBlock lb;\n      //get a new datanode\n      lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, stat.getFileId(), block.getCurrentBlock(), nodes, storageIDs,\n          exclude.toArray(new DatanodeInfo[exclude.size()]),\n          1, dfsClient.clientName);\n      // a new node was allocated by the namenode. Update nodes.\n      setPipeline(lb);\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n      //transfer replica. pick a source from the original nodes\n      final DatanodeInfo src \u003d original[tried % original.length];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n      final String[] targetStorageIDs \u003d {storageIDs[d]};\n\n      try {\n        transfer(src, targets, targetStorageTypes, targetStorageIDs,\n            lb.getBlockToken());\n      } catch (IOException ioe) {\n        DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n            nodes[d] + \": \" + ioe.getMessage());\n        caughtException \u003d ioe;\n        // add the allocated node to the exclude list.\n        exclude.add(nodes[d]);\n        setPipeline(original, originalTypes, originalIDs);\n        tried++;\n        continue;\n      }\n      return; // finished successfully\n    }\n    // All retries failed\n    throw (caughtException !\u003d null) ? caughtException :\n        new IOException(\"Failed to add a node\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "627da6f7178e18aa41996969c408b6f344e297d1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8498. Blocks can be committed with wrong size. Contributed by Jing Zhao.\n",
      "commitDate": "15/02/17 10:44 AM",
      "commitName": "627da6f7178e18aa41996969c408b6f344e297d1",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "02/02/17 10:08 AM",
      "commitNameOld": "0914fcca312b5e9d20bcf1b6633bc13c9034ba46",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 13.03,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,72 +1,72 @@\n   private void addDatanode2ExistingPipeline() throws IOException {\n     DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n       /*\n        * Is data transfer necessary?  We have the following cases.\n        *\n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n        *    + If there are data written, transfer RBW. This case may happens\n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n        *\n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n     if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n         \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n       //no data have been written\n       return;\n     } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n         || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n       //pipeline is closing\n       return;\n     }\n \n     int tried \u003d 0;\n     final DatanodeInfo[] original \u003d nodes;\n     final StorageType[] originalTypes \u003d storageTypes;\n     final String[] originalIDs \u003d storageIDs;\n     IOException caughtException \u003d null;\n     ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003c\u003e(failed);\n     while (tried \u003c 3) {\n       LocatedBlock lb;\n       //get a new datanode\n       lb \u003d dfsClient.namenode.getAdditionalDatanode(\n-          src, stat.getFileId(), block, nodes, storageIDs,\n+          src, stat.getFileId(), block.getCurrentBlock(), nodes, storageIDs,\n           exclude.toArray(new DatanodeInfo[exclude.size()]),\n           1, dfsClient.clientName);\n       // a new node was allocated by the namenode. Update nodes.\n       setPipeline(lb);\n \n       //find the new datanode\n       final int d \u003d findNewDatanode(original);\n       //transfer replica. pick a source from the original nodes\n       final DatanodeInfo src \u003d original[tried % original.length];\n       final DatanodeInfo[] targets \u003d {nodes[d]};\n       final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n \n       try {\n         transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n       } catch (IOException ioe) {\n         DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n             nodes[d] + \": \" + ioe.getMessage());\n         caughtException \u003d ioe;\n         // add the allocated node to the exclude list.\n         exclude.add(nodes[d]);\n         setPipeline(original, originalTypes, originalIDs);\n         tried++;\n         continue;\n       }\n       return; // finished successfully\n     }\n     // All retries failed\n     throw (caughtException !\u003d null) ? caughtException :\n         new IOException(\"Failed to add a node\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addDatanode2ExistingPipeline() throws IOException {\n    DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       *\n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens\n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       *\n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n    if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n        \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n      //no data have been written\n      return;\n    } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n        || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n      //pipeline is closing\n      return;\n    }\n\n    int tried \u003d 0;\n    final DatanodeInfo[] original \u003d nodes;\n    final StorageType[] originalTypes \u003d storageTypes;\n    final String[] originalIDs \u003d storageIDs;\n    IOException caughtException \u003d null;\n    ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003c\u003e(failed);\n    while (tried \u003c 3) {\n      LocatedBlock lb;\n      //get a new datanode\n      lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, stat.getFileId(), block.getCurrentBlock(), nodes, storageIDs,\n          exclude.toArray(new DatanodeInfo[exclude.size()]),\n          1, dfsClient.clientName);\n      // a new node was allocated by the namenode. Update nodes.\n      setPipeline(lb);\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n      //transfer replica. pick a source from the original nodes\n      final DatanodeInfo src \u003d original[tried % original.length];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n\n      try {\n        transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n      } catch (IOException ioe) {\n        DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n            nodes[d] + \": \" + ioe.getMessage());\n        caughtException \u003d ioe;\n        // add the allocated node to the exclude list.\n        exclude.add(nodes[d]);\n        setPipeline(original, originalTypes, originalIDs);\n        tried++;\n        continue;\n      }\n      return; // finished successfully\n    }\n    // All retries failed\n    throw (caughtException !\u003d null) ? caughtException :\n        new IOException(\"Failed to add a node\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,72 +1,72 @@\n   private void addDatanode2ExistingPipeline() throws IOException {\n     DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n       /*\n        * Is data transfer necessary?  We have the following cases.\n        *\n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n        *    + If there are data written, transfer RBW. This case may happens\n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n        *\n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n     if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n         \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n       //no data have been written\n       return;\n     } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n         || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n       //pipeline is closing\n       return;\n     }\n \n     int tried \u003d 0;\n     final DatanodeInfo[] original \u003d nodes;\n     final StorageType[] originalTypes \u003d storageTypes;\n     final String[] originalIDs \u003d storageIDs;\n     IOException caughtException \u003d null;\n-    ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003cDatanodeInfo\u003e(failed);\n+    ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003c\u003e(failed);\n     while (tried \u003c 3) {\n       LocatedBlock lb;\n       //get a new datanode\n       lb \u003d dfsClient.namenode.getAdditionalDatanode(\n           src, stat.getFileId(), block, nodes, storageIDs,\n           exclude.toArray(new DatanodeInfo[exclude.size()]),\n           1, dfsClient.clientName);\n       // a new node was allocated by the namenode. Update nodes.\n       setPipeline(lb);\n \n       //find the new datanode\n       final int d \u003d findNewDatanode(original);\n       //transfer replica. pick a source from the original nodes\n       final DatanodeInfo src \u003d original[tried % original.length];\n       final DatanodeInfo[] targets \u003d {nodes[d]};\n       final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n \n       try {\n         transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n       } catch (IOException ioe) {\n         DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n             nodes[d] + \": \" + ioe.getMessage());\n         caughtException \u003d ioe;\n         // add the allocated node to the exclude list.\n         exclude.add(nodes[d]);\n         setPipeline(original, originalTypes, originalIDs);\n         tried++;\n         continue;\n       }\n       return; // finished successfully\n     }\n     // All retries failed\n     throw (caughtException !\u003d null) ? caughtException :\n         new IOException(\"Failed to add a node\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addDatanode2ExistingPipeline() throws IOException {\n    DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       *\n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens\n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       *\n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n    if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n        \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n      //no data have been written\n      return;\n    } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n        || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n      //pipeline is closing\n      return;\n    }\n\n    int tried \u003d 0;\n    final DatanodeInfo[] original \u003d nodes;\n    final StorageType[] originalTypes \u003d storageTypes;\n    final String[] originalIDs \u003d storageIDs;\n    IOException caughtException \u003d null;\n    ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003c\u003e(failed);\n    while (tried \u003c 3) {\n      LocatedBlock lb;\n      //get a new datanode\n      lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, stat.getFileId(), block, nodes, storageIDs,\n          exclude.toArray(new DatanodeInfo[exclude.size()]),\n          1, dfsClient.clientName);\n      // a new node was allocated by the namenode. Update nodes.\n      setPipeline(lb);\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n      //transfer replica. pick a source from the original nodes\n      final DatanodeInfo src \u003d original[tried % original.length];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n\n      try {\n        transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n      } catch (IOException ioe) {\n        DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n            nodes[d] + \": \" + ioe.getMessage());\n        caughtException \u003d ioe;\n        // add the allocated node to the exclude list.\n        exclude.add(nodes[d]);\n        setPipeline(original, originalTypes, originalIDs);\n        tried++;\n        continue;\n      }\n      return; // finished successfully\n    }\n    // All retries failed\n    throw (caughtException !\u003d null) ? caughtException :\n        new IOException(\"Failed to add a node\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,72 @@\n   private void addDatanode2ExistingPipeline() throws IOException {\n-    if (DataTransferProtocol.LOG.isDebugEnabled()) {\n-      DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n-    }\n+    DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n       /*\n        * Is data transfer necessary?  We have the following cases.\n        *\n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n        *    + If there are data written, transfer RBW. This case may happens\n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n        *\n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n     if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n         \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n       //no data have been written\n       return;\n     } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n         || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n       //pipeline is closing\n       return;\n     }\n \n     int tried \u003d 0;\n     final DatanodeInfo[] original \u003d nodes;\n     final StorageType[] originalTypes \u003d storageTypes;\n     final String[] originalIDs \u003d storageIDs;\n     IOException caughtException \u003d null;\n     ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003cDatanodeInfo\u003e(failed);\n     while (tried \u003c 3) {\n       LocatedBlock lb;\n       //get a new datanode\n       lb \u003d dfsClient.namenode.getAdditionalDatanode(\n           src, stat.getFileId(), block, nodes, storageIDs,\n           exclude.toArray(new DatanodeInfo[exclude.size()]),\n           1, dfsClient.clientName);\n       // a new node was allocated by the namenode. Update nodes.\n       setPipeline(lb);\n \n       //find the new datanode\n       final int d \u003d findNewDatanode(original);\n       //transfer replica. pick a source from the original nodes\n       final DatanodeInfo src \u003d original[tried % original.length];\n       final DatanodeInfo[] targets \u003d {nodes[d]};\n       final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n \n       try {\n         transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n       } catch (IOException ioe) {\n         DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n             nodes[d] + \": \" + ioe.getMessage());\n         caughtException \u003d ioe;\n         // add the allocated node to the exclude list.\n         exclude.add(nodes[d]);\n         setPipeline(original, originalTypes, originalIDs);\n         tried++;\n         continue;\n       }\n       return; // finished successfully\n     }\n     // All retries failed\n     throw (caughtException !\u003d null) ? caughtException :\n         new IOException(\"Failed to add a node\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addDatanode2ExistingPipeline() throws IOException {\n    DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       *\n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens\n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       *\n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n    if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n        \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n      //no data have been written\n      return;\n    } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n        || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n      //pipeline is closing\n      return;\n    }\n\n    int tried \u003d 0;\n    final DatanodeInfo[] original \u003d nodes;\n    final StorageType[] originalTypes \u003d storageTypes;\n    final String[] originalIDs \u003d storageIDs;\n    IOException caughtException \u003d null;\n    ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003cDatanodeInfo\u003e(failed);\n    while (tried \u003c 3) {\n      LocatedBlock lb;\n      //get a new datanode\n      lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, stat.getFileId(), block, nodes, storageIDs,\n          exclude.toArray(new DatanodeInfo[exclude.size()]),\n          1, dfsClient.clientName);\n      // a new node was allocated by the namenode. Update nodes.\n      setPipeline(lb);\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n      //transfer replica. pick a source from the original nodes\n      final DatanodeInfo src \u003d original[tried % original.length];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n\n      try {\n        transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n      } catch (IOException ioe) {\n        DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n            nodes[d] + \": \" + ioe.getMessage());\n        caughtException \u003d ioe;\n        // add the allocated node to the exclude list.\n        exclude.add(nodes[d]);\n        setPipeline(original, originalTypes, originalIDs);\n        tried++;\n        continue;\n      }\n      return; // finished successfully\n    }\n    // All retries failed\n    throw (caughtException !\u003d null) ? caughtException :\n        new IOException(\"Failed to add a node\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,72 +1,74 @@\n   private void addDatanode2ExistingPipeline() throws IOException {\n-    DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n+    if (DataTransferProtocol.LOG.isDebugEnabled()) {\n+      DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n+    }\n       /*\n        * Is data transfer necessary?  We have the following cases.\n        *\n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n        *    + If there are data written, transfer RBW. This case may happens\n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n        *\n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n     if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n         \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n       //no data have been written\n       return;\n     } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n         || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n       //pipeline is closing\n       return;\n     }\n \n     int tried \u003d 0;\n     final DatanodeInfo[] original \u003d nodes;\n     final StorageType[] originalTypes \u003d storageTypes;\n     final String[] originalIDs \u003d storageIDs;\n     IOException caughtException \u003d null;\n     ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003cDatanodeInfo\u003e(failed);\n     while (tried \u003c 3) {\n       LocatedBlock lb;\n       //get a new datanode\n       lb \u003d dfsClient.namenode.getAdditionalDatanode(\n           src, stat.getFileId(), block, nodes, storageIDs,\n           exclude.toArray(new DatanodeInfo[exclude.size()]),\n           1, dfsClient.clientName);\n       // a new node was allocated by the namenode. Update nodes.\n       setPipeline(lb);\n \n       //find the new datanode\n       final int d \u003d findNewDatanode(original);\n       //transfer replica. pick a source from the original nodes\n       final DatanodeInfo src \u003d original[tried % original.length];\n       final DatanodeInfo[] targets \u003d {nodes[d]};\n       final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n \n       try {\n         transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n       } catch (IOException ioe) {\n         DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n             nodes[d] + \": \" + ioe.getMessage());\n         caughtException \u003d ioe;\n         // add the allocated node to the exclude list.\n         exclude.add(nodes[d]);\n         setPipeline(original, originalTypes, originalIDs);\n         tried++;\n         continue;\n       }\n       return; // finished successfully\n     }\n     // All retries failed\n     throw (caughtException !\u003d null) ? caughtException :\n         new IOException(\"Failed to add a node\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addDatanode2ExistingPipeline() throws IOException {\n    if (DataTransferProtocol.LOG.isDebugEnabled()) {\n      DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n    }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       *\n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens\n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       *\n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n    if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n        \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n      //no data have been written\n      return;\n    } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n        || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n      //pipeline is closing\n      return;\n    }\n\n    int tried \u003d 0;\n    final DatanodeInfo[] original \u003d nodes;\n    final StorageType[] originalTypes \u003d storageTypes;\n    final String[] originalIDs \u003d storageIDs;\n    IOException caughtException \u003d null;\n    ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003cDatanodeInfo\u003e(failed);\n    while (tried \u003c 3) {\n      LocatedBlock lb;\n      //get a new datanode\n      lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, stat.getFileId(), block, nodes, storageIDs,\n          exclude.toArray(new DatanodeInfo[exclude.size()]),\n          1, dfsClient.clientName);\n      // a new node was allocated by the namenode. Update nodes.\n      setPipeline(lb);\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n      //transfer replica. pick a source from the original nodes\n      final DatanodeInfo src \u003d original[tried % original.length];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n\n      try {\n        transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n      } catch (IOException ioe) {\n        DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n            nodes[d] + \": \" + ioe.getMessage());\n        caughtException \u003d ioe;\n        // add the allocated node to the exclude list.\n        exclude.add(nodes[d]);\n        setPipeline(original, originalTypes, originalIDs);\n        tried++;\n        continue;\n      }\n      return; // finished successfully\n    }\n    // All retries failed\n    throw (caughtException !\u003d null) ? caughtException :\n        new IOException(\"Failed to add a node\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 11:29 AM",
      "commitNameOld": "4c9497cbf02ecc82532a4e79e18912d8e0eb4731",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 1.26,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,72 @@\n   private void addDatanode2ExistingPipeline() throws IOException {\n-    if (DataTransferProtocol.LOG.isDebugEnabled()) {\n-      DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n-    }\n+    DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n       /*\n        * Is data transfer necessary?  We have the following cases.\n        *\n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n        *    + If there are data written, transfer RBW. This case may happens\n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n        *\n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n     if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n         \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n       //no data have been written\n       return;\n     } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n         || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n       //pipeline is closing\n       return;\n     }\n \n     int tried \u003d 0;\n     final DatanodeInfo[] original \u003d nodes;\n     final StorageType[] originalTypes \u003d storageTypes;\n     final String[] originalIDs \u003d storageIDs;\n     IOException caughtException \u003d null;\n     ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003cDatanodeInfo\u003e(failed);\n     while (tried \u003c 3) {\n       LocatedBlock lb;\n       //get a new datanode\n       lb \u003d dfsClient.namenode.getAdditionalDatanode(\n           src, stat.getFileId(), block, nodes, storageIDs,\n           exclude.toArray(new DatanodeInfo[exclude.size()]),\n           1, dfsClient.clientName);\n       // a new node was allocated by the namenode. Update nodes.\n       setPipeline(lb);\n \n       //find the new datanode\n       final int d \u003d findNewDatanode(original);\n       //transfer replica. pick a source from the original nodes\n       final DatanodeInfo src \u003d original[tried % original.length];\n       final DatanodeInfo[] targets \u003d {nodes[d]};\n       final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n \n       try {\n         transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n       } catch (IOException ioe) {\n         DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n             nodes[d] + \": \" + ioe.getMessage());\n         caughtException \u003d ioe;\n         // add the allocated node to the exclude list.\n         exclude.add(nodes[d]);\n         setPipeline(original, originalTypes, originalIDs);\n         tried++;\n         continue;\n       }\n       return; // finished successfully\n     }\n     // All retries failed\n     throw (caughtException !\u003d null) ? caughtException :\n         new IOException(\"Failed to add a node\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addDatanode2ExistingPipeline() throws IOException {\n    DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d {}\", lastAckedSeqno);\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       *\n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens\n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       *\n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n    if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n        \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n      //no data have been written\n      return;\n    } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n        || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n      //pipeline is closing\n      return;\n    }\n\n    int tried \u003d 0;\n    final DatanodeInfo[] original \u003d nodes;\n    final StorageType[] originalTypes \u003d storageTypes;\n    final String[] originalIDs \u003d storageIDs;\n    IOException caughtException \u003d null;\n    ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003cDatanodeInfo\u003e(failed);\n    while (tried \u003c 3) {\n      LocatedBlock lb;\n      //get a new datanode\n      lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, stat.getFileId(), block, nodes, storageIDs,\n          exclude.toArray(new DatanodeInfo[exclude.size()]),\n          1, dfsClient.clientName);\n      // a new node was allocated by the namenode. Update nodes.\n      setPipeline(lb);\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n      //transfer replica. pick a source from the original nodes\n      final DatanodeInfo src \u003d original[tried % original.length];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n\n      try {\n        transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n      } catch (IOException ioe) {\n        DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n            nodes[d] + \": \" + ioe.getMessage());\n        caughtException \u003d ioe;\n        // add the allocated node to the exclude list.\n        exclude.add(nodes[d]);\n        setPipeline(original, originalTypes, originalIDs);\n        tried++;\n        continue;\n      }\n      return; // finished successfully\n    }\n    // All retries failed\n    throw (caughtException !\u003d null) ? caughtException :\n        new IOException(\"Failed to add a node\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "4c9497cbf02ecc82532a4e79e18912d8e0eb4731": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9106. Transfer failure during pipeline recovery causes permanent write failures. Contributed by Kihwal Lee.\n",
      "commitDate": "28/09/15 11:29 AM",
      "commitName": "4c9497cbf02ecc82532a4e79e18912d8e0eb4731",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.16,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,74 @@\n   private void addDatanode2ExistingPipeline() throws IOException {\n     if (DataTransferProtocol.LOG.isDebugEnabled()) {\n       DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n     }\n       /*\n        * Is data transfer necessary?  We have the following cases.\n        *\n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n        *    + If there are data written, transfer RBW. This case may happens\n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n        *\n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n     if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n         \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n       //no data have been written\n       return;\n     } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n         || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n       //pipeline is closing\n       return;\n     }\n \n-    //get a new datanode\n+    int tried \u003d 0;\n     final DatanodeInfo[] original \u003d nodes;\n-    final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n-        src, stat.getFileId(), block, nodes, storageIDs,\n-        failed.toArray(new DatanodeInfo[failed.size()]),\n-        1, dfsClient.clientName);\n-    setPipeline(lb);\n+    final StorageType[] originalTypes \u003d storageTypes;\n+    final String[] originalIDs \u003d storageIDs;\n+    IOException caughtException \u003d null;\n+    ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003cDatanodeInfo\u003e(failed);\n+    while (tried \u003c 3) {\n+      LocatedBlock lb;\n+      //get a new datanode\n+      lb \u003d dfsClient.namenode.getAdditionalDatanode(\n+          src, stat.getFileId(), block, nodes, storageIDs,\n+          exclude.toArray(new DatanodeInfo[exclude.size()]),\n+          1, dfsClient.clientName);\n+      // a new node was allocated by the namenode. Update nodes.\n+      setPipeline(lb);\n \n-    //find the new datanode\n-    final int d \u003d findNewDatanode(original);\n+      //find the new datanode\n+      final int d \u003d findNewDatanode(original);\n+      //transfer replica. pick a source from the original nodes\n+      final DatanodeInfo src \u003d original[tried % original.length];\n+      final DatanodeInfo[] targets \u003d {nodes[d]};\n+      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n \n-    //transfer replica\n-    final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n-    final DatanodeInfo[] targets \u003d {nodes[d]};\n-    final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n-    transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n+      try {\n+        transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n+      } catch (IOException ioe) {\n+        DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n+            nodes[d] + \": \" + ioe.getMessage());\n+        caughtException \u003d ioe;\n+        // add the allocated node to the exclude list.\n+        exclude.add(nodes[d]);\n+        setPipeline(original, originalTypes, originalIDs);\n+        tried++;\n+        continue;\n+      }\n+      return; // finished successfully\n+    }\n+    // All retries failed\n+    throw (caughtException !\u003d null) ? caughtException :\n+        new IOException(\"Failed to add a node\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void addDatanode2ExistingPipeline() throws IOException {\n    if (DataTransferProtocol.LOG.isDebugEnabled()) {\n      DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n    }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       *\n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens\n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       *\n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n    if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n        \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n      //no data have been written\n      return;\n    } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n        || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n      //pipeline is closing\n      return;\n    }\n\n    int tried \u003d 0;\n    final DatanodeInfo[] original \u003d nodes;\n    final StorageType[] originalTypes \u003d storageTypes;\n    final String[] originalIDs \u003d storageIDs;\n    IOException caughtException \u003d null;\n    ArrayList\u003cDatanodeInfo\u003e exclude \u003d new ArrayList\u003cDatanodeInfo\u003e(failed);\n    while (tried \u003c 3) {\n      LocatedBlock lb;\n      //get a new datanode\n      lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, stat.getFileId(), block, nodes, storageIDs,\n          exclude.toArray(new DatanodeInfo[exclude.size()]),\n          1, dfsClient.clientName);\n      // a new node was allocated by the namenode. Update nodes.\n      setPipeline(lb);\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n      //transfer replica. pick a source from the original nodes\n      final DatanodeInfo src \u003d original[tried % original.length];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n\n      try {\n        transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n      } catch (IOException ioe) {\n        DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n            nodes[d] + \": \" + ioe.getMessage());\n        caughtException \u003d ioe;\n        // add the allocated node to the exclude list.\n        exclude.add(nodes[d]);\n        setPipeline(original, originalTypes, originalIDs);\n        tried++;\n        continue;\n      }\n      return; // finished successfully\n    }\n    // All retries failed\n    throw (caughtException !\u003d null) ? caughtException :\n        new IOException(\"Failed to add a node\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void addDatanode2ExistingPipeline() throws IOException {\n    if (DataTransferProtocol.LOG.isDebugEnabled()) {\n      DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n    }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       *\n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens\n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       *\n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n    if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n        \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n      //no data have been written\n      return;\n    } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n        || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n      //pipeline is closing\n      return;\n    }\n\n    //get a new datanode\n    final DatanodeInfo[] original \u003d nodes;\n    final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n        src, stat.getFileId(), block, nodes, storageIDs,\n        failed.toArray(new DatanodeInfo[failed.size()]),\n        1, dfsClient.clientName);\n    setPipeline(lb);\n\n    //find the new datanode\n    final int d \u003d findNewDatanode(original);\n\n    //transfer replica\n    final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n    final DatanodeInfo[] targets \u003d {nodes[d]};\n    final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n    transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
      }
    },
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
      "commitDate": "24/03/15 11:06 AM",
      "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "24/03/15 11:06 AM",
          "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/03/15 10:49 AM",
          "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,50 @@\n-    private void addDatanode2ExistingPipeline() throws IOException {\n-      if (DataTransferProtocol.LOG.isDebugEnabled()) {\n-        DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n-      }\n+  private void addDatanode2ExistingPipeline() throws IOException {\n+    if (DataTransferProtocol.LOG.isDebugEnabled()) {\n+      DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n+    }\n       /*\n        * Is data transfer necessary?  We have the following cases.\n-       * \n+       *\n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n-       *    + If there are data written, transfer RBW. This case may happens \n+       *    + If there are data written, transfer RBW. This case may happens\n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n-       * \n+       *\n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n-      if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n-          \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n-        //no data have been written\n-        return;\n-      } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n-          || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n-        //pipeline is closing\n-        return;\n-      }\n+    if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n+        \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n+      //no data have been written\n+      return;\n+    } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n+        || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n+      //pipeline is closing\n+      return;\n+    }\n \n-      //get a new datanode\n-      final DatanodeInfo[] original \u003d nodes;\n-      final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n-          src, fileId, block, nodes, storageIDs,\n-          failed.toArray(new DatanodeInfo[failed.size()]),\n-          1, dfsClient.clientName);\n-      setPipeline(lb);\n+    //get a new datanode\n+    final DatanodeInfo[] original \u003d nodes;\n+    final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n+        src, stat.getFileId(), block, nodes, storageIDs,\n+        failed.toArray(new DatanodeInfo[failed.size()]),\n+        1, dfsClient.clientName);\n+    setPipeline(lb);\n \n-      //find the new datanode\n-      final int d \u003d findNewDatanode(original);\n+    //find the new datanode\n+    final int d \u003d findNewDatanode(original);\n \n-      //transfer replica\n-      final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n-      final DatanodeInfo[] targets \u003d {nodes[d]};\n-      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n-      transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n-    }\n\\ No newline at end of file\n+    //transfer replica\n+    final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n+    final DatanodeInfo[] targets \u003d {nodes[d]};\n+    final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n+    transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void addDatanode2ExistingPipeline() throws IOException {\n    if (DataTransferProtocol.LOG.isDebugEnabled()) {\n      DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n    }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       *\n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens\n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       *\n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n    if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n        \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n      //no data have been written\n      return;\n    } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n        || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n      //pipeline is closing\n      return;\n    }\n\n    //get a new datanode\n    final DatanodeInfo[] original \u003d nodes;\n    final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n        src, stat.getFileId(), block, nodes, storageIDs,\n        failed.toArray(new DatanodeInfo[failed.size()]),\n        1, dfsClient.clientName);\n    setPipeline(lb);\n\n    //find the new datanode\n    final int d \u003d findNewDatanode(original);\n\n    //transfer replica\n    final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n    final DatanodeInfo[] targets \u003d {nodes[d]};\n    final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n    transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
            "oldMethodName": "addDatanode2ExistingPipeline",
            "newMethodName": "addDatanode2ExistingPipeline"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "24/03/15 11:06 AM",
          "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/03/15 10:49 AM",
          "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,50 +1,50 @@\n-    private void addDatanode2ExistingPipeline() throws IOException {\n-      if (DataTransferProtocol.LOG.isDebugEnabled()) {\n-        DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n-      }\n+  private void addDatanode2ExistingPipeline() throws IOException {\n+    if (DataTransferProtocol.LOG.isDebugEnabled()) {\n+      DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n+    }\n       /*\n        * Is data transfer necessary?  We have the following cases.\n-       * \n+       *\n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n-       *    + If there are data written, transfer RBW. This case may happens \n+       *    + If there are data written, transfer RBW. This case may happens\n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n-       * \n+       *\n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n-      if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n-          \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n-        //no data have been written\n-        return;\n-      } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n-          || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n-        //pipeline is closing\n-        return;\n-      }\n+    if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n+        \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n+      //no data have been written\n+      return;\n+    } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n+        || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n+      //pipeline is closing\n+      return;\n+    }\n \n-      //get a new datanode\n-      final DatanodeInfo[] original \u003d nodes;\n-      final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n-          src, fileId, block, nodes, storageIDs,\n-          failed.toArray(new DatanodeInfo[failed.size()]),\n-          1, dfsClient.clientName);\n-      setPipeline(lb);\n+    //get a new datanode\n+    final DatanodeInfo[] original \u003d nodes;\n+    final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n+        src, stat.getFileId(), block, nodes, storageIDs,\n+        failed.toArray(new DatanodeInfo[failed.size()]),\n+        1, dfsClient.clientName);\n+    setPipeline(lb);\n \n-      //find the new datanode\n-      final int d \u003d findNewDatanode(original);\n+    //find the new datanode\n+    final int d \u003d findNewDatanode(original);\n \n-      //transfer replica\n-      final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n-      final DatanodeInfo[] targets \u003d {nodes[d]};\n-      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n-      transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n-    }\n\\ No newline at end of file\n+    //transfer replica\n+    final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n+    final DatanodeInfo[] targets \u003d {nodes[d]};\n+    final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n+    transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private void addDatanode2ExistingPipeline() throws IOException {\n    if (DataTransferProtocol.LOG.isDebugEnabled()) {\n      DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n    }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       *\n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens\n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       *\n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n    if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n        \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n      //no data have been written\n      return;\n    } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n        || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n      //pipeline is closing\n      return;\n    }\n\n    //get a new datanode\n    final DatanodeInfo[] original \u003d nodes;\n    final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n        src, stat.getFileId(), block, nodes, storageIDs,\n        failed.toArray(new DatanodeInfo[failed.size()]),\n        1, dfsClient.clientName);\n    setPipeline(lb);\n\n    //find the new datanode\n    final int d \u003d findNewDatanode(original);\n\n    //transfer replica\n    final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n    final DatanodeInfo[] targets \u003d {nodes[d]};\n    final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n    transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {}
        }
      ]
    },
    "552b4fb9f9a76b18605322c0b0e8072613d67773": {
      "type": "Ybodychange",
      "commitMessage": "Merge from trunk to branch\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1612928 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/07/14 12:26 PM",
      "commitName": "552b4fb9f9a76b18605322c0b0e8072613d67773",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "15/07/14 2:10 PM",
      "commitNameOld": "56c0bd4d37ab13b6cbcf860eda852da603ab2f62",
      "commitAuthorOld": "",
      "daysBetweenCommits": 7.93,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,50 @@\n     private void addDatanode2ExistingPipeline() throws IOException {\n       if (DataTransferProtocol.LOG.isDebugEnabled()) {\n         DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n       }\n       /*\n        * Is data transfer necessary?  We have the following cases.\n        * \n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n        *    + If there are data written, transfer RBW. This case may happens \n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n        * \n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n       if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n           \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n         //no data have been written\n         return;\n       } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n           || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n         //pipeline is closing\n         return;\n       }\n \n       //get a new datanode\n       final DatanodeInfo[] original \u003d nodes;\n       final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n           src, fileId, block, nodes, storageIDs,\n           failed.toArray(new DatanodeInfo[failed.size()]),\n           1, dfsClient.clientName);\n       setPipeline(lb);\n \n       //find the new datanode\n       final int d \u003d findNewDatanode(original);\n \n       //transfer replica\n       final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n       final DatanodeInfo[] targets \u003d {nodes[d]};\n-      transfer(src, targets, lb.getBlockToken());\n+      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n+      transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void addDatanode2ExistingPipeline() throws IOException {\n      if (DataTransferProtocol.LOG.isDebugEnabled()) {\n        DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n      }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       * \n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens \n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       * \n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n      if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n          \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n        //no data have been written\n        return;\n      } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n          || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        //pipeline is closing\n        return;\n      }\n\n      //get a new datanode\n      final DatanodeInfo[] original \u003d nodes;\n      final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, fileId, block, nodes, storageIDs,\n          failed.toArray(new DatanodeInfo[failed.size()]),\n          1, dfsClient.clientName);\n      setPipeline(lb);\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n\n      //transfer replica\n      final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n      transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/14 12:41 AM",
      "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "14/07/14 11:10 AM",
      "commitNameOld": "3b54223c0f32d42a84436c670d80b791a8e9696d",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 7.56,
      "commitsBetweenForRepo": 68,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,50 @@\n     private void addDatanode2ExistingPipeline() throws IOException {\n       if (DataTransferProtocol.LOG.isDebugEnabled()) {\n         DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n       }\n       /*\n        * Is data transfer necessary?  We have the following cases.\n        * \n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n        *    + If there are data written, transfer RBW. This case may happens \n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n        * \n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n       if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n           \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n         //no data have been written\n         return;\n       } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n           || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n         //pipeline is closing\n         return;\n       }\n \n       //get a new datanode\n       final DatanodeInfo[] original \u003d nodes;\n       final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n           src, fileId, block, nodes, storageIDs,\n           failed.toArray(new DatanodeInfo[failed.size()]),\n           1, dfsClient.clientName);\n       setPipeline(lb);\n \n       //find the new datanode\n       final int d \u003d findNewDatanode(original);\n \n       //transfer replica\n       final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n       final DatanodeInfo[] targets \u003d {nodes[d]};\n-      transfer(src, targets, lb.getBlockToken());\n+      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n+      transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void addDatanode2ExistingPipeline() throws IOException {\n      if (DataTransferProtocol.LOG.isDebugEnabled()) {\n        DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n      }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       * \n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens \n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       * \n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n      if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n          \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n        //no data have been written\n        return;\n      } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n          || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        //pipeline is closing\n        return;\n      }\n\n      //get a new datanode\n      final DatanodeInfo[] original \u003d nodes;\n      final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, fileId, block, nodes, storageIDs,\n          failed.toArray(new DatanodeInfo[failed.size()]),\n          1, dfsClient.clientName);\n      setPipeline(lb);\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n\n      //transfer replica\n      final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      final StorageType[] targetStorageTypes \u003d {storageTypes[d]};\n      transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6294. Use INode IDs to avoid conflicts when a file open for write is renamed (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1593634 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/05/14 3:36 PM",
      "commitName": "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "25/03/14 9:11 PM",
      "commitNameOld": "1fbb04e367d7c330e6052207f9f11911f4f5f368",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 44.77,
      "commitsBetweenForRepo": 257,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,49 @@\n     private void addDatanode2ExistingPipeline() throws IOException {\n       if (DataTransferProtocol.LOG.isDebugEnabled()) {\n         DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n       }\n       /*\n        * Is data transfer necessary?  We have the following cases.\n        * \n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n        *    + If there are data written, transfer RBW. This case may happens \n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n        * \n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n       if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n           \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n         //no data have been written\n         return;\n       } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n           || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n         //pipeline is closing\n         return;\n       }\n \n       //get a new datanode\n       final DatanodeInfo[] original \u003d nodes;\n       final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n-          src, block, nodes, storageIDs,\n+          src, fileId, block, nodes, storageIDs,\n           failed.toArray(new DatanodeInfo[failed.size()]),\n           1, dfsClient.clientName);\n       setPipeline(lb);\n \n       //find the new datanode\n       final int d \u003d findNewDatanode(original);\n \n       //transfer replica\n       final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n       final DatanodeInfo[] targets \u003d {nodes[d]};\n       transfer(src, targets, lb.getBlockToken());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void addDatanode2ExistingPipeline() throws IOException {\n      if (DataTransferProtocol.LOG.isDebugEnabled()) {\n        DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n      }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       * \n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens \n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       * \n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n      if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n          \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n        //no data have been written\n        return;\n      } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n          || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        //pipeline is closing\n        return;\n      }\n\n      //get a new datanode\n      final DatanodeInfo[] original \u003d nodes;\n      final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, fileId, block, nodes, storageIDs,\n          failed.toArray(new DatanodeInfo[failed.size()]),\n          1, dfsClient.clientName);\n      setPipeline(lb);\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n\n      //transfer replica\n      final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      transfer(src, targets, lb.getBlockToken());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "3e3fea7cd433a77ab2b92a5035ffc4bdea02c6cf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5466. Update storage IDs when the pipeline is updated. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1539203 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/11/13 4:25 PM",
      "commitName": "3e3fea7cd433a77ab2b92a5035ffc4bdea02c6cf",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "28/10/13 10:29 AM",
      "commitNameOld": "dc0b44a884700cda3665aa04b16d1e3474328e05",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 8.29,
      "commitsBetweenForRepo": 40,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,49 @@\n     private void addDatanode2ExistingPipeline() throws IOException {\n       if (DataTransferProtocol.LOG.isDebugEnabled()) {\n         DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n       }\n       /*\n        * Is data transfer necessary?  We have the following cases.\n        * \n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n        *    + If there are data written, transfer RBW. This case may happens \n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n        * \n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n       if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n           \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n         //no data have been written\n         return;\n       } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n           || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n         //pipeline is closing\n         return;\n       }\n \n       //get a new datanode\n       final DatanodeInfo[] original \u003d nodes;\n       final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n           src, block, nodes, storageIDs,\n           failed.toArray(new DatanodeInfo[failed.size()]),\n           1, dfsClient.clientName);\n-      nodes \u003d lb.getLocations();\n+      setPipeline(lb);\n \n       //find the new datanode\n       final int d \u003d findNewDatanode(original);\n \n       //transfer replica\n       final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n       final DatanodeInfo[] targets \u003d {nodes[d]};\n       transfer(src, targets, lb.getBlockToken());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void addDatanode2ExistingPipeline() throws IOException {\n      if (DataTransferProtocol.LOG.isDebugEnabled()) {\n        DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n      }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       * \n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens \n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       * \n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n      if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n          \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n        //no data have been written\n        return;\n      } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n          || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        //pipeline is closing\n        return;\n      }\n\n      //get a new datanode\n      final DatanodeInfo[] original \u003d nodes;\n      final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, block, nodes, storageIDs,\n          failed.toArray(new DatanodeInfo[failed.size()]),\n          1, dfsClient.clientName);\n      setPipeline(lb);\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n\n      //transfer replica\n      final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      transfer(src, targets, lb.getBlockToken());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "abf09f090f77a7e54e331b7a07354e7926b60dc9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4990. Change BlockPlacementPolicy to choose storages instead of datanodes.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1524444 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/09/13 8:12 AM",
      "commitName": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "03/09/13 7:03 AM",
      "commitNameOld": "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 15.05,
      "commitsBetweenForRepo": 72,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,49 @@\n     private void addDatanode2ExistingPipeline() throws IOException {\n       if (DataTransferProtocol.LOG.isDebugEnabled()) {\n         DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n       }\n       /*\n        * Is data transfer necessary?  We have the following cases.\n        * \n        * Case 1: Failure in Pipeline Setup\n        * - Append\n        *    + Transfer the stored replica, which may be a RBW or a finalized.\n        * - Create\n        *    + If no data, then no transfer is required.\n        *    + If there are data written, transfer RBW. This case may happens \n        *      when there are streaming failure earlier in this pipeline.\n        *\n        * Case 2: Failure in Streaming\n        * - Append/Create:\n        *    + transfer RBW\n        * \n        * Case 3: Failure in Close\n        * - Append/Create:\n        *    + no transfer, let NameNode replicates the block.\n        */\n       if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n           \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n         //no data have been written\n         return;\n       } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n           || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n         //pipeline is closing\n         return;\n       }\n \n       //get a new datanode\n       final DatanodeInfo[] original \u003d nodes;\n       final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n-          src, block, nodes, failed.toArray(new DatanodeInfo[failed.size()]),\n+          src, block, nodes, storageIDs,\n+          failed.toArray(new DatanodeInfo[failed.size()]),\n           1, dfsClient.clientName);\n       nodes \u003d lb.getLocations();\n \n       //find the new datanode\n       final int d \u003d findNewDatanode(original);\n \n       //transfer replica\n       final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n       final DatanodeInfo[] targets \u003d {nodes[d]};\n       transfer(src, targets, lb.getBlockToken());\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void addDatanode2ExistingPipeline() throws IOException {\n      if (DataTransferProtocol.LOG.isDebugEnabled()) {\n        DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n      }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       * \n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens \n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       * \n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n      if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n          \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n        //no data have been written\n        return;\n      } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n          || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        //pipeline is closing\n        return;\n      }\n\n      //get a new datanode\n      final DatanodeInfo[] original \u003d nodes;\n      final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, block, nodes, storageIDs,\n          failed.toArray(new DatanodeInfo[failed.size()]),\n          1, dfsClient.clientName);\n      nodes \u003d lb.getLocations();\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n\n      //transfer replica\n      final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      transfer(src, targets, lb.getBlockToken());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private void addDatanode2ExistingPipeline() throws IOException {\n      if (DataTransferProtocol.LOG.isDebugEnabled()) {\n        DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n      }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       * \n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens \n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       * \n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n      if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n          \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n        //no data have been written\n        return;\n      } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n          || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        //pipeline is closing\n        return;\n      }\n\n      //get a new datanode\n      final DatanodeInfo[] original \u003d nodes;\n      final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, block, nodes, failed.toArray(new DatanodeInfo[failed.size()]),\n          1, dfsClient.clientName);\n      nodes \u003d lb.getLocations();\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n\n      //transfer replica\n      final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      transfer(src, targets, lb.getBlockToken());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private void addDatanode2ExistingPipeline() throws IOException {\n      if (DataTransferProtocol.LOG.isDebugEnabled()) {\n        DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n      }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       * \n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens \n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       * \n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n      if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n          \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n        //no data have been written\n        return;\n      } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n          || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        //pipeline is closing\n        return;\n      }\n\n      //get a new datanode\n      final DatanodeInfo[] original \u003d nodes;\n      final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, block, nodes, failed.toArray(new DatanodeInfo[failed.size()]),\n          1, dfsClient.clientName);\n      nodes \u003d lb.getLocations();\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n\n      //transfer replica\n      final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      transfer(src, targets, lb.getBlockToken());\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,48 @@\n+    private void addDatanode2ExistingPipeline() throws IOException {\n+      if (DataTransferProtocol.LOG.isDebugEnabled()) {\n+        DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n+      }\n+      /*\n+       * Is data transfer necessary?  We have the following cases.\n+       * \n+       * Case 1: Failure in Pipeline Setup\n+       * - Append\n+       *    + Transfer the stored replica, which may be a RBW or a finalized.\n+       * - Create\n+       *    + If no data, then no transfer is required.\n+       *    + If there are data written, transfer RBW. This case may happens \n+       *      when there are streaming failure earlier in this pipeline.\n+       *\n+       * Case 2: Failure in Streaming\n+       * - Append/Create:\n+       *    + transfer RBW\n+       * \n+       * Case 3: Failure in Close\n+       * - Append/Create:\n+       *    + no transfer, let NameNode replicates the block.\n+       */\n+      if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n+          \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n+        //no data have been written\n+        return;\n+      } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n+          || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n+        //pipeline is closing\n+        return;\n+      }\n+\n+      //get a new datanode\n+      final DatanodeInfo[] original \u003d nodes;\n+      final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n+          src, block, nodes, failed.toArray(new DatanodeInfo[failed.size()]),\n+          1, dfsClient.clientName);\n+      nodes \u003d lb.getLocations();\n+\n+      //find the new datanode\n+      final int d \u003d findNewDatanode(original);\n+\n+      //transfer replica\n+      final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n+      final DatanodeInfo[] targets \u003d {nodes[d]};\n+      transfer(src, targets, lb.getBlockToken());\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void addDatanode2ExistingPipeline() throws IOException {\n      if (DataTransferProtocol.LOG.isDebugEnabled()) {\n        DataTransferProtocol.LOG.debug(\"lastAckedSeqno \u003d \" + lastAckedSeqno);\n      }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       * \n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens \n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       * \n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n      if (!isAppend \u0026\u0026 lastAckedSeqno \u003c 0\n          \u0026\u0026 stage \u003d\u003d BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n        //no data have been written\n        return;\n      } else if (stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE\n          || stage \u003d\u003d BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        //pipeline is closing\n        return;\n      }\n\n      //get a new datanode\n      final DatanodeInfo[] original \u003d nodes;\n      final LocatedBlock lb \u003d dfsClient.namenode.getAdditionalDatanode(\n          src, block, nodes, failed.toArray(new DatanodeInfo[failed.size()]),\n          1, dfsClient.clientName);\n      nodes \u003d lb.getLocations();\n\n      //find the new datanode\n      final int d \u003d findNewDatanode(original);\n\n      //transfer replica\n      final DatanodeInfo src \u003d d \u003d\u003d 0? nodes[1]: nodes[d - 1];\n      final DatanodeInfo[] targets \u003d {nodes[d]};\n      transfer(src, targets, lb.getBlockToken());\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}