{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "JobBuilder.java",
  "functionName": "build",
  "functionId": "build",
  "sourceFilePath": "hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/JobBuilder.java",
  "functionStartLine": 278,
  "functionEndLine": 408,
  "numCommitsSeen": 18,
  "timeTaken": 4691,
  "changeHistory": [
    "10325d97329c214bb3899c8535df5a366bc86d2f",
    "1dcc4b57ee29c372934b72511302b689cd93c1cf",
    "a238f931ea7dce0ca620d1798156c84ff77097ff",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "10325d97329c214bb3899c8535df5a366bc86d2f": "Yfilerename",
    "1dcc4b57ee29c372934b72511302b689cd93c1cf": "Yreturntypechange",
    "a238f931ea7dce0ca620d1798156c84ff77097ff": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "10325d97329c214bb3899c8535df5a366bc86d2f": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-3582. Move successfully passing MR1 tests to MR2 maven tree.(ahmed via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1233090 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/01/12 2:10 PM",
      "commitName": "10325d97329c214bb3899c8535df5a366bc86d2f",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "18/01/12 10:20 AM",
      "commitNameOld": "8b2f6909ec7df5cffb5ef417f5c9cffdee43e38a",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.16,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public ParsedJob build() {\n    // The main job here is to build CDFs and manage the conf\n    finalized \u003d true;\n\n    // set the conf\n    if (jobConfigurationParameters !\u003d null) {\n      result.setJobProperties(jobConfigurationParameters);\n    }\n    \n    // initialize all the per-job statistics gathering places\n    Histogram[] successfulMapAttemptTimes \u003d\n        new Histogram[ParsedHost.numberOfDistances() + 1];\n    for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n      successfulMapAttemptTimes[i] \u003d new Histogram();\n    }\n\n    Histogram successfulReduceAttemptTimes \u003d new Histogram();\n    Histogram[] failedMapAttemptTimes \u003d\n        new Histogram[ParsedHost.numberOfDistances() + 1];\n    for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n      failedMapAttemptTimes[i] \u003d new Histogram();\n    }\n    Histogram failedReduceAttemptTimes \u003d new Histogram();\n\n    Histogram successfulNthMapperAttempts \u003d new Histogram();\n    // Histogram successfulNthReducerAttempts \u003d new Histogram();\n    // Histogram mapperLocality \u003d new Histogram();\n\n    for (LoggedTask task : result.getMapTasks()) {\n      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n        int distance \u003d successfulMapAttemptTimes.length - 1;\n        Long runtime \u003d null;\n\n        if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n          runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n\n          if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n            LoggedLocation host \u003d attempt.getLocation();\n\n            List\u003cLoggedLocation\u003e locs \u003d task.getPreferredLocations();\n\n            if (host !\u003d null \u0026\u0026 locs !\u003d null) {\n              for (LoggedLocation loc : locs) {\n                ParsedHost preferedLoc \u003d new ParsedHost(loc);\n\n                distance \u003d\n                    Math.min(distance, preferedLoc\n                        .distance(new ParsedHost(host)));\n              }\n\n              // mapperLocality.enter(distance);\n            }\n\n            if (attempt.getStartTime() \u003e 0 \u0026\u0026 attempt.getFinishTime() \u003e 0) {\n              if (runtime !\u003d null) {\n                successfulMapAttemptTimes[distance].enter(runtime);\n              }\n            }\n\n            TaskAttemptID attemptID \u003d attempt.getAttemptID();\n\n            if (attemptID !\u003d null) {\n              successfulNthMapperAttempts.enter(attemptID.getId());\n            }\n          } else {\n            if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n              if (runtime !\u003d null) {\n                failedMapAttemptTimes[distance].enter(runtime);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    for (LoggedTask task : result.getReduceTasks()) {\n      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n        Long runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n\n        if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n          runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n        }\n        if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n          if (runtime !\u003d null) {\n            successfulReduceAttemptTimes.enter(runtime);\n          }\n        } else if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n          failedReduceAttemptTimes.enter(runtime);\n        }\n      }\n    }\n\n    result.setFailedMapAttemptCDFs(mapCDFArrayList(failedMapAttemptTimes));\n\n    LoggedDiscreteCDF failedReduce \u003d new LoggedDiscreteCDF();\n    failedReduce.setCDF(failedReduceAttemptTimes, attemptTimesPercentiles, 100);\n    result.setFailedReduceAttemptCDF(failedReduce);\n\n    result\n        .setSuccessfulMapAttemptCDFs(mapCDFArrayList(successfulMapAttemptTimes));\n\n    LoggedDiscreteCDF succReduce \u003d new LoggedDiscreteCDF();\n    succReduce.setCDF(successfulReduceAttemptTimes, attemptTimesPercentiles,\n        100);\n    result.setSuccessfulReduceAttemptCDF(succReduce);\n\n    long totalSuccessfulAttempts \u003d 0L;\n    long maxTriesToSucceed \u003d 0L;\n\n    for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n      totalSuccessfulAttempts +\u003d ent.getValue();\n      maxTriesToSucceed \u003d Math.max(maxTriesToSucceed, ent.getKey());\n    }\n\n    if (totalSuccessfulAttempts \u003e 0L) {\n      double[] successAfterI \u003d new double[(int) maxTriesToSucceed + 1];\n      for (int i \u003d 0; i \u003c successAfterI.length; ++i) {\n        successAfterI[i] \u003d 0.0D;\n      }\n\n      for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n        successAfterI[ent.getKey().intValue()] \u003d\n            ((double) ent.getValue()) / totalSuccessfulAttempts;\n      }\n      result.setMapperTriesToSucceed(successAfterI);\n    } else {\n      result.setMapperTriesToSucceed(null);\n    }\n\n    return result;\n  }",
      "path": "hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/JobBuilder.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
        "newPath": "hadoop-tools/hadoop-rumen/src/main/java/org/apache/hadoop/tools/rumen/JobBuilder.java"
      }
    },
    "1dcc4b57ee29c372934b72511302b689cd93c1cf": {
      "type": "Yreturntypechange",
      "commitMessage": "MAPREDUCE-3597. [Rumen] Rumen should provide APIs to access all the job-history related information.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1222695 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/12/11 6:47 AM",
      "commitName": "1dcc4b57ee29c372934b72511302b689cd93c1cf",
      "commitAuthor": "Amar Kamat",
      "commitDateOld": "20/12/11 6:58 PM",
      "commitNameOld": "264d3b7dd0c81fe02baaa09b6e3aaad5ee6d191a",
      "commitAuthorOld": "Amar Kamat",
      "daysBetweenCommits": 2.49,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,131 +1,131 @@\n-  public LoggedJob build() {\n+  public ParsedJob build() {\n     // The main job here is to build CDFs and manage the conf\n     finalized \u003d true;\n \n     // set the conf\n     if (jobConfigurationParameters !\u003d null) {\n       result.setJobProperties(jobConfigurationParameters);\n     }\n     \n     // initialize all the per-job statistics gathering places\n     Histogram[] successfulMapAttemptTimes \u003d\n         new Histogram[ParsedHost.numberOfDistances() + 1];\n     for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n       successfulMapAttemptTimes[i] \u003d new Histogram();\n     }\n \n     Histogram successfulReduceAttemptTimes \u003d new Histogram();\n     Histogram[] failedMapAttemptTimes \u003d\n         new Histogram[ParsedHost.numberOfDistances() + 1];\n     for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n       failedMapAttemptTimes[i] \u003d new Histogram();\n     }\n     Histogram failedReduceAttemptTimes \u003d new Histogram();\n \n     Histogram successfulNthMapperAttempts \u003d new Histogram();\n     // Histogram successfulNthReducerAttempts \u003d new Histogram();\n     // Histogram mapperLocality \u003d new Histogram();\n \n     for (LoggedTask task : result.getMapTasks()) {\n       for (LoggedTaskAttempt attempt : task.getAttempts()) {\n         int distance \u003d successfulMapAttemptTimes.length - 1;\n         Long runtime \u003d null;\n \n         if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n           runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n \n           if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n             LoggedLocation host \u003d attempt.getLocation();\n \n             List\u003cLoggedLocation\u003e locs \u003d task.getPreferredLocations();\n \n             if (host !\u003d null \u0026\u0026 locs !\u003d null) {\n               for (LoggedLocation loc : locs) {\n                 ParsedHost preferedLoc \u003d new ParsedHost(loc);\n \n                 distance \u003d\n                     Math.min(distance, preferedLoc\n                         .distance(new ParsedHost(host)));\n               }\n \n               // mapperLocality.enter(distance);\n             }\n \n             if (attempt.getStartTime() \u003e 0 \u0026\u0026 attempt.getFinishTime() \u003e 0) {\n               if (runtime !\u003d null) {\n                 successfulMapAttemptTimes[distance].enter(runtime);\n               }\n             }\n \n             TaskAttemptID attemptID \u003d attempt.getAttemptID();\n \n             if (attemptID !\u003d null) {\n               successfulNthMapperAttempts.enter(attemptID.getId());\n             }\n           } else {\n             if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n               if (runtime !\u003d null) {\n                 failedMapAttemptTimes[distance].enter(runtime);\n               }\n             }\n           }\n         }\n       }\n     }\n \n     for (LoggedTask task : result.getReduceTasks()) {\n       for (LoggedTaskAttempt attempt : task.getAttempts()) {\n         Long runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n \n         if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n           runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n         }\n         if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n           if (runtime !\u003d null) {\n             successfulReduceAttemptTimes.enter(runtime);\n           }\n         } else if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n           failedReduceAttemptTimes.enter(runtime);\n         }\n       }\n     }\n \n     result.setFailedMapAttemptCDFs(mapCDFArrayList(failedMapAttemptTimes));\n \n     LoggedDiscreteCDF failedReduce \u003d new LoggedDiscreteCDF();\n     failedReduce.setCDF(failedReduceAttemptTimes, attemptTimesPercentiles, 100);\n     result.setFailedReduceAttemptCDF(failedReduce);\n \n     result\n         .setSuccessfulMapAttemptCDFs(mapCDFArrayList(successfulMapAttemptTimes));\n \n     LoggedDiscreteCDF succReduce \u003d new LoggedDiscreteCDF();\n     succReduce.setCDF(successfulReduceAttemptTimes, attemptTimesPercentiles,\n         100);\n     result.setSuccessfulReduceAttemptCDF(succReduce);\n \n     long totalSuccessfulAttempts \u003d 0L;\n     long maxTriesToSucceed \u003d 0L;\n \n     for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n       totalSuccessfulAttempts +\u003d ent.getValue();\n       maxTriesToSucceed \u003d Math.max(maxTriesToSucceed, ent.getKey());\n     }\n \n     if (totalSuccessfulAttempts \u003e 0L) {\n       double[] successAfterI \u003d new double[(int) maxTriesToSucceed + 1];\n       for (int i \u003d 0; i \u003c successAfterI.length; ++i) {\n         successAfterI[i] \u003d 0.0D;\n       }\n \n       for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n         successAfterI[ent.getKey().intValue()] \u003d\n             ((double) ent.getValue()) / totalSuccessfulAttempts;\n       }\n       result.setMapperTriesToSucceed(successAfterI);\n     } else {\n       result.setMapperTriesToSucceed(null);\n     }\n \n     return result;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public ParsedJob build() {\n    // The main job here is to build CDFs and manage the conf\n    finalized \u003d true;\n\n    // set the conf\n    if (jobConfigurationParameters !\u003d null) {\n      result.setJobProperties(jobConfigurationParameters);\n    }\n    \n    // initialize all the per-job statistics gathering places\n    Histogram[] successfulMapAttemptTimes \u003d\n        new Histogram[ParsedHost.numberOfDistances() + 1];\n    for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n      successfulMapAttemptTimes[i] \u003d new Histogram();\n    }\n\n    Histogram successfulReduceAttemptTimes \u003d new Histogram();\n    Histogram[] failedMapAttemptTimes \u003d\n        new Histogram[ParsedHost.numberOfDistances() + 1];\n    for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n      failedMapAttemptTimes[i] \u003d new Histogram();\n    }\n    Histogram failedReduceAttemptTimes \u003d new Histogram();\n\n    Histogram successfulNthMapperAttempts \u003d new Histogram();\n    // Histogram successfulNthReducerAttempts \u003d new Histogram();\n    // Histogram mapperLocality \u003d new Histogram();\n\n    for (LoggedTask task : result.getMapTasks()) {\n      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n        int distance \u003d successfulMapAttemptTimes.length - 1;\n        Long runtime \u003d null;\n\n        if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n          runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n\n          if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n            LoggedLocation host \u003d attempt.getLocation();\n\n            List\u003cLoggedLocation\u003e locs \u003d task.getPreferredLocations();\n\n            if (host !\u003d null \u0026\u0026 locs !\u003d null) {\n              for (LoggedLocation loc : locs) {\n                ParsedHost preferedLoc \u003d new ParsedHost(loc);\n\n                distance \u003d\n                    Math.min(distance, preferedLoc\n                        .distance(new ParsedHost(host)));\n              }\n\n              // mapperLocality.enter(distance);\n            }\n\n            if (attempt.getStartTime() \u003e 0 \u0026\u0026 attempt.getFinishTime() \u003e 0) {\n              if (runtime !\u003d null) {\n                successfulMapAttemptTimes[distance].enter(runtime);\n              }\n            }\n\n            TaskAttemptID attemptID \u003d attempt.getAttemptID();\n\n            if (attemptID !\u003d null) {\n              successfulNthMapperAttempts.enter(attemptID.getId());\n            }\n          } else {\n            if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n              if (runtime !\u003d null) {\n                failedMapAttemptTimes[distance].enter(runtime);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    for (LoggedTask task : result.getReduceTasks()) {\n      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n        Long runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n\n        if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n          runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n        }\n        if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n          if (runtime !\u003d null) {\n            successfulReduceAttemptTimes.enter(runtime);\n          }\n        } else if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n          failedReduceAttemptTimes.enter(runtime);\n        }\n      }\n    }\n\n    result.setFailedMapAttemptCDFs(mapCDFArrayList(failedMapAttemptTimes));\n\n    LoggedDiscreteCDF failedReduce \u003d new LoggedDiscreteCDF();\n    failedReduce.setCDF(failedReduceAttemptTimes, attemptTimesPercentiles, 100);\n    result.setFailedReduceAttemptCDF(failedReduce);\n\n    result\n        .setSuccessfulMapAttemptCDFs(mapCDFArrayList(successfulMapAttemptTimes));\n\n    LoggedDiscreteCDF succReduce \u003d new LoggedDiscreteCDF();\n    succReduce.setCDF(successfulReduceAttemptTimes, attemptTimesPercentiles,\n        100);\n    result.setSuccessfulReduceAttemptCDF(succReduce);\n\n    long totalSuccessfulAttempts \u003d 0L;\n    long maxTriesToSucceed \u003d 0L;\n\n    for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n      totalSuccessfulAttempts +\u003d ent.getValue();\n      maxTriesToSucceed \u003d Math.max(maxTriesToSucceed, ent.getKey());\n    }\n\n    if (totalSuccessfulAttempts \u003e 0L) {\n      double[] successAfterI \u003d new double[(int) maxTriesToSucceed + 1];\n      for (int i \u003d 0; i \u003c successAfterI.length; ++i) {\n        successAfterI[i] \u003d 0.0D;\n      }\n\n      for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n        successAfterI[ent.getKey().intValue()] \u003d\n            ((double) ent.getValue()) / totalSuccessfulAttempts;\n      }\n      result.setMapperTriesToSucceed(successAfterI);\n    } else {\n      result.setMapperTriesToSucceed(null);\n    }\n\n    return result;\n  }",
      "path": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
      "extendedDetails": {
        "oldValue": "LoggedJob",
        "newValue": "ParsedJob"
      }
    },
    "a238f931ea7dce0ca620d1798156c84ff77097ff": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-778. Rumen Anonymizer. (Amar Kamat and Chris Douglas via amarrk)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1215141 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/12/11 6:20 AM",
      "commitName": "a238f931ea7dce0ca620d1798156c84ff77097ff",
      "commitAuthor": "Amar Kamat",
      "commitDateOld": "31/10/11 10:27 AM",
      "commitNameOld": "9db078212f5a37154925cc8872f9adaeca0ed371",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 45.87,
      "commitsBetweenForRepo": 279,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,139 +1,131 @@\n   public LoggedJob build() {\n     // The main job here is to build CDFs and manage the conf\n     finalized \u003d true;\n \n     // set the conf\n-    result.setJobProperties(jobConfigurationParameters);\n+    if (jobConfigurationParameters !\u003d null) {\n+      result.setJobProperties(jobConfigurationParameters);\n+    }\n     \n     // initialize all the per-job statistics gathering places\n     Histogram[] successfulMapAttemptTimes \u003d\n         new Histogram[ParsedHost.numberOfDistances() + 1];\n     for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n       successfulMapAttemptTimes[i] \u003d new Histogram();\n     }\n \n     Histogram successfulReduceAttemptTimes \u003d new Histogram();\n     Histogram[] failedMapAttemptTimes \u003d\n         new Histogram[ParsedHost.numberOfDistances() + 1];\n     for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n       failedMapAttemptTimes[i] \u003d new Histogram();\n     }\n     Histogram failedReduceAttemptTimes \u003d new Histogram();\n \n     Histogram successfulNthMapperAttempts \u003d new Histogram();\n     // Histogram successfulNthReducerAttempts \u003d new Histogram();\n     // Histogram mapperLocality \u003d new Histogram();\n \n     for (LoggedTask task : result.getMapTasks()) {\n       for (LoggedTaskAttempt attempt : task.getAttempts()) {\n         int distance \u003d successfulMapAttemptTimes.length - 1;\n         Long runtime \u003d null;\n \n         if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n           runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n \n           if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n             LoggedLocation host \u003d attempt.getLocation();\n \n             List\u003cLoggedLocation\u003e locs \u003d task.getPreferredLocations();\n \n             if (host !\u003d null \u0026\u0026 locs !\u003d null) {\n               for (LoggedLocation loc : locs) {\n                 ParsedHost preferedLoc \u003d new ParsedHost(loc);\n \n                 distance \u003d\n                     Math.min(distance, preferedLoc\n                         .distance(new ParsedHost(host)));\n               }\n \n               // mapperLocality.enter(distance);\n             }\n \n             if (attempt.getStartTime() \u003e 0 \u0026\u0026 attempt.getFinishTime() \u003e 0) {\n               if (runtime !\u003d null) {\n                 successfulMapAttemptTimes[distance].enter(runtime);\n               }\n             }\n \n-            String attemptID \u003d attempt.getAttemptID();\n+            TaskAttemptID attemptID \u003d attempt.getAttemptID();\n \n             if (attemptID !\u003d null) {\n-              Matcher matcher \u003d taskAttemptIDPattern.matcher(attemptID);\n-\n-              if (matcher.matches()) {\n-                String attemptNumberString \u003d matcher.group(1);\n-\n-                if (attemptNumberString !\u003d null) {\n-                  int attemptNumber \u003d Integer.parseInt(attemptNumberString);\n-\n-                  successfulNthMapperAttempts.enter(attemptNumber);\n-                }\n-              }\n+              successfulNthMapperAttempts.enter(attemptID.getId());\n             }\n           } else {\n             if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n               if (runtime !\u003d null) {\n                 failedMapAttemptTimes[distance].enter(runtime);\n               }\n             }\n           }\n         }\n       }\n     }\n \n     for (LoggedTask task : result.getReduceTasks()) {\n       for (LoggedTaskAttempt attempt : task.getAttempts()) {\n         Long runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n \n         if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n           runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n         }\n         if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n           if (runtime !\u003d null) {\n             successfulReduceAttemptTimes.enter(runtime);\n           }\n         } else if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n           failedReduceAttemptTimes.enter(runtime);\n         }\n       }\n     }\n \n     result.setFailedMapAttemptCDFs(mapCDFArrayList(failedMapAttemptTimes));\n \n     LoggedDiscreteCDF failedReduce \u003d new LoggedDiscreteCDF();\n     failedReduce.setCDF(failedReduceAttemptTimes, attemptTimesPercentiles, 100);\n     result.setFailedReduceAttemptCDF(failedReduce);\n \n     result\n         .setSuccessfulMapAttemptCDFs(mapCDFArrayList(successfulMapAttemptTimes));\n \n     LoggedDiscreteCDF succReduce \u003d new LoggedDiscreteCDF();\n     succReduce.setCDF(successfulReduceAttemptTimes, attemptTimesPercentiles,\n         100);\n     result.setSuccessfulReduceAttemptCDF(succReduce);\n \n     long totalSuccessfulAttempts \u003d 0L;\n     long maxTriesToSucceed \u003d 0L;\n \n     for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n       totalSuccessfulAttempts +\u003d ent.getValue();\n       maxTriesToSucceed \u003d Math.max(maxTriesToSucceed, ent.getKey());\n     }\n \n     if (totalSuccessfulAttempts \u003e 0L) {\n       double[] successAfterI \u003d new double[(int) maxTriesToSucceed + 1];\n       for (int i \u003d 0; i \u003c successAfterI.length; ++i) {\n         successAfterI[i] \u003d 0.0D;\n       }\n \n       for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n         successAfterI[ent.getKey().intValue()] \u003d\n             ((double) ent.getValue()) / totalSuccessfulAttempts;\n       }\n       result.setMapperTriesToSucceed(successAfterI);\n     } else {\n       result.setMapperTriesToSucceed(null);\n     }\n \n     return result;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public LoggedJob build() {\n    // The main job here is to build CDFs and manage the conf\n    finalized \u003d true;\n\n    // set the conf\n    if (jobConfigurationParameters !\u003d null) {\n      result.setJobProperties(jobConfigurationParameters);\n    }\n    \n    // initialize all the per-job statistics gathering places\n    Histogram[] successfulMapAttemptTimes \u003d\n        new Histogram[ParsedHost.numberOfDistances() + 1];\n    for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n      successfulMapAttemptTimes[i] \u003d new Histogram();\n    }\n\n    Histogram successfulReduceAttemptTimes \u003d new Histogram();\n    Histogram[] failedMapAttemptTimes \u003d\n        new Histogram[ParsedHost.numberOfDistances() + 1];\n    for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n      failedMapAttemptTimes[i] \u003d new Histogram();\n    }\n    Histogram failedReduceAttemptTimes \u003d new Histogram();\n\n    Histogram successfulNthMapperAttempts \u003d new Histogram();\n    // Histogram successfulNthReducerAttempts \u003d new Histogram();\n    // Histogram mapperLocality \u003d new Histogram();\n\n    for (LoggedTask task : result.getMapTasks()) {\n      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n        int distance \u003d successfulMapAttemptTimes.length - 1;\n        Long runtime \u003d null;\n\n        if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n          runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n\n          if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n            LoggedLocation host \u003d attempt.getLocation();\n\n            List\u003cLoggedLocation\u003e locs \u003d task.getPreferredLocations();\n\n            if (host !\u003d null \u0026\u0026 locs !\u003d null) {\n              for (LoggedLocation loc : locs) {\n                ParsedHost preferedLoc \u003d new ParsedHost(loc);\n\n                distance \u003d\n                    Math.min(distance, preferedLoc\n                        .distance(new ParsedHost(host)));\n              }\n\n              // mapperLocality.enter(distance);\n            }\n\n            if (attempt.getStartTime() \u003e 0 \u0026\u0026 attempt.getFinishTime() \u003e 0) {\n              if (runtime !\u003d null) {\n                successfulMapAttemptTimes[distance].enter(runtime);\n              }\n            }\n\n            TaskAttemptID attemptID \u003d attempt.getAttemptID();\n\n            if (attemptID !\u003d null) {\n              successfulNthMapperAttempts.enter(attemptID.getId());\n            }\n          } else {\n            if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n              if (runtime !\u003d null) {\n                failedMapAttemptTimes[distance].enter(runtime);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    for (LoggedTask task : result.getReduceTasks()) {\n      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n        Long runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n\n        if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n          runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n        }\n        if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n          if (runtime !\u003d null) {\n            successfulReduceAttemptTimes.enter(runtime);\n          }\n        } else if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n          failedReduceAttemptTimes.enter(runtime);\n        }\n      }\n    }\n\n    result.setFailedMapAttemptCDFs(mapCDFArrayList(failedMapAttemptTimes));\n\n    LoggedDiscreteCDF failedReduce \u003d new LoggedDiscreteCDF();\n    failedReduce.setCDF(failedReduceAttemptTimes, attemptTimesPercentiles, 100);\n    result.setFailedReduceAttemptCDF(failedReduce);\n\n    result\n        .setSuccessfulMapAttemptCDFs(mapCDFArrayList(successfulMapAttemptTimes));\n\n    LoggedDiscreteCDF succReduce \u003d new LoggedDiscreteCDF();\n    succReduce.setCDF(successfulReduceAttemptTimes, attemptTimesPercentiles,\n        100);\n    result.setSuccessfulReduceAttemptCDF(succReduce);\n\n    long totalSuccessfulAttempts \u003d 0L;\n    long maxTriesToSucceed \u003d 0L;\n\n    for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n      totalSuccessfulAttempts +\u003d ent.getValue();\n      maxTriesToSucceed \u003d Math.max(maxTriesToSucceed, ent.getKey());\n    }\n\n    if (totalSuccessfulAttempts \u003e 0L) {\n      double[] successAfterI \u003d new double[(int) maxTriesToSucceed + 1];\n      for (int i \u003d 0; i \u003c successAfterI.length; ++i) {\n        successAfterI[i] \u003d 0.0D;\n      }\n\n      for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n        successAfterI[ent.getKey().intValue()] \u003d\n            ((double) ent.getValue()) / totalSuccessfulAttempts;\n      }\n      result.setMapperTriesToSucceed(successAfterI);\n    } else {\n      result.setMapperTriesToSucceed(null);\n    }\n\n    return result;\n  }",
      "path": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public LoggedJob build() {\n    // The main job here is to build CDFs and manage the conf\n    finalized \u003d true;\n\n    // set the conf\n    result.setJobProperties(jobConfigurationParameters);\n    \n    // initialize all the per-job statistics gathering places\n    Histogram[] successfulMapAttemptTimes \u003d\n        new Histogram[ParsedHost.numberOfDistances() + 1];\n    for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n      successfulMapAttemptTimes[i] \u003d new Histogram();\n    }\n\n    Histogram successfulReduceAttemptTimes \u003d new Histogram();\n    Histogram[] failedMapAttemptTimes \u003d\n        new Histogram[ParsedHost.numberOfDistances() + 1];\n    for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n      failedMapAttemptTimes[i] \u003d new Histogram();\n    }\n    Histogram failedReduceAttemptTimes \u003d new Histogram();\n\n    Histogram successfulNthMapperAttempts \u003d new Histogram();\n    // Histogram successfulNthReducerAttempts \u003d new Histogram();\n    // Histogram mapperLocality \u003d new Histogram();\n\n    for (LoggedTask task : result.getMapTasks()) {\n      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n        int distance \u003d successfulMapAttemptTimes.length - 1;\n        Long runtime \u003d null;\n\n        if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n          runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n\n          if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n            LoggedLocation host \u003d attempt.getLocation();\n\n            List\u003cLoggedLocation\u003e locs \u003d task.getPreferredLocations();\n\n            if (host !\u003d null \u0026\u0026 locs !\u003d null) {\n              for (LoggedLocation loc : locs) {\n                ParsedHost preferedLoc \u003d new ParsedHost(loc);\n\n                distance \u003d\n                    Math.min(distance, preferedLoc\n                        .distance(new ParsedHost(host)));\n              }\n\n              // mapperLocality.enter(distance);\n            }\n\n            if (attempt.getStartTime() \u003e 0 \u0026\u0026 attempt.getFinishTime() \u003e 0) {\n              if (runtime !\u003d null) {\n                successfulMapAttemptTimes[distance].enter(runtime);\n              }\n            }\n\n            String attemptID \u003d attempt.getAttemptID();\n\n            if (attemptID !\u003d null) {\n              Matcher matcher \u003d taskAttemptIDPattern.matcher(attemptID);\n\n              if (matcher.matches()) {\n                String attemptNumberString \u003d matcher.group(1);\n\n                if (attemptNumberString !\u003d null) {\n                  int attemptNumber \u003d Integer.parseInt(attemptNumberString);\n\n                  successfulNthMapperAttempts.enter(attemptNumber);\n                }\n              }\n            }\n          } else {\n            if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n              if (runtime !\u003d null) {\n                failedMapAttemptTimes[distance].enter(runtime);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    for (LoggedTask task : result.getReduceTasks()) {\n      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n        Long runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n\n        if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n          runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n        }\n        if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n          if (runtime !\u003d null) {\n            successfulReduceAttemptTimes.enter(runtime);\n          }\n        } else if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n          failedReduceAttemptTimes.enter(runtime);\n        }\n      }\n    }\n\n    result.setFailedMapAttemptCDFs(mapCDFArrayList(failedMapAttemptTimes));\n\n    LoggedDiscreteCDF failedReduce \u003d new LoggedDiscreteCDF();\n    failedReduce.setCDF(failedReduceAttemptTimes, attemptTimesPercentiles, 100);\n    result.setFailedReduceAttemptCDF(failedReduce);\n\n    result\n        .setSuccessfulMapAttemptCDFs(mapCDFArrayList(successfulMapAttemptTimes));\n\n    LoggedDiscreteCDF succReduce \u003d new LoggedDiscreteCDF();\n    succReduce.setCDF(successfulReduceAttemptTimes, attemptTimesPercentiles,\n        100);\n    result.setSuccessfulReduceAttemptCDF(succReduce);\n\n    long totalSuccessfulAttempts \u003d 0L;\n    long maxTriesToSucceed \u003d 0L;\n\n    for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n      totalSuccessfulAttempts +\u003d ent.getValue();\n      maxTriesToSucceed \u003d Math.max(maxTriesToSucceed, ent.getKey());\n    }\n\n    if (totalSuccessfulAttempts \u003e 0L) {\n      double[] successAfterI \u003d new double[(int) maxTriesToSucceed + 1];\n      for (int i \u003d 0; i \u003c successAfterI.length; ++i) {\n        successAfterI[i] \u003d 0.0D;\n      }\n\n      for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n        successAfterI[ent.getKey().intValue()] \u003d\n            ((double) ent.getValue()) / totalSuccessfulAttempts;\n      }\n      result.setMapperTriesToSucceed(successAfterI);\n    } else {\n      result.setMapperTriesToSucceed(null);\n    }\n\n    return result;\n  }",
      "path": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
        "newPath": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public LoggedJob build() {\n    // The main job here is to build CDFs and manage the conf\n    finalized \u003d true;\n\n    // set the conf\n    result.setJobProperties(jobConfigurationParameters);\n    \n    // initialize all the per-job statistics gathering places\n    Histogram[] successfulMapAttemptTimes \u003d\n        new Histogram[ParsedHost.numberOfDistances() + 1];\n    for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n      successfulMapAttemptTimes[i] \u003d new Histogram();\n    }\n\n    Histogram successfulReduceAttemptTimes \u003d new Histogram();\n    Histogram[] failedMapAttemptTimes \u003d\n        new Histogram[ParsedHost.numberOfDistances() + 1];\n    for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n      failedMapAttemptTimes[i] \u003d new Histogram();\n    }\n    Histogram failedReduceAttemptTimes \u003d new Histogram();\n\n    Histogram successfulNthMapperAttempts \u003d new Histogram();\n    // Histogram successfulNthReducerAttempts \u003d new Histogram();\n    // Histogram mapperLocality \u003d new Histogram();\n\n    for (LoggedTask task : result.getMapTasks()) {\n      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n        int distance \u003d successfulMapAttemptTimes.length - 1;\n        Long runtime \u003d null;\n\n        if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n          runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n\n          if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n            LoggedLocation host \u003d attempt.getLocation();\n\n            List\u003cLoggedLocation\u003e locs \u003d task.getPreferredLocations();\n\n            if (host !\u003d null \u0026\u0026 locs !\u003d null) {\n              for (LoggedLocation loc : locs) {\n                ParsedHost preferedLoc \u003d new ParsedHost(loc);\n\n                distance \u003d\n                    Math.min(distance, preferedLoc\n                        .distance(new ParsedHost(host)));\n              }\n\n              // mapperLocality.enter(distance);\n            }\n\n            if (attempt.getStartTime() \u003e 0 \u0026\u0026 attempt.getFinishTime() \u003e 0) {\n              if (runtime !\u003d null) {\n                successfulMapAttemptTimes[distance].enter(runtime);\n              }\n            }\n\n            String attemptID \u003d attempt.getAttemptID();\n\n            if (attemptID !\u003d null) {\n              Matcher matcher \u003d taskAttemptIDPattern.matcher(attemptID);\n\n              if (matcher.matches()) {\n                String attemptNumberString \u003d matcher.group(1);\n\n                if (attemptNumberString !\u003d null) {\n                  int attemptNumber \u003d Integer.parseInt(attemptNumberString);\n\n                  successfulNthMapperAttempts.enter(attemptNumber);\n                }\n              }\n            }\n          } else {\n            if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n              if (runtime !\u003d null) {\n                failedMapAttemptTimes[distance].enter(runtime);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    for (LoggedTask task : result.getReduceTasks()) {\n      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n        Long runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n\n        if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n          runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n        }\n        if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n          if (runtime !\u003d null) {\n            successfulReduceAttemptTimes.enter(runtime);\n          }\n        } else if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n          failedReduceAttemptTimes.enter(runtime);\n        }\n      }\n    }\n\n    result.setFailedMapAttemptCDFs(mapCDFArrayList(failedMapAttemptTimes));\n\n    LoggedDiscreteCDF failedReduce \u003d new LoggedDiscreteCDF();\n    failedReduce.setCDF(failedReduceAttemptTimes, attemptTimesPercentiles, 100);\n    result.setFailedReduceAttemptCDF(failedReduce);\n\n    result\n        .setSuccessfulMapAttemptCDFs(mapCDFArrayList(successfulMapAttemptTimes));\n\n    LoggedDiscreteCDF succReduce \u003d new LoggedDiscreteCDF();\n    succReduce.setCDF(successfulReduceAttemptTimes, attemptTimesPercentiles,\n        100);\n    result.setSuccessfulReduceAttemptCDF(succReduce);\n\n    long totalSuccessfulAttempts \u003d 0L;\n    long maxTriesToSucceed \u003d 0L;\n\n    for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n      totalSuccessfulAttempts +\u003d ent.getValue();\n      maxTriesToSucceed \u003d Math.max(maxTriesToSucceed, ent.getKey());\n    }\n\n    if (totalSuccessfulAttempts \u003e 0L) {\n      double[] successAfterI \u003d new double[(int) maxTriesToSucceed + 1];\n      for (int i \u003d 0; i \u003c successAfterI.length; ++i) {\n        successAfterI[i] \u003d 0.0D;\n      }\n\n      for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n        successAfterI[ent.getKey().intValue()] \u003d\n            ((double) ent.getValue()) / totalSuccessfulAttempts;\n      }\n      result.setMapperTriesToSucceed(successAfterI);\n    } else {\n      result.setMapperTriesToSucceed(null);\n    }\n\n    return result;\n  }",
      "path": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
        "newPath": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,139 @@\n+  public LoggedJob build() {\n+    // The main job here is to build CDFs and manage the conf\n+    finalized \u003d true;\n+\n+    // set the conf\n+    result.setJobProperties(jobConfigurationParameters);\n+    \n+    // initialize all the per-job statistics gathering places\n+    Histogram[] successfulMapAttemptTimes \u003d\n+        new Histogram[ParsedHost.numberOfDistances() + 1];\n+    for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n+      successfulMapAttemptTimes[i] \u003d new Histogram();\n+    }\n+\n+    Histogram successfulReduceAttemptTimes \u003d new Histogram();\n+    Histogram[] failedMapAttemptTimes \u003d\n+        new Histogram[ParsedHost.numberOfDistances() + 1];\n+    for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n+      failedMapAttemptTimes[i] \u003d new Histogram();\n+    }\n+    Histogram failedReduceAttemptTimes \u003d new Histogram();\n+\n+    Histogram successfulNthMapperAttempts \u003d new Histogram();\n+    // Histogram successfulNthReducerAttempts \u003d new Histogram();\n+    // Histogram mapperLocality \u003d new Histogram();\n+\n+    for (LoggedTask task : result.getMapTasks()) {\n+      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n+        int distance \u003d successfulMapAttemptTimes.length - 1;\n+        Long runtime \u003d null;\n+\n+        if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n+          runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n+\n+          if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n+            LoggedLocation host \u003d attempt.getLocation();\n+\n+            List\u003cLoggedLocation\u003e locs \u003d task.getPreferredLocations();\n+\n+            if (host !\u003d null \u0026\u0026 locs !\u003d null) {\n+              for (LoggedLocation loc : locs) {\n+                ParsedHost preferedLoc \u003d new ParsedHost(loc);\n+\n+                distance \u003d\n+                    Math.min(distance, preferedLoc\n+                        .distance(new ParsedHost(host)));\n+              }\n+\n+              // mapperLocality.enter(distance);\n+            }\n+\n+            if (attempt.getStartTime() \u003e 0 \u0026\u0026 attempt.getFinishTime() \u003e 0) {\n+              if (runtime !\u003d null) {\n+                successfulMapAttemptTimes[distance].enter(runtime);\n+              }\n+            }\n+\n+            String attemptID \u003d attempt.getAttemptID();\n+\n+            if (attemptID !\u003d null) {\n+              Matcher matcher \u003d taskAttemptIDPattern.matcher(attemptID);\n+\n+              if (matcher.matches()) {\n+                String attemptNumberString \u003d matcher.group(1);\n+\n+                if (attemptNumberString !\u003d null) {\n+                  int attemptNumber \u003d Integer.parseInt(attemptNumberString);\n+\n+                  successfulNthMapperAttempts.enter(attemptNumber);\n+                }\n+              }\n+            }\n+          } else {\n+            if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n+              if (runtime !\u003d null) {\n+                failedMapAttemptTimes[distance].enter(runtime);\n+              }\n+            }\n+          }\n+        }\n+      }\n+    }\n+\n+    for (LoggedTask task : result.getReduceTasks()) {\n+      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n+        Long runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n+\n+        if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n+          runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n+        }\n+        if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n+          if (runtime !\u003d null) {\n+            successfulReduceAttemptTimes.enter(runtime);\n+          }\n+        } else if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n+          failedReduceAttemptTimes.enter(runtime);\n+        }\n+      }\n+    }\n+\n+    result.setFailedMapAttemptCDFs(mapCDFArrayList(failedMapAttemptTimes));\n+\n+    LoggedDiscreteCDF failedReduce \u003d new LoggedDiscreteCDF();\n+    failedReduce.setCDF(failedReduceAttemptTimes, attemptTimesPercentiles, 100);\n+    result.setFailedReduceAttemptCDF(failedReduce);\n+\n+    result\n+        .setSuccessfulMapAttemptCDFs(mapCDFArrayList(successfulMapAttemptTimes));\n+\n+    LoggedDiscreteCDF succReduce \u003d new LoggedDiscreteCDF();\n+    succReduce.setCDF(successfulReduceAttemptTimes, attemptTimesPercentiles,\n+        100);\n+    result.setSuccessfulReduceAttemptCDF(succReduce);\n+\n+    long totalSuccessfulAttempts \u003d 0L;\n+    long maxTriesToSucceed \u003d 0L;\n+\n+    for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n+      totalSuccessfulAttempts +\u003d ent.getValue();\n+      maxTriesToSucceed \u003d Math.max(maxTriesToSucceed, ent.getKey());\n+    }\n+\n+    if (totalSuccessfulAttempts \u003e 0L) {\n+      double[] successAfterI \u003d new double[(int) maxTriesToSucceed + 1];\n+      for (int i \u003d 0; i \u003c successAfterI.length; ++i) {\n+        successAfterI[i] \u003d 0.0D;\n+      }\n+\n+      for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n+        successAfterI[ent.getKey().intValue()] \u003d\n+            ((double) ent.getValue()) / totalSuccessfulAttempts;\n+      }\n+      result.setMapperTriesToSucceed(successAfterI);\n+    } else {\n+      result.setMapperTriesToSucceed(null);\n+    }\n+\n+    return result;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public LoggedJob build() {\n    // The main job here is to build CDFs and manage the conf\n    finalized \u003d true;\n\n    // set the conf\n    result.setJobProperties(jobConfigurationParameters);\n    \n    // initialize all the per-job statistics gathering places\n    Histogram[] successfulMapAttemptTimes \u003d\n        new Histogram[ParsedHost.numberOfDistances() + 1];\n    for (int i \u003d 0; i \u003c successfulMapAttemptTimes.length; ++i) {\n      successfulMapAttemptTimes[i] \u003d new Histogram();\n    }\n\n    Histogram successfulReduceAttemptTimes \u003d new Histogram();\n    Histogram[] failedMapAttemptTimes \u003d\n        new Histogram[ParsedHost.numberOfDistances() + 1];\n    for (int i \u003d 0; i \u003c failedMapAttemptTimes.length; ++i) {\n      failedMapAttemptTimes[i] \u003d new Histogram();\n    }\n    Histogram failedReduceAttemptTimes \u003d new Histogram();\n\n    Histogram successfulNthMapperAttempts \u003d new Histogram();\n    // Histogram successfulNthReducerAttempts \u003d new Histogram();\n    // Histogram mapperLocality \u003d new Histogram();\n\n    for (LoggedTask task : result.getMapTasks()) {\n      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n        int distance \u003d successfulMapAttemptTimes.length - 1;\n        Long runtime \u003d null;\n\n        if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n          runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n\n          if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n            LoggedLocation host \u003d attempt.getLocation();\n\n            List\u003cLoggedLocation\u003e locs \u003d task.getPreferredLocations();\n\n            if (host !\u003d null \u0026\u0026 locs !\u003d null) {\n              for (LoggedLocation loc : locs) {\n                ParsedHost preferedLoc \u003d new ParsedHost(loc);\n\n                distance \u003d\n                    Math.min(distance, preferedLoc\n                        .distance(new ParsedHost(host)));\n              }\n\n              // mapperLocality.enter(distance);\n            }\n\n            if (attempt.getStartTime() \u003e 0 \u0026\u0026 attempt.getFinishTime() \u003e 0) {\n              if (runtime !\u003d null) {\n                successfulMapAttemptTimes[distance].enter(runtime);\n              }\n            }\n\n            String attemptID \u003d attempt.getAttemptID();\n\n            if (attemptID !\u003d null) {\n              Matcher matcher \u003d taskAttemptIDPattern.matcher(attemptID);\n\n              if (matcher.matches()) {\n                String attemptNumberString \u003d matcher.group(1);\n\n                if (attemptNumberString !\u003d null) {\n                  int attemptNumber \u003d Integer.parseInt(attemptNumberString);\n\n                  successfulNthMapperAttempts.enter(attemptNumber);\n                }\n              }\n            }\n          } else {\n            if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n              if (runtime !\u003d null) {\n                failedMapAttemptTimes[distance].enter(runtime);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    for (LoggedTask task : result.getReduceTasks()) {\n      for (LoggedTaskAttempt attempt : task.getAttempts()) {\n        Long runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n\n        if (attempt.getFinishTime() \u003e 0 \u0026\u0026 attempt.getStartTime() \u003e 0) {\n          runtime \u003d attempt.getFinishTime() - attempt.getStartTime();\n        }\n        if (attempt.getResult() \u003d\u003d Values.SUCCESS) {\n          if (runtime !\u003d null) {\n            successfulReduceAttemptTimes.enter(runtime);\n          }\n        } else if (attempt.getResult() \u003d\u003d Pre21JobHistoryConstants.Values.FAILED) {\n          failedReduceAttemptTimes.enter(runtime);\n        }\n      }\n    }\n\n    result.setFailedMapAttemptCDFs(mapCDFArrayList(failedMapAttemptTimes));\n\n    LoggedDiscreteCDF failedReduce \u003d new LoggedDiscreteCDF();\n    failedReduce.setCDF(failedReduceAttemptTimes, attemptTimesPercentiles, 100);\n    result.setFailedReduceAttemptCDF(failedReduce);\n\n    result\n        .setSuccessfulMapAttemptCDFs(mapCDFArrayList(successfulMapAttemptTimes));\n\n    LoggedDiscreteCDF succReduce \u003d new LoggedDiscreteCDF();\n    succReduce.setCDF(successfulReduceAttemptTimes, attemptTimesPercentiles,\n        100);\n    result.setSuccessfulReduceAttemptCDF(succReduce);\n\n    long totalSuccessfulAttempts \u003d 0L;\n    long maxTriesToSucceed \u003d 0L;\n\n    for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n      totalSuccessfulAttempts +\u003d ent.getValue();\n      maxTriesToSucceed \u003d Math.max(maxTriesToSucceed, ent.getKey());\n    }\n\n    if (totalSuccessfulAttempts \u003e 0L) {\n      double[] successAfterI \u003d new double[(int) maxTriesToSucceed + 1];\n      for (int i \u003d 0; i \u003c successAfterI.length; ++i) {\n        successAfterI[i] \u003d 0.0D;\n      }\n\n      for (Map.Entry\u003cLong, Long\u003e ent : successfulNthMapperAttempts) {\n        successAfterI[ent.getKey().intValue()] \u003d\n            ((double) ent.getValue()) / totalSuccessfulAttempts;\n      }\n      result.setMapperTriesToSucceed(successAfterI);\n    } else {\n      result.setMapperTriesToSucceed(null);\n    }\n\n    return result;\n  }",
      "path": "mapreduce/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java"
    }
  }
}