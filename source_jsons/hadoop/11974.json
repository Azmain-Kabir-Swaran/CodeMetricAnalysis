{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetImpl.java",
  "functionName": "copyBlockFiles",
  "functionId": "copyBlockFiles___blockId-long__genStamp-long__srcReplica-ReplicaInfo__destRoot-File__calculateChecksum-boolean__smallBufferSize-int__conf-Configuration(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
  "functionStartLine": 952,
  "functionEndLine": 961,
  "numCommitsSeen": 600,
  "timeTaken": 10659,
  "changeHistory": [
    "86c9862bec0248d671e657aa56094a2919b8ac14",
    "c992bcf9c136d3df686655a80e636bb7bb0664da",
    "4da8490b512a33a255ed27309860859388d7c168",
    "08ac06283a3e9bf0d49d873823aabd419b08e41f",
    "058af60c56207907f2bedf76df4284e86d923e0c",
    "463aec11718e47d4aabb86a7a539cb973460aae6",
    "1770bb942f9ebea38b6811ba0bc3cc249ef3ccbb"
  ],
  "changeHistoryShort": {
    "86c9862bec0248d671e657aa56094a2919b8ac14": "Ymultichange(Yparameterchange,Ybodychange)",
    "c992bcf9c136d3df686655a80e636bb7bb0664da": "Ymultichange(Yparameterchange,Ybodychange)",
    "4da8490b512a33a255ed27309860859388d7c168": "Ymultichange(Yparameterchange,Ybodychange)",
    "08ac06283a3e9bf0d49d873823aabd419b08e41f": "Ybodychange",
    "058af60c56207907f2bedf76df4284e86d923e0c": "Ymultichange(Yparameterchange,Ybodychange)",
    "463aec11718e47d4aabb86a7a539cb973460aae6": "Ybodychange",
    "1770bb942f9ebea38b6811ba0bc3cc249ef3ccbb": "Ybodychange"
  },
  "changeHistoryDetails": {
    "86c9862bec0248d671e657aa56094a2919b8ac14": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "13/09/16 12:54 PM",
      "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
      "commitAuthor": "Lei Xu",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
          "commitDate": "13/09/16 12:54 PM",
          "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "10/09/16 6:22 PM",
          "commitNameOld": "a99bf26a0899bcc4307c3a242c8414eaef555aa7",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 2.77,
          "commitsBetweenForRepo": 15,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,9 +1,10 @@\n-  static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n-      File srcFile, File destRoot, boolean calculateChecksum,\n+  static File[] copyBlockFiles(long blockId, long genStamp,\n+      ReplicaInfo srcReplica, File destRoot, boolean calculateChecksum,\n       int smallBufferSize, final Configuration conf) throws IOException {\n     final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n-    final File dstFile \u003d new File(destDir, srcFile.getName());\n+    // blockName is same as the filename for the block\n+    final File dstFile \u003d new File(destDir, srcReplica.getBlockName());\n     final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n-    return copyBlockFiles(srcMeta, srcFile, dstMeta, dstFile, calculateChecksum,\n+    return copyBlockFiles(srcReplica, dstMeta, dstFile, calculateChecksum,\n         smallBufferSize, conf);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static File[] copyBlockFiles(long blockId, long genStamp,\n      ReplicaInfo srcReplica, File destRoot, boolean calculateChecksum,\n      int smallBufferSize, final Configuration conf) throws IOException {\n    final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n    // blockName is same as the filename for the block\n    final File dstFile \u003d new File(destDir, srcReplica.getBlockName());\n    final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n    return copyBlockFiles(srcReplica, dstMeta, dstFile, calculateChecksum,\n        smallBufferSize, conf);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[blockId-long, genStamp-long, srcMeta-File, srcFile-File, destRoot-File, calculateChecksum-boolean, smallBufferSize-int, conf-Configuration(modifiers-final)]",
            "newValue": "[blockId-long, genStamp-long, srcReplica-ReplicaInfo, destRoot-File, calculateChecksum-boolean, smallBufferSize-int, conf-Configuration(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
          "commitDate": "13/09/16 12:54 PM",
          "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "10/09/16 6:22 PM",
          "commitNameOld": "a99bf26a0899bcc4307c3a242c8414eaef555aa7",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 2.77,
          "commitsBetweenForRepo": 15,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,9 +1,10 @@\n-  static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n-      File srcFile, File destRoot, boolean calculateChecksum,\n+  static File[] copyBlockFiles(long blockId, long genStamp,\n+      ReplicaInfo srcReplica, File destRoot, boolean calculateChecksum,\n       int smallBufferSize, final Configuration conf) throws IOException {\n     final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n-    final File dstFile \u003d new File(destDir, srcFile.getName());\n+    // blockName is same as the filename for the block\n+    final File dstFile \u003d new File(destDir, srcReplica.getBlockName());\n     final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n-    return copyBlockFiles(srcMeta, srcFile, dstMeta, dstFile, calculateChecksum,\n+    return copyBlockFiles(srcReplica, dstMeta, dstFile, calculateChecksum,\n         smallBufferSize, conf);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static File[] copyBlockFiles(long blockId, long genStamp,\n      ReplicaInfo srcReplica, File destRoot, boolean calculateChecksum,\n      int smallBufferSize, final Configuration conf) throws IOException {\n    final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n    // blockName is same as the filename for the block\n    final File dstFile \u003d new File(destDir, srcReplica.getBlockName());\n    final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n    return copyBlockFiles(srcReplica, dstMeta, dstFile, calculateChecksum,\n        smallBufferSize, conf);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "c992bcf9c136d3df686655a80e636bb7bb0664da": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8951. Move the shortcircuit package to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/08/15 2:02 PM",
      "commitName": "c992bcf9c136d3df686655a80e636bb7bb0664da",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8951. Move the shortcircuit package to hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "26/08/15 2:02 PM",
          "commitName": "c992bcf9c136d3df686655a80e636bb7bb0664da",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/08/15 5:40 PM",
          "commitNameOld": "eee4d716b48074825e1afcd9c74038a393ddeb69",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 8.85,
          "commitsBetweenForRepo": 45,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,9 +1,9 @@\n   static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n       File srcFile, File destRoot, boolean calculateChecksum,\n-      int smallBufferSize) throws IOException {\n+      int smallBufferSize, final Configuration conf) throws IOException {\n     final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n     final File dstFile \u003d new File(destDir, srcFile.getName());\n     final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n     return copyBlockFiles(srcMeta, srcFile, dstMeta, dstFile, calculateChecksum,\n-        smallBufferSize);\n+        smallBufferSize, conf);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n      File srcFile, File destRoot, boolean calculateChecksum,\n      int smallBufferSize, final Configuration conf) throws IOException {\n    final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n    final File dstFile \u003d new File(destDir, srcFile.getName());\n    final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n    return copyBlockFiles(srcMeta, srcFile, dstMeta, dstFile, calculateChecksum,\n        smallBufferSize, conf);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[blockId-long, genStamp-long, srcMeta-File, srcFile-File, destRoot-File, calculateChecksum-boolean, smallBufferSize-int]",
            "newValue": "[blockId-long, genStamp-long, srcMeta-File, srcFile-File, destRoot-File, calculateChecksum-boolean, smallBufferSize-int, conf-Configuration(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8951. Move the shortcircuit package to hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "26/08/15 2:02 PM",
          "commitName": "c992bcf9c136d3df686655a80e636bb7bb0664da",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/08/15 5:40 PM",
          "commitNameOld": "eee4d716b48074825e1afcd9c74038a393ddeb69",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 8.85,
          "commitsBetweenForRepo": 45,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,9 +1,9 @@\n   static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n       File srcFile, File destRoot, boolean calculateChecksum,\n-      int smallBufferSize) throws IOException {\n+      int smallBufferSize, final Configuration conf) throws IOException {\n     final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n     final File dstFile \u003d new File(destDir, srcFile.getName());\n     final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n     return copyBlockFiles(srcMeta, srcFile, dstMeta, dstFile, calculateChecksum,\n-        smallBufferSize);\n+        smallBufferSize, conf);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n      File srcFile, File destRoot, boolean calculateChecksum,\n      int smallBufferSize, final Configuration conf) throws IOException {\n    final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n    final File dstFile \u003d new File(destDir, srcFile.getName());\n    final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n    return copyBlockFiles(srcMeta, srcFile, dstMeta, dstFile, calculateChecksum,\n        smallBufferSize, conf);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "4da8490b512a33a255ed27309860859388d7c168": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
      "commitDate": "05/05/15 3:41 PM",
      "commitName": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
          "commitDate": "05/05/15 3:41 PM",
          "commitName": "4da8490b512a33a255ed27309860859388d7c168",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "05/05/15 11:08 AM",
          "commitNameOld": "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.19,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,9 @@\n   static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n-      File srcFile, File destRoot, boolean calculateChecksum)\n-      throws IOException {\n+      File srcFile, File destRoot, boolean calculateChecksum,\n+      int smallBufferSize) throws IOException {\n     final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n     final File dstFile \u003d new File(destDir, srcFile.getName());\n     final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n-    return copyBlockFiles(srcMeta, srcFile, dstMeta, dstFile, calculateChecksum);\n+    return copyBlockFiles(srcMeta, srcFile, dstMeta, dstFile, calculateChecksum,\n+        smallBufferSize);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n      File srcFile, File destRoot, boolean calculateChecksum,\n      int smallBufferSize) throws IOException {\n    final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n    final File dstFile \u003d new File(destDir, srcFile.getName());\n    final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n    return copyBlockFiles(srcMeta, srcFile, dstMeta, dstFile, calculateChecksum,\n        smallBufferSize);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[blockId-long, genStamp-long, srcMeta-File, srcFile-File, destRoot-File, calculateChecksum-boolean]",
            "newValue": "[blockId-long, genStamp-long, srcMeta-File, srcFile-File, destRoot-File, calculateChecksum-boolean, smallBufferSize-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
          "commitDate": "05/05/15 3:41 PM",
          "commitName": "4da8490b512a33a255ed27309860859388d7c168",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "05/05/15 11:08 AM",
          "commitNameOld": "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.19,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,9 @@\n   static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n-      File srcFile, File destRoot, boolean calculateChecksum)\n-      throws IOException {\n+      File srcFile, File destRoot, boolean calculateChecksum,\n+      int smallBufferSize) throws IOException {\n     final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n     final File dstFile \u003d new File(destDir, srcFile.getName());\n     final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n-    return copyBlockFiles(srcMeta, srcFile, dstMeta, dstFile, calculateChecksum);\n+    return copyBlockFiles(srcMeta, srcFile, dstMeta, dstFile, calculateChecksum,\n+        smallBufferSize);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n      File srcFile, File destRoot, boolean calculateChecksum,\n      int smallBufferSize) throws IOException {\n    final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n    final File dstFile \u003d new File(destDir, srcFile.getName());\n    final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n    return copyBlockFiles(srcMeta, srcFile, dstMeta, dstFile, calculateChecksum,\n        smallBufferSize);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "08ac06283a3e9bf0d49d873823aabd419b08e41f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7056. Snapshot support for truncate. Contributed by Konstantin Shvachko and Plamen Jeliazkov.",
      "commitDate": "13/01/15 12:24 AM",
      "commitName": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "17/12/14 4:41 PM",
      "commitNameOld": "3b173d95171d01ab55042b1162569d1cf14a8d43",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 26.32,
      "commitsBetweenForRepo": 116,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,8 @@\n   static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n       File srcFile, File destRoot, boolean calculateChecksum)\n       throws IOException {\n     final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n     final File dstFile \u003d new File(destDir, srcFile.getName());\n     final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n-    if (calculateChecksum) {\n-      computeChecksum(srcMeta, dstMeta, srcFile);\n-    } else {\n-      try {\n-        Storage.nativeCopyFileUnbuffered(srcMeta, dstMeta, true);\n-      } catch (IOException e) {\n-        throw new IOException(\"Failed to copy \" + srcMeta + \" to \" + dstMeta, e);\n-      }\n-    }\n-\n-    try {\n-      Storage.nativeCopyFileUnbuffered(srcFile, dstFile, true);\n-    } catch (IOException e) {\n-      throw new IOException(\"Failed to copy \" + srcFile + \" to \" + dstFile, e);\n-    }\n-    if (LOG.isDebugEnabled()) {\n-      if (calculateChecksum) {\n-        LOG.debug(\"Copied \" + srcMeta + \" to \" + dstMeta\n-            + \" and calculated checksum\");\n-      } else {\n-        LOG.debug(\"Copied \" + srcFile + \" to \" + dstFile);\n-      }\n-    }\n-    return new File[] {dstMeta, dstFile};\n+    return copyBlockFiles(srcMeta, srcFile, dstMeta, dstFile, calculateChecksum);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n      File srcFile, File destRoot, boolean calculateChecksum)\n      throws IOException {\n    final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n    final File dstFile \u003d new File(destDir, srcFile.getName());\n    final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n    return copyBlockFiles(srcMeta, srcFile, dstMeta, dstFile, calculateChecksum);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "058af60c56207907f2bedf76df4284e86d923e0c": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7310. Mover can give first priority to local DN if it has target storage type available in local DN. (Vinayakumar B via umamahesh)\n",
      "commitDate": "26/11/14 9:57 AM",
      "commitName": "058af60c56207907f2bedf76df4284e86d923e0c",
      "commitAuthor": "Uma Maheswara Rao G",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7310. Mover can give first priority to local DN if it has target storage type available in local DN. (Vinayakumar B via umamahesh)\n",
          "commitDate": "26/11/14 9:57 AM",
          "commitName": "058af60c56207907f2bedf76df4284e86d923e0c",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "30/10/14 5:31 PM",
          "commitNameOld": "a9331fe9b071fdcdae0c6c747d7b6b306142e671",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 26.73,
          "commitsBetweenForRepo": 232,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,31 @@\n-  static File[] copyBlockFiles(long blockId, long genStamp,\n-                               File srcMeta, File srcFile, File destRoot)\n+  static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n+      File srcFile, File destRoot, boolean calculateChecksum)\n       throws IOException {\n     final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n     final File dstFile \u003d new File(destDir, srcFile.getName());\n     final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n-    computeChecksum(srcMeta, dstMeta, srcFile);\n+    if (calculateChecksum) {\n+      computeChecksum(srcMeta, dstMeta, srcFile);\n+    } else {\n+      try {\n+        Storage.nativeCopyFileUnbuffered(srcMeta, dstMeta, true);\n+      } catch (IOException e) {\n+        throw new IOException(\"Failed to copy \" + srcMeta + \" to \" + dstMeta, e);\n+      }\n+    }\n \n     try {\n       Storage.nativeCopyFileUnbuffered(srcFile, dstFile, true);\n     } catch (IOException e) {\n       throw new IOException(\"Failed to copy \" + srcFile + \" to \" + dstFile, e);\n     }\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Copied \" + srcMeta + \" to \" + dstMeta +\n-          \" and calculated checksum\");\n-      LOG.debug(\"Copied \" + srcFile + \" to \" + dstFile);\n+      if (calculateChecksum) {\n+        LOG.debug(\"Copied \" + srcMeta + \" to \" + dstMeta\n+            + \" and calculated checksum\");\n+      } else {\n+        LOG.debug(\"Copied \" + srcFile + \" to \" + dstFile);\n+      }\n     }\n     return new File[] {dstMeta, dstFile};\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n      File srcFile, File destRoot, boolean calculateChecksum)\n      throws IOException {\n    final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n    final File dstFile \u003d new File(destDir, srcFile.getName());\n    final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n    if (calculateChecksum) {\n      computeChecksum(srcMeta, dstMeta, srcFile);\n    } else {\n      try {\n        Storage.nativeCopyFileUnbuffered(srcMeta, dstMeta, true);\n      } catch (IOException e) {\n        throw new IOException(\"Failed to copy \" + srcMeta + \" to \" + dstMeta, e);\n      }\n    }\n\n    try {\n      Storage.nativeCopyFileUnbuffered(srcFile, dstFile, true);\n    } catch (IOException e) {\n      throw new IOException(\"Failed to copy \" + srcFile + \" to \" + dstFile, e);\n    }\n    if (LOG.isDebugEnabled()) {\n      if (calculateChecksum) {\n        LOG.debug(\"Copied \" + srcMeta + \" to \" + dstMeta\n            + \" and calculated checksum\");\n      } else {\n        LOG.debug(\"Copied \" + srcFile + \" to \" + dstFile);\n      }\n    }\n    return new File[] {dstMeta, dstFile};\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[blockId-long, genStamp-long, srcMeta-File, srcFile-File, destRoot-File]",
            "newValue": "[blockId-long, genStamp-long, srcMeta-File, srcFile-File, destRoot-File, calculateChecksum-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7310. Mover can give first priority to local DN if it has target storage type available in local DN. (Vinayakumar B via umamahesh)\n",
          "commitDate": "26/11/14 9:57 AM",
          "commitName": "058af60c56207907f2bedf76df4284e86d923e0c",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "30/10/14 5:31 PM",
          "commitNameOld": "a9331fe9b071fdcdae0c6c747d7b6b306142e671",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 26.73,
          "commitsBetweenForRepo": 232,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,31 @@\n-  static File[] copyBlockFiles(long blockId, long genStamp,\n-                               File srcMeta, File srcFile, File destRoot)\n+  static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n+      File srcFile, File destRoot, boolean calculateChecksum)\n       throws IOException {\n     final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n     final File dstFile \u003d new File(destDir, srcFile.getName());\n     final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n-    computeChecksum(srcMeta, dstMeta, srcFile);\n+    if (calculateChecksum) {\n+      computeChecksum(srcMeta, dstMeta, srcFile);\n+    } else {\n+      try {\n+        Storage.nativeCopyFileUnbuffered(srcMeta, dstMeta, true);\n+      } catch (IOException e) {\n+        throw new IOException(\"Failed to copy \" + srcMeta + \" to \" + dstMeta, e);\n+      }\n+    }\n \n     try {\n       Storage.nativeCopyFileUnbuffered(srcFile, dstFile, true);\n     } catch (IOException e) {\n       throw new IOException(\"Failed to copy \" + srcFile + \" to \" + dstFile, e);\n     }\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Copied \" + srcMeta + \" to \" + dstMeta +\n-          \" and calculated checksum\");\n-      LOG.debug(\"Copied \" + srcFile + \" to \" + dstFile);\n+      if (calculateChecksum) {\n+        LOG.debug(\"Copied \" + srcMeta + \" to \" + dstMeta\n+            + \" and calculated checksum\");\n+      } else {\n+        LOG.debug(\"Copied \" + srcFile + \" to \" + dstFile);\n+      }\n     }\n     return new File[] {dstMeta, dstFile};\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static File[] copyBlockFiles(long blockId, long genStamp, File srcMeta,\n      File srcFile, File destRoot, boolean calculateChecksum)\n      throws IOException {\n    final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n    final File dstFile \u003d new File(destDir, srcFile.getName());\n    final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n    if (calculateChecksum) {\n      computeChecksum(srcMeta, dstMeta, srcFile);\n    } else {\n      try {\n        Storage.nativeCopyFileUnbuffered(srcMeta, dstMeta, true);\n      } catch (IOException e) {\n        throw new IOException(\"Failed to copy \" + srcMeta + \" to \" + dstMeta, e);\n      }\n    }\n\n    try {\n      Storage.nativeCopyFileUnbuffered(srcFile, dstFile, true);\n    } catch (IOException e) {\n      throw new IOException(\"Failed to copy \" + srcFile + \" to \" + dstFile, e);\n    }\n    if (LOG.isDebugEnabled()) {\n      if (calculateChecksum) {\n        LOG.debug(\"Copied \" + srcMeta + \" to \" + dstMeta\n            + \" and calculated checksum\");\n      } else {\n        LOG.debug(\"Copied \" + srcFile + \" to \" + dstFile);\n      }\n    }\n    return new File[] {dstMeta, dstFile};\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "463aec11718e47d4aabb86a7a539cb973460aae6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6934. Move checksum computation off the hot path when writing to RAM disk. Contributed by Chris Nauroth.\n",
      "commitDate": "27/10/14 9:38 AM",
      "commitName": "463aec11718e47d4aabb86a7a539cb973460aae6",
      "commitAuthor": "cnauroth",
      "commitDateOld": "24/10/14 1:08 PM",
      "commitNameOld": "a52eb4bc5fb21574859f779001ea9d95bf5207fe",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 2.85,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,20 @@\n   static File[] copyBlockFiles(long blockId, long genStamp,\n                                File srcMeta, File srcFile, File destRoot)\n       throws IOException {\n     final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n     final File dstFile \u003d new File(destDir, srcFile.getName());\n     final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n-    try {\n-      Storage.nativeCopyFileUnbuffered(srcMeta, dstMeta, true);\n-    } catch (IOException e) {\n-      throw new IOException(\"Failed to copy \" + srcMeta + \" to \" + dstMeta, e);\n-    }\n+    computeChecksum(srcMeta, dstMeta, srcFile);\n+\n     try {\n       Storage.nativeCopyFileUnbuffered(srcFile, dstFile, true);\n     } catch (IOException e) {\n       throw new IOException(\"Failed to copy \" + srcFile + \" to \" + dstFile, e);\n     }\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Copied \" + srcMeta + \" to \" + dstMeta);\n+      LOG.debug(\"Copied \" + srcMeta + \" to \" + dstMeta +\n+          \" and calculated checksum\");\n       LOG.debug(\"Copied \" + srcFile + \" to \" + dstFile);\n     }\n     return new File[] {dstMeta, dstFile};\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static File[] copyBlockFiles(long blockId, long genStamp,\n                               File srcMeta, File srcFile, File destRoot)\n      throws IOException {\n    final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n    final File dstFile \u003d new File(destDir, srcFile.getName());\n    final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n    computeChecksum(srcMeta, dstMeta, srcFile);\n\n    try {\n      Storage.nativeCopyFileUnbuffered(srcFile, dstFile, true);\n    } catch (IOException e) {\n      throw new IOException(\"Failed to copy \" + srcFile + \" to \" + dstFile, e);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Copied \" + srcMeta + \" to \" + dstMeta +\n          \" and calculated checksum\");\n      LOG.debug(\"Copied \" + srcFile + \" to \" + dstFile);\n    }\n    return new File[] {dstMeta, dstFile};\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "1770bb942f9ebea38b6811ba0bc3cc249ef3ccbb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7090. Use unbuffered writes when persisting in-memory replicas. Contributed by Xiaoyu Yao.\n",
      "commitDate": "13/10/14 10:56 AM",
      "commitName": "1770bb942f9ebea38b6811ba0bc3cc249ef3ccbb",
      "commitAuthor": "cnauroth",
      "commitDateOld": "07/10/14 8:25 PM",
      "commitNameOld": "1efd9c98258fbb973d2058dcf0850042e53bd02f",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 5.61,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   static File[] copyBlockFiles(long blockId, long genStamp,\n                                File srcMeta, File srcFile, File destRoot)\n       throws IOException {\n     final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n     final File dstFile \u003d new File(destDir, srcFile.getName());\n     final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n     try {\n-      FileUtils.copyFile(srcMeta, dstMeta);\n+      Storage.nativeCopyFileUnbuffered(srcMeta, dstMeta, true);\n     } catch (IOException e) {\n       throw new IOException(\"Failed to copy \" + srcMeta + \" to \" + dstMeta, e);\n     }\n     try {\n-      FileUtils.copyFile(srcFile, dstFile);\n+      Storage.nativeCopyFileUnbuffered(srcFile, dstFile, true);\n     } catch (IOException e) {\n       throw new IOException(\"Failed to copy \" + srcFile + \" to \" + dstFile, e);\n     }\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Copied \" + srcMeta + \" to \" + dstMeta);\n       LOG.debug(\"Copied \" + srcFile + \" to \" + dstFile);\n     }\n     return new File[] {dstMeta, dstFile};\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static File[] copyBlockFiles(long blockId, long genStamp,\n                               File srcMeta, File srcFile, File destRoot)\n      throws IOException {\n    final File destDir \u003d DatanodeUtil.idToBlockDir(destRoot, blockId);\n    final File dstFile \u003d new File(destDir, srcFile.getName());\n    final File dstMeta \u003d FsDatasetUtil.getMetaFile(dstFile, genStamp);\n    try {\n      Storage.nativeCopyFileUnbuffered(srcMeta, dstMeta, true);\n    } catch (IOException e) {\n      throw new IOException(\"Failed to copy \" + srcMeta + \" to \" + dstMeta, e);\n    }\n    try {\n      Storage.nativeCopyFileUnbuffered(srcFile, dstFile, true);\n    } catch (IOException e) {\n      throw new IOException(\"Failed to copy \" + srcFile + \" to \" + dstFile, e);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Copied \" + srcMeta + \" to \" + dstMeta);\n      LOG.debug(\"Copied \" + srcFile + \" to \" + dstFile);\n    }\n    return new File[] {dstMeta, dstFile};\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    }
  }
}