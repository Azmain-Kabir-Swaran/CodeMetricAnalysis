{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSOutputStream.java",
  "functionName": "closeImpl",
  "functionId": "closeImpl",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
  "functionStartLine": 863,
  "functionEndLine": 907,
  "numCommitsSeen": 239,
  "timeTaken": 12742,
  "changeHistory": [
    "d207aba0265e786904ee2ac4e612c5537401c90d",
    "7bebad61d9c3dbff81fdcf243585fd3e9ae59dde",
    "51088d323359587dca7831f74c9d065c2fccc60d",
    "352d299cf8ebe330d24117df98d1e6a64ae38c26",
    "e8bd1ba74b2fc7a6a1b71d068ef01a0fb0bbe294",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "892ade689f9bcce76daae8f66fc00a49bee8548e",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "1c13519e1e7588c3e2974138d37bf3449ca8b3df",
    "efc510a570cf880e7df1b69932aa41932658ee51",
    "98a61766286321468bf801a9f17a843d7eae8d9e",
    "7947e5b53b9ac9524b535b0384c1c355b74723ff",
    "9ed43f2189fb4674b7379e8e995d53d4970d5c3a",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
    "952640fa4cbdc23fe8781e5627c2e8eab565c535",
    "394ba94c5d2801fbc5d95c7872eeeede28eed1eb",
    "36ccf097a95eae0761de7b657752e4808a86c094",
    "463aec11718e47d4aabb86a7a539cb973460aae6",
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc",
    "f2f5cdb5554d294a29ebf465101c5607fd56e244",
    "735046ebecd9e803398be56fbf79dbde5226b4c1",
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
    "0a6806ce8c946b26eceac7d16b467c54c453df84",
    "83cf475050dba27e72b4e399491638c670621175",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "d207aba0265e786904ee2ac4e612c5537401c90d": "Ybodychange",
    "7bebad61d9c3dbff81fdcf243585fd3e9ae59dde": "Ybodychange",
    "51088d323359587dca7831f74c9d065c2fccc60d": "Ybodychange",
    "352d299cf8ebe330d24117df98d1e6a64ae38c26": "Ybodychange",
    "e8bd1ba74b2fc7a6a1b71d068ef01a0fb0bbe294": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "892ade689f9bcce76daae8f66fc00a49bee8548e": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "1c13519e1e7588c3e2974138d37bf3449ca8b3df": "Ybodychange",
    "efc510a570cf880e7df1b69932aa41932658ee51": "Ybodychange",
    "98a61766286321468bf801a9f17a843d7eae8d9e": "Ybodychange",
    "7947e5b53b9ac9524b535b0384c1c355b74723ff": "Ybodychange",
    "9ed43f2189fb4674b7379e8e995d53d4970d5c3a": "Ymodifierchange",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": "Ybodychange",
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e": "Ymultichange(Yrename,Ymodifierchange,Ybodychange)",
    "952640fa4cbdc23fe8781e5627c2e8eab565c535": "Ybodychange",
    "394ba94c5d2801fbc5d95c7872eeeede28eed1eb": "Ybodychange",
    "36ccf097a95eae0761de7b657752e4808a86c094": "Ybodychange",
    "463aec11718e47d4aabb86a7a539cb973460aae6": "Ybodychange",
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc": "Ybodychange",
    "f2f5cdb5554d294a29ebf465101c5607fd56e244": "Ybodychange",
    "735046ebecd9e803398be56fbf79dbde5226b4c1": "Ybodychange",
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": "Ybodychange",
    "0a6806ce8c946b26eceac7d16b467c54c453df84": "Ybodychange",
    "83cf475050dba27e72b4e399491638c670621175": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "d207aba0265e786904ee2ac4e612c5537401c90d": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-14706. Checksums are not checked if block meta file is less than 7 bytes. Contributed by Stephen O\u0027Donnell.\"\n\nThis reverts commit 7bebad61d9c3dbff81fdcf243585fd3e9ae59dde.\n",
      "commitDate": "02/09/19 9:43 AM",
      "commitName": "d207aba0265e786904ee2ac4e612c5537401c90d",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "29/08/19 5:38 PM",
      "commitNameOld": "7bebad61d9c3dbff81fdcf243585fd3e9ae59dde",
      "commitAuthorOld": "Stephen O\u0027Donnell",
      "daysBetweenCommits": 3.67,
      "commitsBetweenForRepo": 28,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,45 @@\n   protected synchronized void closeImpl() throws IOException {\n-    boolean recoverOnCloseException \u003d dfsClient.getConfiguration().getBoolean(\n-        HdfsClientConfigKeys.Write.RECOVER_ON_CLOSE_EXCEPTION_KEY,\n-        HdfsClientConfigKeys.Write.RECOVER_ON_CLOSE_EXCEPTION_DEFAULT);\n     if (isClosed()) {\n-      if (recoverOnCloseException \u0026\u0026 !leaseRecovered) {\n-        try {\n-          dfsClient.endFileLease(fileId);\n-          dfsClient.recoverLease(src);\n-          leaseRecovered \u003d true;\n-        } catch (Exception e) {\n-          LOG.warn(\"Fail to recover lease for {}\", src, e);\n-        }\n-      }\n-\n       LOG.debug(\"Closing an already closed stream. [Stream:{}, streamer:{}]\",\n           closed, getStreamer().streamerClosed());\n       try {\n         getStreamer().getLastException().check(true);\n       } catch (IOException ioe) {\n         cleanupAndRethrowIOException(ioe);\n       } finally {\n         if (!closed) {\n           // If stream is not closed but streamer closed, clean up the stream.\n           // Most importantly, end the file lease.\n           closeThreads(true);\n         }\n       }\n       return;\n     }\n \n-    EmulateExceptionInClose eei \u003d new EmulateExceptionInClose(5);\n     try {\n-      flushBuffer(); // flush from all upper layers\n-      // for test\n-      eei.kickRandomException();\n+      flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) {\n         enqueueCurrentPacket();\n       }\n \n       if (getStreamer().getBytesCurBlock() !\u003d 0) {\n         setCurrentPacketToEmpty();\n       }\n \n       try {\n-        flushInternal(); // flush all data to Datanodes\n+        flushInternal();             // flush all data to Datanodes\n       } catch (IOException ioe) {\n         cleanupAndRethrowIOException(ioe);\n       }\n-      // for test\n-      eei.kickRandomException();\n       completeFile();\n-      // for test\n-      eei.kickException();\n     } catch (ClosedChannelException ignored) {\n-    } catch (IOException ioe) {\n-      if (recoverOnCloseException) {\n-        try {\n-          dfsClient.endFileLease(fileId);\n-          dfsClient.recoverLease(src);\n-          leaseRecovered \u003d true;\n-        } catch (Exception e) {\n-          // Ignore exception rendered by recoverLease. Throw original\n-          // exception\n-        }\n-      }\n-      throw ioe;\n     } finally {\n       // Failures may happen when flushing data.\n       // Streamers may keep waiting for the new block information.\n       // Thus need to force closing these threads.\n       // Don\u0027t need to call setClosed() because closeThreads(true)\n       // calls setClosed() in the finally block.\n       closeThreads(true);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      LOG.debug(\"Closing an already closed stream. [Stream:{}, streamer:{}]\",\n          closed, getStreamer().streamerClosed());\n      try {\n        getStreamer().getLastException().check(true);\n      } catch (IOException ioe) {\n        cleanupAndRethrowIOException(ioe);\n      } finally {\n        if (!closed) {\n          // If stream is not closed but streamer closed, clean up the stream.\n          // Most importantly, end the file lease.\n          closeThreads(true);\n        }\n      }\n      return;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) {\n        enqueueCurrentPacket();\n      }\n\n      if (getStreamer().getBytesCurBlock() !\u003d 0) {\n        setCurrentPacketToEmpty();\n      }\n\n      try {\n        flushInternal();             // flush all data to Datanodes\n      } catch (IOException ioe) {\n        cleanupAndRethrowIOException(ioe);\n      }\n      completeFile();\n    } catch (ClosedChannelException ignored) {\n    } finally {\n      // Failures may happen when flushing data.\n      // Streamers may keep waiting for the new block information.\n      // Thus need to force closing these threads.\n      // Don\u0027t need to call setClosed() because closeThreads(true)\n      // calls setClosed() in the finally block.\n      closeThreads(true);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "7bebad61d9c3dbff81fdcf243585fd3e9ae59dde": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14706. Checksums are not checked if block meta file is less than 7 bytes. Contributed by Stephen O\u0027Donnell.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "29/08/19 5:38 PM",
      "commitName": "7bebad61d9c3dbff81fdcf243585fd3e9ae59dde",
      "commitAuthor": "Stephen O\u0027Donnell",
      "commitDateOld": "18/06/19 10:21 AM",
      "commitNameOld": "3c1a1ceea9e35ac53376276139416b728ed57f10",
      "commitAuthorOld": "Shweta Yakkali",
      "daysBetweenCommits": 72.3,
      "commitsBetweenForRepo": 657,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,77 @@\n   protected synchronized void closeImpl() throws IOException {\n+    boolean recoverOnCloseException \u003d dfsClient.getConfiguration().getBoolean(\n+        HdfsClientConfigKeys.Write.RECOVER_ON_CLOSE_EXCEPTION_KEY,\n+        HdfsClientConfigKeys.Write.RECOVER_ON_CLOSE_EXCEPTION_DEFAULT);\n     if (isClosed()) {\n+      if (recoverOnCloseException \u0026\u0026 !leaseRecovered) {\n+        try {\n+          dfsClient.endFileLease(fileId);\n+          dfsClient.recoverLease(src);\n+          leaseRecovered \u003d true;\n+        } catch (Exception e) {\n+          LOG.warn(\"Fail to recover lease for {}\", src, e);\n+        }\n+      }\n+\n       LOG.debug(\"Closing an already closed stream. [Stream:{}, streamer:{}]\",\n           closed, getStreamer().streamerClosed());\n       try {\n         getStreamer().getLastException().check(true);\n       } catch (IOException ioe) {\n         cleanupAndRethrowIOException(ioe);\n       } finally {\n         if (!closed) {\n           // If stream is not closed but streamer closed, clean up the stream.\n           // Most importantly, end the file lease.\n           closeThreads(true);\n         }\n       }\n       return;\n     }\n \n+    EmulateExceptionInClose eei \u003d new EmulateExceptionInClose(5);\n     try {\n-      flushBuffer();       // flush from all upper layers\n+      flushBuffer(); // flush from all upper layers\n+      // for test\n+      eei.kickRandomException();\n \n       if (currentPacket !\u003d null) {\n         enqueueCurrentPacket();\n       }\n \n       if (getStreamer().getBytesCurBlock() !\u003d 0) {\n         setCurrentPacketToEmpty();\n       }\n \n       try {\n-        flushInternal();             // flush all data to Datanodes\n+        flushInternal(); // flush all data to Datanodes\n       } catch (IOException ioe) {\n         cleanupAndRethrowIOException(ioe);\n       }\n+      // for test\n+      eei.kickRandomException();\n       completeFile();\n+      // for test\n+      eei.kickException();\n     } catch (ClosedChannelException ignored) {\n+    } catch (IOException ioe) {\n+      if (recoverOnCloseException) {\n+        try {\n+          dfsClient.endFileLease(fileId);\n+          dfsClient.recoverLease(src);\n+          leaseRecovered \u003d true;\n+        } catch (Exception e) {\n+          // Ignore exception rendered by recoverLease. Throw original\n+          // exception\n+        }\n+      }\n+      throw ioe;\n     } finally {\n       // Failures may happen when flushing data.\n       // Streamers may keep waiting for the new block information.\n       // Thus need to force closing these threads.\n       // Don\u0027t need to call setClosed() because closeThreads(true)\n       // calls setClosed() in the finally block.\n       closeThreads(true);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void closeImpl() throws IOException {\n    boolean recoverOnCloseException \u003d dfsClient.getConfiguration().getBoolean(\n        HdfsClientConfigKeys.Write.RECOVER_ON_CLOSE_EXCEPTION_KEY,\n        HdfsClientConfigKeys.Write.RECOVER_ON_CLOSE_EXCEPTION_DEFAULT);\n    if (isClosed()) {\n      if (recoverOnCloseException \u0026\u0026 !leaseRecovered) {\n        try {\n          dfsClient.endFileLease(fileId);\n          dfsClient.recoverLease(src);\n          leaseRecovered \u003d true;\n        } catch (Exception e) {\n          LOG.warn(\"Fail to recover lease for {}\", src, e);\n        }\n      }\n\n      LOG.debug(\"Closing an already closed stream. [Stream:{}, streamer:{}]\",\n          closed, getStreamer().streamerClosed());\n      try {\n        getStreamer().getLastException().check(true);\n      } catch (IOException ioe) {\n        cleanupAndRethrowIOException(ioe);\n      } finally {\n        if (!closed) {\n          // If stream is not closed but streamer closed, clean up the stream.\n          // Most importantly, end the file lease.\n          closeThreads(true);\n        }\n      }\n      return;\n    }\n\n    EmulateExceptionInClose eei \u003d new EmulateExceptionInClose(5);\n    try {\n      flushBuffer(); // flush from all upper layers\n      // for test\n      eei.kickRandomException();\n\n      if (currentPacket !\u003d null) {\n        enqueueCurrentPacket();\n      }\n\n      if (getStreamer().getBytesCurBlock() !\u003d 0) {\n        setCurrentPacketToEmpty();\n      }\n\n      try {\n        flushInternal(); // flush all data to Datanodes\n      } catch (IOException ioe) {\n        cleanupAndRethrowIOException(ioe);\n      }\n      // for test\n      eei.kickRandomException();\n      completeFile();\n      // for test\n      eei.kickException();\n    } catch (ClosedChannelException ignored) {\n    } catch (IOException ioe) {\n      if (recoverOnCloseException) {\n        try {\n          dfsClient.endFileLease(fileId);\n          dfsClient.recoverLease(src);\n          leaseRecovered \u003d true;\n        } catch (Exception e) {\n          // Ignore exception rendered by recoverLease. Throw original\n          // exception\n        }\n      }\n      throw ioe;\n    } finally {\n      // Failures may happen when flushing data.\n      // Streamers may keep waiting for the new block information.\n      // Thus need to force closing these threads.\n      // Don\u0027t need to call setClosed() because closeThreads(true)\n      // calls setClosed() in the finally block.\n      closeThreads(true);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "51088d323359587dca7831f74c9d065c2fccc60d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13164. File not closed if streamer fail with DSQuotaExceededException.\n",
      "commitDate": "23/02/18 1:49 PM",
      "commitName": "51088d323359587dca7831f74c9d065c2fccc60d",
      "commitAuthor": "Xiao Chen",
      "commitDateOld": "09/11/17 10:16 AM",
      "commitNameOld": "bf6a660232b01642b07697a289c773ea5b97217c",
      "commitAuthorOld": "John Zhuge",
      "daysBetweenCommits": 106.15,
      "commitsBetweenForRepo": 604,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,45 @@\n   protected synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n-      getStreamer().getLastException().check(true);\n+      LOG.debug(\"Closing an already closed stream. [Stream:{}, streamer:{}]\",\n+          closed, getStreamer().streamerClosed());\n+      try {\n+        getStreamer().getLastException().check(true);\n+      } catch (IOException ioe) {\n+        cleanupAndRethrowIOException(ioe);\n+      } finally {\n+        if (!closed) {\n+          // If stream is not closed but streamer closed, clean up the stream.\n+          // Most importantly, end the file lease.\n+          closeThreads(true);\n+        }\n+      }\n       return;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) {\n         enqueueCurrentPacket();\n       }\n \n       if (getStreamer().getBytesCurBlock() !\u003d 0) {\n         setCurrentPacketToEmpty();\n       }\n \n-      flushInternal();             // flush all data to Datanodes\n-      // get last block before destroying the streamer\n-      ExtendedBlock lastBlock \u003d getStreamer().getBlock();\n-\n-      try (TraceScope ignored \u003d\n-               dfsClient.getTracer().newScope(\"completeFile\")) {\n-        completeFile(lastBlock);\n+      try {\n+        flushInternal();             // flush all data to Datanodes\n+      } catch (IOException ioe) {\n+        cleanupAndRethrowIOException(ioe);\n       }\n+      completeFile();\n     } catch (ClosedChannelException ignored) {\n     } finally {\n       // Failures may happen when flushing data.\n       // Streamers may keep waiting for the new block information.\n       // Thus need to force closing these threads.\n       // Don\u0027t need to call setClosed() because closeThreads(true)\n       // calls setClosed() in the finally block.\n       closeThreads(true);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      LOG.debug(\"Closing an already closed stream. [Stream:{}, streamer:{}]\",\n          closed, getStreamer().streamerClosed());\n      try {\n        getStreamer().getLastException().check(true);\n      } catch (IOException ioe) {\n        cleanupAndRethrowIOException(ioe);\n      } finally {\n        if (!closed) {\n          // If stream is not closed but streamer closed, clean up the stream.\n          // Most importantly, end the file lease.\n          closeThreads(true);\n        }\n      }\n      return;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) {\n        enqueueCurrentPacket();\n      }\n\n      if (getStreamer().getBytesCurBlock() !\u003d 0) {\n        setCurrentPacketToEmpty();\n      }\n\n      try {\n        flushInternal();             // flush all data to Datanodes\n      } catch (IOException ioe) {\n        cleanupAndRethrowIOException(ioe);\n      }\n      completeFile();\n    } catch (ClosedChannelException ignored) {\n    } finally {\n      // Failures may happen when flushing data.\n      // Streamers may keep waiting for the new block information.\n      // Thus need to force closing these threads.\n      // Don\u0027t need to call setClosed() because closeThreads(true)\n      // calls setClosed() in the finally block.\n      closeThreads(true);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "352d299cf8ebe330d24117df98d1e6a64ae38c26": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9812. Streamer threads leak if failure happens when closing DFSOutputStream. Contributed by Lin Yiqun.\n",
      "commitDate": "07/03/16 5:43 PM",
      "commitName": "352d299cf8ebe330d24117df98d1e6a64ae38c26",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "01/02/16 1:02 PM",
      "commitNameOld": "e30ce01ddce1cfd1e9d49c4784eb4a6bc87e36ca",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 35.19,
      "commitsBetweenForRepo": 248,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,35 @@\n   protected synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n       getStreamer().getLastException().check(true);\n       return;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) {\n         enqueueCurrentPacket();\n       }\n \n       if (getStreamer().getBytesCurBlock() !\u003d 0) {\n         setCurrentPacketToEmpty();\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d getStreamer().getBlock();\n-      closeThreads(false);\n+\n       try (TraceScope ignored \u003d\n                dfsClient.getTracer().newScope(\"completeFile\")) {\n         completeFile(lastBlock);\n       }\n     } catch (ClosedChannelException ignored) {\n     } finally {\n-      setClosed();\n+      // Failures may happen when flushing data.\n+      // Streamers may keep waiting for the new block information.\n+      // Thus need to force closing these threads.\n+      // Don\u0027t need to call setClosed() because closeThreads(true)\n+      // calls setClosed() in the finally block.\n+      closeThreads(true);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      getStreamer().getLastException().check(true);\n      return;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) {\n        enqueueCurrentPacket();\n      }\n\n      if (getStreamer().getBytesCurBlock() !\u003d 0) {\n        setCurrentPacketToEmpty();\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d getStreamer().getBlock();\n\n      try (TraceScope ignored \u003d\n               dfsClient.getTracer().newScope(\"completeFile\")) {\n        completeFile(lastBlock);\n      }\n    } catch (ClosedChannelException ignored) {\n    } finally {\n      // Failures may happen when flushing data.\n      // Streamers may keep waiting for the new block information.\n      // Thus need to force closing these threads.\n      // Don\u0027t need to call setClosed() because closeThreads(true)\n      // calls setClosed() in the finally block.\n      closeThreads(true);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "e8bd1ba74b2fc7a6a1b71d068ef01a0fb0bbe294": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9294. DFSClient deadlock when close file and failed to renew lease.  Contributed by Brahma Reddy Battula\n",
      "commitDate": "02/12/15 5:39 PM",
      "commitName": "e8bd1ba74b2fc7a6a1b71d068ef01a0fb0bbe294",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "06/10/15 10:22 PM",
      "commitNameOld": "8b7339312cb06b7e021f8f9ea6e3a20ebf009af3",
      "commitAuthorOld": "Uma Mahesh",
      "daysBetweenCommits": 56.85,
      "commitsBetweenForRepo": 444,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,30 @@\n   protected synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n       getStreamer().getLastException().check(true);\n       return;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) {\n         enqueueCurrentPacket();\n       }\n \n       if (getStreamer().getBytesCurBlock() !\u003d 0) {\n         setCurrentPacketToEmpty();\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d getStreamer().getBlock();\n       closeThreads(false);\n       try (TraceScope ignored \u003d\n                dfsClient.getTracer().newScope(\"completeFile\")) {\n         completeFile(lastBlock);\n       }\n-      dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException ignored) {\n     } finally {\n       setClosed();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      getStreamer().getLastException().check(true);\n      return;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) {\n        enqueueCurrentPacket();\n      }\n\n      if (getStreamer().getBytesCurBlock() !\u003d 0) {\n        setCurrentPacketToEmpty();\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d getStreamer().getBlock();\n      closeThreads(false);\n      try (TraceScope ignored \u003d\n               dfsClient.getTracer().newScope(\"completeFile\")) {\n        completeFile(lastBlock);\n      }\n    } catch (ClosedChannelException ignored) {\n    } finally {\n      setClosed();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,31 @@\n   protected synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n       getStreamer().getLastException().check(true);\n       return;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) {\n         enqueueCurrentPacket();\n       }\n \n       if (getStreamer().getBytesCurBlock() !\u003d 0) {\n         setCurrentPacketToEmpty();\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d getStreamer().getBlock();\n       closeThreads(false);\n-      TraceScope scope \u003d dfsClient.getTracer().newScope(\"completeFile\");\n-      try {\n+      try (TraceScope ignored \u003d\n+               dfsClient.getTracer().newScope(\"completeFile\")) {\n         completeFile(lastBlock);\n-      } finally {\n-        scope.close();\n       }\n       dfsClient.endFileLease(fileId);\n-    } catch (ClosedChannelException e) {\n+    } catch (ClosedChannelException ignored) {\n     } finally {\n       setClosed();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      getStreamer().getLastException().check(true);\n      return;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) {\n        enqueueCurrentPacket();\n      }\n\n      if (getStreamer().getBytesCurBlock() !\u003d 0) {\n        setCurrentPacketToEmpty();\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d getStreamer().getBlock();\n      closeThreads(false);\n      try (TraceScope ignored \u003d\n               dfsClient.getTracer().newScope(\"completeFile\")) {\n        completeFile(lastBlock);\n      }\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException ignored) {\n    } finally {\n      setClosed();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "892ade689f9bcce76daae8f66fc00a49bee8548e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
      "commitDate": "28/09/15 7:42 AM",
      "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "26/09/15 11:08 AM",
      "commitNameOld": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.86,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   protected synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n       getStreamer().getLastException().check(true);\n       return;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) {\n         enqueueCurrentPacket();\n       }\n \n       if (getStreamer().getBytesCurBlock() !\u003d 0) {\n         setCurrentPacketToEmpty();\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d getStreamer().getBlock();\n       closeThreads(false);\n-      TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n+      TraceScope scope \u003d dfsClient.getTracer().newScope(\"completeFile\");\n       try {\n         completeFile(lastBlock);\n       } finally {\n         scope.close();\n       }\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       setClosed();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      getStreamer().getLastException().check(true);\n      return;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) {\n        enqueueCurrentPacket();\n      }\n\n      if (getStreamer().getBytesCurBlock() !\u003d 0) {\n        setCurrentPacketToEmpty();\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d getStreamer().getBlock();\n      closeThreads(false);\n      TraceScope scope \u003d dfsClient.getTracer().newScope(\"completeFile\");\n      try {\n        completeFile(lastBlock);\n      } finally {\n        scope.close();\n      }\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      getStreamer().getLastException().check(true);\n      return;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) {\n        enqueueCurrentPacket();\n      }\n\n      if (getStreamer().getBytesCurBlock() !\u003d 0) {\n        setCurrentPacketToEmpty();\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d getStreamer().getBlock();\n      closeThreads(false);\n      TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n      try {\n        completeFile(lastBlock);\n      } finally {\n        scope.close();\n      }\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "1c13519e1e7588c3e2974138d37bf3449ca8b3df": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8605. Merge Refactor of DFSOutputStream from HDFS-7285 branch. (vinayakumarb)\n",
      "commitDate": "18/06/15 8:48 AM",
      "commitName": "1c13519e1e7588c3e2974138d37bf3449ca8b3df",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "16/06/15 6:08 PM",
      "commitNameOld": "d4929f448f95815af99100780a08b172e0262c17",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 1.61,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,33 @@\n   protected synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n       getStreamer().getLastException().check(true);\n       return;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) {\n-        getStreamer().waitAndQueuePacket(currentPacket);\n-        currentPacket \u003d null;\n+        enqueueCurrentPacket();\n       }\n \n       if (getStreamer().getBytesCurBlock() !\u003d 0) {\n-        // send an empty packet to mark the end of the block\n-        currentPacket \u003d createPacket(0, 0, getStreamer().getBytesCurBlock(),\n-            getStreamer().getAndIncCurrentSeqno(), true);\n-        currentPacket.setSyncBlock(shouldSyncBlock);\n+        setCurrentPacketToEmpty();\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d getStreamer().getBlock();\n       closeThreads(false);\n       TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n       try {\n         completeFile(lastBlock);\n       } finally {\n         scope.close();\n       }\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       setClosed();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      getStreamer().getLastException().check(true);\n      return;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) {\n        enqueueCurrentPacket();\n      }\n\n      if (getStreamer().getBytesCurBlock() !\u003d 0) {\n        setCurrentPacketToEmpty();\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d getStreamer().getBlock();\n      closeThreads(false);\n      TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n      try {\n        completeFile(lastBlock);\n      } finally {\n        scope.close();\n      }\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "efc510a570cf880e7df1b69932aa41932658ee51": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8386. Improve synchronization of \u0027streamer\u0027 reference in DFSOutputStream. Contributed by Rakesh R.\n",
      "commitDate": "02/06/15 3:39 PM",
      "commitName": "efc510a570cf880e7df1b69932aa41932658ee51",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "30/04/15 7:27 PM",
      "commitNameOld": "98a61766286321468bf801a9f17a843d7eae8d9e",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 32.84,
      "commitsBetweenForRepo": 337,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,37 @@\n   protected synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n-      streamer.getLastException().check(true);\n+      getStreamer().getLastException().check(true);\n       return;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) {\n-        streamer.waitAndQueuePacket(currentPacket);\n+        getStreamer().waitAndQueuePacket(currentPacket);\n         currentPacket \u003d null;\n       }\n \n-      if (streamer.getBytesCurBlock() !\u003d 0) {\n+      if (getStreamer().getBytesCurBlock() !\u003d 0) {\n         // send an empty packet to mark the end of the block\n-        currentPacket \u003d createPacket(0, 0, streamer.getBytesCurBlock(),\n-            streamer.getAndIncCurrentSeqno(), true);\n+        currentPacket \u003d createPacket(0, 0, getStreamer().getBytesCurBlock(),\n+            getStreamer().getAndIncCurrentSeqno(), true);\n         currentPacket.setSyncBlock(shouldSyncBlock);\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n-      ExtendedBlock lastBlock \u003d streamer.getBlock();\n+      ExtendedBlock lastBlock \u003d getStreamer().getBlock();\n       closeThreads(false);\n       TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n       try {\n         completeFile(lastBlock);\n       } finally {\n         scope.close();\n       }\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       setClosed();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      getStreamer().getLastException().check(true);\n      return;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) {\n        getStreamer().waitAndQueuePacket(currentPacket);\n        currentPacket \u003d null;\n      }\n\n      if (getStreamer().getBytesCurBlock() !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d createPacket(0, 0, getStreamer().getBytesCurBlock(),\n            getStreamer().getAndIncCurrentSeqno(), true);\n        currentPacket.setSyncBlock(shouldSyncBlock);\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d getStreamer().getBlock();\n      closeThreads(false);\n      TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n      try {\n        completeFile(lastBlock);\n      } finally {\n        scope.close();\n      }\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "98a61766286321468bf801a9f17a843d7eae8d9e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8300. Fix unit test failures and findbugs warning caused by HDFS-8283. Contributed by Jing Zhao.\n",
      "commitDate": "30/04/15 7:27 PM",
      "commitName": "98a61766286321468bf801a9f17a843d7eae8d9e",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "29/04/15 10:41 AM",
      "commitNameOld": "7947e5b53b9ac9524b535b0384c1c355b74723ff",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.36,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,37 @@\n   protected synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n-      streamer.getLastException().check();\n+      streamer.getLastException().check(true);\n       return;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) {\n         streamer.waitAndQueuePacket(currentPacket);\n         currentPacket \u003d null;\n       }\n \n       if (streamer.getBytesCurBlock() !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d createPacket(0, 0, streamer.getBytesCurBlock(),\n             streamer.getAndIncCurrentSeqno(), true);\n         currentPacket.setSyncBlock(shouldSyncBlock);\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n       try {\n         completeFile(lastBlock);\n       } finally {\n         scope.close();\n       }\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       setClosed();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      streamer.getLastException().check(true);\n      return;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) {\n        streamer.waitAndQueuePacket(currentPacket);\n        currentPacket \u003d null;\n      }\n\n      if (streamer.getBytesCurBlock() !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d createPacket(0, 0, streamer.getBytesCurBlock(),\n            streamer.getAndIncCurrentSeqno(), true);\n        currentPacket.setSyncBlock(shouldSyncBlock);\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n      try {\n        completeFile(lastBlock);\n      } finally {\n        scope.close();\n      }\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "7947e5b53b9ac9524b535b0384c1c355b74723ff": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8283. DataStreamer cleanup and some minor improvement. Contributed by Tsz Wo Nicholas Sze.\n",
      "commitDate": "29/04/15 10:41 AM",
      "commitName": "7947e5b53b9ac9524b535b0384c1c355b74723ff",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/04/15 12:21 AM",
      "commitNameOld": "c8d72907ff5a4cb9ce1effca8ad9b69689d11d1d",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 5.43,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,37 @@\n   protected synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n-      IOException e \u003d streamer.getLastException().getAndSet(null);\n-      if (e \u003d\u003d null)\n-        return;\n-      else\n-        throw e;\n+      streamer.getLastException().check();\n+      return;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) {\n         streamer.waitAndQueuePacket(currentPacket);\n         currentPacket \u003d null;\n       }\n \n       if (streamer.getBytesCurBlock() !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d createPacket(0, 0, streamer.getBytesCurBlock(),\n             streamer.getAndIncCurrentSeqno(), true);\n         currentPacket.setSyncBlock(shouldSyncBlock);\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n       try {\n         completeFile(lastBlock);\n       } finally {\n         scope.close();\n       }\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       setClosed();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      streamer.getLastException().check();\n      return;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) {\n        streamer.waitAndQueuePacket(currentPacket);\n        currentPacket \u003d null;\n      }\n\n      if (streamer.getBytesCurBlock() !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d createPacket(0, 0, streamer.getBytesCurBlock(),\n            streamer.getAndIncCurrentSeqno(), true);\n        currentPacket.setSyncBlock(shouldSyncBlock);\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n      try {\n        completeFile(lastBlock);\n      } finally {\n        scope.close();\n      }\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "9ed43f2189fb4674b7379e8e995d53d4970d5c3a": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-7888. Change DFSOutputStream and DataStreamer for convenience of subclassing. Contributed by Li Bo\n",
      "commitDate": "02/04/15 10:59 AM",
      "commitName": "9ed43f2189fb4674b7379e8e995d53d4970d5c3a",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "01/04/15 2:10 PM",
      "commitNameOld": "c94d594a57806dec515e2a2053a1221f8ce48cc4",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.87,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n-  private synchronized void closeImpl() throws IOException {\n+  protected synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n       IOException e \u003d streamer.getLastException().getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n-      if (currentPacket !\u003d null) { \n+      if (currentPacket !\u003d null) {\n         streamer.waitAndQueuePacket(currentPacket);\n         currentPacket \u003d null;\n       }\n \n       if (streamer.getBytesCurBlock() !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d createPacket(0, 0, streamer.getBytesCurBlock(),\n             streamer.getAndIncCurrentSeqno(), true);\n         currentPacket.setSyncBlock(shouldSyncBlock);\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n       try {\n         completeFile(lastBlock);\n       } finally {\n         scope.close();\n       }\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       setClosed();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      IOException e \u003d streamer.getLastException().getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) {\n        streamer.waitAndQueuePacket(currentPacket);\n        currentPacket \u003d null;\n      }\n\n      if (streamer.getBytesCurBlock() !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d createPacket(0, 0, streamer.getBytesCurBlock(),\n            streamer.getAndIncCurrentSeqno(), true);\n        currentPacket.setSyncBlock(shouldSyncBlock);\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n      try {\n        completeFile(lastBlock);\n      } finally {\n        scope.close();\n      }\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldValue": "[private, synchronized]",
        "newValue": "[protected, synchronized]"
      }
    },
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
      "commitDate": "24/03/15 11:06 AM",
      "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "20/03/15 12:02 PM",
      "commitNameOld": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 3.96,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,40 @@\n   private synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n-      IOException e \u003d lastException.getAndSet(null);\n+      IOException e \u003d streamer.getLastException().getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n-        waitAndQueueCurrentPacket();\n+        streamer.waitAndQueuePacket(currentPacket);\n+        currentPacket \u003d null;\n       }\n \n-      if (bytesCurBlock !\u003d 0) {\n+      if (streamer.getBytesCurBlock() !\u003d 0) {\n         // send an empty packet to mark the end of the block\n-        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n+        currentPacket \u003d createPacket(0, 0, streamer.getBytesCurBlock(),\n+            streamer.getAndIncCurrentSeqno(), true);\n         currentPacket.setSyncBlock(shouldSyncBlock);\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n       try {\n         completeFile(lastBlock);\n       } finally {\n         scope.close();\n       }\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       setClosed();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      IOException e \u003d streamer.getLastException().getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        streamer.waitAndQueuePacket(currentPacket);\n        currentPacket \u003d null;\n      }\n\n      if (streamer.getBytesCurBlock() !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d createPacket(0, 0, streamer.getBytesCurBlock(),\n            streamer.getAndIncCurrentSeqno(), true);\n        currentPacket.setSyncBlock(shouldSyncBlock);\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n      try {\n        completeFile(lastBlock);\n      } finally {\n        scope.close();\n      }\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "8234fd0e1087e0e49aa1d6f286f292b7f70b368e": {
      "type": "Ymultichange(Yrename,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-7054. Make DFSOutputStream tracing more fine-grained (cmccabe)\n",
      "commitDate": "18/03/15 6:14 PM",
      "commitName": "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
      "commitAuthor": "Colin Patrick Mccabe",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-7054. Make DFSOutputStream tracing more fine-grained (cmccabe)\n",
          "commitDate": "18/03/15 6:14 PM",
          "commitName": "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "16/03/15 9:58 PM",
          "commitNameOld": "046521cd6511b7fc6d9478cb2bed90d8e75fca20",
          "commitAuthorOld": "Harsh J",
          "daysBetweenCommits": 1.84,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,38 @@\n-  public synchronized void close() throws IOException {\n+  private synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n       IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n         currentPacket.setSyncBlock(shouldSyncBlock);\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n-      completeFile(lastBlock);\n+      TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n+      try {\n+        completeFile(lastBlock);\n+      } finally {\n+        scope.close();\n+      }\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       setClosed();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n        currentPacket.setSyncBlock(shouldSyncBlock);\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n      try {\n        completeFile(lastBlock);\n      } finally {\n        scope.close();\n      }\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {
            "oldValue": "close",
            "newValue": "closeImpl"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7054. Make DFSOutputStream tracing more fine-grained (cmccabe)\n",
          "commitDate": "18/03/15 6:14 PM",
          "commitName": "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "16/03/15 9:58 PM",
          "commitNameOld": "046521cd6511b7fc6d9478cb2bed90d8e75fca20",
          "commitAuthorOld": "Harsh J",
          "daysBetweenCommits": 1.84,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,38 @@\n-  public synchronized void close() throws IOException {\n+  private synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n       IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n         currentPacket.setSyncBlock(shouldSyncBlock);\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n-      completeFile(lastBlock);\n+      TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n+      try {\n+        completeFile(lastBlock);\n+      } finally {\n+        scope.close();\n+      }\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       setClosed();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n        currentPacket.setSyncBlock(shouldSyncBlock);\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n      try {\n        completeFile(lastBlock);\n      } finally {\n        scope.close();\n      }\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[private, synchronized]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7054. Make DFSOutputStream tracing more fine-grained (cmccabe)\n",
          "commitDate": "18/03/15 6:14 PM",
          "commitName": "8234fd0e1087e0e49aa1d6f286f292b7f70b368e",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "16/03/15 9:58 PM",
          "commitNameOld": "046521cd6511b7fc6d9478cb2bed90d8e75fca20",
          "commitAuthorOld": "Harsh J",
          "daysBetweenCommits": 1.84,
          "commitsBetweenForRepo": 25,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,38 @@\n-  public synchronized void close() throws IOException {\n+  private synchronized void closeImpl() throws IOException {\n     if (isClosed()) {\n       IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n         currentPacket.setSyncBlock(shouldSyncBlock);\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n-      completeFile(lastBlock);\n+      TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n+      try {\n+        completeFile(lastBlock);\n+      } finally {\n+        scope.close();\n+      }\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       setClosed();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n        currentPacket.setSyncBlock(shouldSyncBlock);\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      TraceScope scope \u003d Trace.startSpan(\"completeFile\", Sampler.NEVER);\n      try {\n        completeFile(lastBlock);\n      } finally {\n        scope.close();\n      }\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "952640fa4cbdc23fe8781e5627c2e8eab565c535": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7855. Separate class Packet from DFSOutputStream. Contributed by Li Bo.\n",
      "commitDate": "05/03/15 10:58 AM",
      "commitName": "952640fa4cbdc23fe8781e5627c2e8eab565c535",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "01/03/15 11:03 PM",
      "commitNameOld": "67ed59348d638d56e6752ba2c71fdcd69567546d",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 3.5,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,33 @@\n   public synchronized void close() throws IOException {\n     if (isClosed()) {\n       IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n-        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++);\n-        currentPacket.lastPacketInBlock \u003d true;\n-        currentPacket.syncBlock \u003d shouldSyncBlock;\n+        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n+        currentPacket.setSyncBlock(shouldSyncBlock);\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       setClosed();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (isClosed()) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++, true);\n        currentPacket.setSyncBlock(shouldSyncBlock);\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "394ba94c5d2801fbc5d95c7872eeeede28eed1eb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7358. Clients may get stuck waiting when using ByteArrayManager.\n",
      "commitDate": "13/11/14 12:28 PM",
      "commitName": "394ba94c5d2801fbc5d95c7872eeeede28eed1eb",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "05/11/14 10:51 AM",
      "commitNameOld": "56257fab1d5a7f66bebd9149c7df0436c0a57adb",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 8.07,
      "commitsBetweenForRepo": 93,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   public synchronized void close() throws IOException {\n-    if (closed) {\n+    if (isClosed()) {\n       IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++);\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n-      closed \u003d true;\n+      setClosed();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (isClosed()) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++);\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      setClosed();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "36ccf097a95eae0761de7b657752e4808a86c094": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7276. Limit the number of byte arrays used by DFSOutputStream and provide a mechanism for recycling arrays.\n",
      "commitDate": "01/11/14 11:22 AM",
      "commitName": "36ccf097a95eae0761de7b657752e4808a86c094",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "27/10/14 9:38 AM",
      "commitNameOld": "463aec11718e47d4aabb86a7a539cb973460aae6",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 5.07,
      "commitsBetweenForRepo": 75,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n-        currentPacket \u003d new Packet(0, 0, bytesCurBlock, currentSeqno++, getChecksumSize());\n+        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++);\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d createPacket(0, 0, bytesCurBlock, currentSeqno++);\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "463aec11718e47d4aabb86a7a539cb973460aae6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6934. Move checksum computation off the hot path when writing to RAM disk. Contributed by Chris Nauroth.\n",
      "commitDate": "27/10/14 9:38 AM",
      "commitName": "463aec11718e47d4aabb86a7a539cb973460aae6",
      "commitAuthor": "cnauroth",
      "commitDateOld": "17/10/14 6:30 PM",
      "commitNameOld": "2e140523d3ccb27809cde4a55e95f7e0006c028f",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 9.63,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,34 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n-        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n-            currentSeqno++, this.checksum.getChecksumSize());\n+        currentPacket \u003d new Packet(0, 0, bytesCurBlock, currentSeqno++, getChecksumSize());\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock, currentSeqno++, getChecksumSize());\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6294. Use INode IDs to avoid conflicts when a file open for write is renamed (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1593634 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/05/14 3:36 PM",
      "commitName": "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "25/03/14 9:11 PM",
      "commitNameOld": "1fbb04e367d7c330e6052207f9f11911f4f5f368",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 44.77,
      "commitsBetweenForRepo": 257,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,35 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n             currentSeqno++, this.checksum.getChecksumSize());\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n-      dfsClient.endFileLease(src);\n+      dfsClient.endFileLease(fileId);\n     } catch (ClosedChannelException e) {\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n            currentSeqno++, this.checksum.getChecksumSize());\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(fileId);\n    } catch (ClosedChannelException e) {\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "f2f5cdb5554d294a29ebf465101c5607fd56e244": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5335. Hive query failed with possible race in dfs output stream. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1531152 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/10/13 4:58 PM",
      "commitName": "f2f5cdb5554d294a29ebf465101c5607fd56e244",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "22/07/13 11:15 AM",
      "commitNameOld": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 80.24,
      "commitsBetweenForRepo": 499,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,35 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n-      IOException e \u003d lastException;\n+      IOException e \u003d lastException.getAndSet(null);\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n             currentSeqno++, this.checksum.getChecksumSize());\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.endFileLease(src);\n+    } catch (ClosedChannelException e) {\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException.getAndSet(null);\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n            currentSeqno++, this.checksum.getChecksumSize());\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(src);\n    } catch (ClosedChannelException e) {\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "735046ebecd9e803398be56fbf79dbde5226b4c1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3851. DFSOutputStream class code cleanup. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1377372 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/08/12 9:00 PM",
      "commitName": "735046ebecd9e803398be56fbf79dbde5226b4c1",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "14/08/12 1:59 PM",
      "commitNameOld": "f98d8eb291be364102b5c3011ce72e8f43eab389",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 11.29,
      "commitsBetweenForRepo": 87,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,34 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       IOException e \u003d lastException;\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n-        currentPacket \u003d new Packet(0, 0, bytesCurBlock);\n+        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n+            currentSeqno++, this.checksum.getChecksumSize());\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.endFileLease(src);\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException;\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock, \n            currentSeqno++, this.checksum.getChecksumSize());\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(src);\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3721. hsync support broke wire compatibility. Contributed by Todd Lipcon and Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1371495 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/08/12 2:31 PM",
      "commitName": "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "07/08/12 9:40 AM",
      "commitNameOld": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 2.2,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,33 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       IOException e \u003d lastException;\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n-        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n-            bytesCurBlock);\n+        currentPacket \u003d new Packet(0, 0, bytesCurBlock);\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.endFileLease(src);\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException;\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(0, 0, bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(src);\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "0a6806ce8c946b26eceac7d16b467c54c453df84": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3646. LeaseRenewer can hold reference to inactive DFSClient instances forever (Kihwal Lee via daryn)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1363170 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/07/12 4:21 PM",
      "commitName": "0a6806ce8c946b26eceac7d16b467c54c453df84",
      "commitAuthor": "Daryn Sharp",
      "commitDateOld": "15/07/12 7:58 PM",
      "commitNameOld": "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 2.85,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       IOException e \u003d lastException;\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n             bytesCurBlock);\n         currentPacket.lastPacketInBlock \u003d true;\n         currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n-      dfsClient.leaserenewer.closeFile(src, dfsClient);\n+      dfsClient.endFileLease(src);\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException;\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.endFileLease(src);\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "83cf475050dba27e72b4e399491638c670621175": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-744. Support hsync in HDFS. Contributed by Lars Hofhans\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1344419 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/05/12 12:10 PM",
      "commitName": "83cf475050dba27e72b4e399491638c670621175",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "29/05/12 12:37 PM",
      "commitNameOld": "47a29c63291f1f9f09b89ce6f3305c0a2ef27b3f",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.98,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,34 @@\n   public synchronized void close() throws IOException {\n     if (closed) {\n       IOException e \u003d lastException;\n       if (e \u003d\u003d null)\n         return;\n       else\n         throw e;\n     }\n \n     try {\n       flushBuffer();       // flush from all upper layers\n \n       if (currentPacket !\u003d null) { \n         waitAndQueueCurrentPacket();\n       }\n \n       if (bytesCurBlock !\u003d 0) {\n         // send an empty packet to mark the end of the block\n         currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n             bytesCurBlock);\n         currentPacket.lastPacketInBlock \u003d true;\n+        currentPacket.syncBlock \u003d shouldSyncBlock;\n       }\n \n       flushInternal();             // flush all data to Datanodes\n       // get last block before destroying the streamer\n       ExtendedBlock lastBlock \u003d streamer.getBlock();\n       closeThreads(false);\n       completeFile(lastBlock);\n       dfsClient.leaserenewer.closeFile(src, dfsClient);\n     } finally {\n       closed \u003d true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException;\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n        currentPacket.syncBlock \u003d shouldSyncBlock;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.leaserenewer.closeFile(src, dfsClient);\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException;\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.leaserenewer.closeFile(src, dfsClient);\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException;\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.leaserenewer.closeFile(src, dfsClient);\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,33 @@\n+  public synchronized void close() throws IOException {\n+    if (closed) {\n+      IOException e \u003d lastException;\n+      if (e \u003d\u003d null)\n+        return;\n+      else\n+        throw e;\n+    }\n+\n+    try {\n+      flushBuffer();       // flush from all upper layers\n+\n+      if (currentPacket !\u003d null) { \n+        waitAndQueueCurrentPacket();\n+      }\n+\n+      if (bytesCurBlock !\u003d 0) {\n+        // send an empty packet to mark the end of the block\n+        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n+            bytesCurBlock);\n+        currentPacket.lastPacketInBlock \u003d true;\n+      }\n+\n+      flushInternal();             // flush all data to Datanodes\n+      // get last block before destroying the streamer\n+      ExtendedBlock lastBlock \u003d streamer.getBlock();\n+      closeThreads(false);\n+      completeFile(lastBlock);\n+      dfsClient.leaserenewer.closeFile(src, dfsClient);\n+    } finally {\n+      closed \u003d true;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() throws IOException {\n    if (closed) {\n      IOException e \u003d lastException;\n      if (e \u003d\u003d null)\n        return;\n      else\n        throw e;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket !\u003d null) { \n        waitAndQueueCurrentPacket();\n      }\n\n      if (bytesCurBlock !\u003d 0) {\n        // send an empty packet to mark the end of the block\n        currentPacket \u003d new Packet(PacketHeader.PKT_HEADER_LEN, 0, \n            bytesCurBlock);\n        currentPacket.lastPacketInBlock \u003d true;\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock \u003d streamer.getBlock();\n      closeThreads(false);\n      completeFile(lastBlock);\n      dfsClient.leaserenewer.closeFile(src, dfsClient);\n    } finally {\n      closed \u003d true;\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}