{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockReaderLocal.java",
  "functionName": "read",
  "functionId": "read___arr-byte[]__off-int__len-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java",
  "functionStartLine": 552,
  "functionEndLine": 578,
  "numCommitsSeen": 69,
  "timeTaken": 3544,
  "changeHistory": [
    "f308561f1d885491b88db73ac63003202056d661",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "e2c9b288b223b9fd82dc12018936e13128413492",
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f",
    "124e507674c0d396f8494585e64226957199097b",
    "f55a1c08763e5f865fd9193d640c89a06ab49c4a",
    "5f39d6c239305bb5bdd20bfe5e84a0fcef635e04",
    "2ab10e29d9cca5018064be46a40e3c74423615a8"
  ],
  "changeHistoryShort": {
    "f308561f1d885491b88db73ac63003202056d661": "Yfilerename",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "e2c9b288b223b9fd82dc12018936e13128413492": "Yfilerename",
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f": "Ybodychange",
    "124e507674c0d396f8494585e64226957199097b": "Ymultichange(Yparameterchange,Ybodychange)",
    "f55a1c08763e5f865fd9193d640c89a06ab49c4a": "Ybodychange",
    "5f39d6c239305bb5bdd20bfe5e84a0fcef635e04": "Ybodychange",
    "2ab10e29d9cca5018064be46a40e3c74423615a8": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f308561f1d885491b88db73ac63003202056d661": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8057 Move BlockReader implementation to the client implementation package.  Contributed by Takanobu Asanuma\n",
      "commitDate": "25/04/16 12:01 PM",
      "commitName": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "25/04/16 9:38 AM",
      "commitNameOld": "10f0f7851a3255caab775777e8fb6c2781d97062",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized int read(byte[] arr, int off, int len)\n        throws IOException {\n    boolean canSkipChecksum \u003d createNoChecksumContext();\n    int nRead;\n    try {\n      final String traceFormatStr \u003d \"read(arr.length\u003d{}, off\u003d{}, len\u003d{}, \"\n          + \"filename\u003d{}, block\u003d{}, canSkipChecksum\u003d{})\";\n      LOG.trace(traceFormatStr + \": starting\",\n          arr.length, off, len, filename, block, canSkipChecksum);\n      try {\n        if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n          nRead \u003d readWithoutBounceBuffer(arr, off, len);\n        } else {\n          nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n        }\n      } catch (IOException e) {\n        LOG.trace(traceFormatStr + \": I/O error\",\n            arr.length, off, len, filename, block, canSkipChecksum, e);\n        throw e;\n      }\n      LOG.trace(traceFormatStr + \": returning {}\",\n          arr.length, off, len, filename, block, canSkipChecksum, nRead);\n    } finally {\n      if (canSkipChecksum) releaseNoChecksumContext();\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java"
      }
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,27 @@\n   public synchronized int read(byte[] arr, int off, int len)\n         throws IOException {\n     boolean canSkipChecksum \u003d createNoChecksumContext();\n     int nRead;\n     try {\n-      String traceString \u003d null;\n-      if (LOG.isTraceEnabled()) {\n-        traceString \u003d new StringBuilder().\n-            append(\"read(arr.length\u003d\").append(arr.length).\n-            append(\", off\u003d\").append(off).\n-            append(\", len\u003d\").append(len).\n-            append(\", filename\u003d\").append(filename).\n-            append(\", block\u003d\").append(block).\n-            append(\", canSkipChecksum\u003d\").append(canSkipChecksum).\n-            append(\")\").toString();\n-        LOG.trace(traceString + \": starting\");\n-      }\n+      final String traceFormatStr \u003d \"read(arr.length\u003d{}, off\u003d{}, len\u003d{}, \"\n+          + \"filename\u003d{}, block\u003d{}, canSkipChecksum\u003d{})\";\n+      LOG.trace(traceFormatStr + \": starting\",\n+          arr.length, off, len, filename, block, canSkipChecksum);\n       try {\n         if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n           nRead \u003d readWithoutBounceBuffer(arr, off, len);\n         } else {\n           nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n         }\n       } catch (IOException e) {\n-        if (LOG.isTraceEnabled()) {\n-          LOG.trace(traceString + \": I/O error\", e);\n-        }\n+        LOG.trace(traceFormatStr + \": I/O error\",\n+            arr.length, off, len, filename, block, canSkipChecksum, e);\n         throw e;\n       }\n-      if (LOG.isTraceEnabled()) {\n-        LOG.trace(traceString + \": returning \" + nRead);\n-      }\n+      LOG.trace(traceFormatStr + \": returning {}\",\n+          arr.length, off, len, filename, block, canSkipChecksum, nRead);\n     } finally {\n       if (canSkipChecksum) releaseNoChecksumContext();\n     }\n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] arr, int off, int len)\n        throws IOException {\n    boolean canSkipChecksum \u003d createNoChecksumContext();\n    int nRead;\n    try {\n      final String traceFormatStr \u003d \"read(arr.length\u003d{}, off\u003d{}, len\u003d{}, \"\n          + \"filename\u003d{}, block\u003d{}, canSkipChecksum\u003d{})\";\n      LOG.trace(traceFormatStr + \": starting\",\n          arr.length, off, len, filename, block, canSkipChecksum);\n      try {\n        if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n          nRead \u003d readWithoutBounceBuffer(arr, off, len);\n        } else {\n          nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n        }\n      } catch (IOException e) {\n        LOG.trace(traceFormatStr + \": I/O error\",\n            arr.length, off, len, filename, block, canSkipChecksum, e);\n        throw e;\n      }\n      LOG.trace(traceFormatStr + \": returning {}\",\n          arr.length, off, len, filename, block, canSkipChecksum, nRead);\n    } finally {\n      if (canSkipChecksum) releaseNoChecksumContext();\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,37 @@\n   public synchronized int read(byte[] arr, int off, int len)\n         throws IOException {\n     boolean canSkipChecksum \u003d createNoChecksumContext();\n     int nRead;\n     try {\n-      final String traceFormatStr \u003d \"read(arr.length\u003d{}, off\u003d{}, len\u003d{}, \"\n-          + \"filename\u003d{}, block\u003d{}, canSkipChecksum\u003d{})\";\n-      LOG.trace(traceFormatStr + \": starting\",\n-          arr.length, off, len, filename, block, canSkipChecksum);\n+      String traceString \u003d null;\n+      if (LOG.isTraceEnabled()) {\n+        traceString \u003d new StringBuilder().\n+            append(\"read(arr.length\u003d\").append(arr.length).\n+            append(\", off\u003d\").append(off).\n+            append(\", len\u003d\").append(len).\n+            append(\", filename\u003d\").append(filename).\n+            append(\", block\u003d\").append(block).\n+            append(\", canSkipChecksum\u003d\").append(canSkipChecksum).\n+            append(\")\").toString();\n+        LOG.trace(traceString + \": starting\");\n+      }\n       try {\n         if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n           nRead \u003d readWithoutBounceBuffer(arr, off, len);\n         } else {\n           nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n         }\n       } catch (IOException e) {\n-        LOG.trace(traceFormatStr + \": I/O error\",\n-            arr.length, off, len, filename, block, canSkipChecksum, e);\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(traceString + \": I/O error\", e);\n+        }\n         throw e;\n       }\n-      LOG.trace(traceFormatStr + \": returning {}\",\n-          arr.length, off, len, filename, block, canSkipChecksum, nRead);\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(traceString + \": returning \" + nRead);\n+      }\n     } finally {\n       if (canSkipChecksum) releaseNoChecksumContext();\n     }\n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] arr, int off, int len)\n        throws IOException {\n    boolean canSkipChecksum \u003d createNoChecksumContext();\n    int nRead;\n    try {\n      String traceString \u003d null;\n      if (LOG.isTraceEnabled()) {\n        traceString \u003d new StringBuilder().\n            append(\"read(arr.length\u003d\").append(arr.length).\n            append(\", off\u003d\").append(off).\n            append(\", len\u003d\").append(len).\n            append(\", filename\u003d\").append(filename).\n            append(\", block\u003d\").append(block).\n            append(\", canSkipChecksum\u003d\").append(canSkipChecksum).\n            append(\")\").toString();\n        LOG.trace(traceString + \": starting\");\n      }\n      try {\n        if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n          nRead \u003d readWithoutBounceBuffer(arr, off, len);\n        } else {\n          nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n        }\n      } catch (IOException e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(traceString + \": I/O error\", e);\n        }\n        throw e;\n      }\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(traceString + \": returning \" + nRead);\n      }\n    } finally {\n      if (canSkipChecksum) releaseNoChecksumContext();\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.42,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,27 @@\n   public synchronized int read(byte[] arr, int off, int len)\n         throws IOException {\n     boolean canSkipChecksum \u003d createNoChecksumContext();\n     int nRead;\n     try {\n-      String traceString \u003d null;\n-      if (LOG.isTraceEnabled()) {\n-        traceString \u003d new StringBuilder().\n-            append(\"read(arr.length\u003d\").append(arr.length).\n-            append(\", off\u003d\").append(off).\n-            append(\", len\u003d\").append(len).\n-            append(\", filename\u003d\").append(filename).\n-            append(\", block\u003d\").append(block).\n-            append(\", canSkipChecksum\u003d\").append(canSkipChecksum).\n-            append(\")\").toString();\n-        LOG.trace(traceString + \": starting\");\n-      }\n+      final String traceFormatStr \u003d \"read(arr.length\u003d{}, off\u003d{}, len\u003d{}, \"\n+          + \"filename\u003d{}, block\u003d{}, canSkipChecksum\u003d{})\";\n+      LOG.trace(traceFormatStr + \": starting\",\n+          arr.length, off, len, filename, block, canSkipChecksum);\n       try {\n         if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n           nRead \u003d readWithoutBounceBuffer(arr, off, len);\n         } else {\n           nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n         }\n       } catch (IOException e) {\n-        if (LOG.isTraceEnabled()) {\n-          LOG.trace(traceString + \": I/O error\", e);\n-        }\n+        LOG.trace(traceFormatStr + \": I/O error\",\n+            arr.length, off, len, filename, block, canSkipChecksum, e);\n         throw e;\n       }\n-      if (LOG.isTraceEnabled()) {\n-        LOG.trace(traceString + \": returning \" + nRead);\n-      }\n+      LOG.trace(traceFormatStr + \": returning {}\",\n+          arr.length, off, len, filename, block, canSkipChecksum, nRead);\n     } finally {\n       if (canSkipChecksum) releaseNoChecksumContext();\n     }\n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] arr, int off, int len)\n        throws IOException {\n    boolean canSkipChecksum \u003d createNoChecksumContext();\n    int nRead;\n    try {\n      final String traceFormatStr \u003d \"read(arr.length\u003d{}, off\u003d{}, len\u003d{}, \"\n          + \"filename\u003d{}, block\u003d{}, canSkipChecksum\u003d{})\";\n      LOG.trace(traceFormatStr + \": starting\",\n          arr.length, off, len, filename, block, canSkipChecksum);\n      try {\n        if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n          nRead \u003d readWithoutBounceBuffer(arr, off, len);\n        } else {\n          nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n        }\n      } catch (IOException e) {\n        LOG.trace(traceFormatStr + \": I/O error\",\n            arr.length, off, len, filename, block, canSkipChecksum, e);\n        throw e;\n      }\n      LOG.trace(traceFormatStr + \": returning {}\",\n          arr.length, off, len, filename, block, canSkipChecksum, nRead);\n    } finally {\n      if (canSkipChecksum) releaseNoChecksumContext();\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "e2c9b288b223b9fd82dc12018936e13128413492": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "28/08/15 2:38 PM",
      "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/08/15 2:21 PM",
      "commitNameOld": "b94b56806d3d6e04984e229b479f7ac15b62bbfa",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized int read(byte[] arr, int off, int len)\n        throws IOException {\n    boolean canSkipChecksum \u003d createNoChecksumContext();\n    int nRead;\n    try {\n      String traceString \u003d null;\n      if (LOG.isTraceEnabled()) {\n        traceString \u003d new StringBuilder().\n            append(\"read(arr.length\u003d\").append(arr.length).\n            append(\", off\u003d\").append(off).\n            append(\", len\u003d\").append(len).\n            append(\", filename\u003d\").append(filename).\n            append(\", block\u003d\").append(block).\n            append(\", canSkipChecksum\u003d\").append(canSkipChecksum).\n            append(\")\").toString();\n        LOG.trace(traceString + \": starting\");\n      }\n      try {\n        if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n          nRead \u003d readWithoutBounceBuffer(arr, off, len);\n        } else {\n          nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n        }\n      } catch (IOException e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(traceString + \": I/O error\", e);\n        }\n        throw e;\n      }\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(traceString + \": returning \" + nRead);\n      }\n    } finally {\n      if (canSkipChecksum) releaseNoChecksumContext();\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java"
      }
    },
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5950. The DFSClient and DataNode should use shared memory segments to communicate short-circuit information (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1573433 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/03/14 7:58 PM",
      "commitName": "dd049a2f6097da189ccce2f5890a2b9bc77fa73f",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "12/02/14 11:08 AM",
      "commitNameOld": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 18.37,
      "commitsBetweenForRepo": 140,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,37 @@\n   public synchronized int read(byte[] arr, int off, int len)\n         throws IOException {\n-    boolean canSkipChecksum \u003d getCanSkipChecksum();\n-    String traceString \u003d null;\n-    if (LOG.isTraceEnabled()) {\n-      traceString \u003d new StringBuilder().\n-          append(\"read(arr.length\u003d\").append(arr.length).\n-          append(\", off\u003d\").append(off).\n-          append(\", len\u003d\").append(len).\n-          append(\", filename\u003d\").append(filename).\n-          append(\", block\u003d\").append(block).\n-          append(\", canSkipChecksum\u003d\").append(canSkipChecksum).\n-          append(\")\").toString();\n-      LOG.trace(traceString + \": starting\");\n-    }\n+    boolean canSkipChecksum \u003d createNoChecksumContext();\n     int nRead;\n     try {\n-      if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n-        nRead \u003d readWithoutBounceBuffer(arr, off, len);\n-      } else {\n-        nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n-      }\n-    } catch (IOException e) {\n+      String traceString \u003d null;\n       if (LOG.isTraceEnabled()) {\n-        LOG.trace(traceString + \": I/O error\", e);\n+        traceString \u003d new StringBuilder().\n+            append(\"read(arr.length\u003d\").append(arr.length).\n+            append(\", off\u003d\").append(off).\n+            append(\", len\u003d\").append(len).\n+            append(\", filename\u003d\").append(filename).\n+            append(\", block\u003d\").append(block).\n+            append(\", canSkipChecksum\u003d\").append(canSkipChecksum).\n+            append(\")\").toString();\n+        LOG.trace(traceString + \": starting\");\n       }\n-      throw e;\n-    }\n-    if (LOG.isTraceEnabled()) {\n-      LOG.trace(traceString + \": returning \" + nRead);\n+      try {\n+        if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n+          nRead \u003d readWithoutBounceBuffer(arr, off, len);\n+        } else {\n+          nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n+        }\n+      } catch (IOException e) {\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(traceString + \": I/O error\", e);\n+        }\n+        throw e;\n+      }\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(traceString + \": returning \" + nRead);\n+      }\n+    } finally {\n+      if (canSkipChecksum) releaseNoChecksumContext();\n     }\n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] arr, int off, int len)\n        throws IOException {\n    boolean canSkipChecksum \u003d createNoChecksumContext();\n    int nRead;\n    try {\n      String traceString \u003d null;\n      if (LOG.isTraceEnabled()) {\n        traceString \u003d new StringBuilder().\n            append(\"read(arr.length\u003d\").append(arr.length).\n            append(\", off\u003d\").append(off).\n            append(\", len\u003d\").append(len).\n            append(\", filename\u003d\").append(filename).\n            append(\", block\u003d\").append(block).\n            append(\", canSkipChecksum\u003d\").append(canSkipChecksum).\n            append(\")\").toString();\n        LOG.trace(traceString + \": starting\");\n      }\n      try {\n        if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n          nRead \u003d readWithoutBounceBuffer(arr, off, len);\n        } else {\n          nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n        }\n      } catch (IOException e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(traceString + \": I/O error\", e);\n        }\n        throw e;\n      }\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(traceString + \": returning \" + nRead);\n      }\n    } finally {\n      if (canSkipChecksum) releaseNoChecksumContext();\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "124e507674c0d396f8494585e64226957199097b": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5634. Allow BlockReaderLocal to switch between checksumming and not (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551701 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 12:57 PM",
      "commitName": "124e507674c0d396f8494585e64226957199097b",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5634. Allow BlockReaderLocal to switch between checksumming and not (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551701 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/12/13 12:57 PM",
          "commitName": "124e507674c0d396f8494585e64226957199097b",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "27/09/13 3:51 PM",
          "commitNameOld": "eccdb9aa8bcdee750583d16a1253f1c5faabd036",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 80.92,
          "commitsBetweenForRepo": 532,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,33 @@\n-  public synchronized int read(byte[] buf, int off, int len) throws IOException {\n+  public synchronized int read(byte[] arr, int off, int len)\n+        throws IOException {\n+    boolean canSkipChecksum \u003d getCanSkipChecksum();\n+    String traceString \u003d null;\n     if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"read off \" + off + \" len \" + len);\n+      traceString \u003d new StringBuilder().\n+          append(\"read(arr.length\u003d\").append(arr.length).\n+          append(\", off\u003d\").append(off).\n+          append(\", len\u003d\").append(len).\n+          append(\", filename\u003d\").append(filename).\n+          append(\", block\u003d\").append(block).\n+          append(\", canSkipChecksum\u003d\").append(canSkipChecksum).\n+          append(\")\").toString();\n+      LOG.trace(traceString + \": starting\");\n     }\n-    if (!verifyChecksum) {\n-      return dataIn.read(buf, off, len);\n+    int nRead;\n+    try {\n+      if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n+        nRead \u003d readWithoutBounceBuffer(arr, off, len);\n+      } else {\n+        nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n+      }\n+    } catch (IOException e) {\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(traceString + \": I/O error\", e);\n+      }\n+      throw e;\n     }\n-\n-    int nRead \u003d fillSlowReadBuffer(slowReadBuff.capacity());\n-\n-    if (nRead \u003e 0) {\n-      // Possible that buffer is filled with a larger read than we need, since\n-      // we tried to read as much as possible at once\n-      nRead \u003d Math.min(len, nRead);\n-      slowReadBuff.get(buf, off, nRead);\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(traceString + \": returning \" + nRead);\n     }\n-\n     return nRead;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized int read(byte[] arr, int off, int len)\n        throws IOException {\n    boolean canSkipChecksum \u003d getCanSkipChecksum();\n    String traceString \u003d null;\n    if (LOG.isTraceEnabled()) {\n      traceString \u003d new StringBuilder().\n          append(\"read(arr.length\u003d\").append(arr.length).\n          append(\", off\u003d\").append(off).\n          append(\", len\u003d\").append(len).\n          append(\", filename\u003d\").append(filename).\n          append(\", block\u003d\").append(block).\n          append(\", canSkipChecksum\u003d\").append(canSkipChecksum).\n          append(\")\").toString();\n      LOG.trace(traceString + \": starting\");\n    }\n    int nRead;\n    try {\n      if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n        nRead \u003d readWithoutBounceBuffer(arr, off, len);\n      } else {\n        nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n      }\n    } catch (IOException e) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(traceString + \": I/O error\", e);\n      }\n      throw e;\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(traceString + \": returning \" + nRead);\n    }\n    return nRead;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
          "extendedDetails": {
            "oldValue": "[buf-byte[], off-int, len-int]",
            "newValue": "[arr-byte[], off-int, len-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5634. Allow BlockReaderLocal to switch between checksumming and not (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551701 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/12/13 12:57 PM",
          "commitName": "124e507674c0d396f8494585e64226957199097b",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "27/09/13 3:51 PM",
          "commitNameOld": "eccdb9aa8bcdee750583d16a1253f1c5faabd036",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 80.92,
          "commitsBetweenForRepo": 532,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,33 @@\n-  public synchronized int read(byte[] buf, int off, int len) throws IOException {\n+  public synchronized int read(byte[] arr, int off, int len)\n+        throws IOException {\n+    boolean canSkipChecksum \u003d getCanSkipChecksum();\n+    String traceString \u003d null;\n     if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"read off \" + off + \" len \" + len);\n+      traceString \u003d new StringBuilder().\n+          append(\"read(arr.length\u003d\").append(arr.length).\n+          append(\", off\u003d\").append(off).\n+          append(\", len\u003d\").append(len).\n+          append(\", filename\u003d\").append(filename).\n+          append(\", block\u003d\").append(block).\n+          append(\", canSkipChecksum\u003d\").append(canSkipChecksum).\n+          append(\")\").toString();\n+      LOG.trace(traceString + \": starting\");\n     }\n-    if (!verifyChecksum) {\n-      return dataIn.read(buf, off, len);\n+    int nRead;\n+    try {\n+      if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n+        nRead \u003d readWithoutBounceBuffer(arr, off, len);\n+      } else {\n+        nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n+      }\n+    } catch (IOException e) {\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(traceString + \": I/O error\", e);\n+      }\n+      throw e;\n     }\n-\n-    int nRead \u003d fillSlowReadBuffer(slowReadBuff.capacity());\n-\n-    if (nRead \u003e 0) {\n-      // Possible that buffer is filled with a larger read than we need, since\n-      // we tried to read as much as possible at once\n-      nRead \u003d Math.min(len, nRead);\n-      slowReadBuff.get(buf, off, nRead);\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(traceString + \": returning \" + nRead);\n     }\n-\n     return nRead;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized int read(byte[] arr, int off, int len)\n        throws IOException {\n    boolean canSkipChecksum \u003d getCanSkipChecksum();\n    String traceString \u003d null;\n    if (LOG.isTraceEnabled()) {\n      traceString \u003d new StringBuilder().\n          append(\"read(arr.length\u003d\").append(arr.length).\n          append(\", off\u003d\").append(off).\n          append(\", len\u003d\").append(len).\n          append(\", filename\u003d\").append(filename).\n          append(\", block\u003d\").append(block).\n          append(\", canSkipChecksum\u003d\").append(canSkipChecksum).\n          append(\")\").toString();\n      LOG.trace(traceString + \": starting\");\n    }\n    int nRead;\n    try {\n      if (canSkipChecksum \u0026\u0026 zeroReadaheadRequested) {\n        nRead \u003d readWithoutBounceBuffer(arr, off, len);\n      } else {\n        nRead \u003d readWithBounceBuffer(arr, off, len, canSkipChecksum);\n      }\n    } catch (IOException e) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(traceString + \": I/O error\", e);\n      }\n      throw e;\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(traceString + \": returning \" + nRead);\n    }\n    return nRead;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
          "extendedDetails": {}
        }
      ]
    },
    "f55a1c08763e5f865fd9193d640c89a06ab49c4a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2834. Add a ByteBuffer-based read API to DFSInputStream. Contributed by Henry Robinson.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1303474 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/03/12 10:30 AM",
      "commitName": "f55a1c08763e5f865fd9193d640c89a06ab49c4a",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "16/02/12 10:58 AM",
      "commitNameOld": "b8448dea82c72ff6c1558b9ebf3f24cd1c6e728b",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 33.94,
      "commitsBetweenForRepo": 229,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,19 @@\n   public synchronized int read(byte[] buf, int off, int len) throws IOException {\n-    if (LOG.isDebugEnabled()) {\n-      LOG.info(\"read off \" + off + \" len \" + len);\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"read off \" + off + \" len \" + len);\n     }\n     if (!verifyChecksum) {\n       return dataIn.read(buf, off, len);\n-    } else {\n-      int dataRead \u003d -1;\n-      if (dataBuff.remaining() \u003d\u003d 0) {\n-        dataBuff.clear();\n-        checksumBuff.clear();\n-        dataRead \u003d readIntoBuffer(dataIn, dataBuff);\n-        readIntoBuffer(checksumIn, checksumBuff);\n-        checksumBuff.flip();\n-        dataBuff.flip();\n-        checksum.verifyChunkedSums(dataBuff, checksumBuff, filename,\n-            this.startOffset);\n-      } else {\n-        dataRead \u003d dataBuff.remaining();\n-      }\n-      if (dataRead \u003e 0) {\n-        int nRead \u003d Math.min(dataRead - offsetFromChunkBoundary, len);\n-        if (offsetFromChunkBoundary \u003e 0) {\n-          dataBuff.position(offsetFromChunkBoundary);\n-          // Its either end of file or dataRead is greater than the\n-          // offsetFromChunkBoundary\n-          offsetFromChunkBoundary \u003d 0;\n-        }\n-        if (nRead \u003e 0) {\n-          dataBuff.get(buf, off, nRead);\n-          return nRead;\n-        } else {\n-          return 0;\n-        }\n-      } else {\n-        return -1;\n-      }\n     }\n+\n+    int nRead \u003d fillSlowReadBuffer(slowReadBuff.capacity());\n+\n+    if (nRead \u003e 0) {\n+      // Possible that buffer is filled with a larger read than we need, since\n+      // we tried to read as much as possible at once\n+      nRead \u003d Math.min(len, nRead);\n+      slowReadBuff.get(buf, off, nRead);\n+    }\n+\n+    return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len) throws IOException {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"read off \" + off + \" len \" + len);\n    }\n    if (!verifyChecksum) {\n      return dataIn.read(buf, off, len);\n    }\n\n    int nRead \u003d fillSlowReadBuffer(slowReadBuff.capacity());\n\n    if (nRead \u003e 0) {\n      // Possible that buffer is filled with a larger read than we need, since\n      // we tried to read as much as possible at once\n      nRead \u003d Math.min(len, nRead);\n      slowReadBuff.get(buf, off, nRead);\n    }\n\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "5f39d6c239305bb5bdd20bfe5e84a0fcef635e04": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2654. Make BlockReaderLocal not extend RemoteBlockReader2. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213592 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 12:09 AM",
      "commitName": "5f39d6c239305bb5bdd20bfe5e84a0fcef635e04",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "21/11/11 6:57 PM",
      "commitNameOld": "2ab10e29d9cca5018064be46a40e3c74423615a8",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 21.22,
      "commitsBetweenForRepo": 111,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,39 @@\n   public synchronized int read(byte[] buf, int off, int len) throws IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.info(\"read off \" + off + \" len \" + len);\n     }\n     if (!verifyChecksum) {\n       return dataIn.read(buf, off, len);\n     } else {\n       int dataRead \u003d -1;\n       if (dataBuff.remaining() \u003d\u003d 0) {\n         dataBuff.clear();\n         checksumBuff.clear();\n         dataRead \u003d readIntoBuffer(dataIn, dataBuff);\n         readIntoBuffer(checksumIn, checksumBuff);\n         checksumBuff.flip();\n         dataBuff.flip();\n-        if (verifyChecksum) {\n-          checksum.verifyChunkedSums(dataBuff, checksumBuff, filename,\n-              this.startOffset);\n-        }\n+        checksum.verifyChunkedSums(dataBuff, checksumBuff, filename,\n+            this.startOffset);\n       } else {\n         dataRead \u003d dataBuff.remaining();\n       }\n       if (dataRead \u003e 0) {\n         int nRead \u003d Math.min(dataRead - offsetFromChunkBoundary, len);\n         if (offsetFromChunkBoundary \u003e 0) {\n           dataBuff.position(offsetFromChunkBoundary);\n           // Its either end of file or dataRead is greater than the\n           // offsetFromChunkBoundary\n           offsetFromChunkBoundary \u003d 0;\n         }\n         if (nRead \u003e 0) {\n           dataBuff.get(buf, off, nRead);\n           return nRead;\n         } else {\n           return 0;\n         }\n       } else {\n         return -1;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.info(\"read off \" + off + \" len \" + len);\n    }\n    if (!verifyChecksum) {\n      return dataIn.read(buf, off, len);\n    } else {\n      int dataRead \u003d -1;\n      if (dataBuff.remaining() \u003d\u003d 0) {\n        dataBuff.clear();\n        checksumBuff.clear();\n        dataRead \u003d readIntoBuffer(dataIn, dataBuff);\n        readIntoBuffer(checksumIn, checksumBuff);\n        checksumBuff.flip();\n        dataBuff.flip();\n        checksum.verifyChunkedSums(dataBuff, checksumBuff, filename,\n            this.startOffset);\n      } else {\n        dataRead \u003d dataBuff.remaining();\n      }\n      if (dataRead \u003e 0) {\n        int nRead \u003d Math.min(dataRead - offsetFromChunkBoundary, len);\n        if (offsetFromChunkBoundary \u003e 0) {\n          dataBuff.position(offsetFromChunkBoundary);\n          // Its either end of file or dataRead is greater than the\n          // offsetFromChunkBoundary\n          offsetFromChunkBoundary \u003d 0;\n        }\n        if (nRead \u003e 0) {\n          dataBuff.get(buf, off, nRead);\n          return nRead;\n        } else {\n          return 0;\n        }\n      } else {\n        return -1;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "2ab10e29d9cca5018064be46a40e3c74423615a8": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2246. Enable reading a block directly from local file system for a client on the same node as the block file.  Contributed by Andrew Purtell, Suresh Srinivas and Jitendra Nath Pandey\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1204792 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/11/11 6:57 PM",
      "commitName": "2ab10e29d9cca5018064be46a40e3c74423615a8",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,41 @@\n+  public synchronized int read(byte[] buf, int off, int len) throws IOException {\n+    if (LOG.isDebugEnabled()) {\n+      LOG.info(\"read off \" + off + \" len \" + len);\n+    }\n+    if (!verifyChecksum) {\n+      return dataIn.read(buf, off, len);\n+    } else {\n+      int dataRead \u003d -1;\n+      if (dataBuff.remaining() \u003d\u003d 0) {\n+        dataBuff.clear();\n+        checksumBuff.clear();\n+        dataRead \u003d readIntoBuffer(dataIn, dataBuff);\n+        readIntoBuffer(checksumIn, checksumBuff);\n+        checksumBuff.flip();\n+        dataBuff.flip();\n+        if (verifyChecksum) {\n+          checksum.verifyChunkedSums(dataBuff, checksumBuff, filename,\n+              this.startOffset);\n+        }\n+      } else {\n+        dataRead \u003d dataBuff.remaining();\n+      }\n+      if (dataRead \u003e 0) {\n+        int nRead \u003d Math.min(dataRead - offsetFromChunkBoundary, len);\n+        if (offsetFromChunkBoundary \u003e 0) {\n+          dataBuff.position(offsetFromChunkBoundary);\n+          // Its either end of file or dataRead is greater than the\n+          // offsetFromChunkBoundary\n+          offsetFromChunkBoundary \u003d 0;\n+        }\n+        if (nRead \u003e 0) {\n+          dataBuff.get(buf, off, nRead);\n+          return nRead;\n+        } else {\n+          return 0;\n+        }\n+      } else {\n+        return -1;\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized int read(byte[] buf, int off, int len) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.info(\"read off \" + off + \" len \" + len);\n    }\n    if (!verifyChecksum) {\n      return dataIn.read(buf, off, len);\n    } else {\n      int dataRead \u003d -1;\n      if (dataBuff.remaining() \u003d\u003d 0) {\n        dataBuff.clear();\n        checksumBuff.clear();\n        dataRead \u003d readIntoBuffer(dataIn, dataBuff);\n        readIntoBuffer(checksumIn, checksumBuff);\n        checksumBuff.flip();\n        dataBuff.flip();\n        if (verifyChecksum) {\n          checksum.verifyChunkedSums(dataBuff, checksumBuff, filename,\n              this.startOffset);\n        }\n      } else {\n        dataRead \u003d dataBuff.remaining();\n      }\n      if (dataRead \u003e 0) {\n        int nRead \u003d Math.min(dataRead - offsetFromChunkBoundary, len);\n        if (offsetFromChunkBoundary \u003e 0) {\n          dataBuff.position(offsetFromChunkBoundary);\n          // Its either end of file or dataRead is greater than the\n          // offsetFromChunkBoundary\n          offsetFromChunkBoundary \u003d 0;\n        }\n        if (nRead \u003e 0) {\n          dataBuff.get(buf, off, nRead);\n          return nRead;\n        } else {\n          return 0;\n        }\n      } else {\n        return -1;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java"
    }
  }
}