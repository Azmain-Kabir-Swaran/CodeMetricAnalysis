{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSImageFormatPBSnapshot.java",
  "functionName": "loadSnapshots",
  "functionId": "loadSnapshots___in-InputStream__size-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java",
  "functionStartLine": 167,
  "functionEndLine": 181,
  "numCommitsSeen": 34,
  "timeTaken": 2384,
  "changeHistory": [
    "76a621ffd2d66bf012a554f4400091a92a5b473e",
    "d03acc756094a332f98167426a39db8faf38f450",
    "5c978a43c3052cc1466b23653c354399186b4e10",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305"
  ],
  "changeHistoryShort": {
    "76a621ffd2d66bf012a554f4400091a92a5b473e": "Ybodychange",
    "d03acc756094a332f98167426a39db8faf38f450": "Ybodychange",
    "5c978a43c3052cc1466b23653c354399186b4e10": "Ybodychange",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": "Yintroduced"
  },
  "changeHistoryDetails": {
    "76a621ffd2d66bf012a554f4400091a92a5b473e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6609. Use DirectorySnapshottableFeature to represent a snapshottable directory. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1608631 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/07/14 5:08 PM",
      "commitName": "76a621ffd2d66bf012a554f4400091a92a5b473e",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "21/05/14 6:57 AM",
      "commitNameOld": "ac23a55547716df29b3e25c98a113399e184d9d1",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 47.42,
      "commitsBetweenForRepo": 284,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,15 @@\n     private void loadSnapshots(InputStream in, int size) throws IOException {\n       for (int i \u003d 0; i \u003c size; i++) {\n         SnapshotSection.Snapshot pbs \u003d SnapshotSection.Snapshot\n             .parseDelimitedFrom(in);\n         INodeDirectory root \u003d loadINodeDirectory(pbs.getRoot(),\n             parent.getLoaderContext());\n         int sid \u003d pbs.getSnapshotId();\n-        INodeDirectorySnapshottable parent \u003d (INodeDirectorySnapshottable) fsDir\n-            .getInode(root.getId()).asDirectory();\n+        INodeDirectory parent \u003d fsDir.getInode(root.getId()).asDirectory();\n         Snapshot snapshot \u003d new Snapshot(sid, root, parent);\n         // add the snapshot to parent, since we follow the sequence of\n         // snapshotsByNames when saving, we do not need to sort when loading\n-        parent.addSnapshot(snapshot);\n+        parent.getDirectorySnapshottableFeature().addSnapshot(snapshot);\n         snapshotMap.put(sid, snapshot);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void loadSnapshots(InputStream in, int size) throws IOException {\n      for (int i \u003d 0; i \u003c size; i++) {\n        SnapshotSection.Snapshot pbs \u003d SnapshotSection.Snapshot\n            .parseDelimitedFrom(in);\n        INodeDirectory root \u003d loadINodeDirectory(pbs.getRoot(),\n            parent.getLoaderContext());\n        int sid \u003d pbs.getSnapshotId();\n        INodeDirectory parent \u003d fsDir.getInode(root.getId()).asDirectory();\n        Snapshot snapshot \u003d new Snapshot(sid, root, parent);\n        // add the snapshot to parent, since we follow the sequence of\n        // snapshotsByNames when saving, we do not need to sort when loading\n        parent.getDirectorySnapshottableFeature().addSnapshot(snapshot);\n        snapshotMap.put(sid, snapshot);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java",
      "extendedDetails": {}
    },
    "d03acc756094a332f98167426a39db8faf38f450": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5914. Incorporate ACLs with the changes from HDFS-5698. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4685@1566991 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/02/14 10:25 PM",
      "commitName": "d03acc756094a332f98167426a39db8faf38f450",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "10/02/14 10:00 PM",
      "commitNameOld": "3bf2f04baca59ee7c74f13193a569ed1d6f5458e",
      "commitAuthorOld": "",
      "daysBetweenCommits": 0.02,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n     private void loadSnapshots(InputStream in, int size) throws IOException {\n       for (int i \u003d 0; i \u003c size; i++) {\n         SnapshotSection.Snapshot pbs \u003d SnapshotSection.Snapshot\n             .parseDelimitedFrom(in);\n         INodeDirectory root \u003d loadINodeDirectory(pbs.getRoot(),\n-            parent.getLoaderContext().getStringTable());\n+            parent.getLoaderContext());\n         int sid \u003d pbs.getSnapshotId();\n         INodeDirectorySnapshottable parent \u003d (INodeDirectorySnapshottable) fsDir\n             .getInode(root.getId()).asDirectory();\n         Snapshot snapshot \u003d new Snapshot(sid, root, parent);\n         // add the snapshot to parent, since we follow the sequence of\n         // snapshotsByNames when saving, we do not need to sort when loading\n         parent.addSnapshot(snapshot);\n         snapshotMap.put(sid, snapshot);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void loadSnapshots(InputStream in, int size) throws IOException {\n      for (int i \u003d 0; i \u003c size; i++) {\n        SnapshotSection.Snapshot pbs \u003d SnapshotSection.Snapshot\n            .parseDelimitedFrom(in);\n        INodeDirectory root \u003d loadINodeDirectory(pbs.getRoot(),\n            parent.getLoaderContext());\n        int sid \u003d pbs.getSnapshotId();\n        INodeDirectorySnapshottable parent \u003d (INodeDirectorySnapshottable) fsDir\n            .getInode(root.getId()).asDirectory();\n        Snapshot snapshot \u003d new Snapshot(sid, root, parent);\n        // add the snapshot to parent, since we follow the sequence of\n        // snapshotsByNames when saving, we do not need to sort when loading\n        parent.addSnapshot(snapshot);\n        snapshotMap.put(sid, snapshot);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java",
      "extendedDetails": {}
    },
    "5c978a43c3052cc1466b23653c354399186b4e10": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5915. Refactor FSImageFormatProtobuf to simplify cross section reads. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566824 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/02/14 3:13 PM",
      "commitName": "5c978a43c3052cc1466b23653c354399186b4e10",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "09/02/14 11:18 AM",
      "commitNameOld": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.16,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n     private void loadSnapshots(InputStream in, int size) throws IOException {\n       for (int i \u003d 0; i \u003c size; i++) {\n         SnapshotSection.Snapshot pbs \u003d SnapshotSection.Snapshot\n             .parseDelimitedFrom(in);\n         INodeDirectory root \u003d loadINodeDirectory(pbs.getRoot(),\n-            parent.getStringTable());\n+            parent.getLoaderContext().getStringTable());\n         int sid \u003d pbs.getSnapshotId();\n         INodeDirectorySnapshottable parent \u003d (INodeDirectorySnapshottable) fsDir\n             .getInode(root.getId()).asDirectory();\n         Snapshot snapshot \u003d new Snapshot(sid, root, parent);\n         // add the snapshot to parent, since we follow the sequence of\n         // snapshotsByNames when saving, we do not need to sort when loading\n         parent.addSnapshot(snapshot);\n         snapshotMap.put(sid, snapshot);\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void loadSnapshots(InputStream in, int size) throws IOException {\n      for (int i \u003d 0; i \u003c size; i++) {\n        SnapshotSection.Snapshot pbs \u003d SnapshotSection.Snapshot\n            .parseDelimitedFrom(in);\n        INodeDirectory root \u003d loadINodeDirectory(pbs.getRoot(),\n            parent.getLoaderContext().getStringTable());\n        int sid \u003d pbs.getSnapshotId();\n        INodeDirectorySnapshottable parent \u003d (INodeDirectorySnapshottable) fsDir\n            .getInode(root.getId()).asDirectory();\n        Snapshot snapshot \u003d new Snapshot(sid, root, parent);\n        // add the snapshot to parent, since we follow the sequence of\n        // snapshotsByNames when saving, we do not need to sort when loading\n        parent.addSnapshot(snapshot);\n        snapshotMap.put(sid, snapshot);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java",
      "extendedDetails": {}
    },
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5698. Use protobuf to serialize / deserialize FSImage. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/02/14 11:18 AM",
      "commitName": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,16 @@\n+    private void loadSnapshots(InputStream in, int size) throws IOException {\n+      for (int i \u003d 0; i \u003c size; i++) {\n+        SnapshotSection.Snapshot pbs \u003d SnapshotSection.Snapshot\n+            .parseDelimitedFrom(in);\n+        INodeDirectory root \u003d loadINodeDirectory(pbs.getRoot(),\n+            parent.getStringTable());\n+        int sid \u003d pbs.getSnapshotId();\n+        INodeDirectorySnapshottable parent \u003d (INodeDirectorySnapshottable) fsDir\n+            .getInode(root.getId()).asDirectory();\n+        Snapshot snapshot \u003d new Snapshot(sid, root, parent);\n+        // add the snapshot to parent, since we follow the sequence of\n+        // snapshotsByNames when saving, we do not need to sort when loading\n+        parent.addSnapshot(snapshot);\n+        snapshotMap.put(sid, snapshot);\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void loadSnapshots(InputStream in, int size) throws IOException {\n      for (int i \u003d 0; i \u003c size; i++) {\n        SnapshotSection.Snapshot pbs \u003d SnapshotSection.Snapshot\n            .parseDelimitedFrom(in);\n        INodeDirectory root \u003d loadINodeDirectory(pbs.getRoot(),\n            parent.getStringTable());\n        int sid \u003d pbs.getSnapshotId();\n        INodeDirectorySnapshottable parent \u003d (INodeDirectorySnapshottable) fsDir\n            .getInode(root.getId()).asDirectory();\n        Snapshot snapshot \u003d new Snapshot(sid, root, parent);\n        // add the snapshot to parent, since we follow the sequence of\n        // snapshotsByNames when saving, we do not need to sort when loading\n        parent.addSnapshot(snapshot);\n        snapshotMap.put(sid, snapshot);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FSImageFormatPBSnapshot.java"
    }
  }
}