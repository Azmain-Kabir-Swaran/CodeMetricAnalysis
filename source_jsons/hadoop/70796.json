{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockBlobAppendStream.java",
  "functionName": "blockCompaction",
  "functionId": "blockCompaction",
  "sourceFilePath": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/BlockBlobAppendStream.java",
  "functionStartLine": 997,
  "functionEndLine": 1063,
  "numCommitsSeen": 7,
  "timeTaken": 1647,
  "changeHistory": [
    "13eda5000304099d1145631f9be13ce8a00b600d"
  ],
  "changeHistoryShort": {
    "13eda5000304099d1145631f9be13ce8a00b600d": "Yintroduced"
  },
  "changeHistoryDetails": {
    "13eda5000304099d1145631f9be13ce8a00b600d": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-14520. WASB: Block compaction for Azure Block Blobs.\nContributed by Georgi Chalakov\n",
      "commitDate": "07/09/17 10:35 AM",
      "commitName": "13eda5000304099d1145631f9be13ce8a00b600d",
      "commitAuthor": "Steve Loughran",
      "diff": "@@ -0,0 +1,67 @@\n+    private void blockCompaction() throws IOException {\n+      //current segment [segmentBegin, segmentEnd) and file offset/size of the\n+      // current segment\n+      int segmentBegin \u003d 0, segmentEnd \u003d 0;\n+      long segmentOffsetBegin \u003d 0, segmentOffsetEnd \u003d 0;\n+\n+      //longest segment [maxSegmentBegin, maxSegmentEnd) and file offset/size of\n+      // the longest segment\n+      int maxSegmentBegin \u003d 0, maxSegmentEnd \u003d 0;\n+      long maxSegmentOffsetBegin \u003d 0, maxSegmentOffsetEnd \u003d 0;\n+\n+      for (BlockEntry block : blockEntries) {\n+        segmentEnd++;\n+        segmentOffsetEnd +\u003d block.getSize();\n+        if (segmentOffsetEnd - segmentOffsetBegin \u003e maxBlockSize.get()) {\n+          if (segmentEnd - segmentBegin \u003e 2) {\n+            if (maxSegmentEnd - maxSegmentBegin \u003c segmentEnd - segmentBegin) {\n+              maxSegmentBegin \u003d segmentBegin;\n+              maxSegmentEnd \u003d segmentEnd;\n+              maxSegmentOffsetBegin \u003d segmentOffsetBegin;\n+              maxSegmentOffsetEnd \u003d segmentOffsetEnd - block.getSize();\n+            }\n+          }\n+          segmentBegin \u003d segmentEnd - 1;\n+          segmentOffsetBegin \u003d segmentOffsetEnd - block.getSize();\n+        }\n+      }\n+\n+      if (maxSegmentEnd - maxSegmentBegin \u003e 1) {\n+\n+        LOG.debug(\"Block compaction: {} blocks for {}\",\n+            maxSegmentEnd - maxSegmentBegin, key);\n+\n+        // download synchronously all the blocks from the azure storage\n+        ByteArrayOutputStreamInternal blockOutputStream\n+            \u003d new ByteArrayOutputStreamInternal(maxBlockSize.get());\n+\n+        try {\n+          long length \u003d maxSegmentOffsetEnd - maxSegmentOffsetBegin;\n+          blob.downloadRange(maxSegmentOffsetBegin, length, blockOutputStream,\n+              new BlobRequestOptions(), opContext);\n+        } catch(StorageException ex) {\n+          LOG.error(\n+              \"Storage exception encountered during block compaction phase\"\n+                  + \" : {} Storage Exception : {} Error Code: {}\",\n+              key, ex, ex.getErrorCode());\n+          throw new AzureException(\n+              \"Encountered Exception while committing append blocks \" + ex, ex);\n+        }\n+\n+        // upload synchronously new block to the azure storage\n+        String blockId \u003d generateBlockId();\n+\n+        ByteBuffer byteBuffer \u003d ByteBuffer.wrap(\n+            blockOutputStream.getByteArray());\n+        byteBuffer.position(blockOutputStream.size());\n+\n+        writeBlockRequestInternal(blockId, byteBuffer, false);\n+\n+        // replace blocks from the longest segment with new block id\n+        blockEntries.subList(maxSegmentBegin + 1, maxSegmentEnd - 1).clear();\n+        BlockEntry newBlock \u003d blockEntries.get(maxSegmentBegin);\n+        newBlock.setId(blockId);\n+        newBlock.setSearchMode(BlockSearchMode.LATEST);\n+        newBlock.setSize(maxSegmentOffsetEnd - maxSegmentOffsetBegin);\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void blockCompaction() throws IOException {\n      //current segment [segmentBegin, segmentEnd) and file offset/size of the\n      // current segment\n      int segmentBegin \u003d 0, segmentEnd \u003d 0;\n      long segmentOffsetBegin \u003d 0, segmentOffsetEnd \u003d 0;\n\n      //longest segment [maxSegmentBegin, maxSegmentEnd) and file offset/size of\n      // the longest segment\n      int maxSegmentBegin \u003d 0, maxSegmentEnd \u003d 0;\n      long maxSegmentOffsetBegin \u003d 0, maxSegmentOffsetEnd \u003d 0;\n\n      for (BlockEntry block : blockEntries) {\n        segmentEnd++;\n        segmentOffsetEnd +\u003d block.getSize();\n        if (segmentOffsetEnd - segmentOffsetBegin \u003e maxBlockSize.get()) {\n          if (segmentEnd - segmentBegin \u003e 2) {\n            if (maxSegmentEnd - maxSegmentBegin \u003c segmentEnd - segmentBegin) {\n              maxSegmentBegin \u003d segmentBegin;\n              maxSegmentEnd \u003d segmentEnd;\n              maxSegmentOffsetBegin \u003d segmentOffsetBegin;\n              maxSegmentOffsetEnd \u003d segmentOffsetEnd - block.getSize();\n            }\n          }\n          segmentBegin \u003d segmentEnd - 1;\n          segmentOffsetBegin \u003d segmentOffsetEnd - block.getSize();\n        }\n      }\n\n      if (maxSegmentEnd - maxSegmentBegin \u003e 1) {\n\n        LOG.debug(\"Block compaction: {} blocks for {}\",\n            maxSegmentEnd - maxSegmentBegin, key);\n\n        // download synchronously all the blocks from the azure storage\n        ByteArrayOutputStreamInternal blockOutputStream\n            \u003d new ByteArrayOutputStreamInternal(maxBlockSize.get());\n\n        try {\n          long length \u003d maxSegmentOffsetEnd - maxSegmentOffsetBegin;\n          blob.downloadRange(maxSegmentOffsetBegin, length, blockOutputStream,\n              new BlobRequestOptions(), opContext);\n        } catch(StorageException ex) {\n          LOG.error(\n              \"Storage exception encountered during block compaction phase\"\n                  + \" : {} Storage Exception : {} Error Code: {}\",\n              key, ex, ex.getErrorCode());\n          throw new AzureException(\n              \"Encountered Exception while committing append blocks \" + ex, ex);\n        }\n\n        // upload synchronously new block to the azure storage\n        String blockId \u003d generateBlockId();\n\n        ByteBuffer byteBuffer \u003d ByteBuffer.wrap(\n            blockOutputStream.getByteArray());\n        byteBuffer.position(blockOutputStream.size());\n\n        writeBlockRequestInternal(blockId, byteBuffer, false);\n\n        // replace blocks from the longest segment with new block id\n        blockEntries.subList(maxSegmentBegin + 1, maxSegmentEnd - 1).clear();\n        BlockEntry newBlock \u003d blockEntries.get(maxSegmentBegin);\n        newBlock.setId(blockId);\n        newBlock.setSearchMode(BlockSearchMode.LATEST);\n        newBlock.setSize(maxSegmentOffsetEnd - maxSegmentOffsetBegin);\n      }\n    }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/BlockBlobAppendStream.java"
    }
  }
}