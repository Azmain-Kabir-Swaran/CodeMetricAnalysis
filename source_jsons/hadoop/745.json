{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStreamer.java",
  "functionName": "nextBlockOutputStream",
  "functionId": "nextBlockOutputStream",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
  "functionStartLine": 1656,
  "functionEndLine": 1698,
  "numCommitsSeen": 198,
  "timeTaken": 11255,
  "changeHistory": [
    "a3954ccab148bddc290cb96528e63ff19799bcc9",
    "627da6f7178e18aa41996969c408b6f344e297d1",
    "16c07cc68a3e0a06f57b7f4c7207cc8e5dce211f",
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "8f378733423a5244461df79a92c00239514b8b93",
    "7947e5b53b9ac9524b535b0384c1c355b74723ff",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498",
    "552b4fb9f9a76b18605322c0b0e8072613d67773",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54",
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc",
    "440c3cd1050f2a871a73d44406c0013b6ff73f2e",
    "3e3fea7cd433a77ab2b92a5035ffc4bdea02c6cf",
    "631ccbdd2031a8387d4c2b743a4fc64c990391ce",
    "f2f5cdb5554d294a29ebf465101c5607fd56e244",
    "a10fbb93e759c3351851f5422b192c9bd04820a9",
    "cea7bbc630deede93dbe6a1bbda56ad49de4f3de",
    "4a5ba3b7bd2360fd9605863630b477d362874e1e",
    "bcdb125643d4ec834f6bd5d4fafb079391f31fc6",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "fd9997989c1f1c6f806c57a806e7225ca599fc0c",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "a3954ccab148bddc290cb96528e63ff19799bcc9": "Ybodychange",
    "627da6f7178e18aa41996969c408b6f344e297d1": "Ybodychange",
    "16c07cc68a3e0a06f57b7f4c7207cc8e5dce211f": "Ybodychange",
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "8f378733423a5244461df79a92c00239514b8b93": "Ybodychange",
    "7947e5b53b9ac9524b535b0384c1c355b74723ff": "Ybodychange",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ybodychange",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": "Ymultichange(Ymovefromfile,Ybodychange)",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": "Ybodychange",
    "552b4fb9f9a76b18605322c0b0e8072613d67773": "Ybodychange",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": "Ybodychange",
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc": "Ybodychange",
    "440c3cd1050f2a871a73d44406c0013b6ff73f2e": "Ybodychange",
    "3e3fea7cd433a77ab2b92a5035ffc4bdea02c6cf": "Ymultichange(Yreturntypechange,Ybodychange)",
    "631ccbdd2031a8387d4c2b743a4fc64c990391ce": "Yparameterchange",
    "f2f5cdb5554d294a29ebf465101c5607fd56e244": "Ybodychange",
    "a10fbb93e759c3351851f5422b192c9bd04820a9": "Ybodychange",
    "cea7bbc630deede93dbe6a1bbda56ad49de4f3de": "Ybodychange",
    "4a5ba3b7bd2360fd9605863630b477d362874e1e": "Ybodychange",
    "bcdb125643d4ec834f6bd5d4fafb079391f31fc6": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "fd9997989c1f1c6f806c57a806e7225ca599fc0c": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "a3954ccab148bddc290cb96528e63ff19799bcc9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
      "commitDate": "05/05/17 12:01 PM",
      "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "15/02/17 10:44 AM",
      "commitNameOld": "627da6f7178e18aa41996969c408b6f344e297d1",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 79.01,
      "commitsBetweenForRepo": 465,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,43 @@\n   protected LocatedBlock nextBlockOutputStream() throws IOException {\n     LocatedBlock lb;\n     DatanodeInfo[] nodes;\n-    StorageType[] storageTypes;\n+    StorageType[] nextStorageTypes;\n+    String[] nextStorageIDs;\n     int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n     boolean success;\n     final ExtendedBlock oldBlock \u003d block.getCurrentBlock();\n     do {\n       errorState.resetInternalError();\n       lastException.clear();\n \n       DatanodeInfo[] excluded \u003d getExcludedNodes();\n       lb \u003d locateFollowingBlock(\n           excluded.length \u003e 0 ? excluded : null, oldBlock);\n       block.setCurrentBlock(lb.getBlock());\n       block.setNumBytes(0);\n       bytesSent \u003d 0;\n       accessToken \u003d lb.getBlockToken();\n       nodes \u003d lb.getLocations();\n-      storageTypes \u003d lb.getStorageTypes();\n+      nextStorageTypes \u003d lb.getStorageTypes();\n+      nextStorageIDs \u003d lb.getStorageIDs();\n \n       // Connect to first DataNode in the list.\n-      success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n+      success \u003d createBlockOutputStream(nodes, nextStorageTypes, nextStorageIDs,\n+          0L, false);\n \n       if (!success) {\n         LOG.warn(\"Abandoning \" + block);\n         dfsClient.namenode.abandonBlock(block.getCurrentBlock(),\n             stat.getFileId(), src, dfsClient.clientName);\n         block.setCurrentBlock(null);\n         final DatanodeInfo badNode \u003d nodes[errorState.getBadNodeIndex()];\n         LOG.warn(\"Excluding datanode \" + badNode);\n         excludedNodes.put(badNode, badNode);\n       }\n     } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n     if (!success) {\n       throw new IOException(\"Unable to create new block.\");\n     }\n     return lb;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected LocatedBlock nextBlockOutputStream() throws IOException {\n    LocatedBlock lb;\n    DatanodeInfo[] nodes;\n    StorageType[] nextStorageTypes;\n    String[] nextStorageIDs;\n    int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n    boolean success;\n    final ExtendedBlock oldBlock \u003d block.getCurrentBlock();\n    do {\n      errorState.resetInternalError();\n      lastException.clear();\n\n      DatanodeInfo[] excluded \u003d getExcludedNodes();\n      lb \u003d locateFollowingBlock(\n          excluded.length \u003e 0 ? excluded : null, oldBlock);\n      block.setCurrentBlock(lb.getBlock());\n      block.setNumBytes(0);\n      bytesSent \u003d 0;\n      accessToken \u003d lb.getBlockToken();\n      nodes \u003d lb.getLocations();\n      nextStorageTypes \u003d lb.getStorageTypes();\n      nextStorageIDs \u003d lb.getStorageIDs();\n\n      // Connect to first DataNode in the list.\n      success \u003d createBlockOutputStream(nodes, nextStorageTypes, nextStorageIDs,\n          0L, false);\n\n      if (!success) {\n        LOG.warn(\"Abandoning \" + block);\n        dfsClient.namenode.abandonBlock(block.getCurrentBlock(),\n            stat.getFileId(), src, dfsClient.clientName);\n        block.setCurrentBlock(null);\n        final DatanodeInfo badNode \u003d nodes[errorState.getBadNodeIndex()];\n        LOG.warn(\"Excluding datanode \" + badNode);\n        excludedNodes.put(badNode, badNode);\n      }\n    } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n    if (!success) {\n      throw new IOException(\"Unable to create new block.\");\n    }\n    return lb;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "627da6f7178e18aa41996969c408b6f344e297d1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8498. Blocks can be committed with wrong size. Contributed by Jing Zhao.\n",
      "commitDate": "15/02/17 10:44 AM",
      "commitName": "627da6f7178e18aa41996969c408b6f344e297d1",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "02/02/17 10:08 AM",
      "commitNameOld": "0914fcca312b5e9d20bcf1b6633bc13c9034ba46",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 13.03,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,40 @@\n   protected LocatedBlock nextBlockOutputStream() throws IOException {\n     LocatedBlock lb;\n     DatanodeInfo[] nodes;\n     StorageType[] storageTypes;\n     int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n     boolean success;\n-    ExtendedBlock oldBlock \u003d block;\n+    final ExtendedBlock oldBlock \u003d block.getCurrentBlock();\n     do {\n       errorState.resetInternalError();\n       lastException.clear();\n \n       DatanodeInfo[] excluded \u003d getExcludedNodes();\n-      block \u003d oldBlock;\n-      lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n-      block \u003d lb.getBlock();\n+      lb \u003d locateFollowingBlock(\n+          excluded.length \u003e 0 ? excluded : null, oldBlock);\n+      block.setCurrentBlock(lb.getBlock());\n       block.setNumBytes(0);\n       bytesSent \u003d 0;\n       accessToken \u003d lb.getBlockToken();\n       nodes \u003d lb.getLocations();\n       storageTypes \u003d lb.getStorageTypes();\n \n-      //\n       // Connect to first DataNode in the list.\n-      //\n       success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n \n       if (!success) {\n         LOG.warn(\"Abandoning \" + block);\n-        dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n-            dfsClient.clientName);\n-        block \u003d null;\n+        dfsClient.namenode.abandonBlock(block.getCurrentBlock(),\n+            stat.getFileId(), src, dfsClient.clientName);\n+        block.setCurrentBlock(null);\n         final DatanodeInfo badNode \u003d nodes[errorState.getBadNodeIndex()];\n         LOG.warn(\"Excluding datanode \" + badNode);\n         excludedNodes.put(badNode, badNode);\n       }\n     } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n     if (!success) {\n       throw new IOException(\"Unable to create new block.\");\n     }\n     return lb;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected LocatedBlock nextBlockOutputStream() throws IOException {\n    LocatedBlock lb;\n    DatanodeInfo[] nodes;\n    StorageType[] storageTypes;\n    int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n    boolean success;\n    final ExtendedBlock oldBlock \u003d block.getCurrentBlock();\n    do {\n      errorState.resetInternalError();\n      lastException.clear();\n\n      DatanodeInfo[] excluded \u003d getExcludedNodes();\n      lb \u003d locateFollowingBlock(\n          excluded.length \u003e 0 ? excluded : null, oldBlock);\n      block.setCurrentBlock(lb.getBlock());\n      block.setNumBytes(0);\n      bytesSent \u003d 0;\n      accessToken \u003d lb.getBlockToken();\n      nodes \u003d lb.getLocations();\n      storageTypes \u003d lb.getStorageTypes();\n\n      // Connect to first DataNode in the list.\n      success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n\n      if (!success) {\n        LOG.warn(\"Abandoning \" + block);\n        dfsClient.namenode.abandonBlock(block.getCurrentBlock(),\n            stat.getFileId(), src, dfsClient.clientName);\n        block.setCurrentBlock(null);\n        final DatanodeInfo badNode \u003d nodes[errorState.getBadNodeIndex()];\n        LOG.warn(\"Excluding datanode \" + badNode);\n        excludedNodes.put(badNode, badNode);\n      }\n    } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n    if (!success) {\n      throw new IOException(\"Unable to create new block.\");\n    }\n    return lb;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "16c07cc68a3e0a06f57b7f4c7207cc8e5dce211f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10381, DataStreamer DataNode exclusion log message should be warning. (John Zhuge via Yongjun Zhang)\n",
      "commitDate": "17/05/16 3:57 PM",
      "commitName": "16c07cc68a3e0a06f57b7f4c7207cc8e5dce211f",
      "commitAuthor": "Yongjun Zhang",
      "commitDateOld": "17/05/16 6:53 AM",
      "commitNameOld": "4a5819dae2b0ca8f8b6d94ef464882d079d86593",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.38,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n   protected LocatedBlock nextBlockOutputStream() throws IOException {\n     LocatedBlock lb;\n     DatanodeInfo[] nodes;\n     StorageType[] storageTypes;\n     int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n     boolean success;\n     ExtendedBlock oldBlock \u003d block;\n     do {\n       errorState.resetInternalError();\n       lastException.clear();\n \n       DatanodeInfo[] excluded \u003d getExcludedNodes();\n       block \u003d oldBlock;\n       lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n       block \u003d lb.getBlock();\n       block.setNumBytes(0);\n       bytesSent \u003d 0;\n       accessToken \u003d lb.getBlockToken();\n       nodes \u003d lb.getLocations();\n       storageTypes \u003d lb.getStorageTypes();\n \n       //\n       // Connect to first DataNode in the list.\n       //\n       success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n \n       if (!success) {\n-        LOG.info(\"Abandoning \" + block);\n+        LOG.warn(\"Abandoning \" + block);\n         dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n             dfsClient.clientName);\n         block \u003d null;\n         final DatanodeInfo badNode \u003d nodes[errorState.getBadNodeIndex()];\n-        LOG.info(\"Excluding datanode \" + badNode);\n+        LOG.warn(\"Excluding datanode \" + badNode);\n         excludedNodes.put(badNode, badNode);\n       }\n     } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n     if (!success) {\n       throw new IOException(\"Unable to create new block.\");\n     }\n     return lb;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected LocatedBlock nextBlockOutputStream() throws IOException {\n    LocatedBlock lb;\n    DatanodeInfo[] nodes;\n    StorageType[] storageTypes;\n    int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n    boolean success;\n    ExtendedBlock oldBlock \u003d block;\n    do {\n      errorState.resetInternalError();\n      lastException.clear();\n\n      DatanodeInfo[] excluded \u003d getExcludedNodes();\n      block \u003d oldBlock;\n      lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n      block \u003d lb.getBlock();\n      block.setNumBytes(0);\n      bytesSent \u003d 0;\n      accessToken \u003d lb.getBlockToken();\n      nodes \u003d lb.getLocations();\n      storageTypes \u003d lb.getStorageTypes();\n\n      //\n      // Connect to first DataNode in the list.\n      //\n      success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n\n      if (!success) {\n        LOG.warn(\"Abandoning \" + block);\n        dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n            dfsClient.clientName);\n        block \u003d null;\n        final DatanodeInfo badNode \u003d nodes[errorState.getBadNodeIndex()];\n        LOG.warn(\"Excluding datanode \" + badNode);\n        excludedNodes.put(badNode, badNode);\n      }\n    } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n    if (!success) {\n      throw new IOException(\"Unable to create new block.\");\n    }\n    return lb;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9180. Update excluded DataNodes in DFSStripedOutputStream based on failures in data streamers. Contributed by Jing Zhao.\n",
      "commitDate": "06/10/15 10:56 AM",
      "commitName": "a8b4d0ff283a0af1075aaa94904d4c6e63a9a3dd",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "03/10/15 11:38 AM",
      "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 2.97,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,42 @@\n   protected LocatedBlock nextBlockOutputStream() throws IOException {\n     LocatedBlock lb;\n     DatanodeInfo[] nodes;\n     StorageType[] storageTypes;\n     int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n     boolean success;\n     ExtendedBlock oldBlock \u003d block;\n     do {\n       errorState.resetInternalError();\n       lastException.clear();\n \n-      DatanodeInfo[] excluded \u003d\n-          excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n-              .keySet()\n-              .toArray(new DatanodeInfo[0]);\n+      DatanodeInfo[] excluded \u003d getExcludedNodes();\n       block \u003d oldBlock;\n       lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n       block \u003d lb.getBlock();\n       block.setNumBytes(0);\n       bytesSent \u003d 0;\n       accessToken \u003d lb.getBlockToken();\n       nodes \u003d lb.getLocations();\n       storageTypes \u003d lb.getStorageTypes();\n \n       //\n       // Connect to first DataNode in the list.\n       //\n       success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n \n       if (!success) {\n         LOG.info(\"Abandoning \" + block);\n         dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n             dfsClient.clientName);\n         block \u003d null;\n         final DatanodeInfo badNode \u003d nodes[errorState.getBadNodeIndex()];\n         LOG.info(\"Excluding datanode \" + badNode);\n         excludedNodes.put(badNode, badNode);\n       }\n     } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n     if (!success) {\n       throw new IOException(\"Unable to create new block.\");\n     }\n     return lb;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected LocatedBlock nextBlockOutputStream() throws IOException {\n    LocatedBlock lb;\n    DatanodeInfo[] nodes;\n    StorageType[] storageTypes;\n    int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n    boolean success;\n    ExtendedBlock oldBlock \u003d block;\n    do {\n      errorState.resetInternalError();\n      lastException.clear();\n\n      DatanodeInfo[] excluded \u003d getExcludedNodes();\n      block \u003d oldBlock;\n      lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n      block \u003d lb.getBlock();\n      block.setNumBytes(0);\n      bytesSent \u003d 0;\n      accessToken \u003d lb.getBlockToken();\n      nodes \u003d lb.getLocations();\n      storageTypes \u003d lb.getStorageTypes();\n\n      //\n      // Connect to first DataNode in the list.\n      //\n      success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n\n      if (!success) {\n        LOG.info(\"Abandoning \" + block);\n        dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n            dfsClient.clientName);\n        block \u003d null;\n        final DatanodeInfo badNode \u003d nodes[errorState.getBadNodeIndex()];\n        LOG.info(\"Excluding datanode \" + badNode);\n        excludedNodes.put(badNode, badNode);\n      }\n    } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n    if (!success) {\n      throw new IOException(\"Unable to create new block.\");\n    }\n    return lb;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   protected LocatedBlock nextBlockOutputStream() throws IOException {\n-    LocatedBlock lb \u003d null;\n-    DatanodeInfo[] nodes \u003d null;\n-    StorageType[] storageTypes \u003d null;\n-    int count \u003d getNumBlockWriteRetry();\n-    boolean success \u003d false;\n+    LocatedBlock lb;\n+    DatanodeInfo[] nodes;\n+    StorageType[] storageTypes;\n+    int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n+    boolean success;\n     ExtendedBlock oldBlock \u003d block;\n     do {\n       errorState.resetInternalError();\n       lastException.clear();\n \n       DatanodeInfo[] excluded \u003d\n           excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n               .keySet()\n               .toArray(new DatanodeInfo[0]);\n       block \u003d oldBlock;\n       lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n       block \u003d lb.getBlock();\n       block.setNumBytes(0);\n       bytesSent \u003d 0;\n       accessToken \u003d lb.getBlockToken();\n       nodes \u003d lb.getLocations();\n       storageTypes \u003d lb.getStorageTypes();\n \n       //\n       // Connect to first DataNode in the list.\n       //\n       success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n \n       if (!success) {\n         LOG.info(\"Abandoning \" + block);\n         dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n             dfsClient.clientName);\n         block \u003d null;\n         final DatanodeInfo badNode \u003d nodes[errorState.getBadNodeIndex()];\n         LOG.info(\"Excluding datanode \" + badNode);\n         excludedNodes.put(badNode, badNode);\n       }\n     } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n     if (!success) {\n       throw new IOException(\"Unable to create new block.\");\n     }\n     return lb;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected LocatedBlock nextBlockOutputStream() throws IOException {\n    LocatedBlock lb;\n    DatanodeInfo[] nodes;\n    StorageType[] storageTypes;\n    int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n    boolean success;\n    ExtendedBlock oldBlock \u003d block;\n    do {\n      errorState.resetInternalError();\n      lastException.clear();\n\n      DatanodeInfo[] excluded \u003d\n          excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n              .keySet()\n              .toArray(new DatanodeInfo[0]);\n      block \u003d oldBlock;\n      lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n      block \u003d lb.getBlock();\n      block.setNumBytes(0);\n      bytesSent \u003d 0;\n      accessToken \u003d lb.getBlockToken();\n      nodes \u003d lb.getLocations();\n      storageTypes \u003d lb.getStorageTypes();\n\n      //\n      // Connect to first DataNode in the list.\n      //\n      success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n\n      if (!success) {\n        LOG.info(\"Abandoning \" + block);\n        dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n            dfsClient.clientName);\n        block \u003d null;\n        final DatanodeInfo badNode \u003d nodes[errorState.getBadNodeIndex()];\n        LOG.info(\"Excluding datanode \" + badNode);\n        excludedNodes.put(badNode, badNode);\n      }\n    } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n    if (!success) {\n      throw new IOException(\"Unable to create new block.\");\n    }\n    return lb;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private LocatedBlock nextBlockOutputStream() throws IOException {\n    LocatedBlock lb \u003d null;\n    DatanodeInfo[] nodes \u003d null;\n    StorageType[] storageTypes \u003d null;\n    int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n    boolean success \u003d false;\n    ExtendedBlock oldBlock \u003d block;\n    do {\n      errorState.reset();\n      lastException.clear();\n      success \u003d false;\n\n      DatanodeInfo[] excluded \u003d\n          excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n              .keySet()\n              .toArray(new DatanodeInfo[0]);\n      block \u003d oldBlock;\n      lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n      block \u003d lb.getBlock();\n      block.setNumBytes(0);\n      bytesSent \u003d 0;\n      accessToken \u003d lb.getBlockToken();\n      nodes \u003d lb.getLocations();\n      storageTypes \u003d lb.getStorageTypes();\n\n      //\n      // Connect to first DataNode in the list.\n      //\n      success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n\n      if (!success) {\n        LOG.info(\"Abandoning \" + block);\n        dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n            dfsClient.clientName);\n        block \u003d null;\n        final DatanodeInfo badNode \u003d nodes[errorState.getBadNodeIndex()];\n        LOG.info(\"Excluding datanode \" + badNode);\n        excludedNodes.put(badNode, badNode);\n      }\n    } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n    if (!success) {\n      throw new IOException(\"Unable to create new block.\");\n    }\n    return lb;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
      }
    },
    "8f378733423a5244461df79a92c00239514b8b93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8397. Refactor the error handling code in DataStreamer. Contributed by Tsz Wo Nicholas Sze.\n",
      "commitDate": "15/05/15 4:14 PM",
      "commitName": "8f378733423a5244461df79a92c00239514b8b93",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "08/05/15 12:11 AM",
      "commitNameOld": "730f9930a48259f34e48404aee51e8d641cc3d36",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 7.67,
      "commitsBetweenForRepo": 95,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,46 @@\n   private LocatedBlock nextBlockOutputStream() throws IOException {\n     LocatedBlock lb \u003d null;\n     DatanodeInfo[] nodes \u003d null;\n     StorageType[] storageTypes \u003d null;\n     int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n     boolean success \u003d false;\n     ExtendedBlock oldBlock \u003d block;\n     do {\n-      hasError \u003d false;\n+      errorState.reset();\n       lastException.clear();\n-      errorIndex \u003d -1;\n       success \u003d false;\n \n       DatanodeInfo[] excluded \u003d\n           excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n               .keySet()\n               .toArray(new DatanodeInfo[0]);\n       block \u003d oldBlock;\n       lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n       block \u003d lb.getBlock();\n       block.setNumBytes(0);\n       bytesSent \u003d 0;\n       accessToken \u003d lb.getBlockToken();\n       nodes \u003d lb.getLocations();\n       storageTypes \u003d lb.getStorageTypes();\n \n       //\n       // Connect to first DataNode in the list.\n       //\n       success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n \n       if (!success) {\n         LOG.info(\"Abandoning \" + block);\n         dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n             dfsClient.clientName);\n         block \u003d null;\n-        LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n-        excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n+        final DatanodeInfo badNode \u003d nodes[errorState.getBadNodeIndex()];\n+        LOG.info(\"Excluding datanode \" + badNode);\n+        excludedNodes.put(badNode, badNode);\n       }\n     } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n     if (!success) {\n       throw new IOException(\"Unable to create new block.\");\n     }\n     return lb;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private LocatedBlock nextBlockOutputStream() throws IOException {\n    LocatedBlock lb \u003d null;\n    DatanodeInfo[] nodes \u003d null;\n    StorageType[] storageTypes \u003d null;\n    int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n    boolean success \u003d false;\n    ExtendedBlock oldBlock \u003d block;\n    do {\n      errorState.reset();\n      lastException.clear();\n      success \u003d false;\n\n      DatanodeInfo[] excluded \u003d\n          excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n              .keySet()\n              .toArray(new DatanodeInfo[0]);\n      block \u003d oldBlock;\n      lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n      block \u003d lb.getBlock();\n      block.setNumBytes(0);\n      bytesSent \u003d 0;\n      accessToken \u003d lb.getBlockToken();\n      nodes \u003d lb.getLocations();\n      storageTypes \u003d lb.getStorageTypes();\n\n      //\n      // Connect to first DataNode in the list.\n      //\n      success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n\n      if (!success) {\n        LOG.info(\"Abandoning \" + block);\n        dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n            dfsClient.clientName);\n        block \u003d null;\n        final DatanodeInfo badNode \u003d nodes[errorState.getBadNodeIndex()];\n        LOG.info(\"Excluding datanode \" + badNode);\n        excludedNodes.put(badNode, badNode);\n      }\n    } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n    if (!success) {\n      throw new IOException(\"Unable to create new block.\");\n    }\n    return lb;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "7947e5b53b9ac9524b535b0384c1c355b74723ff": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8283. DataStreamer cleanup and some minor improvement. Contributed by Tsz Wo Nicholas Sze.\n",
      "commitDate": "29/04/15 10:41 AM",
      "commitName": "7947e5b53b9ac9524b535b0384c1c355b74723ff",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/04/15 12:21 AM",
      "commitNameOld": "c8d72907ff5a4cb9ce1effca8ad9b69689d11d1d",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 5.43,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,46 @@\n   private LocatedBlock nextBlockOutputStream() throws IOException {\n     LocatedBlock lb \u003d null;\n     DatanodeInfo[] nodes \u003d null;\n     StorageType[] storageTypes \u003d null;\n     int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n     boolean success \u003d false;\n     ExtendedBlock oldBlock \u003d block;\n     do {\n       hasError \u003d false;\n-      lastException.set(null);\n+      lastException.clear();\n       errorIndex \u003d -1;\n       success \u003d false;\n \n       DatanodeInfo[] excluded \u003d\n           excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n               .keySet()\n               .toArray(new DatanodeInfo[0]);\n       block \u003d oldBlock;\n       lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n       block \u003d lb.getBlock();\n       block.setNumBytes(0);\n       bytesSent \u003d 0;\n       accessToken \u003d lb.getBlockToken();\n       nodes \u003d lb.getLocations();\n       storageTypes \u003d lb.getStorageTypes();\n \n       //\n       // Connect to first DataNode in the list.\n       //\n       success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n \n       if (!success) {\n-        DFSClient.LOG.info(\"Abandoning \" + block);\n+        LOG.info(\"Abandoning \" + block);\n         dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n             dfsClient.clientName);\n         block \u003d null;\n-        DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n+        LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n         excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n       }\n     } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n     if (!success) {\n       throw new IOException(\"Unable to create new block.\");\n     }\n     return lb;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private LocatedBlock nextBlockOutputStream() throws IOException {\n    LocatedBlock lb \u003d null;\n    DatanodeInfo[] nodes \u003d null;\n    StorageType[] storageTypes \u003d null;\n    int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n    boolean success \u003d false;\n    ExtendedBlock oldBlock \u003d block;\n    do {\n      hasError \u003d false;\n      lastException.clear();\n      errorIndex \u003d -1;\n      success \u003d false;\n\n      DatanodeInfo[] excluded \u003d\n          excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n              .keySet()\n              .toArray(new DatanodeInfo[0]);\n      block \u003d oldBlock;\n      lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n      block \u003d lb.getBlock();\n      block.setNumBytes(0);\n      bytesSent \u003d 0;\n      accessToken \u003d lb.getBlockToken();\n      nodes \u003d lb.getLocations();\n      storageTypes \u003d lb.getStorageTypes();\n\n      //\n      // Connect to first DataNode in the list.\n      //\n      success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n\n      if (!success) {\n        LOG.info(\"Abandoning \" + block);\n        dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n            dfsClient.clientName);\n        block \u003d null;\n        LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n        excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n      }\n    } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n    if (!success) {\n      throw new IOException(\"Unable to create new block.\");\n    }\n    return lb;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "07/04/15 1:59 PM",
      "commitNameOld": "571a1ce9d037d99e7c9042bcb77ae7a2c4daf6d3",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 3.03,
      "commitsBetweenForRepo": 48,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,46 @@\n   private LocatedBlock nextBlockOutputStream() throws IOException {\n     LocatedBlock lb \u003d null;\n     DatanodeInfo[] nodes \u003d null;\n     StorageType[] storageTypes \u003d null;\n-    int count \u003d dfsClient.getConf().nBlockWriteRetry;\n+    int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n     boolean success \u003d false;\n     ExtendedBlock oldBlock \u003d block;\n     do {\n       hasError \u003d false;\n       lastException.set(null);\n       errorIndex \u003d -1;\n       success \u003d false;\n \n       DatanodeInfo[] excluded \u003d\n           excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n               .keySet()\n               .toArray(new DatanodeInfo[0]);\n       block \u003d oldBlock;\n       lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n       block \u003d lb.getBlock();\n       block.setNumBytes(0);\n       bytesSent \u003d 0;\n       accessToken \u003d lb.getBlockToken();\n       nodes \u003d lb.getLocations();\n       storageTypes \u003d lb.getStorageTypes();\n \n       //\n       // Connect to first DataNode in the list.\n       //\n       success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n \n       if (!success) {\n         DFSClient.LOG.info(\"Abandoning \" + block);\n         dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n             dfsClient.clientName);\n         block \u003d null;\n         DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n         excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n       }\n     } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n     if (!success) {\n       throw new IOException(\"Unable to create new block.\");\n     }\n     return lb;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private LocatedBlock nextBlockOutputStream() throws IOException {\n    LocatedBlock lb \u003d null;\n    DatanodeInfo[] nodes \u003d null;\n    StorageType[] storageTypes \u003d null;\n    int count \u003d dfsClient.getConf().getNumBlockWriteRetry();\n    boolean success \u003d false;\n    ExtendedBlock oldBlock \u003d block;\n    do {\n      hasError \u003d false;\n      lastException.set(null);\n      errorIndex \u003d -1;\n      success \u003d false;\n\n      DatanodeInfo[] excluded \u003d\n          excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n              .keySet()\n              .toArray(new DatanodeInfo[0]);\n      block \u003d oldBlock;\n      lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n      block \u003d lb.getBlock();\n      block.setNumBytes(0);\n      bytesSent \u003d 0;\n      accessToken \u003d lb.getBlockToken();\n      nodes \u003d lb.getLocations();\n      storageTypes \u003d lb.getStorageTypes();\n\n      //\n      // Connect to first DataNode in the list.\n      //\n      success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n\n      if (!success) {\n        DFSClient.LOG.info(\"Abandoning \" + block);\n        dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n            dfsClient.clientName);\n        block \u003d null;\n        DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n        excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n      }\n    } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n    if (!success) {\n      throw new IOException(\"Unable to create new block.\");\n    }\n    return lb;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
      "commitDate": "24/03/15 11:06 AM",
      "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "24/03/15 11:06 AM",
          "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/03/15 10:49 AM",
          "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,46 +1,46 @@\n-    private LocatedBlock nextBlockOutputStream() throws IOException {\n-      LocatedBlock lb \u003d null;\n-      DatanodeInfo[] nodes \u003d null;\n-      StorageType[] storageTypes \u003d null;\n-      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n-      boolean success \u003d false;\n-      ExtendedBlock oldBlock \u003d block;\n-      do {\n-        hasError \u003d false;\n-        lastException.set(null);\n-        errorIndex \u003d -1;\n-        success \u003d false;\n+  private LocatedBlock nextBlockOutputStream() throws IOException {\n+    LocatedBlock lb \u003d null;\n+    DatanodeInfo[] nodes \u003d null;\n+    StorageType[] storageTypes \u003d null;\n+    int count \u003d dfsClient.getConf().nBlockWriteRetry;\n+    boolean success \u003d false;\n+    ExtendedBlock oldBlock \u003d block;\n+    do {\n+      hasError \u003d false;\n+      lastException.set(null);\n+      errorIndex \u003d -1;\n+      success \u003d false;\n \n-        DatanodeInfo[] excluded \u003d\n-            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n-            .keySet()\n-            .toArray(new DatanodeInfo[0]);\n-        block \u003d oldBlock;\n-        lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n-        block \u003d lb.getBlock();\n-        block.setNumBytes(0);\n-        bytesSent \u003d 0;\n-        accessToken \u003d lb.getBlockToken();\n-        nodes \u003d lb.getLocations();\n-        storageTypes \u003d lb.getStorageTypes();\n+      DatanodeInfo[] excluded \u003d\n+          excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n+              .keySet()\n+              .toArray(new DatanodeInfo[0]);\n+      block \u003d oldBlock;\n+      lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n+      block \u003d lb.getBlock();\n+      block.setNumBytes(0);\n+      bytesSent \u003d 0;\n+      accessToken \u003d lb.getBlockToken();\n+      nodes \u003d lb.getLocations();\n+      storageTypes \u003d lb.getStorageTypes();\n \n-        //\n-        // Connect to first DataNode in the list.\n-        //\n-        success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n-\n-        if (!success) {\n-          DFSClient.LOG.info(\"Abandoning \" + block);\n-          dfsClient.namenode.abandonBlock(block, fileId, src,\n-              dfsClient.clientName);\n-          block \u003d null;\n-          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n-          excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n-        }\n-      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n+      //\n+      // Connect to first DataNode in the list.\n+      //\n+      success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n \n       if (!success) {\n-        throw new IOException(\"Unable to create new block.\");\n+        DFSClient.LOG.info(\"Abandoning \" + block);\n+        dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n+            dfsClient.clientName);\n+        block \u003d null;\n+        DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n+        excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n       }\n-      return lb;\n-    }\n\\ No newline at end of file\n+    } while (!success \u0026\u0026 --count \u003e\u003d 0);\n+\n+    if (!success) {\n+      throw new IOException(\"Unable to create new block.\");\n+    }\n+    return lb;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private LocatedBlock nextBlockOutputStream() throws IOException {\n    LocatedBlock lb \u003d null;\n    DatanodeInfo[] nodes \u003d null;\n    StorageType[] storageTypes \u003d null;\n    int count \u003d dfsClient.getConf().nBlockWriteRetry;\n    boolean success \u003d false;\n    ExtendedBlock oldBlock \u003d block;\n    do {\n      hasError \u003d false;\n      lastException.set(null);\n      errorIndex \u003d -1;\n      success \u003d false;\n\n      DatanodeInfo[] excluded \u003d\n          excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n              .keySet()\n              .toArray(new DatanodeInfo[0]);\n      block \u003d oldBlock;\n      lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n      block \u003d lb.getBlock();\n      block.setNumBytes(0);\n      bytesSent \u003d 0;\n      accessToken \u003d lb.getBlockToken();\n      nodes \u003d lb.getLocations();\n      storageTypes \u003d lb.getStorageTypes();\n\n      //\n      // Connect to first DataNode in the list.\n      //\n      success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n\n      if (!success) {\n        DFSClient.LOG.info(\"Abandoning \" + block);\n        dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n            dfsClient.clientName);\n        block \u003d null;\n        DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n        excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n      }\n    } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n    if (!success) {\n      throw new IOException(\"Unable to create new block.\");\n    }\n    return lb;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
            "oldMethodName": "nextBlockOutputStream",
            "newMethodName": "nextBlockOutputStream"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "24/03/15 11:06 AM",
          "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/03/15 10:49 AM",
          "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,46 +1,46 @@\n-    private LocatedBlock nextBlockOutputStream() throws IOException {\n-      LocatedBlock lb \u003d null;\n-      DatanodeInfo[] nodes \u003d null;\n-      StorageType[] storageTypes \u003d null;\n-      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n-      boolean success \u003d false;\n-      ExtendedBlock oldBlock \u003d block;\n-      do {\n-        hasError \u003d false;\n-        lastException.set(null);\n-        errorIndex \u003d -1;\n-        success \u003d false;\n+  private LocatedBlock nextBlockOutputStream() throws IOException {\n+    LocatedBlock lb \u003d null;\n+    DatanodeInfo[] nodes \u003d null;\n+    StorageType[] storageTypes \u003d null;\n+    int count \u003d dfsClient.getConf().nBlockWriteRetry;\n+    boolean success \u003d false;\n+    ExtendedBlock oldBlock \u003d block;\n+    do {\n+      hasError \u003d false;\n+      lastException.set(null);\n+      errorIndex \u003d -1;\n+      success \u003d false;\n \n-        DatanodeInfo[] excluded \u003d\n-            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n-            .keySet()\n-            .toArray(new DatanodeInfo[0]);\n-        block \u003d oldBlock;\n-        lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n-        block \u003d lb.getBlock();\n-        block.setNumBytes(0);\n-        bytesSent \u003d 0;\n-        accessToken \u003d lb.getBlockToken();\n-        nodes \u003d lb.getLocations();\n-        storageTypes \u003d lb.getStorageTypes();\n+      DatanodeInfo[] excluded \u003d\n+          excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n+              .keySet()\n+              .toArray(new DatanodeInfo[0]);\n+      block \u003d oldBlock;\n+      lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n+      block \u003d lb.getBlock();\n+      block.setNumBytes(0);\n+      bytesSent \u003d 0;\n+      accessToken \u003d lb.getBlockToken();\n+      nodes \u003d lb.getLocations();\n+      storageTypes \u003d lb.getStorageTypes();\n \n-        //\n-        // Connect to first DataNode in the list.\n-        //\n-        success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n-\n-        if (!success) {\n-          DFSClient.LOG.info(\"Abandoning \" + block);\n-          dfsClient.namenode.abandonBlock(block, fileId, src,\n-              dfsClient.clientName);\n-          block \u003d null;\n-          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n-          excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n-        }\n-      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n+      //\n+      // Connect to first DataNode in the list.\n+      //\n+      success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n \n       if (!success) {\n-        throw new IOException(\"Unable to create new block.\");\n+        DFSClient.LOG.info(\"Abandoning \" + block);\n+        dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n+            dfsClient.clientName);\n+        block \u003d null;\n+        DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n+        excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n       }\n-      return lb;\n-    }\n\\ No newline at end of file\n+    } while (!success \u0026\u0026 --count \u003e\u003d 0);\n+\n+    if (!success) {\n+      throw new IOException(\"Unable to create new block.\");\n+    }\n+    return lb;\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private LocatedBlock nextBlockOutputStream() throws IOException {\n    LocatedBlock lb \u003d null;\n    DatanodeInfo[] nodes \u003d null;\n    StorageType[] storageTypes \u003d null;\n    int count \u003d dfsClient.getConf().nBlockWriteRetry;\n    boolean success \u003d false;\n    ExtendedBlock oldBlock \u003d block;\n    do {\n      hasError \u003d false;\n      lastException.set(null);\n      errorIndex \u003d -1;\n      success \u003d false;\n\n      DatanodeInfo[] excluded \u003d\n          excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n              .keySet()\n              .toArray(new DatanodeInfo[0]);\n      block \u003d oldBlock;\n      lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n      block \u003d lb.getBlock();\n      block.setNumBytes(0);\n      bytesSent \u003d 0;\n      accessToken \u003d lb.getBlockToken();\n      nodes \u003d lb.getLocations();\n      storageTypes \u003d lb.getStorageTypes();\n\n      //\n      // Connect to first DataNode in the list.\n      //\n      success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n\n      if (!success) {\n        DFSClient.LOG.info(\"Abandoning \" + block);\n        dfsClient.namenode.abandonBlock(block, stat.getFileId(), src,\n            dfsClient.clientName);\n        block \u003d null;\n        DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n        excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n      }\n    } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n    if (!success) {\n      throw new IOException(\"Unable to create new block.\");\n    }\n    return lb;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {}
        }
      ]
    },
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6841. Use Time.monotonicNow() wherever applicable instead of Time.now(). Contributed by Vinayakumar B\n",
      "commitDate": "20/03/15 12:02 PM",
      "commitName": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "20/03/15 9:12 AM",
      "commitNameOld": "15612313f578a5115f8d03885e9b0c8c376ed56e",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 0.12,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,46 @@\n     private LocatedBlock nextBlockOutputStream() throws IOException {\n       LocatedBlock lb \u003d null;\n       DatanodeInfo[] nodes \u003d null;\n       StorageType[] storageTypes \u003d null;\n       int count \u003d dfsClient.getConf().nBlockWriteRetry;\n       boolean success \u003d false;\n       ExtendedBlock oldBlock \u003d block;\n       do {\n         hasError \u003d false;\n         lastException.set(null);\n         errorIndex \u003d -1;\n         success \u003d false;\n \n-        long startTime \u003d Time.now();\n         DatanodeInfo[] excluded \u003d\n             excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n             .keySet()\n             .toArray(new DatanodeInfo[0]);\n         block \u003d oldBlock;\n-        lb \u003d locateFollowingBlock(startTime,\n-            excluded.length \u003e 0 ? excluded : null);\n+        lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n         block \u003d lb.getBlock();\n         block.setNumBytes(0);\n         bytesSent \u003d 0;\n         accessToken \u003d lb.getBlockToken();\n         nodes \u003d lb.getLocations();\n         storageTypes \u003d lb.getStorageTypes();\n \n         //\n         // Connect to first DataNode in the list.\n         //\n         success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n \n         if (!success) {\n           DFSClient.LOG.info(\"Abandoning \" + block);\n           dfsClient.namenode.abandonBlock(block, fileId, src,\n               dfsClient.clientName);\n           block \u003d null;\n           DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n           excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n         }\n       } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n       if (!success) {\n         throw new IOException(\"Unable to create new block.\");\n       }\n       return lb;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private LocatedBlock nextBlockOutputStream() throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      StorageType[] storageTypes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      ExtendedBlock oldBlock \u003d block;\n      do {\n        hasError \u003d false;\n        lastException.set(null);\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        DatanodeInfo[] excluded \u003d\n            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n            .keySet()\n            .toArray(new DatanodeInfo[0]);\n        block \u003d oldBlock;\n        lb \u003d locateFollowingBlock(excluded.length \u003e 0 ? excluded : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        bytesSent \u003d 0;\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n        storageTypes \u003d lb.getStorageTypes();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning \" + block);\n          dfsClient.namenode.abandonBlock(block, fileId, src,\n              dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return lb;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "552b4fb9f9a76b18605322c0b0e8072613d67773": {
      "type": "Ybodychange",
      "commitMessage": "Merge from trunk to branch\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1612928 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/07/14 12:26 PM",
      "commitName": "552b4fb9f9a76b18605322c0b0e8072613d67773",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "15/07/14 2:10 PM",
      "commitNameOld": "56c0bd4d37ab13b6cbcf860eda852da603ab2f62",
      "commitAuthorOld": "",
      "daysBetweenCommits": 7.93,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,48 @@\n     private LocatedBlock nextBlockOutputStream() throws IOException {\n       LocatedBlock lb \u003d null;\n       DatanodeInfo[] nodes \u003d null;\n+      StorageType[] storageTypes \u003d null;\n       int count \u003d dfsClient.getConf().nBlockWriteRetry;\n       boolean success \u003d false;\n       ExtendedBlock oldBlock \u003d block;\n       do {\n         hasError \u003d false;\n         lastException.set(null);\n         errorIndex \u003d -1;\n         success \u003d false;\n \n         long startTime \u003d Time.now();\n         DatanodeInfo[] excluded \u003d\n             excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n             .keySet()\n             .toArray(new DatanodeInfo[0]);\n         block \u003d oldBlock;\n         lb \u003d locateFollowingBlock(startTime,\n             excluded.length \u003e 0 ? excluded : null);\n         block \u003d lb.getBlock();\n         block.setNumBytes(0);\n         bytesSent \u003d 0;\n         accessToken \u003d lb.getBlockToken();\n         nodes \u003d lb.getLocations();\n+        storageTypes \u003d lb.getStorageTypes();\n \n         //\n         // Connect to first DataNode in the list.\n         //\n-        success \u003d createBlockOutputStream(nodes, 0L, false);\n+        success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n \n         if (!success) {\n           DFSClient.LOG.info(\"Abandoning \" + block);\n           dfsClient.namenode.abandonBlock(block, fileId, src,\n               dfsClient.clientName);\n           block \u003d null;\n           DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n           excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n         }\n       } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n       if (!success) {\n         throw new IOException(\"Unable to create new block.\");\n       }\n       return lb;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private LocatedBlock nextBlockOutputStream() throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      StorageType[] storageTypes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      ExtendedBlock oldBlock \u003d block;\n      do {\n        hasError \u003d false;\n        lastException.set(null);\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d Time.now();\n        DatanodeInfo[] excluded \u003d\n            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n            .keySet()\n            .toArray(new DatanodeInfo[0]);\n        block \u003d oldBlock;\n        lb \u003d locateFollowingBlock(startTime,\n            excluded.length \u003e 0 ? excluded : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        bytesSent \u003d 0;\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n        storageTypes \u003d lb.getStorageTypes();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning \" + block);\n          dfsClient.namenode.abandonBlock(block, fileId, src,\n              dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return lb;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/14 12:41 AM",
      "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "14/07/14 11:10 AM",
      "commitNameOld": "3b54223c0f32d42a84436c670d80b791a8e9696d",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 7.56,
      "commitsBetweenForRepo": 68,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,48 @@\n     private LocatedBlock nextBlockOutputStream() throws IOException {\n       LocatedBlock lb \u003d null;\n       DatanodeInfo[] nodes \u003d null;\n+      StorageType[] storageTypes \u003d null;\n       int count \u003d dfsClient.getConf().nBlockWriteRetry;\n       boolean success \u003d false;\n       ExtendedBlock oldBlock \u003d block;\n       do {\n         hasError \u003d false;\n         lastException.set(null);\n         errorIndex \u003d -1;\n         success \u003d false;\n \n         long startTime \u003d Time.now();\n         DatanodeInfo[] excluded \u003d\n             excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n             .keySet()\n             .toArray(new DatanodeInfo[0]);\n         block \u003d oldBlock;\n         lb \u003d locateFollowingBlock(startTime,\n             excluded.length \u003e 0 ? excluded : null);\n         block \u003d lb.getBlock();\n         block.setNumBytes(0);\n         bytesSent \u003d 0;\n         accessToken \u003d lb.getBlockToken();\n         nodes \u003d lb.getLocations();\n+        storageTypes \u003d lb.getStorageTypes();\n \n         //\n         // Connect to first DataNode in the list.\n         //\n-        success \u003d createBlockOutputStream(nodes, 0L, false);\n+        success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n \n         if (!success) {\n           DFSClient.LOG.info(\"Abandoning \" + block);\n           dfsClient.namenode.abandonBlock(block, fileId, src,\n               dfsClient.clientName);\n           block \u003d null;\n           DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n           excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n         }\n       } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n       if (!success) {\n         throw new IOException(\"Unable to create new block.\");\n       }\n       return lb;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private LocatedBlock nextBlockOutputStream() throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      StorageType[] storageTypes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      ExtendedBlock oldBlock \u003d block;\n      do {\n        hasError \u003d false;\n        lastException.set(null);\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d Time.now();\n        DatanodeInfo[] excluded \u003d\n            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n            .keySet()\n            .toArray(new DatanodeInfo[0]);\n        block \u003d oldBlock;\n        lb \u003d locateFollowingBlock(startTime,\n            excluded.length \u003e 0 ? excluded : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        bytesSent \u003d 0;\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n        storageTypes \u003d lb.getStorageTypes();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, storageTypes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning \" + block);\n          dfsClient.namenode.abandonBlock(block, fileId, src,\n              dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return lb;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6294. Use INode IDs to avoid conflicts when a file open for write is renamed (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1593634 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/05/14 3:36 PM",
      "commitName": "f131dba8a3d603a5d15c4f035ed3da75b4daf0dc",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "25/03/14 9:11 PM",
      "commitNameOld": "1fbb04e367d7c330e6052207f9f11911f4f5f368",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 44.77,
      "commitsBetweenForRepo": 257,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,46 @@\n     private LocatedBlock nextBlockOutputStream() throws IOException {\n       LocatedBlock lb \u003d null;\n       DatanodeInfo[] nodes \u003d null;\n       int count \u003d dfsClient.getConf().nBlockWriteRetry;\n       boolean success \u003d false;\n       ExtendedBlock oldBlock \u003d block;\n       do {\n         hasError \u003d false;\n         lastException.set(null);\n         errorIndex \u003d -1;\n         success \u003d false;\n \n         long startTime \u003d Time.now();\n         DatanodeInfo[] excluded \u003d\n             excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n             .keySet()\n             .toArray(new DatanodeInfo[0]);\n         block \u003d oldBlock;\n         lb \u003d locateFollowingBlock(startTime,\n             excluded.length \u003e 0 ? excluded : null);\n         block \u003d lb.getBlock();\n         block.setNumBytes(0);\n         bytesSent \u003d 0;\n         accessToken \u003d lb.getBlockToken();\n         nodes \u003d lb.getLocations();\n \n         //\n         // Connect to first DataNode in the list.\n         //\n         success \u003d createBlockOutputStream(nodes, 0L, false);\n \n         if (!success) {\n           DFSClient.LOG.info(\"Abandoning \" + block);\n-          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n+          dfsClient.namenode.abandonBlock(block, fileId, src,\n+              dfsClient.clientName);\n           block \u003d null;\n           DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n           excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n         }\n       } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n       if (!success) {\n         throw new IOException(\"Unable to create new block.\");\n       }\n       return lb;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private LocatedBlock nextBlockOutputStream() throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      ExtendedBlock oldBlock \u003d block;\n      do {\n        hasError \u003d false;\n        lastException.set(null);\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d Time.now();\n        DatanodeInfo[] excluded \u003d\n            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n            .keySet()\n            .toArray(new DatanodeInfo[0]);\n        block \u003d oldBlock;\n        lb \u003d locateFollowingBlock(startTime,\n            excluded.length \u003e 0 ? excluded : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        bytesSent \u003d 0;\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning \" + block);\n          dfsClient.namenode.abandonBlock(block, fileId, src,\n              dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return lb;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "440c3cd1050f2a871a73d44406c0013b6ff73f2e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5922. DN heartbeat thread can get stuck in tight loop. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1571542 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/02/14 6:16 PM",
      "commitName": "440c3cd1050f2a871a73d44406c0013b6ff73f2e",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "18/12/13 3:29 PM",
      "commitNameOld": "90122f25e142ff5ae9e2610b6b8968ac5fee8f79",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 68.12,
      "commitsBetweenForRepo": 429,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,45 @@\n     private LocatedBlock nextBlockOutputStream() throws IOException {\n       LocatedBlock lb \u003d null;\n       DatanodeInfo[] nodes \u003d null;\n       int count \u003d dfsClient.getConf().nBlockWriteRetry;\n       boolean success \u003d false;\n       ExtendedBlock oldBlock \u003d block;\n       do {\n         hasError \u003d false;\n         lastException.set(null);\n         errorIndex \u003d -1;\n         success \u003d false;\n \n         long startTime \u003d Time.now();\n         DatanodeInfo[] excluded \u003d\n             excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n             .keySet()\n             .toArray(new DatanodeInfo[0]);\n         block \u003d oldBlock;\n         lb \u003d locateFollowingBlock(startTime,\n             excluded.length \u003e 0 ? excluded : null);\n         block \u003d lb.getBlock();\n         block.setNumBytes(0);\n+        bytesSent \u003d 0;\n         accessToken \u003d lb.getBlockToken();\n         nodes \u003d lb.getLocations();\n \n         //\n         // Connect to first DataNode in the list.\n         //\n         success \u003d createBlockOutputStream(nodes, 0L, false);\n \n         if (!success) {\n           DFSClient.LOG.info(\"Abandoning \" + block);\n           dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n           block \u003d null;\n           DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n           excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n         }\n       } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n       if (!success) {\n         throw new IOException(\"Unable to create new block.\");\n       }\n       return lb;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private LocatedBlock nextBlockOutputStream() throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      ExtendedBlock oldBlock \u003d block;\n      do {\n        hasError \u003d false;\n        lastException.set(null);\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d Time.now();\n        DatanodeInfo[] excluded \u003d\n            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n            .keySet()\n            .toArray(new DatanodeInfo[0]);\n        block \u003d oldBlock;\n        lb \u003d locateFollowingBlock(startTime,\n            excluded.length \u003e 0 ? excluded : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        bytesSent \u003d 0;\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return lb;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "3e3fea7cd433a77ab2b92a5035ffc4bdea02c6cf": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-5466. Update storage IDs when the pipeline is updated. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1539203 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/11/13 4:25 PM",
      "commitName": "3e3fea7cd433a77ab2b92a5035ffc4bdea02c6cf",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-5466. Update storage IDs when the pipeline is updated. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1539203 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "05/11/13 4:25 PM",
          "commitName": "3e3fea7cd433a77ab2b92a5035ffc4bdea02c6cf",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "28/10/13 10:29 AM",
          "commitNameOld": "dc0b44a884700cda3665aa04b16d1e3474328e05",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 8.29,
          "commitsBetweenForRepo": 40,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,44 @@\n-    private DatanodeInfo[] nextBlockOutputStream() throws IOException {\n+    private LocatedBlock nextBlockOutputStream() throws IOException {\n       LocatedBlock lb \u003d null;\n       DatanodeInfo[] nodes \u003d null;\n       int count \u003d dfsClient.getConf().nBlockWriteRetry;\n       boolean success \u003d false;\n       ExtendedBlock oldBlock \u003d block;\n       do {\n         hasError \u003d false;\n         lastException.set(null);\n         errorIndex \u003d -1;\n         success \u003d false;\n \n         long startTime \u003d Time.now();\n         DatanodeInfo[] excluded \u003d\n             excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n             .keySet()\n             .toArray(new DatanodeInfo[0]);\n         block \u003d oldBlock;\n         lb \u003d locateFollowingBlock(startTime,\n             excluded.length \u003e 0 ? excluded : null);\n         block \u003d lb.getBlock();\n         block.setNumBytes(0);\n         accessToken \u003d lb.getBlockToken();\n         nodes \u003d lb.getLocations();\n \n         //\n         // Connect to first DataNode in the list.\n         //\n         success \u003d createBlockOutputStream(nodes, 0L, false);\n \n         if (!success) {\n           DFSClient.LOG.info(\"Abandoning \" + block);\n           dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n           block \u003d null;\n           DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n           excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n         }\n       } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n       if (!success) {\n         throw new IOException(\"Unable to create new block.\");\n       }\n-      return nodes;\n+      return lb;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private LocatedBlock nextBlockOutputStream() throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      ExtendedBlock oldBlock \u003d block;\n      do {\n        hasError \u003d false;\n        lastException.set(null);\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d Time.now();\n        DatanodeInfo[] excluded \u003d\n            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n            .keySet()\n            .toArray(new DatanodeInfo[0]);\n        block \u003d oldBlock;\n        lb \u003d locateFollowingBlock(startTime,\n            excluded.length \u003e 0 ? excluded : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return lb;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {
            "oldValue": "DatanodeInfo[]",
            "newValue": "LocatedBlock"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5466. Update storage IDs when the pipeline is updated. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1539203 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "05/11/13 4:25 PM",
          "commitName": "3e3fea7cd433a77ab2b92a5035ffc4bdea02c6cf",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "28/10/13 10:29 AM",
          "commitNameOld": "dc0b44a884700cda3665aa04b16d1e3474328e05",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 8.29,
          "commitsBetweenForRepo": 40,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,44 @@\n-    private DatanodeInfo[] nextBlockOutputStream() throws IOException {\n+    private LocatedBlock nextBlockOutputStream() throws IOException {\n       LocatedBlock lb \u003d null;\n       DatanodeInfo[] nodes \u003d null;\n       int count \u003d dfsClient.getConf().nBlockWriteRetry;\n       boolean success \u003d false;\n       ExtendedBlock oldBlock \u003d block;\n       do {\n         hasError \u003d false;\n         lastException.set(null);\n         errorIndex \u003d -1;\n         success \u003d false;\n \n         long startTime \u003d Time.now();\n         DatanodeInfo[] excluded \u003d\n             excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n             .keySet()\n             .toArray(new DatanodeInfo[0]);\n         block \u003d oldBlock;\n         lb \u003d locateFollowingBlock(startTime,\n             excluded.length \u003e 0 ? excluded : null);\n         block \u003d lb.getBlock();\n         block.setNumBytes(0);\n         accessToken \u003d lb.getBlockToken();\n         nodes \u003d lb.getLocations();\n \n         //\n         // Connect to first DataNode in the list.\n         //\n         success \u003d createBlockOutputStream(nodes, 0L, false);\n \n         if (!success) {\n           DFSClient.LOG.info(\"Abandoning \" + block);\n           dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n           block \u003d null;\n           DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n           excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n         }\n       } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n       if (!success) {\n         throw new IOException(\"Unable to create new block.\");\n       }\n-      return nodes;\n+      return lb;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private LocatedBlock nextBlockOutputStream() throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      ExtendedBlock oldBlock \u003d block;\n      do {\n        hasError \u003d false;\n        lastException.set(null);\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d Time.now();\n        DatanodeInfo[] excluded \u003d\n            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n            .keySet()\n            .toArray(new DatanodeInfo[0]);\n        block \u003d oldBlock;\n        lb \u003d locateFollowingBlock(startTime,\n            excluded.length \u003e 0 ? excluded : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return lb;\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "631ccbdd2031a8387d4c2b743a4fc64c990391ce": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-5374. Remove deadcode in DFSOutputStream. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1533258 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/10/13 1:47 PM",
      "commitName": "631ccbdd2031a8387d4c2b743a4fc64c990391ce",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "10/10/13 4:58 PM",
      "commitNameOld": "f2f5cdb5554d294a29ebf465101c5607fd56e244",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 6.87,
      "commitsBetweenForRepo": 49,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,44 @@\n-    private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n+    private DatanodeInfo[] nextBlockOutputStream() throws IOException {\n       LocatedBlock lb \u003d null;\n       DatanodeInfo[] nodes \u003d null;\n       int count \u003d dfsClient.getConf().nBlockWriteRetry;\n       boolean success \u003d false;\n       ExtendedBlock oldBlock \u003d block;\n       do {\n         hasError \u003d false;\n         lastException.set(null);\n         errorIndex \u003d -1;\n         success \u003d false;\n \n         long startTime \u003d Time.now();\n         DatanodeInfo[] excluded \u003d\n             excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n             .keySet()\n             .toArray(new DatanodeInfo[0]);\n         block \u003d oldBlock;\n         lb \u003d locateFollowingBlock(startTime,\n             excluded.length \u003e 0 ? excluded : null);\n         block \u003d lb.getBlock();\n         block.setNumBytes(0);\n         accessToken \u003d lb.getBlockToken();\n         nodes \u003d lb.getLocations();\n \n         //\n         // Connect to first DataNode in the list.\n         //\n         success \u003d createBlockOutputStream(nodes, 0L, false);\n \n         if (!success) {\n           DFSClient.LOG.info(\"Abandoning \" + block);\n           dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n           block \u003d null;\n           DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n           excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n         }\n       } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n       if (!success) {\n         throw new IOException(\"Unable to create new block.\");\n       }\n       return nodes;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private DatanodeInfo[] nextBlockOutputStream() throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      ExtendedBlock oldBlock \u003d block;\n      do {\n        hasError \u003d false;\n        lastException.set(null);\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d Time.now();\n        DatanodeInfo[] excluded \u003d\n            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n            .keySet()\n            .toArray(new DatanodeInfo[0]);\n        block \u003d oldBlock;\n        lb \u003d locateFollowingBlock(startTime,\n            excluded.length \u003e 0 ? excluded : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return nodes;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldValue": "[client-String]",
        "newValue": "[]"
      }
    },
    "f2f5cdb5554d294a29ebf465101c5607fd56e244": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5335. Hive query failed with possible race in dfs output stream. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1531152 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/10/13 4:58 PM",
      "commitName": "f2f5cdb5554d294a29ebf465101c5607fd56e244",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "22/07/13 11:15 AM",
      "commitNameOld": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 80.24,
      "commitsBetweenForRepo": 499,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,44 @@\n     private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n       LocatedBlock lb \u003d null;\n       DatanodeInfo[] nodes \u003d null;\n       int count \u003d dfsClient.getConf().nBlockWriteRetry;\n       boolean success \u003d false;\n       ExtendedBlock oldBlock \u003d block;\n       do {\n         hasError \u003d false;\n-        lastException \u003d null;\n+        lastException.set(null);\n         errorIndex \u003d -1;\n         success \u003d false;\n \n         long startTime \u003d Time.now();\n         DatanodeInfo[] excluded \u003d\n             excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n             .keySet()\n             .toArray(new DatanodeInfo[0]);\n         block \u003d oldBlock;\n         lb \u003d locateFollowingBlock(startTime,\n             excluded.length \u003e 0 ? excluded : null);\n         block \u003d lb.getBlock();\n         block.setNumBytes(0);\n         accessToken \u003d lb.getBlockToken();\n         nodes \u003d lb.getLocations();\n \n         //\n         // Connect to first DataNode in the list.\n         //\n         success \u003d createBlockOutputStream(nodes, 0L, false);\n \n         if (!success) {\n           DFSClient.LOG.info(\"Abandoning \" + block);\n           dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n           block \u003d null;\n           DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n           excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n         }\n       } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n       if (!success) {\n         throw new IOException(\"Unable to create new block.\");\n       }\n       return nodes;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      ExtendedBlock oldBlock \u003d block;\n      do {\n        hasError \u003d false;\n        lastException.set(null);\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d Time.now();\n        DatanodeInfo[] excluded \u003d\n            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n            .keySet()\n            .toArray(new DatanodeInfo[0]);\n        block \u003d oldBlock;\n        lb \u003d locateFollowingBlock(startTime,\n            excluded.length \u003e 0 ? excluded : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return nodes;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "a10fbb93e759c3351851f5422b192c9bd04820a9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4246. The exclude node list should be more forgiving, for each output stream. Contributed by Harsh J. Chouraria.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1459475 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/03/13 11:58 AM",
      "commitName": "a10fbb93e759c3351851f5422b192c9bd04820a9",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "06/02/13 11:52 AM",
      "commitNameOld": "4525c4a25ba90163c9543116e2bd54239e0dd097",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 42.96,
      "commitsBetweenForRepo": 178,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,44 @@\n     private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n       LocatedBlock lb \u003d null;\n       DatanodeInfo[] nodes \u003d null;\n       int count \u003d dfsClient.getConf().nBlockWriteRetry;\n       boolean success \u003d false;\n       ExtendedBlock oldBlock \u003d block;\n       do {\n         hasError \u003d false;\n         lastException \u003d null;\n         errorIndex \u003d -1;\n         success \u003d false;\n \n         long startTime \u003d Time.now();\n-        DatanodeInfo[] excluded \u003d excludedNodes.toArray(\n-            new DatanodeInfo[excludedNodes.size()]);\n+        DatanodeInfo[] excluded \u003d\n+            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n+            .keySet()\n+            .toArray(new DatanodeInfo[0]);\n         block \u003d oldBlock;\n         lb \u003d locateFollowingBlock(startTime,\n             excluded.length \u003e 0 ? excluded : null);\n         block \u003d lb.getBlock();\n         block.setNumBytes(0);\n         accessToken \u003d lb.getBlockToken();\n         nodes \u003d lb.getLocations();\n \n         //\n         // Connect to first DataNode in the list.\n         //\n         success \u003d createBlockOutputStream(nodes, 0L, false);\n \n         if (!success) {\n           DFSClient.LOG.info(\"Abandoning \" + block);\n           dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n           block \u003d null;\n           DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n-          excludedNodes.add(nodes[errorIndex]);\n+          excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n         }\n       } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n       if (!success) {\n         throw new IOException(\"Unable to create new block.\");\n       }\n       return nodes;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      ExtendedBlock oldBlock \u003d block;\n      do {\n        hasError \u003d false;\n        lastException \u003d null;\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d Time.now();\n        DatanodeInfo[] excluded \u003d\n            excludedNodes.getAllPresent(excludedNodes.asMap().keySet())\n            .keySet()\n            .toArray(new DatanodeInfo[0]);\n        block \u003d oldBlock;\n        lb \u003d locateFollowingBlock(startTime,\n            excluded.length \u003e 0 ? excluded : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.put(nodes[errorIndex], nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return nodes;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cea7bbc630deede93dbe6a1bbda56ad49de4f3de": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4122. Cleanup HDFS logs and reduce the size of logged messages. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1403120 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/10/12 4:10 PM",
      "commitName": "cea7bbc630deede93dbe6a1bbda56ad49de4f3de",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "25/08/12 9:00 PM",
      "commitNameOld": "735046ebecd9e803398be56fbf79dbde5226b4c1",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 63.8,
      "commitsBetweenForRepo": 381,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n     private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n       LocatedBlock lb \u003d null;\n       DatanodeInfo[] nodes \u003d null;\n       int count \u003d dfsClient.getConf().nBlockWriteRetry;\n       boolean success \u003d false;\n       ExtendedBlock oldBlock \u003d block;\n       do {\n         hasError \u003d false;\n         lastException \u003d null;\n         errorIndex \u003d -1;\n         success \u003d false;\n \n         long startTime \u003d Time.now();\n         DatanodeInfo[] excluded \u003d excludedNodes.toArray(\n             new DatanodeInfo[excludedNodes.size()]);\n         block \u003d oldBlock;\n         lb \u003d locateFollowingBlock(startTime,\n             excluded.length \u003e 0 ? excluded : null);\n         block \u003d lb.getBlock();\n         block.setNumBytes(0);\n         accessToken \u003d lb.getBlockToken();\n         nodes \u003d lb.getLocations();\n \n         //\n         // Connect to first DataNode in the list.\n         //\n         success \u003d createBlockOutputStream(nodes, 0L, false);\n \n         if (!success) {\n-          DFSClient.LOG.info(\"Abandoning block \" + block);\n+          DFSClient.LOG.info(\"Abandoning \" + block);\n           dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n           block \u003d null;\n           DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n           excludedNodes.add(nodes[errorIndex]);\n         }\n       } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n       if (!success) {\n         throw new IOException(\"Unable to create new block.\");\n       }\n       return nodes;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      ExtendedBlock oldBlock \u003d block;\n      do {\n        hasError \u003d false;\n        lastException \u003d null;\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d Time.now();\n        DatanodeInfo[] excluded \u003d excludedNodes.toArray(\n            new DatanodeInfo[excludedNodes.size()]);\n        block \u003d oldBlock;\n        lb \u003d locateFollowingBlock(startTime,\n            excluded.length \u003e 0 ? excluded : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.add(nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return nodes;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "4a5ba3b7bd2360fd9605863630b477d362874e1e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3641. Move server Util time methods to common and use now instead of System#currentTimeMillis. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1360858 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/07/12 12:01 PM",
      "commitName": "4a5ba3b7bd2360fd9605863630b477d362874e1e",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "30/05/12 12:10 PM",
      "commitNameOld": "83cf475050dba27e72b4e399491638c670621175",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 42.99,
      "commitsBetweenForRepo": 208,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n     private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n       LocatedBlock lb \u003d null;\n       DatanodeInfo[] nodes \u003d null;\n       int count \u003d dfsClient.getConf().nBlockWriteRetry;\n       boolean success \u003d false;\n       ExtendedBlock oldBlock \u003d block;\n       do {\n         hasError \u003d false;\n         lastException \u003d null;\n         errorIndex \u003d -1;\n         success \u003d false;\n \n-        long startTime \u003d System.currentTimeMillis();\n+        long startTime \u003d Time.now();\n         DatanodeInfo[] excluded \u003d excludedNodes.toArray(\n             new DatanodeInfo[excludedNodes.size()]);\n         block \u003d oldBlock;\n         lb \u003d locateFollowingBlock(startTime,\n             excluded.length \u003e 0 ? excluded : null);\n         block \u003d lb.getBlock();\n         block.setNumBytes(0);\n         accessToken \u003d lb.getBlockToken();\n         nodes \u003d lb.getLocations();\n \n         //\n         // Connect to first DataNode in the list.\n         //\n         success \u003d createBlockOutputStream(nodes, 0L, false);\n \n         if (!success) {\n           DFSClient.LOG.info(\"Abandoning block \" + block);\n           dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n           block \u003d null;\n           DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n           excludedNodes.add(nodes[errorIndex]);\n         }\n       } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n       if (!success) {\n         throw new IOException(\"Unable to create new block.\");\n       }\n       return nodes;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      ExtendedBlock oldBlock \u003d block;\n      do {\n        hasError \u003d false;\n        lastException \u003d null;\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d Time.now();\n        DatanodeInfo[] excluded \u003d excludedNodes.toArray(\n            new DatanodeInfo[excludedNodes.size()]);\n        block \u003d oldBlock;\n        lb \u003d locateFollowingBlock(startTime,\n            excluded.length \u003e 0 ? excluded : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning block \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.add(nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return nodes;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "bcdb125643d4ec834f6bd5d4fafb079391f31fc6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3031. Fix complete() and getAdditionalBlock() RPCs to be idempotent. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1338466 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/05/12 3:35 PM",
      "commitName": "bcdb125643d4ec834f6bd5d4fafb079391f31fc6",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "26/04/12 2:50 PM",
      "commitNameOld": "1a76c82a31958aeb549b544fe81960a59b2a9d0b",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 18.03,
      "commitsBetweenForRepo": 111,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,42 @@\n     private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n       LocatedBlock lb \u003d null;\n       DatanodeInfo[] nodes \u003d null;\n       int count \u003d dfsClient.getConf().nBlockWriteRetry;\n       boolean success \u003d false;\n+      ExtendedBlock oldBlock \u003d block;\n       do {\n         hasError \u003d false;\n         lastException \u003d null;\n         errorIndex \u003d -1;\n         success \u003d false;\n \n         long startTime \u003d System.currentTimeMillis();\n-        DatanodeInfo[] w \u003d excludedNodes.toArray(\n+        DatanodeInfo[] excluded \u003d excludedNodes.toArray(\n             new DatanodeInfo[excludedNodes.size()]);\n-        lb \u003d locateFollowingBlock(startTime, w.length \u003e 0 ? w : null);\n+        block \u003d oldBlock;\n+        lb \u003d locateFollowingBlock(startTime,\n+            excluded.length \u003e 0 ? excluded : null);\n         block \u003d lb.getBlock();\n         block.setNumBytes(0);\n         accessToken \u003d lb.getBlockToken();\n         nodes \u003d lb.getLocations();\n \n         //\n         // Connect to first DataNode in the list.\n         //\n         success \u003d createBlockOutputStream(nodes, 0L, false);\n \n         if (!success) {\n           DFSClient.LOG.info(\"Abandoning block \" + block);\n           dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n           block \u003d null;\n           DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n           excludedNodes.add(nodes[errorIndex]);\n         }\n       } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n       if (!success) {\n         throw new IOException(\"Unable to create new block.\");\n       }\n       return nodes;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      ExtendedBlock oldBlock \u003d block;\n      do {\n        hasError \u003d false;\n        lastException \u003d null;\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d System.currentTimeMillis();\n        DatanodeInfo[] excluded \u003d excludedNodes.toArray(\n            new DatanodeInfo[excludedNodes.size()]);\n        block \u003d oldBlock;\n        lb \u003d locateFollowingBlock(startTime,\n            excluded.length \u003e 0 ? excluded : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning block \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.add(nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return nodes;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      do {\n        hasError \u003d false;\n        lastException \u003d null;\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d System.currentTimeMillis();\n        DatanodeInfo[] w \u003d excludedNodes.toArray(\n            new DatanodeInfo[excludedNodes.size()]);\n        lb \u003d locateFollowingBlock(startTime, w.length \u003e 0 ? w : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning block \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.add(nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return nodes;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      do {\n        hasError \u003d false;\n        lastException \u003d null;\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d System.currentTimeMillis();\n        DatanodeInfo[] w \u003d excludedNodes.toArray(\n            new DatanodeInfo[excludedNodes.size()]);\n        lb \u003d locateFollowingBlock(startTime, w.length \u003e 0 ? w : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning block \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.add(nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return nodes;\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "fd9997989c1f1c6f806c57a806e7225ca599fc0c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2092. Remove some object references to Configuration in DFSClient.  Contributed by Bharath Mundlapudi\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139097 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/06/11 3:24 PM",
      "commitName": "fd9997989c1f1c6f806c57a806e7225ca599fc0c",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 11.02,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,39 @@\n     private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n       LocatedBlock lb \u003d null;\n       DatanodeInfo[] nodes \u003d null;\n-      int count \u003d conf.getInt(DFSConfigKeys.DFS_CLIENT_BLOCK_WRITE_RETRIES_KEY,\n-                              DFSConfigKeys.DFS_CLIENT_BLOCK_WRITE_RETRIES_DEFAULT);\n+      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n       boolean success \u003d false;\n       do {\n         hasError \u003d false;\n         lastException \u003d null;\n         errorIndex \u003d -1;\n         success \u003d false;\n \n         long startTime \u003d System.currentTimeMillis();\n         DatanodeInfo[] w \u003d excludedNodes.toArray(\n             new DatanodeInfo[excludedNodes.size()]);\n         lb \u003d locateFollowingBlock(startTime, w.length \u003e 0 ? w : null);\n         block \u003d lb.getBlock();\n         block.setNumBytes(0);\n         accessToken \u003d lb.getBlockToken();\n         nodes \u003d lb.getLocations();\n \n         //\n         // Connect to first DataNode in the list.\n         //\n         success \u003d createBlockOutputStream(nodes, 0L, false);\n \n         if (!success) {\n           DFSClient.LOG.info(\"Abandoning block \" + block);\n           dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n           block \u003d null;\n           DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n           excludedNodes.add(nodes[errorIndex]);\n         }\n       } while (!success \u0026\u0026 --count \u003e\u003d 0);\n \n       if (!success) {\n         throw new IOException(\"Unable to create new block.\");\n       }\n       return nodes;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      int count \u003d dfsClient.getConf().nBlockWriteRetry;\n      boolean success \u003d false;\n      do {\n        hasError \u003d false;\n        lastException \u003d null;\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d System.currentTimeMillis();\n        DatanodeInfo[] w \u003d excludedNodes.toArray(\n            new DatanodeInfo[excludedNodes.size()]);\n        lb \u003d locateFollowingBlock(startTime, w.length \u003e 0 ? w : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning block \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.add(nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return nodes;\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,40 @@\n+    private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n+      LocatedBlock lb \u003d null;\n+      DatanodeInfo[] nodes \u003d null;\n+      int count \u003d conf.getInt(DFSConfigKeys.DFS_CLIENT_BLOCK_WRITE_RETRIES_KEY,\n+                              DFSConfigKeys.DFS_CLIENT_BLOCK_WRITE_RETRIES_DEFAULT);\n+      boolean success \u003d false;\n+      do {\n+        hasError \u003d false;\n+        lastException \u003d null;\n+        errorIndex \u003d -1;\n+        success \u003d false;\n+\n+        long startTime \u003d System.currentTimeMillis();\n+        DatanodeInfo[] w \u003d excludedNodes.toArray(\n+            new DatanodeInfo[excludedNodes.size()]);\n+        lb \u003d locateFollowingBlock(startTime, w.length \u003e 0 ? w : null);\n+        block \u003d lb.getBlock();\n+        block.setNumBytes(0);\n+        accessToken \u003d lb.getBlockToken();\n+        nodes \u003d lb.getLocations();\n+\n+        //\n+        // Connect to first DataNode in the list.\n+        //\n+        success \u003d createBlockOutputStream(nodes, 0L, false);\n+\n+        if (!success) {\n+          DFSClient.LOG.info(\"Abandoning block \" + block);\n+          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n+          block \u003d null;\n+          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n+          excludedNodes.add(nodes[errorIndex]);\n+        }\n+      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n+\n+      if (!success) {\n+        throw new IOException(\"Unable to create new block.\");\n+      }\n+      return nodes;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {\n      LocatedBlock lb \u003d null;\n      DatanodeInfo[] nodes \u003d null;\n      int count \u003d conf.getInt(DFSConfigKeys.DFS_CLIENT_BLOCK_WRITE_RETRIES_KEY,\n                              DFSConfigKeys.DFS_CLIENT_BLOCK_WRITE_RETRIES_DEFAULT);\n      boolean success \u003d false;\n      do {\n        hasError \u003d false;\n        lastException \u003d null;\n        errorIndex \u003d -1;\n        success \u003d false;\n\n        long startTime \u003d System.currentTimeMillis();\n        DatanodeInfo[] w \u003d excludedNodes.toArray(\n            new DatanodeInfo[excludedNodes.size()]);\n        lb \u003d locateFollowingBlock(startTime, w.length \u003e 0 ? w : null);\n        block \u003d lb.getBlock();\n        block.setNumBytes(0);\n        accessToken \u003d lb.getBlockToken();\n        nodes \u003d lb.getLocations();\n\n        //\n        // Connect to first DataNode in the list.\n        //\n        success \u003d createBlockOutputStream(nodes, 0L, false);\n\n        if (!success) {\n          DFSClient.LOG.info(\"Abandoning block \" + block);\n          dfsClient.namenode.abandonBlock(block, src, dfsClient.clientName);\n          block \u003d null;\n          DFSClient.LOG.info(\"Excluding datanode \" + nodes[errorIndex]);\n          excludedNodes.add(nodes[errorIndex]);\n        }\n      } while (!success \u0026\u0026 --count \u003e\u003d 0);\n\n      if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n      }\n      return nodes;\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}