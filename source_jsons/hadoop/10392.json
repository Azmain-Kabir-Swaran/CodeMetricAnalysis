{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataXceiver.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
  "functionStartLine": 222,
  "functionEndLine": 336,
  "numCommitsSeen": 123,
  "timeTaken": 11600,
  "changeHistory": [
    "49ddd8a6ed5b40d12defb0771b4c8b53d4ffde3f",
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
    "abce61335678da753cd0f7965a236370274abee8",
    "4e9029653dfa7a803d73c173cb7044f7e0dc1eb1",
    "c4ee6915a14e00342755d7cdcbf2d61518f306aa",
    "aede8c10ecad4f2a8802a834e4bd0b8286cebade",
    "03bab8dea163a9ee45d09d2a0483d45cf6fe57c9",
    "4da8490b512a33a255ed27309860859388d7c168",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498",
    "5df7ecb33ab24de903f0fd98e2a055164874def5",
    "2d4f3e567e4bb8068c028de12df118a4f3fa6343",
    "9ba8d8c7eb65eeb6fe673f04e493d9eedd95a822",
    "5573b3476a5a6fce0ac99c654a9a9ec90f744a20",
    "c2866ac34d063a4d2e150fe35632111b37a1514d",
    "86cad007d7d6366b293bb9a073814889081c8662",
    "2fb04d2a30919bde350f566a39faa7085f1a1d7b",
    "3b54223c0f32d42a84436c670d80b791a8e9696d",
    "1fbb04e367d7c330e6052207f9f11911f4f5f368",
    "cfb468332e482a51f0a8aea61da4fe5245419a89",
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f",
    "1c6b5d2b5841e5219a98937088cde4ae63869f80",
    "155020a6f4612f8f18890066007a54fb9e3150c2",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
    "837e17b2eac1471d93e2eff395272063b265fee7",
    "239b2742d0e80d13c970fd062af4930e672fe903",
    "cea7bbc630deede93dbe6a1bbda56ad49de4f3de",
    "9b4a7900c7dfc0590316eedaa97144f938885651",
    "a701c792f880c43ba807f00a92a99dadf89eab0c",
    "0663dbaac0a19719ddf9cd4290ba893bfca69da2",
    "8e8bb50afd823a26d9a7ed1311ad050ef059fb5d",
    "513718f92dc572da22a995830fe203b969f23493",
    "905a127850d5e0cba85c2e075f989fa0f5cf129a",
    "6e0991704ffda5cf4cff758f0e7086523fa7bcb4",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "b04aafee504c0e11aeb5b647f5cea4abdd0a28eb",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
    "3f190b3e1acc5ea9e9a03e85a4df0e3f0ab73b9f",
    "6a3963cc8b4cdadf6dc8d2a9ca4f3af4da50a1d2",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "49ddd8a6ed5b40d12defb0771b4c8b53d4ffde3f": "Ybodychange",
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": "Ybodychange",
    "abce61335678da753cd0f7965a236370274abee8": "Ybodychange",
    "4e9029653dfa7a803d73c173cb7044f7e0dc1eb1": "Ybodychange",
    "c4ee6915a14e00342755d7cdcbf2d61518f306aa": "Ybodychange",
    "aede8c10ecad4f2a8802a834e4bd0b8286cebade": "Ybodychange",
    "03bab8dea163a9ee45d09d2a0483d45cf6fe57c9": "Ybodychange",
    "4da8490b512a33a255ed27309860859388d7c168": "Ybodychange",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": "Ybodychange",
    "5df7ecb33ab24de903f0fd98e2a055164874def5": "Ybodychange",
    "2d4f3e567e4bb8068c028de12df118a4f3fa6343": "Ybodychange",
    "9ba8d8c7eb65eeb6fe673f04e493d9eedd95a822": "Ybodychange",
    "5573b3476a5a6fce0ac99c654a9a9ec90f744a20": "Ybodychange",
    "c2866ac34d063a4d2e150fe35632111b37a1514d": "Ybodychange",
    "86cad007d7d6366b293bb9a073814889081c8662": "Ybodychange",
    "2fb04d2a30919bde350f566a39faa7085f1a1d7b": "Ybodychange",
    "3b54223c0f32d42a84436c670d80b791a8e9696d": "Ybodychange",
    "1fbb04e367d7c330e6052207f9f11911f4f5f368": "Ybodychange",
    "cfb468332e482a51f0a8aea61da4fe5245419a89": "Ybodychange",
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f": "Ybodychange",
    "1c6b5d2b5841e5219a98937088cde4ae63869f80": "Ybodychange",
    "155020a6f4612f8f18890066007a54fb9e3150c2": "Ybodychange",
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": "Ybodychange",
    "837e17b2eac1471d93e2eff395272063b265fee7": "Ybodychange",
    "239b2742d0e80d13c970fd062af4930e672fe903": "Ybodychange",
    "cea7bbc630deede93dbe6a1bbda56ad49de4f3de": "Ybodychange",
    "9b4a7900c7dfc0590316eedaa97144f938885651": "Ybodychange",
    "a701c792f880c43ba807f00a92a99dadf89eab0c": "Ybodychange",
    "0663dbaac0a19719ddf9cd4290ba893bfca69da2": "Ybodychange",
    "8e8bb50afd823a26d9a7ed1311ad050ef059fb5d": "Ybodychange",
    "513718f92dc572da22a995830fe203b969f23493": "Ybodychange",
    "905a127850d5e0cba85c2e075f989fa0f5cf129a": "Ybodychange",
    "6e0991704ffda5cf4cff758f0e7086523fa7bcb4": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "b04aafee504c0e11aeb5b647f5cea4abdd0a28eb": "Ybodychange",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2": "Ybodychange",
    "3f190b3e1acc5ea9e9a03e85a4df0e3f0ab73b9f": "Ybodychange",
    "6a3963cc8b4cdadf6dc8d2a9ca4f3af4da50a1d2": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "49ddd8a6ed5b40d12defb0771b4c8b53d4ffde3f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14231. DataXceiver#run() should not log exceptions caused by InvalidToken exception as an error. Contributed by Kitti Nanasi.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "05/02/19 3:14 PM",
      "commitName": "49ddd8a6ed5b40d12defb0771b4c8b53d4ffde3f",
      "commitAuthor": "Kitti Nanasi",
      "commitDateOld": "10/04/18 9:31 PM",
      "commitNameOld": "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 300.78,
      "commitsBetweenForRepo": 2808,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,115 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       synchronized(this) {\n         xceiver \u003d Thread.currentThread();\n       }\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       try {\n         IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n           socketIn, datanode.getXferAddress().getPort(),\n           datanode.getDatanodeId());\n         input \u003d new BufferedInputStream(saslStreams.in,\n             smallBufferSize);\n         socketOut \u003d saslStreams.out;\n       } catch (InvalidMagicNumberException imne) {\n         if (imne.isHandshake4Encryption()) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at {}. Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n               \"encryption\", peer.getRemoteAddressString(), imne);\n         } else {\n           LOG.info(\"Failed to read expected SASL data transfer protection \" +\n               \"handshake from client at {}\" +\n               \". Perhaps the client is running an older version of Hadoop \" +\n               \"which does not support SASL data transfer protection\",\n               peer.getRemoteAddressString(), imne);\n         }\n         return;\n       }\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (EOFException | ClosedChannelException e) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to\n           // get EOF here.\n           LOG.debug(\"Cached {} closing after {} ops.  \" +\n               \"This message is usually benign.\", peer, opsProcessed);\n           break;\n         } catch (IOException err) {\n           incrDatanodeNetworkErrors();\n           throw err;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d monotonicNow();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(\"{}; {}\", s, t.toString());\n         }\n       } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n         String s1 \u003d\n             \"Likely the client has stopped reading, disconnecting it\";\n         s1 +\u003d \" (\" + s + \")\";\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s1, t);\n         } else {\n           LOG.info(\"{}; {}\", s1, t.toString());\n         }\n-      } else if (t instanceof InvalidToken) {\n+      } else if (t instanceof InvalidToken ||\n+          t.getCause() instanceof InvalidToken) {\n         // The InvalidToken exception has already been logged in\n         // checkAccess() method and this is not a server error.\n         LOG.trace(s, t);\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       collectThreadLocalStates();\n       LOG.debug(\"{}:Number of active connections is: {}\",\n           datanode.getDisplayName(), datanode.getXceiverCount());\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      synchronized(this) {\n        xceiver \u003d Thread.currentThread();\n      }\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      try {\n        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input \u003d new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut \u003d saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at {}. Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\", peer.getRemoteAddressString(), imne);\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at {}\" +\n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\",\n              peer.getRemoteAddressString(), imne);\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (EOFException | ClosedChannelException e) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to\n          // get EOF here.\n          LOG.debug(\"Cached {} closing after {} ops.  \" +\n              \"This message is usually benign.\", peer, opsProcessed);\n          break;\n        } catch (IOException err) {\n          incrDatanodeNetworkErrors();\n          throw err;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(\"{}; {}\", s, t.toString());\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(\"{}; {}\", s1, t.toString());\n        }\n      } else if (t instanceof InvalidToken ||\n          t.getCause() instanceof InvalidToken) {\n        // The InvalidToken exception has already been logged in\n        // checkAccess() method and this is not a server error.\n        LOG.trace(s, t);\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      collectThreadLocalStates();\n      LOG.debug(\"{}:Number of active connections is: {}\",\n          datanode.getDisplayName(), datanode.getXceiverCount());\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10571. Use Log.*(Object, Throwable) overload to log exceptions.\nContributed by Andras Bokor.\n",
      "commitDate": "14/02/18 8:20 AM",
      "commitName": "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "01/11/17 1:41 AM",
      "commitNameOld": "56b88b06705441f6f171eec7fb2fa77946ca204b",
      "commitAuthorOld": "Weiwei Yang",
      "daysBetweenCommits": 105.32,
      "commitsBetweenForRepo": 696,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,117 +1,114 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       synchronized(this) {\n         xceiver \u003d Thread.currentThread();\n       }\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       try {\n         IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n           socketIn, datanode.getXferAddress().getPort(),\n           datanode.getDatanodeId());\n         input \u003d new BufferedInputStream(saslStreams.in,\n             smallBufferSize);\n         socketOut \u003d saslStreams.out;\n       } catch (InvalidMagicNumberException imne) {\n         if (imne.isHandshake4Encryption()) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n-              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n+              \"at {}. Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n-              \"encryption\", imne);\n+              \"encryption\", peer.getRemoteAddressString(), imne);\n         } else {\n           LOG.info(\"Failed to read expected SASL data transfer protection \" +\n-              \"handshake from client at \" + peer.getRemoteAddressString() + \n+              \"handshake from client at {}\" +\n               \". Perhaps the client is running an older version of Hadoop \" +\n-              \"which does not support SASL data transfer protection\", imne);\n+              \"which does not support SASL data transfer protection\",\n+              peer.getRemoteAddressString(), imne);\n         }\n         return;\n       }\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (EOFException | ClosedChannelException e) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to\n           // get EOF here.\n           LOG.debug(\"Cached {} closing after {} ops.  \" +\n               \"This message is usually benign.\", peer, opsProcessed);\n           break;\n         } catch (IOException err) {\n           incrDatanodeNetworkErrors();\n           throw err;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d monotonicNow();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n-          LOG.info(s + \"; \" + t);\n+          LOG.info(\"{}; {}\", s, t.toString());\n         }\n       } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n         String s1 \u003d\n             \"Likely the client has stopped reading, disconnecting it\";\n         s1 +\u003d \" (\" + s + \")\";\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s1, t);\n         } else {\n-          LOG.info(s1 + \"; \" + t);          \n+          LOG.info(\"{}; {}\", s1, t.toString());\n         }\n       } else if (t instanceof InvalidToken) {\n         // The InvalidToken exception has already been logged in\n         // checkAccess() method and this is not a server error.\n-        if (LOG.isTraceEnabled()) {\n-          LOG.trace(s, t);\n-        }\n+        LOG.trace(s, t);\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       collectThreadLocalStates();\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n-            + datanode.getXceiverCount());\n-      }\n+      LOG.debug(\"{}:Number of active connections is: {}\",\n+          datanode.getDisplayName(), datanode.getXceiverCount());\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      synchronized(this) {\n        xceiver \u003d Thread.currentThread();\n      }\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      try {\n        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input \u003d new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut \u003d saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at {}. Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\", peer.getRemoteAddressString(), imne);\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at {}\" +\n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\",\n              peer.getRemoteAddressString(), imne);\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (EOFException | ClosedChannelException e) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to\n          // get EOF here.\n          LOG.debug(\"Cached {} closing after {} ops.  \" +\n              \"This message is usually benign.\", peer, opsProcessed);\n          break;\n        } catch (IOException err) {\n          incrDatanodeNetworkErrors();\n          throw err;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(\"{}; {}\", s, t.toString());\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(\"{}; {}\", s1, t.toString());\n        }\n      } else if (t instanceof InvalidToken) {\n        // The InvalidToken exception has already been logged in\n        // checkAccess() method and this is not a server error.\n        LOG.trace(s, t);\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      collectThreadLocalStates();\n      LOG.debug(\"{}:Number of active connections is: {}\",\n          datanode.getDisplayName(), datanode.getXceiverCount());\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "abce61335678da753cd0f7965a236370274abee8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11645. DataXceiver thread should log the actual error when getting InvalidMagicNumberException. Contributed by Chen Liang.\n",
      "commitDate": "12/04/17 11:40 AM",
      "commitName": "abce61335678da753cd0f7965a236370274abee8",
      "commitAuthor": "Anu Engineer",
      "commitDateOld": "24/01/17 4:58 PM",
      "commitNameOld": "b57368b6f893cb27d77fc9425e116f1312f4790f",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 77.74,
      "commitsBetweenForRepo": 443,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,117 +1,117 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       synchronized(this) {\n         xceiver \u003d Thread.currentThread();\n       }\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       try {\n         IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n           socketIn, datanode.getXferAddress().getPort(),\n           datanode.getDatanodeId());\n         input \u003d new BufferedInputStream(saslStreams.in,\n             smallBufferSize);\n         socketOut \u003d saslStreams.out;\n       } catch (InvalidMagicNumberException imne) {\n         if (imne.isHandshake4Encryption()) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n-              \"encryption\");\n+              \"encryption\", imne);\n         } else {\n           LOG.info(\"Failed to read expected SASL data transfer protection \" +\n               \"handshake from client at \" + peer.getRemoteAddressString() + \n               \". Perhaps the client is running an older version of Hadoop \" +\n-              \"which does not support SASL data transfer protection\");\n+              \"which does not support SASL data transfer protection\", imne);\n         }\n         return;\n       }\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (EOFException | ClosedChannelException e) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to\n           // get EOF here.\n           LOG.debug(\"Cached {} closing after {} ops.  \" +\n               \"This message is usually benign.\", peer, opsProcessed);\n           break;\n         } catch (IOException err) {\n           incrDatanodeNetworkErrors();\n           throw err;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d monotonicNow();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n         String s1 \u003d\n             \"Likely the client has stopped reading, disconnecting it\";\n         s1 +\u003d \" (\" + s + \")\";\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s1, t);\n         } else {\n           LOG.info(s1 + \"; \" + t);          \n         }\n       } else if (t instanceof InvalidToken) {\n         // The InvalidToken exception has already been logged in\n         // checkAccess() method and this is not a server error.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       collectThreadLocalStates();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      synchronized(this) {\n        xceiver \u003d Thread.currentThread();\n      }\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      try {\n        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input \u003d new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut \u003d saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\", imne);\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\", imne);\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (EOFException | ClosedChannelException e) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to\n          // get EOF here.\n          LOG.debug(\"Cached {} closing after {} ops.  \" +\n              \"This message is usually benign.\", peer, opsProcessed);\n          break;\n        } catch (IOException err) {\n          incrDatanodeNetworkErrors();\n          throw err;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else if (t instanceof InvalidToken) {\n        // The InvalidToken exception has already been logged in\n        // checkAccess() method and this is not a server error.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      collectThreadLocalStates();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "4e9029653dfa7a803d73c173cb7044f7e0dc1eb1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10917. Collect peer performance statistics on DataNode. Contributed by Xiaobing Zhou.\n",
      "commitDate": "22/12/16 11:46 PM",
      "commitName": "4e9029653dfa7a803d73c173cb7044f7e0dc1eb1",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "13/12/16 5:09 PM",
      "commitNameOld": "e24a923db50879f7dbe5d2afac0e6757089fb07d",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 9.28,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,116 +1,117 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       synchronized(this) {\n         xceiver \u003d Thread.currentThread();\n       }\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       try {\n         IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n           socketIn, datanode.getXferAddress().getPort(),\n           datanode.getDatanodeId());\n         input \u003d new BufferedInputStream(saslStreams.in,\n             smallBufferSize);\n         socketOut \u003d saslStreams.out;\n       } catch (InvalidMagicNumberException imne) {\n         if (imne.isHandshake4Encryption()) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n               \"encryption\");\n         } else {\n           LOG.info(\"Failed to read expected SASL data transfer protection \" +\n               \"handshake from client at \" + peer.getRemoteAddressString() + \n               \". Perhaps the client is running an older version of Hadoop \" +\n               \"which does not support SASL data transfer protection\");\n         }\n         return;\n       }\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (EOFException | ClosedChannelException e) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to\n           // get EOF here.\n           LOG.debug(\"Cached {} closing after {} ops.  \" +\n               \"This message is usually benign.\", peer, opsProcessed);\n           break;\n         } catch (IOException err) {\n           incrDatanodeNetworkErrors();\n           throw err;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d monotonicNow();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n         String s1 \u003d\n             \"Likely the client has stopped reading, disconnecting it\";\n         s1 +\u003d \" (\" + s + \")\";\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s1, t);\n         } else {\n           LOG.info(s1 + \"; \" + t);          \n         }\n       } else if (t instanceof InvalidToken) {\n         // The InvalidToken exception has already been logged in\n         // checkAccess() method and this is not a server error.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n+      collectThreadLocalStates();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      synchronized(this) {\n        xceiver \u003d Thread.currentThread();\n      }\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      try {\n        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input \u003d new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut \u003d saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (EOFException | ClosedChannelException e) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to\n          // get EOF here.\n          LOG.debug(\"Cached {} closing after {} ops.  \" +\n              \"This message is usually benign.\", peer, opsProcessed);\n          break;\n        } catch (IOException err) {\n          incrDatanodeNetworkErrors();\n          throw err;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else if (t instanceof InvalidToken) {\n        // The InvalidToken exception has already been logged in\n        // checkAccess() method and this is not a server error.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      collectThreadLocalStates();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "c4ee6915a14e00342755d7cdcbf2d61518f306aa": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10760. DataXceiver#run() should not log InvalidToken exception as an error. Contributed by Pan Yuxuan.\n",
      "commitDate": "30/08/16 10:43 AM",
      "commitName": "c4ee6915a14e00342755d7cdcbf2d61518f306aa",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "29/06/16 12:41 PM",
      "commitNameOld": "e4a25456202feeee9880d822a8e6f9c19cbcf24a",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 61.92,
      "commitsBetweenForRepo": 517,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,110 +1,116 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       synchronized(this) {\n         xceiver \u003d Thread.currentThread();\n       }\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       try {\n         IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n           socketIn, datanode.getXferAddress().getPort(),\n           datanode.getDatanodeId());\n         input \u003d new BufferedInputStream(saslStreams.in,\n             smallBufferSize);\n         socketOut \u003d saslStreams.out;\n       } catch (InvalidMagicNumberException imne) {\n         if (imne.isHandshake4Encryption()) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n               \"encryption\");\n         } else {\n           LOG.info(\"Failed to read expected SASL data transfer protection \" +\n               \"handshake from client at \" + peer.getRemoteAddressString() + \n               \". Perhaps the client is running an older version of Hadoop \" +\n               \"which does not support SASL data transfer protection\");\n         }\n         return;\n       }\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (EOFException | ClosedChannelException e) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to\n           // get EOF here.\n           LOG.debug(\"Cached {} closing after {} ops.  \" +\n               \"This message is usually benign.\", peer, opsProcessed);\n           break;\n         } catch (IOException err) {\n           incrDatanodeNetworkErrors();\n           throw err;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d monotonicNow();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n         String s1 \u003d\n             \"Likely the client has stopped reading, disconnecting it\";\n         s1 +\u003d \" (\" + s + \")\";\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s1, t);\n         } else {\n           LOG.info(s1 + \"; \" + t);          \n         }\n+      } else if (t instanceof InvalidToken) {\n+        // The InvalidToken exception has already been logged in\n+        // checkAccess() method and this is not a server error.\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(s, t);\n+        }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      synchronized(this) {\n        xceiver \u003d Thread.currentThread();\n      }\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      try {\n        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input \u003d new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut \u003d saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (EOFException | ClosedChannelException e) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to\n          // get EOF here.\n          LOG.debug(\"Cached {} closing after {} ops.  \" +\n              \"This message is usually benign.\", peer, opsProcessed);\n          break;\n        } catch (IOException err) {\n          incrDatanodeNetworkErrors();\n          throw err;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else if (t instanceof InvalidToken) {\n        // The InvalidToken exception has already been logged in\n        // checkAccess() method and this is not a server error.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "aede8c10ecad4f2a8802a834e4bd0b8286cebade": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9945. Datanode command for evicting writers. Contributed by Kihwal Lee\n",
      "commitDate": "06/04/16 1:20 PM",
      "commitName": "aede8c10ecad4f2a8802a834e4bd0b8286cebade",
      "commitAuthor": "Eric Payne",
      "commitDateOld": "26/03/16 7:58 PM",
      "commitNameOld": "3a4ff7776e8fab6cc87932b9aa8fb48f7b69c720",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 10.72,
      "commitsBetweenForRepo": 76,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,107 +1,110 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n+      synchronized(this) {\n+        xceiver \u003d Thread.currentThread();\n+      }\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       try {\n         IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n           socketIn, datanode.getXferAddress().getPort(),\n           datanode.getDatanodeId());\n         input \u003d new BufferedInputStream(saslStreams.in,\n             smallBufferSize);\n         socketOut \u003d saslStreams.out;\n       } catch (InvalidMagicNumberException imne) {\n         if (imne.isHandshake4Encryption()) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n               \"encryption\");\n         } else {\n           LOG.info(\"Failed to read expected SASL data transfer protection \" +\n               \"handshake from client at \" + peer.getRemoteAddressString() + \n               \". Perhaps the client is running an older version of Hadoop \" +\n               \"which does not support SASL data transfer protection\");\n         }\n         return;\n       }\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (EOFException | ClosedChannelException e) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to\n           // get EOF here.\n           LOG.debug(\"Cached {} closing after {} ops.  \" +\n               \"This message is usually benign.\", peer, opsProcessed);\n           break;\n         } catch (IOException err) {\n           incrDatanodeNetworkErrors();\n           throw err;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d monotonicNow();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n         String s1 \u003d\n             \"Likely the client has stopped reading, disconnecting it\";\n         s1 +\u003d \" (\" + s + \")\";\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s1, t);\n         } else {\n           LOG.info(s1 + \"; \" + t);          \n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      synchronized(this) {\n        xceiver \u003d Thread.currentThread();\n      }\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      try {\n        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input \u003d new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut \u003d saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (EOFException | ClosedChannelException e) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to\n          // get EOF here.\n          LOG.debug(\"Cached {} closing after {} ops.  \" +\n              \"This message is usually benign.\", peer, opsProcessed);\n          break;\n        } catch (IOException err) {\n          incrDatanodeNetworkErrors();\n          throw err;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "03bab8dea163a9ee45d09d2a0483d45cf6fe57c9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9572. Prevent DataNode log spam if a client connects on the data transfer port but sends no data. Contributed by Chris Nauroth\n",
      "commitDate": "17/12/15 2:04 PM",
      "commitName": "03bab8dea163a9ee45d09d2a0483d45cf6fe57c9",
      "commitAuthor": "cnauroth",
      "commitDateOld": "15/12/15 2:38 PM",
      "commitNameOld": "49949a4bb03aa81cbb9115e91ab1c61cc6dc8a62",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.98,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,110 +1,107 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       try {\n         IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n           socketIn, datanode.getXferAddress().getPort(),\n           datanode.getDatanodeId());\n         input \u003d new BufferedInputStream(saslStreams.in,\n             smallBufferSize);\n         socketOut \u003d saslStreams.out;\n       } catch (InvalidMagicNumberException imne) {\n         if (imne.isHandshake4Encryption()) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n               \"encryption\");\n         } else {\n           LOG.info(\"Failed to read expected SASL data transfer protection \" +\n               \"handshake from client at \" + peer.getRemoteAddressString() + \n               \". Perhaps the client is running an older version of Hadoop \" +\n               \"which does not support SASL data transfer protection\");\n         }\n         return;\n       }\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n-        } catch (IOException err) {\n-          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n-          if (opsProcessed \u003e 0 \u0026\u0026\n-              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n-            }\n-          } else {\n-            incrDatanodeNetworkErrors();\n-            throw err;\n-          }\n+        } catch (EOFException | ClosedChannelException e) {\n+          // Since we optimistically expect the next op, it\u0027s quite normal to\n+          // get EOF here.\n+          LOG.debug(\"Cached {} closing after {} ops.  \" +\n+              \"This message is usually benign.\", peer, opsProcessed);\n           break;\n+        } catch (IOException err) {\n+          incrDatanodeNetworkErrors();\n+          throw err;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d monotonicNow();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n         String s1 \u003d\n             \"Likely the client has stopped reading, disconnecting it\";\n         s1 +\u003d \" (\" + s + \")\";\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s1, t);\n         } else {\n           LOG.info(s1 + \"; \" + t);          \n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      try {\n        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input \u003d new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut \u003d saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (EOFException | ClosedChannelException e) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to\n          // get EOF here.\n          LOG.debug(\"Cached {} closing after {} ops.  \" +\n              \"This message is usually benign.\", peer, opsProcessed);\n          break;\n        } catch (IOException err) {\n          incrDatanodeNetworkErrors();\n          throw err;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "4da8490b512a33a255ed27309860859388d7c168": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
      "commitDate": "05/05/15 3:41 PM",
      "commitName": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.23,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,110 +1,110 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       try {\n         IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n           socketIn, datanode.getXferAddress().getPort(),\n           datanode.getDatanodeId());\n         input \u003d new BufferedInputStream(saslStreams.in,\n-          HdfsServerConstants.SMALL_BUFFER_SIZE);\n+            smallBufferSize);\n         socketOut \u003d saslStreams.out;\n       } catch (InvalidMagicNumberException imne) {\n         if (imne.isHandshake4Encryption()) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n               \"encryption\");\n         } else {\n           LOG.info(\"Failed to read expected SASL data transfer protection \" +\n               \"handshake from client at \" + peer.getRemoteAddressString() + \n               \". Perhaps the client is running an older version of Hadoop \" +\n               \"which does not support SASL data transfer protection\");\n         }\n         return;\n       }\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             incrDatanodeNetworkErrors();\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d monotonicNow();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n         String s1 \u003d\n             \"Likely the client has stopped reading, disconnecting it\";\n         s1 +\u003d \" (\" + s + \")\";\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s1, t);\n         } else {\n           LOG.info(s1 + \"; \" + t);          \n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      try {\n        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input \u003d new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut \u003d saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            incrDatanodeNetworkErrors();\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "23/04/15 7:00 PM",
      "commitNameOld": "a0e0a63209b5eb17dca5cc503be36aa52defeabd",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 8.63,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,110 +1,110 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       try {\n         IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n           socketIn, datanode.getXferAddress().getPort(),\n           datanode.getDatanodeId());\n         input \u003d new BufferedInputStream(saslStreams.in,\n-          HdfsConstants.SMALL_BUFFER_SIZE);\n+          HdfsServerConstants.SMALL_BUFFER_SIZE);\n         socketOut \u003d saslStreams.out;\n       } catch (InvalidMagicNumberException imne) {\n         if (imne.isHandshake4Encryption()) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n               \"encryption\");\n         } else {\n           LOG.info(\"Failed to read expected SASL data transfer protection \" +\n               \"handshake from client at \" + peer.getRemoteAddressString() + \n               \". Perhaps the client is running an older version of Hadoop \" +\n               \"which does not support SASL data transfer protection\");\n         }\n         return;\n       }\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             incrDatanodeNetworkErrors();\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d monotonicNow();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n         String s1 \u003d\n             \"Likely the client has stopped reading, disconnecting it\";\n         s1 +\u003d \" (\" + s + \")\";\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s1, t);\n         } else {\n           LOG.info(s1 + \"; \" + t);          \n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      try {\n        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input \u003d new BufferedInputStream(saslStreams.in,\n          HdfsServerConstants.SMALL_BUFFER_SIZE);\n        socketOut \u003d saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            incrDatanodeNetworkErrors();\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6841. Use Time.monotonicNow() wherever applicable instead of Time.now(). Contributed by Vinayakumar B\n",
      "commitDate": "20/03/15 12:02 PM",
      "commitName": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "14/03/15 10:36 PM",
      "commitNameOld": "bc9cb3e271b22069a15ca110cd60c860250aaab2",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 5.56,
      "commitsBetweenForRepo": 68,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,110 +1,110 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       try {\n         IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n           socketIn, datanode.getXferAddress().getPort(),\n           datanode.getDatanodeId());\n         input \u003d new BufferedInputStream(saslStreams.in,\n           HdfsConstants.SMALL_BUFFER_SIZE);\n         socketOut \u003d saslStreams.out;\n       } catch (InvalidMagicNumberException imne) {\n         if (imne.isHandshake4Encryption()) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n               \"encryption\");\n         } else {\n           LOG.info(\"Failed to read expected SASL data transfer protection \" +\n               \"handshake from client at \" + peer.getRemoteAddressString() + \n               \". Perhaps the client is running an older version of Hadoop \" +\n               \"which does not support SASL data transfer protection\");\n         }\n         return;\n       }\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             incrDatanodeNetworkErrors();\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n-        opStartTime \u003d now();\n+        opStartTime \u003d monotonicNow();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n         String s1 \u003d\n             \"Likely the client has stopped reading, disconnecting it\";\n         s1 +\u003d \" (\" + s + \")\";\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s1, t);\n         } else {\n           LOG.info(s1 + \"; \" + t);          \n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      try {\n        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input \u003d new BufferedInputStream(saslStreams.in,\n          HdfsConstants.SMALL_BUFFER_SIZE);\n        socketOut \u003d saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            incrDatanodeNetworkErrors();\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "5df7ecb33ab24de903f0fd98e2a055164874def5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7431. log message for InvalidMagicNumberException may be incorrect. Contributed by Yi Liu.\n",
      "commitDate": "18/12/14 3:03 PM",
      "commitName": "5df7ecb33ab24de903f0fd98e2a055164874def5",
      "commitAuthor": "cnauroth",
      "commitDateOld": "26/11/14 9:57 AM",
      "commitNameOld": "058af60c56207907f2bedf76df4284e86d923e0c",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 22.21,
      "commitsBetweenForRepo": 160,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,103 +1,110 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       try {\n         IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n           socketIn, datanode.getXferAddress().getPort(),\n           datanode.getDatanodeId());\n         input \u003d new BufferedInputStream(saslStreams.in,\n           HdfsConstants.SMALL_BUFFER_SIZE);\n         socketOut \u003d saslStreams.out;\n       } catch (InvalidMagicNumberException imne) {\n-        LOG.info(\"Failed to read expected encryption handshake from client \" +\n-            \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n-            \"is running an older version of Hadoop which does not support \" +\n-            \"encryption\");\n+        if (imne.isHandshake4Encryption()) {\n+          LOG.info(\"Failed to read expected encryption handshake from client \" +\n+              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n+              \"is running an older version of Hadoop which does not support \" +\n+              \"encryption\");\n+        } else {\n+          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n+              \"handshake from client at \" + peer.getRemoteAddressString() + \n+              \". Perhaps the client is running an older version of Hadoop \" +\n+              \"which does not support SASL data transfer protection\");\n+        }\n         return;\n       }\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             incrDatanodeNetworkErrors();\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n         String s1 \u003d\n             \"Likely the client has stopped reading, disconnecting it\";\n         s1 +\u003d \" (\" + s + \")\";\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s1, t);\n         } else {\n           LOG.info(s1 + \"; \" + t);          \n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      try {\n        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input \u003d new BufferedInputStream(saslStreams.in,\n          HdfsConstants.SMALL_BUFFER_SIZE);\n        socketOut \u003d saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            incrDatanodeNetworkErrors();\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "2d4f3e567e4bb8068c028de12df118a4f3fa6343": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7331. Add Datanode network counts to datanode jmx page. Contributed by Charles Lamb.\n",
      "commitDate": "21/11/14 4:36 PM",
      "commitName": "2d4f3e567e4bb8068c028de12df118a4f3fa6343",
      "commitAuthor": "Aaron T. Myers",
      "commitDateOld": "08/11/14 10:24 PM",
      "commitNameOld": "9ba8d8c7eb65eeb6fe673f04e493d9eedd95a822",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 12.76,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,103 +1,103 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       try {\n         IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n           socketIn, datanode.getXferAddress().getPort(),\n           datanode.getDatanodeId());\n         input \u003d new BufferedInputStream(saslStreams.in,\n           HdfsConstants.SMALL_BUFFER_SIZE);\n         socketOut \u003d saslStreams.out;\n       } catch (InvalidMagicNumberException imne) {\n         LOG.info(\"Failed to read expected encryption handshake from client \" +\n             \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n             \"is running an older version of Hadoop which does not support \" +\n             \"encryption\");\n         return;\n       }\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n-            datanode.metrics.incrDatanodeNetworkErrors();\n+            incrDatanodeNetworkErrors();\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n         String s1 \u003d\n             \"Likely the client has stopped reading, disconnecting it\";\n         s1 +\u003d \" (\" + s + \")\";\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s1, t);\n         } else {\n           LOG.info(s1 + \"; \" + t);          \n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      try {\n        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input \u003d new BufferedInputStream(saslStreams.in,\n          HdfsConstants.SMALL_BUFFER_SIZE);\n        socketOut \u003d saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        LOG.info(\"Failed to read expected encryption handshake from client \" +\n            \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n            \"is running an older version of Hadoop which does not support \" +\n            \"encryption\");\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            incrDatanodeNetworkErrors();\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "9ba8d8c7eb65eeb6fe673f04e493d9eedd95a822": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7382. DataNode in secure mode may throw NullPointerException if client connects before DataNode registers itself with NameNode. Contributed by Chris Nauroth.\n",
      "commitDate": "08/11/14 10:24 PM",
      "commitName": "9ba8d8c7eb65eeb6fe673f04e493d9eedd95a822",
      "commitAuthor": "cnauroth",
      "commitDateOld": "30/10/14 10:48 PM",
      "commitNameOld": "5573b3476a5a6fce0ac99c654a9a9ec90f744a20",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 9.03,
      "commitsBetweenForRepo": 100,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,102 +1,103 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       try {\n         IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n-          socketIn, datanode.getDatanodeId());\n+          socketIn, datanode.getXferAddress().getPort(),\n+          datanode.getDatanodeId());\n         input \u003d new BufferedInputStream(saslStreams.in,\n           HdfsConstants.SMALL_BUFFER_SIZE);\n         socketOut \u003d saslStreams.out;\n       } catch (InvalidMagicNumberException imne) {\n         LOG.info(\"Failed to read expected encryption handshake from client \" +\n             \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n             \"is running an older version of Hadoop which does not support \" +\n             \"encryption\");\n         return;\n       }\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             datanode.metrics.incrDatanodeNetworkErrors();\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n         String s1 \u003d\n             \"Likely the client has stopped reading, disconnecting it\";\n         s1 +\u003d \" (\" + s + \")\";\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s1, t);\n         } else {\n           LOG.info(s1 + \"; \" + t);          \n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      try {\n        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input \u003d new BufferedInputStream(saslStreams.in,\n          HdfsConstants.SMALL_BUFFER_SIZE);\n        socketOut \u003d saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        LOG.info(\"Failed to read expected encryption handshake from client \" +\n            \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n            \"is running an older version of Hadoop which does not support \" +\n            \"encryption\");\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            datanode.metrics.incrDatanodeNetworkErrors();\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "5573b3476a5a6fce0ac99c654a9a9ec90f744a20": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7313. Support optional configuration of AES cipher suite on DataTransferProtocol. Contributed by Chris Nauroth.\n",
      "commitDate": "30/10/14 10:48 PM",
      "commitName": "5573b3476a5a6fce0ac99c654a9a9ec90f744a20",
      "commitAuthor": "cnauroth",
      "commitDateOld": "30/10/14 11:04 AM",
      "commitNameOld": "c2866ac34d063a4d2e150fe35632111b37a1514d",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.49,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,94 +1,102 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n-      IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n-        socketIn, datanode.getDatanodeId());\n-      input \u003d new BufferedInputStream(saslStreams.in,\n-        HdfsConstants.SMALL_BUFFER_SIZE);\n-      socketOut \u003d saslStreams.out;\n+      try {\n+        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n+          socketIn, datanode.getDatanodeId());\n+        input \u003d new BufferedInputStream(saslStreams.in,\n+          HdfsConstants.SMALL_BUFFER_SIZE);\n+        socketOut \u003d saslStreams.out;\n+      } catch (InvalidMagicNumberException imne) {\n+        LOG.info(\"Failed to read expected encryption handshake from client \" +\n+            \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n+            \"is running an older version of Hadoop which does not support \" +\n+            \"encryption\");\n+        return;\n+      }\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             datanode.metrics.incrDatanodeNetworkErrors();\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n         String s1 \u003d\n             \"Likely the client has stopped reading, disconnecting it\";\n         s1 +\u003d \" (\" + s + \")\";\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s1, t);\n         } else {\n           LOG.info(s1 + \"; \" + t);          \n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      try {\n        IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getDatanodeId());\n        input \u003d new BufferedInputStream(saslStreams.in,\n          HdfsConstants.SMALL_BUFFER_SIZE);\n        socketOut \u003d saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        LOG.info(\"Failed to read expected encryption handshake from client \" +\n            \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n            \"is running an older version of Hadoop which does not support \" +\n            \"encryption\");\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            datanode.metrics.incrDatanodeNetworkErrors();\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "c2866ac34d063a4d2e150fe35632111b37a1514d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3342. SocketTimeoutException in BlockSender.sendChunks could have a better error message. Contributed by Yongjun Zhang.\n",
      "commitDate": "30/10/14 11:04 AM",
      "commitName": "c2866ac34d063a4d2e150fe35632111b37a1514d",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "23/10/14 12:53 PM",
      "commitNameOld": "86cad007d7d6366b293bb9a073814889081c8662",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 6.92,
      "commitsBetweenForRepo": 80,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,94 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n         socketIn, datanode.getDatanodeId());\n       input \u003d new BufferedInputStream(saslStreams.in,\n         HdfsConstants.SMALL_BUFFER_SIZE);\n       socketOut \u003d saslStreams.out;\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             datanode.metrics.incrDatanodeNetworkErrors();\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n+      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n+        String s1 \u003d\n+            \"Likely the client has stopped reading, disconnecting it\";\n+        s1 +\u003d \" (\" + s + \")\";\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(s1, t);\n+        } else {\n+          LOG.info(s1 + \"; \" + t);          \n+        }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n        socketIn, datanode.getDatanodeId());\n      input \u003d new BufferedInputStream(saslStreams.in,\n        HdfsConstants.SMALL_BUFFER_SIZE);\n      socketOut \u003d saslStreams.out;\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            datanode.metrics.incrDatanodeNetworkErrors();\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op \u003d\u003d Op.READ_BLOCK \u0026\u0026 t instanceof SocketTimeoutException) {\n        String s1 \u003d\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 +\u003d \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "86cad007d7d6366b293bb9a073814889081c8662": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7222. Expose DataNode network errors as a metric. (Charles Lamb via wang)\n",
      "commitDate": "23/10/14 12:53 PM",
      "commitName": "86cad007d7d6366b293bb9a073814889081c8662",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/08/14 9:47 PM",
      "commitNameOld": "a317bd7b02c37bd57743bfad59593ec12f53f4ed",
      "commitAuthorOld": "arp",
      "daysBetweenCommits": 56.63,
      "commitsBetweenForRepo": 574,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,85 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n         socketIn, datanode.getDatanodeId());\n       input \u003d new BufferedInputStream(saslStreams.in,\n         HdfsConstants.SMALL_BUFFER_SIZE);\n       socketOut \u003d saslStreams.out;\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n+            datanode.metrics.incrDatanodeNetworkErrors();\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n        socketIn, datanode.getDatanodeId());\n      input \u003d new BufferedInputStream(saslStreams.in,\n        HdfsConstants.SMALL_BUFFER_SIZE);\n      socketOut \u003d saslStreams.out;\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            datanode.metrics.incrDatanodeNetworkErrors();\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "2fb04d2a30919bde350f566a39faa7085f1a1d7b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6569. OOB message can\u0027t be sent to the client when DataNode shuts down for upgrade. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1618742 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/14 2:23 PM",
      "commitName": "2fb04d2a30919bde350f566a39faa7085f1a1d7b",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "13/08/14 11:43 AM",
      "commitNameOld": "195961a7c1da86421761162836766b1de07930fd",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 5.11,
      "commitsBetweenForRepo": 56,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,84 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n-      dataXceiverServer.addPeer(peer, Thread.currentThread());\n+      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n         socketIn, datanode.getDatanodeId());\n       input \u003d new BufferedInputStream(saslStreams.in,\n         HdfsConstants.SMALL_BUFFER_SIZE);\n       socketOut \u003d saslStreams.out;\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n        socketIn, datanode.getDatanodeId());\n      input \u003d new BufferedInputStream(saslStreams.in,\n        HdfsConstants.SMALL_BUFFER_SIZE);\n      socketOut \u003d saslStreams.out;\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "3b54223c0f32d42a84436c670d80b791a8e9696d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2856. Fix block protocol so that Datanodes don\u0027t require root or jsvc. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610474 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/07/14 11:10 AM",
      "commitName": "3b54223c0f32d42a84436c670d80b791a8e9696d",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "22/05/14 11:17 AM",
      "commitNameOld": "3671a5e16fbddbe5a0516289ce98e1305e02291c",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 53.0,
      "commitsBetweenForRepo": 313,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,97 +1,84 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       dataXceiverServer.addPeer(peer, Thread.currentThread());\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n-      if ((!peer.hasSecureChannel()) \u0026\u0026 dnConf.encryptDataTransfer \u0026\u0026\n-          !dnConf.trustedChannelResolver.isTrusted(getClientAddress(peer))){\n-        IOStreamPair encryptedStreams \u003d null;\n-        try {\n-          encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n-              socketIn, datanode.blockPoolTokenSecretManager,\n-              dnConf.encryptionAlgorithm);\n-        } catch (InvalidMagicNumberException imne) {\n-          LOG.info(\"Failed to read expected encryption handshake from client \" +\n-              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n-              \"is running an older version of Hadoop which does not support \" +\n-              \"encryption\");\n-          return;\n-        }\n-        input \u003d encryptedStreams.in;\n-        socketOut \u003d encryptedStreams.out;\n-      }\n-      input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n+      IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n+        socketIn, datanode.getDatanodeId());\n+      input \u003d new BufferedInputStream(saslStreams.in,\n+        HdfsConstants.SMALL_BUFFER_SIZE);\n+      socketOut \u003d saslStreams.out;\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread());\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      IOStreamPair saslStreams \u003d datanode.saslServer.receive(peer, socketOut,\n        socketIn, datanode.getDatanodeId());\n      input \u003d new BufferedInputStream(saslStreams.in,\n        HdfsConstants.SMALL_BUFFER_SIZE);\n      socketOut \u003d saslStreams.out;\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "1fbb04e367d7c330e6052207f9f11911f4f5f368": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5910. Enhance DataTransferProtocol to allow per-connection choice of encryption/plain-text. (Contributed by Benoy Antony)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1581688 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/03/14 9:11 PM",
      "commitName": "1fbb04e367d7c330e6052207f9f11911f4f5f368",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "19/03/14 2:04 PM",
      "commitNameOld": "cfb468332e482a51f0a8aea61da4fe5245419a89",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 6.3,
      "commitsBetweenForRepo": 55,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,96 +1,97 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       dataXceiverServer.addPeer(peer, Thread.currentThread());\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n-      if ((!peer.hasSecureChannel()) \u0026\u0026 dnConf.encryptDataTransfer) {\n+      if ((!peer.hasSecureChannel()) \u0026\u0026 dnConf.encryptDataTransfer \u0026\u0026\n+          !dnConf.trustedChannelResolver.isTrusted(getClientAddress(peer))){\n         IOStreamPair encryptedStreams \u003d null;\n         try {\n           encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n               socketIn, datanode.blockPoolTokenSecretManager,\n               dnConf.encryptionAlgorithm);\n         } catch (InvalidMagicNumberException imne) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n               \"encryption\");\n           return;\n         }\n         input \u003d encryptedStreams.in;\n         socketOut \u003d encryptedStreams.out;\n       }\n       input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n           + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n           + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n       if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n         // For WRITE_BLOCK, it is okay if the replica already exists since\n         // client and replication may write the same block to the same datanode\n         // at the same time.\n         if (LOG.isTraceEnabled()) {\n           LOG.trace(s, t);\n         } else {\n           LOG.info(s + \"; \" + t);\n         }\n       } else {\n         LOG.error(s, t);\n       }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread());\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      if ((!peer.hasSecureChannel()) \u0026\u0026 dnConf.encryptDataTransfer \u0026\u0026\n          !dnConf.trustedChannelResolver.isTrusted(getClientAddress(peer))){\n        IOStreamPair encryptedStreams \u003d null;\n        try {\n          encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n              socketIn, datanode.blockPoolTokenSecretManager,\n              dnConf.encryptionAlgorithm);\n        } catch (InvalidMagicNumberException imne) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n          return;\n        }\n        input \u003d encryptedStreams.in;\n        socketOut \u003d encryptedStreams.out;\n      }\n      input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "cfb468332e482a51f0a8aea61da4fe5245419a89": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6123. Do not log stack trace for ReplicaAlreadyExistsException and SocketTimeoutException.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1579396 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/03/14 2:04 PM",
      "commitName": "cfb468332e482a51f0a8aea61da4fe5245419a89",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "11/03/14 3:41 PM",
      "commitNameOld": "a3616c58dd2ddb16172ca3ab5d66fad52ec0e6d7",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 7.93,
      "commitsBetweenForRepo": 63,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,96 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     try {\n       dataXceiverServer.addPeer(peer, Thread.currentThread());\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       if ((!peer.hasSecureChannel()) \u0026\u0026 dnConf.encryptDataTransfer) {\n         IOStreamPair encryptedStreams \u003d null;\n         try {\n           encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n               socketIn, datanode.blockPoolTokenSecretManager,\n               dnConf.encryptionAlgorithm);\n         } catch (InvalidMagicNumberException imne) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n               \"encryption\");\n           return;\n         }\n         input \u003d encryptedStreams.in;\n         socketOut \u003d encryptedStreams.out;\n       }\n       input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while ((peer !\u003d null) \u0026\u0026\n           (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n-      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n-                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n-                \" src: \" + remoteAddress +\n-                \" dest: \" + localAddress, t);\n+      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n+          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n+          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n+      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n+        // For WRITE_BLOCK, it is okay if the replica already exists since\n+        // client and replication may write the same block to the same datanode\n+        // at the same time.\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(s, t);\n+        } else {\n+          LOG.info(s + \"; \" + t);\n+        }\n+      } else {\n+        LOG.error(s, t);\n+      }\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       if (peer !\u003d null) {\n         dataXceiverServer.closePeer(peer);\n         IOUtils.closeStream(in);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread());\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      if ((!peer.hasSecureChannel()) \u0026\u0026 dnConf.encryptDataTransfer) {\n        IOStreamPair encryptedStreams \u003d null;\n        try {\n          encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n              socketIn, datanode.blockPoolTokenSecretManager,\n              dnConf.encryptionAlgorithm);\n        } catch (InvalidMagicNumberException imne) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n          return;\n        }\n        input \u003d encryptedStreams.in;\n        socketOut \u003d encryptedStreams.out;\n      }\n      input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      String s \u003d datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op \u003d\u003d Op.WRITE_BLOCK \u0026\u0026 t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5950. The DFSClient and DataNode should use shared memory segments to communicate short-circuit information (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1573433 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/03/14 7:58 PM",
      "commitName": "dd049a2f6097da189ccce2f5890a2b9bc77fa73f",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "30/01/14 11:15 AM",
      "commitNameOld": "3d9ad8e3b60dd21db45466f4736abe6b1812b522",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 31.36,
      "commitsBetweenForRepo": 251,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,82 +1,85 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     dataXceiverServer.addPeer(peer);\n     try {\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       if ((!peer.hasSecureChannel()) \u0026\u0026 dnConf.encryptDataTransfer) {\n         IOStreamPair encryptedStreams \u003d null;\n         try {\n           encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n               socketIn, datanode.blockPoolTokenSecretManager,\n               dnConf.encryptionAlgorithm);\n         } catch (InvalidMagicNumberException imne) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n               \"encryption\");\n           return;\n         }\n         input \u003d encryptedStreams.in;\n         socketOut \u003d encryptedStreams.out;\n       }\n       input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n-      } while (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n+      } while ((peer !\u003d null) \u0026\u0026\n+          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n     } catch (Throwable t) {\n       LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n-      dataXceiverServer.closePeer(peer);\n-      IOUtils.closeStream(in);\n+      if (peer !\u003d null) {\n+        dataXceiverServer.closePeer(peer);\n+        IOUtils.closeStream(in);\n+      }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    dataXceiverServer.addPeer(peer);\n    try {\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      if ((!peer.hasSecureChannel()) \u0026\u0026 dnConf.encryptDataTransfer) {\n        IOStreamPair encryptedStreams \u003d null;\n        try {\n          encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n              socketIn, datanode.blockPoolTokenSecretManager,\n              dnConf.encryptionAlgorithm);\n        } catch (InvalidMagicNumberException imne) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n          return;\n        }\n        input \u003d encryptedStreams.in;\n        socketOut \u003d encryptedStreams.out;\n      }\n      input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer !\u003d null) \u0026\u0026\n          (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0));\n    } catch (Throwable t) {\n      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer !\u003d null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "1c6b5d2b5841e5219a98937088cde4ae63869f80": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5583. Make DN send an OOB Ack on shutdown before restarting. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1571491 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/02/14 3:38 PM",
      "commitName": "1c6b5d2b5841e5219a98937088cde4ae63869f80",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "30/01/14 11:15 AM",
      "commitNameOld": "3d9ad8e3b60dd21db45466f4736abe6b1812b522",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 25.18,
      "commitsBetweenForRepo": 228,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,82 +1,82 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n-    dataXceiverServer.addPeer(peer);\n     try {\n+      dataXceiverServer.addPeer(peer, Thread.currentThread());\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       if ((!peer.hasSecureChannel()) \u0026\u0026 dnConf.encryptDataTransfer) {\n         IOStreamPair encryptedStreams \u003d null;\n         try {\n           encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n               socketIn, datanode.blockPoolTokenSecretManager,\n               dnConf.encryptionAlgorithm);\n         } catch (InvalidMagicNumberException imne) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n               \"encryption\");\n           return;\n         }\n         input \u003d encryptedStreams.in;\n         socketOut \u003d encryptedStreams.out;\n       }\n       input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       dataXceiverServer.closePeer(peer);\n       IOUtils.closeStream(in);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    try {\n      dataXceiverServer.addPeer(peer, Thread.currentThread());\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      if ((!peer.hasSecureChannel()) \u0026\u0026 dnConf.encryptDataTransfer) {\n        IOStreamPair encryptedStreams \u003d null;\n        try {\n          encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n              socketIn, datanode.blockPoolTokenSecretManager,\n              dnConf.encryptionAlgorithm);\n        } catch (InvalidMagicNumberException imne) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n          return;\n        }\n        input \u003d encryptedStreams.in;\n        socketOut \u003d encryptedStreams.out;\n      }\n      input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      dataXceiverServer.closePeer(peer);\n      IOUtils.closeStream(in);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "155020a6f4612f8f18890066007a54fb9e3150c2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5353. Short circuit reads fail when dfs.encrypt.data.transfer is enabled. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548987 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/12/13 7:52 PM",
      "commitName": "155020a6f4612f8f18890066007a54fb9e3150c2",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "22/07/13 11:15 AM",
      "commitNameOld": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 138.4,
      "commitsBetweenForRepo": 862,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,82 +1,82 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n \n     dataXceiverServer.addPeer(peer);\n     try {\n       peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n-      if (dnConf.encryptDataTransfer) {\n+      if ((!peer.hasSecureChannel()) \u0026\u0026 dnConf.encryptDataTransfer) {\n         IOStreamPair encryptedStreams \u003d null;\n         try {\n           encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n               socketIn, datanode.blockPoolTokenSecretManager,\n               dnConf.encryptionAlgorithm);\n         } catch (InvalidMagicNumberException imne) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n               \"is running an older version of Hadoop which does not support \" +\n               \"encryption\");\n           return;\n         }\n         input \u003d encryptedStreams.in;\n         socketOut \u003d encryptedStreams.out;\n       }\n       input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       dataXceiverServer.closePeer(peer);\n       IOUtils.closeStream(in);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    dataXceiverServer.addPeer(peer);\n    try {\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      if ((!peer.hasSecureChannel()) \u0026\u0026 dnConf.encryptDataTransfer) {\n        IOStreamPair encryptedStreams \u003d null;\n        try {\n          encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n              socketIn, datanode.blockPoolTokenSecretManager,\n              dnConf.encryptionAlgorithm);\n        } catch (InvalidMagicNumberException imne) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n          return;\n        }\n        input \u003d encryptedStreams.in;\n        socketOut \u003d encryptedStreams.out;\n      }\n      input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      dataXceiverServer.closePeer(peer);\n      IOUtils.closeStream(in);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1431097 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 1:34 PM",
      "commitName": "c9db06f2e4d1c1f71f021d5070323f9fc194cdd7",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "08/01/13 6:39 PM",
      "commitNameOld": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.79,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,82 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n-    \n-    dataXceiverServer.childSockets.add(s);\n-    \n+\n+    dataXceiverServer.addPeer(peer);\n     try {\n-      \n+      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       if (dnConf.encryptDataTransfer) {\n         IOStreamPair encryptedStreams \u003d null;\n         try {\n           encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n               socketIn, datanode.blockPoolTokenSecretManager,\n               dnConf.encryptionAlgorithm);\n         } catch (InvalidMagicNumberException imne) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n-              \"at \" + s.getInetAddress() + \". Perhaps the client is running an \" +\n-              \"older version of Hadoop which does not support encryption\");\n+              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n+              \"is running an older version of Hadoop which does not support \" +\n+              \"encryption\");\n           return;\n         }\n         input \u003d encryptedStreams.in;\n         socketOut \u003d encryptedStreams.out;\n       }\n       input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n-            socketIn.setTimeout(dnConf.socketKeepaliveTimeout);\n+            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n-            socketIn.setTimeout(dnConf.socketTimeout);\n+            peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n+              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n-          s.setSoTimeout(dnConf.socketTimeout);\n+          peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n-      } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n+      } while (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n+      dataXceiverServer.closePeer(peer);\n       IOUtils.closeStream(in);\n-      IOUtils.closeSocket(s);\n-      dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    dataXceiverServer.addPeer(peer);\n    try {\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      if (dnConf.encryptDataTransfer) {\n        IOStreamPair encryptedStreams \u003d null;\n        try {\n          encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n              socketIn, datanode.blockPoolTokenSecretManager,\n              dnConf.encryptionAlgorithm);\n        } catch (InvalidMagicNumberException imne) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n          return;\n        }\n        input \u003d encryptedStreams.in;\n        socketOut \u003d encryptedStreams.out;\n      }\n      input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      dataXceiverServer.closePeer(peer);\n      IOUtils.closeStream(in);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "837e17b2eac1471d93e2eff395272063b265fee7": {
      "type": "Ybodychange",
      "commitMessage": "svn merge -c -1430507 . for reverting HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430662 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 6:39 PM",
      "commitName": "837e17b2eac1471d93e2eff395272063b265fee7",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/01/13 12:44 PM",
      "commitNameOld": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.25,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,82 +1,83 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n-\n-    dataXceiverServer.addPeer(peer);\n+    \n+    dataXceiverServer.childSockets.add(s);\n+    \n     try {\n-      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n+      \n       InputStream input \u003d socketIn;\n       if (dnConf.encryptDataTransfer) {\n         IOStreamPair encryptedStreams \u003d null;\n         try {\n           encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n               socketIn, datanode.blockPoolTokenSecretManager,\n               dnConf.encryptionAlgorithm);\n         } catch (InvalidMagicNumberException imne) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n-              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n-              \"is running an older version of Hadoop which does not support \" +\n-              \"encryption\");\n+              \"at \" + s.getInetAddress() + \". Perhaps the client is running an \" +\n+              \"older version of Hadoop which does not support encryption\");\n           return;\n         }\n         input \u003d encryptedStreams.in;\n         socketOut \u003d encryptedStreams.out;\n       }\n       input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n-            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n+            socketIn.setTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n-            peer.setReadTimeout(dnConf.socketTimeout);\n+            socketIn.setTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n+              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n-          peer.setReadTimeout(dnConf.socketTimeout);\n+          s.setSoTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n-      } while (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n+      } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n-      dataXceiverServer.closePeer(peer);\n       IOUtils.closeStream(in);\n+      IOUtils.closeSocket(s);\n+      dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n    \n    dataXceiverServer.childSockets.add(s);\n    \n    try {\n      \n      InputStream input \u003d socketIn;\n      if (dnConf.encryptDataTransfer) {\n        IOStreamPair encryptedStreams \u003d null;\n        try {\n          encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n              socketIn, datanode.blockPoolTokenSecretManager,\n              dnConf.encryptionAlgorithm);\n        } catch (InvalidMagicNumberException imne) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + s.getInetAddress() + \". Perhaps the client is running an \" +\n              \"older version of Hadoop which does not support encryption\");\n          return;\n        }\n        input \u003d encryptedStreams.in;\n        socketOut \u003d encryptedStreams.out;\n      }\n      input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            socketIn.setTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            socketIn.setTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "239b2742d0e80d13c970fd062af4930e672fe903": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430507 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 12:44 PM",
      "commitName": "239b2742d0e80d13c970fd062af4930e672fe903",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "28/10/12 4:10 PM",
      "commitNameOld": "cea7bbc630deede93dbe6a1bbda56ad49de4f3de",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 71.9,
      "commitsBetweenForRepo": 300,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,82 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n-    \n-    dataXceiverServer.childSockets.add(s);\n-    \n+\n+    dataXceiverServer.addPeer(peer);\n     try {\n-      \n+      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n       InputStream input \u003d socketIn;\n       if (dnConf.encryptDataTransfer) {\n         IOStreamPair encryptedStreams \u003d null;\n         try {\n           encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n               socketIn, datanode.blockPoolTokenSecretManager,\n               dnConf.encryptionAlgorithm);\n         } catch (InvalidMagicNumberException imne) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n-              \"at \" + s.getInetAddress() + \". Perhaps the client is running an \" +\n-              \"older version of Hadoop which does not support encryption\");\n+              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n+              \"is running an older version of Hadoop which does not support \" +\n+              \"encryption\");\n           return;\n         }\n         input \u003d encryptedStreams.in;\n         socketOut \u003d encryptedStreams.out;\n       }\n       input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n-            socketIn.setTimeout(dnConf.socketKeepaliveTimeout);\n+            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n-            socketIn.setTimeout(dnConf.socketTimeout);\n+            peer.setReadTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n+              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n-          s.setSoTimeout(dnConf.socketTimeout);\n+          peer.setReadTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n-      } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n+      } while (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n+      dataXceiverServer.closePeer(peer);\n       IOUtils.closeStream(in);\n-      IOUtils.closeSocket(s);\n-      dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n\n    dataXceiverServer.addPeer(peer);\n    try {\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input \u003d socketIn;\n      if (dnConf.encryptDataTransfer) {\n        IOStreamPair encryptedStreams \u003d null;\n        try {\n          encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n              socketIn, datanode.blockPoolTokenSecretManager,\n              dnConf.encryptionAlgorithm);\n        } catch (InvalidMagicNumberException imne) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n          return;\n        }\n        input \u003d encryptedStreams.in;\n        socketOut \u003d encryptedStreams.out;\n      }\n      input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + peer + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!peer.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      dataXceiverServer.closePeer(peer);\n      IOUtils.closeStream(in);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "cea7bbc630deede93dbe6a1bbda56ad49de4f3de": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4122. Cleanup HDFS logs and reduce the size of logged messages. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1403120 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/10/12 4:10 PM",
      "commitName": "cea7bbc630deede93dbe6a1bbda56ad49de4f3de",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "24/08/12 7:15 AM",
      "commitNameOld": "c46de830da98959f40dd41c95bdebecdfb9ea730",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 65.37,
      "commitsBetweenForRepo": 392,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,83 +1,83 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n     \n     dataXceiverServer.childSockets.add(s);\n     \n     try {\n       \n       InputStream input \u003d socketIn;\n       if (dnConf.encryptDataTransfer) {\n         IOStreamPair encryptedStreams \u003d null;\n         try {\n           encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n               socketIn, datanode.blockPoolTokenSecretManager,\n               dnConf.encryptionAlgorithm);\n         } catch (InvalidMagicNumberException imne) {\n           LOG.info(\"Failed to read expected encryption handshake from client \" +\n               \"at \" + s.getInetAddress() + \". Perhaps the client is running an \" +\n-              \"older version of Hadoop which does not support encryption.\");\n+              \"older version of Hadoop which does not support encryption\");\n           return;\n         }\n         input \u003d encryptedStreams.in;\n         socketOut \u003d encryptedStreams.out;\n       }\n       input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n       \n       super.initialize(new DataInputStream(input));\n       \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             socketIn.setTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n             socketIn.setTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           s.setSoTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       IOUtils.closeStream(in);\n       IOUtils.closeSocket(s);\n       dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n    \n    dataXceiverServer.childSockets.add(s);\n    \n    try {\n      \n      InputStream input \u003d socketIn;\n      if (dnConf.encryptDataTransfer) {\n        IOStreamPair encryptedStreams \u003d null;\n        try {\n          encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n              socketIn, datanode.blockPoolTokenSecretManager,\n              dnConf.encryptionAlgorithm);\n        } catch (InvalidMagicNumberException imne) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + s.getInetAddress() + \". Perhaps the client is running an \" +\n              \"older version of Hadoop which does not support encryption\");\n          return;\n        }\n        input \u003d encryptedStreams.in;\n        socketOut \u003d encryptedStreams.out;\n      }\n      input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            socketIn.setTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            socketIn.setTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "9b4a7900c7dfc0590316eedaa97144f938885651": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 9:40 AM",
      "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "15/07/12 7:58 PM",
      "commitNameOld": "0e8e499ff482c165d21c8e4f5ff9c33f306ca0d9",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 22.57,
      "commitsBetweenForRepo": 106,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,83 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n+    \n     dataXceiverServer.childSockets.add(s);\n+    \n     try {\n+      \n+      InputStream input \u003d socketIn;\n+      if (dnConf.encryptDataTransfer) {\n+        IOStreamPair encryptedStreams \u003d null;\n+        try {\n+          encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n+              socketIn, datanode.blockPoolTokenSecretManager,\n+              dnConf.encryptionAlgorithm);\n+        } catch (InvalidMagicNumberException imne) {\n+          LOG.info(\"Failed to read expected encryption handshake from client \" +\n+              \"at \" + s.getInetAddress() + \". Perhaps the client is running an \" +\n+              \"older version of Hadoop which does not support encryption.\");\n+          return;\n+        }\n+        input \u003d encryptedStreams.in;\n+        socketOut \u003d encryptedStreams.out;\n+      }\n+      input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n+      \n+      super.initialize(new DataInputStream(input));\n+      \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n-            socketInputWrapper.setTimeout(dnConf.socketKeepaliveTimeout);\n+            socketIn.setTimeout(dnConf.socketKeepaliveTimeout);\n           } else {\n-            socketInputWrapper.setTimeout(dnConf.socketTimeout);\n+            socketIn.setTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           s.setSoTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       IOUtils.closeStream(in);\n       IOUtils.closeSocket(s);\n       dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n    \n    dataXceiverServer.childSockets.add(s);\n    \n    try {\n      \n      InputStream input \u003d socketIn;\n      if (dnConf.encryptDataTransfer) {\n        IOStreamPair encryptedStreams \u003d null;\n        try {\n          encryptedStreams \u003d DataTransferEncryptor.getEncryptedStreams(socketOut,\n              socketIn, datanode.blockPoolTokenSecretManager,\n              dnConf.encryptionAlgorithm);\n        } catch (InvalidMagicNumberException imne) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + s.getInetAddress() + \". Perhaps the client is running an \" +\n              \"older version of Hadoop which does not support encryption.\");\n          return;\n        }\n        input \u003d encryptedStreams.in;\n        socketOut \u003d encryptedStreams.out;\n      }\n      input \u003d new BufferedInputStream(input, HdfsConstants.SMALL_BUFFER_SIZE);\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            socketIn.setTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            socketIn.setTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "a701c792f880c43ba807f00a92a99dadf89eab0c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3357. DataXceiver reads from client socket with incorrect/no timeout. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1334116 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/05/12 11:50 AM",
      "commitName": "a701c792f880c43ba807f00a92a99dadf89eab0c",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "01/04/12 3:12 PM",
      "commitNameOld": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 32.86,
      "commitsBetweenForRepo": 265,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n     dataXceiverServer.childSockets.add(s);\n     try {\n-      int stdTimeout \u003d s.getSoTimeout();\n-\n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n-            s.setSoTimeout(dnConf.socketKeepaliveTimeout);\n+            socketInputWrapper.setTimeout(dnConf.socketKeepaliveTimeout);\n+          } else {\n+            socketInputWrapper.setTimeout(dnConf.socketTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n-          s.setSoTimeout(stdTimeout);\n+          s.setSoTimeout(dnConf.socketTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       IOUtils.closeStream(in);\n       IOUtils.closeSocket(s);\n       dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n    dataXceiverServer.childSockets.add(s);\n    try {\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            socketInputWrapper.setTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            socketInputWrapper.setTimeout(dnConf.socketTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "0663dbaac0a19719ddf9cd4290ba893bfca69da2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3171. The DatanodeID \"name\" field is overloaded. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308014 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/03/12 8:41 PM",
      "commitName": "0663dbaac0a19719ddf9cd4290ba893bfca69da2",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "16/03/12 10:32 AM",
      "commitNameOld": "662b1887af4e39f3eadd7dda4953c7f2529b43bc",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 15.42,
      "commitsBetweenForRepo": 91,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n     dataXceiverServer.childSockets.add(s);\n     try {\n       int stdTimeout \u003d s.getSoTimeout();\n \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             s.setSoTimeout(dnConf.socketKeepaliveTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           s.setSoTimeout(stdTimeout);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n-      LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n+      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n+        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       IOUtils.closeStream(in);\n       IOUtils.closeSocket(s);\n       dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n    dataXceiverServer.childSockets.add(s);\n    try {\n      int stdTimeout \u003d s.getSoTimeout();\n\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            s.setSoTimeout(dnConf.socketKeepaliveTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(stdTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getDisplayName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "8e8bb50afd823a26d9a7ed1311ad050ef059fb5d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2454. Move maxXceiverCount check to before starting the thread in dataXceiver. Contributed by Harsh J\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1204124 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/11/11 8:27 PM",
      "commitName": "8e8bb50afd823a26d9a7ed1311ad050ef059fb5d",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "19/11/11 8:13 PM",
      "commitNameOld": "513718f92dc572da22a995830fe203b969f23493",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,60 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n     dataXceiverServer.childSockets.add(s);\n     try {\n       int stdTimeout \u003d s.getSoTimeout();\n \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             s.setSoTimeout(dnConf.socketKeepaliveTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           s.setSoTimeout(stdTimeout);\n         }\n \n-        // Make sure the xceiver count is not exceeded\n-        int curXceiverCount \u003d datanode.getXceiverCount();\n-        if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n-          throw new IOException(\"xceiverCount \" + curXceiverCount\n-                                + \" exceeds the limit of concurrent xcievers \"\n-                                + dataXceiverServer.maxXceiverCount);\n-        }\n-\n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       IOUtils.closeStream(in);\n       IOUtils.closeSocket(s);\n       dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n    dataXceiverServer.childSockets.add(s);\n    try {\n      int stdTimeout \u003d s.getSoTimeout();\n\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            s.setSoTimeout(dnConf.socketKeepaliveTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(stdTimeout);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "513718f92dc572da22a995830fe203b969f23493": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2568. Use a set to manage child sockets in XceiverServer. Contributed by Harsh J\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1204122 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/11/11 8:13 PM",
      "commitName": "513718f92dc572da22a995830fe203b969f23493",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "19/11/11 7:58 PM",
      "commitNameOld": "b2313021fd03d9803a04b120d97bfd91fcbd4d36",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,68 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n-    dataXceiverServer.childSockets.put(s, s);\n+    dataXceiverServer.childSockets.add(s);\n     try {\n       int stdTimeout \u003d s.getSoTimeout();\n \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert dnConf.socketKeepaliveTimeout \u003e 0;\n             s.setSoTimeout(dnConf.socketKeepaliveTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           s.setSoTimeout(stdTimeout);\n         }\n \n         // Make sure the xceiver count is not exceeded\n         int curXceiverCount \u003d datanode.getXceiverCount();\n         if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n           throw new IOException(\"xceiverCount \" + curXceiverCount\n                                 + \" exceeds the limit of concurrent xcievers \"\n                                 + dataXceiverServer.maxXceiverCount);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       IOUtils.closeStream(in);\n       IOUtils.closeSocket(s);\n       dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n    dataXceiverServer.childSockets.add(s);\n    try {\n      int stdTimeout \u003d s.getSoTimeout();\n\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            s.setSoTimeout(dnConf.socketKeepaliveTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(stdTimeout);\n        }\n\n        // Make sure the xceiver count is not exceeded\n        int curXceiverCount \u003d datanode.getXceiverCount();\n        if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n          throw new IOException(\"xceiverCount \" + curXceiverCount\n                                + \" exceeds the limit of concurrent xcievers \"\n                                + dataXceiverServer.maxXceiverCount);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "905a127850d5e0cba85c2e075f989fa0f5cf129a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2562. Refactor DN configuration variables out of DataNode class. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203543 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 1:04 AM",
      "commitName": "905a127850d5e0cba85c2e075f989fa0f5cf129a",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "31/10/11 10:17 PM",
      "commitNameOld": "1c940637b14eee777a65d153d0d712a1aea3866c",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 17.16,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,68 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n     dataXceiverServer.childSockets.put(s, s);\n     try {\n       int stdTimeout \u003d s.getSoTimeout();\n \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n-            assert socketKeepaliveTimeout \u003e 0;\n-            s.setSoTimeout(socketKeepaliveTimeout);\n+            assert dnConf.socketKeepaliveTimeout \u003e 0;\n+            s.setSoTimeout(dnConf.socketKeepaliveTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           s.setSoTimeout(stdTimeout);\n         }\n \n         // Make sure the xceiver count is not exceeded\n         int curXceiverCount \u003d datanode.getXceiverCount();\n         if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n           throw new IOException(\"xceiverCount \" + curXceiverCount\n                                 + \" exceeds the limit of concurrent xcievers \"\n                                 + dataXceiverServer.maxXceiverCount);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n-      } while (!s.isClosed() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n+      } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       IOUtils.closeStream(in);\n       IOUtils.closeSocket(s);\n       dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n    dataXceiverServer.childSockets.put(s, s);\n    try {\n      int stdTimeout \u003d s.getSoTimeout();\n\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert dnConf.socketKeepaliveTimeout \u003e 0;\n            s.setSoTimeout(dnConf.socketKeepaliveTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(stdTimeout);\n        }\n\n        // Make sure the xceiver count is not exceeded\n        int curXceiverCount \u003d datanode.getXceiverCount();\n        if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n          throw new IOException(\"xceiverCount \" + curXceiverCount\n                                + \" exceeds the limit of concurrent xcievers \"\n                                + dataXceiverServer.maxXceiverCount);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 dnConf.socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "6e0991704ffda5cf4cff758f0e7086523fa7bcb4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2452. OutOfMemoryError in DataXceiverServer takes down the DataNode. Contributed by Uma Maheswara Rao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1187965 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/10/11 1:17 PM",
      "commitName": "6e0991704ffda5cf4cff758f0e7086523fa7bcb4",
      "commitAuthor": "Konstantin Shvachko",
      "commitDateOld": "28/09/11 9:40 PM",
      "commitNameOld": "e90a5b40430cc1fbce075d34b31e3cc05fd9831f",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 24.65,
      "commitsBetweenForRepo": 186,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,68 @@\n   public void run() {\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n+    dataXceiverServer.childSockets.put(s, s);\n     try {\n       int stdTimeout \u003d s.getSoTimeout();\n \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n \n         try {\n           if (opsProcessed !\u003d 0) {\n             assert socketKeepaliveTimeout \u003e 0;\n             s.setSoTimeout(socketKeepaliveTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           s.setSoTimeout(stdTimeout);\n         }\n \n         // Make sure the xceiver count is not exceeded\n         int curXceiverCount \u003d datanode.getXceiverCount();\n         if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n           throw new IOException(\"xceiverCount \" + curXceiverCount\n                                 + \" exceeds the limit of concurrent xcievers \"\n                                 + dataXceiverServer.maxXceiverCount);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while (!s.isClosed() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       IOUtils.closeStream(in);\n       IOUtils.closeSocket(s);\n       dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n    dataXceiverServer.childSockets.put(s, s);\n    try {\n      int stdTimeout \u003d s.getSoTimeout();\n\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert socketKeepaliveTimeout \u003e 0;\n            s.setSoTimeout(socketKeepaliveTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(stdTimeout);\n        }\n\n        // Make sure the xceiver count is not exceeded\n        int curXceiverCount \u003d datanode.getXceiverCount();\n        if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n          throw new IOException(\"xceiverCount \" + curXceiverCount\n                                + \" exceeds the limit of concurrent xcievers \"\n                                + dataXceiverServer.maxXceiverCount);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n    try {\n      int stdTimeout \u003d s.getSoTimeout();\n\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert socketKeepaliveTimeout \u003e 0;\n            s.setSoTimeout(socketKeepaliveTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(stdTimeout);\n        }\n\n        // Make sure the xceiver count is not exceeded\n        int curXceiverCount \u003d datanode.getXceiverCount();\n        if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n          throw new IOException(\"xceiverCount \" + curXceiverCount\n                                + \" exceeds the limit of concurrent xcievers \"\n                                + dataXceiverServer.maxXceiverCount);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java"
      }
    },
    "b04aafee504c0e11aeb5b647f5cea4abdd0a28eb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2267. DataXceiver thread name incorrect while waiting on op during keepalive. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1160873 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/08/11 12:47 PM",
      "commitName": "b04aafee504c0e11aeb5b647f5cea4abdd0a28eb",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "19/08/11 10:36 AM",
      "commitNameOld": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 4.09,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,67 @@\n   public void run() {\n-    updateCurrentThreadName(\"Waiting for operation\");\n-\n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n     try {\n       int stdTimeout \u003d s.getSoTimeout();\n \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n+        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n+\n         try {\n           if (opsProcessed !\u003d 0) {\n             assert socketKeepaliveTimeout \u003e 0;\n             s.setSoTimeout(socketKeepaliveTimeout);\n           }\n           op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           s.setSoTimeout(stdTimeout);\n         }\n \n         // Make sure the xceiver count is not exceeded\n         int curXceiverCount \u003d datanode.getXceiverCount();\n         if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n           throw new IOException(\"xceiverCount \" + curXceiverCount\n                                 + \" exceeds the limit of concurrent xcievers \"\n                                 + dataXceiverServer.maxXceiverCount);\n         }\n \n         opStartTime \u003d now();\n         processOp(op);\n         ++opsProcessed;\n       } while (!s.isClosed() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       IOUtils.closeStream(in);\n       IOUtils.closeSocket(s);\n       dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n    try {\n      int stdTimeout \u003d s.getSoTimeout();\n\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert socketKeepaliveTimeout \u003e 0;\n            s.setSoTimeout(socketKeepaliveTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(stdTimeout);\n        }\n\n        // Make sure the xceiver count is not exceeded\n        int curXceiverCount \u003d datanode.getXceiverCount();\n        if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n          throw new IOException(\"xceiverCount \" + curXceiverCount\n                                + \" exceeds the limit of concurrent xcievers \"\n                                + dataXceiverServer.maxXceiverCount);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void run() {\n    updateCurrentThreadName(\"Waiting for operation\");\n\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n    try {\n      int stdTimeout \u003d s.getSoTimeout();\n\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert socketKeepaliveTimeout \u003e 0;\n            s.setSoTimeout(socketKeepaliveTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(stdTimeout);\n        }\n\n        // Make sure the xceiver count is not exceeded\n        int curXceiverCount \u003d datanode.getXceiverCount();\n        if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n          throw new IOException(\"xceiverCount \" + curXceiverCount\n                                + \" exceeds the limit of concurrent xcievers \"\n                                + dataXceiverServer.maxXceiverCount);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java"
      }
    },
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2087. Declare methods in DataTransferProtocol interface, and change Sender and Receiver to implement the interface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139124 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/06/11 4:57 PM",
      "commitName": "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "21/06/11 10:12 AM",
      "commitNameOld": "3f190b3e1acc5ea9e9a03e85a4df0e3f0ab73b9f",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 2.28,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,67 @@\n   public void run() {\n     updateCurrentThreadName(\"Waiting for operation\");\n \n-    DataInputStream in\u003dnull; \n     int opsProcessed \u003d 0;\n     Op op \u003d null;\n     try {\n-      in \u003d new DataInputStream(\n-          new BufferedInputStream(NetUtils.getInputStream(s), \n-                                  SMALL_BUFFER_SIZE));\n       int stdTimeout \u003d s.getSoTimeout();\n \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         try {\n           if (opsProcessed !\u003d 0) {\n             assert socketKeepaliveTimeout \u003e 0;\n             s.setSoTimeout(socketKeepaliveTimeout);\n           }\n-          op \u003d readOp(in);\n+          op \u003d readOp();\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           s.setSoTimeout(stdTimeout);\n         }\n \n         // Make sure the xceiver count is not exceeded\n         int curXceiverCount \u003d datanode.getXceiverCount();\n         if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n           throw new IOException(\"xceiverCount \" + curXceiverCount\n                                 + \" exceeds the limit of concurrent xcievers \"\n                                 + dataXceiverServer.maxXceiverCount);\n         }\n \n         opStartTime \u003d now();\n-        processOp(op, in);\n+        processOp(op);\n         ++opsProcessed;\n       } while (!s.isClosed() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                 ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                 \" src: \" + remoteAddress +\n                 \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       IOUtils.closeStream(in);\n       IOUtils.closeSocket(s);\n       dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    updateCurrentThreadName(\"Waiting for operation\");\n\n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n    try {\n      int stdTimeout \u003d s.getSoTimeout();\n\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert socketKeepaliveTimeout \u003e 0;\n            s.setSoTimeout(socketKeepaliveTimeout);\n          }\n          op \u003d readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(stdTimeout);\n        }\n\n        // Make sure the xceiver count is not exceeded\n        int curXceiverCount \u003d datanode.getXceiverCount();\n        if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n          throw new IOException(\"xceiverCount \" + curXceiverCount\n                                + \" exceeds the limit of concurrent xcievers \"\n                                + dataXceiverServer.maxXceiverCount);\n        }\n\n        opStartTime \u003d now();\n        processOp(op);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "3f190b3e1acc5ea9e9a03e85a4df0e3f0ab73b9f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1568. Improve the log messages in DataXceiver.  Contributed by Joey Echeverria\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1138098 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/06/11 10:12 AM",
      "commitName": "3f190b3e1acc5ea9e9a03e85a4df0e3f0ab73b9f",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "15/06/11 2:36 PM",
      "commitNameOld": "6a3963cc8b4cdadf6dc8d2a9ca4f3af4da50a1d2",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 5.82,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,71 @@\n   public void run() {\n     updateCurrentThreadName(\"Waiting for operation\");\n \n     DataInputStream in\u003dnull; \n     int opsProcessed \u003d 0;\n+    Op op \u003d null;\n     try {\n       in \u003d new DataInputStream(\n           new BufferedInputStream(NetUtils.getInputStream(s), \n                                   SMALL_BUFFER_SIZE));\n       int stdTimeout \u003d s.getSoTimeout();\n \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n-        Op op;\n         try {\n           if (opsProcessed !\u003d 0) {\n             assert socketKeepaliveTimeout \u003e 0;\n             s.setSoTimeout(socketKeepaliveTimeout);\n           }\n           op \u003d readOp(in);\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           s.setSoTimeout(stdTimeout);\n         }\n \n         // Make sure the xceiver count is not exceeded\n         int curXceiverCount \u003d datanode.getXceiverCount();\n         if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n           throw new IOException(\"xceiverCount \" + curXceiverCount\n                                 + \" exceeds the limit of concurrent xcievers \"\n                                 + dataXceiverServer.maxXceiverCount);\n         }\n \n         opStartTime \u003d now();\n         processOp(op, in);\n         ++opsProcessed;\n       } while (!s.isClosed() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n-      LOG.error(datanode.getMachineName() + \":DataXceiver, at \" +\n-        s.toString(), t);\n+      LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n+                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n+                \" src: \" + remoteAddress +\n+                \" dest: \" + localAddress, t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       IOUtils.closeStream(in);\n       IOUtils.closeSocket(s);\n       dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    updateCurrentThreadName(\"Waiting for operation\");\n\n    DataInputStream in\u003dnull; \n    int opsProcessed \u003d 0;\n    Op op \u003d null;\n    try {\n      in \u003d new DataInputStream(\n          new BufferedInputStream(NetUtils.getInputStream(s), \n                                  SMALL_BUFFER_SIZE));\n      int stdTimeout \u003d s.getSoTimeout();\n\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert socketKeepaliveTimeout \u003e 0;\n            s.setSoTimeout(socketKeepaliveTimeout);\n          }\n          op \u003d readOp(in);\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(stdTimeout);\n        }\n\n        // Make sure the xceiver count is not exceeded\n        int curXceiverCount \u003d datanode.getXceiverCount();\n        if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n          throw new IOException(\"xceiverCount \" + curXceiverCount\n                                + \" exceeds the limit of concurrent xcievers \"\n                                + dataXceiverServer.maxXceiverCount);\n        }\n\n        opStartTime \u003d now();\n        processOp(op, in);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getMachineName() + \":DataXceiver error processing \" +\n                ((op \u003d\u003d null) ? \"unknown\" : op.name()) + \" operation \" +\n                \" src: \" + remoteAddress +\n                \" dest: \" + localAddress, t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "6a3963cc8b4cdadf6dc8d2a9ca4f3af4da50a1d2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2071. Use of isConnected() in DataXceiver is invalid. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1136205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/06/11 2:36 PM",
      "commitName": "6a3963cc8b4cdadf6dc8d2a9ca4f3af4da50a1d2",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 2.98,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,69 @@\n   public void run() {\n     updateCurrentThreadName(\"Waiting for operation\");\n \n     DataInputStream in\u003dnull; \n     int opsProcessed \u003d 0;\n     try {\n       in \u003d new DataInputStream(\n           new BufferedInputStream(NetUtils.getInputStream(s), \n                                   SMALL_BUFFER_SIZE));\n       int stdTimeout \u003d s.getSoTimeout();\n \n       // We process requests in a loop, and stay around for a short timeout.\n       // This optimistic behaviour allows the other end to reuse connections.\n       // Setting keepalive timeout to 0 disable this behavior.\n       do {\n         Op op;\n         try {\n           if (opsProcessed !\u003d 0) {\n             assert socketKeepaliveTimeout \u003e 0;\n             s.setSoTimeout(socketKeepaliveTimeout);\n           }\n           op \u003d readOp(in);\n         } catch (InterruptedIOException ignored) {\n           // Time out while we wait for client rpc\n           break;\n         } catch (IOException err) {\n           // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n           if (opsProcessed \u003e 0 \u0026\u0026\n               (err instanceof EOFException || err instanceof ClosedChannelException)) {\n             if (LOG.isDebugEnabled()) {\n               LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n             }\n           } else {\n             throw err;\n           }\n           break;\n         }\n \n         // restore normal timeout\n         if (opsProcessed !\u003d 0) {\n           s.setSoTimeout(stdTimeout);\n         }\n \n         // Make sure the xceiver count is not exceeded\n         int curXceiverCount \u003d datanode.getXceiverCount();\n         if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n           throw new IOException(\"xceiverCount \" + curXceiverCount\n                                 + \" exceeds the limit of concurrent xcievers \"\n                                 + dataXceiverServer.maxXceiverCount);\n         }\n \n         opStartTime \u003d now();\n         processOp(op, in);\n         ++opsProcessed;\n-      } while (s.isConnected() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n+      } while (!s.isClosed() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n     } catch (Throwable t) {\n       LOG.error(datanode.getMachineName() + \":DataXceiver, at \" +\n         s.toString(), t);\n     } finally {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n             + datanode.getXceiverCount());\n       }\n       updateCurrentThreadName(\"Cleaning up\");\n       IOUtils.closeStream(in);\n       IOUtils.closeSocket(s);\n       dataXceiverServer.childSockets.remove(s);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    updateCurrentThreadName(\"Waiting for operation\");\n\n    DataInputStream in\u003dnull; \n    int opsProcessed \u003d 0;\n    try {\n      in \u003d new DataInputStream(\n          new BufferedInputStream(NetUtils.getInputStream(s), \n                                  SMALL_BUFFER_SIZE));\n      int stdTimeout \u003d s.getSoTimeout();\n\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        Op op;\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert socketKeepaliveTimeout \u003e 0;\n            s.setSoTimeout(socketKeepaliveTimeout);\n          }\n          op \u003d readOp(in);\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(stdTimeout);\n        }\n\n        // Make sure the xceiver count is not exceeded\n        int curXceiverCount \u003d datanode.getXceiverCount();\n        if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n          throw new IOException(\"xceiverCount \" + curXceiverCount\n                                + \" exceeds the limit of concurrent xcievers \"\n                                + dataXceiverServer.maxXceiverCount);\n        }\n\n        opStartTime \u003d now();\n        processOp(op, in);\n        ++opsProcessed;\n      } while (!s.isClosed() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getMachineName() + \":DataXceiver, at \" +\n        s.toString(), t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,69 @@\n+  public void run() {\n+    updateCurrentThreadName(\"Waiting for operation\");\n+\n+    DataInputStream in\u003dnull; \n+    int opsProcessed \u003d 0;\n+    try {\n+      in \u003d new DataInputStream(\n+          new BufferedInputStream(NetUtils.getInputStream(s), \n+                                  SMALL_BUFFER_SIZE));\n+      int stdTimeout \u003d s.getSoTimeout();\n+\n+      // We process requests in a loop, and stay around for a short timeout.\n+      // This optimistic behaviour allows the other end to reuse connections.\n+      // Setting keepalive timeout to 0 disable this behavior.\n+      do {\n+        Op op;\n+        try {\n+          if (opsProcessed !\u003d 0) {\n+            assert socketKeepaliveTimeout \u003e 0;\n+            s.setSoTimeout(socketKeepaliveTimeout);\n+          }\n+          op \u003d readOp(in);\n+        } catch (InterruptedIOException ignored) {\n+          // Time out while we wait for client rpc\n+          break;\n+        } catch (IOException err) {\n+          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n+          if (opsProcessed \u003e 0 \u0026\u0026\n+              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n+            }\n+          } else {\n+            throw err;\n+          }\n+          break;\n+        }\n+\n+        // restore normal timeout\n+        if (opsProcessed !\u003d 0) {\n+          s.setSoTimeout(stdTimeout);\n+        }\n+\n+        // Make sure the xceiver count is not exceeded\n+        int curXceiverCount \u003d datanode.getXceiverCount();\n+        if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n+          throw new IOException(\"xceiverCount \" + curXceiverCount\n+                                + \" exceeds the limit of concurrent xcievers \"\n+                                + dataXceiverServer.maxXceiverCount);\n+        }\n+\n+        opStartTime \u003d now();\n+        processOp(op, in);\n+        ++opsProcessed;\n+      } while (s.isConnected() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n+    } catch (Throwable t) {\n+      LOG.error(datanode.getMachineName() + \":DataXceiver, at \" +\n+        s.toString(), t);\n+    } finally {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n+            + datanode.getXceiverCount());\n+      }\n+      updateCurrentThreadName(\"Cleaning up\");\n+      IOUtils.closeStream(in);\n+      IOUtils.closeSocket(s);\n+      dataXceiverServer.childSockets.remove(s);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    updateCurrentThreadName(\"Waiting for operation\");\n\n    DataInputStream in\u003dnull; \n    int opsProcessed \u003d 0;\n    try {\n      in \u003d new DataInputStream(\n          new BufferedInputStream(NetUtils.getInputStream(s), \n                                  SMALL_BUFFER_SIZE));\n      int stdTimeout \u003d s.getSoTimeout();\n\n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        Op op;\n        try {\n          if (opsProcessed !\u003d 0) {\n            assert socketKeepaliveTimeout \u003e 0;\n            s.setSoTimeout(socketKeepaliveTimeout);\n          }\n          op \u003d readOp(in);\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (IOException err) {\n          // Since we optimistically expect the next op, it\u0027s quite normal to get EOF here.\n          if (opsProcessed \u003e 0 \u0026\u0026\n              (err instanceof EOFException || err instanceof ClosedChannelException)) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Cached \" + s.toString() + \" closing after \" + opsProcessed + \" ops\");\n            }\n          } else {\n            throw err;\n          }\n          break;\n        }\n\n        // restore normal timeout\n        if (opsProcessed !\u003d 0) {\n          s.setSoTimeout(stdTimeout);\n        }\n\n        // Make sure the xceiver count is not exceeded\n        int curXceiverCount \u003d datanode.getXceiverCount();\n        if (curXceiverCount \u003e dataXceiverServer.maxXceiverCount) {\n          throw new IOException(\"xceiverCount \" + curXceiverCount\n                                + \" exceeds the limit of concurrent xcievers \"\n                                + dataXceiverServer.maxXceiverCount);\n        }\n\n        opStartTime \u003d now();\n        processOp(op, in);\n        ++opsProcessed;\n      } while (s.isConnected() \u0026\u0026 socketKeepaliveTimeout \u003e 0);\n    } catch (Throwable t) {\n      LOG.error(datanode.getMachineName() + \":DataXceiver, at \" +\n        s.toString(), t);\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getMachineName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      IOUtils.closeStream(in);\n      IOUtils.closeSocket(s);\n      dataXceiverServer.childSockets.remove(s);\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java"
    }
  }
}