{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSNamesystem.java",
  "functionName": "commitBlockSynchronization",
  "functionId": "commitBlockSynchronization___oldBlock-ExtendedBlock__newgenerationstamp-long__newlength-long__closeFile-boolean__deleteblock-boolean__newtargets-DatanodeID[]__newtargetstorages-String[]",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
  "functionStartLine": 3868,
  "functionEndLine": 4028,
  "numCommitsSeen": 1457,
  "timeTaken": 67760,
  "changeHistory": [
    "e9b859f749103dc15fb4b4fc677f5586fceb20b3",
    "42307e3c3abbfe0b83d9a2581deba327435b910f",
    "53bbef3802194b7a0a3ce5cd3c91def9e88856e3",
    "5304698dc8c5667c33e6ed9c4a827ef57172a723",
    "ff0b99eafeda035ebe0dc82cfe689808047a8893",
    "61ab0440f7eaff0f631cbae0378403912f88d7ad",
    "132478e805ba0f955345217b8ad87c2d17cccb2d",
    "3fa33b5c2c289ceaced30c6c5451f3569110459d",
    "0b18e5e8c69b40c9a446fff448d38e0dd10cb45e",
    "164cbe643988f878f0f4100a4de51783e5b6738e",
    "4cbbfa2220e884e91bf18ad1cc2f3b11f895f8c9",
    "e535e0f05b5fbd087c93238deb888cc985254b4c",
    "f4c523b69ba55b1fd35e8995c3011a9f546ac835",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
    "c17439c2ddd921b63b1635e6f1cba634b8da8557",
    "4928f5473394981829e5ffd4b16ea0801baf5c45",
    "51ea117f883f9c049de58987dc66e07e71a68ee4",
    "544f75d6512fefd0e36f24a35e6b7472ca7bf301",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b",
    "ba9371492036983a9899398907ab41fe548f29b3",
    "e5afac5896a1a88e152746598527d91f73cbb724",
    "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f",
    "262c1bc3398ce2ede03f9d86fc97c35ca7a8e9db",
    "997408eaaceef20b053ee7344468e28cb9a1379b",
    "e37ca221bf4e9ae5d5e667d8ca284df9fdb33199",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
    "08ac06283a3e9bf0d49d873823aabd419b08e41f",
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
    "bd616552cce7e46d1cc27ad9aba38f96a1f4c29c",
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
    "a4e0ff5e052abad498595ee198b49c5310c9ec0d",
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
    "ce68f410b05a58ad05965f32ad7f5b246b363a75",
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a",
    "1fe1942328856dd832e9f94fb56a40ab3d810870",
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
    "eb484bb5629e57c97192b6794f30c1fbb290b6ee",
    "5d9d702607913685eab0d8ad077040ddc82bf085",
    "8c7a7e619699386f9e6991842558d78aa0c8053d",
    "f138ae68f9be0ae072a6a4ee50e94a1608c90edb",
    "11c073134afc878619c37c95935d6a3098a21f17",
    "fd1000bcefa07992ff5c6fae3508f3e33b7955c6",
    "3bf09c51501a23b7fa28fd0a0c4c0965858d026c",
    "b1333e5b561d01a010e2e1311e8501879f377bdc",
    "f29fa9e820e25730d00a1a00c51c6f11028fb5a7",
    "b9f965de120b5278ac84a7e98aecb32aafde4c16",
    "9821af9ce8a56a2c583f1ed938902c20e897048f",
    "7e8e983620f3ae3462d115972707c72b7d9cbabd",
    "f0f9a3631fe4950f5cf548f192226836925d0f05",
    "72b3f302dc3590b7f731cad26558cd592c35dc5a",
    "6326605acb5a5bf48d994278c9d3a39733679e81",
    "c14912785d22734d735b5c4f8638b57dff009a97",
    "cf611255d6fcd7016e0ce2a3f80ccd0d4e051d9f",
    "36d1c49486587c2dbb193e8538b1d4510c462fa6",
    "71071b904d0c9aec7b3713d41740f24182e81c36",
    "b7cd8c0f865e88e40eee75fd2690b1fdc4155071",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "2892f6d817d74e90ff50073cd3721ed4ec75ba92",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "969a263188f7015261719fe45fa1505121ebb80e",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "e9b859f749103dc15fb4b4fc677f5586fceb20b3": "Ybodychange",
    "42307e3c3abbfe0b83d9a2581deba327435b910f": "Ybodychange",
    "53bbef3802194b7a0a3ce5cd3c91def9e88856e3": "Ybodychange",
    "5304698dc8c5667c33e6ed9c4a827ef57172a723": "Ybodychange",
    "ff0b99eafeda035ebe0dc82cfe689808047a8893": "Ybodychange",
    "61ab0440f7eaff0f631cbae0378403912f88d7ad": "Ybodychange",
    "132478e805ba0f955345217b8ad87c2d17cccb2d": "Ybodychange",
    "3fa33b5c2c289ceaced30c6c5451f3569110459d": "Ybodychange",
    "0b18e5e8c69b40c9a446fff448d38e0dd10cb45e": "Ybodychange",
    "164cbe643988f878f0f4100a4de51783e5b6738e": "Ybodychange",
    "4cbbfa2220e884e91bf18ad1cc2f3b11f895f8c9": "Ybodychange",
    "e535e0f05b5fbd087c93238deb888cc985254b4c": "Ybodychange",
    "f4c523b69ba55b1fd35e8995c3011a9f546ac835": "Ybodychange",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": "Ybodychange",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": "Ybodychange",
    "c17439c2ddd921b63b1635e6f1cba634b8da8557": "Ybodychange",
    "4928f5473394981829e5ffd4b16ea0801baf5c45": "Ybodychange",
    "51ea117f883f9c049de58987dc66e07e71a68ee4": "Ybodychange",
    "544f75d6512fefd0e36f24a35e6b7472ca7bf301": "Ybodychange",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": "Ybodychange",
    "ba9371492036983a9899398907ab41fe548f29b3": "Ybodychange",
    "e5afac5896a1a88e152746598527d91f73cbb724": "Ybodychange",
    "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f": "Ybodychange",
    "262c1bc3398ce2ede03f9d86fc97c35ca7a8e9db": "Ybodychange",
    "997408eaaceef20b053ee7344468e28cb9a1379b": "Ybodychange",
    "e37ca221bf4e9ae5d5e667d8ca284df9fdb33199": "Ybodychange",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": "Ybodychange",
    "08ac06283a3e9bf0d49d873823aabd419b08e41f": "Ymultichange(Yparameterchange,Ybodychange)",
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19": "Yexceptionschange",
    "bd616552cce7e46d1cc27ad9aba38f96a1f4c29c": "Ybodychange",
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4": "Ybodychange",
    "a4e0ff5e052abad498595ee198b49c5310c9ec0d": "Ybodychange",
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec": "Ybodychange",
    "ce68f410b05a58ad05965f32ad7f5b246b363a75": "Ybodychange",
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a": "Ybodychange",
    "1fe1942328856dd832e9f94fb56a40ab3d810870": "Ybodychange",
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a": "Ybodychange",
    "eb484bb5629e57c97192b6794f30c1fbb290b6ee": "Ybodychange",
    "5d9d702607913685eab0d8ad077040ddc82bf085": "Ybodychange",
    "8c7a7e619699386f9e6991842558d78aa0c8053d": "Ybodychange",
    "f138ae68f9be0ae072a6a4ee50e94a1608c90edb": "Ybodychange",
    "11c073134afc878619c37c95935d6a3098a21f17": "Ybodychange",
    "fd1000bcefa07992ff5c6fae3508f3e33b7955c6": "Ybodychange",
    "3bf09c51501a23b7fa28fd0a0c4c0965858d026c": "Ybodychange",
    "b1333e5b561d01a010e2e1311e8501879f377bdc": "Ybodychange",
    "f29fa9e820e25730d00a1a00c51c6f11028fb5a7": "Ybodychange",
    "b9f965de120b5278ac84a7e98aecb32aafde4c16": "Ybodychange",
    "9821af9ce8a56a2c583f1ed938902c20e897048f": "Ybodychange",
    "7e8e983620f3ae3462d115972707c72b7d9cbabd": "Ybodychange",
    "f0f9a3631fe4950f5cf548f192226836925d0f05": "Ybodychange",
    "72b3f302dc3590b7f731cad26558cd592c35dc5a": "Ybodychange",
    "6326605acb5a5bf48d994278c9d3a39733679e81": "Yparameterchange",
    "c14912785d22734d735b5c4f8638b57dff009a97": "Ybodychange",
    "cf611255d6fcd7016e0ce2a3f80ccd0d4e051d9f": "Ybodychange",
    "36d1c49486587c2dbb193e8538b1d4510c462fa6": "Ybodychange",
    "71071b904d0c9aec7b3713d41740f24182e81c36": "Ybodychange",
    "b7cd8c0f865e88e40eee75fd2690b1fdc4155071": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "2892f6d817d74e90ff50073cd3721ed4ec75ba92": "Ybodychange",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "969a263188f7015261719fe45fa1505121ebb80e": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "e9b859f749103dc15fb4b4fc677f5586fceb20b3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14371. Improve Logging in FSNamesystem by adding parameterized logging. Contributed by Shweta.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "05/04/19 4:09 PM",
      "commitName": "e9b859f749103dc15fb4b4fc677f5586fceb20b3",
      "commitAuthor": "Shweta",
      "commitDateOld": "05/03/19 5:39 PM",
      "commitNameOld": "945b504c256d196c50634f61f3efe65a3b9a13a5",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 30.9,
      "commitsBetweenForRepo": 246,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,164 +1,161 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     final String src;\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n-          if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n-          }\n+          LOG.debug(\"Block (\u003d{}) not found\", oldBlock);\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       final INodeFile iFile \u003d getBlockCollection(storedBlock);\n       src \u003d iFile.getFullPathName();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + src + \", likely due to delayed block removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n-                    + \") since the file (\u003d\" + iFile.getLocalName()\n-                    + \") is not under construction\");\n+          LOG.debug(\"Unexpected block (\u003d{}) since the file (\u003d{}) is not \"\n+                + \"under construction\", oldBlock, iFile.getLocalName());\n         }\n         return;\n       }\n \n       truncatedBlock \u003d iFile.getLastBlock();\n       final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n           .getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       } else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // Find the target DatanodeStorageInfos. If not found because of invalid\n         // or empty DatanodeID/StorageID, the slot of same offset in dsInfos is\n         // null\n         final DatanodeStorageInfo[] dsInfos \u003d blockManager.getDatanodeManager().\n             getDatanodeStorageInfos(newtargets, newtargetstorages,\n                 \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                 src, oldBlock, newgenerationstamp, newlength);\n \n         if (closeFile \u0026\u0026 dsInfos !\u003d null) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c dsInfos.length; i++) {\n             if (dsInfos[i] !\u003d null) {\n               if(copyTruncate) {\n                 dsInfos[i].addBlock(truncatedBlock, truncatedBlock);\n               } else {\n                 Block bi \u003d new Block(storedBlock);\n                 if (storedBlock.isStriped()) {\n                   bi.setBlockId(bi.getBlockId() + i);\n                 }\n                 dsInfos[i].addBlock(storedBlock, bi);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         if(copyTruncate) {\n           iFile.convertLastBlockToUC(truncatedBlock, dsInfos);\n         } else {\n           iFile.convertLastBlockToUC(storedBlock, dsInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                 storedBlock, oldGenerationStamp, oldNumBytes,\n                 dsInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           closeFileCommitBlocks(src, iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           closeFileCommitBlocks(src, iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n       blockManager.successfulBlockRecovery(storedBlock);\n     } finally {\n       writeUnlock(\"commitBlockSynchronization\");\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    final String src;\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          LOG.debug(\"Block (\u003d{}) not found\", oldBlock);\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      final INodeFile iFile \u003d getBlockCollection(storedBlock);\n      src \u003d iFile.getFullPathName();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + src + \", likely due to delayed block removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d{}) since the file (\u003d{}) is not \"\n                + \"under construction\", oldBlock, iFile.getLocalName());\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n          .getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      } else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // Find the target DatanodeStorageInfos. If not found because of invalid\n        // or empty DatanodeID/StorageID, the slot of same offset in dsInfos is\n        // null\n        final DatanodeStorageInfo[] dsInfos \u003d blockManager.getDatanodeManager().\n            getDatanodeStorageInfos(newtargets, newtargetstorages,\n                \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                src, oldBlock, newgenerationstamp, newlength);\n\n        if (closeFile \u0026\u0026 dsInfos !\u003d null) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c dsInfos.length; i++) {\n            if (dsInfos[i] !\u003d null) {\n              if(copyTruncate) {\n                dsInfos[i].addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                Block bi \u003d new Block(storedBlock);\n                if (storedBlock.isStriped()) {\n                  bi.setBlockId(bi.getBlockId() + i);\n                }\n                dsInfos[i].addBlock(storedBlock, bi);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, dsInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, dsInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                storedBlock, oldGenerationStamp, oldNumBytes,\n                dsInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          closeFileCommitBlocks(src, iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          closeFileCommitBlocks(src, iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n      blockManager.successfulBlockRecovery(storedBlock);\n    } finally {\n      writeUnlock(\"commitBlockSynchronization\");\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "42307e3c3abbfe0b83d9a2581deba327435b910f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11576. Block recovery will fail indefinitely if recovery time \u003e heartbeat interval. Contributed by Lukas Majercak\n",
      "commitDate": "01/12/17 10:34 PM",
      "commitName": "42307e3c3abbfe0b83d9a2581deba327435b910f",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "01/12/17 11:19 AM",
      "commitNameOld": "53bbef3802194b7a0a3ce5cd3c91def9e88856e3",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 0.47,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,163 +1,164 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     final String src;\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       final INodeFile iFile \u003d getBlockCollection(storedBlock);\n       src \u003d iFile.getFullPathName();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + src + \", likely due to delayed block removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d iFile.getLastBlock();\n       final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n           .getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       } else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // Find the target DatanodeStorageInfos. If not found because of invalid\n         // or empty DatanodeID/StorageID, the slot of same offset in dsInfos is\n         // null\n         final DatanodeStorageInfo[] dsInfos \u003d blockManager.getDatanodeManager().\n             getDatanodeStorageInfos(newtargets, newtargetstorages,\n                 \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                 src, oldBlock, newgenerationstamp, newlength);\n \n         if (closeFile \u0026\u0026 dsInfos !\u003d null) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c dsInfos.length; i++) {\n             if (dsInfos[i] !\u003d null) {\n               if(copyTruncate) {\n                 dsInfos[i].addBlock(truncatedBlock, truncatedBlock);\n               } else {\n                 Block bi \u003d new Block(storedBlock);\n                 if (storedBlock.isStriped()) {\n                   bi.setBlockId(bi.getBlockId() + i);\n                 }\n                 dsInfos[i].addBlock(storedBlock, bi);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         if(copyTruncate) {\n           iFile.convertLastBlockToUC(truncatedBlock, dsInfos);\n         } else {\n           iFile.convertLastBlockToUC(storedBlock, dsInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                 storedBlock, oldGenerationStamp, oldNumBytes,\n                 dsInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           closeFileCommitBlocks(src, iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           closeFileCommitBlocks(src, iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n+      blockManager.successfulBlockRecovery(storedBlock);\n     } finally {\n       writeUnlock(\"commitBlockSynchronization\");\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    final String src;\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      final INodeFile iFile \u003d getBlockCollection(storedBlock);\n      src \u003d iFile.getFullPathName();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + src + \", likely due to delayed block removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n          .getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      } else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // Find the target DatanodeStorageInfos. If not found because of invalid\n        // or empty DatanodeID/StorageID, the slot of same offset in dsInfos is\n        // null\n        final DatanodeStorageInfo[] dsInfos \u003d blockManager.getDatanodeManager().\n            getDatanodeStorageInfos(newtargets, newtargetstorages,\n                \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                src, oldBlock, newgenerationstamp, newlength);\n\n        if (closeFile \u0026\u0026 dsInfos !\u003d null) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c dsInfos.length; i++) {\n            if (dsInfos[i] !\u003d null) {\n              if(copyTruncate) {\n                dsInfos[i].addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                Block bi \u003d new Block(storedBlock);\n                if (storedBlock.isStriped()) {\n                  bi.setBlockId(bi.getBlockId() + i);\n                }\n                dsInfos[i].addBlock(storedBlock, bi);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, dsInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, dsInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                storedBlock, oldGenerationStamp, oldNumBytes,\n                dsInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          closeFileCommitBlocks(src, iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          closeFileCommitBlocks(src, iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n      blockManager.successfulBlockRecovery(storedBlock);\n    } finally {\n      writeUnlock(\"commitBlockSynchronization\");\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "53bbef3802194b7a0a3ce5cd3c91def9e88856e3": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-11576. Block recovery will fail indefinitely if recovery time \u003e heartbeat interval. Contributed by Lukas Majercak\"\n\nThis reverts commit 5304698dc8c5667c33e6ed9c4a827ef57172a723.\n",
      "commitDate": "01/12/17 11:19 AM",
      "commitName": "53bbef3802194b7a0a3ce5cd3c91def9e88856e3",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "01/12/17 10:29 AM",
      "commitNameOld": "5304698dc8c5667c33e6ed9c4a827ef57172a723",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 0.03,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,164 +1,163 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     final String src;\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       final INodeFile iFile \u003d getBlockCollection(storedBlock);\n       src \u003d iFile.getFullPathName();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + src + \", likely due to delayed block removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d iFile.getLastBlock();\n       final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n           .getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       } else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // Find the target DatanodeStorageInfos. If not found because of invalid\n         // or empty DatanodeID/StorageID, the slot of same offset in dsInfos is\n         // null\n         final DatanodeStorageInfo[] dsInfos \u003d blockManager.getDatanodeManager().\n             getDatanodeStorageInfos(newtargets, newtargetstorages,\n                 \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                 src, oldBlock, newgenerationstamp, newlength);\n \n         if (closeFile \u0026\u0026 dsInfos !\u003d null) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c dsInfos.length; i++) {\n             if (dsInfos[i] !\u003d null) {\n               if(copyTruncate) {\n                 dsInfos[i].addBlock(truncatedBlock, truncatedBlock);\n               } else {\n                 Block bi \u003d new Block(storedBlock);\n                 if (storedBlock.isStriped()) {\n                   bi.setBlockId(bi.getBlockId() + i);\n                 }\n                 dsInfos[i].addBlock(storedBlock, bi);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         if(copyTruncate) {\n           iFile.convertLastBlockToUC(truncatedBlock, dsInfos);\n         } else {\n           iFile.convertLastBlockToUC(storedBlock, dsInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                 storedBlock, oldGenerationStamp, oldNumBytes,\n                 dsInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           closeFileCommitBlocks(src, iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           closeFileCommitBlocks(src, iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n-      blockManager.successfulBlockRecovery(storedBlock);\n     } finally {\n       writeUnlock(\"commitBlockSynchronization\");\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    final String src;\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      final INodeFile iFile \u003d getBlockCollection(storedBlock);\n      src \u003d iFile.getFullPathName();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + src + \", likely due to delayed block removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n          .getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      } else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // Find the target DatanodeStorageInfos. If not found because of invalid\n        // or empty DatanodeID/StorageID, the slot of same offset in dsInfos is\n        // null\n        final DatanodeStorageInfo[] dsInfos \u003d blockManager.getDatanodeManager().\n            getDatanodeStorageInfos(newtargets, newtargetstorages,\n                \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                src, oldBlock, newgenerationstamp, newlength);\n\n        if (closeFile \u0026\u0026 dsInfos !\u003d null) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c dsInfos.length; i++) {\n            if (dsInfos[i] !\u003d null) {\n              if(copyTruncate) {\n                dsInfos[i].addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                Block bi \u003d new Block(storedBlock);\n                if (storedBlock.isStriped()) {\n                  bi.setBlockId(bi.getBlockId() + i);\n                }\n                dsInfos[i].addBlock(storedBlock, bi);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, dsInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, dsInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                storedBlock, oldGenerationStamp, oldNumBytes,\n                dsInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          closeFileCommitBlocks(src, iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          closeFileCommitBlocks(src, iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock(\"commitBlockSynchronization\");\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "5304698dc8c5667c33e6ed9c4a827ef57172a723": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11576. Block recovery will fail indefinitely if recovery time \u003e heartbeat interval. Contributed by Lukas Majercak\n",
      "commitDate": "01/12/17 10:29 AM",
      "commitName": "5304698dc8c5667c33e6ed9c4a827ef57172a723",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "30/11/17 12:18 PM",
      "commitNameOld": "b1c7654ee40b372ed777525a42981c7cf55b5c72",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 0.92,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,163 +1,164 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     final String src;\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       final INodeFile iFile \u003d getBlockCollection(storedBlock);\n       src \u003d iFile.getFullPathName();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + src + \", likely due to delayed block removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d iFile.getLastBlock();\n       final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n           .getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       } else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // Find the target DatanodeStorageInfos. If not found because of invalid\n         // or empty DatanodeID/StorageID, the slot of same offset in dsInfos is\n         // null\n         final DatanodeStorageInfo[] dsInfos \u003d blockManager.getDatanodeManager().\n             getDatanodeStorageInfos(newtargets, newtargetstorages,\n                 \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                 src, oldBlock, newgenerationstamp, newlength);\n \n         if (closeFile \u0026\u0026 dsInfos !\u003d null) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c dsInfos.length; i++) {\n             if (dsInfos[i] !\u003d null) {\n               if(copyTruncate) {\n                 dsInfos[i].addBlock(truncatedBlock, truncatedBlock);\n               } else {\n                 Block bi \u003d new Block(storedBlock);\n                 if (storedBlock.isStriped()) {\n                   bi.setBlockId(bi.getBlockId() + i);\n                 }\n                 dsInfos[i].addBlock(storedBlock, bi);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         if(copyTruncate) {\n           iFile.convertLastBlockToUC(truncatedBlock, dsInfos);\n         } else {\n           iFile.convertLastBlockToUC(storedBlock, dsInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                 storedBlock, oldGenerationStamp, oldNumBytes,\n                 dsInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           closeFileCommitBlocks(src, iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           closeFileCommitBlocks(src, iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n+      blockManager.successfulBlockRecovery(storedBlock);\n     } finally {\n       writeUnlock(\"commitBlockSynchronization\");\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    final String src;\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      final INodeFile iFile \u003d getBlockCollection(storedBlock);\n      src \u003d iFile.getFullPathName();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + src + \", likely due to delayed block removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n          .getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      } else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // Find the target DatanodeStorageInfos. If not found because of invalid\n        // or empty DatanodeID/StorageID, the slot of same offset in dsInfos is\n        // null\n        final DatanodeStorageInfo[] dsInfos \u003d blockManager.getDatanodeManager().\n            getDatanodeStorageInfos(newtargets, newtargetstorages,\n                \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                src, oldBlock, newgenerationstamp, newlength);\n\n        if (closeFile \u0026\u0026 dsInfos !\u003d null) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c dsInfos.length; i++) {\n            if (dsInfos[i] !\u003d null) {\n              if(copyTruncate) {\n                dsInfos[i].addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                Block bi \u003d new Block(storedBlock);\n                if (storedBlock.isStriped()) {\n                  bi.setBlockId(bi.getBlockId() + i);\n                }\n                dsInfos[i].addBlock(storedBlock, bi);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, dsInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, dsInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                storedBlock, oldGenerationStamp, oldNumBytes,\n                dsInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          closeFileCommitBlocks(src, iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          closeFileCommitBlocks(src, iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n      blockManager.successfulBlockRecovery(storedBlock);\n    } finally {\n      writeUnlock(\"commitBlockSynchronization\");\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "ff0b99eafeda035ebe0dc82cfe689808047a8893": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10872. Add MutableRate metrics for FSNamesystemLock operations. Contributed by Erik Krogen.\n",
      "commitDate": "14/11/16 11:05 AM",
      "commitName": "ff0b99eafeda035ebe0dc82cfe689808047a8893",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "08/11/16 6:17 PM",
      "commitNameOld": "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 5.7,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,163 +1,163 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     final String src;\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       final INodeFile iFile \u003d getBlockCollection(storedBlock);\n       src \u003d iFile.getFullPathName();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + src + \", likely due to delayed block removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d iFile.getLastBlock();\n       final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n           .getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       } else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // Find the target DatanodeStorageInfos. If not found because of invalid\n         // or empty DatanodeID/StorageID, the slot of same offset in dsInfos is\n         // null\n         final DatanodeStorageInfo[] dsInfos \u003d blockManager.getDatanodeManager().\n             getDatanodeStorageInfos(newtargets, newtargetstorages,\n                 \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                 src, oldBlock, newgenerationstamp, newlength);\n \n         if (closeFile \u0026\u0026 dsInfos !\u003d null) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c dsInfos.length; i++) {\n             if (dsInfos[i] !\u003d null) {\n               if(copyTruncate) {\n                 dsInfos[i].addBlock(truncatedBlock, truncatedBlock);\n               } else {\n                 Block bi \u003d new Block(storedBlock);\n                 if (storedBlock.isStriped()) {\n                   bi.setBlockId(bi.getBlockId() + i);\n                 }\n                 dsInfos[i].addBlock(storedBlock, bi);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         if(copyTruncate) {\n           iFile.convertLastBlockToUC(truncatedBlock, dsInfos);\n         } else {\n           iFile.convertLastBlockToUC(storedBlock, dsInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                 storedBlock, oldGenerationStamp, oldNumBytes,\n                 dsInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           closeFileCommitBlocks(src, iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           closeFileCommitBlocks(src, iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n-      writeUnlock();\n+      writeUnlock(\"commitBlockSynchronization\");\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    final String src;\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      final INodeFile iFile \u003d getBlockCollection(storedBlock);\n      src \u003d iFile.getFullPathName();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + src + \", likely due to delayed block removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n          .getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      } else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // Find the target DatanodeStorageInfos. If not found because of invalid\n        // or empty DatanodeID/StorageID, the slot of same offset in dsInfos is\n        // null\n        final DatanodeStorageInfo[] dsInfos \u003d blockManager.getDatanodeManager().\n            getDatanodeStorageInfos(newtargets, newtargetstorages,\n                \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                src, oldBlock, newgenerationstamp, newlength);\n\n        if (closeFile \u0026\u0026 dsInfos !\u003d null) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c dsInfos.length; i++) {\n            if (dsInfos[i] !\u003d null) {\n              if(copyTruncate) {\n                dsInfos[i].addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                Block bi \u003d new Block(storedBlock);\n                if (storedBlock.isStriped()) {\n                  bi.setBlockId(bi.getBlockId() + i);\n                }\n                dsInfos[i].addBlock(storedBlock, bi);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, dsInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, dsInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                storedBlock, oldGenerationStamp, oldNumBytes,\n                dsInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          closeFileCommitBlocks(src, iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          closeFileCommitBlocks(src, iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock(\"commitBlockSynchronization\");\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "61ab0440f7eaff0f631cbae0378403912f88d7ad": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9173. Erasure Coding: Lease recovery for striped file. Contributed by Walter Su and Jing Zhao.\n\nChange-Id: I51703a61c9d8454f883028f3f6acb5729fde1b15\n",
      "commitDate": "18/12/15 3:57 PM",
      "commitName": "61ab0440f7eaff0f631cbae0378403912f88d7ad",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "15/12/15 10:47 AM",
      "commitNameOld": "8602692338d6f493647205e0241e4116211fab75",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 3.22,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,178 +1,163 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     final String src;\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       final INodeFile iFile \u003d getBlockCollection(storedBlock);\n       src \u003d iFile.getFullPathName();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + src + \", likely due to delayed block removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d iFile.getLastBlock();\n       final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n           .getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       } else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n-        // find the DatanodeDescriptor objects\n-        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n-            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n-        ArrayList\u003cString\u003e trimmedStorages \u003d\n-            new ArrayList\u003cString\u003e(newtargets.length);\n-        if (newtargets.length \u003e 0) {\n-          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n-            // try to get targetNode\n-            DatanodeDescriptor targetNode \u003d\n-                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n-            if (targetNode !\u003d null) {\n-              trimmedTargets.add(targetNode);\n-              trimmedStorages.add(newtargetstorages[i]);\n-            } else if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n-            }\n-          }\n-        }\n-        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n+        // Find the target DatanodeStorageInfos. If not found because of invalid\n+        // or empty DatanodeID/StorageID, the slot of same offset in dsInfos is\n+        // null\n+        final DatanodeStorageInfo[] dsInfos \u003d blockManager.getDatanodeManager().\n+            getDatanodeStorageInfos(newtargets, newtargetstorages,\n+                \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n+                src, oldBlock, newgenerationstamp, newlength);\n+\n+        if (closeFile \u0026\u0026 dsInfos !\u003d null) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n-          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n-            DatanodeStorageInfo storageInfo \u003d\n-                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n-            if (storageInfo !\u003d null) {\n+          for (int i \u003d 0; i \u003c dsInfos.length; i++) {\n+            if (dsInfos[i] !\u003d null) {\n               if(copyTruncate) {\n-                storageInfo.addBlock(truncatedBlock, truncatedBlock);\n+                dsInfos[i].addBlock(truncatedBlock, truncatedBlock);\n               } else {\n-                storageInfo.addBlock(storedBlock, storedBlock);\n+                Block bi \u003d new Block(storedBlock);\n+                if (storedBlock.isStriped()) {\n+                  bi.setBlockId(bi.getBlockId() + i);\n+                }\n+                dsInfos[i].addBlock(storedBlock, bi);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n-        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n-            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n-                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n-                trimmedStorages.toArray(new String[trimmedStorages.size()]),\n-                \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n-                src, oldBlock, newgenerationstamp, newlength);\n-\n         if(copyTruncate) {\n-          iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n+          iFile.convertLastBlockToUC(truncatedBlock, dsInfos);\n         } else {\n-          iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n+          iFile.convertLastBlockToUC(storedBlock, dsInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                 storedBlock, oldGenerationStamp, oldNumBytes,\n-                trimmedStorageInfos);\n+                dsInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           closeFileCommitBlocks(src, iFile, truncatedBlock);\n-          if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n+          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           closeFileCommitBlocks(src, iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    final String src;\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      final INodeFile iFile \u003d getBlockCollection(storedBlock);\n      src \u003d iFile.getFullPathName();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + src + \", likely due to delayed block removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n          .getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      } else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // Find the target DatanodeStorageInfos. If not found because of invalid\n        // or empty DatanodeID/StorageID, the slot of same offset in dsInfos is\n        // null\n        final DatanodeStorageInfo[] dsInfos \u003d blockManager.getDatanodeManager().\n            getDatanodeStorageInfos(newtargets, newtargetstorages,\n                \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                src, oldBlock, newgenerationstamp, newlength);\n\n        if (closeFile \u0026\u0026 dsInfos !\u003d null) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c dsInfos.length; i++) {\n            if (dsInfos[i] !\u003d null) {\n              if(copyTruncate) {\n                dsInfos[i].addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                Block bi \u003d new Block(storedBlock);\n                if (storedBlock.isStriped()) {\n                  bi.setBlockId(bi.getBlockId() + i);\n                }\n                dsInfos[i].addBlock(storedBlock, bi);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, dsInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, dsInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                storedBlock, oldGenerationStamp, oldNumBytes,\n                dsInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          closeFileCommitBlocks(src, iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          closeFileCommitBlocks(src, iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "132478e805ba0f955345217b8ad87c2d17cccb2d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9527. The return type of FSNamesystem.getBlockCollection should be changed to INodeFile.\n",
      "commitDate": "09/12/15 5:55 PM",
      "commitName": "132478e805ba0f955345217b8ad87c2d17cccb2d",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "04/12/15 9:47 AM",
      "commitNameOld": "3fa33b5c2c289ceaced30c6c5451f3569110459d",
      "commitAuthorOld": "Ming Ma",
      "daysBetweenCommits": 5.34,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,179 +1,178 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     final String src;\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n-      long bcId \u003d storedBlock.getBlockCollectionId();\n-      INodeFile iFile \u003d ((INode)getBlockCollection(bcId)).asFile();\n+      final INodeFile iFile \u003d getBlockCollection(storedBlock);\n       src \u003d iFile.getFullPathName();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + src + \", likely due to delayed block removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d iFile.getLastBlock();\n       final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n           .getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       } else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock, truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock, storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]),\n                 \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                 src, oldBlock, newgenerationstamp, newlength);\n \n         if(copyTruncate) {\n           iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                 storedBlock, oldGenerationStamp, oldNumBytes,\n                 trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           closeFileCommitBlocks(src, iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           closeFileCommitBlocks(src, iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    final String src;\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      final INodeFile iFile \u003d getBlockCollection(storedBlock);\n      src \u003d iFile.getFullPathName();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + src + \", likely due to delayed block removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n          .getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      } else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock, storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]),\n                \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                src, oldBlock, newgenerationstamp, newlength);\n\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                storedBlock, oldGenerationStamp, oldNumBytes,\n                trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          closeFileCommitBlocks(src, iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          closeFileCommitBlocks(src, iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "3fa33b5c2c289ceaced30c6c5451f3569110459d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9430 Remove waitForLoadingFSImage since checkNNStartup has ensured image loaded and namenode started. (Brahma Reddy Battula via mingma)\n",
      "commitDate": "04/12/15 9:47 AM",
      "commitName": "3fa33b5c2c289ceaced30c6c5451f3569110459d",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "01/12/15 4:09 PM",
      "commitNameOld": "a49cc74b4c72195dee1dfb6f9548e5e411dff553",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.74,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,180 +1,179 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     final String src;\n-    waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       long bcId \u003d storedBlock.getBlockCollectionId();\n       INodeFile iFile \u003d ((INode)getBlockCollection(bcId)).asFile();\n       src \u003d iFile.getFullPathName();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + src + \", likely due to delayed block removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d iFile.getLastBlock();\n       final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n           .getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       } else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock, truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock, storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]),\n                 \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                 src, oldBlock, newgenerationstamp, newlength);\n \n         if(copyTruncate) {\n           iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                 storedBlock, oldGenerationStamp, oldNumBytes,\n                 trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           closeFileCommitBlocks(src, iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           closeFileCommitBlocks(src, iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    final String src;\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      long bcId \u003d storedBlock.getBlockCollectionId();\n      INodeFile iFile \u003d ((INode)getBlockCollection(bcId)).asFile();\n      src \u003d iFile.getFullPathName();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + src + \", likely due to delayed block removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n          .getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      } else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock, storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]),\n                \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                src, oldBlock, newgenerationstamp, newlength);\n\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                storedBlock, oldGenerationStamp, oldNumBytes,\n                trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          closeFileCommitBlocks(src, iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          closeFileCommitBlocks(src, iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "0b18e5e8c69b40c9a446fff448d38e0dd10cb45e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6481. DatanodeManager#getDatanodeStorageInfos() should check the length of storageIDs. (Contributed by szetszwo)\n",
      "commitDate": "06/11/15 10:15 AM",
      "commitName": "0b18e5e8c69b40c9a446fff448d38e0dd10cb45e",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "03/11/15 5:16 PM",
      "commitNameOld": "194251c85250fcbe80a6ffee88b2cd4689334be3",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 2.71,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,178 +1,180 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n-    String src \u003d \"\";\n+    final String src;\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       long bcId \u003d storedBlock.getBlockCollectionId();\n       INodeFile iFile \u003d ((INode)getBlockCollection(bcId)).asFile();\n+      src \u003d iFile.getFullPathName();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n-            + iFile.getFullPathName() + \", likely due to delayed block\"\n-            + \" removal\");\n+            + src + \", likely due to delayed block removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d iFile.getLastBlock();\n       final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n           .getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       } else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock, truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock, storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n-                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n+                trimmedStorages.toArray(new String[trimmedStorages.size()]),\n+                \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n+                src, oldBlock, newgenerationstamp, newlength);\n+\n         if(copyTruncate) {\n           iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                 storedBlock, oldGenerationStamp, oldNumBytes,\n                 trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n-          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n+          closeFileCommitBlocks(src, iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n-          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n+          closeFileCommitBlocks(src, iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n-        src \u003d iFile.getFullPathName();\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    final String src;\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      long bcId \u003d storedBlock.getBlockCollectionId();\n      INodeFile iFile \u003d ((INode)getBlockCollection(bcId)).asFile();\n      src \u003d iFile.getFullPathName();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + src + \", likely due to delayed block removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n          .getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      } else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock, storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]),\n                \"src\u003d%s, oldBlock\u003d%s, newgenerationstamp\u003d%d, newlength\u003d%d\",\n                src, oldBlock, newgenerationstamp, newlength);\n\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                storedBlock, oldGenerationStamp, oldNumBytes,\n                trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          closeFileCommitBlocks(src, iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          closeFileCommitBlocks(src, iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "164cbe643988f878f0f4100a4de51783e5b6738e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8909. Erasure coding: update BlockInfoContiguousUC and BlockInfoStripedUC to use BlockUnderConstructionFeature. Contributed by Jing Zhao.\n",
      "commitDate": "27/08/15 1:02 AM",
      "commitName": "164cbe643988f878f0f4100a4de51783e5b6738e",
      "commitAuthor": "Walter Su",
      "commitDateOld": "24/08/15 12:59 PM",
      "commitNameOld": "6b6a63bbbda920315d3d24b61ed3344a78a981b6",
      "commitAuthorOld": "",
      "daysBetweenCommits": 2.5,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,177 +1,177 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d iFile.getLastBlock();\n-      final BlockInfoUnderConstruction uc \u003d (BlockInfoUnderConstruction)truncatedBlock;\n-      final long recoveryId \u003d uc.getBlockRecoveryId();\n+      final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n+          .getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       } else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock, truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock, storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                 storedBlock, oldGenerationStamp, oldNumBytes,\n                 trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      final long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n          .getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      } else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock, storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                storedBlock, oldGenerationStamp, oldNumBytes,\n                trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "4cbbfa2220e884e91bf18ad1cc2f3b11f895f8c9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8248. Store INodeId instead of the INodeFile object in BlockInfoContiguous. Contributed by Haohui Mai.\n",
      "commitDate": "26/08/15 6:14 PM",
      "commitName": "4cbbfa2220e884e91bf18ad1cc2f3b11f895f8c9",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "24/08/15 4:56 PM",
      "commitNameOld": "3b00eaea256d252be3361a7d9106b88756fcb9ba",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 2.05,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,177 +1,178 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n-      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n+      long bcId \u003d storedBlock.getBlockCollectionId();\n+      INodeFile iFile \u003d ((INode)getBlockCollection(bcId)).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d iFile.getLastBlock();\n       long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n           .getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                 oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      long bcId \u003d storedBlock.getBlockCollectionId();\n      INodeFile iFile \u003d ((INode)getBlockCollection(bcId)).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n          .getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "e535e0f05b5fbd087c93238deb888cc985254b4c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8801. Convert BlockInfoUnderConstruction as a feature. Contributed by Jing Zhao.\n",
      "commitDate": "17/08/15 11:28 AM",
      "commitName": "e535e0f05b5fbd087c93238deb888cc985254b4c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "17/08/15 10:15 AM",
      "commitNameOld": "a7862d5fe4c505f5d4b0c675438a971733f1f53a",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 0.05,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,177 +1,177 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n-    BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n+    BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n-      truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n-          .getLastBlock();\n-      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n+      truncatedBlock \u003d iFile.getLastBlock();\n+      long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n+          .getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n-          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n+          iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n         } else {\n-          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n+          iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                 oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      long recoveryId \u003d truncatedBlock.getUnderConstructionFeature()\n          .getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "f4c523b69ba55b1fd35e8995c3011a9f546ac835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8499. Refactor BlockInfo class hierarchy with static helper class. Contributed by Zhe Zhang.\"\n\nThis reverts commit c17439c2ddd921b63b1635e6f1cba634b8da8557.\n",
      "commitDate": "06/08/15 10:21 AM",
      "commitName": "f4c523b69ba55b1fd35e8995c3011a9f546ac835",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "06/08/15 10:21 AM",
      "commitNameOld": "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,177 +1,177 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n-    BlockInfoUnderConstruction truncatedBlock \u003d null;\n+    BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n-      truncatedBlock \u003d (BlockInfoUnderConstruction) iFile\n+      truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n           .getLastBlock();\n       long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                 oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n          .getLastBlock();\n      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\"\n\nThis reverts commit de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5.\n",
      "commitDate": "06/08/15 10:21 AM",
      "commitName": "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "05/08/15 10:40 PM",
      "commitNameOld": "cc71ad80e184fc6e5043729e8cfcf6a62ca3e71f",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.49,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,178 +1,177 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfoUnderConstruction truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d (BlockInfoUnderConstruction) iFile\n           .getLastBlock();\n       long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n-                storageInfo.addBlock(truncatedBlock, truncatedBlock);\n+                storageInfo.addBlock(truncatedBlock);\n               } else {\n-                storageInfo.addBlock(storedBlock, storedBlock);\n+                storageInfo.addBlock(storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n-            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n-                storedBlock, oldGenerationStamp, oldNumBytes,\n-                trimmedStorageInfos);\n+            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n+                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfoUnderConstruction truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d (BlockInfoUnderConstruction) iFile\n          .getLastBlock();\n      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\n",
      "commitDate": "26/06/15 10:49 AM",
      "commitName": "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/06/15 2:42 PM",
      "commitNameOld": "afe9ea3c12e1f5a71922400eadb642960bc87ca1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,177 +1,178 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfoUnderConstruction truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d (BlockInfoUnderConstruction) iFile\n           .getLastBlock();\n       long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n-                storageInfo.addBlock(truncatedBlock);\n+                storageInfo.addBlock(truncatedBlock, truncatedBlock);\n               } else {\n-                storageInfo.addBlock(storedBlock);\n+                storageInfo.addBlock(storedBlock, storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n-            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n-                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n+            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n+                storedBlock, oldGenerationStamp, oldNumBytes,\n+                trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfoUnderConstruction truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d (BlockInfoUnderConstruction) iFile\n          .getLastBlock();\n      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock, storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                storedBlock, oldGenerationStamp, oldNumBytes,\n                trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "c17439c2ddd921b63b1635e6f1cba634b8da8557": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8499. Refactor BlockInfo class hierarchy with static helper class. Contributed by Zhe Zhang.\n",
      "commitDate": "12/06/15 11:38 AM",
      "commitName": "c17439c2ddd921b63b1635e6f1cba634b8da8557",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "12/06/15 11:17 AM",
      "commitNameOld": "12b5b06c063d93e6c683c9b6fac9a96912f59e59",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,177 +1,177 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n-    BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n+    BlockInfoUnderConstruction truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n-      truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n+      truncatedBlock \u003d (BlockInfoUnderConstruction) iFile\n           .getLastBlock();\n       long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                 oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfoUnderConstruction truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d (BlockInfoUnderConstruction) iFile\n          .getLastBlock();\n      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "4928f5473394981829e5ffd4b16ea0801baf5c45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8482. Rename BlockInfoContiguous to BlockInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "27/05/15 3:42 PM",
      "commitName": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "21/05/15 8:08 AM",
      "commitNameOld": "2b6bcfdafa91223a4116e3e9304579f5f91dccac",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 6.32,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,177 +1,177 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n-      final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n+      final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n           .getLastBlock();\n       long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                 oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n          .getLastBlock();\n      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "51ea117f883f9c049de58987dc66e07e71a68ee4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8355. Erasure Coding: Refactor BlockInfo and BlockInfoUnderConstruction. Contributed by Tsz Wo Nicholas Sze.\n",
      "commitDate": "26/05/15 12:01 PM",
      "commitName": "51ea117f883f9c049de58987dc66e07e71a68ee4",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 12:01 PM",
      "commitNameOld": "ac97edd1abcca2be93aa3a8dcdc642734c7c00ab",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,176 +1,177 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d iFile.getLastBlock();\n-      long recoveryId \u003d BlockInfo.getBlockRecoveryId(truncatedBlock);\n+      final BlockInfoUnderConstruction uc \u003d (BlockInfoUnderConstruction)truncatedBlock;\n+      final long recoveryId \u003d uc.getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       } else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock, truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock, storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                 storedBlock, oldGenerationStamp, oldNumBytes,\n                 trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      final BlockInfoUnderConstruction uc \u003d (BlockInfoUnderConstruction)truncatedBlock;\n      final long recoveryId \u003d uc.getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      } else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock, storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                storedBlock, oldGenerationStamp, oldNumBytes,\n                trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "544f75d6512fefd0e36f24a35e6b7472ca7bf301": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7936. Erasure coding: resolving conflicts when merging with HDFS-7903, HDFS-7435 and HDFS-7930 (this commit is for HDFS-7930 only)\n",
      "commitDate": "26/05/15 11:43 AM",
      "commitName": "544f75d6512fefd0e36f24a35e6b7472ca7bf301",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "26/05/15 11:41 AM",
      "commitNameOld": "a38a37c63417a3b19dcdf98251af196c9d7b8c31",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,175 +1,176 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d iFile.getLastBlock();\n       long recoveryId \u003d BlockInfo.getBlockRecoveryId(truncatedBlock);\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       } else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock, truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock, storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n-            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n-                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n+            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n+                storedBlock, oldGenerationStamp, oldNumBytes,\n+                trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      long recoveryId \u003d BlockInfo.getBlockRecoveryId(truncatedBlock);\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      } else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock, storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),\n                storedBlock, oldGenerationStamp, oldNumBytes,\n                trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7749. Erasure Coding: Add striped block support in INodeFile. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:07 AM",
      "commitName": "9f2f583f401189c3f4a2687795a9e3e0b288322b",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:07 AM",
      "commitNameOld": "ba9371492036983a9899398907ab41fe548f29b3",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,177 +1,175 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n-    BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n+    BlockInfo truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n-      final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n+      final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n-      truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n-          .getLastBlock();\n-      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n+      truncatedBlock \u003d iFile.getLastBlock();\n+      long recoveryId \u003d BlockInfo.getBlockRecoveryId(truncatedBlock);\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n-      }\n-      else {\n+      } else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock, truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock, storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n-          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n+          iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n         } else {\n-          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n+          iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                 oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n-          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n+          if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfo truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d iFile.getLastBlock();\n      long recoveryId \u003d BlockInfo.getBlockRecoveryId(truncatedBlock);\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      } else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock, storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.convertLastBlockToUC(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.convertLastBlockToUC(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot((BlockInfoContiguous) storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "ba9371492036983a9899398907ab41fe548f29b3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7716. Erasure Coding: extend BlockInfo to handle EC info. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:07 AM",
      "commitName": "ba9371492036983a9899398907ab41fe548f29b3",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:03 AM",
      "commitNameOld": "bc2833b1c91e107d090619d755c584f6eae82327",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,177 +1,177 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n           .getLastBlock();\n       long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n-                storageInfo.addBlock(truncatedBlock);\n+                storageInfo.addBlock(truncatedBlock, truncatedBlock);\n               } else {\n-                storageInfo.addBlock(storedBlock);\n+                storageInfo.addBlock(storedBlock, storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                 oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n          .getLastBlock();\n      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock, truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock, storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "e5afac5896a1a88e152746598527d91f73cbb724": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8394. Move getAdditionalBlock() and related functionalities into a separate class. Contributed by Haohui Mai.\n",
      "commitDate": "15/05/15 7:09 PM",
      "commitName": "e5afac5896a1a88e152746598527d91f73cbb724",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "14/05/15 10:37 PM",
      "commitNameOld": "3bef7c80a97709b367781180b2e11fc50653d3c8",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.86,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,177 +1,177 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n           .getLastBlock();\n       long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                 oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n-        persistBlocks(src, iFile, false);\n+        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n          .getLastBlock();\n      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        FSDirWriteFileOp.persistBlocks(dir, src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8327. Compute storage type quotas in INodeFile.computeQuotaDeltaForTruncate(). Contributed by Haohui Mai.\n",
      "commitDate": "08/05/15 11:09 PM",
      "commitName": "02a4a22b9c0e22c2e7dd6ec85edd5c5a167fe19f",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "08/05/15 11:04 PM",
      "commitNameOld": "00fe1ed3a4b3ee35fe24be257ec36445d2f44d63",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,177 +1,177 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     boolean copyTruncate \u003d false;\n     BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n           .getLastBlock();\n       long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n       copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n-        boolean remove \u003d iFile.removeLastBlock(blockToDel);\n+        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                 oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         persistBlocks(src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n               : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n          .getLastBlock();\n      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel) !\u003d null;\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "262c1bc3398ce2ede03f9d86fc97c35ca7a8e9db": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8217. During block recovery for truncate Log new Block Id in case of copy-on-truncate is true. (Contributed by Vinayakumar B)\n",
      "commitDate": "23/04/15 11:46 PM",
      "commitName": "262c1bc3398ce2ede03f9d86fc97c35ca7a8e9db",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "23/04/15 11:47 AM",
      "commitNameOld": "26971e52ae65590e618a23621be244e588845adc",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.5,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,175 +1,177 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n+    boolean copyTruncate \u003d false;\n+    BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n-      BlockInfoContiguousUnderConstruction truncatedBlock \u003d\n-          (BlockInfoContiguousUnderConstruction) iFile.getLastBlock();\n+      truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n+          .getLastBlock();\n       long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n-      boolean copyTruncate \u003d\n-          truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n+      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                 oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         persistBlocks(src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n-          + \", newgenerationstamp\u003d\" + newgenerationstamp\n+          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n+              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    boolean copyTruncate \u003d false;\n    BlockInfoContiguousUnderConstruction truncatedBlock \u003d null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      truncatedBlock \u003d (BlockInfoContiguousUnderConstruction) iFile\n          .getLastBlock();\n      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n      copyTruncate \u003d truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + (copyTruncate ? \", newBlock\u003d\" + truncatedBlock\n              : \", newgenerationstamp\u003d\" + newgenerationstamp)\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "997408eaaceef20b053ee7344468e28cb9a1379b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8133. Improve readability of deleted block check (Daryn Sharp via Colin P. McCabe)\n",
      "commitDate": "21/04/15 11:43 AM",
      "commitName": "997408eaaceef20b053ee7344468e28cb9a1379b",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "20/04/15 12:36 AM",
      "commitNameOld": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.46,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,176 +1,175 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n       final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n-      BlockCollection blockCollection \u003d storedBlock.getBlockCollection();\n-      if (blockCollection \u003d\u003d null) {\n+      if (storedBlock.isDeleted()) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n-      INodeFile iFile \u003d ((INode)blockCollection).asFile();\n+      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       BlockInfoContiguousUnderConstruction truncatedBlock \u003d\n           (BlockInfoContiguousUnderConstruction) iFile.getLastBlock();\n       long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n       boolean copyTruncate \u003d\n           truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n           if (closeFile) {\n             blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                 oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n           }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         persistBlocks(src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      if (storedBlock.isDeleted()) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      BlockInfoContiguousUnderConstruction truncatedBlock \u003d\n          (BlockInfoContiguousUnderConstruction) iFile.getLastBlock();\n      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n      boolean copyTruncate \u003d\n          truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "e37ca221bf4e9ae5d5e667d8ca284df9fdb33199": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7930. commitBlockSynchronization() does not remove locations. (yliu)\n",
      "commitDate": "19/03/15 8:23 AM",
      "commitName": "e37ca221bf4e9ae5d5e667d8ca284df9fdb33199",
      "commitAuthor": "yliu",
      "commitDateOld": "18/03/15 6:53 PM",
      "commitNameOld": "c7c71cdba50cb7d8282622cd496cc913c80cff54",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.56,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,172 +1,176 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n+      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n+      final long oldNumBytes \u003d storedBlock.getNumBytes();\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       BlockCollection blockCollection \u003d storedBlock.getBlockCollection();\n       if (blockCollection \u003d\u003d null) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)blockCollection).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       BlockInfoContiguousUnderConstruction truncatedBlock \u003d\n           (BlockInfoContiguousUnderConstruction) iFile.getLastBlock();\n       long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n       boolean copyTruncate \u003d\n           truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n-        // There should be no locations in the blockManager till now because the\n-        // file is underConstruction\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n+          if (closeFile) {\n+            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n+                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n+          }\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         persistBlocks(src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      final long oldGenerationStamp \u003d storedBlock.getGenerationStamp();\n      final long oldNumBytes \u003d storedBlock.getNumBytes();\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      BlockCollection blockCollection \u003d storedBlock.getBlockCollection();\n      if (blockCollection \u003d\u003d null) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)blockCollection).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      BlockInfoContiguousUnderConstruction truncatedBlock \u003d\n          (BlockInfoContiguousUnderConstruction) iFile.getLastBlock();\n      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n      boolean copyTruncate \u003d\n          truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n          if (closeFile) {\n            blockManager.markBlockReplicasAsCorrupt(storedBlock,\n                oldGenerationStamp, oldNumBytes, trimmedStorageInfos);\n          }\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7743. Code cleanup of BlockInfo and rename BlockInfo to BlockInfoContiguous. Contributed by Jing Zhao.\n",
      "commitDate": "08/02/15 11:51 AM",
      "commitName": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "07/02/15 3:21 PM",
      "commitNameOld": "8f7d4bb09f760780dd193c97796ebf4d22cfd2d7",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 0.85,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,172 +1,172 @@\n   void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n-      final BlockInfo storedBlock \u003d getStoredBlock(\n+      final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       BlockCollection blockCollection \u003d storedBlock.getBlockCollection();\n       if (blockCollection \u003d\u003d null) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)blockCollection).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n           iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n-      BlockInfoUnderConstruction truncatedBlock \u003d\n-          (BlockInfoUnderConstruction) iFile.getLastBlock();\n+      BlockInfoContiguousUnderConstruction truncatedBlock \u003d\n+          (BlockInfoContiguousUnderConstruction) iFile.getLastBlock();\n       long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n       boolean copyTruncate \u003d\n           truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         if(!copyTruncate) {\n           storedBlock.setGenerationStamp(newgenerationstamp);\n           storedBlock.setNumBytes(newlength);\n         }\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               if(copyTruncate) {\n                 storageInfo.addBlock(truncatedBlock);\n               } else {\n                 storageInfo.addBlock(storedBlock);\n               }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         if(copyTruncate) {\n           iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n         } else {\n           iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n         }\n       }\n \n       if (closeFile) {\n         if(copyTruncate) {\n           src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n           if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n             blockManager.removeBlock(storedBlock);\n           }\n         } else {\n           src \u003d closeFileCommitBlocks(iFile, storedBlock);\n         }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         persistBlocks(src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfoContiguous storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      BlockCollection blockCollection \u003d storedBlock.getBlockCollection();\n      if (blockCollection \u003d\u003d null) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)blockCollection).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      BlockInfoContiguousUnderConstruction truncatedBlock \u003d\n          (BlockInfoContiguousUnderConstruction) iFile.getLastBlock();\n      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n      boolean copyTruncate \u003d\n          truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "08ac06283a3e9bf0d49d873823aabd419b08e41f": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7056. Snapshot support for truncate. Contributed by Konstantin Shvachko and Plamen Jeliazkov.",
      "commitDate": "13/01/15 12:24 AM",
      "commitName": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
      "commitAuthor": "Konstantin V Shvachko",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7056. Snapshot support for truncate. Contributed by Konstantin Shvachko and Plamen Jeliazkov.",
          "commitDate": "13/01/15 12:24 AM",
          "commitName": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
          "commitAuthor": "Konstantin V Shvachko",
          "commitDateOld": "12/01/15 10:50 PM",
          "commitNameOld": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
          "commitAuthorOld": "Plamen Jeliazkov",
          "daysBetweenCommits": 0.07,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,151 +1,172 @@\n-  void commitBlockSynchronization(ExtendedBlock lastblock,\n+  void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n-    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n+    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n-          ExtendedBlock.getLocalBlock(lastblock));\n+          ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n+            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n-          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n+          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       BlockCollection blockCollection \u003d storedBlock.getBlockCollection();\n       if (blockCollection \u003d\u003d null) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)blockCollection).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n-      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n+      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n+          iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n+          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n-      long recoveryId \u003d\n-        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n+      BlockInfoUnderConstruction truncatedBlock \u003d\n+          (BlockInfoUnderConstruction) iFile.getLastBlock();\n+      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n+      boolean copyTruncate \u003d\n+          truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n-                              + recoveryId + \" for block \" + lastblock); \n+                              + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n-        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n+        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel);\n         if (remove) {\n-          blockManager.removeBlockFromMap(storedBlock);\n+          blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n-        storedBlock.setGenerationStamp(newgenerationstamp);\n-        storedBlock.setNumBytes(newlength);\n+        if(!copyTruncate) {\n+          storedBlock.setGenerationStamp(newgenerationstamp);\n+          storedBlock.setNumBytes(newlength);\n+        }\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n-              storageInfo.addBlock(storedBlock);\n+              if(copyTruncate) {\n+                storageInfo.addBlock(truncatedBlock);\n+              } else {\n+                storageInfo.addBlock(storedBlock);\n+              }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n-        iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n+        if(copyTruncate) {\n+          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n+        } else {\n+          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n+        }\n       }\n \n       if (closeFile) {\n-        src \u003d closeFileCommitBlocks(iFile, storedBlock);\n+        if(copyTruncate) {\n+          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n+          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n+            blockManager.removeBlock(storedBlock);\n+          }\n+        } else {\n+          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n+        }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         persistBlocks(src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n-      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n+      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n-      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n+      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      BlockCollection blockCollection \u003d storedBlock.getBlockCollection();\n      if (blockCollection \u003d\u003d null) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)blockCollection).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      BlockInfoUnderConstruction truncatedBlock \u003d\n          (BlockInfoUnderConstruction) iFile.getLastBlock();\n      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n      boolean copyTruncate \u003d\n          truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[lastblock-ExtendedBlock, newgenerationstamp-long, newlength-long, closeFile-boolean, deleteblock-boolean, newtargets-DatanodeID[], newtargetstorages-String[]]",
            "newValue": "[oldBlock-ExtendedBlock, newgenerationstamp-long, newlength-long, closeFile-boolean, deleteblock-boolean, newtargets-DatanodeID[], newtargetstorages-String[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7056. Snapshot support for truncate. Contributed by Konstantin Shvachko and Plamen Jeliazkov.",
          "commitDate": "13/01/15 12:24 AM",
          "commitName": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
          "commitAuthor": "Konstantin V Shvachko",
          "commitDateOld": "12/01/15 10:50 PM",
          "commitNameOld": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
          "commitAuthorOld": "Plamen Jeliazkov",
          "daysBetweenCommits": 0.07,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,151 +1,172 @@\n-  void commitBlockSynchronization(ExtendedBlock lastblock,\n+  void commitBlockSynchronization(ExtendedBlock oldBlock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages) throws IOException {\n-    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n+    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n-          ExtendedBlock.getLocalBlock(lastblock));\n+          ExtendedBlock.getLocalBlock(oldBlock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n+            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n           }\n           return;\n         } else {\n-          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n+          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n         }\n       }\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       BlockCollection blockCollection \u003d storedBlock.getBlockCollection();\n       if (blockCollection \u003d\u003d null) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)blockCollection).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n-      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n+      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n+          iFile.getLastBlock().isComplete()) {\n         if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n+          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n-      long recoveryId \u003d\n-        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n+      BlockInfoUnderConstruction truncatedBlock \u003d\n+          (BlockInfoUnderConstruction) iFile.getLastBlock();\n+      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n+      boolean copyTruncate \u003d\n+          truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n-                              + recoveryId + \" for block \" + lastblock); \n+                              + recoveryId + \" for block \" + oldBlock);\n       }\n \n       if (deleteblock) {\n-        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n+        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel);\n         if (remove) {\n-          blockManager.removeBlockFromMap(storedBlock);\n+          blockManager.removeBlock(storedBlock);\n         }\n       }\n       else {\n         // update last block\n-        storedBlock.setGenerationStamp(newgenerationstamp);\n-        storedBlock.setNumBytes(newlength);\n+        if(!copyTruncate) {\n+          storedBlock.setGenerationStamp(newgenerationstamp);\n+          storedBlock.setNumBytes(newlength);\n+        }\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n-              storageInfo.addBlock(storedBlock);\n+              if(copyTruncate) {\n+                storageInfo.addBlock(truncatedBlock);\n+              } else {\n+                storageInfo.addBlock(storedBlock);\n+              }\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n-        iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n+        if(copyTruncate) {\n+          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n+        } else {\n+          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n+        }\n       }\n \n       if (closeFile) {\n-        src \u003d closeFileCommitBlocks(iFile, storedBlock);\n+        if(copyTruncate) {\n+          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n+          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n+            blockManager.removeBlock(storedBlock);\n+          }\n+        } else {\n+          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n+        }\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         persistBlocks(src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n-      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n+      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n-      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n+      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void commitBlockSynchronization(ExtendedBlock oldBlock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(oldBlock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + oldBlock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + oldBlock + \") not found\");\n        }\n      }\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      BlockCollection blockCollection \u003d storedBlock.getBlockCollection();\n      if (blockCollection \u003d\u003d null) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)blockCollection).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) \u0026\u0026\n          iFile.getLastBlock().isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + oldBlock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      BlockInfoUnderConstruction truncatedBlock \u003d\n          (BlockInfoUnderConstruction) iFile.getLastBlock();\n      long recoveryId \u003d truncatedBlock.getBlockRecoveryId();\n      boolean copyTruncate \u003d\n          truncatedBlock.getBlockId() !\u003d storedBlock.getBlockId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + oldBlock);\n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(oldBlock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlock(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        if(!copyTruncate) {\n          storedBlock.setGenerationStamp(newgenerationstamp);\n          storedBlock.setNumBytes(newlength);\n        }\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              if(copyTruncate) {\n                storageInfo.addBlock(truncatedBlock);\n              } else {\n                storageInfo.addBlock(storedBlock);\n              }\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        if(copyTruncate) {\n          iFile.setLastBlock(truncatedBlock, trimmedStorageInfos);\n        } else {\n          iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n        }\n      }\n\n      if (closeFile) {\n        if(copyTruncate) {\n          src \u003d closeFileCommitBlocks(iFile, truncatedBlock);\n          if(!iFile.isBlockInLatestSnapshot(storedBlock)) {\n            blockManager.removeBlock(storedBlock);\n          }\n        } else {\n          src \u003d closeFileCommitBlocks(iFile, storedBlock);\n        }\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(oldBlock\u003d\" + oldBlock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + oldBlock + \") successful\");\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19": {
      "type": "Yexceptionschange",
      "commitMessage": "HDFS-7059. Avoid resolving path multiple times. Contributed by Jing Zhao.\n",
      "commitDate": "12/12/14 3:13 PM",
      "commitName": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/12/14 11:51 AM",
      "commitNameOld": "46612c7a5135d20b20403780b47dd00654aab057",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.14,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,152 +1,151 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n-      String[] newtargetstorages)\n-      throws IOException, UnresolvedLinkException {\n+      String[] newtargetstorages) throws IOException {\n     LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n         }\n       }\n       //\n       // The implementation of delete operation (see @deleteInternal method)\n       // first removes the file paths from namespace, and delays the removal\n       // of blocks to later time for better performance. When\n       // commitBlockSynchronization (this method) is called in between, the\n       // blockCollection of storedBlock could have been assigned to null by\n       // the delete operation, throw IOException here instead of NPE; if the\n       // file path is already removed from namespace by the delete operation,\n       // throw FileNotFoundException here, so not to proceed to the end of\n       // this method to add a CloseOp to the edit log for an already deleted\n       // file (See HDFS-6825).\n       //\n       BlockCollection blockCollection \u003d storedBlock.getBlockCollection();\n       if (blockCollection \u003d\u003d null) {\n         throw new IOException(\"The blockCollection of \" + storedBlock\n             + \" is null, likely because the file owning this block was\"\n             + \" deleted and the block removal is delayed\");\n       }\n       INodeFile iFile \u003d ((INode)blockCollection).asFile();\n       if (isFileDeleted(iFile)) {\n         throw new FileNotFoundException(\"File not found: \"\n             + iFile.getFullPathName() + \", likely due to delayed block\"\n             + \" removal\");\n       }\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlockFromMap(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               storageInfo.addBlock(storedBlock);\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n       }\n \n       if (closeFile) {\n         src \u003d closeFileCommitBlocks(iFile, storedBlock);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         persistBlocks(src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages) throws IOException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n        }\n      }\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      BlockCollection blockCollection \u003d storedBlock.getBlockCollection();\n      if (blockCollection \u003d\u003d null) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)blockCollection).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              storageInfo.addBlock(storedBlock);\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n      }\n\n      if (closeFile) {\n        src \u003d closeFileCommitBlocks(iFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldValue": "[IOException, UnresolvedLinkException]",
        "newValue": "[IOException]"
      }
    },
    "bd616552cce7e46d1cc27ad9aba38f96a1f4c29c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6825. Edit log corruption due to delayed block removal. Contributed by Yongjun Zhang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1618684 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/14 11:08 AM",
      "commitName": "bd616552cce7e46d1cc27ad9aba38f96a1f4c29c",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "11/08/14 2:28 PM",
      "commitNameOld": "80691b073fe7c104a8684c0a8900a1657bcdc03f",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 6.86,
      "commitsBetweenForRepo": 68,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,129 +1,152 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n         }\n       }\n-      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n+      //\n+      // The implementation of delete operation (see @deleteInternal method)\n+      // first removes the file paths from namespace, and delays the removal\n+      // of blocks to later time for better performance. When\n+      // commitBlockSynchronization (this method) is called in between, the\n+      // blockCollection of storedBlock could have been assigned to null by\n+      // the delete operation, throw IOException here instead of NPE; if the\n+      // file path is already removed from namespace by the delete operation,\n+      // throw FileNotFoundException here, so not to proceed to the end of\n+      // this method to add a CloseOp to the edit log for an already deleted\n+      // file (See HDFS-6825).\n+      //\n+      BlockCollection blockCollection \u003d storedBlock.getBlockCollection();\n+      if (blockCollection \u003d\u003d null) {\n+        throw new IOException(\"The blockCollection of \" + storedBlock\n+            + \" is null, likely because the file owning this block was\"\n+            + \" deleted and the block removal is delayed\");\n+      }\n+      INodeFile iFile \u003d ((INode)blockCollection).asFile();\n+      if (isFileDeleted(iFile)) {\n+        throw new FileNotFoundException(\"File not found: \"\n+            + iFile.getFullPathName() + \", likely due to delayed block\"\n+            + \" removal\");\n+      }\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlockFromMap(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             DatanodeStorageInfo storageInfo \u003d\n                 trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n             if (storageInfo !\u003d null) {\n               storageInfo.addBlock(storedBlock);\n             }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n       }\n \n       if (closeFile) {\n         src \u003d closeFileCommitBlocks(iFile, storedBlock);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         persistBlocks(src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n        }\n      }\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      BlockCollection blockCollection \u003d storedBlock.getBlockCollection();\n      if (blockCollection \u003d\u003d null) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile \u003d ((INode)blockCollection).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              storageInfo.addBlock(storedBlock);\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n      }\n\n      if (closeFile) {\n        src \u003d closeFileCommitBlocks(iFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "45db4d204b796eee6dd0e39d3cc94b70c47028d4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6794. Update BlockManager methods to use DatanodeStorageInfo where possible. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/08/14 9:58 AM",
      "commitName": "45db4d204b796eee6dd0e39d3cc94b70c47028d4",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "30/07/14 10:49 AM",
      "commitNameOld": "535fe14dedbf919442ec03ac573315c7a16a6dbe",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 1.96,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,126 +1,129 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n         }\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlockFromMap(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n-            trimmedTargets.get(i).addBlock(\n-              trimmedStorages.get(i), storedBlock);\n+            DatanodeStorageInfo storageInfo \u003d\n+                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n+            if (storageInfo !\u003d null) {\n+              storageInfo.addBlock(storedBlock);\n+            }\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n       }\n \n       if (closeFile) {\n         src \u003d closeFileCommitBlocks(iFile, storedBlock);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         persistBlocks(src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo \u003d\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo !\u003d null) {\n              storageInfo.addBlock(storedBlock);\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n      }\n\n      if (closeFile) {\n        src \u003d closeFileCommitBlocks(iFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "a4e0ff5e052abad498595ee198b49c5310c9ec0d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6480. Move waitForReady() from FSDirectory to FSNamesystem. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603705 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/06/14 9:13 PM",
      "commitName": "a4e0ff5e052abad498595ee198b49c5310c9ec0d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "17/06/14 6:00 PM",
      "commitNameOld": "8e8a769e7f5ce806ffdf584f017512ab58cd84e8",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.13,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,125 +1,126 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n+    waitForLoadingFSImage();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n         }\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlockFromMap(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             trimmedTargets.get(i).addBlock(\n               trimmedStorages.get(i), storedBlock);\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n       }\n \n       if (closeFile) {\n         src \u003d closeFileCommitBlocks(iFile, storedBlock);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d iFile.getFullPathName();\n         persistBlocks(src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            trimmedTargets.get(i).addBlock(\n              trimmedStorages.get(i), storedBlock);\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n      }\n\n      if (closeFile) {\n        src \u003d closeFileCommitBlocks(iFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6315. Decouple recording edit logs from FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601960 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/06/14 10:22 AM",
      "commitName": "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "03/06/14 11:33 AM",
      "commitNameOld": "02fcb6b6bae7c3fe2a10b00b2a563e4098ff225e",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 7.95,
      "commitsBetweenForRepo": 35,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,124 +1,125 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n         }\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n         boolean remove \u003d iFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlockFromMap(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         ArrayList\u003cString\u003e trimmedStorages \u003d\n             new ArrayList\u003cString\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n             if (targetNode !\u003d null) {\n               trimmedTargets.add(targetNode);\n               trimmedStorages.add(newtargetstorages[i]);\n             } else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n             trimmedTargets.get(i).addBlock(\n               trimmedStorages.get(i), storedBlock);\n           }\n         }\n \n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeStorageInfo[] trimmedStorageInfos \u003d\n             blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                 trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                 trimmedStorages.toArray(new String[trimmedStorages.size()]));\n         iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n       }\n \n       if (closeFile) {\n         src \u003d closeFileCommitBlocks(iFile, storedBlock);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n-        src \u003d persistBlocks(iFile, false);\n+        src \u003d iFile.getFullPathName();\n+        persistBlocks(src, iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        ArrayList\u003cDatanodeDescriptor\u003e trimmedTargets \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        ArrayList\u003cString\u003e trimmedStorages \u003d\n            new ArrayList\u003cString\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (int i \u003d 0; i \u003c newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode !\u003d null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c trimmedTargets.size(); i++) {\n            trimmedTargets.get(i).addBlock(\n              trimmedStorages.get(i), storedBlock);\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos \u003d\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n      }\n\n      if (closeFile) {\n        src \u003d closeFileCommitBlocks(iFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "ce68f410b05a58ad05965f32ad7f5b246b363a75": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5285. Flatten INodeFile hierarchy: Replace INodeFileUnderConstruction and INodeFileUnderConstructionWithSnapshot with FileUnderContructionFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544389 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/11/13 5:39 PM",
      "commitName": "ce68f410b05a58ad05965f32ad7f5b246b363a75",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "21/11/13 9:12 AM",
      "commitNameOld": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 0.35,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,119 +1,117 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       checkNameNodeSafeMode(\n           \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n         }\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n-      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n-\n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n-        boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n+        boolean remove \u003d iFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlockFromMap(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         List\u003cDatanodeDescriptor\u003e targetList \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (DatanodeID newtarget : newtargets) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtarget);\n             if (targetNode !\u003d null)\n               targetList.add(targetNode);\n             else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtarget + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !targetList.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (DatanodeDescriptor targetNode : targetList) {\n             targetNode.addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeDescriptor[] targetArray \u003d\n             new DatanodeDescriptor[targetList.size()];\n-        pendingFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n+        iFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n       }\n \n       if (closeFile) {\n-        src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n+        src \u003d closeFileCommitBlocks(iFile, storedBlock);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n-        src \u003d persistBlocks(pendingFile, false);\n+        src \u003d persistBlocks(iFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove \u003d iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        List\u003cDatanodeDescriptor\u003e targetList \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (DatanodeID newtarget : newtargets) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtarget);\n            if (targetNode !\u003d null)\n              targetList.add(targetNode);\n            else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtarget + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !targetList.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (DatanodeDescriptor targetNode : targetList) {\n            targetNode.addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeDescriptor[] targetArray \u003d\n            new DatanodeDescriptor[targetList.size()];\n        iFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n      }\n\n      if (closeFile) {\n        src \u003d closeFileCommitBlocks(iFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d persistBlocks(iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a": {
      "type": "Ybodychange",
      "commitMessage": "merge trunk to branch HDFS-4949\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532952 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/10/13 7:14 PM",
      "commitName": "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "16/10/13 3:15 PM",
      "commitNameOld": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 0.17,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,122 +1,119 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n-      if (isInSafeMode()) {\n-        throw new SafeModeException(\n-          \"Cannot commitBlockSynchronization while in safe mode\",\n-          safeMode);\n-      }\n+      checkNameNodeSafeMode(\n+          \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n         }\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n         boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlockFromMap(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         List\u003cDatanodeDescriptor\u003e targetList \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (DatanodeID newtarget : newtargets) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtarget);\n             if (targetNode !\u003d null)\n               targetList.add(targetNode);\n             else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtarget + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !targetList.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (DatanodeDescriptor targetNode : targetList) {\n             targetNode.addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeDescriptor[] targetArray \u003d\n             new DatanodeDescriptor[targetList.size()];\n         pendingFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n       }\n \n       if (closeFile) {\n         src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d persistBlocks(pendingFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        List\u003cDatanodeDescriptor\u003e targetList \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (DatanodeID newtarget : newtargets) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtarget);\n            if (targetNode !\u003d null)\n              targetList.add(targetNode);\n            else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtarget + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !targetList.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (DatanodeDescriptor targetNode : targetList) {\n            targetNode.addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeDescriptor[] targetArray \u003d\n            new DatanodeDescriptor[targetList.size()];\n        pendingFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n      }\n\n      if (closeFile) {\n        src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d persistBlocks(pendingFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "1fe1942328856dd832e9f94fb56a40ab3d810870": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5291. Standby namenode after transition to active goes into safemode. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1530112 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/10/13 4:58 PM",
      "commitName": "1fe1942328856dd832e9f94fb56a40ab3d810870",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "06/10/13 11:39 AM",
      "commitNameOld": "7317e97bd72ca30f5db37fa94389dbdb52ae079e",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.22,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,122 +1,119 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n-      if (isInSafeMode()) {\n-        throw new SafeModeException(\n-          \"Cannot commitBlockSynchronization while in safe mode\",\n-          safeMode);\n-      }\n+      checkNameNodeSafeMode(\n+          \"Cannot commitBlockSynchronization while in safe mode\");\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n         }\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n         boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlockFromMap(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         List\u003cDatanodeDescriptor\u003e targetList \u003d\n             new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n           for (DatanodeID newtarget : newtargets) {\n             // try to get targetNode\n             DatanodeDescriptor targetNode \u003d\n                 blockManager.getDatanodeManager().getDatanode(newtarget);\n             if (targetNode !\u003d null)\n               targetList.add(targetNode);\n             else if (LOG.isDebugEnabled()) {\n               LOG.debug(\"DatanodeDescriptor (\u003d\" + newtarget + \") not found\");\n             }\n           }\n         }\n         if ((closeFile) \u0026\u0026 !targetList.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (DatanodeDescriptor targetNode : targetList) {\n             targetNode.addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         DatanodeDescriptor[] targetArray \u003d\n             new DatanodeDescriptor[targetList.size()];\n         pendingFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n       }\n \n       if (closeFile) {\n         src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d persistBlocks(pendingFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        List\u003cDatanodeDescriptor\u003e targetList \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (DatanodeID newtarget : newtargets) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtarget);\n            if (targetNode !\u003d null)\n              targetList.add(targetNode);\n            else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtarget + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !targetList.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (DatanodeDescriptor targetNode : targetList) {\n            targetNode.addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeDescriptor[] targetArray \u003d\n            new DatanodeDescriptor[targetList.size()];\n        pendingFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n      }\n\n      if (closeFile) {\n        src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d persistBlocks(pendingFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5009. Include storage information in the LocatedBlock.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1519691 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/09/13 7:03 AM",
      "commitName": "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "27/08/13 11:30 PM",
      "commitNameOld": "5d9d702607913685eab0d8ad077040ddc82bf085",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 6.31,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,108 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n         }\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n         boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlockFromMap(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n-        DatanodeDescriptor[] descriptors \u003d null;\n-        if (newtargets.length \u003e 0) {\n-          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n-          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n-            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n-                newtargets[i]);\n-          }\n-        }\n-        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n+        final DatanodeStorageInfo[] storages \u003d blockManager.getDatanodeManager()\n+            .getDatanodeStorageInfos(newtargets, newtargetstorages);\n+        if (closeFile \u0026\u0026 storages !\u003d null) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n-          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n-            descriptors[i].addBlock(newtargetstorages[i], storedBlock);\n+          for (int i \u003d 0; i \u003c storages.length; i++) {\n+            storages[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n-        pendingFile.setLastBlock(storedBlock, descriptors);\n+        pendingFile.setLastBlock(storedBlock, storages);\n       }\n \n       if (closeFile) {\n         src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d persistBlocks(pendingFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        final DatanodeStorageInfo[] storages \u003d blockManager.getDatanodeManager()\n            .getDatanodeStorageInfos(newtargets, newtargetstorages);\n        if (closeFile \u0026\u0026 storages !\u003d null) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c storages.length; i++) {\n            storages[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, storages);\n      }\n\n      if (closeFile) {\n        src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d persistBlocks(pendingFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "eb484bb5629e57c97192b6794f30c1fbb290b6ee": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5077. NPE in FSNamesystem.commitBlockSynchronization(). Contributed by Plamen Jeliazkov.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1518851 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/08/13 4:08 PM",
      "commitName": "eb484bb5629e57c97192b6794f30c1fbb290b6ee",
      "commitAuthor": "Konstantin Shvachko",
      "commitDateOld": "27/08/13 2:04 PM",
      "commitNameOld": "1bfcab9689588c6add9dcf1caad59ad68a3d1866",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 2.09,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,122 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n         }\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n         boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlockFromMap(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n-        DatanodeDescriptor[] descriptors \u003d null;\n+        List\u003cDatanodeDescriptor\u003e targetList \u003d\n+            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n         if (newtargets.length \u003e 0) {\n-          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n-          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n-            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n-                newtargets[i]);\n+          for (DatanodeID newtarget : newtargets) {\n+            // try to get targetNode\n+            DatanodeDescriptor targetNode \u003d\n+                blockManager.getDatanodeManager().getDatanode(newtarget);\n+            if (targetNode !\u003d null)\n+              targetList.add(targetNode);\n+            else if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtarget + \") not found\");\n+            }\n           }\n         }\n-        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n+        if ((closeFile) \u0026\u0026 !targetList.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n-          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n-            descriptors[i].addBlock(storedBlock);\n+          for (DatanodeDescriptor targetNode : targetList) {\n+            targetNode.addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n-        pendingFile.setLastBlock(storedBlock, descriptors);\n+        DatanodeDescriptor[] targetArray \u003d\n+            new DatanodeDescriptor[targetList.size()];\n+        pendingFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n       }\n \n       if (closeFile) {\n         src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d persistBlocks(pendingFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        List\u003cDatanodeDescriptor\u003e targetList \u003d\n            new ArrayList\u003cDatanodeDescriptor\u003e(newtargets.length);\n        if (newtargets.length \u003e 0) {\n          for (DatanodeID newtarget : newtargets) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode \u003d\n                blockManager.getDatanodeManager().getDatanode(newtarget);\n            if (targetNode !\u003d null)\n              targetList.add(targetNode);\n            else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (\u003d\" + newtarget + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) \u0026\u0026 !targetList.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (DatanodeDescriptor targetNode : targetList) {\n            targetNode.addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeDescriptor[] targetArray \u003d\n            new DatanodeDescriptor[targetList.size()];\n        pendingFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n      }\n\n      if (closeFile) {\n        src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d persistBlocks(pendingFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "5d9d702607913685eab0d8ad077040ddc82bf085": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4987. Namenode changes to track multiple storages per datanode.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1518087 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/08/13 11:30 PM",
      "commitName": "5d9d702607913685eab0d8ad077040ddc82bf085",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "21/08/13 11:15 AM",
      "commitNameOld": "2499a86664103eb2d16bd53bf424446599b61820",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 6.51,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,114 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n         }\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n         boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlockFromMap(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n-            descriptors[i].addBlock(storedBlock);\n+            descriptors[i].addBlock(newtargetstorages[i], storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       if (closeFile) {\n         src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         src \u003d persistBlocks(pendingFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(newtargetstorages[i], storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      if (closeFile) {\n        src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d persistBlocks(pendingFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "8c7a7e619699386f9e6991842558d78aa0c8053d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/07/13 12:51 AM",
      "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "26/07/13 4:59 PM",
      "commitNameOld": "dc17bda4b677e30c02c2a9a053895a43e41f7a12",
      "commitAuthorOld": "Konstantin Boudnik",
      "daysBetweenCommits": 3.33,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,114 +1,114 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       final BlockInfo storedBlock \u003d getStoredBlock(\n           ExtendedBlock.getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         if (deleteblock) {\n           // This may be a retry attempt so ignore the failure\n           // to locate the block.\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n           }\n           return;\n         } else {\n           throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n         }\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                     + \") since the file (\u003d\" + iFile.getLocalName()\n                     + \") is not under construction\");\n         }\n         return;\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n         boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n         if (remove) {\n           blockManager.removeBlockFromMap(storedBlock);\n         }\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       if (closeFile) {\n         src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n-        src \u003d persistBlocks(pendingFile);\n+        src \u003d persistBlocks(pendingFile, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      if (closeFile) {\n        src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d persistBlocks(pendingFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "f138ae68f9be0ae072a6a4ee50e94a1608c90edb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5024. Make DatanodeProtocol#commitBlockSynchronization idempotent. Contributed by Arpit Agarwal.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1506789 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/07/13 5:32 PM",
      "commitName": "f138ae68f9be0ae072a6a4ee50e94a1608c90edb",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "22/07/13 11:22 AM",
      "commitNameOld": "11c073134afc878619c37c95935d6a3098a21f17",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.26,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,110 +1,114 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n-      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n-        .getLocalBlock(lastblock));\n+      final BlockInfo storedBlock \u003d getStoredBlock(\n+          ExtendedBlock.getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n-        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n+        if (deleteblock) {\n+          // This may be a retry attempt so ignore the failure\n+          // to locate the block.\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n+          }\n+          return;\n+        } else {\n+          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n+        }\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n-        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n-                              + \") since the file (\u003d\" + iFile.getLocalName()\n-                              + \") is not under construction\");\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n+                    + \") since the file (\u003d\" + iFile.getLocalName()\n+                    + \") is not under construction\");\n+        }\n+        return;\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n         boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n-        if (!remove) {\n-          throw new IOException(\"Trying to delete non-existant block \"\n-              + blockToDel);\n+        if (remove) {\n+          blockManager.removeBlockFromMap(storedBlock);\n         }\n-        blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n-      src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n-        // commit the last block and complete it if it has minimum replicas\n-        commitOrCompleteLastBlock(pendingFile, storedBlock);\n-\n-        //remove lease, close file\n-        finalizeINodeFileUnderConstruction(src, pendingFile,\n-            Snapshot.findLatestSnapshot(pendingFile, null));\n+        src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n-        dir.persistBlocks(src, pendingFile);\n+        src \u003d persistBlocks(pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      final BlockInfo storedBlock \u003d getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (\u003d\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (\u003d\" + lastblock\n                    + \") since the file (\u003d\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      if (closeFile) {\n        src \u003d closeFileCommitBlocks(pendingFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src \u003d persistBlocks(pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "11c073134afc878619c37c95935d6a3098a21f17": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5008. Make ClientProtocol#abandonBlock() idempotent. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505761 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/13 11:22 AM",
      "commitName": "11c073134afc878619c37c95935d6a3098a21f17",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "20/07/13 9:22 AM",
      "commitNameOld": "313dd0250543177752ebbad7f7f6a6bcf3a8ab42",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 2.08,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,105 +1,110 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n              + \", newgenerationstamp\u003d\" + newgenerationstamp\n              + \", newlength\u003d\" + newlength\n              + \", newtargets\u003d\" + Arrays.asList(newtargets)\n              + \", closeFile\u003d\" + closeFile\n              + \", deleteBlock\u003d\" + deleteblock\n              + \")\");\n     checkOperation(OperationCategory.WRITE);\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n-        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n+        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n+        boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n+        if (!remove) {\n+          throw new IOException(\"Trying to delete non-existant block \"\n+              + blockToDel);\n+        }\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile,\n             Snapshot.findLatestSnapshot(pendingFile, null));\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        Block blockToDel \u003d ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove \u003d pendingFile.removeLastBlock(blockToDel);\n        if (!remove) {\n          throw new IOException(\"Trying to delete non-existant block \"\n              + blockToDel);\n        }\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile,\n            Snapshot.findLatestSnapshot(pendingFile, null));\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "fd1000bcefa07992ff5c6fae3508f3e33b7955c6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4679. Namenode operation checks should be done in a consistent manner. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1466721 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/04/13 3:45 PM",
      "commitName": "fd1000bcefa07992ff5c6fae3508f3e33b7955c6",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "08/04/13 6:21 PM",
      "commitNameOld": "f680865d994b8b75c11fa85f3241b1b9c6851187",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 1.89,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,104 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n-    String src \u003d \"\";\n+    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n+             + \", newgenerationstamp\u003d\" + newgenerationstamp\n+             + \", newlength\u003d\" + newlength\n+             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n+             + \", closeFile\u003d\" + closeFile\n+             + \", deleteBlock\u003d\" + deleteblock\n+             + \")\");\n     checkOperation(OperationCategory.WRITE);\n+    String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n-      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n-               + \", newgenerationstamp\u003d\" + newgenerationstamp\n-               + \", newlength\u003d\" + newlength\n-               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n-               + \", closeFile\u003d\" + closeFile\n-               + \", deleteBlock\u003d\" + deleteblock\n-               + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d (INodeFile) storedBlock.getBlockCollection();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n             + \", newgenerationstamp\u003d\" + newgenerationstamp\n             + \", newlength\u003d\" + newlength\n             + \", newtargets\u003d\" + Arrays.asList(newtargets)\n             + \", closeFile\u003d\" + closeFile\n             + \", deleteBlock\u003d\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d (INodeFile) storedBlock.getBlockCollection();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "3bf09c51501a23b7fa28fd0a0c4c0965858d026c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4591. HA clients can fail to fail over while Standby NN is performing long checkpoint. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1456107 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/03/13 12:51 PM",
      "commitName": "3bf09c51501a23b7fa28fd0a0c4c0965858d026c",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "12/03/13 7:32 PM",
      "commitNameOld": "86a940f7adc5bd9c9eaea2283df5e014e5079ab6",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 0.72,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,103 +1,104 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n+    checkOperation(OperationCategory.WRITE);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d (INodeFile) storedBlock.getBlockCollection();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d (INodeFile) storedBlock.getBlockCollection();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "b1333e5b561d01a010e2e1311e8501879f377bdc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4545. With snapshots, FSDirectory.unprotectedSetReplication(..) always changes file replication but it may or may not changes block replication.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1452636 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/03/13 6:23 PM",
      "commitName": "b1333e5b561d01a010e2e1311e8501879f377bdc",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "26/02/13 2:04 PM",
      "commitNameOld": "e2a618e1cc3fb99115547af6540932860dc6766e",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 6.18,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,104 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n-      INodeFile iFile \u003d (INodeFile) storedBlock.getBlockCollection();\n+      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile,\n             Snapshot.findLatestSnapshot(pendingFile, null));\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile,\n            Snapshot.findLatestSnapshot(pendingFile, null));\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "f29fa9e820e25730d00a1a00c51c6f11028fb5a7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4499. Fix file/directory/snapshot deletion for file diff.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1448504 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/02/13 7:27 PM",
      "commitName": "f29fa9e820e25730d00a1a00c51c6f11028fb5a7",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "20/02/13 12:02 PM",
      "commitNameOld": "fac3883188d9c4f1fe188d98f88cb3c83b243bbd",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.31,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,104 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d (INodeFile) storedBlock.getBlockCollection();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile,\n-            Snapshot.findLatestSnapshot(pendingFile));\n+            Snapshot.findLatestSnapshot(pendingFile, null));\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d (INodeFile) storedBlock.getBlockCollection();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile,\n            Snapshot.findLatestSnapshot(pendingFile, null));\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "b9f965de120b5278ac84a7e98aecb32aafde4c16": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4103. Support O(1) snapshot creation.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1424782 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/12 5:30 PM",
      "commitName": "b9f965de120b5278ac84a7e98aecb32aafde4c16",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "16/12/12 7:40 PM",
      "commitNameOld": "cbbaa93ae09bf5cf643263faf78f99315c4f3a8d",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 3.91,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,103 +1,104 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d (INodeFile) storedBlock.getBlockCollection();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n-        finalizeINodeFileUnderConstruction(src, pendingFile);\n+        finalizeINodeFileUnderConstruction(src, pendingFile,\n+            Snapshot.findLatestSnapshot(pendingFile));\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d (INodeFile) storedBlock.getBlockCollection();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile,\n            Snapshot.findLatestSnapshot(pendingFile));\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "9821af9ce8a56a2c583f1ed938902c20e897048f": {
      "type": "Ybodychange",
      "commitMessage": "Reverting the previous merge r1416603 which committed some extra changes\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1416712 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/12/12 2:43 PM",
      "commitName": "9821af9ce8a56a2c583f1ed938902c20e897048f",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "03/12/12 10:04 AM",
      "commitNameOld": "d500d59cbef51f1b0b0291995893b85a139bcec9",
      "commitAuthorOld": "",
      "daysBetweenCommits": 0.19,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,103 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d (INodeFile) storedBlock.getBlockCollection();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n-        finalizeINodeFileUnderConstruction(src, pendingFile,\n-            INodeDirectorySnapshottable.findLatestSnapshot(pendingFile));\n+        finalizeINodeFileUnderConstruction(src, pendingFile);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d (INodeFile) storedBlock.getBlockCollection();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "7e8e983620f3ae3462d115972707c72b7d9cbabd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3369. Rename {get|set|add}INode(..) methods in BlockManager and BlocksMap to {get|set|add}BlockCollection(..).  Contributed by John George\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1336909 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/05/12 2:41 PM",
      "commitName": "7e8e983620f3ae3462d115972707c72b7d9cbabd",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "07/05/12 5:06 PM",
      "commitNameOld": "f0f9a3631fe4950f5cf548f192226836925d0f05",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 2.9,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,103 +1,103 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n-      INodeFile iFile \u003d (INodeFile) storedBlock.getINode();\n+      INodeFile iFile \u003d (INodeFile) storedBlock.getBlockCollection();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d (INodeFile) storedBlock.getBlockCollection();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "f0f9a3631fe4950f5cf548f192226836925d0f05": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3363. Define BlockCollection and MutableBlockCollection interfaces so that INodeFile and INodeFileUnderConstruction do not have to be used in block management.  Contributed by John George\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1335304 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/05/12 5:06 PM",
      "commitName": "f0f9a3631fe4950f5cf548f192226836925d0f05",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "04/05/12 3:14 PM",
      "commitNameOld": "51e520c68aafb73b784bf690a8a42de3af0f229c",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 3.08,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,103 +1,103 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n-      INodeFile iFile \u003d storedBlock.getINode();\n+      INodeFile iFile \u003d (INodeFile) storedBlock.getINode();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n       } else {\n         // If this commit does not want to close the file, persist blocks\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d (INodeFile) storedBlock.getINode();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "72b3f302dc3590b7f731cad26558cd592c35dc5a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3148. The client should be able to use multiple local interfaces for data transfer. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308614 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/04/12 4:13 PM",
      "commitName": "72b3f302dc3590b7f731cad26558cd592c35dc5a",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "31/03/12 12:56 PM",
      "commitNameOld": "eeb687daa7367d1ce72690397751451195392b55",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 2.14,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,103 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d storedBlock.getINode();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n-      } else if (supportAppends) {\n+      } else {\n         // If this commit does not want to close the file, persist blocks\n-        // only if append is supported or we\u0027re explicitly told to\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d storedBlock.getINode();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "6326605acb5a5bf48d994278c9d3a39733679e81": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-3105.  Add DatanodeStorage information to block recovery.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1302683 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/03/12 3:09 PM",
      "commitName": "6326605acb5a5bf48d994278c9d3a39733679e81",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "12/03/12 12:41 PM",
      "commitNameOld": "1a75ec82885e45baf4d5cd56d6c738d8e68d8bc7",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 7.1,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,103 +1,104 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n-      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n+      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n+      String[] newtargetstorages)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       // If a DN tries to commit to the standby, the recovery will\n       // fail, and the next retry will succeed on the new NN.\n   \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d storedBlock.getINode();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n       } else if (supportAppends) {\n         // If this commit does not want to close the file, persist blocks\n         // only if append is supported or we\u0027re explicitly told to\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d storedBlock.getINode();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else if (supportAppends) {\n        // If this commit does not want to close the file, persist blocks\n        // only if append is supported or we\u0027re explicitly told to\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldValue": "[lastblock-ExtendedBlock, newgenerationstamp-long, newlength-long, closeFile-boolean, deleteblock-boolean, newtargets-DatanodeID[]]",
        "newValue": "[lastblock-ExtendedBlock, newgenerationstamp-long, newlength-long, closeFile-boolean, deleteblock-boolean, newtargets-DatanodeID[], newtargetstorages-String[]]"
      }
    },
    "c14912785d22734d735b5c4f8638b57dff009a97": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2929. Stress test and fixes for block synchronization. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1292494 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/02/12 12:31 PM",
      "commitName": "c14912785d22734d735b5c4f8638b57dff009a97",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "16/02/12 2:45 PM",
      "commitNameOld": "153e0cc37aacf04fec3de51ebc1690e50f16b614",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 5.91,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,106 +1,103 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n-      if (haContext.getState().equals(NameNode.STANDBY_STATE)) {\n-        // TODO(HA) we\u0027ll never get here, since we check for WRITE operation above!\n-        // Need to implement tests, etc, for this - block recovery spanning\n-        // failover.\n-      }\n-\n+      // If a DN tries to commit to the standby, the recovery will\n+      // fail, and the next retry will succeed on the new NN.\n+  \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d storedBlock.getINode();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n       } else if (supportAppends) {\n         // If this commit does not want to close the file, persist blocks\n         // only if append is supported or we\u0027re explicitly told to\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d storedBlock.getINode();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else if (supportAppends) {\n        // If this commit does not want to close the file, persist blocks\n        // only if append is supported or we\u0027re explicitly told to\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "cf611255d6fcd7016e0ce2a3f80ccd0d4e051d9f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2742. HA: observed dataloss in replication stress test. Contributed by Todd Lipcon\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1238940 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/01/12 9:16 PM",
      "commitName": "cf611255d6fcd7016e0ce2a3f80ccd0d4e051d9f",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "30/01/12 11:16 AM",
      "commitNameOld": "846f97312c6db7b84b7401174acd0fc943baa093",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 1.42,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,113 +1,106 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       if (haContext.getState().equals(NameNode.STANDBY_STATE)) {\n         // TODO(HA) we\u0027ll never get here, since we check for WRITE operation above!\n-        if (isGenStampInFuture(newgenerationstamp)) {\n-          LOG.info(\"Required GS\u003d\" + newgenerationstamp\n-              + \", Queuing commitBlockSynchronization message\");\n-          getPendingDataNodeMessages().queueMessage(\n-              new PendingDataNodeMessages.CommitBlockSynchronizationMessage(\n-                  lastblock, newgenerationstamp, newlength, closeFile, deleteblock,\n-                  newtargets, newgenerationstamp));\n-          return;\n-        }\n+        // Need to implement tests, etc, for this - block recovery spanning\n+        // failover.\n       }\n \n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d storedBlock.getINode();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n       } else if (supportAppends) {\n         // If this commit does not want to close the file, persist blocks\n         // only if append is supported or we\u0027re explicitly told to\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (haContext.getState().equals(NameNode.STANDBY_STATE)) {\n        // TODO(HA) we\u0027ll never get here, since we check for WRITE operation above!\n        // Need to implement tests, etc, for this - block recovery spanning\n        // failover.\n      }\n\n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d storedBlock.getINode();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else if (supportAppends) {\n        // If this commit does not want to close the file, persist blocks\n        // only if append is supported or we\u0027re explicitly told to\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "36d1c49486587c2dbb193e8538b1d4510c462fa6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2693. Fix synchronization issues around state transition. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1221582 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/11 7:03 PM",
      "commitName": "36d1c49486587c2dbb193e8538b1d4510c462fa6",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "16/12/11 10:36 AM",
      "commitNameOld": "371f4228e86f5ebffb3d8647fb30b8bdc2b777c4",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 4.35,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,99 +1,113 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n+      checkOperation(OperationCategory.WRITE);\n+      if (haContext.getState().equals(NameNode.STANDBY_STATE)) {\n+        // TODO(HA) we\u0027ll never get here, since we check for WRITE operation above!\n+        if (isGenStampInFuture(newgenerationstamp)) {\n+          LOG.info(\"Required GS\u003d\" + newgenerationstamp\n+              + \", Queuing commitBlockSynchronization message\");\n+          getPendingDataNodeMessages().queueMessage(\n+              new PendingDataNodeMessages.CommitBlockSynchronizationMessage(\n+                  lastblock, newgenerationstamp, newlength, closeFile, deleteblock,\n+                  newtargets, newgenerationstamp));\n+          return;\n+        }\n+      }\n+\n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d storedBlock.getINode();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n       } else if (supportAppends) {\n         // If this commit does not want to close the file, persist blocks\n         // only if append is supported or we\u0027re explicitly told to\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (haContext.getState().equals(NameNode.STANDBY_STATE)) {\n        // TODO(HA) we\u0027ll never get here, since we check for WRITE operation above!\n        if (isGenStampInFuture(newgenerationstamp)) {\n          LOG.info(\"Required GS\u003d\" + newgenerationstamp\n              + \", Queuing commitBlockSynchronization message\");\n          getPendingDataNodeMessages().queueMessage(\n              new PendingDataNodeMessages.CommitBlockSynchronizationMessage(\n                  lastblock, newgenerationstamp, newlength, closeFile, deleteblock,\n                  newtargets, newgenerationstamp));\n          return;\n        }\n      }\n\n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d storedBlock.getINode();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else if (supportAppends) {\n        // If this commit does not want to close the file, persist blocks\n        // only if append is supported or we\u0027re explicitly told to\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "71071b904d0c9aec7b3713d41740f24182e81c36": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2602. NN should log newly-allocated blocks without losing BlockInfo. Contributed by Aaron T. Myers\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1215036 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/12/11 8:18 PM",
      "commitName": "71071b904d0c9aec7b3713d41740f24182e81c36",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "08/12/11 3:55 PM",
      "commitNameOld": "2481474bd9c50a23e4fd2eea67ac2dea11ca1f58",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 7.18,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,99 +1,99 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d storedBlock.getINode();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n       } else if (supportAppends) {\n-        // If this commit does not want to close the file, persist\n-        // blocks only if append is supported \n+        // If this commit does not want to close the file, persist blocks\n+        // only if append is supported or we\u0027re explicitly told to\n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d storedBlock.getINode();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else if (supportAppends) {\n        // If this commit does not want to close the file, persist blocks\n        // only if append is supported or we\u0027re explicitly told to\n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "b7cd8c0f865e88e40eee75fd2690b1fdc4155071": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2564. Cleanup unnecessary exceptions thrown and unnecessary casts. Contributed by Hari Mankude\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1203950 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/11/11 6:34 PM",
      "commitName": "b7cd8c0f865e88e40eee75fd2690b1fdc4155071",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "14/11/11 5:13 PM",
      "commitNameOld": "9a3f147fdd5421460889b266ead3a2300323cda2",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 4.06,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,99 +1,99 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d storedBlock.getINode();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n-        if (closeFile) {\n+        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n       } else if (supportAppends) {\n         // If this commit does not want to close the file, persist\n         // blocks only if append is supported \n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d storedBlock.getINode();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if ((closeFile) \u0026\u0026 (descriptors !\u003d null)) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else if (supportAppends) {\n        // If this commit does not want to close the file, persist\n        // blocks only if append is supported \n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d storedBlock.getINode();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if (closeFile) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else if (supportAppends) {\n        // If this commit does not want to close the file, persist\n        // blocks only if append is supported \n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "2892f6d817d74e90ff50073cd3721ed4ec75ba92": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2266.  Add Namesystem and SafeMode interfaces to avoid directly referring to FSNamesystem in BlockManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1160493 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/08/11 4:14 PM",
      "commitName": "2892f6d817d74e90ff50073cd3721ed4ec75ba92",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "19/08/11 10:36 AM",
      "commitNameOld": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 3.23,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,99 +1,99 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d storedBlock.getINode();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n             descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                 newtargets[i]);\n           }\n         }\n         if (closeFile) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n-        blockManager.commitOrCompleteLastBlock(pendingFile, storedBlock);\n+        commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n       } else if (supportAppends) {\n         // If this commit does not want to close the file, persist\n         // blocks only if append is supported \n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d storedBlock.getINode();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if (closeFile) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else if (supportAppends) {\n        // If this commit does not want to close the file, persist\n        // blocks only if append is supported \n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d storedBlock.getINode();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if (closeFile) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        blockManager.commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else if (supportAppends) {\n        // If this commit does not want to close the file, persist\n        // blocks only if append is supported \n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "969a263188f7015261719fe45fa1505121ebb80e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2191.  Move datanodeMap from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151339 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/07/11 10:46 PM",
      "commitName": "969a263188f7015261719fe45fa1505121ebb80e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "22/07/11 6:01 PM",
      "commitNameOld": "89537b7710b23db7abcd2a77f03818c06a5f5fa7",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.2,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,98 +1,99 @@\n   void commitBlockSynchronization(ExtendedBlock lastblock,\n       long newgenerationstamp, long newlength,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n       throws IOException, UnresolvedLinkException {\n     String src \u003d \"\";\n     writeLock();\n     try {\n       if (isInSafeMode()) {\n         throw new SafeModeException(\n           \"Cannot commitBlockSynchronization while in safe mode\",\n           safeMode);\n       }\n       LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n                + \", newgenerationstamp\u003d\" + newgenerationstamp\n                + \", newlength\u003d\" + newlength\n                + \", newtargets\u003d\" + Arrays.asList(newtargets)\n                + \", closeFile\u003d\" + closeFile\n                + \", deleteBlock\u003d\" + deleteblock\n                + \")\");\n       final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n         .getLocalBlock(lastblock));\n       if (storedBlock \u003d\u003d null) {\n         throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n       }\n       INodeFile iFile \u003d storedBlock.getINode();\n       if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n         throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                               + \") since the file (\u003d\" + iFile.getLocalName()\n                               + \") is not under construction\");\n       }\n \n       long recoveryId \u003d\n         ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n       if(recoveryId !\u003d newgenerationstamp) {\n         throw new IOException(\"The recovery id \" + newgenerationstamp\n                               + \" does not match current recovery id \"\n                               + recoveryId + \" for block \" + lastblock); \n       }\n \n       INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n \n       if (deleteblock) {\n         pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n         blockManager.removeBlockFromMap(storedBlock);\n       }\n       else {\n         // update last block\n         storedBlock.setGenerationStamp(newgenerationstamp);\n         storedBlock.setNumBytes(newlength);\n \n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n         DatanodeDescriptor[] descriptors \u003d null;\n         if (newtargets.length \u003e 0) {\n           descriptors \u003d new DatanodeDescriptor[newtargets.length];\n           for(int i \u003d 0; i \u003c newtargets.length; i++) {\n-            descriptors[i] \u003d getDatanode(newtargets[i]);\n+            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n+                newtargets[i]);\n           }\n         }\n         if (closeFile) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n           for (int i \u003d 0; i \u003c descriptors.length; i++) {\n             descriptors[i].addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n         pendingFile.setLastBlock(storedBlock, descriptors);\n       }\n \n       src \u003d leaseManager.findPath(pendingFile);\n       if (closeFile) {\n         // commit the last block and complete it if it has minimum replicas\n         blockManager.commitOrCompleteLastBlock(pendingFile, storedBlock);\n \n         //remove lease, close file\n         finalizeINodeFileUnderConstruction(src, pendingFile);\n       } else if (supportAppends) {\n         // If this commit does not want to close the file, persist\n         // blocks only if append is supported \n         dir.persistBlocks(src, pendingFile);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (closeFile) {\n       LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n           + \", file\u003d\" + src\n           + \", newgenerationstamp\u003d\" + newgenerationstamp\n           + \", newlength\u003d\" + newlength\n           + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n     } else {\n       LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d storedBlock.getINode();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d blockManager.getDatanodeManager().getDatanode(\n                newtargets[i]);\n          }\n        }\n        if (closeFile) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        blockManager.commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else if (supportAppends) {\n        // If this commit does not want to close the file, persist\n        // blocks only if append is supported \n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,98 @@\n+  void commitBlockSynchronization(ExtendedBlock lastblock,\n+      long newgenerationstamp, long newlength,\n+      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n+      throws IOException, UnresolvedLinkException {\n+    String src \u003d \"\";\n+    writeLock();\n+    try {\n+      if (isInSafeMode()) {\n+        throw new SafeModeException(\n+          \"Cannot commitBlockSynchronization while in safe mode\",\n+          safeMode);\n+      }\n+      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n+               + \", newgenerationstamp\u003d\" + newgenerationstamp\n+               + \", newlength\u003d\" + newlength\n+               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n+               + \", closeFile\u003d\" + closeFile\n+               + \", deleteBlock\u003d\" + deleteblock\n+               + \")\");\n+      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n+        .getLocalBlock(lastblock));\n+      if (storedBlock \u003d\u003d null) {\n+        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n+      }\n+      INodeFile iFile \u003d storedBlock.getINode();\n+      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n+        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n+                              + \") since the file (\u003d\" + iFile.getLocalName()\n+                              + \") is not under construction\");\n+      }\n+\n+      long recoveryId \u003d\n+        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n+      if(recoveryId !\u003d newgenerationstamp) {\n+        throw new IOException(\"The recovery id \" + newgenerationstamp\n+                              + \" does not match current recovery id \"\n+                              + recoveryId + \" for block \" + lastblock); \n+      }\n+\n+      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n+\n+      if (deleteblock) {\n+        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n+        blockManager.removeBlockFromMap(storedBlock);\n+      }\n+      else {\n+        // update last block\n+        storedBlock.setGenerationStamp(newgenerationstamp);\n+        storedBlock.setNumBytes(newlength);\n+\n+        // find the DatanodeDescriptor objects\n+        // There should be no locations in the blockManager till now because the\n+        // file is underConstruction\n+        DatanodeDescriptor[] descriptors \u003d null;\n+        if (newtargets.length \u003e 0) {\n+          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n+          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n+            descriptors[i] \u003d getDatanode(newtargets[i]);\n+          }\n+        }\n+        if (closeFile) {\n+          // the file is getting closed. Insert block locations into blockManager.\n+          // Otherwise fsck will report these blocks as MISSING, especially if the\n+          // blocksReceived from Datanodes take a long time to arrive.\n+          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n+            descriptors[i].addBlock(storedBlock);\n+          }\n+        }\n+        // add pipeline locations into the INodeUnderConstruction\n+        pendingFile.setLastBlock(storedBlock, descriptors);\n+      }\n+\n+      src \u003d leaseManager.findPath(pendingFile);\n+      if (closeFile) {\n+        // commit the last block and complete it if it has minimum replicas\n+        blockManager.commitOrCompleteLastBlock(pendingFile, storedBlock);\n+\n+        //remove lease, close file\n+        finalizeINodeFileUnderConstruction(src, pendingFile);\n+      } else if (supportAppends) {\n+        // If this commit does not want to close the file, persist\n+        // blocks only if append is supported \n+        dir.persistBlocks(src, pendingFile);\n+      }\n+    } finally {\n+      writeUnlock();\n+    }\n+    getEditLog().logSync();\n+    if (closeFile) {\n+      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n+          + \", file\u003d\" + src\n+          + \", newgenerationstamp\u003d\" + newgenerationstamp\n+          + \", newlength\u003d\" + newlength\n+          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n+    } else {\n+      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets)\n      throws IOException, UnresolvedLinkException {\n    String src \u003d \"\";\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      LOG.info(\"commitBlockSynchronization(lastblock\u003d\" + lastblock\n               + \", newgenerationstamp\u003d\" + newgenerationstamp\n               + \", newlength\u003d\" + newlength\n               + \", newtargets\u003d\" + Arrays.asList(newtargets)\n               + \", closeFile\u003d\" + closeFile\n               + \", deleteBlock\u003d\" + deleteblock\n               + \")\");\n      final BlockInfo storedBlock \u003d blockManager.getStoredBlock(ExtendedBlock\n        .getLocalBlock(lastblock));\n      if (storedBlock \u003d\u003d null) {\n        throw new IOException(\"Block (\u003d\" + lastblock + \") not found\");\n      }\n      INodeFile iFile \u003d storedBlock.getINode();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        throw new IOException(\"Unexpected block (\u003d\" + lastblock\n                              + \") since the file (\u003d\" + iFile.getLocalName()\n                              + \") is not under construction\");\n      }\n\n      long recoveryId \u003d\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId !\u003d newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile \u003d (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        pendingFile.removeLastBlock(ExtendedBlock.getLocalBlock(lastblock));\n        blockManager.removeBlockFromMap(storedBlock);\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        DatanodeDescriptor[] descriptors \u003d null;\n        if (newtargets.length \u003e 0) {\n          descriptors \u003d new DatanodeDescriptor[newtargets.length];\n          for(int i \u003d 0; i \u003c newtargets.length; i++) {\n            descriptors[i] \u003d getDatanode(newtargets[i]);\n          }\n        }\n        if (closeFile) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i \u003d 0; i \u003c descriptors.length; i++) {\n            descriptors[i].addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        pendingFile.setLastBlock(storedBlock, descriptors);\n      }\n\n      src \u003d leaseManager.findPath(pendingFile);\n      if (closeFile) {\n        // commit the last block and complete it if it has minimum replicas\n        blockManager.commitOrCompleteLastBlock(pendingFile, storedBlock);\n\n        //remove lease, close file\n        finalizeINodeFileUnderConstruction(src, pendingFile);\n      } else if (supportAppends) {\n        // If this commit does not want to close the file, persist\n        // blocks only if append is supported \n        dir.persistBlocks(src, pendingFile);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock\u003d\" + lastblock\n          + \", file\u003d\" + src\n          + \", newgenerationstamp\u003d\" + newgenerationstamp\n          + \", newlength\u003d\" + newlength\n          + \", newtargets\u003d\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
    }
  }
}