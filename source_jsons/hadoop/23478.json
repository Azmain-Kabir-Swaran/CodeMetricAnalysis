{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "HistoryFileManager.java",
  "functionName": "addIfAbsent",
  "functionId": "addIfAbsent___fileInfo-HistoryFileInfo",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
  "functionStartLine": 214,
  "functionEndLine": 286,
  "numCommitsSeen": 44,
  "timeTaken": 2121,
  "changeHistory": [
    "99c2bbd337942e4bc7b246a88dff53f98e530651",
    "5b7078d06921893200163a3d29c8901c3c0107cb",
    "ac44e0a0d0cebc07904518f32e0d8e34a4391adf",
    "7d04a96027ad75877b41b7cd8f67455dd13159d7"
  ],
  "changeHistoryShort": {
    "99c2bbd337942e4bc7b246a88dff53f98e530651": "Ybodychange",
    "5b7078d06921893200163a3d29c8901c3c0107cb": "Ybodychange",
    "ac44e0a0d0cebc07904518f32e0d8e34a4391adf": "Ybodychange",
    "7d04a96027ad75877b41b7cd8f67455dd13159d7": "Yintroduced"
  },
  "changeHistoryDetails": {
    "99c2bbd337942e4bc7b246a88dff53f98e530651": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6797. Job history server scans can become blocked on a single, slow entry. Contributed by Prabhu Joseph\n",
      "commitDate": "14/11/16 12:20 PM",
      "commitName": "99c2bbd337942e4bc7b246a88dff53f98e530651",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "28/09/16 3:41 PM",
      "commitNameOld": "0d6778d800ff16366911e3b064f3af6162dee2e4",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 46.9,
      "commitsBetweenForRepo": 412,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,73 @@\n     public HistoryFileInfo addIfAbsent(HistoryFileInfo fileInfo) {\n       JobId jobId \u003d fileInfo.getJobId();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Adding \" + jobId + \" to job list cache with \"\n             + fileInfo.getJobIndexInfo());\n       }\n       HistoryFileInfo old \u003d cache.putIfAbsent(jobId, fileInfo);\n       if (cache.size() \u003e maxSize) {\n         //There is a race here, where more then one thread could be trying to\n         // remove entries.  This could result in too many entries being removed\n         // from the cache.  This is considered OK as the size of the cache\n         // should be rather large, and we would rather have performance over\n         // keeping the cache size exactly at the maximum.\n         Iterator\u003cJobId\u003e keys \u003d cache.navigableKeySet().iterator();\n         long cutoff \u003d System.currentTimeMillis() - maxAge;\n \n         // MAPREDUCE-6436: In order to reduce the number of logs written\n         // in case of a lot of move pending histories.\n         JobId firstInIntermediateKey \u003d null;\n         int inIntermediateCount \u003d 0;\n         JobId firstMoveFailedKey \u003d null;\n         int moveFailedCount \u003d 0;\n \n-        while(cache.size() \u003e maxSize \u0026\u0026 keys.hasNext()) {\n+        while (cache.size() \u003e maxSize \u0026\u0026 keys.hasNext()) {\n           JobId key \u003d keys.next();\n           HistoryFileInfo firstValue \u003d cache.get(key);\n-          if(firstValue !\u003d null) {\n-            synchronized(firstValue) {\n-              if (firstValue.isMovePending()) {\n-                if(firstValue.didMoveFail() \u0026\u0026\n-                    firstValue.jobIndexInfo.getFinishTime() \u003c\u003d cutoff) {\n-                  cache.remove(key);\n-                  //Now lets try to delete it\n-                  try {\n-                    firstValue.delete();\n-                  } catch (IOException e) {\n-                    LOG.error(\"Error while trying to delete history files\" +\n-                    \t\t\" that could not be moved to done.\", e);\n-                  }\n-                } else {\n-                  if (firstValue.didMoveFail()) {\n-                    if (moveFailedCount \u003d\u003d 0) {\n-                      firstMoveFailedKey \u003d key;\n-                    }\n-                    moveFailedCount +\u003d 1;\n-                  } else {\n-                    if (inIntermediateCount \u003d\u003d 0) {\n-                      firstInIntermediateKey \u003d key;\n-                    }\n-                    inIntermediateCount +\u003d 1;\n-                  }\n+          if (firstValue !\u003d null) {\n+            if (firstValue.isMovePending()) {\n+              if (firstValue.didMoveFail() \u0026\u0026\n+                  firstValue.jobIndexInfo.getFinishTime() \u003c\u003d cutoff) {\n+                cache.remove(key);\n+                // Now lets try to delete it\n+                try {\n+                  firstValue.delete();\n+                } catch (IOException e) {\n+                  LOG.error(\"Error while trying to delete history files\" +\n+                      \" that could not be moved to done.\", e);\n                 }\n               } else {\n-                cache.remove(key);\n+                if (firstValue.didMoveFail()) {\n+                  if (moveFailedCount \u003d\u003d 0) {\n+                    firstMoveFailedKey \u003d key;\n+                  }\n+                  moveFailedCount +\u003d 1;\n+                } else {\n+                  if (inIntermediateCount \u003d\u003d 0) {\n+                    firstInIntermediateKey \u003d key;\n+                  }\n+                  inIntermediateCount +\u003d 1;\n+                }\n               }\n+            } else {\n+              cache.remove(key);\n             }\n           }\n         }\n         // Log output only for first jobhisotry in pendings to restrict\n         // the total number of logs.\n         if (inIntermediateCount \u003e 0) {\n           LOG.warn(\"Waiting to remove IN_INTERMEDIATE state histories \" +\n                   \"(e.g. \" + firstInIntermediateKey + \") from JobListCache \" +\n                   \"because it is not in done yet. Total count is \" +\n                   inIntermediateCount + \".\");\n         }\n         if (moveFailedCount \u003e 0) {\n           LOG.warn(\"Waiting to remove MOVE_FAILED state histories \" +\n                   \"(e.g. \" + firstMoveFailedKey + \") from JobListCache \" +\n                   \"because it is not in done yet. Total count is \" +\n                   moveFailedCount + \".\");\n         }\n       }\n       return old;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public HistoryFileInfo addIfAbsent(HistoryFileInfo fileInfo) {\n      JobId jobId \u003d fileInfo.getJobId();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Adding \" + jobId + \" to job list cache with \"\n            + fileInfo.getJobIndexInfo());\n      }\n      HistoryFileInfo old \u003d cache.putIfAbsent(jobId, fileInfo);\n      if (cache.size() \u003e maxSize) {\n        //There is a race here, where more then one thread could be trying to\n        // remove entries.  This could result in too many entries being removed\n        // from the cache.  This is considered OK as the size of the cache\n        // should be rather large, and we would rather have performance over\n        // keeping the cache size exactly at the maximum.\n        Iterator\u003cJobId\u003e keys \u003d cache.navigableKeySet().iterator();\n        long cutoff \u003d System.currentTimeMillis() - maxAge;\n\n        // MAPREDUCE-6436: In order to reduce the number of logs written\n        // in case of a lot of move pending histories.\n        JobId firstInIntermediateKey \u003d null;\n        int inIntermediateCount \u003d 0;\n        JobId firstMoveFailedKey \u003d null;\n        int moveFailedCount \u003d 0;\n\n        while (cache.size() \u003e maxSize \u0026\u0026 keys.hasNext()) {\n          JobId key \u003d keys.next();\n          HistoryFileInfo firstValue \u003d cache.get(key);\n          if (firstValue !\u003d null) {\n            if (firstValue.isMovePending()) {\n              if (firstValue.didMoveFail() \u0026\u0026\n                  firstValue.jobIndexInfo.getFinishTime() \u003c\u003d cutoff) {\n                cache.remove(key);\n                // Now lets try to delete it\n                try {\n                  firstValue.delete();\n                } catch (IOException e) {\n                  LOG.error(\"Error while trying to delete history files\" +\n                      \" that could not be moved to done.\", e);\n                }\n              } else {\n                if (firstValue.didMoveFail()) {\n                  if (moveFailedCount \u003d\u003d 0) {\n                    firstMoveFailedKey \u003d key;\n                  }\n                  moveFailedCount +\u003d 1;\n                } else {\n                  if (inIntermediateCount \u003d\u003d 0) {\n                    firstInIntermediateKey \u003d key;\n                  }\n                  inIntermediateCount +\u003d 1;\n                }\n              }\n            } else {\n              cache.remove(key);\n            }\n          }\n        }\n        // Log output only for first jobhisotry in pendings to restrict\n        // the total number of logs.\n        if (inIntermediateCount \u003e 0) {\n          LOG.warn(\"Waiting to remove IN_INTERMEDIATE state histories \" +\n                  \"(e.g. \" + firstInIntermediateKey + \") from JobListCache \" +\n                  \"because it is not in done yet. Total count is \" +\n                  inIntermediateCount + \".\");\n        }\n        if (moveFailedCount \u003e 0) {\n          LOG.warn(\"Waiting to remove MOVE_FAILED state histories \" +\n                  \"(e.g. \" + firstMoveFailedKey + \") from JobListCache \" +\n                  \"because it is not in done yet. Total count is \" +\n                  moveFailedCount + \".\");\n        }\n      }\n      return old;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
      "extendedDetails": {}
    },
    "5b7078d06921893200163a3d29c8901c3c0107cb": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6436. JobHistory cache issue. Contributed by Kai Sasaki\n",
      "commitDate": "15/12/15 12:58 AM",
      "commitName": "5b7078d06921893200163a3d29c8901c3c0107cb",
      "commitAuthor": "Zhihai Xu",
      "commitDateOld": "30/10/15 8:31 AM",
      "commitNameOld": "6344b6a7694c70f296392b6462dba452ff762109",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 45.73,
      "commitsBetweenForRepo": 284,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,75 @@\n     public HistoryFileInfo addIfAbsent(HistoryFileInfo fileInfo) {\n       JobId jobId \u003d fileInfo.getJobId();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Adding \" + jobId + \" to job list cache with \"\n             + fileInfo.getJobIndexInfo());\n       }\n       HistoryFileInfo old \u003d cache.putIfAbsent(jobId, fileInfo);\n       if (cache.size() \u003e maxSize) {\n         //There is a race here, where more then one thread could be trying to\n         // remove entries.  This could result in too many entries being removed\n         // from the cache.  This is considered OK as the size of the cache\n         // should be rather large, and we would rather have performance over\n         // keeping the cache size exactly at the maximum.\n         Iterator\u003cJobId\u003e keys \u003d cache.navigableKeySet().iterator();\n         long cutoff \u003d System.currentTimeMillis() - maxAge;\n+\n+        // MAPREDUCE-6436: In order to reduce the number of logs written\n+        // in case of a lot of move pending histories.\n+        JobId firstInIntermediateKey \u003d null;\n+        int inIntermediateCount \u003d 0;\n+        JobId firstMoveFailedKey \u003d null;\n+        int moveFailedCount \u003d 0;\n+\n         while(cache.size() \u003e maxSize \u0026\u0026 keys.hasNext()) {\n           JobId key \u003d keys.next();\n           HistoryFileInfo firstValue \u003d cache.get(key);\n           if(firstValue !\u003d null) {\n             synchronized(firstValue) {\n               if (firstValue.isMovePending()) {\n-                if(firstValue.didMoveFail() \u0026\u0026 \n+                if(firstValue.didMoveFail() \u0026\u0026\n                     firstValue.jobIndexInfo.getFinishTime() \u003c\u003d cutoff) {\n                   cache.remove(key);\n                   //Now lets try to delete it\n                   try {\n                     firstValue.delete();\n                   } catch (IOException e) {\n                     LOG.error(\"Error while trying to delete history files\" +\n                     \t\t\" that could not be moved to done.\", e);\n                   }\n                 } else {\n-                  LOG.warn(\"Waiting to remove \" + key\n-                      + \" from JobListCache because it is not in done yet.\");\n+                  if (firstValue.didMoveFail()) {\n+                    if (moveFailedCount \u003d\u003d 0) {\n+                      firstMoveFailedKey \u003d key;\n+                    }\n+                    moveFailedCount +\u003d 1;\n+                  } else {\n+                    if (inIntermediateCount \u003d\u003d 0) {\n+                      firstInIntermediateKey \u003d key;\n+                    }\n+                    inIntermediateCount +\u003d 1;\n+                  }\n                 }\n               } else {\n                 cache.remove(key);\n               }\n             }\n           }\n         }\n+        // Log output only for first jobhisotry in pendings to restrict\n+        // the total number of logs.\n+        if (inIntermediateCount \u003e 0) {\n+          LOG.warn(\"Waiting to remove IN_INTERMEDIATE state histories \" +\n+                  \"(e.g. \" + firstInIntermediateKey + \") from JobListCache \" +\n+                  \"because it is not in done yet. Total count is \" +\n+                  inIntermediateCount + \".\");\n+        }\n+        if (moveFailedCount \u003e 0) {\n+          LOG.warn(\"Waiting to remove MOVE_FAILED state histories \" +\n+                  \"(e.g. \" + firstMoveFailedKey + \") from JobListCache \" +\n+                  \"because it is not in done yet. Total count is \" +\n+                  moveFailedCount + \".\");\n+        }\n       }\n       return old;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public HistoryFileInfo addIfAbsent(HistoryFileInfo fileInfo) {\n      JobId jobId \u003d fileInfo.getJobId();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Adding \" + jobId + \" to job list cache with \"\n            + fileInfo.getJobIndexInfo());\n      }\n      HistoryFileInfo old \u003d cache.putIfAbsent(jobId, fileInfo);\n      if (cache.size() \u003e maxSize) {\n        //There is a race here, where more then one thread could be trying to\n        // remove entries.  This could result in too many entries being removed\n        // from the cache.  This is considered OK as the size of the cache\n        // should be rather large, and we would rather have performance over\n        // keeping the cache size exactly at the maximum.\n        Iterator\u003cJobId\u003e keys \u003d cache.navigableKeySet().iterator();\n        long cutoff \u003d System.currentTimeMillis() - maxAge;\n\n        // MAPREDUCE-6436: In order to reduce the number of logs written\n        // in case of a lot of move pending histories.\n        JobId firstInIntermediateKey \u003d null;\n        int inIntermediateCount \u003d 0;\n        JobId firstMoveFailedKey \u003d null;\n        int moveFailedCount \u003d 0;\n\n        while(cache.size() \u003e maxSize \u0026\u0026 keys.hasNext()) {\n          JobId key \u003d keys.next();\n          HistoryFileInfo firstValue \u003d cache.get(key);\n          if(firstValue !\u003d null) {\n            synchronized(firstValue) {\n              if (firstValue.isMovePending()) {\n                if(firstValue.didMoveFail() \u0026\u0026\n                    firstValue.jobIndexInfo.getFinishTime() \u003c\u003d cutoff) {\n                  cache.remove(key);\n                  //Now lets try to delete it\n                  try {\n                    firstValue.delete();\n                  } catch (IOException e) {\n                    LOG.error(\"Error while trying to delete history files\" +\n                    \t\t\" that could not be moved to done.\", e);\n                  }\n                } else {\n                  if (firstValue.didMoveFail()) {\n                    if (moveFailedCount \u003d\u003d 0) {\n                      firstMoveFailedKey \u003d key;\n                    }\n                    moveFailedCount +\u003d 1;\n                  } else {\n                    if (inIntermediateCount \u003d\u003d 0) {\n                      firstInIntermediateKey \u003d key;\n                    }\n                    inIntermediateCount +\u003d 1;\n                  }\n                }\n              } else {\n                cache.remove(key);\n              }\n            }\n          }\n        }\n        // Log output only for first jobhisotry in pendings to restrict\n        // the total number of logs.\n        if (inIntermediateCount \u003e 0) {\n          LOG.warn(\"Waiting to remove IN_INTERMEDIATE state histories \" +\n                  \"(e.g. \" + firstInIntermediateKey + \") from JobListCache \" +\n                  \"because it is not in done yet. Total count is \" +\n                  inIntermediateCount + \".\");\n        }\n        if (moveFailedCount \u003e 0) {\n          LOG.warn(\"Waiting to remove MOVE_FAILED state histories \" +\n                  \"(e.g. \" + firstMoveFailedKey + \") from JobListCache \" +\n                  \"because it is not in done yet. Total count is \" +\n                  moveFailedCount + \".\");\n        }\n      }\n      return old;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
      "extendedDetails": {}
    },
    "ac44e0a0d0cebc07904518f32e0d8e34a4391adf": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5268. Improve history server startup performance. Contributed by Karthik Kambatla\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489012 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/06/13 7:46 AM",
      "commitName": "ac44e0a0d0cebc07904518f32e0d8e34a4391adf",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "05/04/13 8:43 AM",
      "commitNameOld": "a5734cd38aa501bed3b0c27a608b4b3a90f4eae6",
      "commitAuthorOld": "Thomas Graves",
      "daysBetweenCommits": 58.96,
      "commitsBetweenForRepo": 347,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,44 @@\n     public HistoryFileInfo addIfAbsent(HistoryFileInfo fileInfo) {\n-      JobId jobId \u003d fileInfo.getJobIndexInfo().getJobId();\n+      JobId jobId \u003d fileInfo.getJobId();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Adding \" + jobId + \" to job list cache with \"\n             + fileInfo.getJobIndexInfo());\n       }\n       HistoryFileInfo old \u003d cache.putIfAbsent(jobId, fileInfo);\n       if (cache.size() \u003e maxSize) {\n         //There is a race here, where more then one thread could be trying to\n         // remove entries.  This could result in too many entries being removed\n         // from the cache.  This is considered OK as the size of the cache\n         // should be rather large, and we would rather have performance over\n         // keeping the cache size exactly at the maximum.\n         Iterator\u003cJobId\u003e keys \u003d cache.navigableKeySet().iterator();\n         long cutoff \u003d System.currentTimeMillis() - maxAge;\n         while(cache.size() \u003e maxSize \u0026\u0026 keys.hasNext()) {\n           JobId key \u003d keys.next();\n           HistoryFileInfo firstValue \u003d cache.get(key);\n           if(firstValue !\u003d null) {\n             synchronized(firstValue) {\n               if (firstValue.isMovePending()) {\n                 if(firstValue.didMoveFail() \u0026\u0026 \n                     firstValue.jobIndexInfo.getFinishTime() \u003c\u003d cutoff) {\n                   cache.remove(key);\n                   //Now lets try to delete it\n                   try {\n                     firstValue.delete();\n                   } catch (IOException e) {\n                     LOG.error(\"Error while trying to delete history files\" +\n                     \t\t\" that could not be moved to done.\", e);\n                   }\n                 } else {\n                   LOG.warn(\"Waiting to remove \" + key\n                       + \" from JobListCache because it is not in done yet.\");\n                 }\n               } else {\n                 cache.remove(key);\n               }\n             }\n           }\n         }\n       }\n       return old;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public HistoryFileInfo addIfAbsent(HistoryFileInfo fileInfo) {\n      JobId jobId \u003d fileInfo.getJobId();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Adding \" + jobId + \" to job list cache with \"\n            + fileInfo.getJobIndexInfo());\n      }\n      HistoryFileInfo old \u003d cache.putIfAbsent(jobId, fileInfo);\n      if (cache.size() \u003e maxSize) {\n        //There is a race here, where more then one thread could be trying to\n        // remove entries.  This could result in too many entries being removed\n        // from the cache.  This is considered OK as the size of the cache\n        // should be rather large, and we would rather have performance over\n        // keeping the cache size exactly at the maximum.\n        Iterator\u003cJobId\u003e keys \u003d cache.navigableKeySet().iterator();\n        long cutoff \u003d System.currentTimeMillis() - maxAge;\n        while(cache.size() \u003e maxSize \u0026\u0026 keys.hasNext()) {\n          JobId key \u003d keys.next();\n          HistoryFileInfo firstValue \u003d cache.get(key);\n          if(firstValue !\u003d null) {\n            synchronized(firstValue) {\n              if (firstValue.isMovePending()) {\n                if(firstValue.didMoveFail() \u0026\u0026 \n                    firstValue.jobIndexInfo.getFinishTime() \u003c\u003d cutoff) {\n                  cache.remove(key);\n                  //Now lets try to delete it\n                  try {\n                    firstValue.delete();\n                  } catch (IOException e) {\n                    LOG.error(\"Error while trying to delete history files\" +\n                    \t\t\" that could not be moved to done.\", e);\n                  }\n                } else {\n                  LOG.warn(\"Waiting to remove \" + key\n                      + \" from JobListCache because it is not in done yet.\");\n                }\n              } else {\n                cache.remove(key);\n              }\n            }\n          }\n        }\n      }\n      return old;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
      "extendedDetails": {}
    },
    "7d04a96027ad75877b41b7cd8f67455dd13159d7": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-3972. Fix locking and exception issues in JobHistory server. (Contributed by Robert Joseph Evans)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1327354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/04/12 6:59 PM",
      "commitName": "7d04a96027ad75877b41b7cd8f67455dd13159d7",
      "commitAuthor": "Siddharth Seth",
      "diff": "@@ -0,0 +1,44 @@\n+    public HistoryFileInfo addIfAbsent(HistoryFileInfo fileInfo) {\n+      JobId jobId \u003d fileInfo.getJobIndexInfo().getJobId();\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Adding \" + jobId + \" to job list cache with \"\n+            + fileInfo.getJobIndexInfo());\n+      }\n+      HistoryFileInfo old \u003d cache.putIfAbsent(jobId, fileInfo);\n+      if (cache.size() \u003e maxSize) {\n+        //There is a race here, where more then one thread could be trying to\n+        // remove entries.  This could result in too many entries being removed\n+        // from the cache.  This is considered OK as the size of the cache\n+        // should be rather large, and we would rather have performance over\n+        // keeping the cache size exactly at the maximum.\n+        Iterator\u003cJobId\u003e keys \u003d cache.navigableKeySet().iterator();\n+        long cutoff \u003d System.currentTimeMillis() - maxAge;\n+        while(cache.size() \u003e maxSize \u0026\u0026 keys.hasNext()) {\n+          JobId key \u003d keys.next();\n+          HistoryFileInfo firstValue \u003d cache.get(key);\n+          if(firstValue !\u003d null) {\n+            synchronized(firstValue) {\n+              if (firstValue.isMovePending()) {\n+                if(firstValue.didMoveFail() \u0026\u0026 \n+                    firstValue.jobIndexInfo.getFinishTime() \u003c\u003d cutoff) {\n+                  cache.remove(key);\n+                  //Now lets try to delete it\n+                  try {\n+                    firstValue.delete();\n+                  } catch (IOException e) {\n+                    LOG.error(\"Error while trying to delete history files\" +\n+                    \t\t\" that could not be moved to done.\", e);\n+                  }\n+                } else {\n+                  LOG.warn(\"Waiting to remove \" + key\n+                      + \" from JobListCache because it is not in done yet.\");\n+                }\n+              } else {\n+                cache.remove(key);\n+              }\n+            }\n+          }\n+        }\n+      }\n+      return old;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public HistoryFileInfo addIfAbsent(HistoryFileInfo fileInfo) {\n      JobId jobId \u003d fileInfo.getJobIndexInfo().getJobId();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Adding \" + jobId + \" to job list cache with \"\n            + fileInfo.getJobIndexInfo());\n      }\n      HistoryFileInfo old \u003d cache.putIfAbsent(jobId, fileInfo);\n      if (cache.size() \u003e maxSize) {\n        //There is a race here, where more then one thread could be trying to\n        // remove entries.  This could result in too many entries being removed\n        // from the cache.  This is considered OK as the size of the cache\n        // should be rather large, and we would rather have performance over\n        // keeping the cache size exactly at the maximum.\n        Iterator\u003cJobId\u003e keys \u003d cache.navigableKeySet().iterator();\n        long cutoff \u003d System.currentTimeMillis() - maxAge;\n        while(cache.size() \u003e maxSize \u0026\u0026 keys.hasNext()) {\n          JobId key \u003d keys.next();\n          HistoryFileInfo firstValue \u003d cache.get(key);\n          if(firstValue !\u003d null) {\n            synchronized(firstValue) {\n              if (firstValue.isMovePending()) {\n                if(firstValue.didMoveFail() \u0026\u0026 \n                    firstValue.jobIndexInfo.getFinishTime() \u003c\u003d cutoff) {\n                  cache.remove(key);\n                  //Now lets try to delete it\n                  try {\n                    firstValue.delete();\n                  } catch (IOException e) {\n                    LOG.error(\"Error while trying to delete history files\" +\n                    \t\t\" that could not be moved to done.\", e);\n                  }\n                } else {\n                  LOG.warn(\"Waiting to remove \" + key\n                      + \" from JobListCache because it is not in done yet.\");\n                }\n              } else {\n                cache.remove(key);\n              }\n            }\n          }\n        }\n      }\n      return old;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java"
    }
  }
}