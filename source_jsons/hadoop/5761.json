{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "NameNodeRpcServer.java",
  "functionName": "create",
  "functionId": "create___src-String__masked-FsPermission__clientName-String__flag-EnumSetWritable__CreateFlag____createParent-boolean__replication-short__blockSize-long__supportedVersions-CryptoProtocolVersion[]__ecPolicyName-String__storagePolicy-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
  "functionStartLine": 781,
  "functionEndLine": 817,
  "numCommitsSeen": 953,
  "timeTaken": 8976,
  "changeHistory": [
    "0d7a5ac5f526801367a9ec963e6d72783b637d55",
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
    "7817674a3a4d097b647dd77f1345787dd376d5ea",
    "db334bb8625da97c7e518cbcf477530c7ba7001e",
    "8e253cb93030642f5a7324bad0f161cd0ad33206",
    "e96ce6f3e3e549202ce3c48d4733ba34098870ad"
  ],
  "changeHistoryShort": {
    "0d7a5ac5f526801367a9ec963e6d72783b637d55": "Ymultichange(Yparameterchange,Ybodychange)",
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1": "Ymultichange(Yparameterchange,Ybodychange)",
    "7817674a3a4d097b647dd77f1345787dd376d5ea": "Ybodychange",
    "db334bb8625da97c7e518cbcf477530c7ba7001e": "Ybodychange",
    "8e253cb93030642f5a7324bad0f161cd0ad33206": "Ybodychange",
    "e96ce6f3e3e549202ce3c48d4733ba34098870ad": "Ymultichange(Yparameterchange,Ybodychange)"
  },
  "changeHistoryDetails": {
    "0d7a5ac5f526801367a9ec963e6d72783b637d55": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-13209. DistributedFileSystem.create should allow an option to provide StoragePolicy. Contributed by Ayush Saxena.\n",
      "commitDate": "14/02/19 8:43 AM",
      "commitName": "0d7a5ac5f526801367a9ec963e6d72783b637d55",
      "commitAuthor": "Surendra Singh Lilhore",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13209. DistributedFileSystem.create should allow an option to provide StoragePolicy. Contributed by Ayush Saxena.\n",
          "commitDate": "14/02/19 8:43 AM",
          "commitName": "0d7a5ac5f526801367a9ec963e6d72783b637d55",
          "commitAuthor": "Surendra Singh Lilhore",
          "commitDateOld": "13/02/19 12:40 PM",
          "commitNameOld": "024c87291cb4cc67282fe5645fb827427cc581c6",
          "commitAuthorOld": "Chen Liang",
          "daysBetweenCommits": 0.84,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,37 @@\n   public HdfsFileStatus create(String src, FsPermission masked,\n       String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n       boolean createParent, short replication, long blockSize,\n-      CryptoProtocolVersion[] supportedVersions, String ecPolicyName)\n+      CryptoProtocolVersion[] supportedVersions, String ecPolicyName,\n+      String storagePolicy)\n       throws IOException {\n     checkNNStartup();\n     String clientMachine \u003d getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n           +src+\" for \"+clientName+\" at \"+clientMachine);\n     }\n     if (!checkPathLength(src)) {\n       throw new IOException(\"create: Pathname too long.  Limit \"\n           + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n     }\n     namesystem.checkOperation(OperationCategory.WRITE);\n     CacheEntryWithPayload cacheEntry \u003d RetryCache.waitForCompletion(retryCache, null);\n     if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n       return (HdfsFileStatus) cacheEntry.getPayload();\n     }\n \n     HdfsFileStatus status \u003d null;\n     try {\n       PermissionStatus perm \u003d new PermissionStatus(getRemoteUser()\n           .getShortUserName(), null, masked);\n       status \u003d namesystem.startFile(src, perm, clientName, clientMachine,\n           flag.get(), createParent, replication, blockSize, supportedVersions,\n-          ecPolicyName, cacheEntry !\u003d null);\n+          ecPolicyName, storagePolicy, cacheEntry !\u003d null);\n     } finally {\n       RetryCache.setState(cacheEntry, status !\u003d null, status);\n     }\n \n     metrics.incrFilesCreated();\n     metrics.incrCreateFileOps();\n     return status;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public HdfsFileStatus create(String src, FsPermission masked,\n      String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n      boolean createParent, short replication, long blockSize,\n      CryptoProtocolVersion[] supportedVersions, String ecPolicyName,\n      String storagePolicy)\n      throws IOException {\n    checkNNStartup();\n    String clientMachine \u003d getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n          +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    if (!checkPathLength(src)) {\n      throw new IOException(\"create: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n    namesystem.checkOperation(OperationCategory.WRITE);\n    CacheEntryWithPayload cacheEntry \u003d RetryCache.waitForCompletion(retryCache, null);\n    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n      return (HdfsFileStatus) cacheEntry.getPayload();\n    }\n\n    HdfsFileStatus status \u003d null;\n    try {\n      PermissionStatus perm \u003d new PermissionStatus(getRemoteUser()\n          .getShortUserName(), null, masked);\n      status \u003d namesystem.startFile(src, perm, clientName, clientMachine,\n          flag.get(), createParent, replication, blockSize, supportedVersions,\n          ecPolicyName, storagePolicy, cacheEntry !\u003d null);\n    } finally {\n      RetryCache.setState(cacheEntry, status !\u003d null, status);\n    }\n\n    metrics.incrFilesCreated();\n    metrics.incrCreateFileOps();\n    return status;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "[src-String, masked-FsPermission, clientName-String, flag-EnumSetWritable\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, supportedVersions-CryptoProtocolVersion[], ecPolicyName-String]",
            "newValue": "[src-String, masked-FsPermission, clientName-String, flag-EnumSetWritable\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, supportedVersions-CryptoProtocolVersion[], ecPolicyName-String, storagePolicy-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13209. DistributedFileSystem.create should allow an option to provide StoragePolicy. Contributed by Ayush Saxena.\n",
          "commitDate": "14/02/19 8:43 AM",
          "commitName": "0d7a5ac5f526801367a9ec963e6d72783b637d55",
          "commitAuthor": "Surendra Singh Lilhore",
          "commitDateOld": "13/02/19 12:40 PM",
          "commitNameOld": "024c87291cb4cc67282fe5645fb827427cc581c6",
          "commitAuthorOld": "Chen Liang",
          "daysBetweenCommits": 0.84,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,37 @@\n   public HdfsFileStatus create(String src, FsPermission masked,\n       String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n       boolean createParent, short replication, long blockSize,\n-      CryptoProtocolVersion[] supportedVersions, String ecPolicyName)\n+      CryptoProtocolVersion[] supportedVersions, String ecPolicyName,\n+      String storagePolicy)\n       throws IOException {\n     checkNNStartup();\n     String clientMachine \u003d getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n           +src+\" for \"+clientName+\" at \"+clientMachine);\n     }\n     if (!checkPathLength(src)) {\n       throw new IOException(\"create: Pathname too long.  Limit \"\n           + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n     }\n     namesystem.checkOperation(OperationCategory.WRITE);\n     CacheEntryWithPayload cacheEntry \u003d RetryCache.waitForCompletion(retryCache, null);\n     if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n       return (HdfsFileStatus) cacheEntry.getPayload();\n     }\n \n     HdfsFileStatus status \u003d null;\n     try {\n       PermissionStatus perm \u003d new PermissionStatus(getRemoteUser()\n           .getShortUserName(), null, masked);\n       status \u003d namesystem.startFile(src, perm, clientName, clientMachine,\n           flag.get(), createParent, replication, blockSize, supportedVersions,\n-          ecPolicyName, cacheEntry !\u003d null);\n+          ecPolicyName, storagePolicy, cacheEntry !\u003d null);\n     } finally {\n       RetryCache.setState(cacheEntry, status !\u003d null, status);\n     }\n \n     metrics.incrFilesCreated();\n     metrics.incrCreateFileOps();\n     return status;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public HdfsFileStatus create(String src, FsPermission masked,\n      String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n      boolean createParent, short replication, long blockSize,\n      CryptoProtocolVersion[] supportedVersions, String ecPolicyName,\n      String storagePolicy)\n      throws IOException {\n    checkNNStartup();\n    String clientMachine \u003d getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n          +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    if (!checkPathLength(src)) {\n      throw new IOException(\"create: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n    namesystem.checkOperation(OperationCategory.WRITE);\n    CacheEntryWithPayload cacheEntry \u003d RetryCache.waitForCompletion(retryCache, null);\n    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n      return (HdfsFileStatus) cacheEntry.getPayload();\n    }\n\n    HdfsFileStatus status \u003d null;\n    try {\n      PermissionStatus perm \u003d new PermissionStatus(getRemoteUser()\n          .getShortUserName(), null, masked);\n      status \u003d namesystem.startFile(src, perm, clientName, clientMachine,\n          flag.get(), createParent, replication, blockSize, supportedVersions,\n          ecPolicyName, storagePolicy, cacheEntry !\u003d null);\n    } finally {\n      RetryCache.setState(cacheEntry, status !\u003d null, status);\n    }\n\n    metrics.incrFilesCreated();\n    metrics.incrCreateFileOps();\n    return status;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {}
        }
      ]
    },
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10996. Ability to specify per-file EC policy at create time. Contributed by SammiChen.\n",
      "commitDate": "12/04/17 12:27 PM",
      "commitName": "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10996. Ability to specify per-file EC policy at create time. Contributed by SammiChen.\n",
          "commitDate": "12/04/17 12:27 PM",
          "commitName": "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "20/03/17 9:54 PM",
          "commitNameOld": "e7c8da614c37e36fb8081234f4c639d6054f6082",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 22.61,
          "commitsBetweenForRepo": 150,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,36 @@\n   public HdfsFileStatus create(String src, FsPermission masked,\n       String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n-      boolean createParent, short replication, long blockSize, \n-      CryptoProtocolVersion[] supportedVersions)\n+      boolean createParent, short replication, long blockSize,\n+      CryptoProtocolVersion[] supportedVersions, String ecPolicyName)\n       throws IOException {\n     checkNNStartup();\n     String clientMachine \u003d getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n           +src+\" for \"+clientName+\" at \"+clientMachine);\n     }\n     if (!checkPathLength(src)) {\n       throw new IOException(\"create: Pathname too long.  Limit \"\n           + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n     }\n     namesystem.checkOperation(OperationCategory.WRITE);\n     CacheEntryWithPayload cacheEntry \u003d RetryCache.waitForCompletion(retryCache, null);\n     if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n       return (HdfsFileStatus) cacheEntry.getPayload();\n     }\n \n     HdfsFileStatus status \u003d null;\n     try {\n       PermissionStatus perm \u003d new PermissionStatus(getRemoteUser()\n           .getShortUserName(), null, masked);\n       status \u003d namesystem.startFile(src, perm, clientName, clientMachine,\n           flag.get(), createParent, replication, blockSize, supportedVersions,\n-          cacheEntry !\u003d null);\n+          ecPolicyName, cacheEntry !\u003d null);\n     } finally {\n       RetryCache.setState(cacheEntry, status !\u003d null, status);\n     }\n \n     metrics.incrFilesCreated();\n     metrics.incrCreateFileOps();\n     return status;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public HdfsFileStatus create(String src, FsPermission masked,\n      String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n      boolean createParent, short replication, long blockSize,\n      CryptoProtocolVersion[] supportedVersions, String ecPolicyName)\n      throws IOException {\n    checkNNStartup();\n    String clientMachine \u003d getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n          +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    if (!checkPathLength(src)) {\n      throw new IOException(\"create: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n    namesystem.checkOperation(OperationCategory.WRITE);\n    CacheEntryWithPayload cacheEntry \u003d RetryCache.waitForCompletion(retryCache, null);\n    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n      return (HdfsFileStatus) cacheEntry.getPayload();\n    }\n\n    HdfsFileStatus status \u003d null;\n    try {\n      PermissionStatus perm \u003d new PermissionStatus(getRemoteUser()\n          .getShortUserName(), null, masked);\n      status \u003d namesystem.startFile(src, perm, clientName, clientMachine,\n          flag.get(), createParent, replication, blockSize, supportedVersions,\n          ecPolicyName, cacheEntry !\u003d null);\n    } finally {\n      RetryCache.setState(cacheEntry, status !\u003d null, status);\n    }\n\n    metrics.incrFilesCreated();\n    metrics.incrCreateFileOps();\n    return status;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "[src-String, masked-FsPermission, clientName-String, flag-EnumSetWritable\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, supportedVersions-CryptoProtocolVersion[]]",
            "newValue": "[src-String, masked-FsPermission, clientName-String, flag-EnumSetWritable\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, supportedVersions-CryptoProtocolVersion[], ecPolicyName-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10996. Ability to specify per-file EC policy at create time. Contributed by SammiChen.\n",
          "commitDate": "12/04/17 12:27 PM",
          "commitName": "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "20/03/17 9:54 PM",
          "commitNameOld": "e7c8da614c37e36fb8081234f4c639d6054f6082",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 22.61,
          "commitsBetweenForRepo": 150,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,36 @@\n   public HdfsFileStatus create(String src, FsPermission masked,\n       String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n-      boolean createParent, short replication, long blockSize, \n-      CryptoProtocolVersion[] supportedVersions)\n+      boolean createParent, short replication, long blockSize,\n+      CryptoProtocolVersion[] supportedVersions, String ecPolicyName)\n       throws IOException {\n     checkNNStartup();\n     String clientMachine \u003d getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n           +src+\" for \"+clientName+\" at \"+clientMachine);\n     }\n     if (!checkPathLength(src)) {\n       throw new IOException(\"create: Pathname too long.  Limit \"\n           + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n     }\n     namesystem.checkOperation(OperationCategory.WRITE);\n     CacheEntryWithPayload cacheEntry \u003d RetryCache.waitForCompletion(retryCache, null);\n     if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n       return (HdfsFileStatus) cacheEntry.getPayload();\n     }\n \n     HdfsFileStatus status \u003d null;\n     try {\n       PermissionStatus perm \u003d new PermissionStatus(getRemoteUser()\n           .getShortUserName(), null, masked);\n       status \u003d namesystem.startFile(src, perm, clientName, clientMachine,\n           flag.get(), createParent, replication, blockSize, supportedVersions,\n-          cacheEntry !\u003d null);\n+          ecPolicyName, cacheEntry !\u003d null);\n     } finally {\n       RetryCache.setState(cacheEntry, status !\u003d null, status);\n     }\n \n     metrics.incrFilesCreated();\n     metrics.incrCreateFileOps();\n     return status;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public HdfsFileStatus create(String src, FsPermission masked,\n      String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n      boolean createParent, short replication, long blockSize,\n      CryptoProtocolVersion[] supportedVersions, String ecPolicyName)\n      throws IOException {\n    checkNNStartup();\n    String clientMachine \u003d getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n          +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    if (!checkPathLength(src)) {\n      throw new IOException(\"create: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n    namesystem.checkOperation(OperationCategory.WRITE);\n    CacheEntryWithPayload cacheEntry \u003d RetryCache.waitForCompletion(retryCache, null);\n    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n      return (HdfsFileStatus) cacheEntry.getPayload();\n    }\n\n    HdfsFileStatus status \u003d null;\n    try {\n      PermissionStatus perm \u003d new PermissionStatus(getRemoteUser()\n          .getShortUserName(), null, masked);\n      status \u003d namesystem.startFile(src, perm, clientName, clientMachine,\n          flag.get(), createParent, replication, blockSize, supportedVersions,\n          ecPolicyName, cacheEntry !\u003d null);\n    } finally {\n      RetryCache.setState(cacheEntry, status !\u003d null, status);\n    }\n\n    metrics.incrFilesCreated();\n    metrics.incrCreateFileOps();\n    return status;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {}
        }
      ]
    },
    "7817674a3a4d097b647dd77f1345787dd376d5ea": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7609. Avoid retry cache collision when Standby NameNode loading edits. Contributed by Ming Ma.\n",
      "commitDate": "29/05/15 11:05 AM",
      "commitName": "7817674a3a4d097b647dd77f1345787dd376d5ea",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "15/05/15 7:09 PM",
      "commitNameOld": "e5afac5896a1a88e152746598527d91f73cbb724",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 13.66,
      "commitsBetweenForRepo": 105,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,36 @@\n   public HdfsFileStatus create(String src, FsPermission masked,\n       String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n       boolean createParent, short replication, long blockSize, \n       CryptoProtocolVersion[] supportedVersions)\n       throws IOException {\n     checkNNStartup();\n     String clientMachine \u003d getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n           +src+\" for \"+clientName+\" at \"+clientMachine);\n     }\n     if (!checkPathLength(src)) {\n       throw new IOException(\"create: Pathname too long.  Limit \"\n           + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n     }\n-\n+    namesystem.checkOperation(OperationCategory.WRITE);\n     CacheEntryWithPayload cacheEntry \u003d RetryCache.waitForCompletion(retryCache, null);\n     if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n       return (HdfsFileStatus) cacheEntry.getPayload();\n     }\n \n     HdfsFileStatus status \u003d null;\n     try {\n       PermissionStatus perm \u003d new PermissionStatus(getRemoteUser()\n           .getShortUserName(), null, masked);\n       status \u003d namesystem.startFile(src, perm, clientName, clientMachine,\n           flag.get(), createParent, replication, blockSize, supportedVersions,\n           cacheEntry !\u003d null);\n     } finally {\n       RetryCache.setState(cacheEntry, status !\u003d null, status);\n     }\n \n     metrics.incrFilesCreated();\n     metrics.incrCreateFileOps();\n     return status;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public HdfsFileStatus create(String src, FsPermission masked,\n      String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n      boolean createParent, short replication, long blockSize, \n      CryptoProtocolVersion[] supportedVersions)\n      throws IOException {\n    checkNNStartup();\n    String clientMachine \u003d getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n          +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    if (!checkPathLength(src)) {\n      throw new IOException(\"create: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n    namesystem.checkOperation(OperationCategory.WRITE);\n    CacheEntryWithPayload cacheEntry \u003d RetryCache.waitForCompletion(retryCache, null);\n    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n      return (HdfsFileStatus) cacheEntry.getPayload();\n    }\n\n    HdfsFileStatus status \u003d null;\n    try {\n      PermissionStatus perm \u003d new PermissionStatus(getRemoteUser()\n          .getShortUserName(), null, masked);\n      status \u003d namesystem.startFile(src, perm, clientName, clientMachine,\n          flag.get(), createParent, replication, blockSize, supportedVersions,\n          cacheEntry !\u003d null);\n    } finally {\n      RetryCache.setState(cacheEntry, status !\u003d null, status);\n    }\n\n    metrics.incrFilesCreated();\n    metrics.incrCreateFileOps();\n    return status;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
      "extendedDetails": {}
    },
    "db334bb8625da97c7e518cbcf477530c7ba7001e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3443. Fix NPE when namenode transition to active during startup by adding checkNNStartup() in NameNodeRpcServer.  Contributed by Vinayakumar B\n",
      "commitDate": "21/01/15 11:33 AM",
      "commitName": "db334bb8625da97c7e518cbcf477530c7ba7001e",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "12/01/15 10:50 PM",
      "commitNameOld": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
      "commitAuthorOld": "Plamen Jeliazkov",
      "daysBetweenCommits": 8.53,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,36 @@\n   public HdfsFileStatus create(String src, FsPermission masked,\n       String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n       boolean createParent, short replication, long blockSize, \n       CryptoProtocolVersion[] supportedVersions)\n       throws IOException {\n+    checkNNStartup();\n     String clientMachine \u003d getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n           +src+\" for \"+clientName+\" at \"+clientMachine);\n     }\n     if (!checkPathLength(src)) {\n       throw new IOException(\"create: Pathname too long.  Limit \"\n           + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n     }\n \n     CacheEntryWithPayload cacheEntry \u003d RetryCache.waitForCompletion(retryCache, null);\n     if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n       return (HdfsFileStatus) cacheEntry.getPayload();\n     }\n \n     HdfsFileStatus status \u003d null;\n     try {\n       PermissionStatus perm \u003d new PermissionStatus(getRemoteUser()\n           .getShortUserName(), null, masked);\n       status \u003d namesystem.startFile(src, perm, clientName, clientMachine,\n           flag.get(), createParent, replication, blockSize, supportedVersions,\n           cacheEntry !\u003d null);\n     } finally {\n       RetryCache.setState(cacheEntry, status !\u003d null, status);\n     }\n \n     metrics.incrFilesCreated();\n     metrics.incrCreateFileOps();\n     return status;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public HdfsFileStatus create(String src, FsPermission masked,\n      String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n      boolean createParent, short replication, long blockSize, \n      CryptoProtocolVersion[] supportedVersions)\n      throws IOException {\n    checkNNStartup();\n    String clientMachine \u003d getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n          +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    if (!checkPathLength(src)) {\n      throw new IOException(\"create: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n\n    CacheEntryWithPayload cacheEntry \u003d RetryCache.waitForCompletion(retryCache, null);\n    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n      return (HdfsFileStatus) cacheEntry.getPayload();\n    }\n\n    HdfsFileStatus status \u003d null;\n    try {\n      PermissionStatus perm \u003d new PermissionStatus(getRemoteUser()\n          .getShortUserName(), null, masked);\n      status \u003d namesystem.startFile(src, perm, clientName, clientMachine,\n          flag.get(), createParent, replication, blockSize, supportedVersions,\n          cacheEntry !\u003d null);\n    } finally {\n      RetryCache.setState(cacheEntry, status !\u003d null, status);\n    }\n\n    metrics.incrFilesCreated();\n    metrics.incrCreateFileOps();\n    return status;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
      "extendedDetails": {}
    },
    "8e253cb93030642f5a7324bad0f161cd0ad33206": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7412. Move RetryCache to NameNodeRpcServer. Contributed by Haohui Mai.\n",
      "commitDate": "24/11/14 11:11 AM",
      "commitName": "8e253cb93030642f5a7324bad0f161cd0ad33206",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "04/11/14 4:02 PM",
      "commitNameOld": "5bd3a569f941ffcfc425a55288bec78a37a75aa1",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 19.8,
      "commitsBetweenForRepo": 170,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,35 @@\n   public HdfsFileStatus create(String src, FsPermission masked,\n       String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n       boolean createParent, short replication, long blockSize, \n       CryptoProtocolVersion[] supportedVersions)\n       throws IOException {\n     String clientMachine \u003d getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n-                         +src+\" for \"+clientName+\" at \"+clientMachine);\n+          +src+\" for \"+clientName+\" at \"+clientMachine);\n     }\n     if (!checkPathLength(src)) {\n       throw new IOException(\"create: Pathname too long.  Limit \"\n           + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n     }\n-    HdfsFileStatus fileStatus \u003d namesystem.startFile(src, new PermissionStatus(\n-        getRemoteUser().getShortUserName(), null, masked),\n-        clientName, clientMachine, flag.get(), createParent, replication,\n-        blockSize, supportedVersions);\n+\n+    CacheEntryWithPayload cacheEntry \u003d RetryCache.waitForCompletion(retryCache, null);\n+    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n+      return (HdfsFileStatus) cacheEntry.getPayload();\n+    }\n+\n+    HdfsFileStatus status \u003d null;\n+    try {\n+      PermissionStatus perm \u003d new PermissionStatus(getRemoteUser()\n+          .getShortUserName(), null, masked);\n+      status \u003d namesystem.startFile(src, perm, clientName, clientMachine,\n+          flag.get(), createParent, replication, blockSize, supportedVersions,\n+          cacheEntry !\u003d null);\n+    } finally {\n+      RetryCache.setState(cacheEntry, status !\u003d null, status);\n+    }\n+\n     metrics.incrFilesCreated();\n     metrics.incrCreateFileOps();\n-    return fileStatus;\n+    return status;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public HdfsFileStatus create(String src, FsPermission masked,\n      String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n      boolean createParent, short replication, long blockSize, \n      CryptoProtocolVersion[] supportedVersions)\n      throws IOException {\n    String clientMachine \u003d getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n          +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    if (!checkPathLength(src)) {\n      throw new IOException(\"create: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n\n    CacheEntryWithPayload cacheEntry \u003d RetryCache.waitForCompletion(retryCache, null);\n    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n      return (HdfsFileStatus) cacheEntry.getPayload();\n    }\n\n    HdfsFileStatus status \u003d null;\n    try {\n      PermissionStatus perm \u003d new PermissionStatus(getRemoteUser()\n          .getShortUserName(), null, masked);\n      status \u003d namesystem.startFile(src, perm, clientName, clientMachine,\n          flag.get(), createParent, replication, blockSize, supportedVersions,\n          cacheEntry !\u003d null);\n    } finally {\n      RetryCache.setState(cacheEntry, status !\u003d null, status);\n    }\n\n    metrics.incrFilesCreated();\n    metrics.incrCreateFileOps();\n    return status;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
      "extendedDetails": {}
    },
    "e96ce6f3e3e549202ce3c48d4733ba34098870ad": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7077. Separate CipherSuite from crypto protocol version. (wang)\n",
      "commitDate": "25/09/14 6:40 PM",
      "commitName": "e96ce6f3e3e549202ce3c48d4733ba34098870ad",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7077. Separate CipherSuite from crypto protocol version. (wang)\n",
          "commitDate": "25/09/14 6:40 PM",
          "commitName": "e96ce6f3e3e549202ce3c48d4733ba34098870ad",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "24/09/14 7:11 PM",
          "commitNameOld": "428a76663a0de5d0d74cc9525273ddc470760e44",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.98,
          "commitsBetweenForRepo": 13,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,22 @@\n   public HdfsFileStatus create(String src, FsPermission masked,\n       String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n       boolean createParent, short replication, long blockSize, \n-      List\u003cCipherSuite\u003e cipherSuites)\n+      CryptoProtocolVersion[] supportedVersions)\n       throws IOException {\n     String clientMachine \u003d getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n                          +src+\" for \"+clientName+\" at \"+clientMachine);\n     }\n     if (!checkPathLength(src)) {\n       throw new IOException(\"create: Pathname too long.  Limit \"\n           + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n     }\n     HdfsFileStatus fileStatus \u003d namesystem.startFile(src, new PermissionStatus(\n         getRemoteUser().getShortUserName(), null, masked),\n         clientName, clientMachine, flag.get(), createParent, replication,\n-        blockSize, cipherSuites);\n+        blockSize, supportedVersions);\n     metrics.incrFilesCreated();\n     metrics.incrCreateFileOps();\n     return fileStatus;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public HdfsFileStatus create(String src, FsPermission masked,\n      String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n      boolean createParent, short replication, long blockSize, \n      CryptoProtocolVersion[] supportedVersions)\n      throws IOException {\n    String clientMachine \u003d getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n                         +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    if (!checkPathLength(src)) {\n      throw new IOException(\"create: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n    HdfsFileStatus fileStatus \u003d namesystem.startFile(src, new PermissionStatus(\n        getRemoteUser().getShortUserName(), null, masked),\n        clientName, clientMachine, flag.get(), createParent, replication,\n        blockSize, supportedVersions);\n    metrics.incrFilesCreated();\n    metrics.incrCreateFileOps();\n    return fileStatus;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "[src-String, masked-FsPermission, clientName-String, flag-EnumSetWritable\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, cipherSuites-List\u003cCipherSuite\u003e]",
            "newValue": "[src-String, masked-FsPermission, clientName-String, flag-EnumSetWritable\u003cCreateFlag\u003e, createParent-boolean, replication-short, blockSize-long, supportedVersions-CryptoProtocolVersion[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7077. Separate CipherSuite from crypto protocol version. (wang)\n",
          "commitDate": "25/09/14 6:40 PM",
          "commitName": "e96ce6f3e3e549202ce3c48d4733ba34098870ad",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "24/09/14 7:11 PM",
          "commitNameOld": "428a76663a0de5d0d74cc9525273ddc470760e44",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.98,
          "commitsBetweenForRepo": 13,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,22 @@\n   public HdfsFileStatus create(String src, FsPermission masked,\n       String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n       boolean createParent, short replication, long blockSize, \n-      List\u003cCipherSuite\u003e cipherSuites)\n+      CryptoProtocolVersion[] supportedVersions)\n       throws IOException {\n     String clientMachine \u003d getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n                          +src+\" for \"+clientName+\" at \"+clientMachine);\n     }\n     if (!checkPathLength(src)) {\n       throw new IOException(\"create: Pathname too long.  Limit \"\n           + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n     }\n     HdfsFileStatus fileStatus \u003d namesystem.startFile(src, new PermissionStatus(\n         getRemoteUser().getShortUserName(), null, masked),\n         clientName, clientMachine, flag.get(), createParent, replication,\n-        blockSize, cipherSuites);\n+        blockSize, supportedVersions);\n     metrics.incrFilesCreated();\n     metrics.incrCreateFileOps();\n     return fileStatus;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public HdfsFileStatus create(String src, FsPermission masked,\n      String clientName, EnumSetWritable\u003cCreateFlag\u003e flag,\n      boolean createParent, short replication, long blockSize, \n      CryptoProtocolVersion[] supportedVersions)\n      throws IOException {\n    String clientMachine \u003d getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n                         +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    if (!checkPathLength(src)) {\n      throw new IOException(\"create: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n    HdfsFileStatus fileStatus \u003d namesystem.startFile(src, new PermissionStatus(\n        getRemoteUser().getShortUserName(), null, masked),\n        clientName, clientMachine, flag.get(), createParent, replication,\n        blockSize, supportedVersions);\n    metrics.incrFilesCreated();\n    metrics.incrCreateFileOps();\n    return fileStatus;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {}
        }
      ]
    }
  }
}