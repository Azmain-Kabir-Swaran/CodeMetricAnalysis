{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DistCp.java",
  "functionName": "checkSplitLargeFile",
  "functionId": "checkSplitLargeFile",
  "sourceFilePath": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java",
  "functionStartLine": 250,
  "functionEndLine": 276,
  "numCommitsSeen": 31,
  "timeTaken": 1083,
  "changeHistory": [
    "26172a94d6431e70d7fe15d66be9a7e195f79f60"
  ],
  "changeHistoryShort": {
    "26172a94d6431e70d7fe15d66be9a7e195f79f60": "Yintroduced"
  },
  "changeHistoryDetails": {
    "26172a94d6431e70d7fe15d66be9a7e195f79f60": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-14267. Make DistCpOptions immutable. Contributed by Mingliang Liu\n",
      "commitDate": "31/03/17 8:04 PM",
      "commitName": "26172a94d6431e70d7fe15d66be9a7e195f79f60",
      "commitAuthor": "Mingliang Liu",
      "diff": "@@ -0,0 +1,27 @@\n+  private void checkSplitLargeFile() throws IOException {\n+    if (!context.splitLargeFile()) {\n+      return;\n+    }\n+\n+    final Path target \u003d context.getTargetPath();\n+    final FileSystem targetFS \u003d target.getFileSystem(getConf());\n+    try {\n+      Path[] src \u003d null;\n+      Path tgt \u003d null;\n+      targetFS.concat(tgt, src);\n+    } catch (UnsupportedOperationException use) {\n+      throw new UnsupportedOperationException(\n+          DistCpOptionSwitch.BLOCKS_PER_CHUNK.getSwitch() +\n+              \" is not supported since the target file system doesn\u0027t\" +\n+              \" support concat.\", use);\n+    } catch (Exception e) {\n+      // Ignore other exception\n+    }\n+\n+    LOG.info(\"Set \" +\n+        DistCpConstants.CONF_LABEL_SIMPLE_LISTING_RANDOMIZE_FILES\n+        + \" to false since \" + DistCpOptionSwitch.BLOCKS_PER_CHUNK.getSwitch()\n+        + \" is passed.\");\n+    getConf().setBoolean(\n+        DistCpConstants.CONF_LABEL_SIMPLE_LISTING_RANDOMIZE_FILES, false);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void checkSplitLargeFile() throws IOException {\n    if (!context.splitLargeFile()) {\n      return;\n    }\n\n    final Path target \u003d context.getTargetPath();\n    final FileSystem targetFS \u003d target.getFileSystem(getConf());\n    try {\n      Path[] src \u003d null;\n      Path tgt \u003d null;\n      targetFS.concat(tgt, src);\n    } catch (UnsupportedOperationException use) {\n      throw new UnsupportedOperationException(\n          DistCpOptionSwitch.BLOCKS_PER_CHUNK.getSwitch() +\n              \" is not supported since the target file system doesn\u0027t\" +\n              \" support concat.\", use);\n    } catch (Exception e) {\n      // Ignore other exception\n    }\n\n    LOG.info(\"Set \" +\n        DistCpConstants.CONF_LABEL_SIMPLE_LISTING_RANDOMIZE_FILES\n        + \" to false since \" + DistCpOptionSwitch.BLOCKS_PER_CHUNK.getSwitch()\n        + \" is passed.\");\n    getConf().setBoolean(\n        DistCpConstants.CONF_LABEL_SIMPLE_LISTING_RANDOMIZE_FILES, false);\n  }",
      "path": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCp.java"
    }
  }
}