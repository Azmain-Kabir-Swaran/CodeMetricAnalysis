{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DistributedOpportunisticContainerAllocator.java",
  "functionName": "allocateContainersInternal",
  "functionId": "allocateContainersInternal___rmIdentifier-long__appParams-AllocationParams__idCounter-ContainerIdGenerator__blacklist-Set__String____allocatedNodes-Set__String____id-ApplicationAttemptId__allNodes-Map__String,RemoteNode____userName-String__allocations-Map__Resource,List__Allocation______enrichedAsk-EnrichedResourceRequest__maxAllocations-int",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/DistributedOpportunisticContainerAllocator.java",
  "functionStartLine": 177,
  "functionEndLine": 264,
  "numCommitsSeen": 64,
  "timeTaken": 11577,
  "changeHistory": [
    "fb512f50877438acb01fe6b3ec96c12b4db61694",
    "4d3c580b03475a6ec9323d11e6875c542f8e3f6d",
    "96e3027e46a953ca995e4b44ef50bc2a30c7e838",
    "aeadb9432f84e679f00a9a12f63675c456bc14a8",
    "ed3747c1ccc303e206de50c2b74f3c318cb1c416",
    "b733348dde18a242e6c9074c512116a8baf1d281",
    "4a877737182808fe3392a116762ba59973a477a6",
    "ac1e5d4f77e3b9df8dcacb0b1f72eecc27931eb8",
    "aa3cab1eb29c56368d15882d7260a994e615e8d8",
    "10be45986cdf86a89055065b752959bd6369d54f",
    "82c9e061017c32e633e0b0cbb7978749a6df4fb2",
    "e5766b1dbee02ae0ef89618e172f3fb227af19e8",
    "1597630681c784a3d59f5605b87e96197b8139d7",
    "c282a08f3892e2e8ceb58e1e9a411062fbd1fb2b"
  ],
  "changeHistoryShort": {
    "fb512f50877438acb01fe6b3ec96c12b4db61694": "Ybodychange",
    "4d3c580b03475a6ec9323d11e6875c542f8e3f6d": "Ymovefromfile",
    "96e3027e46a953ca995e4b44ef50bc2a30c7e838": "Ymultichange(Yparameterchange,Ybodychange)",
    "aeadb9432f84e679f00a9a12f63675c456bc14a8": "Ymultichange(Yparameterchange,Ybodychange)",
    "ed3747c1ccc303e206de50c2b74f3c318cb1c416": "Ybodychange",
    "b733348dde18a242e6c9074c512116a8baf1d281": "Ymultichange(Yparameterchange,Ybodychange)",
    "4a877737182808fe3392a116762ba59973a477a6": "Ybodychange",
    "ac1e5d4f77e3b9df8dcacb0b1f72eecc27931eb8": "Ybodychange",
    "aa3cab1eb29c56368d15882d7260a994e615e8d8": "Ymultichange(Yparameterchange,Ybodychange)",
    "10be45986cdf86a89055065b752959bd6369d54f": "Ybodychange",
    "82c9e061017c32e633e0b0cbb7978749a6df4fb2": "Ymultichange(Ymovefromfile,Ybodychange,Yrename,Yparameterchange)",
    "e5766b1dbee02ae0ef89618e172f3fb227af19e8": "Ymultichange(Yparameterchange,Ybodychange)",
    "1597630681c784a3d59f5605b87e96197b8139d7": "Ybodychange",
    "c282a08f3892e2e8ceb58e1e9a411062fbd1fb2b": "Yintroduced"
  },
  "changeHistoryDetails": {
    "fb512f50877438acb01fe6b3ec96c12b4db61694": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9697. Efficient allocation of Opportunistic containers. Contributed by Abhishek Modi.\n",
      "commitDate": "12/11/19 3:04 AM",
      "commitName": "fb512f50877438acb01fe6b3ec96c12b4db61694",
      "commitAuthor": "Abhishek Modi",
      "commitDateOld": "30/09/19 11:10 AM",
      "commitNameOld": "4d3c580b03475a6ec9323d11e6875c542f8e3f6d",
      "commitAuthorOld": "Abhishek Modi",
      "daysBetweenCommits": 42.7,
      "commitsBetweenForRepo": 210,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,88 @@\n   private void allocateContainersInternal(long rmIdentifier,\n       AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, Set\u003cString\u003e allocatedNodes,\n       ApplicationAttemptId id, Map\u003cString, RemoteNode\u003e allNodes,\n       String userName, Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n       EnrichedResourceRequest enrichedAsk, int maxAllocations)\n       throws YarnException {\n     if (allNodes.size() \u003d\u003d 0) {\n       LOG.info(\"No nodes currently available to \" +\n           \"allocate OPPORTUNISTIC containers.\");\n       return;\n     }\n     ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (allocations.isEmpty() ? 0 :\n         allocations.get(anyAsk.getCapability()).size());\n     toAllocate \u003d Math.min(toAllocate,\n         appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n     if (maxAllocations \u003e\u003d 0) {\n       toAllocate \u003d Math.min(maxAllocations, toAllocate);\n     }\n     int numAllocated \u003d 0;\n     // Node Candidates are selected as follows:\n     // * Node local candidates selected in loop \u003d\u003d 0\n     // * Rack local candidates selected in loop \u003d\u003d 1\n     // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n     int loopIndex \u003d OFF_SWITCH_LOOP;\n-    if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n+    if (enrichedAsk.getNodeMap().size() \u003e 0) {\n       loopIndex \u003d NODE_LOCAL_LOOP;\n     }\n     while (numAllocated \u003c toAllocate) {\n       Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n           findNodeCandidates(loopIndex, allNodes, blacklist, allocatedNodes,\n               enrichedAsk);\n       for (RemoteNode rNode : nodeCandidates) {\n         String rNodeHost \u003d rNode.getNodeId().getHost();\n         // Ignore black list\n         if (blacklist.contains(rNodeHost)) {\n           LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n               \" [\" + rNodeHost + \"]..\");\n           continue;\n         }\n         String location \u003d ResourceRequest.ANY;\n         if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n-          if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n+          if (enrichedAsk.getNodeMap().containsKey(rNodeHost)) {\n             location \u003d rNodeHost;\n           } else {\n             continue;\n           }\n         } else if (allocatedNodes.contains(rNodeHost)) {\n           LOG.info(\"Opportunistic container has already been allocated on {}.\",\n               rNodeHost);\n           continue;\n         }\n         if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n-          if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n+          if (enrichedAsk.getRackMap().containsKey(\n+              rNode.getRackName())) {\n             location \u003d rNode.getRackName();\n           } else {\n             continue;\n           }\n         }\n         Container container \u003d createContainer(rmIdentifier, appParams,\n             idCounter, id, userName, allocations, location,\n             anyAsk, rNode);\n         numAllocated++;\n         updateMetrics(loopIndex);\n         allocatedNodes.add(rNodeHost);\n         LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n             \"location [\" + location + \"]\");\n         if (numAllocated \u003e\u003d toAllocate) {\n           break;\n         }\n       }\n       if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n-          enrichedAsk.getRackLocations().size() \u003e 0) {\n+          enrichedAsk.getRackMap().size() \u003e 0) {\n         loopIndex \u003d RACK_LOCAL_LOOP;\n       } else {\n         loopIndex++;\n       }\n       // Handle case where there are no nodes remaining after blacklist is\n       // considered.\n       if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n         LOG.warn(\"Unable to allocate any opportunistic containers.\");\n         break;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, Set\u003cString\u003e allocatedNodes,\n      ApplicationAttemptId id, Map\u003cString, RemoteNode\u003e allNodes,\n      String userName, Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n      EnrichedResourceRequest enrichedAsk, int maxAllocations)\n      throws YarnException {\n    if (allNodes.size() \u003d\u003d 0) {\n      LOG.info(\"No nodes currently available to \" +\n          \"allocate OPPORTUNISTIC containers.\");\n      return;\n    }\n    ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (allocations.isEmpty() ? 0 :\n        allocations.get(anyAsk.getCapability()).size());\n    toAllocate \u003d Math.min(toAllocate,\n        appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n    if (maxAllocations \u003e\u003d 0) {\n      toAllocate \u003d Math.min(maxAllocations, toAllocate);\n    }\n    int numAllocated \u003d 0;\n    // Node Candidates are selected as follows:\n    // * Node local candidates selected in loop \u003d\u003d 0\n    // * Rack local candidates selected in loop \u003d\u003d 1\n    // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n    int loopIndex \u003d OFF_SWITCH_LOOP;\n    if (enrichedAsk.getNodeMap().size() \u003e 0) {\n      loopIndex \u003d NODE_LOCAL_LOOP;\n    }\n    while (numAllocated \u003c toAllocate) {\n      Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n          findNodeCandidates(loopIndex, allNodes, blacklist, allocatedNodes,\n              enrichedAsk);\n      for (RemoteNode rNode : nodeCandidates) {\n        String rNodeHost \u003d rNode.getNodeId().getHost();\n        // Ignore black list\n        if (blacklist.contains(rNodeHost)) {\n          LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n              \" [\" + rNodeHost + \"]..\");\n          continue;\n        }\n        String location \u003d ResourceRequest.ANY;\n        if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n          if (enrichedAsk.getNodeMap().containsKey(rNodeHost)) {\n            location \u003d rNodeHost;\n          } else {\n            continue;\n          }\n        } else if (allocatedNodes.contains(rNodeHost)) {\n          LOG.info(\"Opportunistic container has already been allocated on {}.\",\n              rNodeHost);\n          continue;\n        }\n        if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n          if (enrichedAsk.getRackMap().containsKey(\n              rNode.getRackName())) {\n            location \u003d rNode.getRackName();\n          } else {\n            continue;\n          }\n        }\n        Container container \u003d createContainer(rmIdentifier, appParams,\n            idCounter, id, userName, allocations, location,\n            anyAsk, rNode);\n        numAllocated++;\n        updateMetrics(loopIndex);\n        allocatedNodes.add(rNodeHost);\n        LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n            \"location [\" + location + \"]\");\n        if (numAllocated \u003e\u003d toAllocate) {\n          break;\n        }\n      }\n      if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n          enrichedAsk.getRackMap().size() \u003e 0) {\n        loopIndex \u003d RACK_LOCAL_LOOP;\n      } else {\n        loopIndex++;\n      }\n      // Handle case where there are no nodes remaining after blacklist is\n      // considered.\n      if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n        LOG.warn(\"Unable to allocate any opportunistic containers.\");\n        break;\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/DistributedOpportunisticContainerAllocator.java",
      "extendedDetails": {}
    },
    "4d3c580b03475a6ec9323d11e6875c542f8e3f6d": {
      "type": "Ymovefromfile",
      "commitMessage": "YARN-9859. Refactoring of OpportunisticContainerAllocator. Contributed by Abhishek Modi.\n",
      "commitDate": "30/09/19 11:10 AM",
      "commitName": "4d3c580b03475a6ec9323d11e6875c542f8e3f6d",
      "commitAuthor": "Abhishek Modi",
      "commitDateOld": "30/09/19 8:06 AM",
      "commitNameOld": "98ca07ebed2ae3d7e41e5029b5bba6d089d41d43",
      "commitAuthorOld": "bshashikant",
      "daysBetweenCommits": 0.13,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,87 @@\n   private void allocateContainersInternal(long rmIdentifier,\n       AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, Set\u003cString\u003e allocatedNodes,\n       ApplicationAttemptId id, Map\u003cString, RemoteNode\u003e allNodes,\n       String userName, Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n       EnrichedResourceRequest enrichedAsk, int maxAllocations)\n       throws YarnException {\n     if (allNodes.size() \u003d\u003d 0) {\n       LOG.info(\"No nodes currently available to \" +\n           \"allocate OPPORTUNISTIC containers.\");\n       return;\n     }\n     ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (allocations.isEmpty() ? 0 :\n-            allocations.get(anyAsk.getCapability()).size());\n+        allocations.get(anyAsk.getCapability()).size());\n     toAllocate \u003d Math.min(toAllocate,\n         appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n     if (maxAllocations \u003e\u003d 0) {\n       toAllocate \u003d Math.min(maxAllocations, toAllocate);\n     }\n     int numAllocated \u003d 0;\n     // Node Candidates are selected as follows:\n     // * Node local candidates selected in loop \u003d\u003d 0\n     // * Rack local candidates selected in loop \u003d\u003d 1\n     // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n     int loopIndex \u003d OFF_SWITCH_LOOP;\n     if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n       loopIndex \u003d NODE_LOCAL_LOOP;\n     }\n     while (numAllocated \u003c toAllocate) {\n       Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n           findNodeCandidates(loopIndex, allNodes, blacklist, allocatedNodes,\n               enrichedAsk);\n       for (RemoteNode rNode : nodeCandidates) {\n         String rNodeHost \u003d rNode.getNodeId().getHost();\n         // Ignore black list\n         if (blacklist.contains(rNodeHost)) {\n           LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n               \" [\" + rNodeHost + \"]..\");\n           continue;\n         }\n         String location \u003d ResourceRequest.ANY;\n         if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n           if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n             location \u003d rNodeHost;\n           } else {\n             continue;\n           }\n         } else if (allocatedNodes.contains(rNodeHost)) {\n           LOG.info(\"Opportunistic container has already been allocated on {}.\",\n               rNodeHost);\n           continue;\n         }\n         if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n           if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n             location \u003d rNode.getRackName();\n           } else {\n             continue;\n           }\n         }\n         Container container \u003d createContainer(rmIdentifier, appParams,\n             idCounter, id, userName, allocations, location,\n             anyAsk, rNode);\n         numAllocated++;\n         updateMetrics(loopIndex);\n         allocatedNodes.add(rNodeHost);\n         LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n             \"location [\" + location + \"]\");\n         if (numAllocated \u003e\u003d toAllocate) {\n           break;\n         }\n       }\n       if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n           enrichedAsk.getRackLocations().size() \u003e 0) {\n         loopIndex \u003d RACK_LOCAL_LOOP;\n       } else {\n         loopIndex++;\n       }\n       // Handle case where there are no nodes remaining after blacklist is\n       // considered.\n       if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n         LOG.warn(\"Unable to allocate any opportunistic containers.\");\n         break;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, Set\u003cString\u003e allocatedNodes,\n      ApplicationAttemptId id, Map\u003cString, RemoteNode\u003e allNodes,\n      String userName, Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n      EnrichedResourceRequest enrichedAsk, int maxAllocations)\n      throws YarnException {\n    if (allNodes.size() \u003d\u003d 0) {\n      LOG.info(\"No nodes currently available to \" +\n          \"allocate OPPORTUNISTIC containers.\");\n      return;\n    }\n    ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (allocations.isEmpty() ? 0 :\n        allocations.get(anyAsk.getCapability()).size());\n    toAllocate \u003d Math.min(toAllocate,\n        appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n    if (maxAllocations \u003e\u003d 0) {\n      toAllocate \u003d Math.min(maxAllocations, toAllocate);\n    }\n    int numAllocated \u003d 0;\n    // Node Candidates are selected as follows:\n    // * Node local candidates selected in loop \u003d\u003d 0\n    // * Rack local candidates selected in loop \u003d\u003d 1\n    // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n    int loopIndex \u003d OFF_SWITCH_LOOP;\n    if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n      loopIndex \u003d NODE_LOCAL_LOOP;\n    }\n    while (numAllocated \u003c toAllocate) {\n      Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n          findNodeCandidates(loopIndex, allNodes, blacklist, allocatedNodes,\n              enrichedAsk);\n      for (RemoteNode rNode : nodeCandidates) {\n        String rNodeHost \u003d rNode.getNodeId().getHost();\n        // Ignore black list\n        if (blacklist.contains(rNodeHost)) {\n          LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n              \" [\" + rNodeHost + \"]..\");\n          continue;\n        }\n        String location \u003d ResourceRequest.ANY;\n        if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n          if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n            location \u003d rNodeHost;\n          } else {\n            continue;\n          }\n        } else if (allocatedNodes.contains(rNodeHost)) {\n          LOG.info(\"Opportunistic container has already been allocated on {}.\",\n              rNodeHost);\n          continue;\n        }\n        if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n          if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n            location \u003d rNode.getRackName();\n          } else {\n            continue;\n          }\n        }\n        Container container \u003d createContainer(rmIdentifier, appParams,\n            idCounter, id, userName, allocations, location,\n            anyAsk, rNode);\n        numAllocated++;\n        updateMetrics(loopIndex);\n        allocatedNodes.add(rNodeHost);\n        LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n            \"location [\" + location + \"]\");\n        if (numAllocated \u003e\u003d toAllocate) {\n          break;\n        }\n      }\n      if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n          enrichedAsk.getRackLocations().size() \u003e 0) {\n        loopIndex \u003d RACK_LOCAL_LOOP;\n      } else {\n        loopIndex++;\n      }\n      // Handle case where there are no nodes remaining after blacklist is\n      // considered.\n      if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n        LOG.warn(\"Unable to allocate any opportunistic containers.\");\n        break;\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/DistributedOpportunisticContainerAllocator.java",
      "extendedDetails": {
        "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/DistributedOpportunisticContainerAllocator.java",
        "oldMethodName": "allocateContainersInternal",
        "newMethodName": "allocateContainersInternal"
      }
    },
    "96e3027e46a953ca995e4b44ef50bc2a30c7e838": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-2889. Limit the number of opportunistic container allocated per AM heartbeat. Contributed by Abhishek Modi.\n",
      "commitDate": "22/04/19 9:49 AM",
      "commitName": "96e3027e46a953ca995e4b44ef50bc2a30c7e838",
      "commitAuthor": "Inigo Goiri",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-2889. Limit the number of opportunistic container allocated per AM heartbeat. Contributed by Abhishek Modi.\n",
          "commitDate": "22/04/19 9:49 AM",
          "commitName": "96e3027e46a953ca995e4b44ef50bc2a30c7e838",
          "commitAuthor": "Inigo Goiri",
          "commitDateOld": "19/04/19 9:41 AM",
          "commitNameOld": "aeadb9432f84e679f00a9a12f63675c456bc14a8",
          "commitAuthorOld": "Inigo Goiri",
          "daysBetweenCommits": 3.01,
          "commitsBetweenForRepo": 4,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,84 +1,87 @@\n   private void allocateContainersInternal(long rmIdentifier,\n       AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, Set\u003cString\u003e allocatedNodes,\n       ApplicationAttemptId id, Map\u003cString, RemoteNode\u003e allNodes,\n       String userName, Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n-      EnrichedResourceRequest enrichedAsk)\n+      EnrichedResourceRequest enrichedAsk, int maxAllocations)\n       throws YarnException {\n     if (allNodes.size() \u003d\u003d 0) {\n       LOG.info(\"No nodes currently available to \" +\n           \"allocate OPPORTUNISTIC containers.\");\n       return;\n     }\n     ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (allocations.isEmpty() ? 0 :\n             allocations.get(anyAsk.getCapability()).size());\n     toAllocate \u003d Math.min(toAllocate,\n         appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n+    if (maxAllocations \u003e\u003d 0) {\n+      toAllocate \u003d Math.min(maxAllocations, toAllocate);\n+    }\n     int numAllocated \u003d 0;\n     // Node Candidates are selected as follows:\n     // * Node local candidates selected in loop \u003d\u003d 0\n     // * Rack local candidates selected in loop \u003d\u003d 1\n     // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n     int loopIndex \u003d OFF_SWITCH_LOOP;\n     if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n       loopIndex \u003d NODE_LOCAL_LOOP;\n     }\n     while (numAllocated \u003c toAllocate) {\n       Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n           findNodeCandidates(loopIndex, allNodes, blacklist, allocatedNodes,\n               enrichedAsk);\n       for (RemoteNode rNode : nodeCandidates) {\n         String rNodeHost \u003d rNode.getNodeId().getHost();\n         // Ignore black list\n         if (blacklist.contains(rNodeHost)) {\n           LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n               \" [\" + rNodeHost + \"]..\");\n           continue;\n         }\n         String location \u003d ResourceRequest.ANY;\n         if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n           if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n             location \u003d rNodeHost;\n           } else {\n             continue;\n           }\n         } else if (allocatedNodes.contains(rNodeHost)) {\n           LOG.info(\"Opportunistic container has already been allocated on {}.\",\n               rNodeHost);\n           continue;\n         }\n         if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n           if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n             location \u003d rNode.getRackName();\n           } else {\n             continue;\n           }\n         }\n         Container container \u003d createContainer(rmIdentifier, appParams,\n             idCounter, id, userName, allocations, location,\n             anyAsk, rNode);\n         numAllocated++;\n         updateMetrics(loopIndex);\n         allocatedNodes.add(rNodeHost);\n         LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n             \"location [\" + location + \"]\");\n         if (numAllocated \u003e\u003d toAllocate) {\n           break;\n         }\n       }\n       if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n           enrichedAsk.getRackLocations().size() \u003e 0) {\n         loopIndex \u003d RACK_LOCAL_LOOP;\n       } else {\n         loopIndex++;\n       }\n       // Handle case where there are no nodes remaining after blacklist is\n       // considered.\n       if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n         LOG.warn(\"Unable to allocate any opportunistic containers.\");\n         break;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, Set\u003cString\u003e allocatedNodes,\n      ApplicationAttemptId id, Map\u003cString, RemoteNode\u003e allNodes,\n      String userName, Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n      EnrichedResourceRequest enrichedAsk, int maxAllocations)\n      throws YarnException {\n    if (allNodes.size() \u003d\u003d 0) {\n      LOG.info(\"No nodes currently available to \" +\n          \"allocate OPPORTUNISTIC containers.\");\n      return;\n    }\n    ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (allocations.isEmpty() ? 0 :\n            allocations.get(anyAsk.getCapability()).size());\n    toAllocate \u003d Math.min(toAllocate,\n        appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n    if (maxAllocations \u003e\u003d 0) {\n      toAllocate \u003d Math.min(maxAllocations, toAllocate);\n    }\n    int numAllocated \u003d 0;\n    // Node Candidates are selected as follows:\n    // * Node local candidates selected in loop \u003d\u003d 0\n    // * Rack local candidates selected in loop \u003d\u003d 1\n    // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n    int loopIndex \u003d OFF_SWITCH_LOOP;\n    if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n      loopIndex \u003d NODE_LOCAL_LOOP;\n    }\n    while (numAllocated \u003c toAllocate) {\n      Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n          findNodeCandidates(loopIndex, allNodes, blacklist, allocatedNodes,\n              enrichedAsk);\n      for (RemoteNode rNode : nodeCandidates) {\n        String rNodeHost \u003d rNode.getNodeId().getHost();\n        // Ignore black list\n        if (blacklist.contains(rNodeHost)) {\n          LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n              \" [\" + rNodeHost + \"]..\");\n          continue;\n        }\n        String location \u003d ResourceRequest.ANY;\n        if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n          if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n            location \u003d rNodeHost;\n          } else {\n            continue;\n          }\n        } else if (allocatedNodes.contains(rNodeHost)) {\n          LOG.info(\"Opportunistic container has already been allocated on {}.\",\n              rNodeHost);\n          continue;\n        }\n        if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n          if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n            location \u003d rNode.getRackName();\n          } else {\n            continue;\n          }\n        }\n        Container container \u003d createContainer(rmIdentifier, appParams,\n            idCounter, id, userName, allocations, location,\n            anyAsk, rNode);\n        numAllocated++;\n        updateMetrics(loopIndex);\n        allocatedNodes.add(rNodeHost);\n        LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n            \"location [\" + location + \"]\");\n        if (numAllocated \u003e\u003d toAllocate) {\n          break;\n        }\n      }\n      if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n          enrichedAsk.getRackLocations().size() \u003e 0) {\n        loopIndex \u003d RACK_LOCAL_LOOP;\n      } else {\n        loopIndex++;\n      }\n      // Handle case where there are no nodes remaining after blacklist is\n      // considered.\n      if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n        LOG.warn(\"Unable to allocate any opportunistic containers.\");\n        break;\n      }\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "[rmIdentifier-long, appParams-AllocationParams, idCounter-ContainerIdGenerator, blacklist-Set\u003cString\u003e, allocatedNodes-Set\u003cString\u003e, id-ApplicationAttemptId, allNodes-Map\u003cString,RemoteNode\u003e, userName-String, allocations-Map\u003cResource,List\u003cAllocation\u003e\u003e, enrichedAsk-EnrichedResourceRequest]",
            "newValue": "[rmIdentifier-long, appParams-AllocationParams, idCounter-ContainerIdGenerator, blacklist-Set\u003cString\u003e, allocatedNodes-Set\u003cString\u003e, id-ApplicationAttemptId, allNodes-Map\u003cString,RemoteNode\u003e, userName-String, allocations-Map\u003cResource,List\u003cAllocation\u003e\u003e, enrichedAsk-EnrichedResourceRequest, maxAllocations-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-2889. Limit the number of opportunistic container allocated per AM heartbeat. Contributed by Abhishek Modi.\n",
          "commitDate": "22/04/19 9:49 AM",
          "commitName": "96e3027e46a953ca995e4b44ef50bc2a30c7e838",
          "commitAuthor": "Inigo Goiri",
          "commitDateOld": "19/04/19 9:41 AM",
          "commitNameOld": "aeadb9432f84e679f00a9a12f63675c456bc14a8",
          "commitAuthorOld": "Inigo Goiri",
          "daysBetweenCommits": 3.01,
          "commitsBetweenForRepo": 4,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,84 +1,87 @@\n   private void allocateContainersInternal(long rmIdentifier,\n       AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, Set\u003cString\u003e allocatedNodes,\n       ApplicationAttemptId id, Map\u003cString, RemoteNode\u003e allNodes,\n       String userName, Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n-      EnrichedResourceRequest enrichedAsk)\n+      EnrichedResourceRequest enrichedAsk, int maxAllocations)\n       throws YarnException {\n     if (allNodes.size() \u003d\u003d 0) {\n       LOG.info(\"No nodes currently available to \" +\n           \"allocate OPPORTUNISTIC containers.\");\n       return;\n     }\n     ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (allocations.isEmpty() ? 0 :\n             allocations.get(anyAsk.getCapability()).size());\n     toAllocate \u003d Math.min(toAllocate,\n         appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n+    if (maxAllocations \u003e\u003d 0) {\n+      toAllocate \u003d Math.min(maxAllocations, toAllocate);\n+    }\n     int numAllocated \u003d 0;\n     // Node Candidates are selected as follows:\n     // * Node local candidates selected in loop \u003d\u003d 0\n     // * Rack local candidates selected in loop \u003d\u003d 1\n     // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n     int loopIndex \u003d OFF_SWITCH_LOOP;\n     if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n       loopIndex \u003d NODE_LOCAL_LOOP;\n     }\n     while (numAllocated \u003c toAllocate) {\n       Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n           findNodeCandidates(loopIndex, allNodes, blacklist, allocatedNodes,\n               enrichedAsk);\n       for (RemoteNode rNode : nodeCandidates) {\n         String rNodeHost \u003d rNode.getNodeId().getHost();\n         // Ignore black list\n         if (blacklist.contains(rNodeHost)) {\n           LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n               \" [\" + rNodeHost + \"]..\");\n           continue;\n         }\n         String location \u003d ResourceRequest.ANY;\n         if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n           if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n             location \u003d rNodeHost;\n           } else {\n             continue;\n           }\n         } else if (allocatedNodes.contains(rNodeHost)) {\n           LOG.info(\"Opportunistic container has already been allocated on {}.\",\n               rNodeHost);\n           continue;\n         }\n         if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n           if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n             location \u003d rNode.getRackName();\n           } else {\n             continue;\n           }\n         }\n         Container container \u003d createContainer(rmIdentifier, appParams,\n             idCounter, id, userName, allocations, location,\n             anyAsk, rNode);\n         numAllocated++;\n         updateMetrics(loopIndex);\n         allocatedNodes.add(rNodeHost);\n         LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n             \"location [\" + location + \"]\");\n         if (numAllocated \u003e\u003d toAllocate) {\n           break;\n         }\n       }\n       if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n           enrichedAsk.getRackLocations().size() \u003e 0) {\n         loopIndex \u003d RACK_LOCAL_LOOP;\n       } else {\n         loopIndex++;\n       }\n       // Handle case where there are no nodes remaining after blacklist is\n       // considered.\n       if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n         LOG.warn(\"Unable to allocate any opportunistic containers.\");\n         break;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, Set\u003cString\u003e allocatedNodes,\n      ApplicationAttemptId id, Map\u003cString, RemoteNode\u003e allNodes,\n      String userName, Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n      EnrichedResourceRequest enrichedAsk, int maxAllocations)\n      throws YarnException {\n    if (allNodes.size() \u003d\u003d 0) {\n      LOG.info(\"No nodes currently available to \" +\n          \"allocate OPPORTUNISTIC containers.\");\n      return;\n    }\n    ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (allocations.isEmpty() ? 0 :\n            allocations.get(anyAsk.getCapability()).size());\n    toAllocate \u003d Math.min(toAllocate,\n        appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n    if (maxAllocations \u003e\u003d 0) {\n      toAllocate \u003d Math.min(maxAllocations, toAllocate);\n    }\n    int numAllocated \u003d 0;\n    // Node Candidates are selected as follows:\n    // * Node local candidates selected in loop \u003d\u003d 0\n    // * Rack local candidates selected in loop \u003d\u003d 1\n    // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n    int loopIndex \u003d OFF_SWITCH_LOOP;\n    if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n      loopIndex \u003d NODE_LOCAL_LOOP;\n    }\n    while (numAllocated \u003c toAllocate) {\n      Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n          findNodeCandidates(loopIndex, allNodes, blacklist, allocatedNodes,\n              enrichedAsk);\n      for (RemoteNode rNode : nodeCandidates) {\n        String rNodeHost \u003d rNode.getNodeId().getHost();\n        // Ignore black list\n        if (blacklist.contains(rNodeHost)) {\n          LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n              \" [\" + rNodeHost + \"]..\");\n          continue;\n        }\n        String location \u003d ResourceRequest.ANY;\n        if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n          if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n            location \u003d rNodeHost;\n          } else {\n            continue;\n          }\n        } else if (allocatedNodes.contains(rNodeHost)) {\n          LOG.info(\"Opportunistic container has already been allocated on {}.\",\n              rNodeHost);\n          continue;\n        }\n        if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n          if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n            location \u003d rNode.getRackName();\n          } else {\n            continue;\n          }\n        }\n        Container container \u003d createContainer(rmIdentifier, appParams,\n            idCounter, id, userName, allocations, location,\n            anyAsk, rNode);\n        numAllocated++;\n        updateMetrics(loopIndex);\n        allocatedNodes.add(rNodeHost);\n        LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n            \"location [\" + location + \"]\");\n        if (numAllocated \u003e\u003d toAllocate) {\n          break;\n        }\n      }\n      if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n          enrichedAsk.getRackLocations().size() \u003e 0) {\n        loopIndex \u003d RACK_LOCAL_LOOP;\n      } else {\n        loopIndex++;\n      }\n      // Handle case where there are no nodes remaining after blacklist is\n      // considered.\n      if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n        LOG.warn(\"Unable to allocate any opportunistic containers.\");\n        break;\n      }\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
          "extendedDetails": {}
        }
      ]
    },
    "aeadb9432f84e679f00a9a12f63675c456bc14a8": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-9448. Fix Opportunistic Scheduling for node local allocations. Contributed by Abhishek Modi.\n",
      "commitDate": "19/04/19 9:41 AM",
      "commitName": "aeadb9432f84e679f00a9a12f63675c456bc14a8",
      "commitAuthor": "Inigo Goiri",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-9448. Fix Opportunistic Scheduling for node local allocations. Contributed by Abhishek Modi.\n",
          "commitDate": "19/04/19 9:41 AM",
          "commitName": "aeadb9432f84e679f00a9a12f63675c456bc14a8",
          "commitAuthor": "Inigo Goiri",
          "commitDateOld": "11/04/19 11:49 AM",
          "commitNameOld": "ed3747c1ccc303e206de50c2b74f3c318cb1c416",
          "commitAuthorOld": "Giovanni Matteo Fumarola",
          "daysBetweenCommits": 7.91,
          "commitsBetweenForRepo": 56,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,83 +1,84 @@\n   private void allocateContainersInternal(long rmIdentifier,\n       AllocationParams appParams, ContainerIdGenerator idCounter,\n-      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n-      Map\u003cString, RemoteNode\u003e allNodes, String userName,\n-      Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n+      Set\u003cString\u003e blacklist, Set\u003cString\u003e allocatedNodes,\n+      ApplicationAttemptId id, Map\u003cString, RemoteNode\u003e allNodes,\n+      String userName, Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n       EnrichedResourceRequest enrichedAsk)\n       throws YarnException {\n     if (allNodes.size() \u003d\u003d 0) {\n       LOG.info(\"No nodes currently available to \" +\n           \"allocate OPPORTUNISTIC containers.\");\n       return;\n     }\n     ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (allocations.isEmpty() ? 0 :\n             allocations.get(anyAsk.getCapability()).size());\n     toAllocate \u003d Math.min(toAllocate,\n         appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n     int numAllocated \u003d 0;\n     // Node Candidates are selected as follows:\n     // * Node local candidates selected in loop \u003d\u003d 0\n     // * Rack local candidates selected in loop \u003d\u003d 1\n     // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n     int loopIndex \u003d OFF_SWITCH_LOOP;\n     if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n       loopIndex \u003d NODE_LOCAL_LOOP;\n     }\n     while (numAllocated \u003c toAllocate) {\n       Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n-          findNodeCandidates(loopIndex, allNodes, blacklist, enrichedAsk);\n+          findNodeCandidates(loopIndex, allNodes, blacklist, allocatedNodes,\n+              enrichedAsk);\n       for (RemoteNode rNode : nodeCandidates) {\n         String rNodeHost \u003d rNode.getNodeId().getHost();\n         // Ignore black list\n         if (blacklist.contains(rNodeHost)) {\n           LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n               \" [\" + rNodeHost + \"]..\");\n           continue;\n         }\n         String location \u003d ResourceRequest.ANY;\n         if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n           if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n             location \u003d rNodeHost;\n           } else {\n             continue;\n           }\n+        } else if (allocatedNodes.contains(rNodeHost)) {\n+          LOG.info(\"Opportunistic container has already been allocated on {}.\",\n+              rNodeHost);\n+          continue;\n         }\n         if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n           if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n             location \u003d rNode.getRackName();\n           } else {\n             continue;\n           }\n         }\n         Container container \u003d createContainer(rmIdentifier, appParams,\n             idCounter, id, userName, allocations, location,\n             anyAsk, rNode);\n         numAllocated++;\n         updateMetrics(loopIndex);\n-        // Try to spread the allocations across the nodes.\n-        // But don\u0027t add if it is a node local request.\n-        if (loopIndex !\u003d NODE_LOCAL_LOOP) {\n-          blacklist.add(rNode.getNodeId().getHost());\n-        }\n+        allocatedNodes.add(rNodeHost);\n         LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n             \"location [\" + location + \"]\");\n         if (numAllocated \u003e\u003d toAllocate) {\n           break;\n         }\n       }\n       if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n           enrichedAsk.getRackLocations().size() \u003e 0) {\n         loopIndex \u003d RACK_LOCAL_LOOP;\n       } else {\n         loopIndex++;\n       }\n       // Handle case where there are no nodes remaining after blacklist is\n       // considered.\n       if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n         LOG.warn(\"Unable to allocate any opportunistic containers.\");\n         break;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, Set\u003cString\u003e allocatedNodes,\n      ApplicationAttemptId id, Map\u003cString, RemoteNode\u003e allNodes,\n      String userName, Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n      EnrichedResourceRequest enrichedAsk)\n      throws YarnException {\n    if (allNodes.size() \u003d\u003d 0) {\n      LOG.info(\"No nodes currently available to \" +\n          \"allocate OPPORTUNISTIC containers.\");\n      return;\n    }\n    ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (allocations.isEmpty() ? 0 :\n            allocations.get(anyAsk.getCapability()).size());\n    toAllocate \u003d Math.min(toAllocate,\n        appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n    int numAllocated \u003d 0;\n    // Node Candidates are selected as follows:\n    // * Node local candidates selected in loop \u003d\u003d 0\n    // * Rack local candidates selected in loop \u003d\u003d 1\n    // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n    int loopIndex \u003d OFF_SWITCH_LOOP;\n    if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n      loopIndex \u003d NODE_LOCAL_LOOP;\n    }\n    while (numAllocated \u003c toAllocate) {\n      Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n          findNodeCandidates(loopIndex, allNodes, blacklist, allocatedNodes,\n              enrichedAsk);\n      for (RemoteNode rNode : nodeCandidates) {\n        String rNodeHost \u003d rNode.getNodeId().getHost();\n        // Ignore black list\n        if (blacklist.contains(rNodeHost)) {\n          LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n              \" [\" + rNodeHost + \"]..\");\n          continue;\n        }\n        String location \u003d ResourceRequest.ANY;\n        if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n          if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n            location \u003d rNodeHost;\n          } else {\n            continue;\n          }\n        } else if (allocatedNodes.contains(rNodeHost)) {\n          LOG.info(\"Opportunistic container has already been allocated on {}.\",\n              rNodeHost);\n          continue;\n        }\n        if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n          if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n            location \u003d rNode.getRackName();\n          } else {\n            continue;\n          }\n        }\n        Container container \u003d createContainer(rmIdentifier, appParams,\n            idCounter, id, userName, allocations, location,\n            anyAsk, rNode);\n        numAllocated++;\n        updateMetrics(loopIndex);\n        allocatedNodes.add(rNodeHost);\n        LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n            \"location [\" + location + \"]\");\n        if (numAllocated \u003e\u003d toAllocate) {\n          break;\n        }\n      }\n      if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n          enrichedAsk.getRackLocations().size() \u003e 0) {\n        loopIndex \u003d RACK_LOCAL_LOOP;\n      } else {\n        loopIndex++;\n      }\n      // Handle case where there are no nodes remaining after blacklist is\n      // considered.\n      if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n        LOG.warn(\"Unable to allocate any opportunistic containers.\");\n        break;\n      }\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "[rmIdentifier-long, appParams-AllocationParams, idCounter-ContainerIdGenerator, blacklist-Set\u003cString\u003e, id-ApplicationAttemptId, allNodes-Map\u003cString,RemoteNode\u003e, userName-String, allocations-Map\u003cResource,List\u003cAllocation\u003e\u003e, enrichedAsk-EnrichedResourceRequest]",
            "newValue": "[rmIdentifier-long, appParams-AllocationParams, idCounter-ContainerIdGenerator, blacklist-Set\u003cString\u003e, allocatedNodes-Set\u003cString\u003e, id-ApplicationAttemptId, allNodes-Map\u003cString,RemoteNode\u003e, userName-String, allocations-Map\u003cResource,List\u003cAllocation\u003e\u003e, enrichedAsk-EnrichedResourceRequest]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-9448. Fix Opportunistic Scheduling for node local allocations. Contributed by Abhishek Modi.\n",
          "commitDate": "19/04/19 9:41 AM",
          "commitName": "aeadb9432f84e679f00a9a12f63675c456bc14a8",
          "commitAuthor": "Inigo Goiri",
          "commitDateOld": "11/04/19 11:49 AM",
          "commitNameOld": "ed3747c1ccc303e206de50c2b74f3c318cb1c416",
          "commitAuthorOld": "Giovanni Matteo Fumarola",
          "daysBetweenCommits": 7.91,
          "commitsBetweenForRepo": 56,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,83 +1,84 @@\n   private void allocateContainersInternal(long rmIdentifier,\n       AllocationParams appParams, ContainerIdGenerator idCounter,\n-      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n-      Map\u003cString, RemoteNode\u003e allNodes, String userName,\n-      Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n+      Set\u003cString\u003e blacklist, Set\u003cString\u003e allocatedNodes,\n+      ApplicationAttemptId id, Map\u003cString, RemoteNode\u003e allNodes,\n+      String userName, Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n       EnrichedResourceRequest enrichedAsk)\n       throws YarnException {\n     if (allNodes.size() \u003d\u003d 0) {\n       LOG.info(\"No nodes currently available to \" +\n           \"allocate OPPORTUNISTIC containers.\");\n       return;\n     }\n     ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (allocations.isEmpty() ? 0 :\n             allocations.get(anyAsk.getCapability()).size());\n     toAllocate \u003d Math.min(toAllocate,\n         appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n     int numAllocated \u003d 0;\n     // Node Candidates are selected as follows:\n     // * Node local candidates selected in loop \u003d\u003d 0\n     // * Rack local candidates selected in loop \u003d\u003d 1\n     // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n     int loopIndex \u003d OFF_SWITCH_LOOP;\n     if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n       loopIndex \u003d NODE_LOCAL_LOOP;\n     }\n     while (numAllocated \u003c toAllocate) {\n       Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n-          findNodeCandidates(loopIndex, allNodes, blacklist, enrichedAsk);\n+          findNodeCandidates(loopIndex, allNodes, blacklist, allocatedNodes,\n+              enrichedAsk);\n       for (RemoteNode rNode : nodeCandidates) {\n         String rNodeHost \u003d rNode.getNodeId().getHost();\n         // Ignore black list\n         if (blacklist.contains(rNodeHost)) {\n           LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n               \" [\" + rNodeHost + \"]..\");\n           continue;\n         }\n         String location \u003d ResourceRequest.ANY;\n         if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n           if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n             location \u003d rNodeHost;\n           } else {\n             continue;\n           }\n+        } else if (allocatedNodes.contains(rNodeHost)) {\n+          LOG.info(\"Opportunistic container has already been allocated on {}.\",\n+              rNodeHost);\n+          continue;\n         }\n         if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n           if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n             location \u003d rNode.getRackName();\n           } else {\n             continue;\n           }\n         }\n         Container container \u003d createContainer(rmIdentifier, appParams,\n             idCounter, id, userName, allocations, location,\n             anyAsk, rNode);\n         numAllocated++;\n         updateMetrics(loopIndex);\n-        // Try to spread the allocations across the nodes.\n-        // But don\u0027t add if it is a node local request.\n-        if (loopIndex !\u003d NODE_LOCAL_LOOP) {\n-          blacklist.add(rNode.getNodeId().getHost());\n-        }\n+        allocatedNodes.add(rNodeHost);\n         LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n             \"location [\" + location + \"]\");\n         if (numAllocated \u003e\u003d toAllocate) {\n           break;\n         }\n       }\n       if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n           enrichedAsk.getRackLocations().size() \u003e 0) {\n         loopIndex \u003d RACK_LOCAL_LOOP;\n       } else {\n         loopIndex++;\n       }\n       // Handle case where there are no nodes remaining after blacklist is\n       // considered.\n       if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n         LOG.warn(\"Unable to allocate any opportunistic containers.\");\n         break;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, Set\u003cString\u003e allocatedNodes,\n      ApplicationAttemptId id, Map\u003cString, RemoteNode\u003e allNodes,\n      String userName, Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n      EnrichedResourceRequest enrichedAsk)\n      throws YarnException {\n    if (allNodes.size() \u003d\u003d 0) {\n      LOG.info(\"No nodes currently available to \" +\n          \"allocate OPPORTUNISTIC containers.\");\n      return;\n    }\n    ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (allocations.isEmpty() ? 0 :\n            allocations.get(anyAsk.getCapability()).size());\n    toAllocate \u003d Math.min(toAllocate,\n        appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n    int numAllocated \u003d 0;\n    // Node Candidates are selected as follows:\n    // * Node local candidates selected in loop \u003d\u003d 0\n    // * Rack local candidates selected in loop \u003d\u003d 1\n    // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n    int loopIndex \u003d OFF_SWITCH_LOOP;\n    if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n      loopIndex \u003d NODE_LOCAL_LOOP;\n    }\n    while (numAllocated \u003c toAllocate) {\n      Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n          findNodeCandidates(loopIndex, allNodes, blacklist, allocatedNodes,\n              enrichedAsk);\n      for (RemoteNode rNode : nodeCandidates) {\n        String rNodeHost \u003d rNode.getNodeId().getHost();\n        // Ignore black list\n        if (blacklist.contains(rNodeHost)) {\n          LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n              \" [\" + rNodeHost + \"]..\");\n          continue;\n        }\n        String location \u003d ResourceRequest.ANY;\n        if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n          if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n            location \u003d rNodeHost;\n          } else {\n            continue;\n          }\n        } else if (allocatedNodes.contains(rNodeHost)) {\n          LOG.info(\"Opportunistic container has already been allocated on {}.\",\n              rNodeHost);\n          continue;\n        }\n        if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n          if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n            location \u003d rNode.getRackName();\n          } else {\n            continue;\n          }\n        }\n        Container container \u003d createContainer(rmIdentifier, appParams,\n            idCounter, id, userName, allocations, location,\n            anyAsk, rNode);\n        numAllocated++;\n        updateMetrics(loopIndex);\n        allocatedNodes.add(rNodeHost);\n        LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n            \"location [\" + location + \"]\");\n        if (numAllocated \u003e\u003d toAllocate) {\n          break;\n        }\n      }\n      if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n          enrichedAsk.getRackLocations().size() \u003e 0) {\n        loopIndex \u003d RACK_LOCAL_LOOP;\n      } else {\n        loopIndex++;\n      }\n      // Handle case where there are no nodes remaining after blacklist is\n      // considered.\n      if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n        LOG.warn(\"Unable to allocate any opportunistic containers.\");\n        break;\n      }\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
          "extendedDetails": {}
        }
      ]
    },
    "ed3747c1ccc303e206de50c2b74f3c318cb1c416": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9435. Add Opportunistic Scheduler metrics in ResourceManager. Contributed by Abhishek Modi.\n",
      "commitDate": "11/04/19 11:49 AM",
      "commitName": "ed3747c1ccc303e206de50c2b74f3c318cb1c416",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "12/06/18 8:35 AM",
      "commitNameOld": "652bcbb3e4950758e61ce123ecc1798ae2b60a7f",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 303.13,
      "commitsBetweenForRepo": 2343,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,82 +1,83 @@\n   private void allocateContainersInternal(long rmIdentifier,\n       AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n       Map\u003cString, RemoteNode\u003e allNodes, String userName,\n       Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n       EnrichedResourceRequest enrichedAsk)\n       throws YarnException {\n     if (allNodes.size() \u003d\u003d 0) {\n       LOG.info(\"No nodes currently available to \" +\n           \"allocate OPPORTUNISTIC containers.\");\n       return;\n     }\n     ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (allocations.isEmpty() ? 0 :\n             allocations.get(anyAsk.getCapability()).size());\n     toAllocate \u003d Math.min(toAllocate,\n         appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n     int numAllocated \u003d 0;\n     // Node Candidates are selected as follows:\n     // * Node local candidates selected in loop \u003d\u003d 0\n     // * Rack local candidates selected in loop \u003d\u003d 1\n     // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n     int loopIndex \u003d OFF_SWITCH_LOOP;\n     if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n       loopIndex \u003d NODE_LOCAL_LOOP;\n     }\n     while (numAllocated \u003c toAllocate) {\n       Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n           findNodeCandidates(loopIndex, allNodes, blacklist, enrichedAsk);\n       for (RemoteNode rNode : nodeCandidates) {\n         String rNodeHost \u003d rNode.getNodeId().getHost();\n         // Ignore black list\n         if (blacklist.contains(rNodeHost)) {\n           LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n               \" [\" + rNodeHost + \"]..\");\n           continue;\n         }\n         String location \u003d ResourceRequest.ANY;\n         if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n           if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n             location \u003d rNodeHost;\n           } else {\n             continue;\n           }\n         }\n         if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n           if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n             location \u003d rNode.getRackName();\n           } else {\n             continue;\n           }\n         }\n         Container container \u003d createContainer(rmIdentifier, appParams,\n             idCounter, id, userName, allocations, location,\n             anyAsk, rNode);\n         numAllocated++;\n+        updateMetrics(loopIndex);\n         // Try to spread the allocations across the nodes.\n         // But don\u0027t add if it is a node local request.\n         if (loopIndex !\u003d NODE_LOCAL_LOOP) {\n           blacklist.add(rNode.getNodeId().getHost());\n         }\n         LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n             \"location [\" + location + \"]\");\n         if (numAllocated \u003e\u003d toAllocate) {\n           break;\n         }\n       }\n       if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n           enrichedAsk.getRackLocations().size() \u003e 0) {\n         loopIndex \u003d RACK_LOCAL_LOOP;\n       } else {\n         loopIndex++;\n       }\n       // Handle case where there are no nodes remaining after blacklist is\n       // considered.\n       if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n         LOG.warn(\"Unable to allocate any opportunistic containers.\");\n         break;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n      Map\u003cString, RemoteNode\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n      EnrichedResourceRequest enrichedAsk)\n      throws YarnException {\n    if (allNodes.size() \u003d\u003d 0) {\n      LOG.info(\"No nodes currently available to \" +\n          \"allocate OPPORTUNISTIC containers.\");\n      return;\n    }\n    ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (allocations.isEmpty() ? 0 :\n            allocations.get(anyAsk.getCapability()).size());\n    toAllocate \u003d Math.min(toAllocate,\n        appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n    int numAllocated \u003d 0;\n    // Node Candidates are selected as follows:\n    // * Node local candidates selected in loop \u003d\u003d 0\n    // * Rack local candidates selected in loop \u003d\u003d 1\n    // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n    int loopIndex \u003d OFF_SWITCH_LOOP;\n    if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n      loopIndex \u003d NODE_LOCAL_LOOP;\n    }\n    while (numAllocated \u003c toAllocate) {\n      Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n          findNodeCandidates(loopIndex, allNodes, blacklist, enrichedAsk);\n      for (RemoteNode rNode : nodeCandidates) {\n        String rNodeHost \u003d rNode.getNodeId().getHost();\n        // Ignore black list\n        if (blacklist.contains(rNodeHost)) {\n          LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n              \" [\" + rNodeHost + \"]..\");\n          continue;\n        }\n        String location \u003d ResourceRequest.ANY;\n        if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n          if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n            location \u003d rNodeHost;\n          } else {\n            continue;\n          }\n        }\n        if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n          if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n            location \u003d rNode.getRackName();\n          } else {\n            continue;\n          }\n        }\n        Container container \u003d createContainer(rmIdentifier, appParams,\n            idCounter, id, userName, allocations, location,\n            anyAsk, rNode);\n        numAllocated++;\n        updateMetrics(loopIndex);\n        // Try to spread the allocations across the nodes.\n        // But don\u0027t add if it is a node local request.\n        if (loopIndex !\u003d NODE_LOCAL_LOOP) {\n          blacklist.add(rNode.getNodeId().getHost());\n        }\n        LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n            \"location [\" + location + \"]\");\n        if (numAllocated \u003e\u003d toAllocate) {\n          break;\n        }\n      }\n      if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n          enrichedAsk.getRackLocations().size() \u003e 0) {\n        loopIndex \u003d RACK_LOCAL_LOOP;\n      } else {\n        loopIndex++;\n      }\n      // Handle case where there are no nodes remaining after blacklist is\n      // considered.\n      if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n        LOG.warn(\"Unable to allocate any opportunistic containers.\");\n        break;\n      }\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
      "extendedDetails": {}
    },
    "b733348dde18a242e6c9074c512116a8baf1d281": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-7258. Add Node and Rack Hints to Opportunistic Scheduler. (Kartheek Muthyala via asuresh).\n",
      "commitDate": "05/10/17 9:58 AM",
      "commitName": "b733348dde18a242e6c9074c512116a8baf1d281",
      "commitAuthor": "Arun Suresh",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-7258. Add Node and Rack Hints to Opportunistic Scheduler. (Kartheek Muthyala via asuresh).\n",
          "commitDate": "05/10/17 9:58 AM",
          "commitName": "b733348dde18a242e6c9074c512116a8baf1d281",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "03/10/17 2:06 PM",
          "commitNameOld": "4a877737182808fe3392a116762ba59973a477a6",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 1.83,
          "commitsBetweenForRepo": 14,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,82 @@\n   private void allocateContainersInternal(long rmIdentifier,\n       AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n       Map\u003cString, RemoteNode\u003e allNodes, String userName,\n-      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n+      Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n+      EnrichedResourceRequest enrichedAsk)\n       throws YarnException {\n-    int toAllocate \u003d anyAsk.getNumContainers()\n-        - (containers.isEmpty() ? 0 :\n-            containers.get(anyAsk.getCapability()).size());\n-\n-    List\u003cRemoteNode\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n-    for (Entry\u003cString, RemoteNode\u003e nodeEntry : allNodes.entrySet()) {\n-      // Do not use blacklisted nodes for scheduling.\n-      if (blacklist.contains(nodeEntry.getKey())) {\n-        continue;\n-      }\n-      nodesForScheduling.add(nodeEntry.getValue());\n-    }\n-    if (nodesForScheduling.isEmpty()) {\n-      LOG.warn(\"No nodes available for allocating opportunistic containers. [\" +\n-          \"allNodes\u003d{}, blacklist\u003d{}]\", allNodes, blacklist);\n+    if (allNodes.size() \u003d\u003d 0) {\n+      LOG.info(\"No nodes currently available to \" +\n+          \"allocate OPPORTUNISTIC containers.\");\n       return;\n     }\n+    ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n+    int toAllocate \u003d anyAsk.getNumContainers()\n+        - (allocations.isEmpty() ? 0 :\n+            allocations.get(anyAsk.getCapability()).size());\n+    toAllocate \u003d Math.min(toAllocate,\n+        appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n     int numAllocated \u003d 0;\n-    int nextNodeToSchedule \u003d 0;\n-    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n-      nextNodeToSchedule++;\n-      nextNodeToSchedule %\u003d nodesForScheduling.size();\n-      RemoteNode node \u003d nodesForScheduling.get(nextNodeToSchedule);\n-      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n-          anyAsk, id, userName, node);\n-      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n-      if (cList \u003d\u003d null) {\n-        cList \u003d new ArrayList\u003c\u003e();\n-        containers.put(anyAsk.getCapability(), cList);\n-      }\n-      cList.add(container);\n-      numAllocated++;\n-      LOG.info(\"Allocated [{}] as opportunistic.\", container.getId());\n+    // Node Candidates are selected as follows:\n+    // * Node local candidates selected in loop \u003d\u003d 0\n+    // * Rack local candidates selected in loop \u003d\u003d 1\n+    // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n+    int loopIndex \u003d OFF_SWITCH_LOOP;\n+    if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n+      loopIndex \u003d NODE_LOCAL_LOOP;\n     }\n-    LOG.info(\"Allocated {} opportunistic containers.\", numAllocated);\n+    while (numAllocated \u003c toAllocate) {\n+      Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n+          findNodeCandidates(loopIndex, allNodes, blacklist, enrichedAsk);\n+      for (RemoteNode rNode : nodeCandidates) {\n+        String rNodeHost \u003d rNode.getNodeId().getHost();\n+        // Ignore black list\n+        if (blacklist.contains(rNodeHost)) {\n+          LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n+              \" [\" + rNodeHost + \"]..\");\n+          continue;\n+        }\n+        String location \u003d ResourceRequest.ANY;\n+        if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n+          if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n+            location \u003d rNodeHost;\n+          } else {\n+            continue;\n+          }\n+        }\n+        if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n+          if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n+            location \u003d rNode.getRackName();\n+          } else {\n+            continue;\n+          }\n+        }\n+        Container container \u003d createContainer(rmIdentifier, appParams,\n+            idCounter, id, userName, allocations, location,\n+            anyAsk, rNode);\n+        numAllocated++;\n+        // Try to spread the allocations across the nodes.\n+        // But don\u0027t add if it is a node local request.\n+        if (loopIndex !\u003d NODE_LOCAL_LOOP) {\n+          blacklist.add(rNode.getNodeId().getHost());\n+        }\n+        LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n+            \"location [\" + location + \"]\");\n+        if (numAllocated \u003e\u003d toAllocate) {\n+          break;\n+        }\n+      }\n+      if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n+          enrichedAsk.getRackLocations().size() \u003e 0) {\n+        loopIndex \u003d RACK_LOCAL_LOOP;\n+      } else {\n+        loopIndex++;\n+      }\n+      // Handle case where there are no nodes remaining after blacklist is\n+      // considered.\n+      if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n+        LOG.warn(\"Unable to allocate any opportunistic containers.\");\n+        break;\n+      }\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n      Map\u003cString, RemoteNode\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n      EnrichedResourceRequest enrichedAsk)\n      throws YarnException {\n    if (allNodes.size() \u003d\u003d 0) {\n      LOG.info(\"No nodes currently available to \" +\n          \"allocate OPPORTUNISTIC containers.\");\n      return;\n    }\n    ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (allocations.isEmpty() ? 0 :\n            allocations.get(anyAsk.getCapability()).size());\n    toAllocate \u003d Math.min(toAllocate,\n        appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n    int numAllocated \u003d 0;\n    // Node Candidates are selected as follows:\n    // * Node local candidates selected in loop \u003d\u003d 0\n    // * Rack local candidates selected in loop \u003d\u003d 1\n    // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n    int loopIndex \u003d OFF_SWITCH_LOOP;\n    if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n      loopIndex \u003d NODE_LOCAL_LOOP;\n    }\n    while (numAllocated \u003c toAllocate) {\n      Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n          findNodeCandidates(loopIndex, allNodes, blacklist, enrichedAsk);\n      for (RemoteNode rNode : nodeCandidates) {\n        String rNodeHost \u003d rNode.getNodeId().getHost();\n        // Ignore black list\n        if (blacklist.contains(rNodeHost)) {\n          LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n              \" [\" + rNodeHost + \"]..\");\n          continue;\n        }\n        String location \u003d ResourceRequest.ANY;\n        if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n          if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n            location \u003d rNodeHost;\n          } else {\n            continue;\n          }\n        }\n        if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n          if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n            location \u003d rNode.getRackName();\n          } else {\n            continue;\n          }\n        }\n        Container container \u003d createContainer(rmIdentifier, appParams,\n            idCounter, id, userName, allocations, location,\n            anyAsk, rNode);\n        numAllocated++;\n        // Try to spread the allocations across the nodes.\n        // But don\u0027t add if it is a node local request.\n        if (loopIndex !\u003d NODE_LOCAL_LOOP) {\n          blacklist.add(rNode.getNodeId().getHost());\n        }\n        LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n            \"location [\" + location + \"]\");\n        if (numAllocated \u003e\u003d toAllocate) {\n          break;\n        }\n      }\n      if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n          enrichedAsk.getRackLocations().size() \u003e 0) {\n        loopIndex \u003d RACK_LOCAL_LOOP;\n      } else {\n        loopIndex++;\n      }\n      // Handle case where there are no nodes remaining after blacklist is\n      // considered.\n      if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n        LOG.warn(\"Unable to allocate any opportunistic containers.\");\n        break;\n      }\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "[rmIdentifier-long, appParams-AllocationParams, idCounter-ContainerIdGenerator, blacklist-Set\u003cString\u003e, id-ApplicationAttemptId, allNodes-Map\u003cString,RemoteNode\u003e, userName-String, containers-Map\u003cResource,List\u003cContainer\u003e\u003e, anyAsk-ResourceRequest]",
            "newValue": "[rmIdentifier-long, appParams-AllocationParams, idCounter-ContainerIdGenerator, blacklist-Set\u003cString\u003e, id-ApplicationAttemptId, allNodes-Map\u003cString,RemoteNode\u003e, userName-String, allocations-Map\u003cResource,List\u003cAllocation\u003e\u003e, enrichedAsk-EnrichedResourceRequest]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-7258. Add Node and Rack Hints to Opportunistic Scheduler. (Kartheek Muthyala via asuresh).\n",
          "commitDate": "05/10/17 9:58 AM",
          "commitName": "b733348dde18a242e6c9074c512116a8baf1d281",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "03/10/17 2:06 PM",
          "commitNameOld": "4a877737182808fe3392a116762ba59973a477a6",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 1.83,
          "commitsBetweenForRepo": 14,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,82 @@\n   private void allocateContainersInternal(long rmIdentifier,\n       AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n       Map\u003cString, RemoteNode\u003e allNodes, String userName,\n-      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n+      Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n+      EnrichedResourceRequest enrichedAsk)\n       throws YarnException {\n-    int toAllocate \u003d anyAsk.getNumContainers()\n-        - (containers.isEmpty() ? 0 :\n-            containers.get(anyAsk.getCapability()).size());\n-\n-    List\u003cRemoteNode\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n-    for (Entry\u003cString, RemoteNode\u003e nodeEntry : allNodes.entrySet()) {\n-      // Do not use blacklisted nodes for scheduling.\n-      if (blacklist.contains(nodeEntry.getKey())) {\n-        continue;\n-      }\n-      nodesForScheduling.add(nodeEntry.getValue());\n-    }\n-    if (nodesForScheduling.isEmpty()) {\n-      LOG.warn(\"No nodes available for allocating opportunistic containers. [\" +\n-          \"allNodes\u003d{}, blacklist\u003d{}]\", allNodes, blacklist);\n+    if (allNodes.size() \u003d\u003d 0) {\n+      LOG.info(\"No nodes currently available to \" +\n+          \"allocate OPPORTUNISTIC containers.\");\n       return;\n     }\n+    ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n+    int toAllocate \u003d anyAsk.getNumContainers()\n+        - (allocations.isEmpty() ? 0 :\n+            allocations.get(anyAsk.getCapability()).size());\n+    toAllocate \u003d Math.min(toAllocate,\n+        appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n     int numAllocated \u003d 0;\n-    int nextNodeToSchedule \u003d 0;\n-    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n-      nextNodeToSchedule++;\n-      nextNodeToSchedule %\u003d nodesForScheduling.size();\n-      RemoteNode node \u003d nodesForScheduling.get(nextNodeToSchedule);\n-      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n-          anyAsk, id, userName, node);\n-      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n-      if (cList \u003d\u003d null) {\n-        cList \u003d new ArrayList\u003c\u003e();\n-        containers.put(anyAsk.getCapability(), cList);\n-      }\n-      cList.add(container);\n-      numAllocated++;\n-      LOG.info(\"Allocated [{}] as opportunistic.\", container.getId());\n+    // Node Candidates are selected as follows:\n+    // * Node local candidates selected in loop \u003d\u003d 0\n+    // * Rack local candidates selected in loop \u003d\u003d 1\n+    // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n+    int loopIndex \u003d OFF_SWITCH_LOOP;\n+    if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n+      loopIndex \u003d NODE_LOCAL_LOOP;\n     }\n-    LOG.info(\"Allocated {} opportunistic containers.\", numAllocated);\n+    while (numAllocated \u003c toAllocate) {\n+      Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n+          findNodeCandidates(loopIndex, allNodes, blacklist, enrichedAsk);\n+      for (RemoteNode rNode : nodeCandidates) {\n+        String rNodeHost \u003d rNode.getNodeId().getHost();\n+        // Ignore black list\n+        if (blacklist.contains(rNodeHost)) {\n+          LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n+              \" [\" + rNodeHost + \"]..\");\n+          continue;\n+        }\n+        String location \u003d ResourceRequest.ANY;\n+        if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n+          if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n+            location \u003d rNodeHost;\n+          } else {\n+            continue;\n+          }\n+        }\n+        if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n+          if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n+            location \u003d rNode.getRackName();\n+          } else {\n+            continue;\n+          }\n+        }\n+        Container container \u003d createContainer(rmIdentifier, appParams,\n+            idCounter, id, userName, allocations, location,\n+            anyAsk, rNode);\n+        numAllocated++;\n+        // Try to spread the allocations across the nodes.\n+        // But don\u0027t add if it is a node local request.\n+        if (loopIndex !\u003d NODE_LOCAL_LOOP) {\n+          blacklist.add(rNode.getNodeId().getHost());\n+        }\n+        LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n+            \"location [\" + location + \"]\");\n+        if (numAllocated \u003e\u003d toAllocate) {\n+          break;\n+        }\n+      }\n+      if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n+          enrichedAsk.getRackLocations().size() \u003e 0) {\n+        loopIndex \u003d RACK_LOCAL_LOOP;\n+      } else {\n+        loopIndex++;\n+      }\n+      // Handle case where there are no nodes remaining after blacklist is\n+      // considered.\n+      if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n+        LOG.warn(\"Unable to allocate any opportunistic containers.\");\n+        break;\n+      }\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n      Map\u003cString, RemoteNode\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cAllocation\u003e\u003e allocations,\n      EnrichedResourceRequest enrichedAsk)\n      throws YarnException {\n    if (allNodes.size() \u003d\u003d 0) {\n      LOG.info(\"No nodes currently available to \" +\n          \"allocate OPPORTUNISTIC containers.\");\n      return;\n    }\n    ResourceRequest anyAsk \u003d enrichedAsk.getRequest();\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (allocations.isEmpty() ? 0 :\n            allocations.get(anyAsk.getCapability()).size());\n    toAllocate \u003d Math.min(toAllocate,\n        appParams.getMaxAllocationsPerSchedulerKeyPerRound());\n    int numAllocated \u003d 0;\n    // Node Candidates are selected as follows:\n    // * Node local candidates selected in loop \u003d\u003d 0\n    // * Rack local candidates selected in loop \u003d\u003d 1\n    // * From loop \u003d\u003d 2 onwards, we revert to off switch allocations.\n    int loopIndex \u003d OFF_SWITCH_LOOP;\n    if (enrichedAsk.getNodeLocations().size() \u003e 0) {\n      loopIndex \u003d NODE_LOCAL_LOOP;\n    }\n    while (numAllocated \u003c toAllocate) {\n      Collection\u003cRemoteNode\u003e nodeCandidates \u003d\n          findNodeCandidates(loopIndex, allNodes, blacklist, enrichedAsk);\n      for (RemoteNode rNode : nodeCandidates) {\n        String rNodeHost \u003d rNode.getNodeId().getHost();\n        // Ignore black list\n        if (blacklist.contains(rNodeHost)) {\n          LOG.info(\"Nodes for scheduling has a blacklisted node\" +\n              \" [\" + rNodeHost + \"]..\");\n          continue;\n        }\n        String location \u003d ResourceRequest.ANY;\n        if (loopIndex \u003d\u003d NODE_LOCAL_LOOP) {\n          if (enrichedAsk.getNodeLocations().contains(rNodeHost)) {\n            location \u003d rNodeHost;\n          } else {\n            continue;\n          }\n        }\n        if (loopIndex \u003d\u003d RACK_LOCAL_LOOP) {\n          if (enrichedAsk.getRackLocations().contains(rNode.getRackName())) {\n            location \u003d rNode.getRackName();\n          } else {\n            continue;\n          }\n        }\n        Container container \u003d createContainer(rmIdentifier, appParams,\n            idCounter, id, userName, allocations, location,\n            anyAsk, rNode);\n        numAllocated++;\n        // Try to spread the allocations across the nodes.\n        // But don\u0027t add if it is a node local request.\n        if (loopIndex !\u003d NODE_LOCAL_LOOP) {\n          blacklist.add(rNode.getNodeId().getHost());\n        }\n        LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic at \" +\n            \"location [\" + location + \"]\");\n        if (numAllocated \u003e\u003d toAllocate) {\n          break;\n        }\n      }\n      if (loopIndex \u003d\u003d NODE_LOCAL_LOOP \u0026\u0026\n          enrichedAsk.getRackLocations().size() \u003e 0) {\n        loopIndex \u003d RACK_LOCAL_LOOP;\n      } else {\n        loopIndex++;\n      }\n      // Handle case where there are no nodes remaining after blacklist is\n      // considered.\n      if (loopIndex \u003e OFF_SWITCH_LOOP \u0026\u0026 numAllocated \u003d\u003d 0) {\n        LOG.warn(\"Unable to allocate any opportunistic containers.\");\n        break;\n      }\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
          "extendedDetails": {}
        }
      ]
    },
    "4a877737182808fe3392a116762ba59973a477a6": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6916. Moving logging APIs over to slf4j in hadoop-yarn-server-common. Contributed by Bibin A Chundatt and Akira Ajisaka.\n",
      "commitDate": "03/10/17 2:06 PM",
      "commitName": "4a877737182808fe3392a116762ba59973a477a6",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "05/01/17 10:31 AM",
      "commitNameOld": "0a55bd841ec0f2eb89a0383f4c589526e8b138d4",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 271.11,
      "commitsBetweenForRepo": 1688,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,42 @@\n   private void allocateContainersInternal(long rmIdentifier,\n       AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n       Map\u003cString, RemoteNode\u003e allNodes, String userName,\n       Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n       throws YarnException {\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (containers.isEmpty() ? 0 :\n             containers.get(anyAsk.getCapability()).size());\n \n     List\u003cRemoteNode\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n     for (Entry\u003cString, RemoteNode\u003e nodeEntry : allNodes.entrySet()) {\n       // Do not use blacklisted nodes for scheduling.\n       if (blacklist.contains(nodeEntry.getKey())) {\n         continue;\n       }\n       nodesForScheduling.add(nodeEntry.getValue());\n     }\n     if (nodesForScheduling.isEmpty()) {\n       LOG.warn(\"No nodes available for allocating opportunistic containers. [\" +\n-          \"allNodes\u003d\" + allNodes + \", \" +\n-          \"blacklist\u003d\" + blacklist + \"]\");\n+          \"allNodes\u003d{}, blacklist\u003d{}]\", allNodes, blacklist);\n       return;\n     }\n     int numAllocated \u003d 0;\n     int nextNodeToSchedule \u003d 0;\n     for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n       nextNodeToSchedule++;\n       nextNodeToSchedule %\u003d nodesForScheduling.size();\n       RemoteNode node \u003d nodesForScheduling.get(nextNodeToSchedule);\n       Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n           anyAsk, id, userName, node);\n       List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n       if (cList \u003d\u003d null) {\n         cList \u003d new ArrayList\u003c\u003e();\n         containers.put(anyAsk.getCapability(), cList);\n       }\n       cList.add(container);\n       numAllocated++;\n-      LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n+      LOG.info(\"Allocated [{}] as opportunistic.\", container.getId());\n     }\n-    LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n+    LOG.info(\"Allocated {} opportunistic containers.\", numAllocated);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n      Map\u003cString, RemoteNode\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n      throws YarnException {\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (containers.isEmpty() ? 0 :\n            containers.get(anyAsk.getCapability()).size());\n\n    List\u003cRemoteNode\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n    for (Entry\u003cString, RemoteNode\u003e nodeEntry : allNodes.entrySet()) {\n      // Do not use blacklisted nodes for scheduling.\n      if (blacklist.contains(nodeEntry.getKey())) {\n        continue;\n      }\n      nodesForScheduling.add(nodeEntry.getValue());\n    }\n    if (nodesForScheduling.isEmpty()) {\n      LOG.warn(\"No nodes available for allocating opportunistic containers. [\" +\n          \"allNodes\u003d{}, blacklist\u003d{}]\", allNodes, blacklist);\n      return;\n    }\n    int numAllocated \u003d 0;\n    int nextNodeToSchedule \u003d 0;\n    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n      nextNodeToSchedule++;\n      nextNodeToSchedule %\u003d nodesForScheduling.size();\n      RemoteNode node \u003d nodesForScheduling.get(nextNodeToSchedule);\n      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n          anyAsk, id, userName, node);\n      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n      if (cList \u003d\u003d null) {\n        cList \u003d new ArrayList\u003c\u003e();\n        containers.put(anyAsk.getCapability(), cList);\n      }\n      cList.add(container);\n      numAllocated++;\n      LOG.info(\"Allocated [{}] as opportunistic.\", container.getId());\n    }\n    LOG.info(\"Allocated {} opportunistic containers.\", numAllocated);\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
      "extendedDetails": {}
    },
    "ac1e5d4f77e3b9df8dcacb0b1f72eecc27931eb8": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5938. Refactoring OpportunisticContainerAllocator to use SchedulerRequestKey instead of Priority and other misc fixes (asuresh)\n",
      "commitDate": "27/12/16 12:40 PM",
      "commitName": "ac1e5d4f77e3b9df8dcacb0b1f72eecc27931eb8",
      "commitAuthor": "Arun Suresh",
      "commitDateOld": "09/11/16 12:11 AM",
      "commitNameOld": "283fa33febe043bd7b4fa87546be26c9c5a8f8b5",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 48.52,
      "commitsBetweenForRepo": 269,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,43 @@\n   private void allocateContainersInternal(long rmIdentifier,\n       AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n       Map\u003cString, RemoteNode\u003e allNodes, String userName,\n       Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n       throws YarnException {\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (containers.isEmpty() ? 0 :\n             containers.get(anyAsk.getCapability()).size());\n \n     List\u003cRemoteNode\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n     for (Entry\u003cString, RemoteNode\u003e nodeEntry : allNodes.entrySet()) {\n       // Do not use blacklisted nodes for scheduling.\n       if (blacklist.contains(nodeEntry.getKey())) {\n         continue;\n       }\n       nodesForScheduling.add(nodeEntry.getValue());\n     }\n     if (nodesForScheduling.isEmpty()) {\n-      LOG.warn(\"No nodes available for allocating opportunistic containers.\");\n+      LOG.warn(\"No nodes available for allocating opportunistic containers. [\" +\n+          \"allNodes\u003d\" + allNodes + \", \" +\n+          \"blacklist\u003d\" + blacklist + \"]\");\n       return;\n     }\n     int numAllocated \u003d 0;\n     int nextNodeToSchedule \u003d 0;\n     for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n       nextNodeToSchedule++;\n       nextNodeToSchedule %\u003d nodesForScheduling.size();\n       RemoteNode node \u003d nodesForScheduling.get(nextNodeToSchedule);\n       Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n           anyAsk, id, userName, node);\n       List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n       if (cList \u003d\u003d null) {\n         cList \u003d new ArrayList\u003c\u003e();\n         containers.put(anyAsk.getCapability(), cList);\n       }\n       cList.add(container);\n       numAllocated++;\n       LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n     }\n     LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n      Map\u003cString, RemoteNode\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n      throws YarnException {\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (containers.isEmpty() ? 0 :\n            containers.get(anyAsk.getCapability()).size());\n\n    List\u003cRemoteNode\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n    for (Entry\u003cString, RemoteNode\u003e nodeEntry : allNodes.entrySet()) {\n      // Do not use blacklisted nodes for scheduling.\n      if (blacklist.contains(nodeEntry.getKey())) {\n        continue;\n      }\n      nodesForScheduling.add(nodeEntry.getValue());\n    }\n    if (nodesForScheduling.isEmpty()) {\n      LOG.warn(\"No nodes available for allocating opportunistic containers. [\" +\n          \"allNodes\u003d\" + allNodes + \", \" +\n          \"blacklist\u003d\" + blacklist + \"]\");\n      return;\n    }\n    int numAllocated \u003d 0;\n    int nextNodeToSchedule \u003d 0;\n    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n      nextNodeToSchedule++;\n      nextNodeToSchedule %\u003d nodesForScheduling.size();\n      RemoteNode node \u003d nodesForScheduling.get(nextNodeToSchedule);\n      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n          anyAsk, id, userName, node);\n      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n      if (cList \u003d\u003d null) {\n        cList \u003d new ArrayList\u003c\u003e();\n        containers.put(anyAsk.getCapability(), cList);\n      }\n      cList.add(container);\n      numAllocated++;\n      LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n    }\n    LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
      "extendedDetails": {}
    },
    "aa3cab1eb29c56368d15882d7260a994e615e8d8": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-5799. Fix Opportunistic Allocation to set the correct value of Node Http Address. (asuresh)\n",
      "commitDate": "29/10/16 2:03 AM",
      "commitName": "aa3cab1eb29c56368d15882d7260a994e615e8d8",
      "commitAuthor": "Arun Suresh",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-5799. Fix Opportunistic Allocation to set the correct value of Node Http Address. (asuresh)\n",
          "commitDate": "29/10/16 2:03 AM",
          "commitName": "aa3cab1eb29c56368d15882d7260a994e615e8d8",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "29/09/16 3:11 PM",
          "commitNameOld": "10be45986cdf86a89055065b752959bd6369d54f",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 29.45,
          "commitsBetweenForRepo": 258,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,41 @@\n   private void allocateContainersInternal(long rmIdentifier,\n       AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n-      Map\u003cString, NodeId\u003e allNodes, String userName,\n+      Map\u003cString, RemoteNode\u003e allNodes, String userName,\n       Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n       throws YarnException {\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (containers.isEmpty() ? 0 :\n             containers.get(anyAsk.getCapability()).size());\n \n-    List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n-    for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n+    List\u003cRemoteNode\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n+    for (Entry\u003cString, RemoteNode\u003e nodeEntry : allNodes.entrySet()) {\n       // Do not use blacklisted nodes for scheduling.\n       if (blacklist.contains(nodeEntry.getKey())) {\n         continue;\n       }\n       nodesForScheduling.add(nodeEntry.getValue());\n     }\n     if (nodesForScheduling.isEmpty()) {\n       LOG.warn(\"No nodes available for allocating opportunistic containers.\");\n       return;\n     }\n     int numAllocated \u003d 0;\n     int nextNodeToSchedule \u003d 0;\n     for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n       nextNodeToSchedule++;\n       nextNodeToSchedule %\u003d nodesForScheduling.size();\n-      NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n+      RemoteNode node \u003d nodesForScheduling.get(nextNodeToSchedule);\n       Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n-          anyAsk, id, userName, nodeId);\n+          anyAsk, id, userName, node);\n       List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n       if (cList \u003d\u003d null) {\n         cList \u003d new ArrayList\u003c\u003e();\n         containers.put(anyAsk.getCapability(), cList);\n       }\n       cList.add(container);\n       numAllocated++;\n       LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n     }\n     LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n      Map\u003cString, RemoteNode\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n      throws YarnException {\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (containers.isEmpty() ? 0 :\n            containers.get(anyAsk.getCapability()).size());\n\n    List\u003cRemoteNode\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n    for (Entry\u003cString, RemoteNode\u003e nodeEntry : allNodes.entrySet()) {\n      // Do not use blacklisted nodes for scheduling.\n      if (blacklist.contains(nodeEntry.getKey())) {\n        continue;\n      }\n      nodesForScheduling.add(nodeEntry.getValue());\n    }\n    if (nodesForScheduling.isEmpty()) {\n      LOG.warn(\"No nodes available for allocating opportunistic containers.\");\n      return;\n    }\n    int numAllocated \u003d 0;\n    int nextNodeToSchedule \u003d 0;\n    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n      nextNodeToSchedule++;\n      nextNodeToSchedule %\u003d nodesForScheduling.size();\n      RemoteNode node \u003d nodesForScheduling.get(nextNodeToSchedule);\n      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n          anyAsk, id, userName, node);\n      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n      if (cList \u003d\u003d null) {\n        cList \u003d new ArrayList\u003c\u003e();\n        containers.put(anyAsk.getCapability(), cList);\n      }\n      cList.add(container);\n      numAllocated++;\n      LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n    }\n    LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "[rmIdentifier-long, appParams-AllocationParams, idCounter-ContainerIdGenerator, blacklist-Set\u003cString\u003e, id-ApplicationAttemptId, allNodes-Map\u003cString,NodeId\u003e, userName-String, containers-Map\u003cResource,List\u003cContainer\u003e\u003e, anyAsk-ResourceRequest]",
            "newValue": "[rmIdentifier-long, appParams-AllocationParams, idCounter-ContainerIdGenerator, blacklist-Set\u003cString\u003e, id-ApplicationAttemptId, allNodes-Map\u003cString,RemoteNode\u003e, userName-String, containers-Map\u003cResource,List\u003cContainer\u003e\u003e, anyAsk-ResourceRequest]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-5799. Fix Opportunistic Allocation to set the correct value of Node Http Address. (asuresh)\n",
          "commitDate": "29/10/16 2:03 AM",
          "commitName": "aa3cab1eb29c56368d15882d7260a994e615e8d8",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "29/09/16 3:11 PM",
          "commitNameOld": "10be45986cdf86a89055065b752959bd6369d54f",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 29.45,
          "commitsBetweenForRepo": 258,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,41 @@\n   private void allocateContainersInternal(long rmIdentifier,\n       AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n-      Map\u003cString, NodeId\u003e allNodes, String userName,\n+      Map\u003cString, RemoteNode\u003e allNodes, String userName,\n       Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n       throws YarnException {\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (containers.isEmpty() ? 0 :\n             containers.get(anyAsk.getCapability()).size());\n \n-    List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n-    for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n+    List\u003cRemoteNode\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n+    for (Entry\u003cString, RemoteNode\u003e nodeEntry : allNodes.entrySet()) {\n       // Do not use blacklisted nodes for scheduling.\n       if (blacklist.contains(nodeEntry.getKey())) {\n         continue;\n       }\n       nodesForScheduling.add(nodeEntry.getValue());\n     }\n     if (nodesForScheduling.isEmpty()) {\n       LOG.warn(\"No nodes available for allocating opportunistic containers.\");\n       return;\n     }\n     int numAllocated \u003d 0;\n     int nextNodeToSchedule \u003d 0;\n     for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n       nextNodeToSchedule++;\n       nextNodeToSchedule %\u003d nodesForScheduling.size();\n-      NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n+      RemoteNode node \u003d nodesForScheduling.get(nextNodeToSchedule);\n       Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n-          anyAsk, id, userName, nodeId);\n+          anyAsk, id, userName, node);\n       List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n       if (cList \u003d\u003d null) {\n         cList \u003d new ArrayList\u003c\u003e();\n         containers.put(anyAsk.getCapability(), cList);\n       }\n       cList.add(container);\n       numAllocated++;\n       LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n     }\n     LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n      Map\u003cString, RemoteNode\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n      throws YarnException {\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (containers.isEmpty() ? 0 :\n            containers.get(anyAsk.getCapability()).size());\n\n    List\u003cRemoteNode\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n    for (Entry\u003cString, RemoteNode\u003e nodeEntry : allNodes.entrySet()) {\n      // Do not use blacklisted nodes for scheduling.\n      if (blacklist.contains(nodeEntry.getKey())) {\n        continue;\n      }\n      nodesForScheduling.add(nodeEntry.getValue());\n    }\n    if (nodesForScheduling.isEmpty()) {\n      LOG.warn(\"No nodes available for allocating opportunistic containers.\");\n      return;\n    }\n    int numAllocated \u003d 0;\n    int nextNodeToSchedule \u003d 0;\n    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n      nextNodeToSchedule++;\n      nextNodeToSchedule %\u003d nodesForScheduling.size();\n      RemoteNode node \u003d nodesForScheduling.get(nextNodeToSchedule);\n      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n          anyAsk, id, userName, node);\n      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n      if (cList \u003d\u003d null) {\n        cList \u003d new ArrayList\u003c\u003e();\n        containers.put(anyAsk.getCapability(), cList);\n      }\n      cList.add(container);\n      numAllocated++;\n      LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n    }\n    LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
          "extendedDetails": {}
        }
      ]
    },
    "10be45986cdf86a89055065b752959bd6369d54f": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5486. Update OpportunisticContainerAllocatorAMService::allocate method to handle OPPORTUNISTIC container requests. (Konstantinos Karanasos via asuresh)\n",
      "commitDate": "29/09/16 3:11 PM",
      "commitName": "10be45986cdf86a89055065b752959bd6369d54f",
      "commitAuthor": "Arun Suresh",
      "commitDateOld": "30/08/16 3:52 PM",
      "commitNameOld": "d6d9cff21b7b6141ed88359652cf22e8973c0661",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 29.97,
      "commitsBetweenForRepo": 164,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,41 @@\n   private void allocateContainersInternal(long rmIdentifier,\n       AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n       Map\u003cString, NodeId\u003e allNodes, String userName,\n       Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n       throws YarnException {\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (containers.isEmpty() ? 0 :\n             containers.get(anyAsk.getCapability()).size());\n \n     List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n     for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n       // Do not use blacklisted nodes for scheduling.\n       if (blacklist.contains(nodeEntry.getKey())) {\n         continue;\n       }\n       nodesForScheduling.add(nodeEntry.getValue());\n     }\n+    if (nodesForScheduling.isEmpty()) {\n+      LOG.warn(\"No nodes available for allocating opportunistic containers.\");\n+      return;\n+    }\n     int numAllocated \u003d 0;\n     int nextNodeToSchedule \u003d 0;\n     for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n       nextNodeToSchedule++;\n       nextNodeToSchedule %\u003d nodesForScheduling.size();\n       NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n       Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n           anyAsk, id, userName, nodeId);\n       List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n       if (cList \u003d\u003d null) {\n         cList \u003d new ArrayList\u003c\u003e();\n         containers.put(anyAsk.getCapability(), cList);\n       }\n       cList.add(container);\n       numAllocated++;\n       LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n     }\n     LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n      Map\u003cString, NodeId\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n      throws YarnException {\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (containers.isEmpty() ? 0 :\n            containers.get(anyAsk.getCapability()).size());\n\n    List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n    for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n      // Do not use blacklisted nodes for scheduling.\n      if (blacklist.contains(nodeEntry.getKey())) {\n        continue;\n      }\n      nodesForScheduling.add(nodeEntry.getValue());\n    }\n    if (nodesForScheduling.isEmpty()) {\n      LOG.warn(\"No nodes available for allocating opportunistic containers.\");\n      return;\n    }\n    int numAllocated \u003d 0;\n    int nextNodeToSchedule \u003d 0;\n    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n      nextNodeToSchedule++;\n      nextNodeToSchedule %\u003d nodesForScheduling.size();\n      NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n          anyAsk, id, userName, nodeId);\n      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n      if (cList \u003d\u003d null) {\n        cList \u003d new ArrayList\u003c\u003e();\n        containers.put(anyAsk.getCapability(), cList);\n      }\n      cList.add(container);\n      numAllocated++;\n      LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n    }\n    LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
      "extendedDetails": {}
    },
    "82c9e061017c32e633e0b0cbb7978749a6df4fb2": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange,Yrename,Yparameterchange)",
      "commitMessage": "YARN-5457. Refactor DistributedScheduling framework to pull out common functionality. (asuresh)\n",
      "commitDate": "09/08/16 12:42 AM",
      "commitName": "82c9e061017c32e633e0b0cbb7978749a6df4fb2",
      "commitAuthor": "Arun Suresh",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "YARN-5457. Refactor DistributedScheduling framework to pull out common functionality. (asuresh)\n",
          "commitDate": "09/08/16 12:42 AM",
          "commitName": "82c9e061017c32e633e0b0cbb7978749a6df4fb2",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "08/08/16 5:54 PM",
          "commitNameOld": "8f9b61852bf6600b65e49875fec172bac9e0a85d",
          "commitAuthorOld": "Masatake Iwasaki",
          "daysBetweenCommits": 0.28,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,37 @@\n-  private void allocateOpportunisticContainers(\n-      DistributedSchedulerParams appParams, ContainerIdCounter idCounter,\n+  private void allocateContainersInternal(long rmIdentifier,\n+      AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n       Map\u003cString, NodeId\u003e allNodes, String userName,\n       Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n       throws YarnException {\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (containers.isEmpty() ? 0 :\n             containers.get(anyAsk.getCapability()).size());\n \n     List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n     for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n       // Do not use blacklisted nodes for scheduling.\n       if (blacklist.contains(nodeEntry.getKey())) {\n         continue;\n       }\n       nodesForScheduling.add(nodeEntry.getValue());\n     }\n     int numAllocated \u003d 0;\n     int nextNodeToSchedule \u003d 0;\n     for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n       nextNodeToSchedule++;\n       nextNodeToSchedule %\u003d nodesForScheduling.size();\n       NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n-      Container container \u003d buildContainer(appParams, idCounter, anyAsk, id,\n-          userName, nodeId);\n+      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n+          anyAsk, id, userName, nodeId);\n       List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n       if (cList \u003d\u003d null) {\n         cList \u003d new ArrayList\u003c\u003e();\n         containers.put(anyAsk.getCapability(), cList);\n       }\n       cList.add(container);\n       numAllocated++;\n       LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n     }\n     LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n      Map\u003cString, NodeId\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n      throws YarnException {\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (containers.isEmpty() ? 0 :\n            containers.get(anyAsk.getCapability()).size());\n\n    List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n    for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n      // Do not use blacklisted nodes for scheduling.\n      if (blacklist.contains(nodeEntry.getKey())) {\n        continue;\n      }\n      nodesForScheduling.add(nodeEntry.getValue());\n    }\n    int numAllocated \u003d 0;\n    int nextNodeToSchedule \u003d 0;\n    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n      nextNodeToSchedule++;\n      nextNodeToSchedule %\u003d nodesForScheduling.size();\n      NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n          anyAsk, id, userName, nodeId);\n      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n      if (cList \u003d\u003d null) {\n        cList \u003d new ArrayList\u003c\u003e();\n        containers.put(anyAsk.getCapability(), cList);\n      }\n      cList.add(container);\n      numAllocated++;\n      LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n    }\n    LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
          "extendedDetails": {
            "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/OpportunisticContainerAllocator.java",
            "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
            "oldMethodName": "allocateOpportunisticContainers",
            "newMethodName": "allocateContainersInternal"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-5457. Refactor DistributedScheduling framework to pull out common functionality. (asuresh)\n",
          "commitDate": "09/08/16 12:42 AM",
          "commitName": "82c9e061017c32e633e0b0cbb7978749a6df4fb2",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "08/08/16 5:54 PM",
          "commitNameOld": "8f9b61852bf6600b65e49875fec172bac9e0a85d",
          "commitAuthorOld": "Masatake Iwasaki",
          "daysBetweenCommits": 0.28,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,37 @@\n-  private void allocateOpportunisticContainers(\n-      DistributedSchedulerParams appParams, ContainerIdCounter idCounter,\n+  private void allocateContainersInternal(long rmIdentifier,\n+      AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n       Map\u003cString, NodeId\u003e allNodes, String userName,\n       Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n       throws YarnException {\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (containers.isEmpty() ? 0 :\n             containers.get(anyAsk.getCapability()).size());\n \n     List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n     for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n       // Do not use blacklisted nodes for scheduling.\n       if (blacklist.contains(nodeEntry.getKey())) {\n         continue;\n       }\n       nodesForScheduling.add(nodeEntry.getValue());\n     }\n     int numAllocated \u003d 0;\n     int nextNodeToSchedule \u003d 0;\n     for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n       nextNodeToSchedule++;\n       nextNodeToSchedule %\u003d nodesForScheduling.size();\n       NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n-      Container container \u003d buildContainer(appParams, idCounter, anyAsk, id,\n-          userName, nodeId);\n+      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n+          anyAsk, id, userName, nodeId);\n       List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n       if (cList \u003d\u003d null) {\n         cList \u003d new ArrayList\u003c\u003e();\n         containers.put(anyAsk.getCapability(), cList);\n       }\n       cList.add(container);\n       numAllocated++;\n       LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n     }\n     LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n      Map\u003cString, NodeId\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n      throws YarnException {\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (containers.isEmpty() ? 0 :\n            containers.get(anyAsk.getCapability()).size());\n\n    List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n    for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n      // Do not use blacklisted nodes for scheduling.\n      if (blacklist.contains(nodeEntry.getKey())) {\n        continue;\n      }\n      nodesForScheduling.add(nodeEntry.getValue());\n    }\n    int numAllocated \u003d 0;\n    int nextNodeToSchedule \u003d 0;\n    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n      nextNodeToSchedule++;\n      nextNodeToSchedule %\u003d nodesForScheduling.size();\n      NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n          anyAsk, id, userName, nodeId);\n      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n      if (cList \u003d\u003d null) {\n        cList \u003d new ArrayList\u003c\u003e();\n        containers.put(anyAsk.getCapability(), cList);\n      }\n      cList.add(container);\n      numAllocated++;\n      LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n    }\n    LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
          "extendedDetails": {}
        },
        {
          "type": "Yrename",
          "commitMessage": "YARN-5457. Refactor DistributedScheduling framework to pull out common functionality. (asuresh)\n",
          "commitDate": "09/08/16 12:42 AM",
          "commitName": "82c9e061017c32e633e0b0cbb7978749a6df4fb2",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "08/08/16 5:54 PM",
          "commitNameOld": "8f9b61852bf6600b65e49875fec172bac9e0a85d",
          "commitAuthorOld": "Masatake Iwasaki",
          "daysBetweenCommits": 0.28,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,37 @@\n-  private void allocateOpportunisticContainers(\n-      DistributedSchedulerParams appParams, ContainerIdCounter idCounter,\n+  private void allocateContainersInternal(long rmIdentifier,\n+      AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n       Map\u003cString, NodeId\u003e allNodes, String userName,\n       Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n       throws YarnException {\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (containers.isEmpty() ? 0 :\n             containers.get(anyAsk.getCapability()).size());\n \n     List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n     for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n       // Do not use blacklisted nodes for scheduling.\n       if (blacklist.contains(nodeEntry.getKey())) {\n         continue;\n       }\n       nodesForScheduling.add(nodeEntry.getValue());\n     }\n     int numAllocated \u003d 0;\n     int nextNodeToSchedule \u003d 0;\n     for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n       nextNodeToSchedule++;\n       nextNodeToSchedule %\u003d nodesForScheduling.size();\n       NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n-      Container container \u003d buildContainer(appParams, idCounter, anyAsk, id,\n-          userName, nodeId);\n+      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n+          anyAsk, id, userName, nodeId);\n       List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n       if (cList \u003d\u003d null) {\n         cList \u003d new ArrayList\u003c\u003e();\n         containers.put(anyAsk.getCapability(), cList);\n       }\n       cList.add(container);\n       numAllocated++;\n       LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n     }\n     LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n      Map\u003cString, NodeId\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n      throws YarnException {\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (containers.isEmpty() ? 0 :\n            containers.get(anyAsk.getCapability()).size());\n\n    List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n    for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n      // Do not use blacklisted nodes for scheduling.\n      if (blacklist.contains(nodeEntry.getKey())) {\n        continue;\n      }\n      nodesForScheduling.add(nodeEntry.getValue());\n    }\n    int numAllocated \u003d 0;\n    int nextNodeToSchedule \u003d 0;\n    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n      nextNodeToSchedule++;\n      nextNodeToSchedule %\u003d nodesForScheduling.size();\n      NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n          anyAsk, id, userName, nodeId);\n      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n      if (cList \u003d\u003d null) {\n        cList \u003d new ArrayList\u003c\u003e();\n        containers.put(anyAsk.getCapability(), cList);\n      }\n      cList.add(container);\n      numAllocated++;\n      LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n    }\n    LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "allocateOpportunisticContainers",
            "newValue": "allocateContainersInternal"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-5457. Refactor DistributedScheduling framework to pull out common functionality. (asuresh)\n",
          "commitDate": "09/08/16 12:42 AM",
          "commitName": "82c9e061017c32e633e0b0cbb7978749a6df4fb2",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "08/08/16 5:54 PM",
          "commitNameOld": "8f9b61852bf6600b65e49875fec172bac9e0a85d",
          "commitAuthorOld": "Masatake Iwasaki",
          "daysBetweenCommits": 0.28,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,37 @@\n-  private void allocateOpportunisticContainers(\n-      DistributedSchedulerParams appParams, ContainerIdCounter idCounter,\n+  private void allocateContainersInternal(long rmIdentifier,\n+      AllocationParams appParams, ContainerIdGenerator idCounter,\n       Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n       Map\u003cString, NodeId\u003e allNodes, String userName,\n       Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n       throws YarnException {\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (containers.isEmpty() ? 0 :\n             containers.get(anyAsk.getCapability()).size());\n \n     List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n     for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n       // Do not use blacklisted nodes for scheduling.\n       if (blacklist.contains(nodeEntry.getKey())) {\n         continue;\n       }\n       nodesForScheduling.add(nodeEntry.getValue());\n     }\n     int numAllocated \u003d 0;\n     int nextNodeToSchedule \u003d 0;\n     for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n       nextNodeToSchedule++;\n       nextNodeToSchedule %\u003d nodesForScheduling.size();\n       NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n-      Container container \u003d buildContainer(appParams, idCounter, anyAsk, id,\n-          userName, nodeId);\n+      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n+          anyAsk, id, userName, nodeId);\n       List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n       if (cList \u003d\u003d null) {\n         cList \u003d new ArrayList\u003c\u003e();\n         containers.put(anyAsk.getCapability(), cList);\n       }\n       cList.add(container);\n       numAllocated++;\n       LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n     }\n     LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void allocateContainersInternal(long rmIdentifier,\n      AllocationParams appParams, ContainerIdGenerator idCounter,\n      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n      Map\u003cString, NodeId\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n      throws YarnException {\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (containers.isEmpty() ? 0 :\n            containers.get(anyAsk.getCapability()).size());\n\n    List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n    for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n      // Do not use blacklisted nodes for scheduling.\n      if (blacklist.contains(nodeEntry.getKey())) {\n        continue;\n      }\n      nodesForScheduling.add(nodeEntry.getValue());\n    }\n    int numAllocated \u003d 0;\n    int nextNodeToSchedule \u003d 0;\n    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n      nextNodeToSchedule++;\n      nextNodeToSchedule %\u003d nodesForScheduling.size();\n      NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n      Container container \u003d buildContainer(rmIdentifier, appParams, idCounter,\n          anyAsk, id, userName, nodeId);\n      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n      if (cList \u003d\u003d null) {\n        cList \u003d new ArrayList\u003c\u003e();\n        containers.put(anyAsk.getCapability(), cList);\n      }\n      cList.add(container);\n      numAllocated++;\n      LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n    }\n    LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/scheduler/OpportunisticContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "[appParams-DistributedSchedulerParams, idCounter-ContainerIdCounter, blacklist-Set\u003cString\u003e, id-ApplicationAttemptId, allNodes-Map\u003cString,NodeId\u003e, userName-String, containers-Map\u003cResource,List\u003cContainer\u003e\u003e, anyAsk-ResourceRequest]",
            "newValue": "[rmIdentifier-long, appParams-AllocationParams, idCounter-ContainerIdGenerator, blacklist-Set\u003cString\u003e, id-ApplicationAttemptId, allNodes-Map\u003cString,NodeId\u003e, userName-String, containers-Map\u003cResource,List\u003cContainer\u003e\u003e, anyAsk-ResourceRequest]"
          }
        }
      ]
    },
    "e5766b1dbee02ae0ef89618e172f3fb227af19e8": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-5113. Refactoring and other clean-up for distributed scheduling. (Konstantinos Karanasos via asuresh)\n",
      "commitDate": "31/07/16 11:48 AM",
      "commitName": "e5766b1dbee02ae0ef89618e172f3fb227af19e8",
      "commitAuthor": "Arun Suresh",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-5113. Refactoring and other clean-up for distributed scheduling. (Konstantinos Karanasos via asuresh)\n",
          "commitDate": "31/07/16 11:48 AM",
          "commitName": "e5766b1dbee02ae0ef89618e172f3fb227af19e8",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "27/05/16 2:06 PM",
          "commitNameOld": "aa975bc7811fc7c52b814ad9635bff8c2d34655b",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 64.9,
          "commitsBetweenForRepo": 536,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,37 @@\n-  private void allocateOpportunisticContainers(DistSchedulerParams appParams,\n-      ContainerIdCounter idCounter, Set\u003cString\u003e blacklist,\n-      ApplicationAttemptId id, Map\u003cString, NodeId\u003e allNodes, String userName,\n-      Map\u003cResource, List\u003cContainer\u003e\u003e containers, Set\u003cString\u003e nodesAllocated,\n-      ResourceRequest anyAsk) throws YarnException {\n+  private void allocateOpportunisticContainers(\n+      DistributedSchedulerParams appParams, ContainerIdCounter idCounter,\n+      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n+      Map\u003cString, NodeId\u003e allNodes, String userName,\n+      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n+      throws YarnException {\n     int toAllocate \u003d anyAsk.getNumContainers()\n-        - (containers.isEmpty() ?\n-        0 : containers.get(anyAsk.getCapability()).size());\n+        - (containers.isEmpty() ? 0 :\n+            containers.get(anyAsk.getCapability()).size());\n \n-    List\u003cString\u003e topKNodesLeft \u003d new ArrayList\u003c\u003e();\n-    for (String s : allNodes.keySet()) {\n-      // Bias away from whatever we have already allocated and respect blacklist\n-      if (nodesAllocated.contains(s) || blacklist.contains(s)) {\n+    List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n+    for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n+      // Do not use blacklisted nodes for scheduling.\n+      if (blacklist.contains(nodeEntry.getKey())) {\n         continue;\n       }\n-      topKNodesLeft.add(s);\n+      nodesForScheduling.add(nodeEntry.getValue());\n     }\n     int numAllocated \u003d 0;\n-    int nextNodeToAllocate \u003d 0;\n+    int nextNodeToSchedule \u003d 0;\n     for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n-      String topNode \u003d topKNodesLeft.get(nextNodeToAllocate);\n-      nextNodeToAllocate++;\n-      nextNodeToAllocate %\u003d topKNodesLeft.size();\n-      NodeId nodeId \u003d allNodes.get(topNode);\n+      nextNodeToSchedule++;\n+      nextNodeToSchedule %\u003d nodesForScheduling.size();\n+      NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n       Container container \u003d buildContainer(appParams, idCounter, anyAsk, id,\n           userName, nodeId);\n       List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n       if (cList \u003d\u003d null) {\n         cList \u003d new ArrayList\u003c\u003e();\n         containers.put(anyAsk.getCapability(), cList);\n       }\n       cList.add(container);\n       numAllocated++;\n       LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n     }\n     LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void allocateOpportunisticContainers(\n      DistributedSchedulerParams appParams, ContainerIdCounter idCounter,\n      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n      Map\u003cString, NodeId\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n      throws YarnException {\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (containers.isEmpty() ? 0 :\n            containers.get(anyAsk.getCapability()).size());\n\n    List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n    for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n      // Do not use blacklisted nodes for scheduling.\n      if (blacklist.contains(nodeEntry.getKey())) {\n        continue;\n      }\n      nodesForScheduling.add(nodeEntry.getValue());\n    }\n    int numAllocated \u003d 0;\n    int nextNodeToSchedule \u003d 0;\n    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n      nextNodeToSchedule++;\n      nextNodeToSchedule %\u003d nodesForScheduling.size();\n      NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n      Container container \u003d buildContainer(appParams, idCounter, anyAsk, id,\n          userName, nodeId);\n      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n      if (cList \u003d\u003d null) {\n        cList \u003d new ArrayList\u003c\u003e();\n        containers.put(anyAsk.getCapability(), cList);\n      }\n      cList.add(container);\n      numAllocated++;\n      LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n    }\n    LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/OpportunisticContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "[appParams-DistSchedulerParams, idCounter-ContainerIdCounter, blacklist-Set\u003cString\u003e, id-ApplicationAttemptId, allNodes-Map\u003cString,NodeId\u003e, userName-String, containers-Map\u003cResource,List\u003cContainer\u003e\u003e, nodesAllocated-Set\u003cString\u003e, anyAsk-ResourceRequest]",
            "newValue": "[appParams-DistributedSchedulerParams, idCounter-ContainerIdCounter, blacklist-Set\u003cString\u003e, id-ApplicationAttemptId, allNodes-Map\u003cString,NodeId\u003e, userName-String, containers-Map\u003cResource,List\u003cContainer\u003e\u003e, anyAsk-ResourceRequest]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-5113. Refactoring and other clean-up for distributed scheduling. (Konstantinos Karanasos via asuresh)\n",
          "commitDate": "31/07/16 11:48 AM",
          "commitName": "e5766b1dbee02ae0ef89618e172f3fb227af19e8",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "27/05/16 2:06 PM",
          "commitNameOld": "aa975bc7811fc7c52b814ad9635bff8c2d34655b",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 64.9,
          "commitsBetweenForRepo": 536,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,37 +1,37 @@\n-  private void allocateOpportunisticContainers(DistSchedulerParams appParams,\n-      ContainerIdCounter idCounter, Set\u003cString\u003e blacklist,\n-      ApplicationAttemptId id, Map\u003cString, NodeId\u003e allNodes, String userName,\n-      Map\u003cResource, List\u003cContainer\u003e\u003e containers, Set\u003cString\u003e nodesAllocated,\n-      ResourceRequest anyAsk) throws YarnException {\n+  private void allocateOpportunisticContainers(\n+      DistributedSchedulerParams appParams, ContainerIdCounter idCounter,\n+      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n+      Map\u003cString, NodeId\u003e allNodes, String userName,\n+      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n+      throws YarnException {\n     int toAllocate \u003d anyAsk.getNumContainers()\n-        - (containers.isEmpty() ?\n-        0 : containers.get(anyAsk.getCapability()).size());\n+        - (containers.isEmpty() ? 0 :\n+            containers.get(anyAsk.getCapability()).size());\n \n-    List\u003cString\u003e topKNodesLeft \u003d new ArrayList\u003c\u003e();\n-    for (String s : allNodes.keySet()) {\n-      // Bias away from whatever we have already allocated and respect blacklist\n-      if (nodesAllocated.contains(s) || blacklist.contains(s)) {\n+    List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n+    for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n+      // Do not use blacklisted nodes for scheduling.\n+      if (blacklist.contains(nodeEntry.getKey())) {\n         continue;\n       }\n-      topKNodesLeft.add(s);\n+      nodesForScheduling.add(nodeEntry.getValue());\n     }\n     int numAllocated \u003d 0;\n-    int nextNodeToAllocate \u003d 0;\n+    int nextNodeToSchedule \u003d 0;\n     for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n-      String topNode \u003d topKNodesLeft.get(nextNodeToAllocate);\n-      nextNodeToAllocate++;\n-      nextNodeToAllocate %\u003d topKNodesLeft.size();\n-      NodeId nodeId \u003d allNodes.get(topNode);\n+      nextNodeToSchedule++;\n+      nextNodeToSchedule %\u003d nodesForScheduling.size();\n+      NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n       Container container \u003d buildContainer(appParams, idCounter, anyAsk, id,\n           userName, nodeId);\n       List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n       if (cList \u003d\u003d null) {\n         cList \u003d new ArrayList\u003c\u003e();\n         containers.put(anyAsk.getCapability(), cList);\n       }\n       cList.add(container);\n       numAllocated++;\n       LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n     }\n     LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void allocateOpportunisticContainers(\n      DistributedSchedulerParams appParams, ContainerIdCounter idCounter,\n      Set\u003cString\u003e blacklist, ApplicationAttemptId id,\n      Map\u003cString, NodeId\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cContainer\u003e\u003e containers, ResourceRequest anyAsk)\n      throws YarnException {\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (containers.isEmpty() ? 0 :\n            containers.get(anyAsk.getCapability()).size());\n\n    List\u003cNodeId\u003e nodesForScheduling \u003d new ArrayList\u003c\u003e();\n    for (Entry\u003cString, NodeId\u003e nodeEntry : allNodes.entrySet()) {\n      // Do not use blacklisted nodes for scheduling.\n      if (blacklist.contains(nodeEntry.getKey())) {\n        continue;\n      }\n      nodesForScheduling.add(nodeEntry.getValue());\n    }\n    int numAllocated \u003d 0;\n    int nextNodeToSchedule \u003d 0;\n    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n      nextNodeToSchedule++;\n      nextNodeToSchedule %\u003d nodesForScheduling.size();\n      NodeId nodeId \u003d nodesForScheduling.get(nextNodeToSchedule);\n      Container container \u003d buildContainer(appParams, idCounter, anyAsk, id,\n          userName, nodeId);\n      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n      if (cList \u003d\u003d null) {\n        cList \u003d new ArrayList\u003c\u003e();\n        containers.put(anyAsk.getCapability(), cList);\n      }\n      cList.add(container);\n      numAllocated++;\n      LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n    }\n    LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/OpportunisticContainerAllocator.java",
          "extendedDetails": {}
        }
      ]
    },
    "1597630681c784a3d59f5605b87e96197b8139d7": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5110. Fix OpportunisticContainerAllocator to insert complete HostAddress in issued ContainerTokenIds. (Konstantinos Karanasos via asuresh)\n",
      "commitDate": "18/05/16 6:46 PM",
      "commitName": "1597630681c784a3d59f5605b87e96197b8139d7",
      "commitAuthor": "Arun Suresh",
      "commitDateOld": "24/04/16 10:38 PM",
      "commitNameOld": "c282a08f3892e2e8ceb58e1e9a411062fbd1fb2b",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 23.84,
      "commitsBetweenForRepo": 167,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,37 @@\n   private void allocateOpportunisticContainers(DistSchedulerParams appParams,\n       ContainerIdCounter idCounter, Set\u003cString\u003e blacklist,\n       ApplicationAttemptId id, Map\u003cString, NodeId\u003e allNodes, String userName,\n       Map\u003cResource, List\u003cContainer\u003e\u003e containers, Set\u003cString\u003e nodesAllocated,\n       ResourceRequest anyAsk) throws YarnException {\n     int toAllocate \u003d anyAsk.getNumContainers()\n         - (containers.isEmpty() ?\n         0 : containers.get(anyAsk.getCapability()).size());\n \n     List\u003cString\u003e topKNodesLeft \u003d new ArrayList\u003c\u003e();\n     for (String s : allNodes.keySet()) {\n       // Bias away from whatever we have already allocated and respect blacklist\n       if (nodesAllocated.contains(s) || blacklist.contains(s)) {\n         continue;\n       }\n       topKNodesLeft.add(s);\n     }\n     int numAllocated \u003d 0;\n     int nextNodeToAllocate \u003d 0;\n     for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n       String topNode \u003d topKNodesLeft.get(nextNodeToAllocate);\n       nextNodeToAllocate++;\n       nextNodeToAllocate %\u003d topKNodesLeft.size();\n       NodeId nodeId \u003d allNodes.get(topNode);\n       Container container \u003d buildContainer(appParams, idCounter, anyAsk, id,\n           userName, nodeId);\n       List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n       if (cList \u003d\u003d null) {\n         cList \u003d new ArrayList\u003c\u003e();\n         containers.put(anyAsk.getCapability(), cList);\n       }\n       cList.add(container);\n       numAllocated++;\n-      LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n+      LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n     }\n+    LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateOpportunisticContainers(DistSchedulerParams appParams,\n      ContainerIdCounter idCounter, Set\u003cString\u003e blacklist,\n      ApplicationAttemptId id, Map\u003cString, NodeId\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cContainer\u003e\u003e containers, Set\u003cString\u003e nodesAllocated,\n      ResourceRequest anyAsk) throws YarnException {\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (containers.isEmpty() ?\n        0 : containers.get(anyAsk.getCapability()).size());\n\n    List\u003cString\u003e topKNodesLeft \u003d new ArrayList\u003c\u003e();\n    for (String s : allNodes.keySet()) {\n      // Bias away from whatever we have already allocated and respect blacklist\n      if (nodesAllocated.contains(s) || blacklist.contains(s)) {\n        continue;\n      }\n      topKNodesLeft.add(s);\n    }\n    int numAllocated \u003d 0;\n    int nextNodeToAllocate \u003d 0;\n    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n      String topNode \u003d topKNodesLeft.get(nextNodeToAllocate);\n      nextNodeToAllocate++;\n      nextNodeToAllocate %\u003d topKNodesLeft.size();\n      NodeId nodeId \u003d allNodes.get(topNode);\n      Container container \u003d buildContainer(appParams, idCounter, anyAsk, id,\n          userName, nodeId);\n      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n      if (cList \u003d\u003d null) {\n        cList \u003d new ArrayList\u003c\u003e();\n        containers.put(anyAsk.getCapability(), cList);\n      }\n      cList.add(container);\n      numAllocated++;\n      LOG.info(\"Allocated [\" + container.getId() + \"] as opportunistic.\");\n    }\n    LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/OpportunisticContainerAllocator.java",
      "extendedDetails": {}
    },
    "c282a08f3892e2e8ceb58e1e9a411062fbd1fb2b": {
      "type": "Yintroduced",
      "commitMessage": "YARN-2885. Create AMRMProxy request interceptor and ContainerAllocator to distribute OPPORTUNISTIC containers to appropriate Nodes (asuresh)\n\n(cherry picked from commit 2bf025278a318b0452fdc9ece4427b4c42124e39)\n",
      "commitDate": "24/04/16 10:38 PM",
      "commitName": "c282a08f3892e2e8ceb58e1e9a411062fbd1fb2b",
      "commitAuthor": "Arun Suresh",
      "diff": "@@ -0,0 +1,36 @@\n+  private void allocateOpportunisticContainers(DistSchedulerParams appParams,\n+      ContainerIdCounter idCounter, Set\u003cString\u003e blacklist,\n+      ApplicationAttemptId id, Map\u003cString, NodeId\u003e allNodes, String userName,\n+      Map\u003cResource, List\u003cContainer\u003e\u003e containers, Set\u003cString\u003e nodesAllocated,\n+      ResourceRequest anyAsk) throws YarnException {\n+    int toAllocate \u003d anyAsk.getNumContainers()\n+        - (containers.isEmpty() ?\n+        0 : containers.get(anyAsk.getCapability()).size());\n+\n+    List\u003cString\u003e topKNodesLeft \u003d new ArrayList\u003c\u003e();\n+    for (String s : allNodes.keySet()) {\n+      // Bias away from whatever we have already allocated and respect blacklist\n+      if (nodesAllocated.contains(s) || blacklist.contains(s)) {\n+        continue;\n+      }\n+      topKNodesLeft.add(s);\n+    }\n+    int numAllocated \u003d 0;\n+    int nextNodeToAllocate \u003d 0;\n+    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n+      String topNode \u003d topKNodesLeft.get(nextNodeToAllocate);\n+      nextNodeToAllocate++;\n+      nextNodeToAllocate %\u003d topKNodesLeft.size();\n+      NodeId nodeId \u003d allNodes.get(topNode);\n+      Container container \u003d buildContainer(appParams, idCounter, anyAsk, id,\n+          userName, nodeId);\n+      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n+      if (cList \u003d\u003d null) {\n+        cList \u003d new ArrayList\u003c\u003e();\n+        containers.put(anyAsk.getCapability(), cList);\n+      }\n+      cList.add(container);\n+      numAllocated++;\n+      LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void allocateOpportunisticContainers(DistSchedulerParams appParams,\n      ContainerIdCounter idCounter, Set\u003cString\u003e blacklist,\n      ApplicationAttemptId id, Map\u003cString, NodeId\u003e allNodes, String userName,\n      Map\u003cResource, List\u003cContainer\u003e\u003e containers, Set\u003cString\u003e nodesAllocated,\n      ResourceRequest anyAsk) throws YarnException {\n    int toAllocate \u003d anyAsk.getNumContainers()\n        - (containers.isEmpty() ?\n        0 : containers.get(anyAsk.getCapability()).size());\n\n    List\u003cString\u003e topKNodesLeft \u003d new ArrayList\u003c\u003e();\n    for (String s : allNodes.keySet()) {\n      // Bias away from whatever we have already allocated and respect blacklist\n      if (nodesAllocated.contains(s) || blacklist.contains(s)) {\n        continue;\n      }\n      topKNodesLeft.add(s);\n    }\n    int numAllocated \u003d 0;\n    int nextNodeToAllocate \u003d 0;\n    for (int numCont \u003d 0; numCont \u003c toAllocate; numCont++) {\n      String topNode \u003d topKNodesLeft.get(nextNodeToAllocate);\n      nextNodeToAllocate++;\n      nextNodeToAllocate %\u003d topKNodesLeft.size();\n      NodeId nodeId \u003d allNodes.get(topNode);\n      Container container \u003d buildContainer(appParams, idCounter, anyAsk, id,\n          userName, nodeId);\n      List\u003cContainer\u003e cList \u003d containers.get(anyAsk.getCapability());\n      if (cList \u003d\u003d null) {\n        cList \u003d new ArrayList\u003c\u003e();\n        containers.put(anyAsk.getCapability(), cList);\n      }\n      cList.add(container);\n      numAllocated++;\n      LOG.info(\"Allocated \" + numAllocated + \" opportunistic containers.\");\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/OpportunisticContainerAllocator.java"
    }
  }
}