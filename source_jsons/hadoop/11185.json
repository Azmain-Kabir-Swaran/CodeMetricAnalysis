{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "WebHdfsHandler.java",
  "functionName": "onGetFileChecksum",
  "functionId": "onGetFileChecksum___ctx-ChannelHandlerContext",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java",
  "functionStartLine": 288,
  "functionEndLine": 308,
  "numCommitsSeen": 19,
  "timeTaken": 2557,
  "changeHistory": [
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
    "a5fb298e56220a35d61b8d2bda716d8fb8ef8bb7",
    "bf74dbf80dc9379d669779a598950908adffb8a7",
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
    "bf8e4332cb4c33d0287ae6ecca61b335402ac1c4"
  ],
  "changeHistoryShort": {
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": "Ybodychange",
    "a5fb298e56220a35d61b8d2bda716d8fb8ef8bb7": "Ybodychange",
    "bf74dbf80dc9379d669779a598950908adffb8a7": "Ybodychange",
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": "Ybodychange",
    "bf8e4332cb4c33d0287ae6ecca61b335402ac1c4": "Yintroduced"
  },
  "changeHistoryDetails": {
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13695. Move logging to slf4j in HDFS package. Contributed by Ian Pickering.\n",
      "commitDate": "06/09/18 2:48 PM",
      "commitName": "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "07/03/18 12:33 PM",
      "commitNameOld": "46d29e3d7ee8dc9bb1818b886d9cc5336b1d67a4",
      "commitAuthorOld": "Sean Mackrory",
      "daysBetweenCommits": 183.05,
      "commitsBetweenForRepo": 1941,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   private void onGetFileChecksum(ChannelHandlerContext ctx) throws IOException {\n     MD5MD5CRC32FileChecksum checksum \u003d null;\n     final String nnId \u003d params.namenodeId();\n     DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n     try {\n       checksum \u003d dfsclient.getFileChecksum(path, Long.MAX_VALUE);\n       dfsclient.close();\n       dfsclient \u003d null;\n     } finally {\n-      IOUtils.cleanup(LOG, dfsclient);\n+      IOUtils.cleanupWithLogger(LOG, dfsclient);\n     }\n     final byte[] js \u003d\n         JsonUtil.toJsonString(checksum).getBytes(StandardCharsets.UTF_8);\n     resp \u003d\n       new DefaultFullHttpResponse(HTTP_1_1, OK, Unpooled.wrappedBuffer(js));\n \n     resp.headers().set(CONTENT_TYPE, APPLICATION_JSON_UTF8);\n     resp.headers().set(CONTENT_LENGTH, js.length);\n     resp.headers().set(CONNECTION, CLOSE);\n     ctx.writeAndFlush(resp).addListener(ChannelFutureListener.CLOSE);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void onGetFileChecksum(ChannelHandlerContext ctx) throws IOException {\n    MD5MD5CRC32FileChecksum checksum \u003d null;\n    final String nnId \u003d params.namenodeId();\n    DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n    try {\n      checksum \u003d dfsclient.getFileChecksum(path, Long.MAX_VALUE);\n      dfsclient.close();\n      dfsclient \u003d null;\n    } finally {\n      IOUtils.cleanupWithLogger(LOG, dfsclient);\n    }\n    final byte[] js \u003d\n        JsonUtil.toJsonString(checksum).getBytes(StandardCharsets.UTF_8);\n    resp \u003d\n      new DefaultFullHttpResponse(HTTP_1_1, OK, Unpooled.wrappedBuffer(js));\n\n    resp.headers().set(CONTENT_TYPE, APPLICATION_JSON_UTF8);\n    resp.headers().set(CONTENT_LENGTH, js.length);\n    resp.headers().set(CONNECTION, CLOSE);\n    ctx.writeAndFlush(resp).addListener(ChannelFutureListener.CLOSE);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java",
      "extendedDetails": {}
    },
    "a5fb298e56220a35d61b8d2bda716d8fb8ef8bb7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10707. Replace org.apache.commons.io.Charsets with java.nio.charset.StandardCharsets. Contributed by Vincent Poon.\n",
      "commitDate": "02/08/16 1:07 AM",
      "commitName": "a5fb298e56220a35d61b8d2bda716d8fb8ef8bb7",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "24/06/16 2:44 PM",
      "commitNameOld": "bf74dbf80dc9379d669779a598950908adffb8a7",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 38.43,
      "commitsBetweenForRepo": 327,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,21 @@\n   private void onGetFileChecksum(ChannelHandlerContext ctx) throws IOException {\n     MD5MD5CRC32FileChecksum checksum \u003d null;\n     final String nnId \u003d params.namenodeId();\n     DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n     try {\n       checksum \u003d dfsclient.getFileChecksum(path, Long.MAX_VALUE);\n       dfsclient.close();\n       dfsclient \u003d null;\n     } finally {\n       IOUtils.cleanup(LOG, dfsclient);\n     }\n-    final byte[] js \u003d JsonUtil.toJsonString(checksum).getBytes(Charsets.UTF_8);\n+    final byte[] js \u003d\n+        JsonUtil.toJsonString(checksum).getBytes(StandardCharsets.UTF_8);\n     resp \u003d\n       new DefaultFullHttpResponse(HTTP_1_1, OK, Unpooled.wrappedBuffer(js));\n \n     resp.headers().set(CONTENT_TYPE, APPLICATION_JSON_UTF8);\n     resp.headers().set(CONTENT_LENGTH, js.length);\n     resp.headers().set(CONNECTION, CLOSE);\n     ctx.writeAndFlush(resp).addListener(ChannelFutureListener.CLOSE);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void onGetFileChecksum(ChannelHandlerContext ctx) throws IOException {\n    MD5MD5CRC32FileChecksum checksum \u003d null;\n    final String nnId \u003d params.namenodeId();\n    DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n    try {\n      checksum \u003d dfsclient.getFileChecksum(path, Long.MAX_VALUE);\n      dfsclient.close();\n      dfsclient \u003d null;\n    } finally {\n      IOUtils.cleanup(LOG, dfsclient);\n    }\n    final byte[] js \u003d\n        JsonUtil.toJsonString(checksum).getBytes(StandardCharsets.UTF_8);\n    resp \u003d\n      new DefaultFullHttpResponse(HTTP_1_1, OK, Unpooled.wrappedBuffer(js));\n\n    resp.headers().set(CONTENT_TYPE, APPLICATION_JSON_UTF8);\n    resp.headers().set(CONTENT_LENGTH, js.length);\n    resp.headers().set(CONNECTION, CLOSE);\n    ctx.writeAndFlush(resp).addListener(ChannelFutureListener.CLOSE);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java",
      "extendedDetails": {}
    },
    "bf74dbf80dc9379d669779a598950908adffb8a7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7959. WebHdfs logging is missing on Datanode (Kihwal Lee via sjlee)\n",
      "commitDate": "24/06/16 2:44 PM",
      "commitName": "bf74dbf80dc9379d669779a598950908adffb8a7",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "23/05/16 3:52 PM",
      "commitNameOld": "4b0f55b6ea1665e2118fd573f72a6fcd1fce20d6",
      "commitAuthorOld": "Allen Wittenauer",
      "daysBetweenCommits": 31.95,
      "commitsBetweenForRepo": 255,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n   private void onGetFileChecksum(ChannelHandlerContext ctx) throws IOException {\n     MD5MD5CRC32FileChecksum checksum \u003d null;\n     final String nnId \u003d params.namenodeId();\n     DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n     try {\n       checksum \u003d dfsclient.getFileChecksum(path, Long.MAX_VALUE);\n       dfsclient.close();\n       dfsclient \u003d null;\n     } finally {\n       IOUtils.cleanup(LOG, dfsclient);\n     }\n     final byte[] js \u003d JsonUtil.toJsonString(checksum).getBytes(Charsets.UTF_8);\n-    DefaultFullHttpResponse resp \u003d\n+    resp \u003d\n       new DefaultFullHttpResponse(HTTP_1_1, OK, Unpooled.wrappedBuffer(js));\n \n     resp.headers().set(CONTENT_TYPE, APPLICATION_JSON_UTF8);\n     resp.headers().set(CONTENT_LENGTH, js.length);\n     resp.headers().set(CONNECTION, CLOSE);\n     ctx.writeAndFlush(resp).addListener(ChannelFutureListener.CLOSE);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void onGetFileChecksum(ChannelHandlerContext ctx) throws IOException {\n    MD5MD5CRC32FileChecksum checksum \u003d null;\n    final String nnId \u003d params.namenodeId();\n    DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n    try {\n      checksum \u003d dfsclient.getFileChecksum(path, Long.MAX_VALUE);\n      dfsclient.close();\n      dfsclient \u003d null;\n    } finally {\n      IOUtils.cleanup(LOG, dfsclient);\n    }\n    final byte[] js \u003d JsonUtil.toJsonString(checksum).getBytes(Charsets.UTF_8);\n    resp \u003d\n      new DefaultFullHttpResponse(HTTP_1_1, OK, Unpooled.wrappedBuffer(js));\n\n    resp.headers().set(CONTENT_TYPE, APPLICATION_JSON_UTF8);\n    resp.headers().set(CONTENT_LENGTH, js.length);\n    resp.headers().set(CONNECTION, CLOSE);\n    ctx.writeAndFlush(resp).addListener(ChannelFutureListener.CLOSE);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java",
      "extendedDetails": {}
    },
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7515. Fix new findbugs warnings in hadoop-hdfs. Contributed by Haohui Mai.\n",
      "commitDate": "11/12/14 12:36 PM",
      "commitName": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "17/11/14 11:42 AM",
      "commitNameOld": "bf8e4332cb4c33d0287ae6ecca61b335402ac1c4",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 24.04,
      "commitsBetweenForRepo": 164,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n   private void onGetFileChecksum(ChannelHandlerContext ctx) throws IOException {\n     MD5MD5CRC32FileChecksum checksum \u003d null;\n     final String nnId \u003d params.namenodeId();\n     DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n     try {\n       checksum \u003d dfsclient.getFileChecksum(path, Long.MAX_VALUE);\n       dfsclient.close();\n       dfsclient \u003d null;\n     } finally {\n       IOUtils.cleanup(LOG, dfsclient);\n     }\n-    final byte[] js \u003d JsonUtil.toJsonString(checksum).getBytes();\n+    final byte[] js \u003d JsonUtil.toJsonString(checksum).getBytes(Charsets.UTF_8);\n     DefaultFullHttpResponse resp \u003d\n       new DefaultFullHttpResponse(HTTP_1_1, OK, Unpooled.wrappedBuffer(js));\n \n-    resp.headers().set(CONTENT_TYPE, APPLICATION_JSON);\n+    resp.headers().set(CONTENT_TYPE, APPLICATION_JSON_UTF8);\n     resp.headers().set(CONTENT_LENGTH, js.length);\n     resp.headers().set(CONNECTION, CLOSE);\n     ctx.writeAndFlush(resp).addListener(ChannelFutureListener.CLOSE);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void onGetFileChecksum(ChannelHandlerContext ctx) throws IOException {\n    MD5MD5CRC32FileChecksum checksum \u003d null;\n    final String nnId \u003d params.namenodeId();\n    DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n    try {\n      checksum \u003d dfsclient.getFileChecksum(path, Long.MAX_VALUE);\n      dfsclient.close();\n      dfsclient \u003d null;\n    } finally {\n      IOUtils.cleanup(LOG, dfsclient);\n    }\n    final byte[] js \u003d JsonUtil.toJsonString(checksum).getBytes(Charsets.UTF_8);\n    DefaultFullHttpResponse resp \u003d\n      new DefaultFullHttpResponse(HTTP_1_1, OK, Unpooled.wrappedBuffer(js));\n\n    resp.headers().set(CONTENT_TYPE, APPLICATION_JSON_UTF8);\n    resp.headers().set(CONTENT_LENGTH, js.length);\n    resp.headers().set(CONNECTION, CLOSE);\n    ctx.writeAndFlush(resp).addListener(ChannelFutureListener.CLOSE);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java",
      "extendedDetails": {}
    },
    "bf8e4332cb4c33d0287ae6ecca61b335402ac1c4": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7279. Use netty to implement DatanodeWebHdfsMethods. Contributed by Haohui Mai.\n",
      "commitDate": "17/11/14 11:42 AM",
      "commitName": "bf8e4332cb4c33d0287ae6ecca61b335402ac1c4",
      "commitAuthor": "Haohui Mai",
      "diff": "@@ -0,0 +1,20 @@\n+  private void onGetFileChecksum(ChannelHandlerContext ctx) throws IOException {\n+    MD5MD5CRC32FileChecksum checksum \u003d null;\n+    final String nnId \u003d params.namenodeId();\n+    DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n+    try {\n+      checksum \u003d dfsclient.getFileChecksum(path, Long.MAX_VALUE);\n+      dfsclient.close();\n+      dfsclient \u003d null;\n+    } finally {\n+      IOUtils.cleanup(LOG, dfsclient);\n+    }\n+    final byte[] js \u003d JsonUtil.toJsonString(checksum).getBytes();\n+    DefaultFullHttpResponse resp \u003d\n+      new DefaultFullHttpResponse(HTTP_1_1, OK, Unpooled.wrappedBuffer(js));\n+\n+    resp.headers().set(CONTENT_TYPE, APPLICATION_JSON);\n+    resp.headers().set(CONTENT_LENGTH, js.length);\n+    resp.headers().set(CONNECTION, CLOSE);\n+    ctx.writeAndFlush(resp).addListener(ChannelFutureListener.CLOSE);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void onGetFileChecksum(ChannelHandlerContext ctx) throws IOException {\n    MD5MD5CRC32FileChecksum checksum \u003d null;\n    final String nnId \u003d params.namenodeId();\n    DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n    try {\n      checksum \u003d dfsclient.getFileChecksum(path, Long.MAX_VALUE);\n      dfsclient.close();\n      dfsclient \u003d null;\n    } finally {\n      IOUtils.cleanup(LOG, dfsclient);\n    }\n    final byte[] js \u003d JsonUtil.toJsonString(checksum).getBytes();\n    DefaultFullHttpResponse resp \u003d\n      new DefaultFullHttpResponse(HTTP_1_1, OK, Unpooled.wrappedBuffer(js));\n\n    resp.headers().set(CONTENT_TYPE, APPLICATION_JSON);\n    resp.headers().set(CONTENT_LENGTH, js.length);\n    resp.headers().set(CONNECTION, CLOSE);\n    ctx.writeAndFlush(resp).addListener(ChannelFutureListener.CLOSE);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java"
    }
  }
}