{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "GenerateDNBlockInfosReducer.java",
  "functionName": "reduce",
  "functionId": "reduce___key-IntWritable__values-Iterable__BlockInfo____context-Context",
  "sourceFilePath": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-blockgen/src/main/java/org/apache/hadoop/tools/dynamometer/blockgenerator/GenerateDNBlockInfosReducer.java",
  "functionStartLine": 64,
  "functionEndLine": 98,
  "numCommitsSeen": 2,
  "timeTaken": 824,
  "changeHistory": [
    "fc0656dd300f037cb8f97a4c1fac4bf942af3d0a",
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91"
  ],
  "changeHistoryShort": {
    "fc0656dd300f037cb8f97a4c1fac4bf942af3d0a": "Ybodychange",
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91": "Yintroduced"
  },
  "changeHistoryDetails": {
    "fc0656dd300f037cb8f97a4c1fac4bf942af3d0a": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16418. [Dynamometer] Fix checkstyle and findbugs warnings. Contributed by Erik Krogen.\n",
      "commitDate": "11/07/19 8:29 AM",
      "commitName": "fc0656dd300f037cb8f97a4c1fac4bf942af3d0a",
      "commitAuthor": "Erik Krogen",
      "commitDateOld": "25/06/19 8:07 AM",
      "commitNameOld": "ab0b180ddb5d0775a2452d5eeb7badd252aadb91",
      "commitAuthorOld": "Erik Krogen",
      "daysBetweenCommits": 16.02,
      "commitsBetweenForRepo": 74,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,35 @@\n   public void reduce(IntWritable key, Iterable\u003cBlockInfo\u003e values,\n       Context context) throws IOException, InterruptedException {\n     long blockIndex \u003d 0;\n     int datanodeId \u003d key.get();\n     String dnFile \u003d \"dn\" + datanodeId + \"-a-\"\n         + context.getTaskAttemptID().getId();\n     Iterator\u003cBlockInfo\u003e it \u003d values.iterator();\n     long startTimestamp \u003d System.currentTimeMillis();\n     long endTimestamp;\n \n     Path baseOutputPath \u003d FileOutputFormat.getOutputPath(context);\n     String fullPath \u003d new Path(baseOutputPath, dnFile).toString();\n \n     Text out \u003d new Text();\n     while (it.hasNext()) {\n-      BlockInfo blockInfo \u003d new BlockInfo(it.next());\n+      BlockInfo blockInfo \u003d it.next();\n       String blockLine \u003d blockInfo.getBlockId() + \",\"\n           + blockInfo.getBlockGenerationStamp() + \",\" + blockInfo.getSize();\n       out.set(blockLine);\n       multiOutputs.write(NullWritable.get(), out, fullPath);\n \n       blockIndex++;\n \n       // Report progress for every 1000 blocks\n       if (blockIndex % 1000 \u003d\u003d 0) {\n         context.progress();\n         endTimestamp \u003d System.currentTimeMillis();\n         System.out.println(\"Time taken to process 1000 records in ms:\"\n             + (endTimestamp - startTimestamp));\n         startTimestamp \u003d endTimestamp;\n       }\n     }\n \n     LOG.info(\"Number of blocks processed:\" + blockIndex);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void reduce(IntWritable key, Iterable\u003cBlockInfo\u003e values,\n      Context context) throws IOException, InterruptedException {\n    long blockIndex \u003d 0;\n    int datanodeId \u003d key.get();\n    String dnFile \u003d \"dn\" + datanodeId + \"-a-\"\n        + context.getTaskAttemptID().getId();\n    Iterator\u003cBlockInfo\u003e it \u003d values.iterator();\n    long startTimestamp \u003d System.currentTimeMillis();\n    long endTimestamp;\n\n    Path baseOutputPath \u003d FileOutputFormat.getOutputPath(context);\n    String fullPath \u003d new Path(baseOutputPath, dnFile).toString();\n\n    Text out \u003d new Text();\n    while (it.hasNext()) {\n      BlockInfo blockInfo \u003d it.next();\n      String blockLine \u003d blockInfo.getBlockId() + \",\"\n          + blockInfo.getBlockGenerationStamp() + \",\" + blockInfo.getSize();\n      out.set(blockLine);\n      multiOutputs.write(NullWritable.get(), out, fullPath);\n\n      blockIndex++;\n\n      // Report progress for every 1000 blocks\n      if (blockIndex % 1000 \u003d\u003d 0) {\n        context.progress();\n        endTimestamp \u003d System.currentTimeMillis();\n        System.out.println(\"Time taken to process 1000 records in ms:\"\n            + (endTimestamp - startTimestamp));\n        startTimestamp \u003d endTimestamp;\n      }\n    }\n\n    LOG.info(\"Number of blocks processed:\" + blockIndex);\n  }",
      "path": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-blockgen/src/main/java/org/apache/hadoop/tools/dynamometer/blockgenerator/GenerateDNBlockInfosReducer.java",
      "extendedDetails": {}
    },
    "ab0b180ddb5d0775a2452d5eeb7badd252aadb91": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-12345 Add Dynamometer to hadoop-tools, a tool for scale testing the HDFS NameNode with real metadata and workloads. Contributed by Erik Krogen.\n",
      "commitDate": "25/06/19 8:07 AM",
      "commitName": "ab0b180ddb5d0775a2452d5eeb7badd252aadb91",
      "commitAuthor": "Erik Krogen",
      "diff": "@@ -0,0 +1,35 @@\n+  public void reduce(IntWritable key, Iterable\u003cBlockInfo\u003e values,\n+      Context context) throws IOException, InterruptedException {\n+    long blockIndex \u003d 0;\n+    int datanodeId \u003d key.get();\n+    String dnFile \u003d \"dn\" + datanodeId + \"-a-\"\n+        + context.getTaskAttemptID().getId();\n+    Iterator\u003cBlockInfo\u003e it \u003d values.iterator();\n+    long startTimestamp \u003d System.currentTimeMillis();\n+    long endTimestamp;\n+\n+    Path baseOutputPath \u003d FileOutputFormat.getOutputPath(context);\n+    String fullPath \u003d new Path(baseOutputPath, dnFile).toString();\n+\n+    Text out \u003d new Text();\n+    while (it.hasNext()) {\n+      BlockInfo blockInfo \u003d new BlockInfo(it.next());\n+      String blockLine \u003d blockInfo.getBlockId() + \",\"\n+          + blockInfo.getBlockGenerationStamp() + \",\" + blockInfo.getSize();\n+      out.set(blockLine);\n+      multiOutputs.write(NullWritable.get(), out, fullPath);\n+\n+      blockIndex++;\n+\n+      // Report progress for every 1000 blocks\n+      if (blockIndex % 1000 \u003d\u003d 0) {\n+        context.progress();\n+        endTimestamp \u003d System.currentTimeMillis();\n+        System.out.println(\"Time taken to process 1000 records in ms:\"\n+            + (endTimestamp - startTimestamp));\n+        startTimestamp \u003d endTimestamp;\n+      }\n+    }\n+\n+    LOG.info(\"Number of blocks processed:\" + blockIndex);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void reduce(IntWritable key, Iterable\u003cBlockInfo\u003e values,\n      Context context) throws IOException, InterruptedException {\n    long blockIndex \u003d 0;\n    int datanodeId \u003d key.get();\n    String dnFile \u003d \"dn\" + datanodeId + \"-a-\"\n        + context.getTaskAttemptID().getId();\n    Iterator\u003cBlockInfo\u003e it \u003d values.iterator();\n    long startTimestamp \u003d System.currentTimeMillis();\n    long endTimestamp;\n\n    Path baseOutputPath \u003d FileOutputFormat.getOutputPath(context);\n    String fullPath \u003d new Path(baseOutputPath, dnFile).toString();\n\n    Text out \u003d new Text();\n    while (it.hasNext()) {\n      BlockInfo blockInfo \u003d new BlockInfo(it.next());\n      String blockLine \u003d blockInfo.getBlockId() + \",\"\n          + blockInfo.getBlockGenerationStamp() + \",\" + blockInfo.getSize();\n      out.set(blockLine);\n      multiOutputs.write(NullWritable.get(), out, fullPath);\n\n      blockIndex++;\n\n      // Report progress for every 1000 blocks\n      if (blockIndex % 1000 \u003d\u003d 0) {\n        context.progress();\n        endTimestamp \u003d System.currentTimeMillis();\n        System.out.println(\"Time taken to process 1000 records in ms:\"\n            + (endTimestamp - startTimestamp));\n        startTimestamp \u003d endTimestamp;\n      }\n    }\n\n    LOG.info(\"Number of blocks processed:\" + blockIndex);\n  }",
      "path": "hadoop-tools/hadoop-dynamometer/hadoop-dynamometer-blockgen/src/main/java/org/apache/hadoop/tools/dynamometer/blockgenerator/GenerateDNBlockInfosReducer.java"
    }
  }
}