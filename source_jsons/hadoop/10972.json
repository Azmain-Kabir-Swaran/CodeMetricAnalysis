{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockSender.java",
  "functionName": "sendBlock",
  "functionId": "sendBlock___out-DataOutputStream__baseStream-OutputStream__throttler-DataTransferThrottler",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
  "functionStartLine": 751,
  "functionEndLine": 760,
  "numCommitsSeen": 70,
  "timeTaken": 7580,
  "changeHistory": [
    "c6cafc77e697317dad0708309b67b900a2e3a413",
    "892ade689f9bcce76daae8f66fc00a49bee8548e",
    "7f6ed7fe365166e8075359f1d0ad035fa876c70f",
    "a72fba5853d0537bcdbd7851181129173e440dbb",
    "efea68dc3538de9aafae206d64903506e41fc9e1",
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
    "638801cce16fc1dc3259c541dc30a599faaddda1",
    "91c28d440952bdd50f10ff2d892182b8cd7a8065",
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
    "1c940637b14eee777a65d153d0d712a1aea3866c",
    "6b0963c53be360b710614b9f44a29c4171af6b83",
    "e90a5b40430cc1fbce075d34b31e3cc05fd9831f",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "c6cafc77e697317dad0708309b67b900a2e3a413": "Ybodychange",
    "892ade689f9bcce76daae8f66fc00a49bee8548e": "Ybodychange",
    "7f6ed7fe365166e8075359f1d0ad035fa876c70f": "Ybodychange",
    "a72fba5853d0537bcdbd7851181129173e440dbb": "Ybodychange",
    "efea68dc3538de9aafae206d64903506e41fc9e1": "Ybodychange",
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24": "Ybodychange",
    "638801cce16fc1dc3259c541dc30a599faaddda1": "Ybodychange",
    "91c28d440952bdd50f10ff2d892182b8cd7a8065": "Ybodychange",
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": "Ybodychange",
    "1c940637b14eee777a65d153d0d712a1aea3866c": "Ybodychange",
    "6b0963c53be360b710614b9f44a29c4171af6b83": "Ybodychange",
    "e90a5b40430cc1fbce075d34b31e3cc05fd9831f": "Ybodychange",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c6cafc77e697317dad0708309b67b900a2e3a413": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9185. Fix null tracer in ErasureCodingWorker. Contributed by Rakesh R.\n",
      "commitDate": "02/10/15 11:08 AM",
      "commitName": "c6cafc77e697317dad0708309b67b900a2e3a413",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 4.14,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,10 @@\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n-    TraceScope scope \u003d datanode.tracer.\n+    final TraceScope scope \u003d datanode.getTracer().\n         newScope(\"sendBlock_\" + block.getBlockId());\n     try {\n       return doSendBlock(out, baseStream, throttler);\n     } finally {\n       scope.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    final TraceScope scope \u003d datanode.getTracer().\n        newScope(\"sendBlock_\" + block.getBlockId());\n    try {\n      return doSendBlock(out, baseStream, throttler);\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "892ade689f9bcce76daae8f66fc00a49bee8548e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
      "commitDate": "28/09/15 7:42 AM",
      "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "26/09/15 11:08 AM",
      "commitNameOld": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.86,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,10 +1,10 @@\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n-    TraceScope scope \u003d\n-        Trace.startSpan(\"sendBlock_\" + block.getBlockId(), Sampler.NEVER);\n+    TraceScope scope \u003d datanode.tracer.\n+        newScope(\"sendBlock_\" + block.getBlockId());\n     try {\n       return doSendBlock(out, baseStream, throttler);\n     } finally {\n       scope.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    TraceScope scope \u003d datanode.tracer.\n        newScope(\"sendBlock_\" + block.getBlockId());\n    try {\n      return doSendBlock(out, baseStream, throttler);\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "7f6ed7fe365166e8075359f1d0ad035fa876c70f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7055. Add tracing to DFSInputStream (cmccabe)\n",
      "commitDate": "03/10/14 1:35 PM",
      "commitName": "7f6ed7fe365166e8075359f1d0ad035fa876c70f",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "12/08/14 2:49 PM",
      "commitNameOld": "a72fba5853d0537bcdbd7851181129173e440dbb",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 51.95,
      "commitsBetweenForRepo": 556,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,10 @@\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n-    if (out \u003d\u003d null) {\n-      throw new IOException( \"out stream is null\" );\n-    }\n-    initialOffset \u003d offset;\n-    long totalRead \u003d 0;\n-    OutputStream streamForSendChunks \u003d out;\n-    \n-    lastCacheDropOffset \u003d initialOffset;\n-\n-    if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n-      // Advise that this file descriptor will be accessed sequentially.\n-      NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n-          block.getBlockName(), blockInFd, 0, 0,\n-          NativeIO.POSIX.POSIX_FADV_SEQUENTIAL);\n-    }\n-    \n-    // Trigger readahead of beginning of file if configured.\n-    manageOsCache();\n-\n-    final long startTime \u003d ClientTraceLog.isDebugEnabled() ? System.nanoTime() : 0;\n+    TraceScope scope \u003d\n+        Trace.startSpan(\"sendBlock_\" + block.getBlockId(), Sampler.NEVER);\n     try {\n-      int maxChunksPerPacket;\n-      int pktBufSize \u003d PacketHeader.PKT_MAX_HEADER_LEN;\n-      boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n-          \u0026\u0026 baseStream instanceof SocketOutputStream\n-          \u0026\u0026 blockIn instanceof FileInputStream;\n-      if (transferTo) {\n-        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n-        blockInPosition \u003d fileChannel.position();\n-        streamForSendChunks \u003d baseStream;\n-        maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n-        \n-        // Smaller packet size to only hold checksum when doing transferTo\n-        pktBufSize +\u003d checksumSize * maxChunksPerPacket;\n-      } else {\n-        maxChunksPerPacket \u003d Math.max(1,\n-            numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n-        // Packet size includes both checksum and data\n-        pktBufSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n-      }\n-\n-      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktBufSize);\n-\n-      while (endOffset \u003e offset \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n-        manageOsCache();\n-        long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n-            transferTo, throttler);\n-        offset +\u003d len;\n-        totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n-        seqno++;\n-      }\n-      // If this thread was interrupted, then it did not send the full block.\n-      if (!Thread.currentThread().isInterrupted()) {\n-        try {\n-          // send an empty packet to mark the end of the block\n-          sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n-              throttler);\n-          out.flush();\n-        } catch (IOException e) { //socket error\n-          throw ioeToSocketException(e);\n-        }\n-\n-        sentEntireByteRange \u003d true;\n-      }\n+      return doSendBlock(out, baseStream, throttler);\n     } finally {\n-      if ((clientTraceFmt !\u003d null) \u0026\u0026 ClientTraceLog.isDebugEnabled()) {\n-        final long endTime \u003d System.nanoTime();\n-        ClientTraceLog.debug(String.format(clientTraceFmt, totalRead,\n-            initialOffset, endTime - startTime));\n-      }\n-      close();\n+      scope.close();\n     }\n-    return totalRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    TraceScope scope \u003d\n        Trace.startSpan(\"sendBlock_\" + block.getBlockId(), Sampler.NEVER);\n    try {\n      return doSendBlock(out, baseStream, throttler);\n    } finally {\n      scope.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "a72fba5853d0537bcdbd7851181129173e440dbb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6836. HDFS INFO logging is verbose \u0026 uses file appenders. (Contributed by Xiaoyu Yao)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1617603 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/08/14 2:49 PM",
      "commitName": "a72fba5853d0537bcdbd7851181129173e440dbb",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "23/04/14 1:13 PM",
      "commitNameOld": "876fd8ab7913a259ff9f69c16cc2d9af46ad3f9b",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 111.07,
      "commitsBetweenForRepo": 708,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,76 @@\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n     if (out \u003d\u003d null) {\n       throw new IOException( \"out stream is null\" );\n     }\n     initialOffset \u003d offset;\n     long totalRead \u003d 0;\n     OutputStream streamForSendChunks \u003d out;\n     \n     lastCacheDropOffset \u003d initialOffset;\n \n     if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n       // Advise that this file descriptor will be accessed sequentially.\n       NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n           block.getBlockName(), blockInFd, 0, 0,\n           NativeIO.POSIX.POSIX_FADV_SEQUENTIAL);\n     }\n     \n     // Trigger readahead of beginning of file if configured.\n     manageOsCache();\n \n-    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n+    final long startTime \u003d ClientTraceLog.isDebugEnabled() ? System.nanoTime() : 0;\n     try {\n       int maxChunksPerPacket;\n       int pktBufSize \u003d PacketHeader.PKT_MAX_HEADER_LEN;\n       boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n           \u0026\u0026 baseStream instanceof SocketOutputStream\n           \u0026\u0026 blockIn instanceof FileInputStream;\n       if (transferTo) {\n         FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n         blockInPosition \u003d fileChannel.position();\n         streamForSendChunks \u003d baseStream;\n         maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n         \n         // Smaller packet size to only hold checksum when doing transferTo\n         pktBufSize +\u003d checksumSize * maxChunksPerPacket;\n       } else {\n         maxChunksPerPacket \u003d Math.max(1,\n             numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n         // Packet size includes both checksum and data\n         pktBufSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n       }\n \n       ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktBufSize);\n \n       while (endOffset \u003e offset \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n         manageOsCache();\n         long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n             transferTo, throttler);\n         offset +\u003d len;\n         totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n         seqno++;\n       }\n       // If this thread was interrupted, then it did not send the full block.\n       if (!Thread.currentThread().isInterrupted()) {\n         try {\n           // send an empty packet to mark the end of the block\n           sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n               throttler);\n           out.flush();\n         } catch (IOException e) { //socket error\n           throw ioeToSocketException(e);\n         }\n \n         sentEntireByteRange \u003d true;\n       }\n     } finally {\n-      if (clientTraceFmt !\u003d null) {\n+      if ((clientTraceFmt !\u003d null) \u0026\u0026 ClientTraceLog.isDebugEnabled()) {\n         final long endTime \u003d System.nanoTime();\n-        ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n+        ClientTraceLog.debug(String.format(clientTraceFmt, totalRead,\n             initialOffset, endTime - startTime));\n       }\n       close();\n     }\n     return totalRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if (out \u003d\u003d null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset \u003d offset;\n    long totalRead \u003d 0;\n    OutputStream streamForSendChunks \u003d out;\n    \n    lastCacheDropOffset \u003d initialOffset;\n\n    if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n          block.getBlockName(), blockInFd, 0, 0,\n          NativeIO.POSIX.POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime \u003d ClientTraceLog.isDebugEnabled() ? System.nanoTime() : 0;\n    try {\n      int maxChunksPerPacket;\n      int pktBufSize \u003d PacketHeader.PKT_MAX_HEADER_LEN;\n      boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n          \u0026\u0026 baseStream instanceof SocketOutputStream\n          \u0026\u0026 blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n        blockInPosition \u003d fileChannel.position();\n        streamForSendChunks \u003d baseStream;\n        maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktBufSize +\u003d checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket \u003d Math.max(1,\n            numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktBufSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktBufSize);\n\n      while (endOffset \u003e offset \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n        manageOsCache();\n        long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset +\u003d len;\n        totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      // If this thread was interrupted, then it did not send the full block.\n      if (!Thread.currentThread().isInterrupted()) {\n        try {\n          // send an empty packet to mark the end of the block\n          sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n              throttler);\n          out.flush();\n        } catch (IOException e) { //socket error\n          throw ioeToSocketException(e);\n        }\n\n        sentEntireByteRange \u003d true;\n      }\n    } finally {\n      if ((clientTraceFmt !\u003d null) \u0026\u0026 ClientTraceLog.isDebugEnabled()) {\n        final long endTime \u003d System.nanoTime();\n        ClientTraceLog.debug(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "efea68dc3538de9aafae206d64903506e41fc9e1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5511. improve CacheManipulator interface to allow better unit testing (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1543676 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/11/13 4:48 PM",
      "commitName": "efea68dc3538de9aafae206d64903506e41fc9e1",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "22/07/13 11:15 AM",
      "commitNameOld": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 120.27,
      "commitsBetweenForRepo": 765,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,76 @@\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n     if (out \u003d\u003d null) {\n       throw new IOException( \"out stream is null\" );\n     }\n     initialOffset \u003d offset;\n     long totalRead \u003d 0;\n     OutputStream streamForSendChunks \u003d out;\n     \n     lastCacheDropOffset \u003d initialOffset;\n \n     if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n       // Advise that this file descriptor will be accessed sequentially.\n-      NativeIO.POSIX.posixFadviseIfPossible(block.getBlockName(),\n-          blockInFd, 0, 0, NativeIO.POSIX.POSIX_FADV_SEQUENTIAL);\n+      NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n+          block.getBlockName(), blockInFd, 0, 0,\n+          NativeIO.POSIX.POSIX_FADV_SEQUENTIAL);\n     }\n     \n     // Trigger readahead of beginning of file if configured.\n     manageOsCache();\n \n     final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n     try {\n       int maxChunksPerPacket;\n       int pktBufSize \u003d PacketHeader.PKT_MAX_HEADER_LEN;\n       boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n           \u0026\u0026 baseStream instanceof SocketOutputStream\n           \u0026\u0026 blockIn instanceof FileInputStream;\n       if (transferTo) {\n         FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n         blockInPosition \u003d fileChannel.position();\n         streamForSendChunks \u003d baseStream;\n         maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n         \n         // Smaller packet size to only hold checksum when doing transferTo\n         pktBufSize +\u003d checksumSize * maxChunksPerPacket;\n       } else {\n         maxChunksPerPacket \u003d Math.max(1,\n             numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n         // Packet size includes both checksum and data\n         pktBufSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n       }\n \n       ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktBufSize);\n \n       while (endOffset \u003e offset \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n         manageOsCache();\n         long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n             transferTo, throttler);\n         offset +\u003d len;\n         totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n         seqno++;\n       }\n       // If this thread was interrupted, then it did not send the full block.\n       if (!Thread.currentThread().isInterrupted()) {\n         try {\n           // send an empty packet to mark the end of the block\n           sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n               throttler);\n           out.flush();\n         } catch (IOException e) { //socket error\n           throw ioeToSocketException(e);\n         }\n \n         sentEntireByteRange \u003d true;\n       }\n     } finally {\n       if (clientTraceFmt !\u003d null) {\n         final long endTime \u003d System.nanoTime();\n         ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n             initialOffset, endTime - startTime));\n       }\n       close();\n     }\n     return totalRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if (out \u003d\u003d null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset \u003d offset;\n    long totalRead \u003d 0;\n    OutputStream streamForSendChunks \u003d out;\n    \n    lastCacheDropOffset \u003d initialOffset;\n\n    if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n          block.getBlockName(), blockInFd, 0, 0,\n          NativeIO.POSIX.POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      int maxChunksPerPacket;\n      int pktBufSize \u003d PacketHeader.PKT_MAX_HEADER_LEN;\n      boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n          \u0026\u0026 baseStream instanceof SocketOutputStream\n          \u0026\u0026 blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n        blockInPosition \u003d fileChannel.position();\n        streamForSendChunks \u003d baseStream;\n        maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktBufSize +\u003d checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket \u003d Math.max(1,\n            numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktBufSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktBufSize);\n\n      while (endOffset \u003e offset \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n        manageOsCache();\n        long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset +\u003d len;\n        totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      // If this thread was interrupted, then it did not send the full block.\n      if (!Thread.currentThread().isInterrupted()) {\n        try {\n          // send an empty packet to mark the end of the block\n          sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n              throttler);\n          out.flush();\n        } catch (IOException e) { //socket error\n          throw ioeToSocketException(e);\n        }\n\n        sentEntireByteRange \u003d true;\n      }\n    } finally {\n      if (clientTraceFmt !\u003d null) {\n        final long endTime \u003d System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4817.  Make HDFS advisory caching configurable on a per-file basis.  (Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505753 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/13 11:15 AM",
      "commitName": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "06/03/13 11:15 AM",
      "commitNameOld": "638801cce16fc1dc3259c541dc30a599faaddda1",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 137.96,
      "commitsBetweenForRepo": 857,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,75 @@\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n     if (out \u003d\u003d null) {\n       throw new IOException( \"out stream is null\" );\n     }\n     initialOffset \u003d offset;\n     long totalRead \u003d 0;\n     OutputStream streamForSendChunks \u003d out;\n     \n     lastCacheDropOffset \u003d initialOffset;\n \n     if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n       // Advise that this file descriptor will be accessed sequentially.\n-      NativeIO.POSIX.posixFadviseIfPossible(\n+      NativeIO.POSIX.posixFadviseIfPossible(block.getBlockName(),\n           blockInFd, 0, 0, NativeIO.POSIX.POSIX_FADV_SEQUENTIAL);\n     }\n     \n     // Trigger readahead of beginning of file if configured.\n     manageOsCache();\n \n     final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n     try {\n       int maxChunksPerPacket;\n       int pktBufSize \u003d PacketHeader.PKT_MAX_HEADER_LEN;\n       boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n           \u0026\u0026 baseStream instanceof SocketOutputStream\n           \u0026\u0026 blockIn instanceof FileInputStream;\n       if (transferTo) {\n         FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n         blockInPosition \u003d fileChannel.position();\n         streamForSendChunks \u003d baseStream;\n         maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n         \n         // Smaller packet size to only hold checksum when doing transferTo\n         pktBufSize +\u003d checksumSize * maxChunksPerPacket;\n       } else {\n         maxChunksPerPacket \u003d Math.max(1,\n             numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n         // Packet size includes both checksum and data\n         pktBufSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n       }\n \n       ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktBufSize);\n \n       while (endOffset \u003e offset \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n         manageOsCache();\n         long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n             transferTo, throttler);\n         offset +\u003d len;\n         totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n         seqno++;\n       }\n       // If this thread was interrupted, then it did not send the full block.\n       if (!Thread.currentThread().isInterrupted()) {\n         try {\n           // send an empty packet to mark the end of the block\n           sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n               throttler);\n           out.flush();\n         } catch (IOException e) { //socket error\n           throw ioeToSocketException(e);\n         }\n \n         sentEntireByteRange \u003d true;\n       }\n     } finally {\n       if (clientTraceFmt !\u003d null) {\n         final long endTime \u003d System.nanoTime();\n         ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n             initialOffset, endTime - startTime));\n       }\n       close();\n     }\n     return totalRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if (out \u003d\u003d null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset \u003d offset;\n    long totalRead \u003d 0;\n    OutputStream streamForSendChunks \u003d out;\n    \n    lastCacheDropOffset \u003d initialOffset;\n\n    if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      NativeIO.POSIX.posixFadviseIfPossible(block.getBlockName(),\n          blockInFd, 0, 0, NativeIO.POSIX.POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      int maxChunksPerPacket;\n      int pktBufSize \u003d PacketHeader.PKT_MAX_HEADER_LEN;\n      boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n          \u0026\u0026 baseStream instanceof SocketOutputStream\n          \u0026\u0026 blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n        blockInPosition \u003d fileChannel.position();\n        streamForSendChunks \u003d baseStream;\n        maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktBufSize +\u003d checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket \u003d Math.max(1,\n            numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktBufSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktBufSize);\n\n      while (endOffset \u003e offset \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n        manageOsCache();\n        long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset +\u003d len;\n        totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      // If this thread was interrupted, then it did not send the full block.\n      if (!Thread.currentThread().isInterrupted()) {\n        try {\n          // send an empty packet to mark the end of the block\n          sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n              throttler);\n          out.flush();\n        } catch (IOException e) { //socket error\n          throw ioeToSocketException(e);\n        }\n\n        sentEntireByteRange \u003d true;\n      }\n    } finally {\n      if (clientTraceFmt !\u003d null) {\n        final long endTime \u003d System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "638801cce16fc1dc3259c541dc30a599faaddda1": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-8952. Enhancements to support Hadoop on Windows Server and Windows Azure environments. Contributed by Ivan Mitic, Chuan Liu, Ramya Sunil, Bikas Saha, Kanna Karanam, John Gordon, Brandon Li, Chris Nauroth, David Lao, Sumadhur Reddy Bolli, Arpit Agarwal, Ahmed El Baz, Mike Liddell, Jing Zhao, Thejas Nair, Steve Maine, Ganeshan Iyer, Raja Aluri, Giridharan Kesavan, Ramya Bharathi Nimmagadda.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1453486 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/03/13 11:15 AM",
      "commitName": "638801cce16fc1dc3259c541dc30a599faaddda1",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "14/01/13 12:47 PM",
      "commitNameOld": "3052ad1f0069af5caee621374b29d17d7f12ab51",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 50.94,
      "commitsBetweenForRepo": 214,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,75 @@\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n     if (out \u003d\u003d null) {\n       throw new IOException( \"out stream is null\" );\n     }\n     initialOffset \u003d offset;\n     long totalRead \u003d 0;\n     OutputStream streamForSendChunks \u003d out;\n     \n     lastCacheDropOffset \u003d initialOffset;\n \n     if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n       // Advise that this file descriptor will be accessed sequentially.\n-      NativeIO.posixFadviseIfPossible(blockInFd, 0, 0, NativeIO.POSIX_FADV_SEQUENTIAL);\n+      NativeIO.POSIX.posixFadviseIfPossible(\n+          blockInFd, 0, 0, NativeIO.POSIX.POSIX_FADV_SEQUENTIAL);\n     }\n     \n     // Trigger readahead of beginning of file if configured.\n     manageOsCache();\n \n     final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n     try {\n       int maxChunksPerPacket;\n       int pktBufSize \u003d PacketHeader.PKT_MAX_HEADER_LEN;\n       boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n           \u0026\u0026 baseStream instanceof SocketOutputStream\n           \u0026\u0026 blockIn instanceof FileInputStream;\n       if (transferTo) {\n         FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n         blockInPosition \u003d fileChannel.position();\n         streamForSendChunks \u003d baseStream;\n         maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n         \n         // Smaller packet size to only hold checksum when doing transferTo\n         pktBufSize +\u003d checksumSize * maxChunksPerPacket;\n       } else {\n         maxChunksPerPacket \u003d Math.max(1,\n             numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n         // Packet size includes both checksum and data\n         pktBufSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n       }\n \n       ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktBufSize);\n \n       while (endOffset \u003e offset \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n         manageOsCache();\n         long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n             transferTo, throttler);\n         offset +\u003d len;\n         totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n         seqno++;\n       }\n       // If this thread was interrupted, then it did not send the full block.\n       if (!Thread.currentThread().isInterrupted()) {\n         try {\n           // send an empty packet to mark the end of the block\n           sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n               throttler);\n           out.flush();\n         } catch (IOException e) { //socket error\n           throw ioeToSocketException(e);\n         }\n \n         sentEntireByteRange \u003d true;\n       }\n     } finally {\n       if (clientTraceFmt !\u003d null) {\n         final long endTime \u003d System.nanoTime();\n         ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n             initialOffset, endTime - startTime));\n       }\n       close();\n     }\n     return totalRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if (out \u003d\u003d null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset \u003d offset;\n    long totalRead \u003d 0;\n    OutputStream streamForSendChunks \u003d out;\n    \n    lastCacheDropOffset \u003d initialOffset;\n\n    if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      NativeIO.POSIX.posixFadviseIfPossible(\n          blockInFd, 0, 0, NativeIO.POSIX.POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      int maxChunksPerPacket;\n      int pktBufSize \u003d PacketHeader.PKT_MAX_HEADER_LEN;\n      boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n          \u0026\u0026 baseStream instanceof SocketOutputStream\n          \u0026\u0026 blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n        blockInPosition \u003d fileChannel.position();\n        streamForSendChunks \u003d baseStream;\n        maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktBufSize +\u003d checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket \u003d Math.max(1,\n            numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktBufSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktBufSize);\n\n      while (endOffset \u003e offset \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n        manageOsCache();\n        long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset +\u003d len;\n        totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      // If this thread was interrupted, then it did not send the full block.\n      if (!Thread.currentThread().isInterrupted()) {\n        try {\n          // send an empty packet to mark the end of the block\n          sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n              throttler);\n          out.flush();\n        } catch (IOException e) { //socket error\n          throw ioeToSocketException(e);\n        }\n\n        sentEntireByteRange \u003d true;\n      }\n    } finally {\n      if (clientTraceFmt !\u003d null) {\n        final long endTime \u003d System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "91c28d440952bdd50f10ff2d892182b8cd7a8065": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4328. TestLargeBlock#testLargeBlockSize is timing out. Contributed by Chris Nauroth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1431867 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/01/13 7:44 PM",
      "commitName": "91c28d440952bdd50f10ff2d892182b8cd7a8065",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "28/10/12 4:10 PM",
      "commitNameOld": "cea7bbc630deede93dbe6a1bbda56ad49de4f3de",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 74.19,
      "commitsBetweenForRepo": 327,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,74 @@\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n     if (out \u003d\u003d null) {\n       throw new IOException( \"out stream is null\" );\n     }\n     initialOffset \u003d offset;\n     long totalRead \u003d 0;\n     OutputStream streamForSendChunks \u003d out;\n     \n     lastCacheDropOffset \u003d initialOffset;\n \n     if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n       // Advise that this file descriptor will be accessed sequentially.\n       NativeIO.posixFadviseIfPossible(blockInFd, 0, 0, NativeIO.POSIX_FADV_SEQUENTIAL);\n     }\n     \n     // Trigger readahead of beginning of file if configured.\n     manageOsCache();\n \n     final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n     try {\n       int maxChunksPerPacket;\n       int pktBufSize \u003d PacketHeader.PKT_MAX_HEADER_LEN;\n       boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n           \u0026\u0026 baseStream instanceof SocketOutputStream\n           \u0026\u0026 blockIn instanceof FileInputStream;\n       if (transferTo) {\n         FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n         blockInPosition \u003d fileChannel.position();\n         streamForSendChunks \u003d baseStream;\n         maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n         \n         // Smaller packet size to only hold checksum when doing transferTo\n         pktBufSize +\u003d checksumSize * maxChunksPerPacket;\n       } else {\n         maxChunksPerPacket \u003d Math.max(1,\n             numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n         // Packet size includes both checksum and data\n         pktBufSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n       }\n \n       ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktBufSize);\n \n-      while (endOffset \u003e offset) {\n+      while (endOffset \u003e offset \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n         manageOsCache();\n         long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n             transferTo, throttler);\n         offset +\u003d len;\n         totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n         seqno++;\n       }\n-      try {\n-        // send an empty packet to mark the end of the block\n-        sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n-            throttler);\n-        out.flush();\n-      } catch (IOException e) { //socket error\n-        throw ioeToSocketException(e);\n-      }\n+      // If this thread was interrupted, then it did not send the full block.\n+      if (!Thread.currentThread().isInterrupted()) {\n+        try {\n+          // send an empty packet to mark the end of the block\n+          sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n+              throttler);\n+          out.flush();\n+        } catch (IOException e) { //socket error\n+          throw ioeToSocketException(e);\n+        }\n \n-      sentEntireByteRange \u003d true;\n+        sentEntireByteRange \u003d true;\n+      }\n     } finally {\n       if (clientTraceFmt !\u003d null) {\n         final long endTime \u003d System.nanoTime();\n         ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n             initialOffset, endTime - startTime));\n       }\n       close();\n     }\n     return totalRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if (out \u003d\u003d null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset \u003d offset;\n    long totalRead \u003d 0;\n    OutputStream streamForSendChunks \u003d out;\n    \n    lastCacheDropOffset \u003d initialOffset;\n\n    if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      NativeIO.posixFadviseIfPossible(blockInFd, 0, 0, NativeIO.POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      int maxChunksPerPacket;\n      int pktBufSize \u003d PacketHeader.PKT_MAX_HEADER_LEN;\n      boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n          \u0026\u0026 baseStream instanceof SocketOutputStream\n          \u0026\u0026 blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n        blockInPosition \u003d fileChannel.position();\n        streamForSendChunks \u003d baseStream;\n        maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktBufSize +\u003d checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket \u003d Math.max(1,\n            numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktBufSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktBufSize);\n\n      while (endOffset \u003e offset \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n        manageOsCache();\n        long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset +\u003d len;\n        totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      // If this thread was interrupted, then it did not send the full block.\n      if (!Thread.currentThread().isInterrupted()) {\n        try {\n          // send an empty packet to mark the end of the block\n          sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n              throttler);\n          out.flush();\n        } catch (IOException e) { //socket error\n          throw ioeToSocketException(e);\n        }\n\n        sentEntireByteRange \u003d true;\n      }\n    } finally {\n      if (clientTraceFmt !\u003d null) {\n        final long endTime \u003d System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "9ea7c06468d236452f03c38a31d1a45f7f09dc50": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3721. hsync support broke wire compatibility. Contributed by Todd Lipcon and Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1371495 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/08/12 2:31 PM",
      "commitName": "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "07/08/12 1:17 PM",
      "commitNameOld": "c12e994eda0f7e0c34fb0c0ff208789586c7142c",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 2.05,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,71 @@\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n     if (out \u003d\u003d null) {\n       throw new IOException( \"out stream is null\" );\n     }\n     initialOffset \u003d offset;\n     long totalRead \u003d 0;\n     OutputStream streamForSendChunks \u003d out;\n     \n     lastCacheDropOffset \u003d initialOffset;\n \n     if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n       // Advise that this file descriptor will be accessed sequentially.\n       NativeIO.posixFadviseIfPossible(blockInFd, 0, 0, NativeIO.POSIX_FADV_SEQUENTIAL);\n     }\n     \n     // Trigger readahead of beginning of file if configured.\n     manageOsCache();\n \n     final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n     try {\n       int maxChunksPerPacket;\n-      int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n+      int pktBufSize \u003d PacketHeader.PKT_MAX_HEADER_LEN;\n       boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n           \u0026\u0026 baseStream instanceof SocketOutputStream\n           \u0026\u0026 blockIn instanceof FileInputStream;\n       if (transferTo) {\n         FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n         blockInPosition \u003d fileChannel.position();\n         streamForSendChunks \u003d baseStream;\n         maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n         \n         // Smaller packet size to only hold checksum when doing transferTo\n-        pktSize +\u003d checksumSize * maxChunksPerPacket;\n+        pktBufSize +\u003d checksumSize * maxChunksPerPacket;\n       } else {\n         maxChunksPerPacket \u003d Math.max(1,\n             numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n         // Packet size includes both checksum and data\n-        pktSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n+        pktBufSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n       }\n \n-      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n+      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktBufSize);\n \n       while (endOffset \u003e offset) {\n         manageOsCache();\n         long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n             transferTo, throttler);\n         offset +\u003d len;\n         totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n         seqno++;\n       }\n       try {\n         // send an empty packet to mark the end of the block\n         sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n             throttler);\n         out.flush();\n       } catch (IOException e) { //socket error\n         throw ioeToSocketException(e);\n       }\n \n       sentEntireByteRange \u003d true;\n     } finally {\n       if (clientTraceFmt !\u003d null) {\n         final long endTime \u003d System.nanoTime();\n         ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n             initialOffset, endTime - startTime));\n       }\n       close();\n     }\n     return totalRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if (out \u003d\u003d null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset \u003d offset;\n    long totalRead \u003d 0;\n    OutputStream streamForSendChunks \u003d out;\n    \n    lastCacheDropOffset \u003d initialOffset;\n\n    if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      NativeIO.posixFadviseIfPossible(blockInFd, 0, 0, NativeIO.POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      int maxChunksPerPacket;\n      int pktBufSize \u003d PacketHeader.PKT_MAX_HEADER_LEN;\n      boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n          \u0026\u0026 baseStream instanceof SocketOutputStream\n          \u0026\u0026 blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n        blockInPosition \u003d fileChannel.position();\n        streamForSendChunks \u003d baseStream;\n        maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktBufSize +\u003d checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket \u003d Math.max(1,\n            numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktBufSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktBufSize);\n\n      while (endOffset \u003e offset) {\n        manageOsCache();\n        long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset +\u003d len;\n        totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      try {\n        // send an empty packet to mark the end of the block\n        sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n            throttler);\n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n\n      sentEntireByteRange \u003d true;\n    } finally {\n      if (clientTraceFmt !\u003d null) {\n        final long endTime \u003d System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "1c940637b14eee777a65d153d0d712a1aea3866c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2521. Remove custom checksum headers from data transfer protocol. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195829 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/10/11 10:17 PM",
      "commitName": "1c940637b14eee777a65d153d0d712a1aea3866c",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "28/10/11 3:18 PM",
      "commitNameOld": "6b0963c53be360b710614b9f44a29c4171af6b83",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 3.29,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,73 +1,71 @@\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n     if (out \u003d\u003d null) {\n       throw new IOException( \"out stream is null\" );\n     }\n     initialOffset \u003d offset;\n     long totalRead \u003d 0;\n     OutputStream streamForSendChunks \u003d out;\n     \n     lastCacheDropOffset \u003d initialOffset;\n \n     if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n       // Advise that this file descriptor will be accessed sequentially.\n       NativeIO.posixFadviseIfPossible(blockInFd, 0, 0, NativeIO.POSIX_FADV_SEQUENTIAL);\n     }\n     \n     // Trigger readahead of beginning of file if configured.\n     manageOsCache();\n \n     final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n     try {\n-      writeChecksumHeader(out);\n-      \n       int maxChunksPerPacket;\n       int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n       boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n           \u0026\u0026 baseStream instanceof SocketOutputStream\n           \u0026\u0026 blockIn instanceof FileInputStream;\n       if (transferTo) {\n         FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n         blockInPosition \u003d fileChannel.position();\n         streamForSendChunks \u003d baseStream;\n         maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n         \n         // Smaller packet size to only hold checksum when doing transferTo\n         pktSize +\u003d checksumSize * maxChunksPerPacket;\n       } else {\n         maxChunksPerPacket \u003d Math.max(1,\n             numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n         // Packet size includes both checksum and data\n         pktSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n       }\n \n       ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n \n       while (endOffset \u003e offset) {\n         manageOsCache();\n         long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n             transferTo, throttler);\n         offset +\u003d len;\n         totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n         seqno++;\n       }\n       try {\n         // send an empty packet to mark the end of the block\n         sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n             throttler);\n         out.flush();\n       } catch (IOException e) { //socket error\n         throw ioeToSocketException(e);\n       }\n \n       sentEntireByteRange \u003d true;\n     } finally {\n       if (clientTraceFmt !\u003d null) {\n         final long endTime \u003d System.nanoTime();\n         ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n             initialOffset, endTime - startTime));\n       }\n       close();\n     }\n     return totalRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if (out \u003d\u003d null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset \u003d offset;\n    long totalRead \u003d 0;\n    OutputStream streamForSendChunks \u003d out;\n    \n    lastCacheDropOffset \u003d initialOffset;\n\n    if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      NativeIO.posixFadviseIfPossible(blockInFd, 0, 0, NativeIO.POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      int maxChunksPerPacket;\n      int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n      boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n          \u0026\u0026 baseStream instanceof SocketOutputStream\n          \u0026\u0026 blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n        blockInPosition \u003d fileChannel.position();\n        streamForSendChunks \u003d baseStream;\n        maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktSize +\u003d checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket \u003d Math.max(1,\n            numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n\n      while (endOffset \u003e offset) {\n        manageOsCache();\n        long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset +\u003d len;\n        totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      try {\n        // send an empty packet to mark the end of the block\n        sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n            throttler);\n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n\n      sentEntireByteRange \u003d true;\n    } finally {\n      if (clientTraceFmt !\u003d null) {\n        final long endTime \u003d System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "6b0963c53be360b710614b9f44a29c4171af6b83": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2465. Add HDFS support for fadvise readahead and drop-behind. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1190626 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/10/11 3:18 PM",
      "commitName": "6b0963c53be360b710614b9f44a29c4171af6b83",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "28/09/11 9:40 PM",
      "commitNameOld": "e90a5b40430cc1fbce075d34b31e3cc05fd9831f",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 29.74,
      "commitsBetweenForRepo": 266,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,73 @@\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n     if (out \u003d\u003d null) {\n       throw new IOException( \"out stream is null\" );\n     }\n-    final long initialOffset \u003d offset;\n+    initialOffset \u003d offset;\n     long totalRead \u003d 0;\n     OutputStream streamForSendChunks \u003d out;\n     \n+    lastCacheDropOffset \u003d initialOffset;\n+\n+    if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n+      // Advise that this file descriptor will be accessed sequentially.\n+      NativeIO.posixFadviseIfPossible(blockInFd, 0, 0, NativeIO.POSIX_FADV_SEQUENTIAL);\n+    }\n+    \n+    // Trigger readahead of beginning of file if configured.\n+    manageOsCache();\n+\n     final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n     try {\n       writeChecksumHeader(out);\n       \n       int maxChunksPerPacket;\n       int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n       boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n           \u0026\u0026 baseStream instanceof SocketOutputStream\n           \u0026\u0026 blockIn instanceof FileInputStream;\n       if (transferTo) {\n         FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n         blockInPosition \u003d fileChannel.position();\n         streamForSendChunks \u003d baseStream;\n         maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n         \n         // Smaller packet size to only hold checksum when doing transferTo\n         pktSize +\u003d checksumSize * maxChunksPerPacket;\n       } else {\n         maxChunksPerPacket \u003d Math.max(1,\n             numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n         // Packet size includes both checksum and data\n         pktSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n       }\n \n       ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n \n       while (endOffset \u003e offset) {\n+        manageOsCache();\n         long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n             transferTo, throttler);\n         offset +\u003d len;\n         totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n         seqno++;\n       }\n       try {\n         // send an empty packet to mark the end of the block\n         sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n             throttler);\n         out.flush();\n       } catch (IOException e) { //socket error\n         throw ioeToSocketException(e);\n       }\n \n       sentEntireByteRange \u003d true;\n     } finally {\n       if (clientTraceFmt !\u003d null) {\n         final long endTime \u003d System.nanoTime();\n         ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n             initialOffset, endTime - startTime));\n       }\n       close();\n     }\n     return totalRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if (out \u003d\u003d null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    initialOffset \u003d offset;\n    long totalRead \u003d 0;\n    OutputStream streamForSendChunks \u003d out;\n    \n    lastCacheDropOffset \u003d initialOffset;\n\n    if (isLongRead() \u0026\u0026 blockInFd !\u003d null) {\n      // Advise that this file descriptor will be accessed sequentially.\n      NativeIO.posixFadviseIfPossible(blockInFd, 0, 0, NativeIO.POSIX_FADV_SEQUENTIAL);\n    }\n    \n    // Trigger readahead of beginning of file if configured.\n    manageOsCache();\n\n    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      writeChecksumHeader(out);\n      \n      int maxChunksPerPacket;\n      int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n      boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n          \u0026\u0026 baseStream instanceof SocketOutputStream\n          \u0026\u0026 blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n        blockInPosition \u003d fileChannel.position();\n        streamForSendChunks \u003d baseStream;\n        maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktSize +\u003d checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket \u003d Math.max(1,\n            numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n\n      while (endOffset \u003e offset) {\n        manageOsCache();\n        long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset +\u003d len;\n        totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      try {\n        // send an empty packet to mark the end of the block\n        sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n            throttler);\n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n\n      sentEntireByteRange \u003d true;\n    } finally {\n      if (clientTraceFmt !\u003d null) {\n        final long endTime \u003d System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "e90a5b40430cc1fbce075d34b31e3cc05fd9831f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2371. Refactor BlockSender.java for better readability. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1177161 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/09/11 9:40 PM",
      "commitName": "e90a5b40430cc1fbce075d34b31e3cc05fd9831f",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "04/09/11 12:30 PM",
      "commitNameOld": "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 24.38,
      "commitsBetweenForRepo": 173,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,62 @@\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n-    if( out \u003d\u003d null ) {\n+    if (out \u003d\u003d null) {\n       throw new IOException( \"out stream is null\" );\n     }\n-    this.throttler \u003d throttler;\n-\n-    long initialOffset \u003d offset;\n+    final long initialOffset \u003d offset;\n     long totalRead \u003d 0;\n     OutputStream streamForSendChunks \u003d out;\n     \n     final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n     try {\n-      try {\n-        checksum.writeHeader(out);\n-        if ( chunkOffsetOK ) {\n-          out.writeLong( offset );\n-        }\n-        out.flush();\n-      } catch (IOException e) { //socket error\n-        throw ioeToSocketException(e);\n-      }\n+      writeChecksumHeader(out);\n       \n       int maxChunksPerPacket;\n       int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n-      \n-      if (transferToAllowed \u0026\u0026 !verifyChecksum \u0026\u0026 \n-          baseStream instanceof SocketOutputStream \u0026\u0026 \n-          blockIn instanceof FileInputStream) {\n-        \n+      boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n+          \u0026\u0026 baseStream instanceof SocketOutputStream\n+          \u0026\u0026 blockIn instanceof FileInputStream;\n+      if (transferTo) {\n         FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n-        \n-        // blockInPosition also indicates sendChunks() uses transferTo.\n         blockInPosition \u003d fileChannel.position();\n         streamForSendChunks \u003d baseStream;\n+        maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n         \n-        // assure a mininum buffer size.\n-        maxChunksPerPacket \u003d (Math.max(HdfsConstants.IO_FILE_BUFFER_SIZE, \n-                                       MIN_BUFFER_WITH_TRANSFERTO)\n-                              + bytesPerChecksum - 1)/bytesPerChecksum;\n-        \n-        // allocate smaller buffer while using transferTo(). \n+        // Smaller packet size to only hold checksum when doing transferTo\n         pktSize +\u003d checksumSize * maxChunksPerPacket;\n       } else {\n-        maxChunksPerPacket \u003d Math.max(1, (HdfsConstants.IO_FILE_BUFFER_SIZE\n-            + bytesPerChecksum - 1) / bytesPerChecksum);\n-        pktSize +\u003d (bytesPerChecksum + checksumSize) * maxChunksPerPacket;\n+        maxChunksPerPacket \u003d Math.max(1,\n+            numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n+        // Packet size includes both checksum and data\n+        pktSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n       }\n \n       ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n \n       while (endOffset \u003e offset) {\n-        long len \u003d sendChunks(pktBuf, maxChunksPerPacket, \n-                              streamForSendChunks);\n+        long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n+            transferTo, throttler);\n         offset +\u003d len;\n-        totalRead +\u003d len + ((len + bytesPerChecksum - 1)/bytesPerChecksum*\n-                            checksumSize);\n+        totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n         seqno++;\n       }\n       try {\n         // send an empty packet to mark the end of the block\n-        sendChunks(pktBuf, maxChunksPerPacket, streamForSendChunks);        \n+        sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n+            throttler);\n         out.flush();\n       } catch (IOException e) { //socket error\n         throw ioeToSocketException(e);\n       }\n \n       sentEntireByteRange \u003d true;\n     } finally {\n       if (clientTraceFmt !\u003d null) {\n         final long endTime \u003d System.nanoTime();\n-        ClientTraceLog.info(String.format(clientTraceFmt, totalRead, initialOffset, endTime - startTime));\n+        ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n+            initialOffset, endTime - startTime));\n       }\n       close();\n     }\n-\n     return totalRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if (out \u003d\u003d null) {\n      throw new IOException( \"out stream is null\" );\n    }\n    final long initialOffset \u003d offset;\n    long totalRead \u003d 0;\n    OutputStream streamForSendChunks \u003d out;\n    \n    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      writeChecksumHeader(out);\n      \n      int maxChunksPerPacket;\n      int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n      boolean transferTo \u003d transferToAllowed \u0026\u0026 !verifyChecksum\n          \u0026\u0026 baseStream instanceof SocketOutputStream\n          \u0026\u0026 blockIn instanceof FileInputStream;\n      if (transferTo) {\n        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n        blockInPosition \u003d fileChannel.position();\n        streamForSendChunks \u003d baseStream;\n        maxChunksPerPacket \u003d numberOfChunks(TRANSFERTO_BUFFER_SIZE);\n        \n        // Smaller packet size to only hold checksum when doing transferTo\n        pktSize +\u003d checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket \u003d Math.max(1,\n            numberOfChunks(HdfsConstants.IO_FILE_BUFFER_SIZE));\n        // Packet size includes both checksum and data\n        pktSize +\u003d (chunkSize + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n\n      while (endOffset \u003e offset) {\n        long len \u003d sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks,\n            transferTo, throttler);\n        offset +\u003d len;\n        totalRead +\u003d len + (numberOfChunks(len) * checksumSize);\n        seqno++;\n      }\n      try {\n        // send an empty packet to mark the end of the block\n        sendPacket(pktBuf, maxChunksPerPacket, streamForSendChunks, transferTo,\n            throttler);\n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n\n      sentEntireByteRange \u003d true;\n    } finally {\n      if (clientTraceFmt !\u003d null) {\n        final long endTime \u003d System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead,\n            initialOffset, endTime - startTime));\n      }\n      close();\n    }\n    return totalRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1620. Rename HdfsConstants -\u003e HdfsServerConstants, FSConstants -\u003e HdfsConstants. (Harsh J Chouraria via atm)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165096 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/11 12:30 PM",
      "commitName": "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 10.8,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,78 @@\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n     if( out \u003d\u003d null ) {\n       throw new IOException( \"out stream is null\" );\n     }\n     this.throttler \u003d throttler;\n \n     long initialOffset \u003d offset;\n     long totalRead \u003d 0;\n     OutputStream streamForSendChunks \u003d out;\n     \n     final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n     try {\n       try {\n         checksum.writeHeader(out);\n         if ( chunkOffsetOK ) {\n           out.writeLong( offset );\n         }\n         out.flush();\n       } catch (IOException e) { //socket error\n         throw ioeToSocketException(e);\n       }\n       \n       int maxChunksPerPacket;\n       int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n       \n       if (transferToAllowed \u0026\u0026 !verifyChecksum \u0026\u0026 \n           baseStream instanceof SocketOutputStream \u0026\u0026 \n           blockIn instanceof FileInputStream) {\n         \n         FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n         \n         // blockInPosition also indicates sendChunks() uses transferTo.\n         blockInPosition \u003d fileChannel.position();\n         streamForSendChunks \u003d baseStream;\n         \n         // assure a mininum buffer size.\n-        maxChunksPerPacket \u003d (Math.max(FSConstants.IO_FILE_BUFFER_SIZE, \n+        maxChunksPerPacket \u003d (Math.max(HdfsConstants.IO_FILE_BUFFER_SIZE, \n                                        MIN_BUFFER_WITH_TRANSFERTO)\n                               + bytesPerChecksum - 1)/bytesPerChecksum;\n         \n         // allocate smaller buffer while using transferTo(). \n         pktSize +\u003d checksumSize * maxChunksPerPacket;\n       } else {\n-        maxChunksPerPacket \u003d Math.max(1, (FSConstants.IO_FILE_BUFFER_SIZE\n+        maxChunksPerPacket \u003d Math.max(1, (HdfsConstants.IO_FILE_BUFFER_SIZE\n             + bytesPerChecksum - 1) / bytesPerChecksum);\n         pktSize +\u003d (bytesPerChecksum + checksumSize) * maxChunksPerPacket;\n       }\n \n       ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n \n       while (endOffset \u003e offset) {\n         long len \u003d sendChunks(pktBuf, maxChunksPerPacket, \n                               streamForSendChunks);\n         offset +\u003d len;\n         totalRead +\u003d len + ((len + bytesPerChecksum - 1)/bytesPerChecksum*\n                             checksumSize);\n         seqno++;\n       }\n       try {\n         // send an empty packet to mark the end of the block\n         sendChunks(pktBuf, maxChunksPerPacket, streamForSendChunks);        \n         out.flush();\n       } catch (IOException e) { //socket error\n         throw ioeToSocketException(e);\n       }\n \n       sentEntireByteRange \u003d true;\n     } finally {\n       if (clientTraceFmt !\u003d null) {\n         final long endTime \u003d System.nanoTime();\n         ClientTraceLog.info(String.format(clientTraceFmt, totalRead, initialOffset, endTime - startTime));\n       }\n       close();\n     }\n \n     return totalRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if( out \u003d\u003d null ) {\n      throw new IOException( \"out stream is null\" );\n    }\n    this.throttler \u003d throttler;\n\n    long initialOffset \u003d offset;\n    long totalRead \u003d 0;\n    OutputStream streamForSendChunks \u003d out;\n    \n    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      try {\n        checksum.writeHeader(out);\n        if ( chunkOffsetOK ) {\n          out.writeLong( offset );\n        }\n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n      \n      int maxChunksPerPacket;\n      int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n      \n      if (transferToAllowed \u0026\u0026 !verifyChecksum \u0026\u0026 \n          baseStream instanceof SocketOutputStream \u0026\u0026 \n          blockIn instanceof FileInputStream) {\n        \n        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n        \n        // blockInPosition also indicates sendChunks() uses transferTo.\n        blockInPosition \u003d fileChannel.position();\n        streamForSendChunks \u003d baseStream;\n        \n        // assure a mininum buffer size.\n        maxChunksPerPacket \u003d (Math.max(HdfsConstants.IO_FILE_BUFFER_SIZE, \n                                       MIN_BUFFER_WITH_TRANSFERTO)\n                              + bytesPerChecksum - 1)/bytesPerChecksum;\n        \n        // allocate smaller buffer while using transferTo(). \n        pktSize +\u003d checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket \u003d Math.max(1, (HdfsConstants.IO_FILE_BUFFER_SIZE\n            + bytesPerChecksum - 1) / bytesPerChecksum);\n        pktSize +\u003d (bytesPerChecksum + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n\n      while (endOffset \u003e offset) {\n        long len \u003d sendChunks(pktBuf, maxChunksPerPacket, \n                              streamForSendChunks);\n        offset +\u003d len;\n        totalRead +\u003d len + ((len + bytesPerChecksum - 1)/bytesPerChecksum*\n                            checksumSize);\n        seqno++;\n      }\n      try {\n        // send an empty packet to mark the end of the block\n        sendChunks(pktBuf, maxChunksPerPacket, streamForSendChunks);        \n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n\n      sentEntireByteRange \u003d true;\n    } finally {\n      if (clientTraceFmt !\u003d null) {\n        final long endTime \u003d System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead, initialOffset, endTime - startTime));\n      }\n      close();\n    }\n\n    return totalRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if( out \u003d\u003d null ) {\n      throw new IOException( \"out stream is null\" );\n    }\n    this.throttler \u003d throttler;\n\n    long initialOffset \u003d offset;\n    long totalRead \u003d 0;\n    OutputStream streamForSendChunks \u003d out;\n    \n    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      try {\n        checksum.writeHeader(out);\n        if ( chunkOffsetOK ) {\n          out.writeLong( offset );\n        }\n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n      \n      int maxChunksPerPacket;\n      int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n      \n      if (transferToAllowed \u0026\u0026 !verifyChecksum \u0026\u0026 \n          baseStream instanceof SocketOutputStream \u0026\u0026 \n          blockIn instanceof FileInputStream) {\n        \n        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n        \n        // blockInPosition also indicates sendChunks() uses transferTo.\n        blockInPosition \u003d fileChannel.position();\n        streamForSendChunks \u003d baseStream;\n        \n        // assure a mininum buffer size.\n        maxChunksPerPacket \u003d (Math.max(FSConstants.IO_FILE_BUFFER_SIZE, \n                                       MIN_BUFFER_WITH_TRANSFERTO)\n                              + bytesPerChecksum - 1)/bytesPerChecksum;\n        \n        // allocate smaller buffer while using transferTo(). \n        pktSize +\u003d checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket \u003d Math.max(1, (FSConstants.IO_FILE_BUFFER_SIZE\n            + bytesPerChecksum - 1) / bytesPerChecksum);\n        pktSize +\u003d (bytesPerChecksum + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n\n      while (endOffset \u003e offset) {\n        long len \u003d sendChunks(pktBuf, maxChunksPerPacket, \n                              streamForSendChunks);\n        offset +\u003d len;\n        totalRead +\u003d len + ((len + bytesPerChecksum - 1)/bytesPerChecksum*\n                            checksumSize);\n        seqno++;\n      }\n      try {\n        // send an empty packet to mark the end of the block\n        sendChunks(pktBuf, maxChunksPerPacket, streamForSendChunks);        \n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n\n      sentEntireByteRange \u003d true;\n    } finally {\n      if (clientTraceFmt !\u003d null) {\n        final long endTime \u003d System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead, initialOffset, endTime - startTime));\n      }\n      close();\n    }\n\n    return totalRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if( out \u003d\u003d null ) {\n      throw new IOException( \"out stream is null\" );\n    }\n    this.throttler \u003d throttler;\n\n    long initialOffset \u003d offset;\n    long totalRead \u003d 0;\n    OutputStream streamForSendChunks \u003d out;\n    \n    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      try {\n        checksum.writeHeader(out);\n        if ( chunkOffsetOK ) {\n          out.writeLong( offset );\n        }\n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n      \n      int maxChunksPerPacket;\n      int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n      \n      if (transferToAllowed \u0026\u0026 !verifyChecksum \u0026\u0026 \n          baseStream instanceof SocketOutputStream \u0026\u0026 \n          blockIn instanceof FileInputStream) {\n        \n        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n        \n        // blockInPosition also indicates sendChunks() uses transferTo.\n        blockInPosition \u003d fileChannel.position();\n        streamForSendChunks \u003d baseStream;\n        \n        // assure a mininum buffer size.\n        maxChunksPerPacket \u003d (Math.max(FSConstants.IO_FILE_BUFFER_SIZE, \n                                       MIN_BUFFER_WITH_TRANSFERTO)\n                              + bytesPerChecksum - 1)/bytesPerChecksum;\n        \n        // allocate smaller buffer while using transferTo(). \n        pktSize +\u003d checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket \u003d Math.max(1, (FSConstants.IO_FILE_BUFFER_SIZE\n            + bytesPerChecksum - 1) / bytesPerChecksum);\n        pktSize +\u003d (bytesPerChecksum + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n\n      while (endOffset \u003e offset) {\n        long len \u003d sendChunks(pktBuf, maxChunksPerPacket, \n                              streamForSendChunks);\n        offset +\u003d len;\n        totalRead +\u003d len + ((len + bytesPerChecksum - 1)/bytesPerChecksum*\n                            checksumSize);\n        seqno++;\n      }\n      try {\n        // send an empty packet to mark the end of the block\n        sendChunks(pktBuf, maxChunksPerPacket, streamForSendChunks);        \n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n\n      sentEntireByteRange \u003d true;\n    } finally {\n      if (clientTraceFmt !\u003d null) {\n        final long endTime \u003d System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead, initialOffset, endTime - startTime));\n      }\n      close();\n    }\n\n    return totalRead;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java"
      }
    },
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2241. Remove implementing FSConstants interface to just get the constants from the interface. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1156420 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/08/11 5:46 PM",
      "commitName": "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "12/07/11 6:11 PM",
      "commitNameOld": "2c5dd549e31aa5d3377ff2619ede8e92b8dc5d0f",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 28.98,
      "commitsBetweenForRepo": 108,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,78 @@\n   long sendBlock(DataOutputStream out, OutputStream baseStream, \n                  DataTransferThrottler throttler) throws IOException {\n     if( out \u003d\u003d null ) {\n       throw new IOException( \"out stream is null\" );\n     }\n     this.throttler \u003d throttler;\n \n     long initialOffset \u003d offset;\n     long totalRead \u003d 0;\n     OutputStream streamForSendChunks \u003d out;\n     \n     final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n     try {\n       try {\n         checksum.writeHeader(out);\n         if ( chunkOffsetOK ) {\n           out.writeLong( offset );\n         }\n         out.flush();\n       } catch (IOException e) { //socket error\n         throw ioeToSocketException(e);\n       }\n       \n       int maxChunksPerPacket;\n       int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n       \n       if (transferToAllowed \u0026\u0026 !verifyChecksum \u0026\u0026 \n           baseStream instanceof SocketOutputStream \u0026\u0026 \n           blockIn instanceof FileInputStream) {\n         \n         FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n         \n         // blockInPosition also indicates sendChunks() uses transferTo.\n         blockInPosition \u003d fileChannel.position();\n         streamForSendChunks \u003d baseStream;\n         \n         // assure a mininum buffer size.\n-        maxChunksPerPacket \u003d (Math.max(BUFFER_SIZE, \n+        maxChunksPerPacket \u003d (Math.max(FSConstants.IO_FILE_BUFFER_SIZE, \n                                        MIN_BUFFER_WITH_TRANSFERTO)\n                               + bytesPerChecksum - 1)/bytesPerChecksum;\n         \n         // allocate smaller buffer while using transferTo(). \n         pktSize +\u003d checksumSize * maxChunksPerPacket;\n       } else {\n-        maxChunksPerPacket \u003d Math.max(1,\n-                 (BUFFER_SIZE + bytesPerChecksum - 1)/bytesPerChecksum);\n+        maxChunksPerPacket \u003d Math.max(1, (FSConstants.IO_FILE_BUFFER_SIZE\n+            + bytesPerChecksum - 1) / bytesPerChecksum);\n         pktSize +\u003d (bytesPerChecksum + checksumSize) * maxChunksPerPacket;\n       }\n \n       ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n \n       while (endOffset \u003e offset) {\n         long len \u003d sendChunks(pktBuf, maxChunksPerPacket, \n                               streamForSendChunks);\n         offset +\u003d len;\n         totalRead +\u003d len + ((len + bytesPerChecksum - 1)/bytesPerChecksum*\n                             checksumSize);\n         seqno++;\n       }\n       try {\n         // send an empty packet to mark the end of the block\n         sendChunks(pktBuf, maxChunksPerPacket, streamForSendChunks);        \n         out.flush();\n       } catch (IOException e) { //socket error\n         throw ioeToSocketException(e);\n       }\n \n       sentEntireByteRange \u003d true;\n     } finally {\n       if (clientTraceFmt !\u003d null) {\n         final long endTime \u003d System.nanoTime();\n         ClientTraceLog.info(String.format(clientTraceFmt, totalRead, initialOffset, endTime - startTime));\n       }\n       close();\n     }\n \n     return totalRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if( out \u003d\u003d null ) {\n      throw new IOException( \"out stream is null\" );\n    }\n    this.throttler \u003d throttler;\n\n    long initialOffset \u003d offset;\n    long totalRead \u003d 0;\n    OutputStream streamForSendChunks \u003d out;\n    \n    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      try {\n        checksum.writeHeader(out);\n        if ( chunkOffsetOK ) {\n          out.writeLong( offset );\n        }\n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n      \n      int maxChunksPerPacket;\n      int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n      \n      if (transferToAllowed \u0026\u0026 !verifyChecksum \u0026\u0026 \n          baseStream instanceof SocketOutputStream \u0026\u0026 \n          blockIn instanceof FileInputStream) {\n        \n        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n        \n        // blockInPosition also indicates sendChunks() uses transferTo.\n        blockInPosition \u003d fileChannel.position();\n        streamForSendChunks \u003d baseStream;\n        \n        // assure a mininum buffer size.\n        maxChunksPerPacket \u003d (Math.max(FSConstants.IO_FILE_BUFFER_SIZE, \n                                       MIN_BUFFER_WITH_TRANSFERTO)\n                              + bytesPerChecksum - 1)/bytesPerChecksum;\n        \n        // allocate smaller buffer while using transferTo(). \n        pktSize +\u003d checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket \u003d Math.max(1, (FSConstants.IO_FILE_BUFFER_SIZE\n            + bytesPerChecksum - 1) / bytesPerChecksum);\n        pktSize +\u003d (bytesPerChecksum + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n\n      while (endOffset \u003e offset) {\n        long len \u003d sendChunks(pktBuf, maxChunksPerPacket, \n                              streamForSendChunks);\n        offset +\u003d len;\n        totalRead +\u003d len + ((len + bytesPerChecksum - 1)/bytesPerChecksum*\n                            checksumSize);\n        seqno++;\n      }\n      try {\n        // send an empty packet to mark the end of the block\n        sendChunks(pktBuf, maxChunksPerPacket, streamForSendChunks);        \n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n\n      sentEntireByteRange \u003d true;\n    } finally {\n      if (clientTraceFmt !\u003d null) {\n        final long endTime \u003d System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead, initialOffset, endTime - startTime));\n      }\n      close();\n    }\n\n    return totalRead;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,78 @@\n+  long sendBlock(DataOutputStream out, OutputStream baseStream, \n+                 DataTransferThrottler throttler) throws IOException {\n+    if( out \u003d\u003d null ) {\n+      throw new IOException( \"out stream is null\" );\n+    }\n+    this.throttler \u003d throttler;\n+\n+    long initialOffset \u003d offset;\n+    long totalRead \u003d 0;\n+    OutputStream streamForSendChunks \u003d out;\n+    \n+    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n+    try {\n+      try {\n+        checksum.writeHeader(out);\n+        if ( chunkOffsetOK ) {\n+          out.writeLong( offset );\n+        }\n+        out.flush();\n+      } catch (IOException e) { //socket error\n+        throw ioeToSocketException(e);\n+      }\n+      \n+      int maxChunksPerPacket;\n+      int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n+      \n+      if (transferToAllowed \u0026\u0026 !verifyChecksum \u0026\u0026 \n+          baseStream instanceof SocketOutputStream \u0026\u0026 \n+          blockIn instanceof FileInputStream) {\n+        \n+        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n+        \n+        // blockInPosition also indicates sendChunks() uses transferTo.\n+        blockInPosition \u003d fileChannel.position();\n+        streamForSendChunks \u003d baseStream;\n+        \n+        // assure a mininum buffer size.\n+        maxChunksPerPacket \u003d (Math.max(BUFFER_SIZE, \n+                                       MIN_BUFFER_WITH_TRANSFERTO)\n+                              + bytesPerChecksum - 1)/bytesPerChecksum;\n+        \n+        // allocate smaller buffer while using transferTo(). \n+        pktSize +\u003d checksumSize * maxChunksPerPacket;\n+      } else {\n+        maxChunksPerPacket \u003d Math.max(1,\n+                 (BUFFER_SIZE + bytesPerChecksum - 1)/bytesPerChecksum);\n+        pktSize +\u003d (bytesPerChecksum + checksumSize) * maxChunksPerPacket;\n+      }\n+\n+      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n+\n+      while (endOffset \u003e offset) {\n+        long len \u003d sendChunks(pktBuf, maxChunksPerPacket, \n+                              streamForSendChunks);\n+        offset +\u003d len;\n+        totalRead +\u003d len + ((len + bytesPerChecksum - 1)/bytesPerChecksum*\n+                            checksumSize);\n+        seqno++;\n+      }\n+      try {\n+        // send an empty packet to mark the end of the block\n+        sendChunks(pktBuf, maxChunksPerPacket, streamForSendChunks);        \n+        out.flush();\n+      } catch (IOException e) { //socket error\n+        throw ioeToSocketException(e);\n+      }\n+\n+      sentEntireByteRange \u003d true;\n+    } finally {\n+      if (clientTraceFmt !\u003d null) {\n+        final long endTime \u003d System.nanoTime();\n+        ClientTraceLog.info(String.format(clientTraceFmt, totalRead, initialOffset, endTime - startTime));\n+      }\n+      close();\n+    }\n+\n+    return totalRead;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  long sendBlock(DataOutputStream out, OutputStream baseStream, \n                 DataTransferThrottler throttler) throws IOException {\n    if( out \u003d\u003d null ) {\n      throw new IOException( \"out stream is null\" );\n    }\n    this.throttler \u003d throttler;\n\n    long initialOffset \u003d offset;\n    long totalRead \u003d 0;\n    OutputStream streamForSendChunks \u003d out;\n    \n    final long startTime \u003d ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n    try {\n      try {\n        checksum.writeHeader(out);\n        if ( chunkOffsetOK ) {\n          out.writeLong( offset );\n        }\n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n      \n      int maxChunksPerPacket;\n      int pktSize \u003d PacketHeader.PKT_HEADER_LEN;\n      \n      if (transferToAllowed \u0026\u0026 !verifyChecksum \u0026\u0026 \n          baseStream instanceof SocketOutputStream \u0026\u0026 \n          blockIn instanceof FileInputStream) {\n        \n        FileChannel fileChannel \u003d ((FileInputStream)blockIn).getChannel();\n        \n        // blockInPosition also indicates sendChunks() uses transferTo.\n        blockInPosition \u003d fileChannel.position();\n        streamForSendChunks \u003d baseStream;\n        \n        // assure a mininum buffer size.\n        maxChunksPerPacket \u003d (Math.max(BUFFER_SIZE, \n                                       MIN_BUFFER_WITH_TRANSFERTO)\n                              + bytesPerChecksum - 1)/bytesPerChecksum;\n        \n        // allocate smaller buffer while using transferTo(). \n        pktSize +\u003d checksumSize * maxChunksPerPacket;\n      } else {\n        maxChunksPerPacket \u003d Math.max(1,\n                 (BUFFER_SIZE + bytesPerChecksum - 1)/bytesPerChecksum);\n        pktSize +\u003d (bytesPerChecksum + checksumSize) * maxChunksPerPacket;\n      }\n\n      ByteBuffer pktBuf \u003d ByteBuffer.allocate(pktSize);\n\n      while (endOffset \u003e offset) {\n        long len \u003d sendChunks(pktBuf, maxChunksPerPacket, \n                              streamForSendChunks);\n        offset +\u003d len;\n        totalRead +\u003d len + ((len + bytesPerChecksum - 1)/bytesPerChecksum*\n                            checksumSize);\n        seqno++;\n      }\n      try {\n        // send an empty packet to mark the end of the block\n        sendChunks(pktBuf, maxChunksPerPacket, streamForSendChunks);        \n        out.flush();\n      } catch (IOException e) { //socket error\n        throw ioeToSocketException(e);\n      }\n\n      sentEntireByteRange \u003d true;\n    } finally {\n      if (clientTraceFmt !\u003d null) {\n        final long endTime \u003d System.nanoTime();\n        ClientTraceLog.info(String.format(clientTraceFmt, totalRead, initialOffset, endTime - startTime));\n      }\n      close();\n    }\n\n    return totalRead;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java"
    }
  }
}