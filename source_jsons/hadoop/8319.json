{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "NamenodeWebHdfsMethods.java",
  "functionName": "write",
  "functionId": "write___outstream-OutputStream(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
  "functionStartLine": 1405,
  "functionEndLine": 1442,
  "numCommitsSeen": 104,
  "timeTaken": 3268,
  "changeHistory": [
    "2ee0d64aceed876f57f09eb9efe1872b6de98d2e",
    "8293e225657a09b9352539d07ced67008976816a",
    "6449f524552f8c24d20b314ad21f6c579fa08e85",
    "94c631af1fc49f5ae5881fcd5f0e80b17308d15d",
    "c46dbedaf94cb72e35e9e63d7f99e382ae9f0974",
    "676f488efffd50eb47e75cd750f9bc948b9e12fb",
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
    "6c3b59505b863f03629da52a1e9b886fe9b496d0"
  ],
  "changeHistoryShort": {
    "2ee0d64aceed876f57f09eb9efe1872b6de98d2e": "Ybodychange",
    "8293e225657a09b9352539d07ced67008976816a": "Ybodychange",
    "6449f524552f8c24d20b314ad21f6c579fa08e85": "Ybodychange",
    "94c631af1fc49f5ae5881fcd5f0e80b17308d15d": "Ybodychange",
    "c46dbedaf94cb72e35e9e63d7f99e382ae9f0974": "Ybodychange",
    "676f488efffd50eb47e75cd750f9bc948b9e12fb": "Ybodychange",
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57": "Ybodychange",
    "6c3b59505b863f03629da52a1e9b886fe9b496d0": "Yintroduced"
  },
  "changeHistoryDetails": {
    "2ee0d64aceed876f57f09eb9efe1872b6de98d2e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12945. Switch to ClientProtocol instead of NamenodeProtocols in NamenodeWebHdfsMethods. Contributed by Wei Yan.\n",
      "commitDate": "08/01/18 2:25 PM",
      "commitName": "2ee0d64aceed876f57f09eb9efe1872b6de98d2e",
      "commitAuthor": "Wei Yan",
      "commitDateOld": "26/09/17 1:15 PM",
      "commitNameOld": "0da29cbeea40cb7839abcd72566b997962829329",
      "commitAuthorOld": "Daryn Sharp",
      "daysBetweenCommits": 104.09,
      "commitsBetweenForRepo": 716,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n       public void write(final OutputStream outstream) throws IOException {\n         final PrintWriter out \u003d new PrintWriter(new OutputStreamWriter(\n             outstream, Charsets.UTF_8));\n         out.println(\"{\\\"\" + FileStatus.class.getSimpleName() + \"es\\\":{\\\"\"\n             + FileStatus.class.getSimpleName() + \"\\\":[\");\n \n         try {\n           // restore remote user\u0027s ugi\n           ugi.doAs(new PrivilegedExceptionAction\u003cVoid\u003e() {\n             @Override\n             public Void run() throws IOException {\n               long n \u003d 0;\n               for (DirectoryListing dirList \u003d firstDirList; ;\n-                   dirList \u003d getDirectoryListing(np, p, dirList.getLastName())\n+                   dirList \u003d getDirectoryListing(cp, p, dirList.getLastName())\n               ) {\n                 // send each segment of the directory listing\n                 for (HdfsFileStatus s : dirList.getPartialListing()) {\n                   if (n++ \u003e 0) {\n                     out.println(\u0027,\u0027);\n                   }\n                   out.print(JsonUtil.toJsonString(s, false));\n                 }\n                 // stop if last segment\n                 if (!dirList.hasMore()) {\n                   break;\n                 }\n               }\n               return null;\n             }\n           });\n         } catch (InterruptedException e) {\n           throw new IOException(e);\n         }\n         \n         out.println();\n         out.println(\"]}}\");\n         out.flush();\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public void write(final OutputStream outstream) throws IOException {\n        final PrintWriter out \u003d new PrintWriter(new OutputStreamWriter(\n            outstream, Charsets.UTF_8));\n        out.println(\"{\\\"\" + FileStatus.class.getSimpleName() + \"es\\\":{\\\"\"\n            + FileStatus.class.getSimpleName() + \"\\\":[\");\n\n        try {\n          // restore remote user\u0027s ugi\n          ugi.doAs(new PrivilegedExceptionAction\u003cVoid\u003e() {\n            @Override\n            public Void run() throws IOException {\n              long n \u003d 0;\n              for (DirectoryListing dirList \u003d firstDirList; ;\n                   dirList \u003d getDirectoryListing(cp, p, dirList.getLastName())\n              ) {\n                // send each segment of the directory listing\n                for (HdfsFileStatus s : dirList.getPartialListing()) {\n                  if (n++ \u003e 0) {\n                    out.println(\u0027,\u0027);\n                  }\n                  out.print(JsonUtil.toJsonString(s, false));\n                }\n                // stop if last segment\n                if (!dirList.hasMore()) {\n                  break;\n                }\n              }\n              return null;\n            }\n          });\n        } catch (InterruptedException e) {\n          throw new IOException(e);\n        }\n        \n        out.println();\n        out.println(\"]}}\");\n        out.flush();\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "8293e225657a09b9352539d07ced67008976816a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4649. Webhdfs cannot list large directories. Contributed by Daryn Sharp.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1463698 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/04/13 12:11 PM",
      "commitName": "8293e225657a09b9352539d07ced67008976816a",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "08/03/13 9:23 AM",
      "commitNameOld": "a549d6fa2c103965d1bd3cfbfdf2ce08533eb4a4",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 25.08,
      "commitsBetweenForRepo": 131,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,38 @@\n       public void write(final OutputStream outstream) throws IOException {\n         final PrintWriter out \u003d new PrintWriter(new OutputStreamWriter(\n             outstream, Charsets.UTF_8));\n         out.println(\"{\\\"\" + FileStatus.class.getSimpleName() + \"es\\\":{\\\"\"\n             + FileStatus.class.getSimpleName() + \"\\\":[\");\n \n-        final HdfsFileStatus[] partial \u003d first.getPartialListing();\n-        if (partial.length \u003e 0) {\n-          out.print(JsonUtil.toJsonString(partial[0], false));\n-        }\n-        for(int i \u003d 1; i \u003c partial.length; i++) {\n-          out.println(\u0027,\u0027);\n-          out.print(JsonUtil.toJsonString(partial[i], false));\n-        }\n-\n-        for(DirectoryListing curr \u003d first; curr.hasMore(); ) { \n-          curr \u003d getDirectoryListing(np, p, curr.getLastName());\n-          for(HdfsFileStatus s : curr.getPartialListing()) {\n-            out.println(\u0027,\u0027);\n-            out.print(JsonUtil.toJsonString(s, false));\n-          }\n+        try {\n+          // restore remote user\u0027s ugi\n+          ugi.doAs(new PrivilegedExceptionAction\u003cVoid\u003e() {\n+            @Override\n+            public Void run() throws IOException {\n+              long n \u003d 0;\n+              for (DirectoryListing dirList \u003d firstDirList; ;\n+                   dirList \u003d getDirectoryListing(np, p, dirList.getLastName())\n+              ) {\n+                // send each segment of the directory listing\n+                for (HdfsFileStatus s : dirList.getPartialListing()) {\n+                  if (n++ \u003e 0) {\n+                    out.println(\u0027,\u0027);\n+                  }\n+                  out.print(JsonUtil.toJsonString(s, false));\n+                }\n+                // stop if last segment\n+                if (!dirList.hasMore()) {\n+                  break;\n+                }\n+              }\n+              return null;\n+            }\n+          });\n+        } catch (InterruptedException e) {\n+          throw new IOException(e);\n         }\n         \n         out.println();\n         out.println(\"]}}\");\n         out.flush();\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public void write(final OutputStream outstream) throws IOException {\n        final PrintWriter out \u003d new PrintWriter(new OutputStreamWriter(\n            outstream, Charsets.UTF_8));\n        out.println(\"{\\\"\" + FileStatus.class.getSimpleName() + \"es\\\":{\\\"\"\n            + FileStatus.class.getSimpleName() + \"\\\":[\");\n\n        try {\n          // restore remote user\u0027s ugi\n          ugi.doAs(new PrivilegedExceptionAction\u003cVoid\u003e() {\n            @Override\n            public Void run() throws IOException {\n              long n \u003d 0;\n              for (DirectoryListing dirList \u003d firstDirList; ;\n                   dirList \u003d getDirectoryListing(np, p, dirList.getLastName())\n              ) {\n                // send each segment of the directory listing\n                for (HdfsFileStatus s : dirList.getPartialListing()) {\n                  if (n++ \u003e 0) {\n                    out.println(\u0027,\u0027);\n                  }\n                  out.print(JsonUtil.toJsonString(s, false));\n                }\n                // stop if last segment\n                if (!dirList.hasMore()) {\n                  break;\n                }\n              }\n              return null;\n            }\n          });\n        } catch (InterruptedException e) {\n          throw new IOException(e);\n        }\n        \n        out.println();\n        out.println(\"]}}\");\n        out.flush();\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "6449f524552f8c24d20b314ad21f6c579fa08e85": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4032. Specify the charset explicitly rather than rely on the default. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1431179 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 6:30 PM",
      "commitName": "6449f524552f8c24d20b314ad21f6c579fa08e85",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "04/09/12 4:48 PM",
      "commitNameOld": "f3927595cc516381b1ae568e2d883a1d89993cbb",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 127.11,
      "commitsBetweenForRepo": 647,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,27 @@\n       public void write(final OutputStream outstream) throws IOException {\n-        final PrintStream out \u003d new PrintStream(outstream);\n+        final PrintWriter out \u003d new PrintWriter(new OutputStreamWriter(\n+            outstream, Charsets.UTF_8));\n         out.println(\"{\\\"\" + FileStatus.class.getSimpleName() + \"es\\\":{\\\"\"\n             + FileStatus.class.getSimpleName() + \"\\\":[\");\n \n         final HdfsFileStatus[] partial \u003d first.getPartialListing();\n         if (partial.length \u003e 0) {\n           out.print(JsonUtil.toJsonString(partial[0], false));\n         }\n         for(int i \u003d 1; i \u003c partial.length; i++) {\n           out.println(\u0027,\u0027);\n           out.print(JsonUtil.toJsonString(partial[i], false));\n         }\n \n         for(DirectoryListing curr \u003d first; curr.hasMore(); ) { \n           curr \u003d getDirectoryListing(np, p, curr.getLastName());\n           for(HdfsFileStatus s : curr.getPartialListing()) {\n             out.println(\u0027,\u0027);\n             out.print(JsonUtil.toJsonString(s, false));\n           }\n         }\n         \n         out.println();\n         out.println(\"]}}\");\n+        out.flush();\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public void write(final OutputStream outstream) throws IOException {\n        final PrintWriter out \u003d new PrintWriter(new OutputStreamWriter(\n            outstream, Charsets.UTF_8));\n        out.println(\"{\\\"\" + FileStatus.class.getSimpleName() + \"es\\\":{\\\"\"\n            + FileStatus.class.getSimpleName() + \"\\\":[\");\n\n        final HdfsFileStatus[] partial \u003d first.getPartialListing();\n        if (partial.length \u003e 0) {\n          out.print(JsonUtil.toJsonString(partial[0], false));\n        }\n        for(int i \u003d 1; i \u003c partial.length; i++) {\n          out.println(\u0027,\u0027);\n          out.print(JsonUtil.toJsonString(partial[i], false));\n        }\n\n        for(DirectoryListing curr \u003d first; curr.hasMore(); ) { \n          curr \u003d getDirectoryListing(np, p, curr.getLastName());\n          for(HdfsFileStatus s : curr.getPartialListing()) {\n            out.println(\u0027,\u0027);\n            out.print(JsonUtil.toJsonString(s, false));\n          }\n        }\n        \n        out.println();\n        out.println(\"]}}\");\n        out.flush();\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "94c631af1fc49f5ae5881fcd5f0e80b17308d15d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2540. Webhdfs: change \"Expect: 100-continue\" to two-step write; change \"HdfsFileStatus\" and \"localName\" respectively to \"FileStatus\" and \"pathSuffix\" in JSON response.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1199396 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/11/11 11:25 AM",
      "commitName": "94c631af1fc49f5ae5881fcd5f0e80b17308d15d",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "07/11/11 12:05 PM",
      "commitNameOld": "a590b498acf1a424ffbb3a9d8849c0abb409366d",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.97,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,25 @@\n       public void write(final OutputStream outstream) throws IOException {\n         final PrintStream out \u003d new PrintStream(outstream);\n-        out.println(\"{\\\"\" + HdfsFileStatus.class.getSimpleName() + \"es\\\":{\\\"\"\n-            + HdfsFileStatus.class.getSimpleName() + \"\\\":[\");\n+        out.println(\"{\\\"\" + FileStatus.class.getSimpleName() + \"es\\\":{\\\"\"\n+            + FileStatus.class.getSimpleName() + \"\\\":[\");\n \n         final HdfsFileStatus[] partial \u003d first.getPartialListing();\n         if (partial.length \u003e 0) {\n           out.print(JsonUtil.toJsonString(partial[0], false));\n         }\n         for(int i \u003d 1; i \u003c partial.length; i++) {\n           out.println(\u0027,\u0027);\n           out.print(JsonUtil.toJsonString(partial[i], false));\n         }\n \n         for(DirectoryListing curr \u003d first; curr.hasMore(); ) { \n           curr \u003d getDirectoryListing(np, p, curr.getLastName());\n           for(HdfsFileStatus s : curr.getPartialListing()) {\n             out.println(\u0027,\u0027);\n             out.print(JsonUtil.toJsonString(s, false));\n           }\n         }\n         \n         out.println();\n         out.println(\"]}}\");\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public void write(final OutputStream outstream) throws IOException {\n        final PrintStream out \u003d new PrintStream(outstream);\n        out.println(\"{\\\"\" + FileStatus.class.getSimpleName() + \"es\\\":{\\\"\"\n            + FileStatus.class.getSimpleName() + \"\\\":[\");\n\n        final HdfsFileStatus[] partial \u003d first.getPartialListing();\n        if (partial.length \u003e 0) {\n          out.print(JsonUtil.toJsonString(partial[0], false));\n        }\n        for(int i \u003d 1; i \u003c partial.length; i++) {\n          out.println(\u0027,\u0027);\n          out.print(JsonUtil.toJsonString(partial[i], false));\n        }\n\n        for(DirectoryListing curr \u003d first; curr.hasMore(); ) { \n          curr \u003d getDirectoryListing(np, p, curr.getLastName());\n          for(HdfsFileStatus s : curr.getPartialListing()) {\n            out.println(\u0027,\u0027);\n            out.print(JsonUtil.toJsonString(s, false));\n          }\n        }\n        \n        out.println();\n        out.println(\"]}}\");\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "c46dbedaf94cb72e35e9e63d7f99e382ae9f0974": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2424. Added a root element \"HdfsFileStatuses\" for the response of webhdfs listStatus.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1183175 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/10/11 5:30 PM",
      "commitName": "c46dbedaf94cb72e35e9e63d7f99e382ae9f0974",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "13/10/11 1:41 PM",
      "commitNameOld": "7d1a5078b73c8523d63c1250812238dcc0963530",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.16,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,25 @@\n       public void write(final OutputStream outstream) throws IOException {\n         final PrintStream out \u003d new PrintStream(outstream);\n-        out.println(\"{\\\"\" + HdfsFileStatus.class.getSimpleName() + \"\\\":[\");\n+        out.println(\"{\\\"\" + HdfsFileStatus.class.getSimpleName() + \"es\\\":{\\\"\"\n+            + HdfsFileStatus.class.getSimpleName() + \"\\\":[\");\n \n         final HdfsFileStatus[] partial \u003d first.getPartialListing();\n         if (partial.length \u003e 0) {\n           out.print(JsonUtil.toJsonString(partial[0], false));\n         }\n         for(int i \u003d 1; i \u003c partial.length; i++) {\n           out.println(\u0027,\u0027);\n           out.print(JsonUtil.toJsonString(partial[i], false));\n         }\n \n         for(DirectoryListing curr \u003d first; curr.hasMore(); ) { \n           curr \u003d getDirectoryListing(np, p, curr.getLastName());\n           for(HdfsFileStatus s : curr.getPartialListing()) {\n             out.println(\u0027,\u0027);\n             out.print(JsonUtil.toJsonString(s, false));\n           }\n         }\n         \n-        out.println(\"]}\");\n+        out.println();\n+        out.println(\"]}}\");\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public void write(final OutputStream outstream) throws IOException {\n        final PrintStream out \u003d new PrintStream(outstream);\n        out.println(\"{\\\"\" + HdfsFileStatus.class.getSimpleName() + \"es\\\":{\\\"\"\n            + HdfsFileStatus.class.getSimpleName() + \"\\\":[\");\n\n        final HdfsFileStatus[] partial \u003d first.getPartialListing();\n        if (partial.length \u003e 0) {\n          out.print(JsonUtil.toJsonString(partial[0], false));\n        }\n        for(int i \u003d 1; i \u003c partial.length; i++) {\n          out.println(\u0027,\u0027);\n          out.print(JsonUtil.toJsonString(partial[i], false));\n        }\n\n        for(DirectoryListing curr \u003d first; curr.hasMore(); ) { \n          curr \u003d getDirectoryListing(np, p, curr.getLastName());\n          for(HdfsFileStatus s : curr.getPartialListing()) {\n            out.println(\u0027,\u0027);\n            out.print(JsonUtil.toJsonString(s, false));\n          }\n        }\n        \n        out.println();\n        out.println(\"]}}\");\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "676f488efffd50eb47e75cd750f9bc948b9e12fb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2404. webhdfs liststatus json response is not correct. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1180757 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/10/11 6:49 PM",
      "commitName": "676f488efffd50eb47e75cd750f9bc948b9e12fb",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "06/10/11 2:17 PM",
      "commitNameOld": "a2d7287873685249c2ceeda49f9feadfb307f5c2",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 3.19,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n       public void write(final OutputStream outstream) throws IOException {\n         final PrintStream out \u003d new PrintStream(outstream);\n-        out.println(\"{\\\"\" + HdfsFileStatus[].class.getSimpleName() + \"\\\":[\");\n+        out.println(\"{\\\"\" + HdfsFileStatus.class.getSimpleName() + \"\\\":[\");\n \n         final HdfsFileStatus[] partial \u003d first.getPartialListing();\n         if (partial.length \u003e 0) {\n-          out.print(JsonUtil.toJsonString(partial[0]));\n+          out.print(JsonUtil.toJsonString(partial[0], false));\n         }\n         for(int i \u003d 1; i \u003c partial.length; i++) {\n           out.println(\u0027,\u0027);\n-          out.print(JsonUtil.toJsonString(partial[i]));\n+          out.print(JsonUtil.toJsonString(partial[i], false));\n         }\n \n         for(DirectoryListing curr \u003d first; curr.hasMore(); ) { \n           curr \u003d getDirectoryListing(np, p, curr.getLastName());\n           for(HdfsFileStatus s : curr.getPartialListing()) {\n             out.println(\u0027,\u0027);\n-            out.print(JsonUtil.toJsonString(s));\n+            out.print(JsonUtil.toJsonString(s, false));\n           }\n         }\n         \n         out.println(\"]}\");\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public void write(final OutputStream outstream) throws IOException {\n        final PrintStream out \u003d new PrintStream(outstream);\n        out.println(\"{\\\"\" + HdfsFileStatus.class.getSimpleName() + \"\\\":[\");\n\n        final HdfsFileStatus[] partial \u003d first.getPartialListing();\n        if (partial.length \u003e 0) {\n          out.print(JsonUtil.toJsonString(partial[0], false));\n        }\n        for(int i \u003d 1; i \u003c partial.length; i++) {\n          out.println(\u0027,\u0027);\n          out.print(JsonUtil.toJsonString(partial[i], false));\n        }\n\n        for(DirectoryListing curr \u003d first; curr.hasMore(); ) { \n          curr \u003d getDirectoryListing(np, p, curr.getLastName());\n          for(HdfsFileStatus s : curr.getPartialListing()) {\n            out.println(\u0027,\u0027);\n            out.print(JsonUtil.toJsonString(s, false));\n          }\n        }\n        \n        out.println(\"]}\");\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "1b1016beeb716bef8dad93bb2c7c4631a14b3d57": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2395. Add a root element in the JSON responses of webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/10/11 4:29 AM",
      "commitName": "1b1016beeb716bef8dad93bb2c7c4631a14b3d57",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "30/09/11 9:49 PM",
      "commitNameOld": "dc8464f943b61b795df0cc8baec171bf07355763",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.28,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n       public void write(final OutputStream outstream) throws IOException {\n         final PrintStream out \u003d new PrintStream(outstream);\n-        out.print(\u0027[\u0027);\n+        out.println(\"{\\\"\" + HdfsFileStatus[].class.getSimpleName() + \"\\\":[\");\n \n         final HdfsFileStatus[] partial \u003d first.getPartialListing();\n         if (partial.length \u003e 0) {\n           out.print(JsonUtil.toJsonString(partial[0]));\n         }\n         for(int i \u003d 1; i \u003c partial.length; i++) {\n           out.println(\u0027,\u0027);\n           out.print(JsonUtil.toJsonString(partial[i]));\n         }\n \n         for(DirectoryListing curr \u003d first; curr.hasMore(); ) { \n           curr \u003d getDirectoryListing(np, p, curr.getLastName());\n           for(HdfsFileStatus s : curr.getPartialListing()) {\n             out.println(\u0027,\u0027);\n             out.print(JsonUtil.toJsonString(s));\n           }\n         }\n         \n-        out.println(\u0027]\u0027);\n+        out.println(\"]}\");\n       }\n\\ No newline at end of file\n",
      "actualSource": "      public void write(final OutputStream outstream) throws IOException {\n        final PrintStream out \u003d new PrintStream(outstream);\n        out.println(\"{\\\"\" + HdfsFileStatus[].class.getSimpleName() + \"\\\":[\");\n\n        final HdfsFileStatus[] partial \u003d first.getPartialListing();\n        if (partial.length \u003e 0) {\n          out.print(JsonUtil.toJsonString(partial[0]));\n        }\n        for(int i \u003d 1; i \u003c partial.length; i++) {\n          out.println(\u0027,\u0027);\n          out.print(JsonUtil.toJsonString(partial[i]));\n        }\n\n        for(DirectoryListing curr \u003d first; curr.hasMore(); ) { \n          curr \u003d getDirectoryListing(np, p, curr.getLastName());\n          for(HdfsFileStatus s : curr.getPartialListing()) {\n            out.println(\u0027,\u0027);\n            out.print(JsonUtil.toJsonString(s));\n          }\n        }\n        \n        out.println(\"]}\");\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
      "extendedDetails": {}
    },
    "6c3b59505b863f03629da52a1e9b886fe9b496d0": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2317. Support read access to HDFS in webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1170085 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/09/11 1:34 AM",
      "commitName": "6c3b59505b863f03629da52a1e9b886fe9b496d0",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,23 @@\n+      public void write(final OutputStream outstream) throws IOException {\n+        final PrintStream out \u003d new PrintStream(outstream);\n+        out.print(\u0027[\u0027);\n+\n+        final HdfsFileStatus[] partial \u003d first.getPartialListing();\n+        if (partial.length \u003e 0) {\n+          out.print(JsonUtil.toJsonString(partial[0]));\n+        }\n+        for(int i \u003d 1; i \u003c partial.length; i++) {\n+          out.println(\u0027,\u0027);\n+          out.print(JsonUtil.toJsonString(partial[i]));\n+        }\n+\n+        for(DirectoryListing curr \u003d first; curr.hasMore(); ) { \n+          curr \u003d getDirectoryListing(np, p, curr.getLastName());\n+          for(HdfsFileStatus s : curr.getPartialListing()) {\n+            out.println(\u0027,\u0027);\n+            out.print(JsonUtil.toJsonString(s));\n+          }\n+        }\n+        \n+        out.println(\u0027]\u0027);\n+      }\n\\ No newline at end of file\n",
      "actualSource": "      public void write(final OutputStream outstream) throws IOException {\n        final PrintStream out \u003d new PrintStream(outstream);\n        out.print(\u0027[\u0027);\n\n        final HdfsFileStatus[] partial \u003d first.getPartialListing();\n        if (partial.length \u003e 0) {\n          out.print(JsonUtil.toJsonString(partial[0]));\n        }\n        for(int i \u003d 1; i \u003c partial.length; i++) {\n          out.println(\u0027,\u0027);\n          out.print(JsonUtil.toJsonString(partial[i]));\n        }\n\n        for(DirectoryListing curr \u003d first; curr.hasMore(); ) { \n          curr \u003d getDirectoryListing(np, p, curr.getLastName());\n          for(HdfsFileStatus s : curr.getPartialListing()) {\n            out.println(\u0027,\u0027);\n            out.print(JsonUtil.toJsonString(s));\n          }\n        }\n        \n        out.println(\u0027]\u0027);\n      }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java"
    }
  }
}