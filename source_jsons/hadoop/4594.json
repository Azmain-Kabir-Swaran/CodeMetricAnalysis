{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ClientNamenodeProtocolServerSideTranslatorPB.java",
  "functionName": "addBlock",
  "functionId": "addBlock___controller-RpcController__req-AddBlockRequestProto",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
  "functionStartLine": 585,
  "functionEndLine": 607,
  "numCommitsSeen": 141,
  "timeTaken": 4379,
  "changeHistory": [
    "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7",
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "e2c9b288b223b9fd82dc12018936e13128413492",
    "f05c21285ef23b6a973d69f045b1cb46c5abc039",
    "5d2ffde68e2c14ee33fa2ba4a34cb42fbd14b5ec",
    "4525c4a25ba90163c9543116e2bd54239e0dd097",
    "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e",
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
    "b5229fd19bfecc2e5249db652ad34ec08152334b",
    "3001a172c8868763f8e59e866e36f7f50dee62cc",
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e",
    "48da033901d3471ef176a94104158546152353e9"
  ],
  "changeHistoryShort": {
    "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7": "Ybodychange",
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ybodychange",
    "e2c9b288b223b9fd82dc12018936e13128413492": "Ybodychange",
    "f05c21285ef23b6a973d69f045b1cb46c5abc039": "Ybodychange",
    "5d2ffde68e2c14ee33fa2ba4a34cb42fbd14b5ec": "Ybodychange",
    "4525c4a25ba90163c9543116e2bd54239e0dd097": "Ybodychange",
    "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e": "Ybodychange",
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c": "Ybodychange",
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256": "Ybodychange",
    "b5229fd19bfecc2e5249db652ad34ec08152334b": "Ybodychange",
    "3001a172c8868763f8e59e866e36f7f50dee62cc": "Ybodychange",
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e": "Ybodychange",
    "48da033901d3471ef176a94104158546152353e9": "Yintroduced"
  },
  "changeHistoryDetails": {
    "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3702. Add an option for NOT writing the blocks locally if there is a datanode on the same box as the client. (Contributed by Lei (Eddy) Xu)\n",
      "commitDate": "27/04/16 2:22 PM",
      "commitName": "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "02/03/16 6:35 PM",
      "commitNameOld": "27941a1811831e0f2144a2f463d807755cd850b2",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 55.78,
      "commitsBetweenForRepo": 329,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,23 @@\n   public AddBlockResponseProto addBlock(RpcController controller,\n       AddBlockRequestProto req) throws ServiceException {\n     \n     try {\n       List\u003cDatanodeInfoProto\u003e excl \u003d req.getExcludeNodesList();\n       List\u003cString\u003e favor \u003d req.getFavoredNodesList();\n+      EnumSet\u003cAddBlockFlag\u003e flags \u003d\n+          PBHelperClient.convertAddBlockFlags(req.getFlagsList());\n       LocatedBlock result \u003d server.addBlock(\n           req.getSrc(),\n           req.getClientName(),\n           req.hasPrevious() ? PBHelperClient.convert(req.getPrevious()) : null,\n           (excl \u003d\u003d null || excl.size() \u003d\u003d 0) ? null : PBHelperClient.convert(excl\n               .toArray(new DatanodeInfoProto[excl.size()])), req.getFileId(),\n           (favor \u003d\u003d null || favor.size() \u003d\u003d 0) ? null : favor\n-              .toArray(new String[favor.size()]));\n+              .toArray(new String[favor.size()]),\n+          flags);\n       return AddBlockResponseProto.newBuilder()\n           .setBlock(PBHelperClient.convertLocatedBlock(result)).build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public AddBlockResponseProto addBlock(RpcController controller,\n      AddBlockRequestProto req) throws ServiceException {\n    \n    try {\n      List\u003cDatanodeInfoProto\u003e excl \u003d req.getExcludeNodesList();\n      List\u003cString\u003e favor \u003d req.getFavoredNodesList();\n      EnumSet\u003cAddBlockFlag\u003e flags \u003d\n          PBHelperClient.convertAddBlockFlags(req.getFlagsList());\n      LocatedBlock result \u003d server.addBlock(\n          req.getSrc(),\n          req.getClientName(),\n          req.hasPrevious() ? PBHelperClient.convert(req.getPrevious()) : null,\n          (excl \u003d\u003d null || excl.size() \u003d\u003d 0) ? null : PBHelperClient.convert(excl\n              .toArray(new DatanodeInfoProto[excl.size()])), req.getFileId(),\n          (favor \u003d\u003d null || favor.size() \u003d\u003d 0) ? null : favor\n              .toArray(new String[favor.size()]),\n          flags);\n      return AddBlockResponseProto.newBuilder()\n          .setBlock(PBHelperClient.convertLocatedBlock(result)).build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/08/15 2:38 PM",
      "commitNameOld": "e2c9b288b223b9fd82dc12018936e13128413492",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 24.18,
      "commitsBetweenForRepo": 143,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n   public AddBlockResponseProto addBlock(RpcController controller,\n       AddBlockRequestProto req) throws ServiceException {\n     \n     try {\n       List\u003cDatanodeInfoProto\u003e excl \u003d req.getExcludeNodesList();\n       List\u003cString\u003e favor \u003d req.getFavoredNodesList();\n       LocatedBlock result \u003d server.addBlock(\n           req.getSrc(),\n           req.getClientName(),\n           req.hasPrevious() ? PBHelperClient.convert(req.getPrevious()) : null,\n-          (excl \u003d\u003d null || excl.size() \u003d\u003d 0) ? null : PBHelper.convert(excl\n+          (excl \u003d\u003d null || excl.size() \u003d\u003d 0) ? null : PBHelperClient.convert(excl\n               .toArray(new DatanodeInfoProto[excl.size()])), req.getFileId(),\n           (favor \u003d\u003d null || favor.size() \u003d\u003d 0) ? null : favor\n               .toArray(new String[favor.size()]));\n       return AddBlockResponseProto.newBuilder()\n-          .setBlock(PBHelper.convert(result)).build();\n+          .setBlock(PBHelperClient.convert(result)).build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public AddBlockResponseProto addBlock(RpcController controller,\n      AddBlockRequestProto req) throws ServiceException {\n    \n    try {\n      List\u003cDatanodeInfoProto\u003e excl \u003d req.getExcludeNodesList();\n      List\u003cString\u003e favor \u003d req.getFavoredNodesList();\n      LocatedBlock result \u003d server.addBlock(\n          req.getSrc(),\n          req.getClientName(),\n          req.hasPrevious() ? PBHelperClient.convert(req.getPrevious()) : null,\n          (excl \u003d\u003d null || excl.size() \u003d\u003d 0) ? null : PBHelperClient.convert(excl\n              .toArray(new DatanodeInfoProto[excl.size()])), req.getFileId(),\n          (favor \u003d\u003d null || favor.size() \u003d\u003d 0) ? null : favor\n              .toArray(new String[favor.size()]));\n      return AddBlockResponseProto.newBuilder()\n          .setBlock(PBHelperClient.convert(result)).build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "e2c9b288b223b9fd82dc12018936e13128413492": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "28/08/15 2:38 PM",
      "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "22/08/15 1:31 PM",
      "commitNameOld": "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 6.05,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n   public AddBlockResponseProto addBlock(RpcController controller,\n       AddBlockRequestProto req) throws ServiceException {\n     \n     try {\n       List\u003cDatanodeInfoProto\u003e excl \u003d req.getExcludeNodesList();\n       List\u003cString\u003e favor \u003d req.getFavoredNodesList();\n       LocatedBlock result \u003d server.addBlock(\n           req.getSrc(),\n           req.getClientName(),\n-          req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null,\n+          req.hasPrevious() ? PBHelperClient.convert(req.getPrevious()) : null,\n           (excl \u003d\u003d null || excl.size() \u003d\u003d 0) ? null : PBHelper.convert(excl\n               .toArray(new DatanodeInfoProto[excl.size()])), req.getFileId(),\n           (favor \u003d\u003d null || favor.size() \u003d\u003d 0) ? null : favor\n               .toArray(new String[favor.size()]));\n       return AddBlockResponseProto.newBuilder()\n           .setBlock(PBHelper.convert(result)).build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public AddBlockResponseProto addBlock(RpcController controller,\n      AddBlockRequestProto req) throws ServiceException {\n    \n    try {\n      List\u003cDatanodeInfoProto\u003e excl \u003d req.getExcludeNodesList();\n      List\u003cString\u003e favor \u003d req.getFavoredNodesList();\n      LocatedBlock result \u003d server.addBlock(\n          req.getSrc(),\n          req.getClientName(),\n          req.hasPrevious() ? PBHelperClient.convert(req.getPrevious()) : null,\n          (excl \u003d\u003d null || excl.size() \u003d\u003d 0) ? null : PBHelper.convert(excl\n              .toArray(new DatanodeInfoProto[excl.size()])), req.getFileId(),\n          (favor \u003d\u003d null || favor.size() \u003d\u003d 0) ? null : favor\n              .toArray(new String[favor.size()]));\n      return AddBlockResponseProto.newBuilder()\n          .setBlock(PBHelper.convert(result)).build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "f05c21285ef23b6a973d69f045b1cb46c5abc039": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7853. Erasure coding: extend LocatedBlocks to support reading from striped files. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:32 AM",
      "commitName": "f05c21285ef23b6a973d69f045b1cb46c5abc039",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 24.06,
      "commitsBetweenForRepo": 279,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n   public AddBlockResponseProto addBlock(RpcController controller,\n       AddBlockRequestProto req) throws ServiceException {\n     \n     try {\n       List\u003cDatanodeInfoProto\u003e excl \u003d req.getExcludeNodesList();\n       List\u003cString\u003e favor \u003d req.getFavoredNodesList();\n       LocatedBlock result \u003d server.addBlock(\n           req.getSrc(),\n           req.getClientName(),\n           req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null,\n           (excl \u003d\u003d null || excl.size() \u003d\u003d 0) ? null : PBHelper.convert(excl\n               .toArray(new DatanodeInfoProto[excl.size()])), req.getFileId(),\n           (favor \u003d\u003d null || favor.size() \u003d\u003d 0) ? null : favor\n               .toArray(new String[favor.size()]));\n       return AddBlockResponseProto.newBuilder()\n-          .setBlock(PBHelper.convert(result)).build();\n+          .setBlock(PBHelper.convertLocatedBlock(result)).build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public AddBlockResponseProto addBlock(RpcController controller,\n      AddBlockRequestProto req) throws ServiceException {\n    \n    try {\n      List\u003cDatanodeInfoProto\u003e excl \u003d req.getExcludeNodesList();\n      List\u003cString\u003e favor \u003d req.getFavoredNodesList();\n      LocatedBlock result \u003d server.addBlock(\n          req.getSrc(),\n          req.getClientName(),\n          req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null,\n          (excl \u003d\u003d null || excl.size() \u003d\u003d 0) ? null : PBHelper.convert(excl\n              .toArray(new DatanodeInfoProto[excl.size()])), req.getFileId(),\n          (favor \u003d\u003d null || favor.size() \u003d\u003d 0) ? null : favor\n              .toArray(new String[favor.size()]));\n      return AddBlockResponseProto.newBuilder()\n          .setBlock(PBHelper.convertLocatedBlock(result)).build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "5d2ffde68e2c14ee33fa2ba4a34cb42fbd14b5ec": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2576. Enhances the DistributedFileSystem\u0027s create API so that clients can specify favored datanodes for a file\u0027s blocks. Contributed by Devaraj Das and Pritam Damania.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1476395 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/04/13 1:39 PM",
      "commitName": "5d2ffde68e2c14ee33fa2ba4a34cb42fbd14b5ec",
      "commitAuthor": "Devaraj Das",
      "commitDateOld": "07/04/13 11:01 AM",
      "commitNameOld": "c5bb615317f1aa8d3cba4cf331f732126655b68e",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 19.11,
      "commitsBetweenForRepo": 105,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,20 @@\n   public AddBlockResponseProto addBlock(RpcController controller,\n       AddBlockRequestProto req) throws ServiceException {\n     \n     try {\n       List\u003cDatanodeInfoProto\u003e excl \u003d req.getExcludeNodesList();\n+      List\u003cString\u003e favor \u003d req.getFavoredNodesList();\n       LocatedBlock result \u003d server.addBlock(\n           req.getSrc(),\n           req.getClientName(),\n           req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null,\n           (excl \u003d\u003d null || excl.size() \u003d\u003d 0) ? null : PBHelper.convert(excl\n-              .toArray(new DatanodeInfoProto[excl.size()])), req.getFileId());\n+              .toArray(new DatanodeInfoProto[excl.size()])), req.getFileId(),\n+          (favor \u003d\u003d null || favor.size() \u003d\u003d 0) ? null : favor\n+              .toArray(new String[favor.size()]));\n       return AddBlockResponseProto.newBuilder()\n           .setBlock(PBHelper.convert(result)).build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public AddBlockResponseProto addBlock(RpcController controller,\n      AddBlockRequestProto req) throws ServiceException {\n    \n    try {\n      List\u003cDatanodeInfoProto\u003e excl \u003d req.getExcludeNodesList();\n      List\u003cString\u003e favor \u003d req.getFavoredNodesList();\n      LocatedBlock result \u003d server.addBlock(\n          req.getSrc(),\n          req.getClientName(),\n          req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null,\n          (excl \u003d\u003d null || excl.size() \u003d\u003d 0) ? null : PBHelper.convert(excl\n              .toArray(new DatanodeInfoProto[excl.size()])), req.getFileId(),\n          (favor \u003d\u003d null || favor.size() \u003d\u003d 0) ? null : favor\n              .toArray(new String[favor.size()]));\n      return AddBlockResponseProto.newBuilder()\n          .setBlock(PBHelper.convert(result)).build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "4525c4a25ba90163c9543116e2bd54239e0dd097": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4340. Update addBlock() to inculde inode id as additional argument. Contributed Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1443169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/02/13 11:52 AM",
      "commitName": "4525c4a25ba90163c9543116e2bd54239e0dd097",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "17/01/13 10:21 AM",
      "commitNameOld": "8f70a25b1c5df498c441a9b3475a8ada5a92111f",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 20.06,
      "commitsBetweenForRepo": 88,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,17 @@\n   public AddBlockResponseProto addBlock(RpcController controller,\n       AddBlockRequestProto req) throws ServiceException {\n     \n     try {\n       List\u003cDatanodeInfoProto\u003e excl \u003d req.getExcludeNodesList();\n-      LocatedBlock result \u003d server.addBlock(req.getSrc(), req.getClientName(),\n+      LocatedBlock result \u003d server.addBlock(\n+          req.getSrc(),\n+          req.getClientName(),\n           req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null,\n-          (excl \u003d\u003d null || \n-           excl.size() \u003d\u003d 0) ? null : \n-            PBHelper.convert(excl.toArray(new DatanodeInfoProto[excl.size()])));\n-      return AddBlockResponseProto.newBuilder().setBlock(\n-          PBHelper.convert(result)).build();\n+          (excl \u003d\u003d null || excl.size() \u003d\u003d 0) ? null : PBHelper.convert(excl\n+              .toArray(new DatanodeInfoProto[excl.size()])), req.getFileId());\n+      return AddBlockResponseProto.newBuilder()\n+          .setBlock(PBHelper.convert(result)).build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public AddBlockResponseProto addBlock(RpcController controller,\n      AddBlockRequestProto req) throws ServiceException {\n    \n    try {\n      List\u003cDatanodeInfoProto\u003e excl \u003d req.getExcludeNodesList();\n      LocatedBlock result \u003d server.addBlock(\n          req.getSrc(),\n          req.getClientName(),\n          req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null,\n          (excl \u003d\u003d null || excl.size() \u003d\u003d 0) ? null : PBHelper.convert(excl\n              .toArray(new DatanodeInfoProto[excl.size()])), req.getFileId());\n      return AddBlockResponseProto.newBuilder()\n          .setBlock(PBHelper.convert(result)).build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e": {
      "type": "Ybodychange",
      "commitMessage": "    HDFS-2669 Enable protobuf rpc for ClientNamenodeProtocol\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1214128 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/12/11 1:27 AM",
      "commitName": "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e",
      "commitAuthor": "Sanjay Radia",
      "commitDateOld": "13/12/11 3:31 PM",
      "commitNameOld": "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.41,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,16 @@\n   public AddBlockResponseProto addBlock(RpcController controller,\n       AddBlockRequestProto req) throws ServiceException {\n+    \n     try {\n+      List\u003cDatanodeInfoProto\u003e excl \u003d req.getExcludeNodesList();\n+      LocatedBlock result \u003d server.addBlock(req.getSrc(), req.getClientName(),\n+          req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null,\n+          (excl \u003d\u003d null || \n+           excl.size() \u003d\u003d 0) ? null : \n+            PBHelper.convert(excl.toArray(new DatanodeInfoProto[excl.size()])));\n       return AddBlockResponseProto.newBuilder().setBlock(\n-          PBHelper.convert(\n-          server.addBlock(req.getSrc(), req.getClientName(), \n-                req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null, \n-                PBHelper.convert(\n-                  (DatanodeInfoProto[]) req.getExcludeNodesList().toArray()))))\n-           .build();\n+          PBHelper.convert(result)).build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public AddBlockResponseProto addBlock(RpcController controller,\n      AddBlockRequestProto req) throws ServiceException {\n    \n    try {\n      List\u003cDatanodeInfoProto\u003e excl \u003d req.getExcludeNodesList();\n      LocatedBlock result \u003d server.addBlock(req.getSrc(), req.getClientName(),\n          req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null,\n          (excl \u003d\u003d null || \n           excl.size() \u003d\u003d 0) ? null : \n            PBHelper.convert(excl.toArray(new DatanodeInfoProto[excl.size()])));\n      return AddBlockResponseProto.newBuilder().setBlock(\n          PBHelper.convert(result)).build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Optional protobuf parameters are not handled correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213985 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:31 PM",
      "commitName": "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:27 PM",
      "commitNameOld": "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   public AddBlockResponseProto addBlock(RpcController controller,\n       AddBlockRequestProto req) throws ServiceException {\n     try {\n       return AddBlockResponseProto.newBuilder().setBlock(\n           PBHelper.convert(\n           server.addBlock(req.getSrc(), req.getClientName(), \n-                PBHelper.convert(req.getPrevious()), \n+                req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null, \n                 PBHelper.convert(\n                   (DatanodeInfoProto[]) req.getExcludeNodesList().toArray()))))\n            .build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public AddBlockResponseProto addBlock(RpcController controller,\n      AddBlockRequestProto req) throws ServiceException {\n    try {\n      return AddBlockResponseProto.newBuilder().setBlock(\n          PBHelper.convert(\n          server.addBlock(req.getSrc(), req.getClientName(), \n                req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null, \n                PBHelper.convert(\n                  (DatanodeInfoProto[]) req.getExcludeNodesList().toArray()))))\n           .build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256": {
      "type": "Ybodychange",
      "commitMessage": "Reverting the patch r1213981\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213984 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:27 PM",
      "commitName": "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:22 PM",
      "commitNameOld": "b5229fd19bfecc2e5249db652ad34ec08152334b",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   public AddBlockResponseProto addBlock(RpcController controller,\n       AddBlockRequestProto req) throws ServiceException {\n     try {\n       return AddBlockResponseProto.newBuilder().setBlock(\n           PBHelper.convert(\n           server.addBlock(req.getSrc(), req.getClientName(), \n-                req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null, \n+                PBHelper.convert(req.getPrevious()), \n                 PBHelper.convert(\n                   (DatanodeInfoProto[]) req.getExcludeNodesList().toArray()))))\n            .build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public AddBlockResponseProto addBlock(RpcController controller,\n      AddBlockRequestProto req) throws ServiceException {\n    try {\n      return AddBlockResponseProto.newBuilder().setBlock(\n          PBHelper.convert(\n          server.addBlock(req.getSrc(), req.getClientName(), \n                PBHelper.convert(req.getPrevious()), \n                PBHelper.convert(\n                  (DatanodeInfoProto[]) req.getExcludeNodesList().toArray()))))\n           .build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "b5229fd19bfecc2e5249db652ad34ec08152334b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Optional protobuf parameters are not handled correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213981 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:22 PM",
      "commitName": "b5229fd19bfecc2e5249db652ad34ec08152334b",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:17 PM",
      "commitNameOld": "3001a172c8868763f8e59e866e36f7f50dee62cc",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   public AddBlockResponseProto addBlock(RpcController controller,\n       AddBlockRequestProto req) throws ServiceException {\n     try {\n       return AddBlockResponseProto.newBuilder().setBlock(\n           PBHelper.convert(\n           server.addBlock(req.getSrc(), req.getClientName(), \n-                PBHelper.convert(req.getPrevious()), \n+                req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null, \n                 PBHelper.convert(\n                   (DatanodeInfoProto[]) req.getExcludeNodesList().toArray()))))\n            .build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public AddBlockResponseProto addBlock(RpcController controller,\n      AddBlockRequestProto req) throws ServiceException {\n    try {\n      return AddBlockResponseProto.newBuilder().setBlock(\n          PBHelper.convert(\n          server.addBlock(req.getSrc(), req.getClientName(), \n                req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null, \n                PBHelper.convert(\n                  (DatanodeInfoProto[]) req.getExcludeNodesList().toArray()))))\n           .build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "3001a172c8868763f8e59e866e36f7f50dee62cc": {
      "type": "Ybodychange",
      "commitMessage": "Reverting r1213512 because it committed changes that were not part of the patch.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213980 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:17 PM",
      "commitName": "3001a172c8868763f8e59e866e36f7f50dee62cc",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "12/12/11 4:21 PM",
      "commitNameOld": "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.96,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   public AddBlockResponseProto addBlock(RpcController controller,\n       AddBlockRequestProto req) throws ServiceException {\n     try {\n       return AddBlockResponseProto.newBuilder().setBlock(\n           PBHelper.convert(\n           server.addBlock(req.getSrc(), req.getClientName(), \n-                req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null, \n+                PBHelper.convert(req.getPrevious()), \n                 PBHelper.convert(\n                   (DatanodeInfoProto[]) req.getExcludeNodesList().toArray()))))\n            .build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public AddBlockResponseProto addBlock(RpcController controller,\n      AddBlockRequestProto req) throws ServiceException {\n    try {\n      return AddBlockResponseProto.newBuilder().setBlock(\n          PBHelper.convert(\n          server.addBlock(req.getSrc(), req.getClientName(), \n                PBHelper.convert(req.getPrevious()), \n                PBHelper.convert(\n                  (DatanodeInfoProto[]) req.getExcludeNodesList().toArray()))))\n           .build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Handle protobuf optional parameters correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213512 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/12/11 4:21 PM",
      "commitName": "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "11/12/11 9:36 PM",
      "commitNameOld": "48da033901d3471ef176a94104158546152353e9",
      "commitAuthorOld": "Sanjay Radia",
      "daysBetweenCommits": 0.78,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   public AddBlockResponseProto addBlock(RpcController controller,\n       AddBlockRequestProto req) throws ServiceException {\n     try {\n       return AddBlockResponseProto.newBuilder().setBlock(\n           PBHelper.convert(\n           server.addBlock(req.getSrc(), req.getClientName(), \n-                PBHelper.convert(req.getPrevious()), \n+                req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null, \n                 PBHelper.convert(\n                   (DatanodeInfoProto[]) req.getExcludeNodesList().toArray()))))\n            .build();\n     } catch (IOException e) {\n       throw new ServiceException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public AddBlockResponseProto addBlock(RpcController controller,\n      AddBlockRequestProto req) throws ServiceException {\n    try {\n      return AddBlockResponseProto.newBuilder().setBlock(\n          PBHelper.convert(\n          server.addBlock(req.getSrc(), req.getClientName(), \n                req.hasPrevious() ? PBHelper.convert(req.getPrevious()) : null, \n                PBHelper.convert(\n                  (DatanodeInfoProto[]) req.getExcludeNodesList().toArray()))))\n           .build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java",
      "extendedDetails": {}
    },
    "48da033901d3471ef176a94104158546152353e9": {
      "type": "Yintroduced",
      "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/12/11 9:36 PM",
      "commitName": "48da033901d3471ef176a94104158546152353e9",
      "commitAuthor": "Sanjay Radia",
      "diff": "@@ -0,0 +1,14 @@\n+  public AddBlockResponseProto addBlock(RpcController controller,\n+      AddBlockRequestProto req) throws ServiceException {\n+    try {\n+      return AddBlockResponseProto.newBuilder().setBlock(\n+          PBHelper.convert(\n+          server.addBlock(req.getSrc(), req.getClientName(), \n+                PBHelper.convert(req.getPrevious()), \n+                PBHelper.convert(\n+                  (DatanodeInfoProto[]) req.getExcludeNodesList().toArray()))))\n+           .build();\n+    } catch (IOException e) {\n+      throw new ServiceException(e);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public AddBlockResponseProto addBlock(RpcController controller,\n      AddBlockRequestProto req) throws ServiceException {\n    try {\n      return AddBlockResponseProto.newBuilder().setBlock(\n          PBHelper.convert(\n          server.addBlock(req.getSrc(), req.getClientName(), \n                PBHelper.convert(req.getPrevious()), \n                PBHelper.convert(\n                  (DatanodeInfoProto[]) req.getExcludeNodesList().toArray()))))\n           .build();\n    } catch (IOException e) {\n      throw new ServiceException(e);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolServerSideTranslatorPB.java"
    }
  }
}