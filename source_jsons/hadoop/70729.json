{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "AzureNativeFileSystemStore.java",
  "functionName": "rename",
  "functionId": "rename___srcKey-String__dstKey-String__acquireLease-boolean__existingLease-SelfRenewingLease",
  "sourceFilePath": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
  "functionStartLine": 2719,
  "functionEndLine": 2722,
  "numCommitsSeen": 56,
  "timeTaken": 4716,
  "changeHistory": [
    "52babbb4a0e3c89f2025bf6e9a1b51a96e8f8fb0",
    "f92913c35bfda0d565606f9fb9a301ddd4105fd8",
    "47641fcbc9c41f4a338d8899501e4a310d2e81ad",
    "5f6edb30c2bb648d5564c951edc25645e17e6636",
    "94e7d49a6dab7e7f4e873dcca67e7fcc98e7e1f8",
    "7a346bcb4fa5b56191ed00a39e72e51c9bdf1b56",
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40",
    "81bc395deb3ba00567dc067d6ca71bacf9e3bc82"
  ],
  "changeHistoryShort": {
    "52babbb4a0e3c89f2025bf6e9a1b51a96e8f8fb0": "Ybodychange",
    "f92913c35bfda0d565606f9fb9a301ddd4105fd8": "Ybodychange",
    "47641fcbc9c41f4a338d8899501e4a310d2e81ad": "Ybodychange",
    "5f6edb30c2bb648d5564c951edc25645e17e6636": "Ybodychange",
    "94e7d49a6dab7e7f4e873dcca67e7fcc98e7e1f8": "Ybodychange",
    "7a346bcb4fa5b56191ed00a39e72e51c9bdf1b56": "Ybodychange",
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40": "Ymultichange(Yparameterchange,Ybodychange)",
    "81bc395deb3ba00567dc067d6ca71bacf9e3bc82": "Yintroduced"
  },
  "changeHistoryDetails": {
    "52babbb4a0e3c89f2025bf6e9a1b51a96e8f8fb0": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15086. NativeAzureFileSystem file rename is not atomic.\nContributed by Thomas Marquardt\n",
      "commitDate": "22/12/17 3:39 AM",
      "commitName": "52babbb4a0e3c89f2025bf6e9a1b51a96e8f8fb0",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "15/09/17 9:03 AM",
      "commitNameOld": "2d2d97fa7d4224369b3c13bc4a45e8cc9e29afb1",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 97.82,
      "commitsBetweenForRepo": 765,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,129 +1,4 @@\n   public void rename(String srcKey, String dstKey, boolean acquireLease,\n-      SelfRenewingLease existingLease) throws IOException {\n-\n-    LOG.debug(\"Moving {} to {}\", srcKey, dstKey);\n-\n-    if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n-      throw new IOException(\"Cannot acquire new lease if one already exists.\");\n-    }\n-\n-    CloudBlobWrapper srcBlob \u003d null;\n-    CloudBlobWrapper dstBlob \u003d null;\n-    SelfRenewingLease lease \u003d null;\n-    try {\n-      // Attempts rename may occur before opening any streams so first,\n-      // check if a session exists, if not create a session with the Azure\n-      // storage server.\n-      if (null \u003d\u003d storageInteractionLayer) {\n-        final String errMsg \u003d String.format(\n-            \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n-            sessionUri);\n-        throw new AssertionError(errMsg);\n-      }\n-\n-      checkContainer(ContainerAccessType.ReadThenWrite);\n-      // Get the source blob and assert its existence. If the source key\n-      // needs to be normalized then normalize it.\n-      //\n-\n-      srcBlob \u003d getBlobReference(srcKey);\n-      if (!srcBlob.exists(getInstrumentedContext())) {\n-        throw new AzureException(\"Source blob \" + srcKey + \" does not exist.\");\n-      }\n-\n-      /**\n-       * Conditionally get a lease on the source blob to prevent other writers\n-       * from changing it. This is used for correctness in HBase when log files\n-       * are renamed. It generally should do no harm other than take a little\n-       * more time for other rename scenarios. When the HBase master renames a\n-       * log file folder, the lease locks out other writers.  This\n-       * prevents a region server that the master thinks is dead, but is still\n-       * alive, from committing additional updates.  This is different than\n-       * when HBase runs on HDFS, where the region server recovers the lease\n-       * on a log file, to gain exclusive access to it, before it splits it.\n-       */\n-      if (acquireLease) {\n-        lease \u003d srcBlob.acquireLease();\n-      } else if (existingLease !\u003d null) {\n-        lease \u003d existingLease;\n-      }\n-\n-      // Get the destination blob. The destination key always needs to be\n-      // normalized.\n-      //\n-      dstBlob \u003d getBlobReference(dstKey);\n-\n-      // Rename the source blob to the destination blob by copying it to\n-      // the destination blob then deleting it.\n-      //\n-      // Copy blob operation in Azure storage is very costly. It will be highly\n-      // likely throttled during Azure storage gc. Short term fix will be using\n-      // a more intensive exponential retry policy when the cluster is getting\n-      // throttled.\n-      try {\n-        dstBlob.startCopyFromBlob(srcBlob, null, getInstrumentedContext());\n-      } catch (StorageException se) {\n-        if (se.getHttpStatusCode() \u003d\u003d HttpURLConnection.HTTP_UNAVAILABLE) {\n-          int copyBlobMinBackoff \u003d sessionConfiguration.getInt(\n-            KEY_COPYBLOB_MIN_BACKOFF_INTERVAL,\n-            DEFAULT_COPYBLOB_MIN_BACKOFF_INTERVAL);\n-\n-          int copyBlobMaxBackoff \u003d sessionConfiguration.getInt(\n-            KEY_COPYBLOB_MAX_BACKOFF_INTERVAL,\n-            DEFAULT_COPYBLOB_MAX_BACKOFF_INTERVAL);\n-\n-          int copyBlobDeltaBackoff \u003d sessionConfiguration.getInt(\n-            KEY_COPYBLOB_BACKOFF_INTERVAL,\n-            DEFAULT_COPYBLOB_BACKOFF_INTERVAL);\n-\n-          int copyBlobMaxRetries \u003d sessionConfiguration.getInt(\n-            KEY_COPYBLOB_MAX_IO_RETRIES,\n-            DEFAULT_COPYBLOB_MAX_RETRY_ATTEMPTS);\n-\n-          BlobRequestOptions options \u003d new BlobRequestOptions();\n-          options.setRetryPolicyFactory(new RetryExponentialRetry(\n-            copyBlobMinBackoff, copyBlobDeltaBackoff, copyBlobMaxBackoff,\n-            copyBlobMaxRetries));\n-          dstBlob.startCopyFromBlob(srcBlob, options, getInstrumentedContext());\n-        } else {\n-          throw se;\n-        }\n-      }\n-      waitForCopyToComplete(dstBlob, getInstrumentedContext());\n-      safeDelete(srcBlob, lease);\n-    } catch (StorageException e) {\n-      if (e.getHttpStatusCode() \u003d\u003d HttpURLConnection.HTTP_UNAVAILABLE) {\n-        LOG.warn(\"Rename: CopyBlob: StorageException: ServerBusy: Retry complete, will attempt client side copy for page blob\");\n-        InputStream ipStream \u003d null;\n-        OutputStream opStream \u003d null;\n-        try {\n-          if (srcBlob.getProperties().getBlobType() \u003d\u003d BlobType.PAGE_BLOB){\n-            ipStream \u003d openInputStream(srcBlob);\n-            opStream \u003d openOutputStream(dstBlob);\n-            byte[] buffer \u003d new byte[PageBlobFormatHelpers.PAGE_SIZE];\n-            int len;\n-            while ((len \u003d ipStream.read(buffer)) !\u003d -1) {\n-              opStream.write(buffer, 0, len);\n-            }\n-            opStream.flush();\n-            opStream.close();\n-            ipStream.close();\n-          } else {\n-            throw new AzureException(e);\n-          }\n-          safeDelete(srcBlob, lease);\n-        } catch(StorageException se) {\n-          LOG.warn(\"Rename: CopyBlob: StorageException: Failed\");\n-          throw new AzureException(se);\n-        } finally {\n-          IOUtils.closeStream(ipStream);\n-          IOUtils.closeStream(opStream);\n-        }\n-      } else {\n-        throw new AzureException(e);\n-      }\n-    } catch (URISyntaxException e) {\n-      // Re-throw exception as an Azure storage exception.\n-      throw new AzureException(e);\n-    }\n+                     SelfRenewingLease existingLease) throws IOException {\n+    rename(srcKey, dstKey, acquireLease, existingLease, true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void rename(String srcKey, String dstKey, boolean acquireLease,\n                     SelfRenewingLease existingLease) throws IOException {\n    rename(srcKey, dstKey, acquireLease, existingLease, true);\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
      "extendedDetails": {}
    },
    "f92913c35bfda0d565606f9fb9a301ddd4105fd8": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13831. Correct check for error code to detect Azure Storage Throttling and provide retries. Contributed by Gaurav Kanade\n",
      "commitDate": "15/12/16 12:35 PM",
      "commitName": "f92913c35bfda0d565606f9fb9a301ddd4105fd8",
      "commitAuthor": "Mingliang Liu",
      "commitDateOld": "05/12/16 12:04 PM",
      "commitNameOld": "15dd1f3381069c5fdc6690e3ab1907a133ba14bf",
      "commitAuthorOld": "Mingliang Liu",
      "daysBetweenCommits": 10.02,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,132 +1,130 @@\n   public void rename(String srcKey, String dstKey, boolean acquireLease,\n       SelfRenewingLease existingLease) throws IOException {\n \n     LOG.debug(\"Moving {} to {}\", srcKey, dstKey);\n \n     if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n       throw new IOException(\"Cannot acquire new lease if one already exists.\");\n     }\n \n     CloudBlobWrapper srcBlob \u003d null;\n     CloudBlobWrapper dstBlob \u003d null;\n     SelfRenewingLease lease \u003d null;\n     try {\n       // Attempts rename may occur before opening any streams so first,\n       // check if a session exists, if not create a session with the Azure\n       // storage server.\n       if (null \u003d\u003d storageInteractionLayer) {\n         final String errMsg \u003d String.format(\n             \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n             sessionUri);\n         throw new AssertionError(errMsg);\n       }\n \n       checkContainer(ContainerAccessType.ReadThenWrite);\n       // Get the source blob and assert its existence. If the source key\n       // needs to be normalized then normalize it.\n       //\n \n       srcBlob \u003d getBlobReference(srcKey);\n       if (!srcBlob.exists(getInstrumentedContext())) {\n         throw new AzureException (\"Source blob \" + srcKey +\n             \" does not exist.\");\n       }\n \n       /**\n        * Conditionally get a lease on the source blob to prevent other writers\n        * from changing it. This is used for correctness in HBase when log files\n        * are renamed. It generally should do no harm other than take a little\n        * more time for other rename scenarios. When the HBase master renames a\n        * log file folder, the lease locks out other writers.  This\n        * prevents a region server that the master thinks is dead, but is still\n        * alive, from committing additional updates.  This is different than\n        * when HBase runs on HDFS, where the region server recovers the lease\n        * on a log file, to gain exclusive access to it, before it splits it.\n        */\n       if (acquireLease) {\n         lease \u003d srcBlob.acquireLease();\n       } else if (existingLease !\u003d null) {\n         lease \u003d existingLease;\n       }\n \n       // Get the destination blob. The destination key always needs to be\n       // normalized.\n       //\n       dstBlob \u003d getBlobReference(dstKey);\n \n       // Rename the source blob to the destination blob by copying it to\n       // the destination blob then deleting it.\n       //\n       // Copy blob operation in Azure storage is very costly. It will be highly\n       // likely throttled during Azure storage gc. Short term fix will be using\n       // a more intensive exponential retry policy when the cluster is getting \n       // throttled.\n       try {\n         dstBlob.startCopyFromBlob(srcBlob, null, getInstrumentedContext());\n       } catch (StorageException se) {\n-        if (se.getErrorCode().equals(\n-          StorageErrorCode.SERVER_BUSY.toString())) {\n+        if (se.getHttpStatusCode() \u003d\u003d HttpURLConnection.HTTP_UNAVAILABLE) {\n           int copyBlobMinBackoff \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_MIN_BACKOFF_INTERVAL,\n \t\t\tDEFAULT_COPYBLOB_MIN_BACKOFF_INTERVAL);\n \n           int copyBlobMaxBackoff \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_MAX_BACKOFF_INTERVAL,\n \t\t\tDEFAULT_COPYBLOB_MAX_BACKOFF_INTERVAL);\n \n           int copyBlobDeltaBackoff \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_BACKOFF_INTERVAL,\n \t\t\tDEFAULT_COPYBLOB_BACKOFF_INTERVAL);\n \n           int copyBlobMaxRetries \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_MAX_IO_RETRIES,\n \t\t\tDEFAULT_COPYBLOB_MAX_RETRY_ATTEMPTS);\n \t        \n           BlobRequestOptions options \u003d new BlobRequestOptions();\n           options.setRetryPolicyFactory(new RetryExponentialRetry(\n             copyBlobMinBackoff, copyBlobDeltaBackoff, copyBlobMaxBackoff, \n             copyBlobMaxRetries));\n           dstBlob.startCopyFromBlob(srcBlob, options, getInstrumentedContext());\n         } else {\n           throw se;\n         }\n       }\n       waitForCopyToComplete(dstBlob, getInstrumentedContext());\n       safeDelete(srcBlob, lease);\n     } catch (StorageException e) {\n-      if (e.getErrorCode().equals(\n-        StorageErrorCode.SERVER_BUSY.toString())) {\n+      if (e.getHttpStatusCode() \u003d\u003d HttpURLConnection.HTTP_UNAVAILABLE) {\n         LOG.warn(\"Rename: CopyBlob: StorageException: ServerBusy: Retry complete, will attempt client side copy for page blob\");\n         InputStream ipStream \u003d null;\n         OutputStream opStream \u003d null;\n         try {\n           if(srcBlob.getProperties().getBlobType() \u003d\u003d BlobType.PAGE_BLOB){\n             ipStream \u003d openInputStream(srcBlob);\n             opStream \u003d openOutputStream(dstBlob);\n             byte[] buffer \u003d new byte[PageBlobFormatHelpers.PAGE_SIZE];\n             int len;\n             while ((len \u003d ipStream.read(buffer)) !\u003d -1) {\n               opStream.write(buffer, 0, len);\n             }\n             opStream.flush();\n             opStream.close();\n             ipStream.close();\n           } else {\n             throw new AzureException(e);\n           }\n           safeDelete(srcBlob, lease);\n         } catch(StorageException se) {\n           LOG.warn(\"Rename: CopyBlob: StorageException: Failed\");\n           throw new AzureException(se);\n         } finally {\n           IOUtils.closeStream(ipStream);\n           IOUtils.closeStream(opStream);\n         }\n       } else {\n         throw new AzureException(e);\n       }\n     } catch (URISyntaxException e) {\n       // Re-throw exception as an Azure storage exception.\n       throw new AzureException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void rename(String srcKey, String dstKey, boolean acquireLease,\n      SelfRenewingLease existingLease) throws IOException {\n\n    LOG.debug(\"Moving {} to {}\", srcKey, dstKey);\n\n    if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n      throw new IOException(\"Cannot acquire new lease if one already exists.\");\n    }\n\n    CloudBlobWrapper srcBlob \u003d null;\n    CloudBlobWrapper dstBlob \u003d null;\n    SelfRenewingLease lease \u003d null;\n    try {\n      // Attempts rename may occur before opening any streams so first,\n      // check if a session exists, if not create a session with the Azure\n      // storage server.\n      if (null \u003d\u003d storageInteractionLayer) {\n        final String errMsg \u003d String.format(\n            \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n            sessionUri);\n        throw new AssertionError(errMsg);\n      }\n\n      checkContainer(ContainerAccessType.ReadThenWrite);\n      // Get the source blob and assert its existence. If the source key\n      // needs to be normalized then normalize it.\n      //\n\n      srcBlob \u003d getBlobReference(srcKey);\n      if (!srcBlob.exists(getInstrumentedContext())) {\n        throw new AzureException (\"Source blob \" + srcKey +\n            \" does not exist.\");\n      }\n\n      /**\n       * Conditionally get a lease on the source blob to prevent other writers\n       * from changing it. This is used for correctness in HBase when log files\n       * are renamed. It generally should do no harm other than take a little\n       * more time for other rename scenarios. When the HBase master renames a\n       * log file folder, the lease locks out other writers.  This\n       * prevents a region server that the master thinks is dead, but is still\n       * alive, from committing additional updates.  This is different than\n       * when HBase runs on HDFS, where the region server recovers the lease\n       * on a log file, to gain exclusive access to it, before it splits it.\n       */\n      if (acquireLease) {\n        lease \u003d srcBlob.acquireLease();\n      } else if (existingLease !\u003d null) {\n        lease \u003d existingLease;\n      }\n\n      // Get the destination blob. The destination key always needs to be\n      // normalized.\n      //\n      dstBlob \u003d getBlobReference(dstKey);\n\n      // Rename the source blob to the destination blob by copying it to\n      // the destination blob then deleting it.\n      //\n      // Copy blob operation in Azure storage is very costly. It will be highly\n      // likely throttled during Azure storage gc. Short term fix will be using\n      // a more intensive exponential retry policy when the cluster is getting \n      // throttled.\n      try {\n        dstBlob.startCopyFromBlob(srcBlob, null, getInstrumentedContext());\n      } catch (StorageException se) {\n        if (se.getHttpStatusCode() \u003d\u003d HttpURLConnection.HTTP_UNAVAILABLE) {\n          int copyBlobMinBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MIN_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_MIN_BACKOFF_INTERVAL);\n\n          int copyBlobMaxBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MAX_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_MAX_BACKOFF_INTERVAL);\n\n          int copyBlobDeltaBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_BACKOFF_INTERVAL);\n\n          int copyBlobMaxRetries \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MAX_IO_RETRIES,\n\t\t\tDEFAULT_COPYBLOB_MAX_RETRY_ATTEMPTS);\n\t        \n          BlobRequestOptions options \u003d new BlobRequestOptions();\n          options.setRetryPolicyFactory(new RetryExponentialRetry(\n            copyBlobMinBackoff, copyBlobDeltaBackoff, copyBlobMaxBackoff, \n            copyBlobMaxRetries));\n          dstBlob.startCopyFromBlob(srcBlob, options, getInstrumentedContext());\n        } else {\n          throw se;\n        }\n      }\n      waitForCopyToComplete(dstBlob, getInstrumentedContext());\n      safeDelete(srcBlob, lease);\n    } catch (StorageException e) {\n      if (e.getHttpStatusCode() \u003d\u003d HttpURLConnection.HTTP_UNAVAILABLE) {\n        LOG.warn(\"Rename: CopyBlob: StorageException: ServerBusy: Retry complete, will attempt client side copy for page blob\");\n        InputStream ipStream \u003d null;\n        OutputStream opStream \u003d null;\n        try {\n          if(srcBlob.getProperties().getBlobType() \u003d\u003d BlobType.PAGE_BLOB){\n            ipStream \u003d openInputStream(srcBlob);\n            opStream \u003d openOutputStream(dstBlob);\n            byte[] buffer \u003d new byte[PageBlobFormatHelpers.PAGE_SIZE];\n            int len;\n            while ((len \u003d ipStream.read(buffer)) !\u003d -1) {\n              opStream.write(buffer, 0, len);\n            }\n            opStream.flush();\n            opStream.close();\n            ipStream.close();\n          } else {\n            throw new AzureException(e);\n          }\n          safeDelete(srcBlob, lease);\n        } catch(StorageException se) {\n          LOG.warn(\"Rename: CopyBlob: StorageException: Failed\");\n          throw new AzureException(se);\n        } finally {\n          IOUtils.closeStream(ipStream);\n          IOUtils.closeStream(opStream);\n        }\n      } else {\n        throw new AzureException(e);\n      }\n    } catch (URISyntaxException e) {\n      // Re-throw exception as an Azure storage exception.\n      throw new AzureException(e);\n    }\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
      "extendedDetails": {}
    },
    "47641fcbc9c41f4a338d8899501e4a310d2e81ad": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12334. Change Mode Of Copy Operation of HBase WAL Archiving to bypass Azure Storage Throttling after retries. Contributed by Gaurav Kanade.\n",
      "commitDate": "22/10/15 12:21 PM",
      "commitName": "47641fcbc9c41f4a338d8899501e4a310d2e81ad",
      "commitAuthor": "cnauroth",
      "commitDateOld": "05/10/15 8:11 PM",
      "commitNameOld": "5f6edb30c2bb648d5564c951edc25645e17e6636",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 16.67,
      "commitsBetweenForRepo": 138,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,132 @@\n   public void rename(String srcKey, String dstKey, boolean acquireLease,\n       SelfRenewingLease existingLease) throws IOException {\n \n     LOG.debug(\"Moving {} to {}\", srcKey, dstKey);\n \n     if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n       throw new IOException(\"Cannot acquire new lease if one already exists.\");\n     }\n \n+    CloudBlobWrapper srcBlob \u003d null;\n+    CloudBlobWrapper dstBlob \u003d null;\n+    SelfRenewingLease lease \u003d null;\n     try {\n       // Attempts rename may occur before opening any streams so first,\n       // check if a session exists, if not create a session with the Azure\n       // storage server.\n       if (null \u003d\u003d storageInteractionLayer) {\n         final String errMsg \u003d String.format(\n             \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n             sessionUri);\n         throw new AssertionError(errMsg);\n       }\n \n       checkContainer(ContainerAccessType.ReadThenWrite);\n       // Get the source blob and assert its existence. If the source key\n       // needs to be normalized then normalize it.\n       //\n-      CloudBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n \n+      srcBlob \u003d getBlobReference(srcKey);\n       if (!srcBlob.exists(getInstrumentedContext())) {\n         throw new AzureException (\"Source blob \" + srcKey +\n             \" does not exist.\");\n       }\n \n       /**\n        * Conditionally get a lease on the source blob to prevent other writers\n        * from changing it. This is used for correctness in HBase when log files\n        * are renamed. It generally should do no harm other than take a little\n        * more time for other rename scenarios. When the HBase master renames a\n        * log file folder, the lease locks out other writers.  This\n        * prevents a region server that the master thinks is dead, but is still\n        * alive, from committing additional updates.  This is different than\n        * when HBase runs on HDFS, where the region server recovers the lease\n        * on a log file, to gain exclusive access to it, before it splits it.\n        */\n-      SelfRenewingLease lease \u003d null;\n       if (acquireLease) {\n         lease \u003d srcBlob.acquireLease();\n       } else if (existingLease !\u003d null) {\n         lease \u003d existingLease;\n       }\n \n       // Get the destination blob. The destination key always needs to be\n       // normalized.\n       //\n-      CloudBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n+      dstBlob \u003d getBlobReference(dstKey);\n \n       // Rename the source blob to the destination blob by copying it to\n       // the destination blob then deleting it.\n       //\n       // Copy blob operation in Azure storage is very costly. It will be highly\n       // likely throttled during Azure storage gc. Short term fix will be using\n       // a more intensive exponential retry policy when the cluster is getting \n       // throttled.\n       try {\n         dstBlob.startCopyFromBlob(srcBlob, null, getInstrumentedContext());\n       } catch (StorageException se) {\n         if (se.getErrorCode().equals(\n-\t\t  StorageErrorCode.SERVER_BUSY.toString())) {\n+          StorageErrorCode.SERVER_BUSY.toString())) {\n           int copyBlobMinBackoff \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_MIN_BACKOFF_INTERVAL,\n \t\t\tDEFAULT_COPYBLOB_MIN_BACKOFF_INTERVAL);\n \n           int copyBlobMaxBackoff \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_MAX_BACKOFF_INTERVAL,\n \t\t\tDEFAULT_COPYBLOB_MAX_BACKOFF_INTERVAL);\n \n           int copyBlobDeltaBackoff \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_BACKOFF_INTERVAL,\n \t\t\tDEFAULT_COPYBLOB_BACKOFF_INTERVAL);\n \n           int copyBlobMaxRetries \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_MAX_IO_RETRIES,\n \t\t\tDEFAULT_COPYBLOB_MAX_RETRY_ATTEMPTS);\n \t        \n           BlobRequestOptions options \u003d new BlobRequestOptions();\n           options.setRetryPolicyFactory(new RetryExponentialRetry(\n             copyBlobMinBackoff, copyBlobDeltaBackoff, copyBlobMaxBackoff, \n-\t\t\tcopyBlobMaxRetries));\n+            copyBlobMaxRetries));\n           dstBlob.startCopyFromBlob(srcBlob, options, getInstrumentedContext());\n         } else {\n           throw se;\n         }\n       }\n       waitForCopyToComplete(dstBlob, getInstrumentedContext());\n       safeDelete(srcBlob, lease);\n     } catch (StorageException e) {\n-      // Re-throw exception as an Azure storage exception.\n-      throw new AzureException(e);\n+      if (e.getErrorCode().equals(\n+        StorageErrorCode.SERVER_BUSY.toString())) {\n+        LOG.warn(\"Rename: CopyBlob: StorageException: ServerBusy: Retry complete, will attempt client side copy for page blob\");\n+        InputStream ipStream \u003d null;\n+        OutputStream opStream \u003d null;\n+        try {\n+          if(srcBlob.getProperties().getBlobType() \u003d\u003d BlobType.PAGE_BLOB){\n+            ipStream \u003d openInputStream(srcBlob);\n+            opStream \u003d openOutputStream(dstBlob);\n+            byte[] buffer \u003d new byte[PageBlobFormatHelpers.PAGE_SIZE];\n+            int len;\n+            while ((len \u003d ipStream.read(buffer)) !\u003d -1) {\n+              opStream.write(buffer, 0, len);\n+            }\n+            opStream.flush();\n+            opStream.close();\n+            ipStream.close();\n+          } else {\n+            throw new AzureException(e);\n+          }\n+          safeDelete(srcBlob, lease);\n+        } catch(StorageException se) {\n+          LOG.warn(\"Rename: CopyBlob: StorageException: Failed\");\n+          throw new AzureException(se);\n+        } finally {\n+          IOUtils.closeStream(ipStream);\n+          IOUtils.closeStream(opStream);\n+        }\n+      } else {\n+        throw new AzureException(e);\n+      }\n     } catch (URISyntaxException e) {\n       // Re-throw exception as an Azure storage exception.\n       throw new AzureException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void rename(String srcKey, String dstKey, boolean acquireLease,\n      SelfRenewingLease existingLease) throws IOException {\n\n    LOG.debug(\"Moving {} to {}\", srcKey, dstKey);\n\n    if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n      throw new IOException(\"Cannot acquire new lease if one already exists.\");\n    }\n\n    CloudBlobWrapper srcBlob \u003d null;\n    CloudBlobWrapper dstBlob \u003d null;\n    SelfRenewingLease lease \u003d null;\n    try {\n      // Attempts rename may occur before opening any streams so first,\n      // check if a session exists, if not create a session with the Azure\n      // storage server.\n      if (null \u003d\u003d storageInteractionLayer) {\n        final String errMsg \u003d String.format(\n            \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n            sessionUri);\n        throw new AssertionError(errMsg);\n      }\n\n      checkContainer(ContainerAccessType.ReadThenWrite);\n      // Get the source blob and assert its existence. If the source key\n      // needs to be normalized then normalize it.\n      //\n\n      srcBlob \u003d getBlobReference(srcKey);\n      if (!srcBlob.exists(getInstrumentedContext())) {\n        throw new AzureException (\"Source blob \" + srcKey +\n            \" does not exist.\");\n      }\n\n      /**\n       * Conditionally get a lease on the source blob to prevent other writers\n       * from changing it. This is used for correctness in HBase when log files\n       * are renamed. It generally should do no harm other than take a little\n       * more time for other rename scenarios. When the HBase master renames a\n       * log file folder, the lease locks out other writers.  This\n       * prevents a region server that the master thinks is dead, but is still\n       * alive, from committing additional updates.  This is different than\n       * when HBase runs on HDFS, where the region server recovers the lease\n       * on a log file, to gain exclusive access to it, before it splits it.\n       */\n      if (acquireLease) {\n        lease \u003d srcBlob.acquireLease();\n      } else if (existingLease !\u003d null) {\n        lease \u003d existingLease;\n      }\n\n      // Get the destination blob. The destination key always needs to be\n      // normalized.\n      //\n      dstBlob \u003d getBlobReference(dstKey);\n\n      // Rename the source blob to the destination blob by copying it to\n      // the destination blob then deleting it.\n      //\n      // Copy blob operation in Azure storage is very costly. It will be highly\n      // likely throttled during Azure storage gc. Short term fix will be using\n      // a more intensive exponential retry policy when the cluster is getting \n      // throttled.\n      try {\n        dstBlob.startCopyFromBlob(srcBlob, null, getInstrumentedContext());\n      } catch (StorageException se) {\n        if (se.getErrorCode().equals(\n          StorageErrorCode.SERVER_BUSY.toString())) {\n          int copyBlobMinBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MIN_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_MIN_BACKOFF_INTERVAL);\n\n          int copyBlobMaxBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MAX_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_MAX_BACKOFF_INTERVAL);\n\n          int copyBlobDeltaBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_BACKOFF_INTERVAL);\n\n          int copyBlobMaxRetries \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MAX_IO_RETRIES,\n\t\t\tDEFAULT_COPYBLOB_MAX_RETRY_ATTEMPTS);\n\t        \n          BlobRequestOptions options \u003d new BlobRequestOptions();\n          options.setRetryPolicyFactory(new RetryExponentialRetry(\n            copyBlobMinBackoff, copyBlobDeltaBackoff, copyBlobMaxBackoff, \n            copyBlobMaxRetries));\n          dstBlob.startCopyFromBlob(srcBlob, options, getInstrumentedContext());\n        } else {\n          throw se;\n        }\n      }\n      waitForCopyToComplete(dstBlob, getInstrumentedContext());\n      safeDelete(srcBlob, lease);\n    } catch (StorageException e) {\n      if (e.getErrorCode().equals(\n        StorageErrorCode.SERVER_BUSY.toString())) {\n        LOG.warn(\"Rename: CopyBlob: StorageException: ServerBusy: Retry complete, will attempt client side copy for page blob\");\n        InputStream ipStream \u003d null;\n        OutputStream opStream \u003d null;\n        try {\n          if(srcBlob.getProperties().getBlobType() \u003d\u003d BlobType.PAGE_BLOB){\n            ipStream \u003d openInputStream(srcBlob);\n            opStream \u003d openOutputStream(dstBlob);\n            byte[] buffer \u003d new byte[PageBlobFormatHelpers.PAGE_SIZE];\n            int len;\n            while ((len \u003d ipStream.read(buffer)) !\u003d -1) {\n              opStream.write(buffer, 0, len);\n            }\n            opStream.flush();\n            opStream.close();\n            ipStream.close();\n          } else {\n            throw new AzureException(e);\n          }\n          safeDelete(srcBlob, lease);\n        } catch(StorageException se) {\n          LOG.warn(\"Rename: CopyBlob: StorageException: Failed\");\n          throw new AzureException(se);\n        } finally {\n          IOUtils.closeStream(ipStream);\n          IOUtils.closeStream(opStream);\n        }\n      } else {\n        throw new AzureException(e);\n      }\n    } catch (URISyntaxException e) {\n      // Re-throw exception as an Azure storage exception.\n      throw new AzureException(e);\n    }\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
      "extendedDetails": {}
    },
    "5f6edb30c2bb648d5564c951edc25645e17e6636": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12350. WASB Logging: Improve WASB Logging around deletes, reads and writes. Contributed by Dushyanth.\n",
      "commitDate": "05/10/15 8:11 PM",
      "commitName": "5f6edb30c2bb648d5564c951edc25645e17e6636",
      "commitAuthor": "cnauroth",
      "commitDateOld": "08/06/15 10:42 PM",
      "commitNameOld": "c45784bc9031353b938f4756473937cca759b3dc",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 118.9,
      "commitsBetweenForRepo": 778,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,103 +1,101 @@\n   public void rename(String srcKey, String dstKey, boolean acquireLease,\n       SelfRenewingLease existingLease) throws IOException {\n \n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Moving \" + srcKey + \" to \" + dstKey);\n-    }\n+    LOG.debug(\"Moving {} to {}\", srcKey, dstKey);\n \n     if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n       throw new IOException(\"Cannot acquire new lease if one already exists.\");\n     }\n \n     try {\n       // Attempts rename may occur before opening any streams so first,\n       // check if a session exists, if not create a session with the Azure\n       // storage server.\n       if (null \u003d\u003d storageInteractionLayer) {\n         final String errMsg \u003d String.format(\n             \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n             sessionUri);\n         throw new AssertionError(errMsg);\n       }\n \n       checkContainer(ContainerAccessType.ReadThenWrite);\n       // Get the source blob and assert its existence. If the source key\n       // needs to be normalized then normalize it.\n       //\n       CloudBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n \n       if (!srcBlob.exists(getInstrumentedContext())) {\n         throw new AzureException (\"Source blob \" + srcKey +\n             \" does not exist.\");\n       }\n \n       /**\n        * Conditionally get a lease on the source blob to prevent other writers\n        * from changing it. This is used for correctness in HBase when log files\n        * are renamed. It generally should do no harm other than take a little\n        * more time for other rename scenarios. When the HBase master renames a\n        * log file folder, the lease locks out other writers.  This\n        * prevents a region server that the master thinks is dead, but is still\n        * alive, from committing additional updates.  This is different than\n        * when HBase runs on HDFS, where the region server recovers the lease\n        * on a log file, to gain exclusive access to it, before it splits it.\n        */\n       SelfRenewingLease lease \u003d null;\n       if (acquireLease) {\n         lease \u003d srcBlob.acquireLease();\n       } else if (existingLease !\u003d null) {\n         lease \u003d existingLease;\n       }\n \n       // Get the destination blob. The destination key always needs to be\n       // normalized.\n       //\n       CloudBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n \n       // Rename the source blob to the destination blob by copying it to\n       // the destination blob then deleting it.\n       //\n       // Copy blob operation in Azure storage is very costly. It will be highly\n       // likely throttled during Azure storage gc. Short term fix will be using\n       // a more intensive exponential retry policy when the cluster is getting \n       // throttled.\n       try {\n         dstBlob.startCopyFromBlob(srcBlob, null, getInstrumentedContext());\n       } catch (StorageException se) {\n         if (se.getErrorCode().equals(\n \t\t  StorageErrorCode.SERVER_BUSY.toString())) {\n           int copyBlobMinBackoff \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_MIN_BACKOFF_INTERVAL,\n \t\t\tDEFAULT_COPYBLOB_MIN_BACKOFF_INTERVAL);\n \n           int copyBlobMaxBackoff \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_MAX_BACKOFF_INTERVAL,\n \t\t\tDEFAULT_COPYBLOB_MAX_BACKOFF_INTERVAL);\n \n           int copyBlobDeltaBackoff \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_BACKOFF_INTERVAL,\n \t\t\tDEFAULT_COPYBLOB_BACKOFF_INTERVAL);\n \n           int copyBlobMaxRetries \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_MAX_IO_RETRIES,\n \t\t\tDEFAULT_COPYBLOB_MAX_RETRY_ATTEMPTS);\n \t        \n           BlobRequestOptions options \u003d new BlobRequestOptions();\n           options.setRetryPolicyFactory(new RetryExponentialRetry(\n             copyBlobMinBackoff, copyBlobDeltaBackoff, copyBlobMaxBackoff, \n \t\t\tcopyBlobMaxRetries));\n           dstBlob.startCopyFromBlob(srcBlob, options, getInstrumentedContext());\n         } else {\n           throw se;\n         }\n       }\n       waitForCopyToComplete(dstBlob, getInstrumentedContext());\n       safeDelete(srcBlob, lease);\n     } catch (StorageException e) {\n       // Re-throw exception as an Azure storage exception.\n       throw new AzureException(e);\n     } catch (URISyntaxException e) {\n       // Re-throw exception as an Azure storage exception.\n       throw new AzureException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void rename(String srcKey, String dstKey, boolean acquireLease,\n      SelfRenewingLease existingLease) throws IOException {\n\n    LOG.debug(\"Moving {} to {}\", srcKey, dstKey);\n\n    if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n      throw new IOException(\"Cannot acquire new lease if one already exists.\");\n    }\n\n    try {\n      // Attempts rename may occur before opening any streams so first,\n      // check if a session exists, if not create a session with the Azure\n      // storage server.\n      if (null \u003d\u003d storageInteractionLayer) {\n        final String errMsg \u003d String.format(\n            \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n            sessionUri);\n        throw new AssertionError(errMsg);\n      }\n\n      checkContainer(ContainerAccessType.ReadThenWrite);\n      // Get the source blob and assert its existence. If the source key\n      // needs to be normalized then normalize it.\n      //\n      CloudBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n\n      if (!srcBlob.exists(getInstrumentedContext())) {\n        throw new AzureException (\"Source blob \" + srcKey +\n            \" does not exist.\");\n      }\n\n      /**\n       * Conditionally get a lease on the source blob to prevent other writers\n       * from changing it. This is used for correctness in HBase when log files\n       * are renamed. It generally should do no harm other than take a little\n       * more time for other rename scenarios. When the HBase master renames a\n       * log file folder, the lease locks out other writers.  This\n       * prevents a region server that the master thinks is dead, but is still\n       * alive, from committing additional updates.  This is different than\n       * when HBase runs on HDFS, where the region server recovers the lease\n       * on a log file, to gain exclusive access to it, before it splits it.\n       */\n      SelfRenewingLease lease \u003d null;\n      if (acquireLease) {\n        lease \u003d srcBlob.acquireLease();\n      } else if (existingLease !\u003d null) {\n        lease \u003d existingLease;\n      }\n\n      // Get the destination blob. The destination key always needs to be\n      // normalized.\n      //\n      CloudBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n\n      // Rename the source blob to the destination blob by copying it to\n      // the destination blob then deleting it.\n      //\n      // Copy blob operation in Azure storage is very costly. It will be highly\n      // likely throttled during Azure storage gc. Short term fix will be using\n      // a more intensive exponential retry policy when the cluster is getting \n      // throttled.\n      try {\n        dstBlob.startCopyFromBlob(srcBlob, null, getInstrumentedContext());\n      } catch (StorageException se) {\n        if (se.getErrorCode().equals(\n\t\t  StorageErrorCode.SERVER_BUSY.toString())) {\n          int copyBlobMinBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MIN_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_MIN_BACKOFF_INTERVAL);\n\n          int copyBlobMaxBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MAX_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_MAX_BACKOFF_INTERVAL);\n\n          int copyBlobDeltaBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_BACKOFF_INTERVAL);\n\n          int copyBlobMaxRetries \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MAX_IO_RETRIES,\n\t\t\tDEFAULT_COPYBLOB_MAX_RETRY_ATTEMPTS);\n\t        \n          BlobRequestOptions options \u003d new BlobRequestOptions();\n          options.setRetryPolicyFactory(new RetryExponentialRetry(\n            copyBlobMinBackoff, copyBlobDeltaBackoff, copyBlobMaxBackoff, \n\t\t\tcopyBlobMaxRetries));\n          dstBlob.startCopyFromBlob(srcBlob, options, getInstrumentedContext());\n        } else {\n          throw se;\n        }\n      }\n      waitForCopyToComplete(dstBlob, getInstrumentedContext());\n      safeDelete(srcBlob, lease);\n    } catch (StorageException e) {\n      // Re-throw exception as an Azure storage exception.\n      throw new AzureException(e);\n    } catch (URISyntaxException e) {\n      // Re-throw exception as an Azure storage exception.\n      throw new AzureException(e);\n    }\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
      "extendedDetails": {}
    },
    "94e7d49a6dab7e7f4e873dcca67e7fcc98e7e1f8": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11959. WASB should configure client side socket timeout in storage client blob request options. Contributed by Ivan Mitic.\n",
      "commitDate": "28/05/15 12:31 PM",
      "commitName": "94e7d49a6dab7e7f4e873dcca67e7fcc98e7e1f8",
      "commitAuthor": "cnauroth",
      "commitDateOld": "14/05/15 10:22 PM",
      "commitNameOld": "cb8e69a80cecb95abdfc93a787bea0bedef275ed",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 13.59,
      "commitsBetweenForRepo": 103,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,112 +1,103 @@\n   public void rename(String srcKey, String dstKey, boolean acquireLease,\n       SelfRenewingLease existingLease) throws IOException {\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Moving \" + srcKey + \" to \" + dstKey);\n     }\n \n     if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n       throw new IOException(\"Cannot acquire new lease if one already exists.\");\n     }\n \n     try {\n       // Attempts rename may occur before opening any streams so first,\n       // check if a session exists, if not create a session with the Azure\n       // storage server.\n       if (null \u003d\u003d storageInteractionLayer) {\n         final String errMsg \u003d String.format(\n             \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n             sessionUri);\n         throw new AssertionError(errMsg);\n       }\n \n       checkContainer(ContainerAccessType.ReadThenWrite);\n       // Get the source blob and assert its existence. If the source key\n       // needs to be normalized then normalize it.\n       //\n       CloudBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n \n       if (!srcBlob.exists(getInstrumentedContext())) {\n         throw new AzureException (\"Source blob \" + srcKey +\n             \" does not exist.\");\n       }\n \n       /**\n        * Conditionally get a lease on the source blob to prevent other writers\n        * from changing it. This is used for correctness in HBase when log files\n        * are renamed. It generally should do no harm other than take a little\n        * more time for other rename scenarios. When the HBase master renames a\n        * log file folder, the lease locks out other writers.  This\n        * prevents a region server that the master thinks is dead, but is still\n        * alive, from committing additional updates.  This is different than\n        * when HBase runs on HDFS, where the region server recovers the lease\n        * on a log file, to gain exclusive access to it, before it splits it.\n        */\n       SelfRenewingLease lease \u003d null;\n       if (acquireLease) {\n         lease \u003d srcBlob.acquireLease();\n       } else if (existingLease !\u003d null) {\n         lease \u003d existingLease;\n       }\n \n       // Get the destination blob. The destination key always needs to be\n       // normalized.\n       //\n       CloudBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n \n-      // TODO: Remove at the time when we move to Azure Java SDK 1.2+.\n-      // This is the workaround provided by Azure Java SDK team to\n-      // mitigate the issue with un-encoded x-ms-copy-source HTTP\n-      // request header. Azure sdk version before 1.2+ does not encode this\n-      // header what causes all URIs that have special (category \"other\")\n-      // characters in the URI not to work with startCopyFromBlob when\n-      // specified as source (requests fail with HTTP 403).\n-      URI srcUri \u003d new URI(srcBlob.getUri().toASCIIString());\n-\n       // Rename the source blob to the destination blob by copying it to\n       // the destination blob then deleting it.\n       //\n       // Copy blob operation in Azure storage is very costly. It will be highly\n       // likely throttled during Azure storage gc. Short term fix will be using\n       // a more intensive exponential retry policy when the cluster is getting \n       // throttled.\n       try {\n-        dstBlob.startCopyFromBlob(srcUri, null, getInstrumentedContext());\n+        dstBlob.startCopyFromBlob(srcBlob, null, getInstrumentedContext());\n       } catch (StorageException se) {\n         if (se.getErrorCode().equals(\n \t\t  StorageErrorCode.SERVER_BUSY.toString())) {\n           int copyBlobMinBackoff \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_MIN_BACKOFF_INTERVAL,\n \t\t\tDEFAULT_COPYBLOB_MIN_BACKOFF_INTERVAL);\n \n           int copyBlobMaxBackoff \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_MAX_BACKOFF_INTERVAL,\n \t\t\tDEFAULT_COPYBLOB_MAX_BACKOFF_INTERVAL);\n \n           int copyBlobDeltaBackoff \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_BACKOFF_INTERVAL,\n \t\t\tDEFAULT_COPYBLOB_BACKOFF_INTERVAL);\n \n           int copyBlobMaxRetries \u003d sessionConfiguration.getInt(\n             KEY_COPYBLOB_MAX_IO_RETRIES,\n \t\t\tDEFAULT_COPYBLOB_MAX_RETRY_ATTEMPTS);\n \t        \n           BlobRequestOptions options \u003d new BlobRequestOptions();\n           options.setRetryPolicyFactory(new RetryExponentialRetry(\n             copyBlobMinBackoff, copyBlobDeltaBackoff, copyBlobMaxBackoff, \n \t\t\tcopyBlobMaxRetries));\n-          dstBlob.startCopyFromBlob(srcUri, options, getInstrumentedContext());\n+          dstBlob.startCopyFromBlob(srcBlob, options, getInstrumentedContext());\n         } else {\n           throw se;\n         }\n       }\n       waitForCopyToComplete(dstBlob, getInstrumentedContext());\n       safeDelete(srcBlob, lease);\n     } catch (StorageException e) {\n       // Re-throw exception as an Azure storage exception.\n       throw new AzureException(e);\n     } catch (URISyntaxException e) {\n       // Re-throw exception as an Azure storage exception.\n       throw new AzureException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void rename(String srcKey, String dstKey, boolean acquireLease,\n      SelfRenewingLease existingLease) throws IOException {\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Moving \" + srcKey + \" to \" + dstKey);\n    }\n\n    if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n      throw new IOException(\"Cannot acquire new lease if one already exists.\");\n    }\n\n    try {\n      // Attempts rename may occur before opening any streams so first,\n      // check if a session exists, if not create a session with the Azure\n      // storage server.\n      if (null \u003d\u003d storageInteractionLayer) {\n        final String errMsg \u003d String.format(\n            \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n            sessionUri);\n        throw new AssertionError(errMsg);\n      }\n\n      checkContainer(ContainerAccessType.ReadThenWrite);\n      // Get the source blob and assert its existence. If the source key\n      // needs to be normalized then normalize it.\n      //\n      CloudBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n\n      if (!srcBlob.exists(getInstrumentedContext())) {\n        throw new AzureException (\"Source blob \" + srcKey +\n            \" does not exist.\");\n      }\n\n      /**\n       * Conditionally get a lease on the source blob to prevent other writers\n       * from changing it. This is used for correctness in HBase when log files\n       * are renamed. It generally should do no harm other than take a little\n       * more time for other rename scenarios. When the HBase master renames a\n       * log file folder, the lease locks out other writers.  This\n       * prevents a region server that the master thinks is dead, but is still\n       * alive, from committing additional updates.  This is different than\n       * when HBase runs on HDFS, where the region server recovers the lease\n       * on a log file, to gain exclusive access to it, before it splits it.\n       */\n      SelfRenewingLease lease \u003d null;\n      if (acquireLease) {\n        lease \u003d srcBlob.acquireLease();\n      } else if (existingLease !\u003d null) {\n        lease \u003d existingLease;\n      }\n\n      // Get the destination blob. The destination key always needs to be\n      // normalized.\n      //\n      CloudBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n\n      // Rename the source blob to the destination blob by copying it to\n      // the destination blob then deleting it.\n      //\n      // Copy blob operation in Azure storage is very costly. It will be highly\n      // likely throttled during Azure storage gc. Short term fix will be using\n      // a more intensive exponential retry policy when the cluster is getting \n      // throttled.\n      try {\n        dstBlob.startCopyFromBlob(srcBlob, null, getInstrumentedContext());\n      } catch (StorageException se) {\n        if (se.getErrorCode().equals(\n\t\t  StorageErrorCode.SERVER_BUSY.toString())) {\n          int copyBlobMinBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MIN_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_MIN_BACKOFF_INTERVAL);\n\n          int copyBlobMaxBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MAX_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_MAX_BACKOFF_INTERVAL);\n\n          int copyBlobDeltaBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_BACKOFF_INTERVAL);\n\n          int copyBlobMaxRetries \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MAX_IO_RETRIES,\n\t\t\tDEFAULT_COPYBLOB_MAX_RETRY_ATTEMPTS);\n\t        \n          BlobRequestOptions options \u003d new BlobRequestOptions();\n          options.setRetryPolicyFactory(new RetryExponentialRetry(\n            copyBlobMinBackoff, copyBlobDeltaBackoff, copyBlobMaxBackoff, \n\t\t\tcopyBlobMaxRetries));\n          dstBlob.startCopyFromBlob(srcBlob, options, getInstrumentedContext());\n        } else {\n          throw se;\n        }\n      }\n      waitForCopyToComplete(dstBlob, getInstrumentedContext());\n      safeDelete(srcBlob, lease);\n    } catch (StorageException e) {\n      // Re-throw exception as an Azure storage exception.\n      throw new AzureException(e);\n    } catch (URISyntaxException e) {\n      // Re-throw exception as an Azure storage exception.\n      throw new AzureException(e);\n    }\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
      "extendedDetails": {}
    },
    "7a346bcb4fa5b56191ed00a39e72e51c9bdf1b56": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11693. Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving. Contributed by Duo Xu.\n",
      "commitDate": "11/03/15 2:36 PM",
      "commitName": "7a346bcb4fa5b56191ed00a39e72e51c9bdf1b56",
      "commitAuthor": "cnauroth",
      "commitDateOld": "06/03/15 3:25 PM",
      "commitNameOld": "608ebd52bafecf980e9726d397c786a9c2422eba",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 4.92,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,112 @@\n   public void rename(String srcKey, String dstKey, boolean acquireLease,\n       SelfRenewingLease existingLease) throws IOException {\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Moving \" + srcKey + \" to \" + dstKey);\n     }\n \n     if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n       throw new IOException(\"Cannot acquire new lease if one already exists.\");\n     }\n \n     try {\n       // Attempts rename may occur before opening any streams so first,\n       // check if a session exists, if not create a session with the Azure\n       // storage server.\n       if (null \u003d\u003d storageInteractionLayer) {\n         final String errMsg \u003d String.format(\n             \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n             sessionUri);\n         throw new AssertionError(errMsg);\n       }\n \n       checkContainer(ContainerAccessType.ReadThenWrite);\n       // Get the source blob and assert its existence. If the source key\n       // needs to be normalized then normalize it.\n       //\n       CloudBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n \n       if (!srcBlob.exists(getInstrumentedContext())) {\n         throw new AzureException (\"Source blob \" + srcKey +\n             \" does not exist.\");\n       }\n \n       /**\n        * Conditionally get a lease on the source blob to prevent other writers\n        * from changing it. This is used for correctness in HBase when log files\n        * are renamed. It generally should do no harm other than take a little\n        * more time for other rename scenarios. When the HBase master renames a\n        * log file folder, the lease locks out other writers.  This\n        * prevents a region server that the master thinks is dead, but is still\n        * alive, from committing additional updates.  This is different than\n        * when HBase runs on HDFS, where the region server recovers the lease\n        * on a log file, to gain exclusive access to it, before it splits it.\n        */\n       SelfRenewingLease lease \u003d null;\n       if (acquireLease) {\n         lease \u003d srcBlob.acquireLease();\n       } else if (existingLease !\u003d null) {\n         lease \u003d existingLease;\n       }\n \n       // Get the destination blob. The destination key always needs to be\n       // normalized.\n       //\n       CloudBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n \n       // TODO: Remove at the time when we move to Azure Java SDK 1.2+.\n       // This is the workaround provided by Azure Java SDK team to\n       // mitigate the issue with un-encoded x-ms-copy-source HTTP\n       // request header. Azure sdk version before 1.2+ does not encode this\n       // header what causes all URIs that have special (category \"other\")\n       // characters in the URI not to work with startCopyFromBlob when\n       // specified as source (requests fail with HTTP 403).\n       URI srcUri \u003d new URI(srcBlob.getUri().toASCIIString());\n \n       // Rename the source blob to the destination blob by copying it to\n       // the destination blob then deleting it.\n       //\n-      dstBlob.startCopyFromBlob(srcUri, getInstrumentedContext());\n-      waitForCopyToComplete(dstBlob, getInstrumentedContext());\n+      // Copy blob operation in Azure storage is very costly. It will be highly\n+      // likely throttled during Azure storage gc. Short term fix will be using\n+      // a more intensive exponential retry policy when the cluster is getting \n+      // throttled.\n+      try {\n+        dstBlob.startCopyFromBlob(srcUri, null, getInstrumentedContext());\n+      } catch (StorageException se) {\n+        if (se.getErrorCode().equals(\n+\t\t  StorageErrorCode.SERVER_BUSY.toString())) {\n+          int copyBlobMinBackoff \u003d sessionConfiguration.getInt(\n+            KEY_COPYBLOB_MIN_BACKOFF_INTERVAL,\n+\t\t\tDEFAULT_COPYBLOB_MIN_BACKOFF_INTERVAL);\n \n+          int copyBlobMaxBackoff \u003d sessionConfiguration.getInt(\n+            KEY_COPYBLOB_MAX_BACKOFF_INTERVAL,\n+\t\t\tDEFAULT_COPYBLOB_MAX_BACKOFF_INTERVAL);\n+\n+          int copyBlobDeltaBackoff \u003d sessionConfiguration.getInt(\n+            KEY_COPYBLOB_BACKOFF_INTERVAL,\n+\t\t\tDEFAULT_COPYBLOB_BACKOFF_INTERVAL);\n+\n+          int copyBlobMaxRetries \u003d sessionConfiguration.getInt(\n+            KEY_COPYBLOB_MAX_IO_RETRIES,\n+\t\t\tDEFAULT_COPYBLOB_MAX_RETRY_ATTEMPTS);\n+\t        \n+          BlobRequestOptions options \u003d new BlobRequestOptions();\n+          options.setRetryPolicyFactory(new RetryExponentialRetry(\n+            copyBlobMinBackoff, copyBlobDeltaBackoff, copyBlobMaxBackoff, \n+\t\t\tcopyBlobMaxRetries));\n+          dstBlob.startCopyFromBlob(srcUri, options, getInstrumentedContext());\n+        } else {\n+          throw se;\n+        }\n+      }\n+      waitForCopyToComplete(dstBlob, getInstrumentedContext());\n       safeDelete(srcBlob, lease);\n-    } catch (Exception e) {\n+    } catch (StorageException e) {\n+      // Re-throw exception as an Azure storage exception.\n+      throw new AzureException(e);\n+    } catch (URISyntaxException e) {\n       // Re-throw exception as an Azure storage exception.\n       throw new AzureException(e);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void rename(String srcKey, String dstKey, boolean acquireLease,\n      SelfRenewingLease existingLease) throws IOException {\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Moving \" + srcKey + \" to \" + dstKey);\n    }\n\n    if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n      throw new IOException(\"Cannot acquire new lease if one already exists.\");\n    }\n\n    try {\n      // Attempts rename may occur before opening any streams so first,\n      // check if a session exists, if not create a session with the Azure\n      // storage server.\n      if (null \u003d\u003d storageInteractionLayer) {\n        final String errMsg \u003d String.format(\n            \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n            sessionUri);\n        throw new AssertionError(errMsg);\n      }\n\n      checkContainer(ContainerAccessType.ReadThenWrite);\n      // Get the source blob and assert its existence. If the source key\n      // needs to be normalized then normalize it.\n      //\n      CloudBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n\n      if (!srcBlob.exists(getInstrumentedContext())) {\n        throw new AzureException (\"Source blob \" + srcKey +\n            \" does not exist.\");\n      }\n\n      /**\n       * Conditionally get a lease on the source blob to prevent other writers\n       * from changing it. This is used for correctness in HBase when log files\n       * are renamed. It generally should do no harm other than take a little\n       * more time for other rename scenarios. When the HBase master renames a\n       * log file folder, the lease locks out other writers.  This\n       * prevents a region server that the master thinks is dead, but is still\n       * alive, from committing additional updates.  This is different than\n       * when HBase runs on HDFS, where the region server recovers the lease\n       * on a log file, to gain exclusive access to it, before it splits it.\n       */\n      SelfRenewingLease lease \u003d null;\n      if (acquireLease) {\n        lease \u003d srcBlob.acquireLease();\n      } else if (existingLease !\u003d null) {\n        lease \u003d existingLease;\n      }\n\n      // Get the destination blob. The destination key always needs to be\n      // normalized.\n      //\n      CloudBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n\n      // TODO: Remove at the time when we move to Azure Java SDK 1.2+.\n      // This is the workaround provided by Azure Java SDK team to\n      // mitigate the issue with un-encoded x-ms-copy-source HTTP\n      // request header. Azure sdk version before 1.2+ does not encode this\n      // header what causes all URIs that have special (category \"other\")\n      // characters in the URI not to work with startCopyFromBlob when\n      // specified as source (requests fail with HTTP 403).\n      URI srcUri \u003d new URI(srcBlob.getUri().toASCIIString());\n\n      // Rename the source blob to the destination blob by copying it to\n      // the destination blob then deleting it.\n      //\n      // Copy blob operation in Azure storage is very costly. It will be highly\n      // likely throttled during Azure storage gc. Short term fix will be using\n      // a more intensive exponential retry policy when the cluster is getting \n      // throttled.\n      try {\n        dstBlob.startCopyFromBlob(srcUri, null, getInstrumentedContext());\n      } catch (StorageException se) {\n        if (se.getErrorCode().equals(\n\t\t  StorageErrorCode.SERVER_BUSY.toString())) {\n          int copyBlobMinBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MIN_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_MIN_BACKOFF_INTERVAL);\n\n          int copyBlobMaxBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MAX_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_MAX_BACKOFF_INTERVAL);\n\n          int copyBlobDeltaBackoff \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_BACKOFF_INTERVAL,\n\t\t\tDEFAULT_COPYBLOB_BACKOFF_INTERVAL);\n\n          int copyBlobMaxRetries \u003d sessionConfiguration.getInt(\n            KEY_COPYBLOB_MAX_IO_RETRIES,\n\t\t\tDEFAULT_COPYBLOB_MAX_RETRY_ATTEMPTS);\n\t        \n          BlobRequestOptions options \u003d new BlobRequestOptions();\n          options.setRetryPolicyFactory(new RetryExponentialRetry(\n            copyBlobMinBackoff, copyBlobDeltaBackoff, copyBlobMaxBackoff, \n\t\t\tcopyBlobMaxRetries));\n          dstBlob.startCopyFromBlob(srcUri, options, getInstrumentedContext());\n        } else {\n          throw se;\n        }\n      }\n      waitForCopyToComplete(dstBlob, getInstrumentedContext());\n      safeDelete(srcBlob, lease);\n    } catch (StorageException e) {\n      // Re-throw exception as an Azure storage exception.\n      throw new AzureException(e);\n    } catch (URISyntaxException e) {\n      // Re-throw exception as an Azure storage exception.\n      throw new AzureException(e);\n    }\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
      "extendedDetails": {}
    },
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-10809. hadoop-azure: page blob support. Contributed by Dexter Bradshaw, Mostafa Elhemali, Eric Hanson, and Mike Liddell.\n",
      "commitDate": "08/10/14 2:20 PM",
      "commitName": "2217e2f8ff418b88eac6ad36cafe3a9795a11f40",
      "commitAuthor": "cnauroth",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-10809. hadoop-azure: page blob support. Contributed by Dexter Bradshaw, Mostafa Elhemali, Eric Hanson, and Mike Liddell.\n",
          "commitDate": "08/10/14 2:20 PM",
          "commitName": "2217e2f8ff418b88eac6ad36cafe3a9795a11f40",
          "commitAuthor": "cnauroth",
          "commitDateOld": "24/06/14 1:52 PM",
          "commitNameOld": "0d91576ec31f63402f2db6107a04155368e2632d",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 106.02,
          "commitsBetweenForRepo": 1005,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,77 @@\n-  public void rename(String srcKey, String dstKey) throws IOException {\n+  public void rename(String srcKey, String dstKey, boolean acquireLease,\n+      SelfRenewingLease existingLease) throws IOException {\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Moving \" + srcKey + \" to \" + dstKey);\n     }\n \n+    if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n+      throw new IOException(\"Cannot acquire new lease if one already exists.\");\n+    }\n+\n     try {\n       // Attempts rename may occur before opening any streams so first,\n       // check if a session exists, if not create a session with the Azure\n       // storage server.\n       if (null \u003d\u003d storageInteractionLayer) {\n         final String errMsg \u003d String.format(\n             \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n             sessionUri);\n         throw new AssertionError(errMsg);\n       }\n \n       checkContainer(ContainerAccessType.ReadThenWrite);\n       // Get the source blob and assert its existence. If the source key\n       // needs to be normalized then normalize it.\n-      CloudBlockBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n+      //\n+      CloudBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n \n       if (!srcBlob.exists(getInstrumentedContext())) {\n-        throw new AzureException(\"Source blob \" + srcKey + \" does not exist.\");\n+        throw new AzureException (\"Source blob \" + srcKey +\n+            \" does not exist.\");\n+      }\n+\n+      /**\n+       * Conditionally get a lease on the source blob to prevent other writers\n+       * from changing it. This is used for correctness in HBase when log files\n+       * are renamed. It generally should do no harm other than take a little\n+       * more time for other rename scenarios. When the HBase master renames a\n+       * log file folder, the lease locks out other writers.  This\n+       * prevents a region server that the master thinks is dead, but is still\n+       * alive, from committing additional updates.  This is different than\n+       * when HBase runs on HDFS, where the region server recovers the lease\n+       * on a log file, to gain exclusive access to it, before it splits it.\n+       */\n+      SelfRenewingLease lease \u003d null;\n+      if (acquireLease) {\n+        lease \u003d srcBlob.acquireLease();\n+      } else if (existingLease !\u003d null) {\n+        lease \u003d existingLease;\n       }\n \n       // Get the destination blob. The destination key always needs to be\n       // normalized.\n-      CloudBlockBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n+      //\n+      CloudBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n+\n+      // TODO: Remove at the time when we move to Azure Java SDK 1.2+.\n+      // This is the workaround provided by Azure Java SDK team to\n+      // mitigate the issue with un-encoded x-ms-copy-source HTTP\n+      // request header. Azure sdk version before 1.2+ does not encode this\n+      // header what causes all URIs that have special (category \"other\")\n+      // characters in the URI not to work with startCopyFromBlob when\n+      // specified as source (requests fail with HTTP 403).\n+      URI srcUri \u003d new URI(srcBlob.getUri().toASCIIString());\n \n       // Rename the source blob to the destination blob by copying it to\n       // the destination blob then deleting it.\n       //\n-      dstBlob.startCopyFromBlob(srcBlob, getInstrumentedContext());\n+      dstBlob.startCopyFromBlob(srcUri, getInstrumentedContext());\n       waitForCopyToComplete(dstBlob, getInstrumentedContext());\n \n-      safeDelete(srcBlob);\n+      safeDelete(srcBlob, lease);\n     } catch (Exception e) {\n       // Re-throw exception as an Azure storage exception.\n       throw new AzureException(e);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void rename(String srcKey, String dstKey, boolean acquireLease,\n      SelfRenewingLease existingLease) throws IOException {\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Moving \" + srcKey + \" to \" + dstKey);\n    }\n\n    if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n      throw new IOException(\"Cannot acquire new lease if one already exists.\");\n    }\n\n    try {\n      // Attempts rename may occur before opening any streams so first,\n      // check if a session exists, if not create a session with the Azure\n      // storage server.\n      if (null \u003d\u003d storageInteractionLayer) {\n        final String errMsg \u003d String.format(\n            \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n            sessionUri);\n        throw new AssertionError(errMsg);\n      }\n\n      checkContainer(ContainerAccessType.ReadThenWrite);\n      // Get the source blob and assert its existence. If the source key\n      // needs to be normalized then normalize it.\n      //\n      CloudBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n\n      if (!srcBlob.exists(getInstrumentedContext())) {\n        throw new AzureException (\"Source blob \" + srcKey +\n            \" does not exist.\");\n      }\n\n      /**\n       * Conditionally get a lease on the source blob to prevent other writers\n       * from changing it. This is used for correctness in HBase when log files\n       * are renamed. It generally should do no harm other than take a little\n       * more time for other rename scenarios. When the HBase master renames a\n       * log file folder, the lease locks out other writers.  This\n       * prevents a region server that the master thinks is dead, but is still\n       * alive, from committing additional updates.  This is different than\n       * when HBase runs on HDFS, where the region server recovers the lease\n       * on a log file, to gain exclusive access to it, before it splits it.\n       */\n      SelfRenewingLease lease \u003d null;\n      if (acquireLease) {\n        lease \u003d srcBlob.acquireLease();\n      } else if (existingLease !\u003d null) {\n        lease \u003d existingLease;\n      }\n\n      // Get the destination blob. The destination key always needs to be\n      // normalized.\n      //\n      CloudBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n\n      // TODO: Remove at the time when we move to Azure Java SDK 1.2+.\n      // This is the workaround provided by Azure Java SDK team to\n      // mitigate the issue with un-encoded x-ms-copy-source HTTP\n      // request header. Azure sdk version before 1.2+ does not encode this\n      // header what causes all URIs that have special (category \"other\")\n      // characters in the URI not to work with startCopyFromBlob when\n      // specified as source (requests fail with HTTP 403).\n      URI srcUri \u003d new URI(srcBlob.getUri().toASCIIString());\n\n      // Rename the source blob to the destination blob by copying it to\n      // the destination blob then deleting it.\n      //\n      dstBlob.startCopyFromBlob(srcUri, getInstrumentedContext());\n      waitForCopyToComplete(dstBlob, getInstrumentedContext());\n\n      safeDelete(srcBlob, lease);\n    } catch (Exception e) {\n      // Re-throw exception as an Azure storage exception.\n      throw new AzureException(e);\n    }\n  }",
          "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
          "extendedDetails": {
            "oldValue": "[srcKey-String, dstKey-String]",
            "newValue": "[srcKey-String, dstKey-String, acquireLease-boolean, existingLease-SelfRenewingLease]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-10809. hadoop-azure: page blob support. Contributed by Dexter Bradshaw, Mostafa Elhemali, Eric Hanson, and Mike Liddell.\n",
          "commitDate": "08/10/14 2:20 PM",
          "commitName": "2217e2f8ff418b88eac6ad36cafe3a9795a11f40",
          "commitAuthor": "cnauroth",
          "commitDateOld": "24/06/14 1:52 PM",
          "commitNameOld": "0d91576ec31f63402f2db6107a04155368e2632d",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 106.02,
          "commitsBetweenForRepo": 1005,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,42 +1,77 @@\n-  public void rename(String srcKey, String dstKey) throws IOException {\n+  public void rename(String srcKey, String dstKey, boolean acquireLease,\n+      SelfRenewingLease existingLease) throws IOException {\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Moving \" + srcKey + \" to \" + dstKey);\n     }\n \n+    if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n+      throw new IOException(\"Cannot acquire new lease if one already exists.\");\n+    }\n+\n     try {\n       // Attempts rename may occur before opening any streams so first,\n       // check if a session exists, if not create a session with the Azure\n       // storage server.\n       if (null \u003d\u003d storageInteractionLayer) {\n         final String errMsg \u003d String.format(\n             \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n             sessionUri);\n         throw new AssertionError(errMsg);\n       }\n \n       checkContainer(ContainerAccessType.ReadThenWrite);\n       // Get the source blob and assert its existence. If the source key\n       // needs to be normalized then normalize it.\n-      CloudBlockBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n+      //\n+      CloudBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n \n       if (!srcBlob.exists(getInstrumentedContext())) {\n-        throw new AzureException(\"Source blob \" + srcKey + \" does not exist.\");\n+        throw new AzureException (\"Source blob \" + srcKey +\n+            \" does not exist.\");\n+      }\n+\n+      /**\n+       * Conditionally get a lease on the source blob to prevent other writers\n+       * from changing it. This is used for correctness in HBase when log files\n+       * are renamed. It generally should do no harm other than take a little\n+       * more time for other rename scenarios. When the HBase master renames a\n+       * log file folder, the lease locks out other writers.  This\n+       * prevents a region server that the master thinks is dead, but is still\n+       * alive, from committing additional updates.  This is different than\n+       * when HBase runs on HDFS, where the region server recovers the lease\n+       * on a log file, to gain exclusive access to it, before it splits it.\n+       */\n+      SelfRenewingLease lease \u003d null;\n+      if (acquireLease) {\n+        lease \u003d srcBlob.acquireLease();\n+      } else if (existingLease !\u003d null) {\n+        lease \u003d existingLease;\n       }\n \n       // Get the destination blob. The destination key always needs to be\n       // normalized.\n-      CloudBlockBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n+      //\n+      CloudBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n+\n+      // TODO: Remove at the time when we move to Azure Java SDK 1.2+.\n+      // This is the workaround provided by Azure Java SDK team to\n+      // mitigate the issue with un-encoded x-ms-copy-source HTTP\n+      // request header. Azure sdk version before 1.2+ does not encode this\n+      // header what causes all URIs that have special (category \"other\")\n+      // characters in the URI not to work with startCopyFromBlob when\n+      // specified as source (requests fail with HTTP 403).\n+      URI srcUri \u003d new URI(srcBlob.getUri().toASCIIString());\n \n       // Rename the source blob to the destination blob by copying it to\n       // the destination blob then deleting it.\n       //\n-      dstBlob.startCopyFromBlob(srcBlob, getInstrumentedContext());\n+      dstBlob.startCopyFromBlob(srcUri, getInstrumentedContext());\n       waitForCopyToComplete(dstBlob, getInstrumentedContext());\n \n-      safeDelete(srcBlob);\n+      safeDelete(srcBlob, lease);\n     } catch (Exception e) {\n       // Re-throw exception as an Azure storage exception.\n       throw new AzureException(e);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void rename(String srcKey, String dstKey, boolean acquireLease,\n      SelfRenewingLease existingLease) throws IOException {\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Moving \" + srcKey + \" to \" + dstKey);\n    }\n\n    if (acquireLease \u0026\u0026 existingLease !\u003d null) {\n      throw new IOException(\"Cannot acquire new lease if one already exists.\");\n    }\n\n    try {\n      // Attempts rename may occur before opening any streams so first,\n      // check if a session exists, if not create a session with the Azure\n      // storage server.\n      if (null \u003d\u003d storageInteractionLayer) {\n        final String errMsg \u003d String.format(\n            \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n            sessionUri);\n        throw new AssertionError(errMsg);\n      }\n\n      checkContainer(ContainerAccessType.ReadThenWrite);\n      // Get the source blob and assert its existence. If the source key\n      // needs to be normalized then normalize it.\n      //\n      CloudBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n\n      if (!srcBlob.exists(getInstrumentedContext())) {\n        throw new AzureException (\"Source blob \" + srcKey +\n            \" does not exist.\");\n      }\n\n      /**\n       * Conditionally get a lease on the source blob to prevent other writers\n       * from changing it. This is used for correctness in HBase when log files\n       * are renamed. It generally should do no harm other than take a little\n       * more time for other rename scenarios. When the HBase master renames a\n       * log file folder, the lease locks out other writers.  This\n       * prevents a region server that the master thinks is dead, but is still\n       * alive, from committing additional updates.  This is different than\n       * when HBase runs on HDFS, where the region server recovers the lease\n       * on a log file, to gain exclusive access to it, before it splits it.\n       */\n      SelfRenewingLease lease \u003d null;\n      if (acquireLease) {\n        lease \u003d srcBlob.acquireLease();\n      } else if (existingLease !\u003d null) {\n        lease \u003d existingLease;\n      }\n\n      // Get the destination blob. The destination key always needs to be\n      // normalized.\n      //\n      CloudBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n\n      // TODO: Remove at the time when we move to Azure Java SDK 1.2+.\n      // This is the workaround provided by Azure Java SDK team to\n      // mitigate the issue with un-encoded x-ms-copy-source HTTP\n      // request header. Azure sdk version before 1.2+ does not encode this\n      // header what causes all URIs that have special (category \"other\")\n      // characters in the URI not to work with startCopyFromBlob when\n      // specified as source (requests fail with HTTP 403).\n      URI srcUri \u003d new URI(srcBlob.getUri().toASCIIString());\n\n      // Rename the source blob to the destination blob by copying it to\n      // the destination blob then deleting it.\n      //\n      dstBlob.startCopyFromBlob(srcUri, getInstrumentedContext());\n      waitForCopyToComplete(dstBlob, getInstrumentedContext());\n\n      safeDelete(srcBlob, lease);\n    } catch (Exception e) {\n      // Re-throw exception as an Azure storage exception.\n      throw new AzureException(e);\n    }\n  }",
          "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
          "extendedDetails": {}
        }
      ]
    },
    "81bc395deb3ba00567dc067d6ca71bacf9e3bc82": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-9629. Support Windows Azure Storage - Blob as a file system in Hadoop. Contributed by Dexter Bradshaw, Mostafa Elhemali, Xi Fang, Johannes Klein, David Lao, Mike Liddell, Chuan Liu, Lengning Liu, Ivan Mitic, Michael Rys, Alexander Stojanovic, Brian Swan, and Min Wei.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601781 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/06/14 3:26 PM",
      "commitName": "81bc395deb3ba00567dc067d6ca71bacf9e3bc82",
      "commitAuthor": "Chris Nauroth",
      "diff": "@@ -0,0 +1,42 @@\n+  public void rename(String srcKey, String dstKey) throws IOException {\n+\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Moving \" + srcKey + \" to \" + dstKey);\n+    }\n+\n+    try {\n+      // Attempts rename may occur before opening any streams so first,\n+      // check if a session exists, if not create a session with the Azure\n+      // storage server.\n+      if (null \u003d\u003d storageInteractionLayer) {\n+        final String errMsg \u003d String.format(\n+            \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n+            sessionUri);\n+        throw new AssertionError(errMsg);\n+      }\n+\n+      checkContainer(ContainerAccessType.ReadThenWrite);\n+      // Get the source blob and assert its existence. If the source key\n+      // needs to be normalized then normalize it.\n+      CloudBlockBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n+\n+      if (!srcBlob.exists(getInstrumentedContext())) {\n+        throw new AzureException(\"Source blob \" + srcKey + \" does not exist.\");\n+      }\n+\n+      // Get the destination blob. The destination key always needs to be\n+      // normalized.\n+      CloudBlockBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n+\n+      // Rename the source blob to the destination blob by copying it to\n+      // the destination blob then deleting it.\n+      //\n+      dstBlob.startCopyFromBlob(srcBlob, getInstrumentedContext());\n+      waitForCopyToComplete(dstBlob, getInstrumentedContext());\n+\n+      safeDelete(srcBlob);\n+    } catch (Exception e) {\n+      // Re-throw exception as an Azure storage exception.\n+      throw new AzureException(e);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void rename(String srcKey, String dstKey) throws IOException {\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Moving \" + srcKey + \" to \" + dstKey);\n    }\n\n    try {\n      // Attempts rename may occur before opening any streams so first,\n      // check if a session exists, if not create a session with the Azure\n      // storage server.\n      if (null \u003d\u003d storageInteractionLayer) {\n        final String errMsg \u003d String.format(\n            \"Storage session expected for URI \u0027%s\u0027 but does not exist.\",\n            sessionUri);\n        throw new AssertionError(errMsg);\n      }\n\n      checkContainer(ContainerAccessType.ReadThenWrite);\n      // Get the source blob and assert its existence. If the source key\n      // needs to be normalized then normalize it.\n      CloudBlockBlobWrapper srcBlob \u003d getBlobReference(srcKey);\n\n      if (!srcBlob.exists(getInstrumentedContext())) {\n        throw new AzureException(\"Source blob \" + srcKey + \" does not exist.\");\n      }\n\n      // Get the destination blob. The destination key always needs to be\n      // normalized.\n      CloudBlockBlobWrapper dstBlob \u003d getBlobReference(dstKey);\n\n      // Rename the source blob to the destination blob by copying it to\n      // the destination blob then deleting it.\n      //\n      dstBlob.startCopyFromBlob(srcBlob, getInstrumentedContext());\n      waitForCopyToComplete(dstBlob, getInstrumentedContext());\n\n      safeDelete(srcBlob);\n    } catch (Exception e) {\n      // Re-throw exception as an Azure storage exception.\n      throw new AzureException(e);\n    }\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java"
    }
  }
}