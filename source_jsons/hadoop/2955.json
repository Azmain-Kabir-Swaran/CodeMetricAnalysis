{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSUtilClient.java",
  "functionName": "bytes2byteArray",
  "functionId": "bytes2byteArray___bytes-byte[]__len-int__separator-byte",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java",
  "functionStartLine": 144,
  "functionEndLine": 179,
  "numCommitsSeen": 44,
  "timeTaken": 1717,
  "changeHistory": [
    "b1c7654ee40b372ed777525a42981c7cf55b5c72"
  ],
  "changeHistoryShort": {
    "b1c7654ee40b372ed777525a42981c7cf55b5c72": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b1c7654ee40b372ed777525a42981c7cf55b5c72": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-12594. snapshotDiff fails if the report exceeds the RPC response limit. Contributed by Shashikant Banerjee\n",
      "commitDate": "30/11/17 12:18 PM",
      "commitName": "b1c7654ee40b372ed777525a42981c7cf55b5c72",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "diff": "@@ -0,0 +1,36 @@\n+  public static byte[][] bytes2byteArray(byte[] bytes, int len,\n+      byte separator) {\n+    Preconditions.checkPositionIndex(len, bytes.length);\n+    if (len \u003d\u003d 0) {\n+      return new byte[][]{null};\n+    }\n+    // Count the splits. Omit multiple separators and the last one by\n+    // peeking at prior byte.\n+    int splits \u003d 0;\n+    for (int i \u003d 1; i \u003c len; i++) {\n+      if (bytes[i-1] \u003d\u003d separator \u0026\u0026 bytes[i] !\u003d separator) {\n+        splits++;\n+      }\n+    }\n+    if (splits \u003d\u003d 0 \u0026\u0026 bytes[0] \u003d\u003d separator) {\n+      return new byte[][]{null};\n+    }\n+    splits++;\n+    byte[][] result \u003d new byte[splits][];\n+    int nextIndex \u003d 0;\n+    // Build the splits.\n+    for (int i \u003d 0; i \u003c splits; i++) {\n+      int startIndex \u003d nextIndex;\n+      // find next separator in the bytes.\n+      while (nextIndex \u003c len \u0026\u0026 bytes[nextIndex] !\u003d separator) {\n+        nextIndex++;\n+      }\n+      result[i] \u003d (nextIndex \u003e 0)\n+          ? Arrays.copyOfRange(bytes, startIndex, nextIndex)\n+          : DFSUtilClient.EMPTY_BYTES; // reuse empty bytes for root.\n+      do { // skip over separators.\n+        nextIndex++;\n+      } while (nextIndex \u003c len \u0026\u0026 bytes[nextIndex] \u003d\u003d separator);\n+    }\n+    return result;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static byte[][] bytes2byteArray(byte[] bytes, int len,\n      byte separator) {\n    Preconditions.checkPositionIndex(len, bytes.length);\n    if (len \u003d\u003d 0) {\n      return new byte[][]{null};\n    }\n    // Count the splits. Omit multiple separators and the last one by\n    // peeking at prior byte.\n    int splits \u003d 0;\n    for (int i \u003d 1; i \u003c len; i++) {\n      if (bytes[i-1] \u003d\u003d separator \u0026\u0026 bytes[i] !\u003d separator) {\n        splits++;\n      }\n    }\n    if (splits \u003d\u003d 0 \u0026\u0026 bytes[0] \u003d\u003d separator) {\n      return new byte[][]{null};\n    }\n    splits++;\n    byte[][] result \u003d new byte[splits][];\n    int nextIndex \u003d 0;\n    // Build the splits.\n    for (int i \u003d 0; i \u003c splits; i++) {\n      int startIndex \u003d nextIndex;\n      // find next separator in the bytes.\n      while (nextIndex \u003c len \u0026\u0026 bytes[nextIndex] !\u003d separator) {\n        nextIndex++;\n      }\n      result[i] \u003d (nextIndex \u003e 0)\n          ? Arrays.copyOfRange(bytes, startIndex, nextIndex)\n          : DFSUtilClient.EMPTY_BYTES; // reuse empty bytes for root.\n      do { // skip over separators.\n        nextIndex++;\n      } while (nextIndex \u003c len \u0026\u0026 bytes[nextIndex] \u003d\u003d separator);\n    }\n    return result;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java"
    }
  }
}