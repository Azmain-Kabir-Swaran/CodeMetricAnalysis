{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Journal.java",
  "functionName": "persistPaxosData",
  "functionId": "persistPaxosData___segmentTxId-long__newData-PersistedRecoveryPaxosData",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
  "functionStartLine": 1084,
  "functionEndLine": 1110,
  "numCommitsSeen": 64,
  "timeTaken": 2250,
  "changeHistory": [
    "d7979079ea8c6514858b77a78f0119cffc178086",
    "6449f524552f8c24d20b314ad21f6c579fa08e85",
    "74d4573a23db5586c6e47ff2277aa7c35237da34"
  ],
  "changeHistoryShort": {
    "d7979079ea8c6514858b77a78f0119cffc178086": "Ybodychange",
    "6449f524552f8c24d20b314ad21f6c579fa08e85": "Ybodychange",
    "74d4573a23db5586c6e47ff2277aa7c35237da34": "Yintroduced"
  },
  "changeHistoryDetails": {
    "d7979079ea8c6514858b77a78f0119cffc178086": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16210. Update guava to 27.0-jre in hadoop-project trunk. Contributed by Gabor Bota.\n",
      "commitDate": "03/04/19 11:59 AM",
      "commitName": "d7979079ea8c6514858b77a78f0119cffc178086",
      "commitAuthor": "Gabor Bota",
      "commitDateOld": "24/12/18 9:34 AM",
      "commitNameOld": "1e22f2bfbb1d9a29f5d4fa641b7a0dabd5b1dbf5",
      "commitAuthorOld": "Erik Krogen",
      "daysBetweenCommits": 100.06,
      "commitsBetweenForRepo": 754,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,27 @@\n   private void persistPaxosData(long segmentTxId,\n       PersistedRecoveryPaxosData newData) throws IOException {\n     File f \u003d storage.getPaxosFile(segmentTxId);\n     boolean success \u003d false;\n     AtomicFileOutputStream fos \u003d new AtomicFileOutputStream(f);\n     try {\n       newData.writeDelimitedTo(fos);\n       fos.write(\u0027\\n\u0027);\n       // Write human-readable data after the protobuf. This is only\n       // to assist in debugging -- it\u0027s not parsed at all.\n-      OutputStreamWriter writer \u003d new OutputStreamWriter(fos, Charsets.UTF_8);\n-      \n-      writer.write(String.valueOf(newData));\n-      writer.write(\u0027\\n\u0027);\n-      writer.flush();\n+      try(OutputStreamWriter writer \u003d\n+          new OutputStreamWriter(fos, Charsets.UTF_8)) {\n+        writer.write(String.valueOf(newData));\n+        writer.write(\u0027\\n\u0027);\n+        writer.flush();\n+      }\n       \n       fos.flush();\n       success \u003d true;\n     } finally {\n       if (success) {\n         IOUtils.closeStream(fos);\n       } else {\n         fos.abort();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void persistPaxosData(long segmentTxId,\n      PersistedRecoveryPaxosData newData) throws IOException {\n    File f \u003d storage.getPaxosFile(segmentTxId);\n    boolean success \u003d false;\n    AtomicFileOutputStream fos \u003d new AtomicFileOutputStream(f);\n    try {\n      newData.writeDelimitedTo(fos);\n      fos.write(\u0027\\n\u0027);\n      // Write human-readable data after the protobuf. This is only\n      // to assist in debugging -- it\u0027s not parsed at all.\n      try(OutputStreamWriter writer \u003d\n          new OutputStreamWriter(fos, Charsets.UTF_8)) {\n        writer.write(String.valueOf(newData));\n        writer.write(\u0027\\n\u0027);\n        writer.flush();\n      }\n      \n      fos.flush();\n      success \u003d true;\n    } finally {\n      if (success) {\n        IOUtils.closeStream(fos);\n      } else {\n        fos.abort();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "6449f524552f8c24d20b314ad21f6c579fa08e85": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4032. Specify the charset explicitly rather than rely on the default. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1431179 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 6:30 PM",
      "commitName": "6449f524552f8c24d20b314ad21f6c579fa08e85",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "19/09/12 4:40 PM",
      "commitNameOld": "8a3e64c77f73998326e9d72df75597bb0ad7b857",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 112.12,
      "commitsBetweenForRepo": 543,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n   private void persistPaxosData(long segmentTxId,\n       PersistedRecoveryPaxosData newData) throws IOException {\n     File f \u003d storage.getPaxosFile(segmentTxId);\n     boolean success \u003d false;\n     AtomicFileOutputStream fos \u003d new AtomicFileOutputStream(f);\n     try {\n       newData.writeDelimitedTo(fos);\n       fos.write(\u0027\\n\u0027);\n       // Write human-readable data after the protobuf. This is only\n       // to assist in debugging -- it\u0027s not parsed at all.\n-      OutputStreamWriter writer \u003d new OutputStreamWriter(fos);\n+      OutputStreamWriter writer \u003d new OutputStreamWriter(fos, Charsets.UTF_8);\n       \n       writer.write(String.valueOf(newData));\n       writer.write(\u0027\\n\u0027);\n       writer.flush();\n       \n       fos.flush();\n       success \u003d true;\n     } finally {\n       if (success) {\n         IOUtils.closeStream(fos);\n       } else {\n         fos.abort();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void persistPaxosData(long segmentTxId,\n      PersistedRecoveryPaxosData newData) throws IOException {\n    File f \u003d storage.getPaxosFile(segmentTxId);\n    boolean success \u003d false;\n    AtomicFileOutputStream fos \u003d new AtomicFileOutputStream(f);\n    try {\n      newData.writeDelimitedTo(fos);\n      fos.write(\u0027\\n\u0027);\n      // Write human-readable data after the protobuf. This is only\n      // to assist in debugging -- it\u0027s not parsed at all.\n      OutputStreamWriter writer \u003d new OutputStreamWriter(fos, Charsets.UTF_8);\n      \n      writer.write(String.valueOf(newData));\n      writer.write(\u0027\\n\u0027);\n      writer.flush();\n      \n      fos.flush();\n      success \u003d true;\n    } finally {\n      if (success) {\n        IOUtils.closeStream(fos);\n      } else {\n        fos.abort();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "74d4573a23db5586c6e47ff2277aa7c35237da34": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3077. Quorum-based protocol for reading and writing edit logs. Contributed by Todd Lipcon based on initial work from Brandon Li and Hari Mankude.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1363596 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/07/12 5:25 PM",
      "commitName": "74d4573a23db5586c6e47ff2277aa7c35237da34",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,26 @@\n+  private void persistPaxosData(long segmentTxId,\n+      PersistedRecoveryPaxosData newData) throws IOException {\n+    File f \u003d storage.getPaxosFile(segmentTxId);\n+    boolean success \u003d false;\n+    AtomicFileOutputStream fos \u003d new AtomicFileOutputStream(f);\n+    try {\n+      newData.writeDelimitedTo(fos);\n+      fos.write(\u0027\\n\u0027);\n+      // Write human-readable data after the protobuf. This is only\n+      // to assist in debugging -- it\u0027s not parsed at all.\n+      OutputStreamWriter writer \u003d new OutputStreamWriter(fos);\n+      \n+      writer.write(String.valueOf(newData));\n+      writer.write(\u0027\\n\u0027);\n+      writer.flush();\n+      \n+      fos.flush();\n+      success \u003d true;\n+    } finally {\n+      if (success) {\n+        IOUtils.closeStream(fos);\n+      } else {\n+        fos.abort();\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void persistPaxosData(long segmentTxId,\n      PersistedRecoveryPaxosData newData) throws IOException {\n    File f \u003d storage.getPaxosFile(segmentTxId);\n    boolean success \u003d false;\n    AtomicFileOutputStream fos \u003d new AtomicFileOutputStream(f);\n    try {\n      newData.writeDelimitedTo(fos);\n      fos.write(\u0027\\n\u0027);\n      // Write human-readable data after the protobuf. This is only\n      // to assist in debugging -- it\u0027s not parsed at all.\n      OutputStreamWriter writer \u003d new OutputStreamWriter(fos);\n      \n      writer.write(String.valueOf(newData));\n      writer.write(\u0027\\n\u0027);\n      writer.flush();\n      \n      fos.flush();\n      success \u003d true;\n    } finally {\n      if (success) {\n        IOUtils.closeStream(fos);\n      } else {\n        fos.abort();\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java"
    }
  }
}