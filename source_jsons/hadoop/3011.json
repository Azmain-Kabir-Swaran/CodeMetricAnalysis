{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PositionStripeReader.java",
  "functionName": "initDecodeInputs",
  "functionId": "initDecodeInputs___alignedStripe-AlignedStripe",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PositionStripeReader.java",
  "functionStartLine": 77,
  "functionEndLine": 94,
  "numCommitsSeen": 14,
  "timeTaken": 3031,
  "changeHistory": [
    "7a96033b15580a01a2867fa3cab9c1e409dbaafd",
    "734d54c1a8950446e68098f62d8964e02ecc2890",
    "401db4fc65140979fe7665983e36905e886df971",
    "c201cf951d5adefefe7c68e882a0c07962248577"
  ],
  "changeHistoryShort": {
    "7a96033b15580a01a2867fa3cab9c1e409dbaafd": "Ybodychange",
    "734d54c1a8950446e68098f62d8964e02ecc2890": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yparameterchange)",
    "401db4fc65140979fe7665983e36905e886df971": "Ymultichange(Yreturntypechange,Ybodychange)",
    "c201cf951d5adefefe7c68e882a0c07962248577": "Ybodychange"
  },
  "changeHistoryDetails": {
    "7a96033b15580a01a2867fa3cab9c1e409dbaafd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11964. Decoding inputs should be correctly prepared in pread. Contributed by Takanobu Asanuma\n",
      "commitDate": "01/09/17 2:48 AM",
      "commitName": "7a96033b15580a01a2867fa3cab9c1e409dbaafd",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "17/11/16 8:48 PM",
      "commitNameOld": "c0b1a44f6c6e6f9e4ac5cecea0d4a50e237a4c9c",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 287.21,
      "commitsBetweenForRepo": 1648,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   void initDecodeInputs(AlignedStripe alignedStripe) {\n     int bufLen \u003d (int) alignedStripe.getSpanInBlock();\n     int bufCount \u003d dataBlkNum + parityBlkNum;\n     codingBuffer \u003d dfsStripedInputStream.getBufferPool().\n         getBuffer(useDirectBuffer(), bufLen * bufCount);\n     ByteBuffer buffer;\n-    for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n+    for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n       buffer \u003d codingBuffer.duplicate();\n       decodeInputs[i] \u003d new ECChunk(buffer, i * bufLen, bufLen);\n     }\n \n     for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n       if (alignedStripe.chunks[i] \u003d\u003d null) {\n         alignedStripe.chunks[i] \u003d\n             new StripingChunk(decodeInputs[i].getBuffer());\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void initDecodeInputs(AlignedStripe alignedStripe) {\n    int bufLen \u003d (int) alignedStripe.getSpanInBlock();\n    int bufCount \u003d dataBlkNum + parityBlkNum;\n    codingBuffer \u003d dfsStripedInputStream.getBufferPool().\n        getBuffer(useDirectBuffer(), bufLen * bufCount);\n    ByteBuffer buffer;\n    for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n      buffer \u003d codingBuffer.duplicate();\n      decodeInputs[i] \u003d new ECChunk(buffer, i * bufLen, bufLen);\n    }\n\n    for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n      if (alignedStripe.chunks[i] \u003d\u003d null) {\n        alignedStripe.chunks[i] \u003d\n            new StripingChunk(decodeInputs[i].getBuffer());\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PositionStripeReader.java",
      "extendedDetails": {}
    },
    "734d54c1a8950446e68098f62d8964e02ecc2890": {
      "type": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
      "commitDate": "21/09/16 6:34 AM",
      "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
      "commitAuthor": "Kai Zheng",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
          "commitDate": "21/09/16 6:34 AM",
          "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "20/09/16 12:03 AM",
          "commitNameOld": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,18 @@\n-  public static ByteBuffer[] initDecodeInputs(AlignedStripe alignedStripe,\n-      int dataBlkNum, int parityBlkNum) {\n-    ByteBuffer[] decodeInputs \u003d new ByteBuffer[dataBlkNum + parityBlkNum];\n+  void initDecodeInputs(AlignedStripe alignedStripe) {\n+    int bufLen \u003d (int) alignedStripe.getSpanInBlock();\n+    int bufCount \u003d dataBlkNum + parityBlkNum;\n+    codingBuffer \u003d dfsStripedInputStream.getBufferPool().\n+        getBuffer(useDirectBuffer(), bufLen * bufCount);\n+    ByteBuffer buffer;\n     for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n-      decodeInputs[i] \u003d ByteBuffer.allocate(\n-          (int) alignedStripe.getSpanInBlock());\n+      buffer \u003d codingBuffer.duplicate();\n+      decodeInputs[i] \u003d new ECChunk(buffer, i * bufLen, bufLen);\n     }\n-    // read the full data aligned stripe\n+\n     for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n       if (alignedStripe.chunks[i] \u003d\u003d null) {\n-        alignedStripe.chunks[i] \u003d new StripingChunk(decodeInputs[i]);\n+        alignedStripe.chunks[i] \u003d\n+            new StripingChunk(decodeInputs[i].getBuffer());\n       }\n     }\n-    return decodeInputs;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void initDecodeInputs(AlignedStripe alignedStripe) {\n    int bufLen \u003d (int) alignedStripe.getSpanInBlock();\n    int bufCount \u003d dataBlkNum + parityBlkNum;\n    codingBuffer \u003d dfsStripedInputStream.getBufferPool().\n        getBuffer(useDirectBuffer(), bufLen * bufCount);\n    ByteBuffer buffer;\n    for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n      buffer \u003d codingBuffer.duplicate();\n      decodeInputs[i] \u003d new ECChunk(buffer, i * bufLen, bufLen);\n    }\n\n    for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n      if (alignedStripe.chunks[i] \u003d\u003d null) {\n        alignedStripe.chunks[i] \u003d\n            new StripingChunk(decodeInputs[i].getBuffer());\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PositionStripeReader.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PositionStripeReader.java",
            "oldMethodName": "initDecodeInputs",
            "newMethodName": "initDecodeInputs"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
          "commitDate": "21/09/16 6:34 AM",
          "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "20/09/16 12:03 AM",
          "commitNameOld": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,18 @@\n-  public static ByteBuffer[] initDecodeInputs(AlignedStripe alignedStripe,\n-      int dataBlkNum, int parityBlkNum) {\n-    ByteBuffer[] decodeInputs \u003d new ByteBuffer[dataBlkNum + parityBlkNum];\n+  void initDecodeInputs(AlignedStripe alignedStripe) {\n+    int bufLen \u003d (int) alignedStripe.getSpanInBlock();\n+    int bufCount \u003d dataBlkNum + parityBlkNum;\n+    codingBuffer \u003d dfsStripedInputStream.getBufferPool().\n+        getBuffer(useDirectBuffer(), bufLen * bufCount);\n+    ByteBuffer buffer;\n     for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n-      decodeInputs[i] \u003d ByteBuffer.allocate(\n-          (int) alignedStripe.getSpanInBlock());\n+      buffer \u003d codingBuffer.duplicate();\n+      decodeInputs[i] \u003d new ECChunk(buffer, i * bufLen, bufLen);\n     }\n-    // read the full data aligned stripe\n+\n     for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n       if (alignedStripe.chunks[i] \u003d\u003d null) {\n-        alignedStripe.chunks[i] \u003d new StripingChunk(decodeInputs[i]);\n+        alignedStripe.chunks[i] \u003d\n+            new StripingChunk(decodeInputs[i].getBuffer());\n       }\n     }\n-    return decodeInputs;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void initDecodeInputs(AlignedStripe alignedStripe) {\n    int bufLen \u003d (int) alignedStripe.getSpanInBlock();\n    int bufCount \u003d dataBlkNum + parityBlkNum;\n    codingBuffer \u003d dfsStripedInputStream.getBufferPool().\n        getBuffer(useDirectBuffer(), bufLen * bufCount);\n    ByteBuffer buffer;\n    for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n      buffer \u003d codingBuffer.duplicate();\n      decodeInputs[i] \u003d new ECChunk(buffer, i * bufLen, bufLen);\n    }\n\n    for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n      if (alignedStripe.chunks[i] \u003d\u003d null) {\n        alignedStripe.chunks[i] \u003d\n            new StripingChunk(decodeInputs[i].getBuffer());\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PositionStripeReader.java",
          "extendedDetails": {
            "oldValue": "ByteBuffer[]",
            "newValue": "void"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
          "commitDate": "21/09/16 6:34 AM",
          "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "20/09/16 12:03 AM",
          "commitNameOld": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,18 @@\n-  public static ByteBuffer[] initDecodeInputs(AlignedStripe alignedStripe,\n-      int dataBlkNum, int parityBlkNum) {\n-    ByteBuffer[] decodeInputs \u003d new ByteBuffer[dataBlkNum + parityBlkNum];\n+  void initDecodeInputs(AlignedStripe alignedStripe) {\n+    int bufLen \u003d (int) alignedStripe.getSpanInBlock();\n+    int bufCount \u003d dataBlkNum + parityBlkNum;\n+    codingBuffer \u003d dfsStripedInputStream.getBufferPool().\n+        getBuffer(useDirectBuffer(), bufLen * bufCount);\n+    ByteBuffer buffer;\n     for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n-      decodeInputs[i] \u003d ByteBuffer.allocate(\n-          (int) alignedStripe.getSpanInBlock());\n+      buffer \u003d codingBuffer.duplicate();\n+      decodeInputs[i] \u003d new ECChunk(buffer, i * bufLen, bufLen);\n     }\n-    // read the full data aligned stripe\n+\n     for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n       if (alignedStripe.chunks[i] \u003d\u003d null) {\n-        alignedStripe.chunks[i] \u003d new StripingChunk(decodeInputs[i]);\n+        alignedStripe.chunks[i] \u003d\n+            new StripingChunk(decodeInputs[i].getBuffer());\n       }\n     }\n-    return decodeInputs;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void initDecodeInputs(AlignedStripe alignedStripe) {\n    int bufLen \u003d (int) alignedStripe.getSpanInBlock();\n    int bufCount \u003d dataBlkNum + parityBlkNum;\n    codingBuffer \u003d dfsStripedInputStream.getBufferPool().\n        getBuffer(useDirectBuffer(), bufLen * bufCount);\n    ByteBuffer buffer;\n    for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n      buffer \u003d codingBuffer.duplicate();\n      decodeInputs[i] \u003d new ECChunk(buffer, i * bufLen, bufLen);\n    }\n\n    for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n      if (alignedStripe.chunks[i] \u003d\u003d null) {\n        alignedStripe.chunks[i] \u003d\n            new StripingChunk(decodeInputs[i].getBuffer());\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PositionStripeReader.java",
          "extendedDetails": {
            "oldValue": "[public, static]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
          "commitDate": "21/09/16 6:34 AM",
          "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "20/09/16 12:03 AM",
          "commitNameOld": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,18 @@\n-  public static ByteBuffer[] initDecodeInputs(AlignedStripe alignedStripe,\n-      int dataBlkNum, int parityBlkNum) {\n-    ByteBuffer[] decodeInputs \u003d new ByteBuffer[dataBlkNum + parityBlkNum];\n+  void initDecodeInputs(AlignedStripe alignedStripe) {\n+    int bufLen \u003d (int) alignedStripe.getSpanInBlock();\n+    int bufCount \u003d dataBlkNum + parityBlkNum;\n+    codingBuffer \u003d dfsStripedInputStream.getBufferPool().\n+        getBuffer(useDirectBuffer(), bufLen * bufCount);\n+    ByteBuffer buffer;\n     for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n-      decodeInputs[i] \u003d ByteBuffer.allocate(\n-          (int) alignedStripe.getSpanInBlock());\n+      buffer \u003d codingBuffer.duplicate();\n+      decodeInputs[i] \u003d new ECChunk(buffer, i * bufLen, bufLen);\n     }\n-    // read the full data aligned stripe\n+\n     for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n       if (alignedStripe.chunks[i] \u003d\u003d null) {\n-        alignedStripe.chunks[i] \u003d new StripingChunk(decodeInputs[i]);\n+        alignedStripe.chunks[i] \u003d\n+            new StripingChunk(decodeInputs[i].getBuffer());\n       }\n     }\n-    return decodeInputs;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void initDecodeInputs(AlignedStripe alignedStripe) {\n    int bufLen \u003d (int) alignedStripe.getSpanInBlock();\n    int bufCount \u003d dataBlkNum + parityBlkNum;\n    codingBuffer \u003d dfsStripedInputStream.getBufferPool().\n        getBuffer(useDirectBuffer(), bufLen * bufCount);\n    ByteBuffer buffer;\n    for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n      buffer \u003d codingBuffer.duplicate();\n      decodeInputs[i] \u003d new ECChunk(buffer, i * bufLen, bufLen);\n    }\n\n    for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n      if (alignedStripe.chunks[i] \u003d\u003d null) {\n        alignedStripe.chunks[i] \u003d\n            new StripingChunk(decodeInputs[i].getBuffer());\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PositionStripeReader.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10861. Refactor StripeReaders and use ECChunk version decode API. Contributed by Sammi Chen\n",
          "commitDate": "21/09/16 6:34 AM",
          "commitName": "734d54c1a8950446e68098f62d8964e02ecc2890",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "20/09/16 12:03 AM",
          "commitNameOld": "2b66d9ec5bdaec7e6b278926fbb6f222c4e3afaa",
          "commitAuthorOld": "Jian He",
          "daysBetweenCommits": 1.27,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,18 @@\n-  public static ByteBuffer[] initDecodeInputs(AlignedStripe alignedStripe,\n-      int dataBlkNum, int parityBlkNum) {\n-    ByteBuffer[] decodeInputs \u003d new ByteBuffer[dataBlkNum + parityBlkNum];\n+  void initDecodeInputs(AlignedStripe alignedStripe) {\n+    int bufLen \u003d (int) alignedStripe.getSpanInBlock();\n+    int bufCount \u003d dataBlkNum + parityBlkNum;\n+    codingBuffer \u003d dfsStripedInputStream.getBufferPool().\n+        getBuffer(useDirectBuffer(), bufLen * bufCount);\n+    ByteBuffer buffer;\n     for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n-      decodeInputs[i] \u003d ByteBuffer.allocate(\n-          (int) alignedStripe.getSpanInBlock());\n+      buffer \u003d codingBuffer.duplicate();\n+      decodeInputs[i] \u003d new ECChunk(buffer, i * bufLen, bufLen);\n     }\n-    // read the full data aligned stripe\n+\n     for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n       if (alignedStripe.chunks[i] \u003d\u003d null) {\n-        alignedStripe.chunks[i] \u003d new StripingChunk(decodeInputs[i]);\n+        alignedStripe.chunks[i] \u003d\n+            new StripingChunk(decodeInputs[i].getBuffer());\n       }\n     }\n-    return decodeInputs;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void initDecodeInputs(AlignedStripe alignedStripe) {\n    int bufLen \u003d (int) alignedStripe.getSpanInBlock();\n    int bufCount \u003d dataBlkNum + parityBlkNum;\n    codingBuffer \u003d dfsStripedInputStream.getBufferPool().\n        getBuffer(useDirectBuffer(), bufLen * bufCount);\n    ByteBuffer buffer;\n    for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n      buffer \u003d codingBuffer.duplicate();\n      decodeInputs[i] \u003d new ECChunk(buffer, i * bufLen, bufLen);\n    }\n\n    for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n      if (alignedStripe.chunks[i] \u003d\u003d null) {\n        alignedStripe.chunks[i] \u003d\n            new StripingChunk(decodeInputs[i].getBuffer());\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/PositionStripeReader.java",
          "extendedDetails": {
            "oldValue": "[alignedStripe-AlignedStripe, dataBlkNum-int, parityBlkNum-int]",
            "newValue": "[alignedStripe-AlignedStripe]"
          }
        }
      ]
    },
    "401db4fc65140979fe7665983e36905e886df971": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
      "commitDate": "08/09/16 11:54 AM",
      "commitName": "401db4fc65140979fe7665983e36905e886df971",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
          "commitDate": "08/09/16 11:54 AM",
          "commitName": "401db4fc65140979fe7665983e36905e886df971",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "06/04/16 10:50 PM",
          "commitNameOld": "3c18a53cbd2efabb2ad108d63a0b0b558424115f",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 154.54,
          "commitsBetweenForRepo": 1132,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,15 @@\n-  public static byte[][] initDecodeInputs(AlignedStripe alignedStripe,\n+  public static ByteBuffer[] initDecodeInputs(AlignedStripe alignedStripe,\n       int dataBlkNum, int parityBlkNum) {\n-    byte[][] decodeInputs \u003d\n-        new byte[dataBlkNum + parityBlkNum][(int) alignedStripe.getSpanInBlock()];\n+    ByteBuffer[] decodeInputs \u003d new ByteBuffer[dataBlkNum + parityBlkNum];\n+    for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n+      decodeInputs[i] \u003d ByteBuffer.allocate(\n+          (int) alignedStripe.getSpanInBlock());\n+    }\n     // read the full data aligned stripe\n     for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n       if (alignedStripe.chunks[i] \u003d\u003d null) {\n         alignedStripe.chunks[i] \u003d new StripingChunk(decodeInputs[i]);\n-        alignedStripe.chunks[i].addByteArraySlice(0,\n-            (int) alignedStripe.getSpanInBlock());\n       }\n     }\n     return decodeInputs;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static ByteBuffer[] initDecodeInputs(AlignedStripe alignedStripe,\n      int dataBlkNum, int parityBlkNum) {\n    ByteBuffer[] decodeInputs \u003d new ByteBuffer[dataBlkNum + parityBlkNum];\n    for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n      decodeInputs[i] \u003d ByteBuffer.allocate(\n          (int) alignedStripe.getSpanInBlock());\n    }\n    // read the full data aligned stripe\n    for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n      if (alignedStripe.chunks[i] \u003d\u003d null) {\n        alignedStripe.chunks[i] \u003d new StripingChunk(decodeInputs[i]);\n      }\n    }\n    return decodeInputs;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
          "extendedDetails": {
            "oldValue": "byte[][]",
            "newValue": "ByteBuffer[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
          "commitDate": "08/09/16 11:54 AM",
          "commitName": "401db4fc65140979fe7665983e36905e886df971",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "06/04/16 10:50 PM",
          "commitNameOld": "3c18a53cbd2efabb2ad108d63a0b0b558424115f",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 154.54,
          "commitsBetweenForRepo": 1132,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,15 @@\n-  public static byte[][] initDecodeInputs(AlignedStripe alignedStripe,\n+  public static ByteBuffer[] initDecodeInputs(AlignedStripe alignedStripe,\n       int dataBlkNum, int parityBlkNum) {\n-    byte[][] decodeInputs \u003d\n-        new byte[dataBlkNum + parityBlkNum][(int) alignedStripe.getSpanInBlock()];\n+    ByteBuffer[] decodeInputs \u003d new ByteBuffer[dataBlkNum + parityBlkNum];\n+    for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n+      decodeInputs[i] \u003d ByteBuffer.allocate(\n+          (int) alignedStripe.getSpanInBlock());\n+    }\n     // read the full data aligned stripe\n     for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n       if (alignedStripe.chunks[i] \u003d\u003d null) {\n         alignedStripe.chunks[i] \u003d new StripingChunk(decodeInputs[i]);\n-        alignedStripe.chunks[i].addByteArraySlice(0,\n-            (int) alignedStripe.getSpanInBlock());\n       }\n     }\n     return decodeInputs;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static ByteBuffer[] initDecodeInputs(AlignedStripe alignedStripe,\n      int dataBlkNum, int parityBlkNum) {\n    ByteBuffer[] decodeInputs \u003d new ByteBuffer[dataBlkNum + parityBlkNum];\n    for (int i \u003d 0; i \u003c decodeInputs.length; i++) {\n      decodeInputs[i] \u003d ByteBuffer.allocate(\n          (int) alignedStripe.getSpanInBlock());\n    }\n    // read the full data aligned stripe\n    for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n      if (alignedStripe.chunks[i] \u003d\u003d null) {\n        alignedStripe.chunks[i] \u003d new StripingChunk(decodeInputs[i]);\n      }\n    }\n    return decodeInputs;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
          "extendedDetails": {}
        }
      ]
    },
    "c201cf951d5adefefe7c68e882a0c07962248577": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12040. Adjust inputs order for the decode API in raw erasure coder. (Kai Zheng via yliu)\n",
      "commitDate": "28/10/15 1:18 AM",
      "commitName": "c201cf951d5adefefe7c68e882a0c07962248577",
      "commitAuthor": "yliu",
      "commitDateOld": "03/10/15 11:38 AM",
      "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 24.57,
      "commitsBetweenForRepo": 211,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,14 @@\n   public static byte[][] initDecodeInputs(AlignedStripe alignedStripe,\n       int dataBlkNum, int parityBlkNum) {\n     byte[][] decodeInputs \u003d\n         new byte[dataBlkNum + parityBlkNum][(int) alignedStripe.getSpanInBlock()];\n     // read the full data aligned stripe\n     for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n       if (alignedStripe.chunks[i] \u003d\u003d null) {\n-        final int decodeIndex \u003d convertIndex4Decode(i, dataBlkNum, parityBlkNum);\n-        alignedStripe.chunks[i] \u003d new StripingChunk(decodeInputs[decodeIndex]);\n+        alignedStripe.chunks[i] \u003d new StripingChunk(decodeInputs[i]);\n         alignedStripe.chunks[i].addByteArraySlice(0,\n             (int) alignedStripe.getSpanInBlock());\n       }\n     }\n     return decodeInputs;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static byte[][] initDecodeInputs(AlignedStripe alignedStripe,\n      int dataBlkNum, int parityBlkNum) {\n    byte[][] decodeInputs \u003d\n        new byte[dataBlkNum + parityBlkNum][(int) alignedStripe.getSpanInBlock()];\n    // read the full data aligned stripe\n    for (int i \u003d 0; i \u003c dataBlkNum; i++) {\n      if (alignedStripe.chunks[i] \u003d\u003d null) {\n        alignedStripe.chunks[i] \u003d new StripingChunk(decodeInputs[i]);\n        alignedStripe.chunks[i].addByteArraySlice(0,\n            (int) alignedStripe.getSpanInBlock());\n      }\n    }\n    return decodeInputs;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java",
      "extendedDetails": {}
    }
  }
}