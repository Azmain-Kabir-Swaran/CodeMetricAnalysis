{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "AbstractLivelinessMonitor.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java",
  "functionStartLine": 134,
  "functionEndLine": 161,
  "numCommitsSeen": 16,
  "timeTaken": 8901,
  "changeHistory": [
    "2ae5a3a5bf5ea355370469a53eeccff0b5220081",
    "e1fdf62123625e4ba399af02f8aad500637d29d1",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "2ae5a3a5bf5ea355370469a53eeccff0b5220081": "Ybodychange",
    "e1fdf62123625e4ba399af02f8aad500637d29d1": "Yfilerename",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymultichange(Ymovefromfile,Ybodychange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "2ae5a3a5bf5ea355370469a53eeccff0b5220081": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4205. Add a service for monitoring application life time out. Contributed by Rohith Sharma K S\n",
      "commitDate": "29/09/16 7:00 AM",
      "commitName": "2ae5a3a5bf5ea355370469a53eeccff0b5220081",
      "commitAuthor": "Jian He",
      "commitDateOld": "14/12/15 1:51 PM",
      "commitNameOld": "1cb3299b48a06a842aa3f6cf37ccf44a49af43b5",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 289.67,
      "commitsBetweenForRepo": 1976,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,28 @@\n     public void run() {\n       while (!stopped \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n         synchronized (AbstractLivelinessMonitor.this) {\n-          Iterator\u003cMap.Entry\u003cO, Long\u003e\u003e iterator \u003d \n-            running.entrySet().iterator();\n+          Iterator\u003cMap.Entry\u003cO, Long\u003e\u003e iterator \u003d running.entrySet().iterator();\n \n-          //avoid calculating current time everytime in loop\n+          // avoid calculating current time everytime in loop\n           long currentTime \u003d clock.getTime();\n \n           while (iterator.hasNext()) {\n             Map.Entry\u003cO, Long\u003e entry \u003d iterator.next();\n-            if (currentTime \u003e entry.getValue() + expireInterval) {\n+            O key \u003d entry.getKey();\n+            long interval \u003d getExpireInterval(key);\n+            if (currentTime \u003e entry.getValue() + interval) {\n               iterator.remove();\n-              expire(entry.getKey());\n-              LOG.info(\"Expired:\" + entry.getKey().toString() + \n-                      \" Timed out after \" + expireInterval/1000 + \" secs\");\n+              expire(key);\n+              LOG.info(\"Expired:\" + entry.getKey().toString()\n+                  + \" Timed out after \" + interval / 1000 + \" secs\");\n             }\n           }\n         }\n         try {\n           Thread.sleep(monitorInterval);\n         } catch (InterruptedException e) {\n           LOG.info(getName() + \" thread interrupted\");\n           break;\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      while (!stopped \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n        synchronized (AbstractLivelinessMonitor.this) {\n          Iterator\u003cMap.Entry\u003cO, Long\u003e\u003e iterator \u003d running.entrySet().iterator();\n\n          // avoid calculating current time everytime in loop\n          long currentTime \u003d clock.getTime();\n\n          while (iterator.hasNext()) {\n            Map.Entry\u003cO, Long\u003e entry \u003d iterator.next();\n            O key \u003d entry.getKey();\n            long interval \u003d getExpireInterval(key);\n            if (currentTime \u003e entry.getValue() + interval) {\n              iterator.remove();\n              expire(key);\n              LOG.info(\"Expired:\" + entry.getKey().toString()\n                  + \" Timed out after \" + interval / 1000 + \" secs\");\n            }\n          }\n        }\n        try {\n          Thread.sleep(monitorInterval);\n        } catch (InterruptedException e) {\n          LOG.info(getName() + \" thread interrupted\");\n          break;\n        }\n      }\n    }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java",
      "extendedDetails": {}
    },
    "e1fdf62123625e4ba399af02f8aad500637d29d1": {
      "type": "Yfilerename",
      "commitMessage": "YARN-1. Promote YARN to be a sub-project of Apache Hadoop.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370666 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 10:22 PM",
      "commitName": "e1fdf62123625e4ba399af02f8aad500637d29d1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "07/08/12 7:53 PM",
      "commitNameOld": "34554d1e11ee1d5b564d7d9ed3e6d55931d72749",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void run() {\n      while (!stopped \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n        synchronized (AbstractLivelinessMonitor.this) {\n          Iterator\u003cMap.Entry\u003cO, Long\u003e\u003e iterator \u003d \n            running.entrySet().iterator();\n\n          //avoid calculating current time everytime in loop\n          long currentTime \u003d clock.getTime();\n\n          while (iterator.hasNext()) {\n            Map.Entry\u003cO, Long\u003e entry \u003d iterator.next();\n            if (currentTime \u003e entry.getValue() + expireInterval) {\n              iterator.remove();\n              expire(entry.getKey());\n              LOG.info(\"Expired:\" + entry.getKey().toString() + \n                      \" Timed out after \" + expireInterval/1000 + \" secs\");\n            }\n          }\n        }\n        try {\n          Thread.sleep(monitorInterval);\n        } catch (InterruptedException e) {\n          LOG.info(getName() + \" thread interrupted\");\n          break;\n        }\n      }\n    }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void run() {\n      while (!stopped \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n        synchronized (AbstractLivelinessMonitor.this) {\n          Iterator\u003cMap.Entry\u003cO, Long\u003e\u003e iterator \u003d \n            running.entrySet().iterator();\n\n          //avoid calculating current time everytime in loop\n          long currentTime \u003d clock.getTime();\n\n          while (iterator.hasNext()) {\n            Map.Entry\u003cO, Long\u003e entry \u003d iterator.next();\n            if (currentTime \u003e entry.getValue() + expireInterval) {\n              iterator.remove();\n              expire(entry.getKey());\n              LOG.info(\"Expired:\" + entry.getKey().toString() + \n                      \" Timed out after \" + expireInterval/1000 + \" secs\");\n            }\n          }\n        }\n        try {\n          Thread.sleep(monitorInterval);\n        } catch (InterruptedException e) {\n          LOG.info(getName() + \" thread interrupted\");\n          break;\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java",
        "newPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,27 @@\n-  public void run() {\n-    int failures \u003d 0;\n-    LOG.info(reduce + \" Thread started: \" + getName());\n-    \n-    try {\n-      while (true) {\n+    public void run() {\n+      while (!stopped \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n+        synchronized (AbstractLivelinessMonitor.this) {\n+          Iterator\u003cMap.Entry\u003cO, Long\u003e\u003e iterator \u003d \n+            running.entrySet().iterator();\n+\n+          //avoid calculating current time everytime in loop\n+          long currentTime \u003d clock.getTime();\n+\n+          while (iterator.hasNext()) {\n+            Map.Entry\u003cO, Long\u003e entry \u003d iterator.next();\n+            if (currentTime \u003e entry.getValue() + expireInterval) {\n+              iterator.remove();\n+              expire(entry.getKey());\n+              LOG.info(\"Expired:\" + entry.getKey().toString() + \n+                      \" Timed out after \" + expireInterval/1000 + \" secs\");\n+            }\n+          }\n+        }\n         try {\n-          int numNewMaps \u003d getMapCompletionEvents();\n-          failures \u003d 0;\n-          if (numNewMaps \u003e 0) {\n-            LOG.info(reduce + \": \" + \"Got \" + numNewMaps + \" new map-outputs\");\n-          }\n-          LOG.debug(\"GetMapEventsThread about to sleep for \" + SLEEP_TIME);\n-          Thread.sleep(SLEEP_TIME);\n-        } catch (IOException ie) {\n-          LOG.info(\"Exception in getting events\", ie);\n-          // check to see whether to abort\n-          if (++failures \u003e\u003d MAX_RETRIES) {\n-            throw new IOException(\"too many failures downloading events\", ie);\n-          }\n-          // sleep for a bit\n-          Thread.sleep(RETRY_PERIOD);\n+          Thread.sleep(monitorInterval);\n+        } catch (InterruptedException e) {\n+          LOG.info(getName() + \" thread interrupted\");\n+          break;\n         }\n       }\n-    } catch (InterruptedException e) {\n-      return;\n-    } catch (Throwable t) {\n-      exceptionReporter.reportException(t);\n-      return;\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "    public void run() {\n      while (!stopped \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n        synchronized (AbstractLivelinessMonitor.this) {\n          Iterator\u003cMap.Entry\u003cO, Long\u003e\u003e iterator \u003d \n            running.entrySet().iterator();\n\n          //avoid calculating current time everytime in loop\n          long currentTime \u003d clock.getTime();\n\n          while (iterator.hasNext()) {\n            Map.Entry\u003cO, Long\u003e entry \u003d iterator.next();\n            if (currentTime \u003e entry.getValue() + expireInterval) {\n              iterator.remove();\n              expire(entry.getKey());\n              LOG.info(\"Expired:\" + entry.getKey().toString() + \n                      \" Timed out after \" + expireInterval/1000 + \" secs\");\n            }\n          }\n        }\n        try {\n          Thread.sleep(monitorInterval);\n        } catch (InterruptedException e) {\n          LOG.info(getName() + \" thread interrupted\");\n          break;\n        }\n      }\n    }",
          "path": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java",
          "extendedDetails": {
            "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java",
            "newPath": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java",
            "oldMethodName": "run",
            "newMethodName": "run"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,27 @@\n-  public void run() {\n-    int failures \u003d 0;\n-    LOG.info(reduce + \" Thread started: \" + getName());\n-    \n-    try {\n-      while (true) {\n+    public void run() {\n+      while (!stopped \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n+        synchronized (AbstractLivelinessMonitor.this) {\n+          Iterator\u003cMap.Entry\u003cO, Long\u003e\u003e iterator \u003d \n+            running.entrySet().iterator();\n+\n+          //avoid calculating current time everytime in loop\n+          long currentTime \u003d clock.getTime();\n+\n+          while (iterator.hasNext()) {\n+            Map.Entry\u003cO, Long\u003e entry \u003d iterator.next();\n+            if (currentTime \u003e entry.getValue() + expireInterval) {\n+              iterator.remove();\n+              expire(entry.getKey());\n+              LOG.info(\"Expired:\" + entry.getKey().toString() + \n+                      \" Timed out after \" + expireInterval/1000 + \" secs\");\n+            }\n+          }\n+        }\n         try {\n-          int numNewMaps \u003d getMapCompletionEvents();\n-          failures \u003d 0;\n-          if (numNewMaps \u003e 0) {\n-            LOG.info(reduce + \": \" + \"Got \" + numNewMaps + \" new map-outputs\");\n-          }\n-          LOG.debug(\"GetMapEventsThread about to sleep for \" + SLEEP_TIME);\n-          Thread.sleep(SLEEP_TIME);\n-        } catch (IOException ie) {\n-          LOG.info(\"Exception in getting events\", ie);\n-          // check to see whether to abort\n-          if (++failures \u003e\u003d MAX_RETRIES) {\n-            throw new IOException(\"too many failures downloading events\", ie);\n-          }\n-          // sleep for a bit\n-          Thread.sleep(RETRY_PERIOD);\n+          Thread.sleep(monitorInterval);\n+        } catch (InterruptedException e) {\n+          LOG.info(getName() + \" thread interrupted\");\n+          break;\n         }\n       }\n-    } catch (InterruptedException e) {\n-      return;\n-    } catch (Throwable t) {\n-      exceptionReporter.reportException(t);\n-      return;\n-    }\n-  }\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n",
          "actualSource": "    public void run() {\n      while (!stopped \u0026\u0026 !Thread.currentThread().isInterrupted()) {\n        synchronized (AbstractLivelinessMonitor.this) {\n          Iterator\u003cMap.Entry\u003cO, Long\u003e\u003e iterator \u003d \n            running.entrySet().iterator();\n\n          //avoid calculating current time everytime in loop\n          long currentTime \u003d clock.getTime();\n\n          while (iterator.hasNext()) {\n            Map.Entry\u003cO, Long\u003e entry \u003d iterator.next();\n            if (currentTime \u003e entry.getValue() + expireInterval) {\n              iterator.remove();\n              expire(entry.getKey());\n              LOG.info(\"Expired:\" + entry.getKey().toString() + \n                      \" Timed out after \" + expireInterval/1000 + \" secs\");\n            }\n          }\n        }\n        try {\n          Thread.sleep(monitorInterval);\n        } catch (InterruptedException e) {\n          LOG.info(getName() + \" thread interrupted\");\n          break;\n        }\n      }\n    }",
          "path": "hadoop-mapreduce/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/AbstractLivelinessMonitor.java",
          "extendedDetails": {}
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,31 @@\n+  public void run() {\n+    int failures \u003d 0;\n+    LOG.info(reduce + \" Thread started: \" + getName());\n+    \n+    try {\n+      while (true) {\n+        try {\n+          int numNewMaps \u003d getMapCompletionEvents();\n+          failures \u003d 0;\n+          if (numNewMaps \u003e 0) {\n+            LOG.info(reduce + \": \" + \"Got \" + numNewMaps + \" new map-outputs\");\n+          }\n+          LOG.debug(\"GetMapEventsThread about to sleep for \" + SLEEP_TIME);\n+          Thread.sleep(SLEEP_TIME);\n+        } catch (IOException ie) {\n+          LOG.info(\"Exception in getting events\", ie);\n+          // check to see whether to abort\n+          if (++failures \u003e\u003d MAX_RETRIES) {\n+            throw new IOException(\"too many failures downloading events\", ie);\n+          }\n+          // sleep for a bit\n+          Thread.sleep(RETRY_PERIOD);\n+        }\n+      }\n+    } catch (InterruptedException e) {\n+      return;\n+    } catch (Throwable t) {\n+      exceptionReporter.reportException(t);\n+      return;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    int failures \u003d 0;\n    LOG.info(reduce + \" Thread started: \" + getName());\n    \n    try {\n      while (true) {\n        try {\n          int numNewMaps \u003d getMapCompletionEvents();\n          failures \u003d 0;\n          if (numNewMaps \u003e 0) {\n            LOG.info(reduce + \": \" + \"Got \" + numNewMaps + \" new map-outputs\");\n          }\n          LOG.debug(\"GetMapEventsThread about to sleep for \" + SLEEP_TIME);\n          Thread.sleep(SLEEP_TIME);\n        } catch (IOException ie) {\n          LOG.info(\"Exception in getting events\", ie);\n          // check to see whether to abort\n          if (++failures \u003e\u003d MAX_RETRIES) {\n            throw new IOException(\"too many failures downloading events\", ie);\n          }\n          // sleep for a bit\n          Thread.sleep(RETRY_PERIOD);\n        }\n      }\n    } catch (InterruptedException e) {\n      return;\n    } catch (Throwable t) {\n      exceptionReporter.reportException(t);\n      return;\n    }\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java"
    }
  }
}