{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirConcatOp.java",
  "functionName": "unprotectedConcat",
  "functionId": "unprotectedConcat___fsd-FSDirectory__targetIIP-INodesInPath__srcList-INodeFile[]__timestamp-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
  "functionStartLine": 234,
  "functionEndLine": 268,
  "numCommitsSeen": 243,
  "timeTaken": 15510,
  "changeHistory": [
    "8faf0b50d435039f69ea35f592856ca04d378809",
    "745d04be59accf80feda0ad38efcc74ba362f2ca",
    "7e091de1366f4b57b5433bc19d738199dc05313d",
    "47f4c54106ebb234a7d3dc71320aa584ecba161a",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b",
    "5dae97a584d30cef3e34141edfaca49c4ec57913",
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
    "5776a41da08af653206bb94d7c76c9c4dcce059a",
    "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
    "431857d09dac2a9554f7ee8a6a92ae05844d0066",
    "0689363343a281a6f7f6f395227668bddc2663eb",
    "70cff9e2f0c8f78c1dc54a064182971bb2106795",
    "44a6560b5da3f79d2299579a36e7a2a60a91f823",
    "03ba436d42418226a5edb754f5119fe69039c8b8",
    "3a3e0f573129c8308332d4b301a9319ee579d85a",
    "38bd7061c1a8408b74ed619b25f948cd3cd85d7a",
    "3b3ea5c4220e674064c7603a449f63904c10bac1",
    "b1333e5b561d01a010e2e1311e8501879f377bdc",
    "c7cf85ccb4ff2f58839e113f1baf903a468b606d",
    "e2a618e1cc3fb99115547af6540932860dc6766e",
    "2372e394dd99d69d396327d5a5e172953a8b8c6a",
    "b9f965de120b5278ac84a7e98aecb32aafde4c16",
    "cbbaa93ae09bf5cf643263faf78f99315c4f3a8d",
    "9821af9ce8a56a2c583f1ed938902c20e897048f",
    "34413c2000d9262faa37fde88a72939587edc776",
    "7ee5ce3176a74d217551b5981f809a56c719424b",
    "d174f574bafcfefc635c64a47f258b1ce5d5c84e",
    "ba2ee1d7fb91462c861169224d250d2d90bec3a6",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "8faf0b50d435039f69ea35f592856ca04d378809": "Ybodychange",
    "745d04be59accf80feda0ad38efcc74ba362f2ca": "Ybodychange",
    "7e091de1366f4b57b5433bc19d738199dc05313d": "Ybodychange",
    "47f4c54106ebb234a7d3dc71320aa584ecba161a": "Ybodychange",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": "Ybodychange",
    "5dae97a584d30cef3e34141edfaca49c4ec57913": "Ybodychange",
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0": "Ymultichange(Yparameterchange,Ybodychange)",
    "5776a41da08af653206bb94d7c76c9c4dcce059a": "Ybodychange",
    "8caf537afabc70b0c74e0a29aea0cc2935ecb162": "Ymultichange(Ymovefromfile,Ymodifierchange,Yexceptionschange,Ybodychange,Yparameterchange)",
    "431857d09dac2a9554f7ee8a6a92ae05844d0066": "Ybodychange",
    "0689363343a281a6f7f6f395227668bddc2663eb": "Ybodychange",
    "70cff9e2f0c8f78c1dc54a064182971bb2106795": "Ybodychange",
    "44a6560b5da3f79d2299579a36e7a2a60a91f823": "Ybodychange",
    "03ba436d42418226a5edb754f5119fe69039c8b8": "Ybodychange",
    "3a3e0f573129c8308332d4b301a9319ee579d85a": "Ybodychange",
    "38bd7061c1a8408b74ed619b25f948cd3cd85d7a": "Ymultichange(Yexceptionschange,Ybodychange)",
    "3b3ea5c4220e674064c7603a449f63904c10bac1": "Yexceptionschange",
    "b1333e5b561d01a010e2e1311e8501879f377bdc": "Ymultichange(Ymodifierchange,Ybodychange)",
    "c7cf85ccb4ff2f58839e113f1baf903a468b606d": "Yexceptionschange",
    "e2a618e1cc3fb99115547af6540932860dc6766e": "Ybodychange",
    "2372e394dd99d69d396327d5a5e172953a8b8c6a": "Ymultichange(Yexceptionschange,Ybodychange)",
    "b9f965de120b5278ac84a7e98aecb32aafde4c16": "Ybodychange",
    "cbbaa93ae09bf5cf643263faf78f99315c4f3a8d": "Ybodychange",
    "9821af9ce8a56a2c583f1ed938902c20e897048f": "Ybodychange",
    "34413c2000d9262faa37fde88a72939587edc776": "Ybodychange",
    "7ee5ce3176a74d217551b5981f809a56c719424b": "Ybodychange",
    "d174f574bafcfefc635c64a47f258b1ce5d5c84e": "Ybodychange",
    "ba2ee1d7fb91462c861169224d250d2d90bec3a6": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "8faf0b50d435039f69ea35f592856ca04d378809": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13120. Snapshot diff could be corrupted after concat. Contributed by Xiaoyu Yao.\n",
      "commitDate": "08/02/18 8:59 AM",
      "commitName": "8faf0b50d435039f69ea35f592856ca04d378809",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "16/05/17 9:28 AM",
      "commitNameOld": "9b90e52f1ec22c18cd535af2a569defcef65b093",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 268.02,
      "commitsBetweenForRepo": 1854,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,35 @@\n   static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n       INodeFile[] srcList, long timestamp) throws IOException {\n     assert fsd.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n           + targetIIP.getPath());\n     }\n \n     final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n     QuotaCounts deltas \u003d computeQuotaDeltas(fsd, trgInode, srcList);\n     verifyQuota(fsd, targetIIP, deltas);\n \n     // the target file can be included in a snapshot\n     trgInode.recordModification(targetIIP.getLatestSnapshotId());\n     INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n     trgInode.concatBlocks(srcList, fsd.getBlockManager());\n \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for (INodeFile nodeToRemove : srcList) {\n       if(nodeToRemove !\u003d null) {\n         nodeToRemove.clearBlocks();\n-        nodeToRemove.getParent().removeChild(nodeToRemove);\n+        // Ensure the nodeToRemove is cleared from snapshot diff list\n+        nodeToRemove.getParent().removeChild(nodeToRemove,\n+            targetIIP.getLatestSnapshotId());\n         fsd.getINodeMap().remove(nodeToRemove);\n         count++;\n       }\n     }\n \n     trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n     trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n     // update quota on the parent directory with deltas\n     FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n      INodeFile[] srcList, long timestamp) throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n          + targetIIP.getPath());\n    }\n\n    final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n    QuotaCounts deltas \u003d computeQuotaDeltas(fsd, trgInode, srcList);\n    verifyQuota(fsd, targetIIP, deltas);\n\n    // the target file can be included in a snapshot\n    trgInode.recordModification(targetIIP.getLatestSnapshotId());\n    INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n    trgInode.concatBlocks(srcList, fsd.getBlockManager());\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for (INodeFile nodeToRemove : srcList) {\n      if(nodeToRemove !\u003d null) {\n        nodeToRemove.clearBlocks();\n        // Ensure the nodeToRemove is cleared from snapshot diff list\n        nodeToRemove.getParent().removeChild(nodeToRemove,\n            targetIIP.getLatestSnapshotId());\n        fsd.getINodeMap().remove(nodeToRemove);\n        count++;\n      }\n    }\n\n    trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    // update quota on the parent directory with deltas\n    FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "745d04be59accf80feda0ad38efcc74ba362f2ca": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8823. Move replication factor into individual blocks. Contributed by Haohui Mai.\n",
      "commitDate": "22/08/15 12:09 AM",
      "commitName": "745d04be59accf80feda0ad38efcc74ba362f2ca",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "14/08/15 2:42 PM",
      "commitNameOld": "dc7a061668a3f4d86fe1b07a40d46774b5386938",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 7.39,
      "commitsBetweenForRepo": 40,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n       INodeFile[] srcList, long timestamp) throws IOException {\n     assert fsd.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n           + targetIIP.getPath());\n     }\n \n     final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n     QuotaCounts deltas \u003d computeQuotaDeltas(fsd, trgInode, srcList);\n     verifyQuota(fsd, targetIIP, deltas);\n \n     // the target file can be included in a snapshot\n     trgInode.recordModification(targetIIP.getLatestSnapshotId());\n     INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n-    trgInode.concatBlocks(srcList);\n+    trgInode.concatBlocks(srcList, fsd.getBlockManager());\n \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for (INodeFile nodeToRemove : srcList) {\n       if(nodeToRemove !\u003d null) {\n         nodeToRemove.clearBlocks();\n         nodeToRemove.getParent().removeChild(nodeToRemove);\n         fsd.getINodeMap().remove(nodeToRemove);\n         count++;\n       }\n     }\n \n     trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n     trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n     // update quota on the parent directory with deltas\n     FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n      INodeFile[] srcList, long timestamp) throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n          + targetIIP.getPath());\n    }\n\n    final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n    QuotaCounts deltas \u003d computeQuotaDeltas(fsd, trgInode, srcList);\n    verifyQuota(fsd, targetIIP, deltas);\n\n    // the target file can be included in a snapshot\n    trgInode.recordModification(targetIIP.getLatestSnapshotId());\n    INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n    trgInode.concatBlocks(srcList, fsd.getBlockManager());\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for (INodeFile nodeToRemove : srcList) {\n      if(nodeToRemove !\u003d null) {\n        nodeToRemove.clearBlocks();\n        nodeToRemove.getParent().removeChild(nodeToRemove);\n        fsd.getINodeMap().remove(nodeToRemove);\n        count++;\n      }\n    }\n\n    trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    // update quota on the parent directory with deltas\n    FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "7e091de1366f4b57b5433bc19d738199dc05313d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8058. Erasure coding: use BlockInfo[] for both striped and contiguous blocks in INodeFile. Contributed by Zhe Zhang and Yi Liu.\n",
      "commitDate": "15/07/15 9:49 AM",
      "commitName": "7e091de1366f4b57b5433bc19d738199dc05313d",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "26/05/15 11:07 AM",
      "commitNameOld": "9f2f583f401189c3f4a2687795a9e3e0b288322b",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 49.95,
      "commitsBetweenForRepo": 169,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n       INodeFile[] srcList, long timestamp) throws IOException {\n     assert fsd.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n           + targetIIP.getPath());\n     }\n \n     final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n     QuotaCounts deltas \u003d computeQuotaDeltas(fsd, trgInode, srcList);\n     verifyQuota(fsd, targetIIP, deltas);\n \n     // the target file can be included in a snapshot\n     trgInode.recordModification(targetIIP.getLatestSnapshotId());\n     INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n     trgInode.concatBlocks(srcList);\n \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for (INodeFile nodeToRemove : srcList) {\n       if(nodeToRemove !\u003d null) {\n-        nodeToRemove.setContiguousBlocks(null);\n+        nodeToRemove.clearBlocks();\n         nodeToRemove.getParent().removeChild(nodeToRemove);\n         fsd.getINodeMap().remove(nodeToRemove);\n         count++;\n       }\n     }\n \n     trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n     trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n     // update quota on the parent directory with deltas\n     FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n      INodeFile[] srcList, long timestamp) throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n          + targetIIP.getPath());\n    }\n\n    final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n    QuotaCounts deltas \u003d computeQuotaDeltas(fsd, trgInode, srcList);\n    verifyQuota(fsd, targetIIP, deltas);\n\n    // the target file can be included in a snapshot\n    trgInode.recordModification(targetIIP.getLatestSnapshotId());\n    INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n    trgInode.concatBlocks(srcList);\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for (INodeFile nodeToRemove : srcList) {\n      if(nodeToRemove !\u003d null) {\n        nodeToRemove.clearBlocks();\n        nodeToRemove.getParent().removeChild(nodeToRemove);\n        fsd.getINodeMap().remove(nodeToRemove);\n        count++;\n      }\n    }\n\n    trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    // update quota on the parent directory with deltas\n    FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "47f4c54106ebb234a7d3dc71320aa584ecba161a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8751. Remove setBlocks API from INodeFile and misc code cleanup. Contributed by Zhe Zhang\n",
      "commitDate": "10/07/15 2:15 PM",
      "commitName": "47f4c54106ebb234a7d3dc71320aa584ecba161a",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/05/15 6:29 AM",
      "commitNameOld": "6d5da9484185ca9f585195d6da069b9cd5be4044",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 59.32,
      "commitsBetweenForRepo": 431,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n       INodeFile[] srcList, long timestamp) throws IOException {\n     assert fsd.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n           + targetIIP.getPath());\n     }\n \n     final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n     QuotaCounts deltas \u003d computeQuotaDeltas(fsd, trgInode, srcList);\n     verifyQuota(fsd, targetIIP, deltas);\n \n     // the target file can be included in a snapshot\n     trgInode.recordModification(targetIIP.getLatestSnapshotId());\n     INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n     trgInode.concatBlocks(srcList);\n \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for (INodeFile nodeToRemove : srcList) {\n       if(nodeToRemove !\u003d null) {\n-        nodeToRemove.setBlocks(null);\n+        nodeToRemove.clearBlocks();\n         nodeToRemove.getParent().removeChild(nodeToRemove);\n         fsd.getINodeMap().remove(nodeToRemove);\n         count++;\n       }\n     }\n \n     trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n     trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n     // update quota on the parent directory with deltas\n     FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n      INodeFile[] srcList, long timestamp) throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n          + targetIIP.getPath());\n    }\n\n    final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n    QuotaCounts deltas \u003d computeQuotaDeltas(fsd, trgInode, srcList);\n    verifyQuota(fsd, targetIIP, deltas);\n\n    // the target file can be included in a snapshot\n    trgInode.recordModification(targetIIP.getLatestSnapshotId());\n    INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n    trgInode.concatBlocks(srcList);\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for (INodeFile nodeToRemove : srcList) {\n      if(nodeToRemove !\u003d null) {\n        nodeToRemove.clearBlocks();\n        nodeToRemove.getParent().removeChild(nodeToRemove);\n        fsd.getINodeMap().remove(nodeToRemove);\n        count++;\n      }\n    }\n\n    trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    // update quota on the parent directory with deltas\n    FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7749. Erasure Coding: Add striped block support in INodeFile. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:07 AM",
      "commitName": "9f2f583f401189c3f4a2687795a9e3e0b288322b",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/05/15 6:29 AM",
      "commitNameOld": "6d5da9484185ca9f585195d6da069b9cd5be4044",
      "commitAuthorOld": "yliu",
      "daysBetweenCommits": 14.19,
      "commitsBetweenForRepo": 118,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,33 @@\n   static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n       INodeFile[] srcList, long timestamp) throws IOException {\n     assert fsd.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n           + targetIIP.getPath());\n     }\n \n     final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n     QuotaCounts deltas \u003d computeQuotaDeltas(fsd, trgInode, srcList);\n     verifyQuota(fsd, targetIIP, deltas);\n \n     // the target file can be included in a snapshot\n     trgInode.recordModification(targetIIP.getLatestSnapshotId());\n     INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n     trgInode.concatBlocks(srcList);\n \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for (INodeFile nodeToRemove : srcList) {\n       if(nodeToRemove !\u003d null) {\n-        nodeToRemove.setBlocks(null);\n+        nodeToRemove.setContiguousBlocks(null);\n         nodeToRemove.getParent().removeChild(nodeToRemove);\n         fsd.getINodeMap().remove(nodeToRemove);\n         count++;\n       }\n     }\n \n     trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n     trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n     // update quota on the parent directory with deltas\n     FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n      INodeFile[] srcList, long timestamp) throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n          + targetIIP.getPath());\n    }\n\n    final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n    QuotaCounts deltas \u003d computeQuotaDeltas(fsd, trgInode, srcList);\n    verifyQuota(fsd, targetIIP, deltas);\n\n    // the target file can be included in a snapshot\n    trgInode.recordModification(targetIIP.getLatestSnapshotId());\n    INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n    trgInode.concatBlocks(srcList);\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for (INodeFile nodeToRemove : srcList) {\n      if(nodeToRemove !\u003d null) {\n        nodeToRemove.setContiguousBlocks(null);\n        nodeToRemove.getParent().removeChild(nodeToRemove);\n        fsd.getINodeMap().remove(nodeToRemove);\n        count++;\n      }\n    }\n\n    trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    // update quota on the parent directory with deltas\n    FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "5dae97a584d30cef3e34141edfaca49c4ec57913": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7723. Quota By Storage Type namenode implemenation. (Contributed by Xiaoyu Yao)\n",
      "commitDate": "11/02/15 10:41 AM",
      "commitName": "5dae97a584d30cef3e34141edfaca49c4ec57913",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "27/01/15 12:58 PM",
      "commitNameOld": "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 14.91,
      "commitsBetweenForRepo": 149,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,33 @@\n   static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n       INodeFile[] srcList, long timestamp) throws IOException {\n     assert fsd.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n           + targetIIP.getPath());\n     }\n \n     final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n-    long delta \u003d computeQuotaDelta(trgInode, srcList);\n-    verifyQuota(fsd, targetIIP, delta);\n+    QuotaCounts deltas \u003d computeQuotaDeltas(fsd, trgInode, srcList);\n+    verifyQuota(fsd, targetIIP, deltas);\n \n     // the target file can be included in a snapshot\n     trgInode.recordModification(targetIIP.getLatestSnapshotId());\n     INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n     trgInode.concatBlocks(srcList);\n \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for (INodeFile nodeToRemove : srcList) {\n       if(nodeToRemove !\u003d null) {\n         nodeToRemove.setBlocks(null);\n         nodeToRemove.getParent().removeChild(nodeToRemove);\n         fsd.getINodeMap().remove(nodeToRemove);\n         count++;\n       }\n     }\n \n     trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n     trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n-    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n-    FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1,\n-        -count, delta);\n+    // update quota on the parent directory with deltas\n+    FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n      INodeFile[] srcList, long timestamp) throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n          + targetIIP.getPath());\n    }\n\n    final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n    QuotaCounts deltas \u003d computeQuotaDeltas(fsd, trgInode, srcList);\n    verifyQuota(fsd, targetIIP, deltas);\n\n    // the target file can be included in a snapshot\n    trgInode.recordModification(targetIIP.getLatestSnapshotId());\n    INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n    trgInode.concatBlocks(srcList);\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for (INodeFile nodeToRemove : srcList) {\n      if(nodeToRemove !\u003d null) {\n        nodeToRemove.setBlocks(null);\n        nodeToRemove.getParent().removeChild(nodeToRemove);\n        fsd.getINodeMap().remove(nodeToRemove);\n        count++;\n      }\n    }\n\n    trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    // update quota on the parent directory with deltas\n    FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-3689. Add support for variable length block. Contributed by Jing Zhao.\n",
      "commitDate": "27/01/15 12:58 PM",
      "commitName": "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-3689. Add support for variable length block. Contributed by Jing Zhao.\n",
          "commitDate": "27/01/15 12:58 PM",
          "commitName": "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "18/12/14 11:25 AM",
          "commitNameOld": "65f2a4ee600dfffa5203450261da3c1989de25a9",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 40.06,
          "commitsBetweenForRepo": 212,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,53 +1,34 @@\n-  static void unprotectedConcat(\n-    FSDirectory fsd, String target, String[] srcs, long timestamp)\n-    throws IOException {\n+  static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n+      INodeFile[] srcList, long timestamp) throws IOException {\n     assert fsd.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n+      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n+          + targetIIP.getPath());\n     }\n-    // do the move\n \n-    final INodesInPath trgIIP \u003d fsd.getINodesInPath4Write(target, true);\n-    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n-    INodeDirectory trgParent \u003d trgIIP.getINode(-2).asDirectory();\n-    final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n+    final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n+    long delta \u003d computeQuotaDelta(trgInode, srcList);\n+    verifyQuota(fsd, targetIIP, delta);\n \n-    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n-    for(int i \u003d 0; i \u003c srcs.length; i++) {\n-      final INodesInPath iip \u003d fsd.getINodesInPath4Write(srcs[i]);\n-      final int latest \u003d iip.getLatestSnapshotId();\n-      final INode inode \u003d iip.getLastINode();\n-\n-      // check if the file in the latest snapshot\n-      if (inode.isInLatestSnapshot(latest)) {\n-        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n-            + \" is in snapshot \" + latest);\n-      }\n-\n-      // check if the file has other references.\n-      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n-          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n-        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n-            + \" is referred by some other reference in some snapshot.\");\n-      }\n-\n-      allSrcInodes[i] \u003d inode.asFile();\n-    }\n-    trgInode.concatBlocks(allSrcInodes);\n+    // the target file can be included in a snapshot\n+    trgInode.recordModification(targetIIP.getLatestSnapshotId());\n+    INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n+    trgInode.concatBlocks(srcList);\n \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n-    for(INodeFile nodeToRemove: allSrcInodes) {\n-      if(nodeToRemove \u003d\u003d null) continue;\n-\n-      nodeToRemove.setBlocks(null);\n-      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n-      fsd.getINodeMap().remove(nodeToRemove);\n-      count++;\n+    for (INodeFile nodeToRemove : srcList) {\n+      if(nodeToRemove !\u003d null) {\n+        nodeToRemove.setBlocks(null);\n+        nodeToRemove.getParent().removeChild(nodeToRemove);\n+        fsd.getINodeMap().remove(nodeToRemove);\n+        count++;\n+      }\n     }\n \n-    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n-    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n+    trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n+    trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n-    FSDirectory.unprotectedUpdateCount(trgIIP, trgIIP.length() - 1, -count, 0);\n+    FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1,\n+        -count, delta);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n      INodeFile[] srcList, long timestamp) throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n          + targetIIP.getPath());\n    }\n\n    final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n    long delta \u003d computeQuotaDelta(trgInode, srcList);\n    verifyQuota(fsd, targetIIP, delta);\n\n    // the target file can be included in a snapshot\n    trgInode.recordModification(targetIIP.getLatestSnapshotId());\n    INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n    trgInode.concatBlocks(srcList);\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for (INodeFile nodeToRemove : srcList) {\n      if(nodeToRemove !\u003d null) {\n        nodeToRemove.setBlocks(null);\n        nodeToRemove.getParent().removeChild(nodeToRemove);\n        fsd.getINodeMap().remove(nodeToRemove);\n        count++;\n      }\n    }\n\n    trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1,\n        -count, delta);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, target-String, srcs-String[], timestamp-long]",
            "newValue": "[fsd-FSDirectory, targetIIP-INodesInPath, srcList-INodeFile[], timestamp-long]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3689. Add support for variable length block. Contributed by Jing Zhao.\n",
          "commitDate": "27/01/15 12:58 PM",
          "commitName": "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "18/12/14 11:25 AM",
          "commitNameOld": "65f2a4ee600dfffa5203450261da3c1989de25a9",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 40.06,
          "commitsBetweenForRepo": 212,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,53 +1,34 @@\n-  static void unprotectedConcat(\n-    FSDirectory fsd, String target, String[] srcs, long timestamp)\n-    throws IOException {\n+  static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n+      INodeFile[] srcList, long timestamp) throws IOException {\n     assert fsd.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n+      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n+          + targetIIP.getPath());\n     }\n-    // do the move\n \n-    final INodesInPath trgIIP \u003d fsd.getINodesInPath4Write(target, true);\n-    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n-    INodeDirectory trgParent \u003d trgIIP.getINode(-2).asDirectory();\n-    final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n+    final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n+    long delta \u003d computeQuotaDelta(trgInode, srcList);\n+    verifyQuota(fsd, targetIIP, delta);\n \n-    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n-    for(int i \u003d 0; i \u003c srcs.length; i++) {\n-      final INodesInPath iip \u003d fsd.getINodesInPath4Write(srcs[i]);\n-      final int latest \u003d iip.getLatestSnapshotId();\n-      final INode inode \u003d iip.getLastINode();\n-\n-      // check if the file in the latest snapshot\n-      if (inode.isInLatestSnapshot(latest)) {\n-        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n-            + \" is in snapshot \" + latest);\n-      }\n-\n-      // check if the file has other references.\n-      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n-          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n-        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n-            + \" is referred by some other reference in some snapshot.\");\n-      }\n-\n-      allSrcInodes[i] \u003d inode.asFile();\n-    }\n-    trgInode.concatBlocks(allSrcInodes);\n+    // the target file can be included in a snapshot\n+    trgInode.recordModification(targetIIP.getLatestSnapshotId());\n+    INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n+    trgInode.concatBlocks(srcList);\n \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n-    for(INodeFile nodeToRemove: allSrcInodes) {\n-      if(nodeToRemove \u003d\u003d null) continue;\n-\n-      nodeToRemove.setBlocks(null);\n-      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n-      fsd.getINodeMap().remove(nodeToRemove);\n-      count++;\n+    for (INodeFile nodeToRemove : srcList) {\n+      if(nodeToRemove !\u003d null) {\n+        nodeToRemove.setBlocks(null);\n+        nodeToRemove.getParent().removeChild(nodeToRemove);\n+        fsd.getINodeMap().remove(nodeToRemove);\n+        count++;\n+      }\n     }\n \n-    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n-    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n+    trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n+    trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n-    FSDirectory.unprotectedUpdateCount(trgIIP, trgIIP.length() - 1, -count, 0);\n+    FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1,\n+        -count, delta);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,\n      INodeFile[] srcList, long timestamp) throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"\n          + targetIIP.getPath());\n    }\n\n    final INodeFile trgInode \u003d targetIIP.getLastINode().asFile();\n    long delta \u003d computeQuotaDelta(trgInode, srcList);\n    verifyQuota(fsd, targetIIP, delta);\n\n    // the target file can be included in a snapshot\n    trgInode.recordModification(targetIIP.getLatestSnapshotId());\n    INodeDirectory trgParent \u003d targetIIP.getINode(-2).asDirectory();\n    trgInode.concatBlocks(srcList);\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for (INodeFile nodeToRemove : srcList) {\n      if(nodeToRemove !\u003d null) {\n        nodeToRemove.setBlocks(null);\n        nodeToRemove.getParent().removeChild(nodeToRemove);\n        fsd.getINodeMap().remove(nodeToRemove);\n        count++;\n      }\n    }\n\n    trgInode.setModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    trgParent.updateModificationTime(timestamp, targetIIP.getLatestSnapshotId());\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    FSDirectory.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1,\n        -count, delta);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "5776a41da08af653206bb94d7c76c9c4dcce059a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7498. Simplify the logic in INodesInPath. Contributed by Jing Zhao.\n",
      "commitDate": "09/12/14 11:37 AM",
      "commitName": "5776a41da08af653206bb94d7c76c9c4dcce059a",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "05/12/14 2:17 PM",
      "commitNameOld": "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 3.89,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,53 @@\n   static void unprotectedConcat(\n     FSDirectory fsd, String target, String[] srcs, long timestamp)\n     throws IOException {\n     assert fsd.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n \n     final INodesInPath trgIIP \u003d fsd.getINodesInPath4Write(target, true);\n-    final INode[] trgINodes \u003d trgIIP.getINodes();\n     final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n-    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n+    INodeDirectory trgParent \u003d trgIIP.getINode(-2).asDirectory();\n     final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n \n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n       final INodesInPath iip \u003d fsd.getINodesInPath4Write(srcs[i]);\n       final int latest \u003d iip.getLatestSnapshotId();\n       final INode inode \u003d iip.getLastINode();\n \n       // check if the file in the latest snapshot\n       if (inode.isInLatestSnapshot(latest)) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is in snapshot \" + latest);\n       }\n \n       // check if the file has other references.\n       if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is referred by some other reference in some snapshot.\");\n       }\n \n       allSrcInodes[i] \u003d inode.asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       fsd.getINodeMap().remove(nodeToRemove);\n       count++;\n     }\n \n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n-    FSDirectory.unprotectedUpdateCount(trgIIP, trgINodes.length - 1, -count, 0);\n+    FSDirectory.unprotectedUpdateCount(trgIIP, trgIIP.length() - 1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void unprotectedConcat(\n    FSDirectory fsd, String target, String[] srcs, long timestamp)\n    throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n\n    final INodesInPath trgIIP \u003d fsd.getINodesInPath4Write(target, true);\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgIIP.getINode(-2).asDirectory();\n    final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n\n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(srcs[i]);\n      final int latest \u003d iip.getLatestSnapshotId();\n      final INode inode \u003d iip.getLastINode();\n\n      // check if the file in the latest snapshot\n      if (inode.isInLatestSnapshot(latest)) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is in snapshot \" + latest);\n      }\n\n      // check if the file has other references.\n      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is referred by some other reference in some snapshot.\");\n      }\n\n      allSrcInodes[i] \u003d inode.asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n\n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      fsd.getINodeMap().remove(nodeToRemove);\n      count++;\n    }\n\n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    FSDirectory.unprotectedUpdateCount(trgIIP, trgIIP.length() - 1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "8caf537afabc70b0c74e0a29aea0cc2935ecb162": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Yexceptionschange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
      "commitDate": "24/11/14 3:42 PM",
      "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 3:42 PM",
          "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 2:58 PM",
          "commitNameOld": "e37a4ff0c1712a1cb80e0412ec53a5d10b8d30f9",
          "commitAuthorOld": "Zhijie Shen",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,54 +1,54 @@\n-  void unprotectedConcat(String target, String [] srcs, long timestamp) \n-      throws UnresolvedLinkException, QuotaExceededException,\n-      SnapshotAccessControlException, SnapshotException {\n-    assert hasWriteLock();\n+  static void unprotectedConcat(\n+    FSDirectory fsd, String target, String[] srcs, long timestamp)\n+    throws IOException {\n+    assert fsd.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n-    \n-    final INodesInPath trgIIP \u003d getINodesInPath4Write(target, true);\n+\n+    final INodesInPath trgIIP \u003d fsd.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgIIP.getINodes();\n     final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n     INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n     final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n-    \n+\n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n-      final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n+      final INodesInPath iip \u003d fsd.getINodesInPath4Write(srcs[i]);\n       final int latest \u003d iip.getLatestSnapshotId();\n       final INode inode \u003d iip.getLastINode();\n \n       // check if the file in the latest snapshot\n       if (inode.isInLatestSnapshot(latest)) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is in snapshot \" + latest);\n       }\n \n       // check if the file has other references.\n       if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is referred by some other reference in some snapshot.\");\n       }\n \n       allSrcInodes[i] \u003d inode.asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n-    \n+\n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n-      \n+\n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n-      inodeMap.remove(nodeToRemove);\n+      fsd.getINodeMap().remove(nodeToRemove);\n       count++;\n     }\n-    \n+\n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n-    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n+    FSDirectory.unprotectedUpdateCount(trgIIP, trgINodes.length - 1, -count, 0);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static void unprotectedConcat(\n    FSDirectory fsd, String target, String[] srcs, long timestamp)\n    throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n\n    final INodesInPath trgIIP \u003d fsd.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n\n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(srcs[i]);\n      final int latest \u003d iip.getLatestSnapshotId();\n      final INode inode \u003d iip.getLastINode();\n\n      // check if the file in the latest snapshot\n      if (inode.isInLatestSnapshot(latest)) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is in snapshot \" + latest);\n      }\n\n      // check if the file has other references.\n      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is referred by some other reference in some snapshot.\");\n      }\n\n      allSrcInodes[i] \u003d inode.asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n\n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      fsd.getINodeMap().remove(nodeToRemove);\n      count++;\n    }\n\n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    FSDirectory.unprotectedUpdateCount(trgIIP, trgINodes.length - 1, -count, 0);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
            "oldMethodName": "unprotectedConcat",
            "newMethodName": "unprotectedConcat"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 3:42 PM",
          "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 2:58 PM",
          "commitNameOld": "e37a4ff0c1712a1cb80e0412ec53a5d10b8d30f9",
          "commitAuthorOld": "Zhijie Shen",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,54 +1,54 @@\n-  void unprotectedConcat(String target, String [] srcs, long timestamp) \n-      throws UnresolvedLinkException, QuotaExceededException,\n-      SnapshotAccessControlException, SnapshotException {\n-    assert hasWriteLock();\n+  static void unprotectedConcat(\n+    FSDirectory fsd, String target, String[] srcs, long timestamp)\n+    throws IOException {\n+    assert fsd.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n-    \n-    final INodesInPath trgIIP \u003d getINodesInPath4Write(target, true);\n+\n+    final INodesInPath trgIIP \u003d fsd.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgIIP.getINodes();\n     final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n     INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n     final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n-    \n+\n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n-      final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n+      final INodesInPath iip \u003d fsd.getINodesInPath4Write(srcs[i]);\n       final int latest \u003d iip.getLatestSnapshotId();\n       final INode inode \u003d iip.getLastINode();\n \n       // check if the file in the latest snapshot\n       if (inode.isInLatestSnapshot(latest)) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is in snapshot \" + latest);\n       }\n \n       // check if the file has other references.\n       if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is referred by some other reference in some snapshot.\");\n       }\n \n       allSrcInodes[i] \u003d inode.asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n-    \n+\n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n-      \n+\n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n-      inodeMap.remove(nodeToRemove);\n+      fsd.getINodeMap().remove(nodeToRemove);\n       count++;\n     }\n-    \n+\n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n-    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n+    FSDirectory.unprotectedUpdateCount(trgIIP, trgINodes.length - 1, -count, 0);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static void unprotectedConcat(\n    FSDirectory fsd, String target, String[] srcs, long timestamp)\n    throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n\n    final INodesInPath trgIIP \u003d fsd.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n\n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(srcs[i]);\n      final int latest \u003d iip.getLatestSnapshotId();\n      final INode inode \u003d iip.getLastINode();\n\n      // check if the file in the latest snapshot\n      if (inode.isInLatestSnapshot(latest)) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is in snapshot \" + latest);\n      }\n\n      // check if the file has other references.\n      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is referred by some other reference in some snapshot.\");\n      }\n\n      allSrcInodes[i] \u003d inode.asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n\n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      fsd.getINodeMap().remove(nodeToRemove);\n      count++;\n    }\n\n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    FSDirectory.unprotectedUpdateCount(trgIIP, trgINodes.length - 1, -count, 0);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 3:42 PM",
          "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 2:58 PM",
          "commitNameOld": "e37a4ff0c1712a1cb80e0412ec53a5d10b8d30f9",
          "commitAuthorOld": "Zhijie Shen",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,54 +1,54 @@\n-  void unprotectedConcat(String target, String [] srcs, long timestamp) \n-      throws UnresolvedLinkException, QuotaExceededException,\n-      SnapshotAccessControlException, SnapshotException {\n-    assert hasWriteLock();\n+  static void unprotectedConcat(\n+    FSDirectory fsd, String target, String[] srcs, long timestamp)\n+    throws IOException {\n+    assert fsd.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n-    \n-    final INodesInPath trgIIP \u003d getINodesInPath4Write(target, true);\n+\n+    final INodesInPath trgIIP \u003d fsd.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgIIP.getINodes();\n     final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n     INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n     final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n-    \n+\n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n-      final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n+      final INodesInPath iip \u003d fsd.getINodesInPath4Write(srcs[i]);\n       final int latest \u003d iip.getLatestSnapshotId();\n       final INode inode \u003d iip.getLastINode();\n \n       // check if the file in the latest snapshot\n       if (inode.isInLatestSnapshot(latest)) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is in snapshot \" + latest);\n       }\n \n       // check if the file has other references.\n       if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is referred by some other reference in some snapshot.\");\n       }\n \n       allSrcInodes[i] \u003d inode.asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n-    \n+\n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n-      \n+\n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n-      inodeMap.remove(nodeToRemove);\n+      fsd.getINodeMap().remove(nodeToRemove);\n       count++;\n     }\n-    \n+\n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n-    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n+    FSDirectory.unprotectedUpdateCount(trgIIP, trgINodes.length - 1, -count, 0);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static void unprotectedConcat(\n    FSDirectory fsd, String target, String[] srcs, long timestamp)\n    throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n\n    final INodesInPath trgIIP \u003d fsd.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n\n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(srcs[i]);\n      final int latest \u003d iip.getLatestSnapshotId();\n      final INode inode \u003d iip.getLastINode();\n\n      // check if the file in the latest snapshot\n      if (inode.isInLatestSnapshot(latest)) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is in snapshot \" + latest);\n      }\n\n      // check if the file has other references.\n      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is referred by some other reference in some snapshot.\");\n      }\n\n      allSrcInodes[i] \u003d inode.asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n\n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      fsd.getINodeMap().remove(nodeToRemove);\n      count++;\n    }\n\n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    FSDirectory.unprotectedUpdateCount(trgIIP, trgINodes.length - 1, -count, 0);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {
            "oldValue": "[UnresolvedLinkException, QuotaExceededException, SnapshotAccessControlException, SnapshotException]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 3:42 PM",
          "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 2:58 PM",
          "commitNameOld": "e37a4ff0c1712a1cb80e0412ec53a5d10b8d30f9",
          "commitAuthorOld": "Zhijie Shen",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,54 +1,54 @@\n-  void unprotectedConcat(String target, String [] srcs, long timestamp) \n-      throws UnresolvedLinkException, QuotaExceededException,\n-      SnapshotAccessControlException, SnapshotException {\n-    assert hasWriteLock();\n+  static void unprotectedConcat(\n+    FSDirectory fsd, String target, String[] srcs, long timestamp)\n+    throws IOException {\n+    assert fsd.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n-    \n-    final INodesInPath trgIIP \u003d getINodesInPath4Write(target, true);\n+\n+    final INodesInPath trgIIP \u003d fsd.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgIIP.getINodes();\n     final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n     INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n     final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n-    \n+\n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n-      final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n+      final INodesInPath iip \u003d fsd.getINodesInPath4Write(srcs[i]);\n       final int latest \u003d iip.getLatestSnapshotId();\n       final INode inode \u003d iip.getLastINode();\n \n       // check if the file in the latest snapshot\n       if (inode.isInLatestSnapshot(latest)) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is in snapshot \" + latest);\n       }\n \n       // check if the file has other references.\n       if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is referred by some other reference in some snapshot.\");\n       }\n \n       allSrcInodes[i] \u003d inode.asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n-    \n+\n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n-      \n+\n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n-      inodeMap.remove(nodeToRemove);\n+      fsd.getINodeMap().remove(nodeToRemove);\n       count++;\n     }\n-    \n+\n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n-    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n+    FSDirectory.unprotectedUpdateCount(trgIIP, trgINodes.length - 1, -count, 0);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static void unprotectedConcat(\n    FSDirectory fsd, String target, String[] srcs, long timestamp)\n    throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n\n    final INodesInPath trgIIP \u003d fsd.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n\n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(srcs[i]);\n      final int latest \u003d iip.getLatestSnapshotId();\n      final INode inode \u003d iip.getLastINode();\n\n      // check if the file in the latest snapshot\n      if (inode.isInLatestSnapshot(latest)) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is in snapshot \" + latest);\n      }\n\n      // check if the file has other references.\n      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is referred by some other reference in some snapshot.\");\n      }\n\n      allSrcInodes[i] \u003d inode.asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n\n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      fsd.getINodeMap().remove(nodeToRemove);\n      count++;\n    }\n\n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    FSDirectory.unprotectedUpdateCount(trgIIP, trgINodes.length - 1, -count, 0);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 3:42 PM",
          "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 2:58 PM",
          "commitNameOld": "e37a4ff0c1712a1cb80e0412ec53a5d10b8d30f9",
          "commitAuthorOld": "Zhijie Shen",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,54 +1,54 @@\n-  void unprotectedConcat(String target, String [] srcs, long timestamp) \n-      throws UnresolvedLinkException, QuotaExceededException,\n-      SnapshotAccessControlException, SnapshotException {\n-    assert hasWriteLock();\n+  static void unprotectedConcat(\n+    FSDirectory fsd, String target, String[] srcs, long timestamp)\n+    throws IOException {\n+    assert fsd.hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n-    \n-    final INodesInPath trgIIP \u003d getINodesInPath4Write(target, true);\n+\n+    final INodesInPath trgIIP \u003d fsd.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgIIP.getINodes();\n     final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n     INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n     final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n-    \n+\n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n-      final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n+      final INodesInPath iip \u003d fsd.getINodesInPath4Write(srcs[i]);\n       final int latest \u003d iip.getLatestSnapshotId();\n       final INode inode \u003d iip.getLastINode();\n \n       // check if the file in the latest snapshot\n       if (inode.isInLatestSnapshot(latest)) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is in snapshot \" + latest);\n       }\n \n       // check if the file has other references.\n       if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is referred by some other reference in some snapshot.\");\n       }\n \n       allSrcInodes[i] \u003d inode.asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n-    \n+\n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n-      \n+\n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n-      inodeMap.remove(nodeToRemove);\n+      fsd.getINodeMap().remove(nodeToRemove);\n       count++;\n     }\n-    \n+\n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n-    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n+    FSDirectory.unprotectedUpdateCount(trgIIP, trgINodes.length - 1, -count, 0);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static void unprotectedConcat(\n    FSDirectory fsd, String target, String[] srcs, long timestamp)\n    throws IOException {\n    assert fsd.hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n\n    final INodesInPath trgIIP \u003d fsd.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n\n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      final INodesInPath iip \u003d fsd.getINodesInPath4Write(srcs[i]);\n      final int latest \u003d iip.getLatestSnapshotId();\n      final INode inode \u003d iip.getLastINode();\n\n      // check if the file in the latest snapshot\n      if (inode.isInLatestSnapshot(latest)) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is in snapshot \" + latest);\n      }\n\n      // check if the file has other references.\n      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is referred by some other reference in some snapshot.\");\n      }\n\n      allSrcInodes[i] \u003d inode.asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n\n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n\n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      fsd.getINodeMap().remove(nodeToRemove);\n      count++;\n    }\n\n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    FSDirectory.unprotectedUpdateCount(trgIIP, trgINodes.length - 1, -count, 0);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {
            "oldValue": "[target-String, srcs-String[], timestamp-long]",
            "newValue": "[fsd-FSDirectory, target-String, srcs-String[], timestamp-long]"
          }
        }
      ]
    },
    "431857d09dac2a9554f7ee8a6a92ae05844d0066": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6787. Remove duplicate code in FSDirectory#unprotectedConcat. Contributed by Yi Liu.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615622 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/08/14 8:44 AM",
      "commitName": "431857d09dac2a9554f7ee8a6a92ae05844d0066",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "09/07/14 8:49 PM",
      "commitNameOld": "8044a12ac02cc4495935a3afded8c1d4369c1445",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 25.5,
      "commitsBetweenForRepo": 183,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,54 @@\n   void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException, QuotaExceededException,\n       SnapshotAccessControlException, SnapshotException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     final INodesInPath trgIIP \u003d getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgIIP.getINodes();\n     final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n     INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n     final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n     \n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n       final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n       final int latest \u003d iip.getLatestSnapshotId();\n       final INode inode \u003d iip.getLastINode();\n \n       // check if the file in the latest snapshot\n       if (inode.isInLatestSnapshot(latest)) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is in snapshot \" + latest);\n       }\n \n       // check if the file has other references.\n       if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is referred by some other reference in some snapshot.\");\n       }\n \n       allSrcInodes[i] \u003d inode.asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       inodeMap.remove(nodeToRemove);\n       count++;\n     }\n     \n-    // update inodeMap\n-    removeFromInodeMap(Arrays.asList(allSrcInodes));\n-    \n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException, QuotaExceededException,\n      SnapshotAccessControlException, SnapshotException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgIIP \u003d getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n    \n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n      final int latest \u003d iip.getLatestSnapshotId();\n      final INode inode \u003d iip.getLastINode();\n\n      // check if the file in the latest snapshot\n      if (inode.isInLatestSnapshot(latest)) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is in snapshot \" + latest);\n      }\n\n      // check if the file has other references.\n      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is referred by some other reference in some snapshot.\");\n      }\n\n      allSrcInodes[i] \u003d inode.asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      inodeMap.remove(nodeToRemove);\n      count++;\n    }\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "0689363343a281a6f7f6f395227668bddc2663eb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6304. Consolidate the logic of path resolution in FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1591411 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/04/14 10:44 AM",
      "commitName": "0689363343a281a6f7f6f395227668bddc2663eb",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "24/04/14 7:05 PM",
      "commitNameOld": "10a037cccb00c9f791da394bf2dc05985fb80612",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 5.65,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException, QuotaExceededException,\n       SnapshotAccessControlException, SnapshotException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n-    final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n+    final INodesInPath trgIIP \u003d getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgIIP.getINodes();\n     final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n     INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n     final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n     \n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n       final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n       final int latest \u003d iip.getLatestSnapshotId();\n       final INode inode \u003d iip.getLastINode();\n \n       // check if the file in the latest snapshot\n       if (inode.isInLatestSnapshot(latest)) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is in snapshot \" + latest);\n       }\n \n       // check if the file has other references.\n       if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is referred by some other reference in some snapshot.\");\n       }\n \n       allSrcInodes[i] \u003d inode.asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       inodeMap.remove(nodeToRemove);\n       count++;\n     }\n     \n     // update inodeMap\n     removeFromInodeMap(Arrays.asList(allSrcInodes));\n     \n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException, QuotaExceededException,\n      SnapshotAccessControlException, SnapshotException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgIIP \u003d getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n    \n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n      final int latest \u003d iip.getLatestSnapshotId();\n      final INode inode \u003d iip.getLastINode();\n\n      // check if the file in the latest snapshot\n      if (inode.isInLatestSnapshot(latest)) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is in snapshot \" + latest);\n      }\n\n      // check if the file has other references.\n      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is referred by some other reference in some snapshot.\");\n      }\n\n      allSrcInodes[i] \u003d inode.asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      inodeMap.remove(nodeToRemove);\n      count++;\n    }\n    \n    // update inodeMap\n    removeFromInodeMap(Arrays.asList(allSrcInodes));\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "70cff9e2f0c8f78c1dc54a064182971bb2106795": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5715. Use Snapshot ID to indicate the corresponding Snapshot for a FileDiff/DirectoryDiff. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1556353 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/01/14 12:52 PM",
      "commitName": "70cff9e2f0c8f78c1dc54a064182971bb2106795",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "14/12/13 2:13 AM",
      "commitNameOld": "44a6560b5da3f79d2299579a36e7a2a60a91f823",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 24.44,
      "commitsBetweenForRepo": 98,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException, QuotaExceededException,\n       SnapshotAccessControlException, SnapshotException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgIIP.getINodes();\n     final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n     INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n-    final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n+    final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n     \n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n       final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n-      final Snapshot latest \u003d iip.getLatestSnapshot();\n+      final int latest \u003d iip.getLatestSnapshotId();\n       final INode inode \u003d iip.getLastINode();\n \n       // check if the file in the latest snapshot\n       if (inode.isInLatestSnapshot(latest)) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is in snapshot \" + latest);\n       }\n \n       // check if the file has other references.\n       if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is referred by some other reference in some snapshot.\");\n       }\n \n       allSrcInodes[i] \u003d inode.asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       inodeMap.remove(nodeToRemove);\n       count++;\n     }\n     \n     // update inodeMap\n     removeFromInodeMap(Arrays.asList(allSrcInodes));\n     \n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException, QuotaExceededException,\n      SnapshotAccessControlException, SnapshotException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final int trgLatestSnapshot \u003d trgIIP.getLatestSnapshotId();\n    \n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n      final int latest \u003d iip.getLatestSnapshotId();\n      final INode inode \u003d iip.getLastINode();\n\n      // check if the file in the latest snapshot\n      if (inode.isInLatestSnapshot(latest)) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is in snapshot \" + latest);\n      }\n\n      // check if the file has other references.\n      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is referred by some other reference in some snapshot.\");\n      }\n\n      allSrcInodes[i] \u003d inode.asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      inodeMap.remove(nodeToRemove);\n      count++;\n    }\n    \n    // update inodeMap\n    removeFromInodeMap(Arrays.asList(allSrcInodes));\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "44a6560b5da3f79d2299579a36e7a2a60a91f823": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5632. Flatten INodeDirectory hierarchy: Replace INodeDirectoryWithSnapshot with DirectoryWithSnapshotFeature.  Contributed by jing9 \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1550917 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/12/13 2:13 AM",
      "commitName": "44a6560b5da3f79d2299579a36e7a2a60a91f823",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "02/12/13 9:41 AM",
      "commitNameOld": "18159be495f96bde4bd4fa2cacb14aafb87e87bc",
      "commitAuthorOld": "",
      "daysBetweenCommits": 11.69,
      "commitsBetweenForRepo": 80,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException, QuotaExceededException,\n       SnapshotAccessControlException, SnapshotException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgIIP.getINodes();\n     final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n     INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n     final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n     \n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n       final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n       final Snapshot latest \u003d iip.getLatestSnapshot();\n       final INode inode \u003d iip.getLastINode();\n \n       // check if the file in the latest snapshot\n       if (inode.isInLatestSnapshot(latest)) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is in snapshot \" + latest);\n       }\n \n       // check if the file has other references.\n       if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is referred by some other reference in some snapshot.\");\n       }\n \n       allSrcInodes[i] \u003d inode.asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n-      trgParent.removeChild(nodeToRemove, trgLatestSnapshot, null);\n+      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       inodeMap.remove(nodeToRemove);\n       count++;\n     }\n     \n     // update inodeMap\n     removeFromInodeMap(Arrays.asList(allSrcInodes));\n     \n-    trgInode.setModificationTime(timestamp, trgLatestSnapshot, inodeMap);\n-    trgParent.updateModificationTime(timestamp, trgLatestSnapshot, inodeMap);\n+    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n+    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException, QuotaExceededException,\n      SnapshotAccessControlException, SnapshotException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n    \n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n      final Snapshot latest \u003d iip.getLatestSnapshot();\n      final INode inode \u003d iip.getLastINode();\n\n      // check if the file in the latest snapshot\n      if (inode.isInLatestSnapshot(latest)) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is in snapshot \" + latest);\n      }\n\n      // check if the file has other references.\n      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is referred by some other reference in some snapshot.\");\n      }\n\n      allSrcInodes[i] \u003d inode.asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      inodeMap.remove(nodeToRemove);\n      count++;\n    }\n    \n    // update inodeMap\n    removeFromInodeMap(Arrays.asList(allSrcInodes));\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "03ba436d42418226a5edb754f5119fe69039c8b8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4785. Concat operation does not remove concatenated files from InodeMap. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1478267 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/05/13 9:51 PM",
      "commitName": "03ba436d42418226a5edb754f5119fe69039c8b8",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "26/04/13 4:47 PM",
      "commitNameOld": "75b4231a8cf7c7d569111d3014d45585caad99e9",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.21,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,39 @@\n   public void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     final INodesInPath trgINodesInPath \u003d rootDir.getExistingPathINodes(target, true);\n     final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n     INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n     INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n     \n     INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     int i \u003d 0;\n     int totalBlocks \u003d 0;\n     for(String src : srcs) {\n       INodeFile srcInode \u003d (INodeFile)getINode(src);\n       allSrcInodes[i++] \u003d srcInode;\n       totalBlocks +\u003d srcInode.numBlocks();  \n     }\n     trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove);\n+      inodeMap.remove(nodeToRemove);\n       count++;\n     }\n     \n     trgInode.setModificationTimeForce(timestamp);\n     trgParent.setModificationTime(timestamp);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgINodesInPath \u003d rootDir.getExistingPathINodes(target, true);\n    final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    \n    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    int i \u003d 0;\n    int totalBlocks \u003d 0;\n    for(String src : srcs) {\n      INodeFile srcInode \u003d (INodeFile)getINode(src);\n      allSrcInodes[i++] \u003d srcInode;\n      totalBlocks +\u003d srcInode.numBlocks();  \n    }\n    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove);\n      inodeMap.remove(nodeToRemove);\n      count++;\n    }\n    \n    trgInode.setModificationTimeForce(timestamp);\n    trgParent.setModificationTime(timestamp);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "3a3e0f573129c8308332d4b301a9319ee579d85a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4760. Update inodeMap after node replacement.  Contributed by Jing Zhao\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1477827 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/04/13 2:05 PM",
      "commitName": "3a3e0f573129c8308332d4b301a9319ee579d85a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "26/04/13 5:05 PM",
      "commitNameOld": "5276f4e04dbeabe4dbcddafaa1cd386b912f6345",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 3.87,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,56 @@\n   void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException, QuotaExceededException,\n       SnapshotAccessControlException, SnapshotException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgIIP.getINodes();\n     final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n     INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n     final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n     \n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n       final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n       final Snapshot latest \u003d iip.getLatestSnapshot();\n       final INode inode \u003d iip.getLastINode();\n \n       // check if the file in the latest snapshot\n       if (inode.isInLatestSnapshot(latest)) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is in snapshot \" + latest);\n       }\n \n       // check if the file has other references.\n       if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n           inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n         throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n             + \" is referred by some other reference in some snapshot.\");\n       }\n \n       allSrcInodes[i] \u003d inode.asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n-      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n+      trgParent.removeChild(nodeToRemove, trgLatestSnapshot, null);\n       count++;\n     }\n     \n-    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n-    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n+    // update inodeMap\n+    removeFromInodeMap(Arrays.asList(allSrcInodes));\n+    \n+    trgInode.setModificationTime(timestamp, trgLatestSnapshot, inodeMap);\n+    trgParent.updateModificationTime(timestamp, trgLatestSnapshot, inodeMap);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException, QuotaExceededException,\n      SnapshotAccessControlException, SnapshotException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n    \n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n      final Snapshot latest \u003d iip.getLatestSnapshot();\n      final INode inode \u003d iip.getLastINode();\n\n      // check if the file in the latest snapshot\n      if (inode.isInLatestSnapshot(latest)) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is in snapshot \" + latest);\n      }\n\n      // check if the file has other references.\n      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is referred by some other reference in some snapshot.\");\n      }\n\n      allSrcInodes[i] \u003d inode.asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot, null);\n      count++;\n    }\n    \n    // update inodeMap\n    removeFromInodeMap(Arrays.asList(allSrcInodes));\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot, inodeMap);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot, inodeMap);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "38bd7061c1a8408b74ed619b25f948cd3cd85d7a": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-4529. Disallow concat when one of the src files is in some snapshot.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1468667 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/04/13 4:16 PM",
      "commitName": "38bd7061c1a8408b74ed619b25f948cd3cd85d7a",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-4529. Disallow concat when one of the src files is in some snapshot.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1468667 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/04/13 4:16 PM",
          "commitName": "38bd7061c1a8408b74ed619b25f948cd3cd85d7a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "16/04/13 3:03 PM",
          "commitNameOld": "6bda1f20ad396918edde211f709f5819a361b51e",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.05,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,53 @@\n   void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException, QuotaExceededException,\n-      SnapshotAccessControlException {\n+      SnapshotAccessControlException, SnapshotException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgIIP.getINodes();\n     final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n     INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n     final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n     \n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n-      allSrcInodes[i] \u003d getINode4Write(srcs[i]).asFile();\n+      final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n+      final Snapshot latest \u003d iip.getLatestSnapshot();\n+      final INode inode \u003d iip.getLastINode();\n+\n+      // check if the file in the latest snapshot\n+      if (inode.isInLatestSnapshot(latest)) {\n+        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n+            + \" is in snapshot \" + latest);\n+      }\n+\n+      // check if the file has other references.\n+      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n+          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n+        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n+            + \" is referred by some other reference in some snapshot.\");\n+      }\n+\n+      allSrcInodes[i] \u003d inode.asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       count++;\n     }\n     \n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException, QuotaExceededException,\n      SnapshotAccessControlException, SnapshotException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n    \n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n      final Snapshot latest \u003d iip.getLatestSnapshot();\n      final INode inode \u003d iip.getLastINode();\n\n      // check if the file in the latest snapshot\n      if (inode.isInLatestSnapshot(latest)) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is in snapshot \" + latest);\n      }\n\n      // check if the file has other references.\n      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is referred by some other reference in some snapshot.\");\n      }\n\n      allSrcInodes[i] \u003d inode.asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      count++;\n    }\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "[UnresolvedLinkException, QuotaExceededException, SnapshotAccessControlException]",
            "newValue": "[UnresolvedLinkException, QuotaExceededException, SnapshotAccessControlException, SnapshotException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4529. Disallow concat when one of the src files is in some snapshot.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1468667 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/04/13 4:16 PM",
          "commitName": "38bd7061c1a8408b74ed619b25f948cd3cd85d7a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "16/04/13 3:03 PM",
          "commitNameOld": "6bda1f20ad396918edde211f709f5819a361b51e",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.05,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,53 @@\n   void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException, QuotaExceededException,\n-      SnapshotAccessControlException {\n+      SnapshotAccessControlException, SnapshotException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgIIP.getINodes();\n     final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n     INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n     final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n     \n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n-      allSrcInodes[i] \u003d getINode4Write(srcs[i]).asFile();\n+      final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n+      final Snapshot latest \u003d iip.getLatestSnapshot();\n+      final INode inode \u003d iip.getLastINode();\n+\n+      // check if the file in the latest snapshot\n+      if (inode.isInLatestSnapshot(latest)) {\n+        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n+            + \" is in snapshot \" + latest);\n+      }\n+\n+      // check if the file has other references.\n+      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n+          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n+        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n+            + \" is referred by some other reference in some snapshot.\");\n+      }\n+\n+      allSrcInodes[i] \u003d inode.asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       count++;\n     }\n     \n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException, QuotaExceededException,\n      SnapshotAccessControlException, SnapshotException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n    \n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      final INodesInPath iip \u003d getINodesInPath4Write(srcs[i]);\n      final Snapshot latest \u003d iip.getLatestSnapshot();\n      final INode inode \u003d iip.getLastINode();\n\n      // check if the file in the latest snapshot\n      if (inode.isInLatestSnapshot(latest)) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is in snapshot \" + latest);\n      }\n\n      // check if the file has other references.\n      if (inode.isReference() \u0026\u0026 ((INodeReference.WithCount)\n          inode.asReference().getReferredINode()).getReferenceCount() \u003e 1) {\n        throw new SnapshotException(\"Concat: the source file \" + srcs[i]\n            + \" is referred by some other reference in some snapshot.\");\n      }\n\n      allSrcInodes[i] \u003d inode.asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      count++;\n    }\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {}
        }
      ]
    },
    "3b3ea5c4220e674064c7603a449f63904c10bac1": {
      "type": "Yexceptionschange",
      "commitMessage": "HDFS-4563. Update namespace/diskspace usage after deleting snapshots.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1455396 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/03/13 7:41 PM",
      "commitName": "3b3ea5c4220e674064c7603a449f63904c10bac1",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "10/03/13 8:45 PM",
      "commitNameOld": "43f8d0b9c9e209eb503451613c2f8d3fed07c203",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.96,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,36 @@\n   void unprotectedConcat(String target, String [] srcs, long timestamp) \n-      throws UnresolvedLinkException, NSQuotaExceededException,\n+      throws UnresolvedLinkException, QuotaExceededException,\n       SnapshotAccessControlException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgIIP.getINodes();\n     final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n     INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n     final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n     \n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n       allSrcInodes[i] \u003d getINode4Write(srcs[i]).asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       count++;\n     }\n     \n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException, QuotaExceededException,\n      SnapshotAccessControlException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n    \n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      allSrcInodes[i] \u003d getINode4Write(srcs[i]).asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      count++;\n    }\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {
        "oldValue": "[UnresolvedLinkException, NSQuotaExceededException, SnapshotAccessControlException]",
        "newValue": "[UnresolvedLinkException, QuotaExceededException, SnapshotAccessControlException]"
      }
    },
    "b1333e5b561d01a010e2e1311e8501879f377bdc": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-4545. With snapshots, FSDirectory.unprotectedSetReplication(..) always changes file replication but it may or may not changes block replication.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1452636 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/03/13 6:23 PM",
      "commitName": "b1333e5b561d01a010e2e1311e8501879f377bdc",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-4545. With snapshots, FSDirectory.unprotectedSetReplication(..) always changes file replication but it may or may not changes block replication.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1452636 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/03/13 6:23 PM",
          "commitName": "b1333e5b561d01a010e2e1311e8501879f377bdc",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "27/02/13 7:08 PM",
          "commitNameOld": "c7cf85ccb4ff2f58839e113f1baf903a468b606d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 4.97,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,36 @@\n-  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n+  void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException, NSQuotaExceededException,\n       SnapshotAccessControlException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n-    final INodesInPath trgINodesInPath \u003d rootDir.getINodesInPath4Write(target, true);\n-    final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n-    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n-    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n-    final Snapshot trgLatestSnapshot \u003d trgINodesInPath.getLatestSnapshot();\n+    final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n+    final INode[] trgINodes \u003d trgIIP.getINodes();\n+    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n+    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n+    final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n     \n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n-      allSrcInodes[i] \u003d (INodeFile)getINode4Write(srcs[i]);\n+      allSrcInodes[i] \u003d getINode4Write(srcs[i]).asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       count++;\n     }\n     \n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n-    unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n+    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException, NSQuotaExceededException,\n      SnapshotAccessControlException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n    \n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      allSrcInodes[i] \u003d getINode4Write(srcs[i]).asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      count++;\n    }\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4545. With snapshots, FSDirectory.unprotectedSetReplication(..) always changes file replication but it may or may not changes block replication.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1452636 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/03/13 6:23 PM",
          "commitName": "b1333e5b561d01a010e2e1311e8501879f377bdc",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "27/02/13 7:08 PM",
          "commitNameOld": "c7cf85ccb4ff2f58839e113f1baf903a468b606d",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 4.97,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,36 @@\n-  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n+  void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException, NSQuotaExceededException,\n       SnapshotAccessControlException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n-    final INodesInPath trgINodesInPath \u003d rootDir.getINodesInPath4Write(target, true);\n-    final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n-    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n-    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n-    final Snapshot trgLatestSnapshot \u003d trgINodesInPath.getLatestSnapshot();\n+    final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n+    final INode[] trgINodes \u003d trgIIP.getINodes();\n+    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n+    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n+    final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n     \n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n-      allSrcInodes[i] \u003d (INodeFile)getINode4Write(srcs[i]);\n+      allSrcInodes[i] \u003d getINode4Write(srcs[i]).asFile();\n     }\n     trgInode.concatBlocks(allSrcInodes);\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       count++;\n     }\n     \n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n-    unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n+    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException, NSQuotaExceededException,\n      SnapshotAccessControlException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgIIP \u003d rootDir.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgIIP.getINodes();\n    final INodeFile trgInode \u003d trgIIP.getLastINode().asFile();\n    INodeDirectory trgParent \u003d trgINodes[trgINodes.length-2].asDirectory();\n    final Snapshot trgLatestSnapshot \u003d trgIIP.getLatestSnapshot();\n    \n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      allSrcInodes[i] \u003d getINode4Write(srcs[i]).asFile();\n    }\n    trgInode.concatBlocks(allSrcInodes);\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      count++;\n    }\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgIIP, trgINodes.length-1, -count, 0);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {}
        }
      ]
    },
    "c7cf85ccb4ff2f58839e113f1baf903a468b606d": {
      "type": "Yexceptionschange",
      "commitMessage": "HDFS-4507. Update quota verification for snapshots.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1451081 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/02/13 7:08 PM",
      "commitName": "c7cf85ccb4ff2f58839e113f1baf903a468b606d",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "26/02/13 2:04 PM",
      "commitNameOld": "e2a618e1cc3fb99115547af6540932860dc6766e",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.21,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,36 @@\n   public void unprotectedConcat(String target, String [] srcs, long timestamp) \n-      throws UnresolvedLinkException, SnapshotAccessControlException {\n+      throws UnresolvedLinkException, NSQuotaExceededException,\n+      SnapshotAccessControlException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     final INodesInPath trgINodesInPath \u003d rootDir.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n     INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n     INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n     final Snapshot trgLatestSnapshot \u003d trgINodesInPath.getLatestSnapshot();\n     \n     final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     for(int i \u003d 0; i \u003c srcs.length; i++) {\n       allSrcInodes[i] \u003d (INodeFile)getINode4Write(srcs[i]);\n     }\n     trgInode.concatBlocks(allSrcInodes);\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       count++;\n     }\n     \n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException, NSQuotaExceededException,\n      SnapshotAccessControlException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgINodesInPath \u003d rootDir.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    final Snapshot trgLatestSnapshot \u003d trgINodesInPath.getLatestSnapshot();\n    \n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      allSrcInodes[i] \u003d (INodeFile)getINode4Write(srcs[i]);\n    }\n    trgInode.concatBlocks(allSrcInodes);\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      count++;\n    }\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {
        "oldValue": "[UnresolvedLinkException, SnapshotAccessControlException]",
        "newValue": "[UnresolvedLinkException, NSQuotaExceededException, SnapshotAccessControlException]"
      }
    },
    "e2a618e1cc3fb99115547af6540932860dc6766e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4523. Fix INodeFile replacement, TestQuota and javac errors from trunk merge.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1450477 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/02/13 2:04 PM",
      "commitName": "e2a618e1cc3fb99115547af6540932860dc6766e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "25/02/13 4:10 PM",
      "commitNameOld": "aa82b03823d809fb70cc3d420570ef20e3368bdf",
      "commitAuthorOld": "",
      "daysBetweenCommits": 0.91,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,35 @@\n   public void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException, SnapshotAccessControlException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     final INodesInPath trgINodesInPath \u003d rootDir.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n     INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n     INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n     final Snapshot trgLatestSnapshot \u003d trgINodesInPath.getLatestSnapshot();\n     \n-    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n-    int i \u003d 0;\n-    int totalBlocks \u003d 0;\n-    for(String src : srcs) {\n-      INodeFile srcInode \u003d (INodeFile)getINode(src);\n-      allSrcInodes[i++] \u003d srcInode;\n-      totalBlocks +\u003d srcInode.numBlocks();  \n+    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n+    for(int i \u003d 0; i \u003c srcs.length; i++) {\n+      allSrcInodes[i] \u003d (INodeFile)getINode4Write(srcs[i]);\n     }\n-    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n+    trgInode.concatBlocks(allSrcInodes);\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       count++;\n     }\n     \n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException, SnapshotAccessControlException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgINodesInPath \u003d rootDir.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    final Snapshot trgLatestSnapshot \u003d trgINodesInPath.getLatestSnapshot();\n    \n    final INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    for(int i \u003d 0; i \u003c srcs.length; i++) {\n      allSrcInodes[i] \u003d (INodeFile)getINode4Write(srcs[i]);\n    }\n    trgInode.concatBlocks(allSrcInodes);\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      count++;\n    }\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "2372e394dd99d69d396327d5a5e172953a8b8c6a": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-4189. Renames the getMutableXxx methods to getXxx4Write and fix a bug that some getExistingPathINodes calls should be getINodesInPath4Write.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1441193 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/01/13 1:13 PM",
      "commitName": "2372e394dd99d69d396327d5a5e172953a8b8c6a",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-4189. Renames the getMutableXxx methods to getXxx4Write and fix a bug that some getExistingPathINodes calls should be getINodesInPath4Write.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1441193 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "31/01/13 1:13 PM",
          "commitName": "2372e394dd99d69d396327d5a5e172953a8b8c6a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "25/01/13 4:01 PM",
          "commitNameOld": "a3bf2083867db5d848ea14f145d120f02b820af2",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 5.88,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,39 @@\n   public void unprotectedConcat(String target, String [] srcs, long timestamp) \n-      throws UnresolvedLinkException {\n+      throws UnresolvedLinkException, SnapshotAccessControlException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n-    final INodesInPath trgINodesInPath \u003d rootDir.getExistingPathINodes(target, true);\n+    final INodesInPath trgINodesInPath \u003d rootDir.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n     INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n     INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n     final Snapshot trgLatestSnapshot \u003d trgINodesInPath.getLatestSnapshot();\n     \n     INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     int i \u003d 0;\n     int totalBlocks \u003d 0;\n     for(String src : srcs) {\n       INodeFile srcInode \u003d (INodeFile)getINode(src);\n       allSrcInodes[i++] \u003d srcInode;\n       totalBlocks +\u003d srcInode.numBlocks();  \n     }\n     trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       count++;\n     }\n     \n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException, SnapshotAccessControlException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgINodesInPath \u003d rootDir.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    final Snapshot trgLatestSnapshot \u003d trgINodesInPath.getLatestSnapshot();\n    \n    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    int i \u003d 0;\n    int totalBlocks \u003d 0;\n    for(String src : srcs) {\n      INodeFile srcInode \u003d (INodeFile)getINode(src);\n      allSrcInodes[i++] \u003d srcInode;\n      totalBlocks +\u003d srcInode.numBlocks();  \n    }\n    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      count++;\n    }\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {
            "oldValue": "[UnresolvedLinkException]",
            "newValue": "[UnresolvedLinkException, SnapshotAccessControlException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4189. Renames the getMutableXxx methods to getXxx4Write and fix a bug that some getExistingPathINodes calls should be getINodesInPath4Write.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1441193 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "31/01/13 1:13 PM",
          "commitName": "2372e394dd99d69d396327d5a5e172953a8b8c6a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "25/01/13 4:01 PM",
          "commitNameOld": "a3bf2083867db5d848ea14f145d120f02b820af2",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 5.88,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,39 @@\n   public void unprotectedConcat(String target, String [] srcs, long timestamp) \n-      throws UnresolvedLinkException {\n+      throws UnresolvedLinkException, SnapshotAccessControlException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n-    final INodesInPath trgINodesInPath \u003d rootDir.getExistingPathINodes(target, true);\n+    final INodesInPath trgINodesInPath \u003d rootDir.getINodesInPath4Write(target, true);\n     final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n     INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n     INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n     final Snapshot trgLatestSnapshot \u003d trgINodesInPath.getLatestSnapshot();\n     \n     INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     int i \u003d 0;\n     int totalBlocks \u003d 0;\n     for(String src : srcs) {\n       INodeFile srcInode \u003d (INodeFile)getINode(src);\n       allSrcInodes[i++] \u003d srcInode;\n       totalBlocks +\u003d srcInode.numBlocks();  \n     }\n     trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       count++;\n     }\n     \n     trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n     trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException, SnapshotAccessControlException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgINodesInPath \u003d rootDir.getINodesInPath4Write(target, true);\n    final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    final Snapshot trgLatestSnapshot \u003d trgINodesInPath.getLatestSnapshot();\n    \n    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    int i \u003d 0;\n    int totalBlocks \u003d 0;\n    for(String src : srcs) {\n      INodeFile srcInode \u003d (INodeFile)getINode(src);\n      allSrcInodes[i++] \u003d srcInode;\n      totalBlocks +\u003d srcInode.numBlocks();  \n    }\n    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      count++;\n    }\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
          "extendedDetails": {}
        }
      ]
    },
    "b9f965de120b5278ac84a7e98aecb32aafde4c16": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4103. Support O(1) snapshot creation.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1424782 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/12 5:30 PM",
      "commitName": "b9f965de120b5278ac84a7e98aecb32aafde4c16",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "16/12/12 7:40 PM",
      "commitNameOld": "cbbaa93ae09bf5cf643263faf78f99315c4f3a8d",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 3.91,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,39 @@\n   public void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     final INodesInPath trgINodesInPath \u003d rootDir.getExistingPathINodes(target, true);\n     final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n     INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n     INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n+    final Snapshot trgLatestSnapshot \u003d trgINodesInPath.getLatestSnapshot();\n     \n     INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     int i \u003d 0;\n     int totalBlocks \u003d 0;\n     for(String src : srcs) {\n       INodeFile srcInode \u003d (INodeFile)getINode(src);\n       allSrcInodes[i++] \u003d srcInode;\n       totalBlocks +\u003d srcInode.numBlocks();  \n     }\n     trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n-      trgParent.removeChild(nodeToRemove);\n+      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n       count++;\n     }\n     \n-    trgInode.setModificationTime(timestamp);\n-    trgParent.updateModificationTime(timestamp);\n+    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n+    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgINodesInPath \u003d rootDir.getExistingPathINodes(target, true);\n    final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    final Snapshot trgLatestSnapshot \u003d trgINodesInPath.getLatestSnapshot();\n    \n    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    int i \u003d 0;\n    int totalBlocks \u003d 0;\n    for(String src : srcs) {\n      INodeFile srcInode \u003d (INodeFile)getINode(src);\n      allSrcInodes[i++] \u003d srcInode;\n      totalBlocks +\u003d srcInode.numBlocks();  \n    }\n    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove, trgLatestSnapshot);\n      count++;\n    }\n    \n    trgInode.setModificationTime(timestamp, trgLatestSnapshot);\n    trgParent.updateModificationTime(timestamp, trgLatestSnapshot);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "cbbaa93ae09bf5cf643263faf78f99315c4f3a8d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4317. Change INode and its subclasses to support HDFS-4103.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1422748 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/12/12 7:40 PM",
      "commitName": "cbbaa93ae09bf5cf643263faf78f99315c4f3a8d",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "10/12/12 3:54 PM",
      "commitNameOld": "39d25fbac331ede57196f7a2d2d5e26e2fbc1c9f",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 6.16,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   public void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     final INodesInPath trgINodesInPath \u003d rootDir.getExistingPathINodes(target, true);\n     final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n     INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n     INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n     \n     INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     int i \u003d 0;\n     int totalBlocks \u003d 0;\n     for(String src : srcs) {\n       INodeFile srcInode \u003d (INodeFile)getINode(src);\n       allSrcInodes[i++] \u003d srcInode;\n       totalBlocks +\u003d srcInode.numBlocks();  \n     }\n     trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove);\n       count++;\n     }\n     \n-    trgInode.setModificationTimeForce(timestamp);\n-    trgParent.setModificationTime(timestamp);\n+    trgInode.setModificationTime(timestamp);\n+    trgParent.updateModificationTime(timestamp);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgINodesInPath \u003d rootDir.getExistingPathINodes(target, true);\n    final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    \n    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    int i \u003d 0;\n    int totalBlocks \u003d 0;\n    for(String src : srcs) {\n      INodeFile srcInode \u003d (INodeFile)getINode(src);\n      allSrcInodes[i++] \u003d srcInode;\n      totalBlocks +\u003d srcInode.numBlocks();  \n    }\n    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove);\n      count++;\n    }\n    \n    trgInode.setModificationTime(timestamp);\n    trgParent.updateModificationTime(timestamp);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "9821af9ce8a56a2c583f1ed938902c20e897048f": {
      "type": "Ybodychange",
      "commitMessage": "Reverting the previous merge r1416603 which committed some extra changes\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1416712 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/12/12 2:43 PM",
      "commitName": "9821af9ce8a56a2c583f1ed938902c20e897048f",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "03/12/12 10:04 AM",
      "commitNameOld": "d500d59cbef51f1b0b0291995893b85a139bcec9",
      "commitAuthorOld": "",
      "daysBetweenCommits": 0.19,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   public void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     final INodesInPath trgINodesInPath \u003d rootDir.getExistingPathINodes(target, true);\n     final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n     INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n     INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n     \n     INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     int i \u003d 0;\n     int totalBlocks \u003d 0;\n     for(String src : srcs) {\n       INodeFile srcInode \u003d (INodeFile)getINode(src);\n       allSrcInodes[i++] \u003d srcInode;\n       totalBlocks +\u003d srcInode.numBlocks();  \n     }\n     trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n-      trgParent.removeChild(nodeToRemove, trgINodesInPath.getLatestSnapshot());\n+      trgParent.removeChild(nodeToRemove);\n       count++;\n     }\n     \n-    trgInode.setModificationTime(timestamp);\n-    trgParent.updateModificationTime(timestamp);\n+    trgInode.setModificationTimeForce(timestamp);\n+    trgParent.setModificationTime(timestamp);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgINodesInPath \u003d rootDir.getExistingPathINodes(target, true);\n    final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    \n    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    int i \u003d 0;\n    int totalBlocks \u003d 0;\n    for(String src : srcs) {\n      INodeFile srcInode \u003d (INodeFile)getINode(src);\n      allSrcInodes[i++] \u003d srcInode;\n      totalBlocks +\u003d srcInode.numBlocks();  \n    }\n    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove);\n      count++;\n    }\n    \n    trgInode.setModificationTimeForce(timestamp);\n    trgParent.setModificationTime(timestamp);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "34413c2000d9262faa37fde88a72939587edc776": {
      "type": "Ybodychange",
      "commitMessage": "svn merge -c 1406006 from trunk for HDFS-4151. Change the methods in FSDirectory to pass INodesInPath instead of INode[] as a parameter.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1406014 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/11/12 3:56 PM",
      "commitName": "34413c2000d9262faa37fde88a72939587edc776",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "04/11/12 5:22 PM",
      "commitNameOld": "b3bc2fb76e1aca8e7327d1d1a6e4c8a013c575de",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.94,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,38 @@\n   public void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n-    INode [] trgINodes \u003d  getExistingPathINodes(target);\n+    final INodesInPath trgINodesInPath \u003d rootDir.getExistingPathINodes(target, true);\n+    final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n     INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n     INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n     \n     INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     int i \u003d 0;\n     int totalBlocks \u003d 0;\n     for(String src : srcs) {\n       INodeFile srcInode \u003d (INodeFile)getINode(src);\n       allSrcInodes[i++] \u003d srcInode;\n       totalBlocks +\u003d srcInode.numBlocks();  \n     }\n     trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove);\n       count++;\n     }\n     \n     trgInode.setModificationTimeForce(timestamp);\n     trgParent.setModificationTime(timestamp);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n-    unprotectedUpdateCount(trgINodes, trgINodes.length-1, - count, 0);\n+    unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgINodesInPath \u003d rootDir.getExistingPathINodes(target, true);\n    final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    \n    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    int i \u003d 0;\n    int totalBlocks \u003d 0;\n    for(String src : srcs) {\n      INodeFile srcInode \u003d (INodeFile)getINode(src);\n      allSrcInodes[i++] \u003d srcInode;\n      totalBlocks +\u003d srcInode.numBlocks();  \n    }\n    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove);\n      count++;\n    }\n    \n    trgInode.setModificationTimeForce(timestamp);\n    trgParent.setModificationTime(timestamp);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "7ee5ce3176a74d217551b5981f809a56c719424b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4151. Change the methods in FSDirectory to pass INodesInPath instead of INode[] as a parameter.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1406006 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/11/12 3:26 PM",
      "commitName": "7ee5ce3176a74d217551b5981f809a56c719424b",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "02/11/12 5:20 PM",
      "commitNameOld": "d174f574bafcfefc635c64a47f258b1ce5d5c84e",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 2.96,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,38 @@\n   public void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n-    INode [] trgINodes \u003d  getExistingPathINodes(target);\n+    final INodesInPath trgINodesInPath \u003d rootDir.getExistingPathINodes(target, true);\n+    final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n     INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n     INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n     \n     INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     int i \u003d 0;\n     int totalBlocks \u003d 0;\n     for(String src : srcs) {\n       INodeFile srcInode \u003d (INodeFile)getINode(src);\n       allSrcInodes[i++] \u003d srcInode;\n       totalBlocks +\u003d srcInode.numBlocks();  \n     }\n     trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove);\n       count++;\n     }\n     \n     trgInode.setModificationTimeForce(timestamp);\n     trgParent.setModificationTime(timestamp);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n-    unprotectedUpdateCount(trgINodes, trgINodes.length-1, - count, 0);\n+    unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    final INodesInPath trgINodesInPath \u003d rootDir.getExistingPathINodes(target, true);\n    final INode[] trgINodes \u003d trgINodesInPath.getINodes();\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    \n    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    int i \u003d 0;\n    int totalBlocks \u003d 0;\n    for(String src : srcs) {\n      INodeFile srcInode \u003d (INodeFile)getINode(src);\n      allSrcInodes[i++] \u003d srcInode;\n      totalBlocks +\u003d srcInode.numBlocks();  \n    }\n    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove);\n      count++;\n    }\n    \n    trgInode.setModificationTimeForce(timestamp);\n    trgParent.setModificationTime(timestamp);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodesInPath, trgINodes.length-1, -count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "d174f574bafcfefc635c64a47f258b1ce5d5c84e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4143. Change blocks to private in INodeFile and renames isLink() to isSymlink() in INode.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1405237 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/11/12 5:20 PM",
      "commitName": "d174f574bafcfefc635c64a47f258b1ce5d5c84e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "29/10/12 7:11 AM",
      "commitNameOld": "1b3b09d94794622e8336220d897a1f10c4654677",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 4.42,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,37 @@\n   public void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     INode [] trgINodes \u003d  getExistingPathINodes(target);\n     INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n     INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n     \n     INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     int i \u003d 0;\n     int totalBlocks \u003d 0;\n     for(String src : srcs) {\n       INodeFile srcInode \u003d (INodeFile)getINode(src);\n       allSrcInodes[i++] \u003d srcInode;\n-      totalBlocks +\u003d srcInode.blocks.length;  \n+      totalBlocks +\u003d srcInode.numBlocks();  \n     }\n     trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n-      nodeToRemove.blocks \u003d null;\n+      nodeToRemove.setBlocks(null);\n       trgParent.removeChild(nodeToRemove);\n       count++;\n     }\n     \n     trgInode.setModificationTimeForce(timestamp);\n     trgParent.setModificationTime(timestamp);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgINodes, trgINodes.length-1, - count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    INode [] trgINodes \u003d  getExistingPathINodes(target);\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    \n    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    int i \u003d 0;\n    int totalBlocks \u003d 0;\n    for(String src : srcs) {\n      INodeFile srcInode \u003d (INodeFile)getINode(src);\n      allSrcInodes[i++] \u003d srcInode;\n      totalBlocks +\u003d srcInode.numBlocks();  \n    }\n    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.setBlocks(null);\n      trgParent.removeChild(nodeToRemove);\n      count++;\n    }\n    \n    trgInode.setModificationTimeForce(timestamp);\n    trgParent.setModificationTime(timestamp);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodes, trgINodes.length-1, - count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "ba2ee1d7fb91462c861169224d250d2d90bec3a6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4107. Add utility methods for casting INode to INodeFile and INodeFileUnderConstruction.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1402265 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/10/12 11:44 AM",
      "commitName": "ba2ee1d7fb91462c861169224d250d2d90bec3a6",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "18/10/12 2:18 PM",
      "commitNameOld": "c1bd54daa3724b0646d2bdbe666b7d44048953ba",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 6.89,
      "commitsBetweenForRepo": 46,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,37 @@\n   public void unprotectedConcat(String target, String [] srcs, long timestamp) \n       throws UnresolvedLinkException {\n     assert hasWriteLock();\n     if (NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n     }\n     // do the move\n     \n     INode [] trgINodes \u003d  getExistingPathINodes(target);\n     INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n     INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n     \n     INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n     int i \u003d 0;\n     int totalBlocks \u003d 0;\n     for(String src : srcs) {\n-      INodeFile srcInode \u003d getFileINode(src);\n+      INodeFile srcInode \u003d (INodeFile)getINode(src);\n       allSrcInodes[i++] \u003d srcInode;\n       totalBlocks +\u003d srcInode.blocks.length;  \n     }\n     trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n     \n     // since we are in the same dir - we can use same parent to remove files\n     int count \u003d 0;\n     for(INodeFile nodeToRemove: allSrcInodes) {\n       if(nodeToRemove \u003d\u003d null) continue;\n       \n       nodeToRemove.blocks \u003d null;\n       trgParent.removeChild(nodeToRemove);\n       count++;\n     }\n     \n     trgInode.setModificationTimeForce(timestamp);\n     trgParent.setModificationTime(timestamp);\n     // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n     unprotectedUpdateCount(trgINodes, trgINodes.length-1, - count, 0);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    INode [] trgINodes \u003d  getExistingPathINodes(target);\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    \n    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    int i \u003d 0;\n    int totalBlocks \u003d 0;\n    for(String src : srcs) {\n      INodeFile srcInode \u003d (INodeFile)getINode(src);\n      allSrcInodes[i++] \u003d srcInode;\n      totalBlocks +\u003d srcInode.blocks.length;  \n    }\n    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.blocks \u003d null;\n      trgParent.removeChild(nodeToRemove);\n      count++;\n    }\n    \n    trgInode.setModificationTimeForce(timestamp);\n    trgParent.setModificationTime(timestamp);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodes, trgINodes.length-1, - count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    INode [] trgINodes \u003d  getExistingPathINodes(target);\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    \n    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    int i \u003d 0;\n    int totalBlocks \u003d 0;\n    for(String src : srcs) {\n      INodeFile srcInode \u003d getFileINode(src);\n      allSrcInodes[i++] \u003d srcInode;\n      totalBlocks +\u003d srcInode.blocks.length;  \n    }\n    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.blocks \u003d null;\n      trgParent.removeChild(nodeToRemove);\n      count++;\n    }\n    \n    trgInode.setModificationTimeForce(timestamp);\n    trgParent.setModificationTime(timestamp);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodes, trgINodes.length-1, - count, 0);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    INode [] trgINodes \u003d  getExistingPathINodes(target);\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    \n    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    int i \u003d 0;\n    int totalBlocks \u003d 0;\n    for(String src : srcs) {\n      INodeFile srcInode \u003d getFileINode(src);\n      allSrcInodes[i++] \u003d srcInode;\n      totalBlocks +\u003d srcInode.blocks.length;  \n    }\n    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.blocks \u003d null;\n      trgParent.removeChild(nodeToRemove);\n      count++;\n    }\n    \n    trgInode.setModificationTimeForce(timestamp);\n    trgParent.setModificationTime(timestamp);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodes, trgINodes.length-1, - count, 0);\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,37 @@\n+  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n+      throws UnresolvedLinkException {\n+    assert hasWriteLock();\n+    if (NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n+    }\n+    // do the move\n+    \n+    INode [] trgINodes \u003d  getExistingPathINodes(target);\n+    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n+    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n+    \n+    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n+    int i \u003d 0;\n+    int totalBlocks \u003d 0;\n+    for(String src : srcs) {\n+      INodeFile srcInode \u003d getFileINode(src);\n+      allSrcInodes[i++] \u003d srcInode;\n+      totalBlocks +\u003d srcInode.blocks.length;  \n+    }\n+    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n+    \n+    // since we are in the same dir - we can use same parent to remove files\n+    int count \u003d 0;\n+    for(INodeFile nodeToRemove: allSrcInodes) {\n+      if(nodeToRemove \u003d\u003d null) continue;\n+      \n+      nodeToRemove.blocks \u003d null;\n+      trgParent.removeChild(nodeToRemove);\n+      count++;\n+    }\n+    \n+    trgInode.setModificationTimeForce(timestamp);\n+    trgParent.setModificationTime(timestamp);\n+    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n+    unprotectedUpdateCount(trgINodes, trgINodes.length-1, - count, 0);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void unprotectedConcat(String target, String [] srcs, long timestamp) \n      throws UnresolvedLinkException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSNamesystem.concat to \"+target);\n    }\n    // do the move\n    \n    INode [] trgINodes \u003d  getExistingPathINodes(target);\n    INodeFile trgInode \u003d (INodeFile) trgINodes[trgINodes.length-1];\n    INodeDirectory trgParent \u003d (INodeDirectory)trgINodes[trgINodes.length-2];\n    \n    INodeFile [] allSrcInodes \u003d new INodeFile[srcs.length];\n    int i \u003d 0;\n    int totalBlocks \u003d 0;\n    for(String src : srcs) {\n      INodeFile srcInode \u003d getFileINode(src);\n      allSrcInodes[i++] \u003d srcInode;\n      totalBlocks +\u003d srcInode.blocks.length;  \n    }\n    trgInode.appendBlocks(allSrcInodes, totalBlocks); // copy the blocks\n    \n    // since we are in the same dir - we can use same parent to remove files\n    int count \u003d 0;\n    for(INodeFile nodeToRemove: allSrcInodes) {\n      if(nodeToRemove \u003d\u003d null) continue;\n      \n      nodeToRemove.blocks \u003d null;\n      trgParent.removeChild(nodeToRemove);\n      count++;\n    }\n    \n    trgInode.setModificationTimeForce(timestamp);\n    trgParent.setModificationTime(timestamp);\n    // update quota on the parent directory (\u0027count\u0027 files removed, 0 space)\n    unprotectedUpdateCount(trgINodes, trgINodes.length-1, - count, 0);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java"
    }
  }
}