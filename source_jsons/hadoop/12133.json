{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataNode.java",
  "functionName": "startDataNode",
  "functionId": "startDataNode___dataDirectories-List__StorageLocation____resources-SecureResources",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
  "functionStartLine": 1389,
  "functionEndLine": 1478,
  "numCommitsSeen": 358,
  "timeTaken": 4479,
  "changeHistory": [
    "915271245b9a3f682409db380eb311ffd465b041",
    "e1dfc060f8f0247f97127c75c9284a068fc93907",
    "68d5dfdc78d121e89eeae4e577d670028a14a955",
    "39ed3a66dbb01383ed16b141183fc48bfd2e613d",
    "dfcb331ba3516264398121c9f23af3a79c0509cc"
  ],
  "changeHistoryShort": {
    "915271245b9a3f682409db380eb311ffd465b041": "Ybodychange",
    "e1dfc060f8f0247f97127c75c9284a068fc93907": "Ybodychange",
    "68d5dfdc78d121e89eeae4e577d670028a14a955": "Ybodychange",
    "39ed3a66dbb01383ed16b141183fc48bfd2e613d": "Ybodychange",
    "dfcb331ba3516264398121c9f23af3a79c0509cc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "915271245b9a3f682409db380eb311ffd465b041": {
      "type": "Ybodychange",
      "commitMessage": "Make DataNodePeerMetrics#minOutlierDetectionSamples configurable (#1314). Contributed by Lisheng Sun.\n\nSigned-off-by: sunlisheng \u003csunlisheng@xiaomi.com\u003e",
      "commitDate": "29/08/19 6:35 PM",
      "commitName": "915271245b9a3f682409db380eb311ffd465b041",
      "commitAuthor": "leosunli",
      "commitDateOld": "19/08/19 1:08 PM",
      "commitNameOld": "360a96f342f3c8cb8246f011abb9bcb0b6ef3eaa",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 10.23,
      "commitsBetweenForRepo": 109,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,90 @@\n   void startDataNode(List\u003cStorageLocation\u003e dataDirectories,\n                      SecureResources resources\n                      ) throws IOException {\n \n     // settings global for all BPs in the Data Node\n     this.secureResources \u003d resources;\n     synchronized (this) {\n       this.dataDirs \u003d dataDirectories;\n     }\n     this.dnConf \u003d new DNConf(this);\n     checkSecureConfig(dnConf, getConf(), resources);\n \n     if (dnConf.maxLockedMemory \u003e 0) {\n       if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {\n         throw new RuntimeException(String.format(\n             \"Cannot start datanode because the configured max locked memory\" +\n             \" size (%s) is greater than zero and native code is not available.\",\n             DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));\n       }\n       if (Path.WINDOWS) {\n         NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);\n       } else {\n         long ulimit \u003d NativeIO.POSIX.getCacheManipulator().getMemlockLimit();\n         if (dnConf.maxLockedMemory \u003e ulimit) {\n           throw new RuntimeException(String.format(\n             \"Cannot start datanode because the configured max locked memory\" +\n             \" size (%s) of %d bytes is more than the datanode\u0027s available\" +\n             \" RLIMIT_MEMLOCK ulimit of %d bytes.\",\n             DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,\n             dnConf.maxLockedMemory,\n             ulimit));\n         }\n       }\n     }\n     LOG.info(\"Starting DataNode with maxLockedMemory \u003d {}\",\n         dnConf.maxLockedMemory);\n \n     int volFailuresTolerated \u003d dnConf.getVolFailuresTolerated();\n     int volsConfigured \u003d dnConf.getVolsConfigured();\n     if (volFailuresTolerated \u003c MAX_VOLUME_FAILURE_TOLERATED_LIMIT\n         || volFailuresTolerated \u003e\u003d volsConfigured) {\n       throw new HadoopIllegalArgumentException(\"Invalid value configured for \"\n           + \"dfs.datanode.failed.volumes.tolerated - \" + volFailuresTolerated\n           + \". Value configured is either less than -1 or \u003e\u003d \"\n           + \"to the number of configured volumes (\" + volsConfigured + \").\");\n     }\n \n     storage \u003d new DataStorage();\n     \n     // global DN settings\n     registerMXBean();\n     initDataXceiver();\n     startInfoServer();\n     pauseMonitor \u003d new JvmPauseMonitor();\n     pauseMonitor.init(getConf());\n     pauseMonitor.start();\n   \n     // BlockPoolTokenSecretManager is required to create ipc server.\n     this.blockPoolTokenSecretManager \u003d new BlockPoolTokenSecretManager();\n \n     // Login is done by now. Set the DN user name.\n     dnUserName \u003d UserGroupInformation.getCurrentUser().getUserName();\n     LOG.info(\"dnUserName \u003d {}\", dnUserName);\n     LOG.info(\"supergroup \u003d {}\", supergroup);\n     initIpcServer();\n \n     metrics \u003d DataNodeMetrics.create(getConf(), getDisplayName());\n     peerMetrics \u003d dnConf.peerStatsEnabled ?\n-        DataNodePeerMetrics.create(getDisplayName()) : null;\n+        DataNodePeerMetrics.create(getDisplayName(), getConf()) : null;\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n \n     ecWorker \u003d new ErasureCodingWorker(getConf(), this);\n     blockRecoveryWorker \u003d new BlockRecoveryWorker(this);\n \n     blockPoolManager \u003d new BlockPoolManager(this);\n     blockPoolManager.refreshNamenodes(getConf());\n \n     // Create the ReadaheadPool from the DataNode context so we can\n     // exit without having to explicitly shutdown its thread pool.\n     readaheadPool \u003d ReadaheadPool.getInstance();\n     saslClient \u003d new SaslDataTransferClient(dnConf.getConf(),\n         dnConf.saslPropsResolver, dnConf.trustedChannelResolver);\n     saslServer \u003d new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);\n     startMetricsLogger();\n \n     if (dnConf.diskStatsEnabled) {\n       diskMetrics \u003d new DataNodeDiskMetrics(this,\n           dnConf.outliersReportIntervalMs);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void startDataNode(List\u003cStorageLocation\u003e dataDirectories,\n                     SecureResources resources\n                     ) throws IOException {\n\n    // settings global for all BPs in the Data Node\n    this.secureResources \u003d resources;\n    synchronized (this) {\n      this.dataDirs \u003d dataDirectories;\n    }\n    this.dnConf \u003d new DNConf(this);\n    checkSecureConfig(dnConf, getConf(), resources);\n\n    if (dnConf.maxLockedMemory \u003e 0) {\n      if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {\n        throw new RuntimeException(String.format(\n            \"Cannot start datanode because the configured max locked memory\" +\n            \" size (%s) is greater than zero and native code is not available.\",\n            DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));\n      }\n      if (Path.WINDOWS) {\n        NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);\n      } else {\n        long ulimit \u003d NativeIO.POSIX.getCacheManipulator().getMemlockLimit();\n        if (dnConf.maxLockedMemory \u003e ulimit) {\n          throw new RuntimeException(String.format(\n            \"Cannot start datanode because the configured max locked memory\" +\n            \" size (%s) of %d bytes is more than the datanode\u0027s available\" +\n            \" RLIMIT_MEMLOCK ulimit of %d bytes.\",\n            DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,\n            dnConf.maxLockedMemory,\n            ulimit));\n        }\n      }\n    }\n    LOG.info(\"Starting DataNode with maxLockedMemory \u003d {}\",\n        dnConf.maxLockedMemory);\n\n    int volFailuresTolerated \u003d dnConf.getVolFailuresTolerated();\n    int volsConfigured \u003d dnConf.getVolsConfigured();\n    if (volFailuresTolerated \u003c MAX_VOLUME_FAILURE_TOLERATED_LIMIT\n        || volFailuresTolerated \u003e\u003d volsConfigured) {\n      throw new HadoopIllegalArgumentException(\"Invalid value configured for \"\n          + \"dfs.datanode.failed.volumes.tolerated - \" + volFailuresTolerated\n          + \". Value configured is either less than -1 or \u003e\u003d \"\n          + \"to the number of configured volumes (\" + volsConfigured + \").\");\n    }\n\n    storage \u003d new DataStorage();\n    \n    // global DN settings\n    registerMXBean();\n    initDataXceiver();\n    startInfoServer();\n    pauseMonitor \u003d new JvmPauseMonitor();\n    pauseMonitor.init(getConf());\n    pauseMonitor.start();\n  \n    // BlockPoolTokenSecretManager is required to create ipc server.\n    this.blockPoolTokenSecretManager \u003d new BlockPoolTokenSecretManager();\n\n    // Login is done by now. Set the DN user name.\n    dnUserName \u003d UserGroupInformation.getCurrentUser().getUserName();\n    LOG.info(\"dnUserName \u003d {}\", dnUserName);\n    LOG.info(\"supergroup \u003d {}\", supergroup);\n    initIpcServer();\n\n    metrics \u003d DataNodeMetrics.create(getConf(), getDisplayName());\n    peerMetrics \u003d dnConf.peerStatsEnabled ?\n        DataNodePeerMetrics.create(getDisplayName(), getConf()) : null;\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    ecWorker \u003d new ErasureCodingWorker(getConf(), this);\n    blockRecoveryWorker \u003d new BlockRecoveryWorker(this);\n\n    blockPoolManager \u003d new BlockPoolManager(this);\n    blockPoolManager.refreshNamenodes(getConf());\n\n    // Create the ReadaheadPool from the DataNode context so we can\n    // exit without having to explicitly shutdown its thread pool.\n    readaheadPool \u003d ReadaheadPool.getInstance();\n    saslClient \u003d new SaslDataTransferClient(dnConf.getConf(),\n        dnConf.saslPropsResolver, dnConf.trustedChannelResolver);\n    saslServer \u003d new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);\n    startMetricsLogger();\n\n    if (dnConf.diskStatsEnabled) {\n      diskMetrics \u003d new DataNodeDiskMetrics(this,\n          dnConf.outliersReportIntervalMs);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {}
    },
    "e1dfc060f8f0247f97127c75c9284a068fc93907": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14486. The exception classes in some throw statements do not accurately describe why they are thrown. Contributed by Ayush Saxena.\n",
      "commitDate": "06/06/19 11:59 AM",
      "commitName": "e1dfc060f8f0247f97127c75c9284a068fc93907",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "12/04/19 5:37 PM",
      "commitNameOld": "626fec652b9f3dae10c9af78fd220b1240f19fc7",
      "commitAuthorOld": "Chen Liang",
      "daysBetweenCommits": 54.77,
      "commitsBetweenForRepo": 316,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,90 @@\n   void startDataNode(List\u003cStorageLocation\u003e dataDirectories,\n                      SecureResources resources\n                      ) throws IOException {\n \n     // settings global for all BPs in the Data Node\n     this.secureResources \u003d resources;\n     synchronized (this) {\n       this.dataDirs \u003d dataDirectories;\n     }\n     this.dnConf \u003d new DNConf(this);\n     checkSecureConfig(dnConf, getConf(), resources);\n \n     if (dnConf.maxLockedMemory \u003e 0) {\n       if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {\n         throw new RuntimeException(String.format(\n             \"Cannot start datanode because the configured max locked memory\" +\n             \" size (%s) is greater than zero and native code is not available.\",\n             DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));\n       }\n       if (Path.WINDOWS) {\n         NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);\n       } else {\n         long ulimit \u003d NativeIO.POSIX.getCacheManipulator().getMemlockLimit();\n         if (dnConf.maxLockedMemory \u003e ulimit) {\n           throw new RuntimeException(String.format(\n             \"Cannot start datanode because the configured max locked memory\" +\n             \" size (%s) of %d bytes is more than the datanode\u0027s available\" +\n             \" RLIMIT_MEMLOCK ulimit of %d bytes.\",\n             DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,\n             dnConf.maxLockedMemory,\n             ulimit));\n         }\n       }\n     }\n     LOG.info(\"Starting DataNode with maxLockedMemory \u003d {}\",\n         dnConf.maxLockedMemory);\n \n     int volFailuresTolerated \u003d dnConf.getVolFailuresTolerated();\n     int volsConfigured \u003d dnConf.getVolsConfigured();\n     if (volFailuresTolerated \u003c MAX_VOLUME_FAILURE_TOLERATED_LIMIT\n         || volFailuresTolerated \u003e\u003d volsConfigured) {\n-      throw new DiskErrorException(\"Invalid value configured for \"\n+      throw new HadoopIllegalArgumentException(\"Invalid value configured for \"\n           + \"dfs.datanode.failed.volumes.tolerated - \" + volFailuresTolerated\n           + \". Value configured is either less than -1 or \u003e\u003d \"\n           + \"to the number of configured volumes (\" + volsConfigured + \").\");\n     }\n \n     storage \u003d new DataStorage();\n     \n     // global DN settings\n     registerMXBean();\n     initDataXceiver();\n     startInfoServer();\n     pauseMonitor \u003d new JvmPauseMonitor();\n     pauseMonitor.init(getConf());\n     pauseMonitor.start();\n   \n     // BlockPoolTokenSecretManager is required to create ipc server.\n     this.blockPoolTokenSecretManager \u003d new BlockPoolTokenSecretManager();\n \n     // Login is done by now. Set the DN user name.\n     dnUserName \u003d UserGroupInformation.getCurrentUser().getUserName();\n     LOG.info(\"dnUserName \u003d {}\", dnUserName);\n     LOG.info(\"supergroup \u003d {}\", supergroup);\n     initIpcServer();\n \n     metrics \u003d DataNodeMetrics.create(getConf(), getDisplayName());\n     peerMetrics \u003d dnConf.peerStatsEnabled ?\n         DataNodePeerMetrics.create(getDisplayName()) : null;\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n \n     ecWorker \u003d new ErasureCodingWorker(getConf(), this);\n     blockRecoveryWorker \u003d new BlockRecoveryWorker(this);\n \n     blockPoolManager \u003d new BlockPoolManager(this);\n     blockPoolManager.refreshNamenodes(getConf());\n \n     // Create the ReadaheadPool from the DataNode context so we can\n     // exit without having to explicitly shutdown its thread pool.\n     readaheadPool \u003d ReadaheadPool.getInstance();\n     saslClient \u003d new SaslDataTransferClient(dnConf.getConf(),\n         dnConf.saslPropsResolver, dnConf.trustedChannelResolver);\n     saslServer \u003d new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);\n     startMetricsLogger();\n \n     if (dnConf.diskStatsEnabled) {\n       diskMetrics \u003d new DataNodeDiskMetrics(this,\n           dnConf.outliersReportIntervalMs);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void startDataNode(List\u003cStorageLocation\u003e dataDirectories,\n                     SecureResources resources\n                     ) throws IOException {\n\n    // settings global for all BPs in the Data Node\n    this.secureResources \u003d resources;\n    synchronized (this) {\n      this.dataDirs \u003d dataDirectories;\n    }\n    this.dnConf \u003d new DNConf(this);\n    checkSecureConfig(dnConf, getConf(), resources);\n\n    if (dnConf.maxLockedMemory \u003e 0) {\n      if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {\n        throw new RuntimeException(String.format(\n            \"Cannot start datanode because the configured max locked memory\" +\n            \" size (%s) is greater than zero and native code is not available.\",\n            DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));\n      }\n      if (Path.WINDOWS) {\n        NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);\n      } else {\n        long ulimit \u003d NativeIO.POSIX.getCacheManipulator().getMemlockLimit();\n        if (dnConf.maxLockedMemory \u003e ulimit) {\n          throw new RuntimeException(String.format(\n            \"Cannot start datanode because the configured max locked memory\" +\n            \" size (%s) of %d bytes is more than the datanode\u0027s available\" +\n            \" RLIMIT_MEMLOCK ulimit of %d bytes.\",\n            DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,\n            dnConf.maxLockedMemory,\n            ulimit));\n        }\n      }\n    }\n    LOG.info(\"Starting DataNode with maxLockedMemory \u003d {}\",\n        dnConf.maxLockedMemory);\n\n    int volFailuresTolerated \u003d dnConf.getVolFailuresTolerated();\n    int volsConfigured \u003d dnConf.getVolsConfigured();\n    if (volFailuresTolerated \u003c MAX_VOLUME_FAILURE_TOLERATED_LIMIT\n        || volFailuresTolerated \u003e\u003d volsConfigured) {\n      throw new HadoopIllegalArgumentException(\"Invalid value configured for \"\n          + \"dfs.datanode.failed.volumes.tolerated - \" + volFailuresTolerated\n          + \". Value configured is either less than -1 or \u003e\u003d \"\n          + \"to the number of configured volumes (\" + volsConfigured + \").\");\n    }\n\n    storage \u003d new DataStorage();\n    \n    // global DN settings\n    registerMXBean();\n    initDataXceiver();\n    startInfoServer();\n    pauseMonitor \u003d new JvmPauseMonitor();\n    pauseMonitor.init(getConf());\n    pauseMonitor.start();\n  \n    // BlockPoolTokenSecretManager is required to create ipc server.\n    this.blockPoolTokenSecretManager \u003d new BlockPoolTokenSecretManager();\n\n    // Login is done by now. Set the DN user name.\n    dnUserName \u003d UserGroupInformation.getCurrentUser().getUserName();\n    LOG.info(\"dnUserName \u003d {}\", dnUserName);\n    LOG.info(\"supergroup \u003d {}\", supergroup);\n    initIpcServer();\n\n    metrics \u003d DataNodeMetrics.create(getConf(), getDisplayName());\n    peerMetrics \u003d dnConf.peerStatsEnabled ?\n        DataNodePeerMetrics.create(getDisplayName()) : null;\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    ecWorker \u003d new ErasureCodingWorker(getConf(), this);\n    blockRecoveryWorker \u003d new BlockRecoveryWorker(this);\n\n    blockPoolManager \u003d new BlockPoolManager(this);\n    blockPoolManager.refreshNamenodes(getConf());\n\n    // Create the ReadaheadPool from the DataNode context so we can\n    // exit without having to explicitly shutdown its thread pool.\n    readaheadPool \u003d ReadaheadPool.getInstance();\n    saslClient \u003d new SaslDataTransferClient(dnConf.getConf(),\n        dnConf.saslPropsResolver, dnConf.trustedChannelResolver);\n    saslServer \u003d new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);\n    startMetricsLogger();\n\n    if (dnConf.diskStatsEnabled) {\n      diskMetrics \u003d new DataNodeDiskMetrics(this,\n          dnConf.outliersReportIntervalMs);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {}
    },
    "68d5dfdc78d121e89eeae4e577d670028a14a955": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14056. Fix error messages in HDFS-12716. Contributed by Ayush Saxena.\n",
      "commitDate": "16/11/18 4:35 AM",
      "commitName": "68d5dfdc78d121e89eeae4e577d670028a14a955",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "30/10/18 10:43 PM",
      "commitNameOld": "fac9f91b2944cee641049fffcafa6b65e0cf68f2",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 16.29,
      "commitsBetweenForRepo": 118,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,90 @@\n   void startDataNode(List\u003cStorageLocation\u003e dataDirectories,\n                      SecureResources resources\n                      ) throws IOException {\n \n     // settings global for all BPs in the Data Node\n     this.secureResources \u003d resources;\n     synchronized (this) {\n       this.dataDirs \u003d dataDirectories;\n     }\n     this.dnConf \u003d new DNConf(this);\n     checkSecureConfig(dnConf, getConf(), resources);\n \n     if (dnConf.maxLockedMemory \u003e 0) {\n       if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {\n         throw new RuntimeException(String.format(\n             \"Cannot start datanode because the configured max locked memory\" +\n             \" size (%s) is greater than zero and native code is not available.\",\n             DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));\n       }\n       if (Path.WINDOWS) {\n         NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);\n       } else {\n         long ulimit \u003d NativeIO.POSIX.getCacheManipulator().getMemlockLimit();\n         if (dnConf.maxLockedMemory \u003e ulimit) {\n           throw new RuntimeException(String.format(\n             \"Cannot start datanode because the configured max locked memory\" +\n             \" size (%s) of %d bytes is more than the datanode\u0027s available\" +\n             \" RLIMIT_MEMLOCK ulimit of %d bytes.\",\n             DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,\n             dnConf.maxLockedMemory,\n             ulimit));\n         }\n       }\n     }\n     LOG.info(\"Starting DataNode with maxLockedMemory \u003d {}\",\n         dnConf.maxLockedMemory);\n \n     int volFailuresTolerated \u003d dnConf.getVolFailuresTolerated();\n     int volsConfigured \u003d dnConf.getVolsConfigured();\n     if (volFailuresTolerated \u003c MAX_VOLUME_FAILURE_TOLERATED_LIMIT\n         || volFailuresTolerated \u003e\u003d volsConfigured) {\n       throw new DiskErrorException(\"Invalid value configured for \"\n           + \"dfs.datanode.failed.volumes.tolerated - \" + volFailuresTolerated\n-          + \". Value configured is either greater than -1 or \u003e\u003d \"\n+          + \". Value configured is either less than -1 or \u003e\u003d \"\n           + \"to the number of configured volumes (\" + volsConfigured + \").\");\n     }\n \n     storage \u003d new DataStorage();\n     \n     // global DN settings\n     registerMXBean();\n     initDataXceiver();\n     startInfoServer();\n     pauseMonitor \u003d new JvmPauseMonitor();\n     pauseMonitor.init(getConf());\n     pauseMonitor.start();\n   \n     // BlockPoolTokenSecretManager is required to create ipc server.\n     this.blockPoolTokenSecretManager \u003d new BlockPoolTokenSecretManager();\n \n     // Login is done by now. Set the DN user name.\n     dnUserName \u003d UserGroupInformation.getCurrentUser().getUserName();\n     LOG.info(\"dnUserName \u003d {}\", dnUserName);\n     LOG.info(\"supergroup \u003d {}\", supergroup);\n     initIpcServer();\n \n     metrics \u003d DataNodeMetrics.create(getConf(), getDisplayName());\n     peerMetrics \u003d dnConf.peerStatsEnabled ?\n         DataNodePeerMetrics.create(getDisplayName()) : null;\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n \n     ecWorker \u003d new ErasureCodingWorker(getConf(), this);\n     blockRecoveryWorker \u003d new BlockRecoveryWorker(this);\n \n     blockPoolManager \u003d new BlockPoolManager(this);\n     blockPoolManager.refreshNamenodes(getConf());\n \n     // Create the ReadaheadPool from the DataNode context so we can\n     // exit without having to explicitly shutdown its thread pool.\n     readaheadPool \u003d ReadaheadPool.getInstance();\n     saslClient \u003d new SaslDataTransferClient(dnConf.getConf(),\n         dnConf.saslPropsResolver, dnConf.trustedChannelResolver);\n     saslServer \u003d new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);\n     startMetricsLogger();\n \n     if (dnConf.diskStatsEnabled) {\n       diskMetrics \u003d new DataNodeDiskMetrics(this,\n           dnConf.outliersReportIntervalMs);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void startDataNode(List\u003cStorageLocation\u003e dataDirectories,\n                     SecureResources resources\n                     ) throws IOException {\n\n    // settings global for all BPs in the Data Node\n    this.secureResources \u003d resources;\n    synchronized (this) {\n      this.dataDirs \u003d dataDirectories;\n    }\n    this.dnConf \u003d new DNConf(this);\n    checkSecureConfig(dnConf, getConf(), resources);\n\n    if (dnConf.maxLockedMemory \u003e 0) {\n      if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {\n        throw new RuntimeException(String.format(\n            \"Cannot start datanode because the configured max locked memory\" +\n            \" size (%s) is greater than zero and native code is not available.\",\n            DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));\n      }\n      if (Path.WINDOWS) {\n        NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);\n      } else {\n        long ulimit \u003d NativeIO.POSIX.getCacheManipulator().getMemlockLimit();\n        if (dnConf.maxLockedMemory \u003e ulimit) {\n          throw new RuntimeException(String.format(\n            \"Cannot start datanode because the configured max locked memory\" +\n            \" size (%s) of %d bytes is more than the datanode\u0027s available\" +\n            \" RLIMIT_MEMLOCK ulimit of %d bytes.\",\n            DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,\n            dnConf.maxLockedMemory,\n            ulimit));\n        }\n      }\n    }\n    LOG.info(\"Starting DataNode with maxLockedMemory \u003d {}\",\n        dnConf.maxLockedMemory);\n\n    int volFailuresTolerated \u003d dnConf.getVolFailuresTolerated();\n    int volsConfigured \u003d dnConf.getVolsConfigured();\n    if (volFailuresTolerated \u003c MAX_VOLUME_FAILURE_TOLERATED_LIMIT\n        || volFailuresTolerated \u003e\u003d volsConfigured) {\n      throw new DiskErrorException(\"Invalid value configured for \"\n          + \"dfs.datanode.failed.volumes.tolerated - \" + volFailuresTolerated\n          + \". Value configured is either less than -1 or \u003e\u003d \"\n          + \"to the number of configured volumes (\" + volsConfigured + \").\");\n    }\n\n    storage \u003d new DataStorage();\n    \n    // global DN settings\n    registerMXBean();\n    initDataXceiver();\n    startInfoServer();\n    pauseMonitor \u003d new JvmPauseMonitor();\n    pauseMonitor.init(getConf());\n    pauseMonitor.start();\n  \n    // BlockPoolTokenSecretManager is required to create ipc server.\n    this.blockPoolTokenSecretManager \u003d new BlockPoolTokenSecretManager();\n\n    // Login is done by now. Set the DN user name.\n    dnUserName \u003d UserGroupInformation.getCurrentUser().getUserName();\n    LOG.info(\"dnUserName \u003d {}\", dnUserName);\n    LOG.info(\"supergroup \u003d {}\", supergroup);\n    initIpcServer();\n\n    metrics \u003d DataNodeMetrics.create(getConf(), getDisplayName());\n    peerMetrics \u003d dnConf.peerStatsEnabled ?\n        DataNodePeerMetrics.create(getDisplayName()) : null;\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    ecWorker \u003d new ErasureCodingWorker(getConf(), this);\n    blockRecoveryWorker \u003d new BlockRecoveryWorker(this);\n\n    blockPoolManager \u003d new BlockPoolManager(this);\n    blockPoolManager.refreshNamenodes(getConf());\n\n    // Create the ReadaheadPool from the DataNode context so we can\n    // exit without having to explicitly shutdown its thread pool.\n    readaheadPool \u003d ReadaheadPool.getInstance();\n    saslClient \u003d new SaslDataTransferClient(dnConf.getConf(),\n        dnConf.saslPropsResolver, dnConf.trustedChannelResolver);\n    saslServer \u003d new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);\n    startMetricsLogger();\n\n    if (dnConf.diskStatsEnabled) {\n      diskMetrics \u003d new DataNodeDiskMetrics(this,\n          dnConf.outliersReportIntervalMs);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {}
    },
    "39ed3a66dbb01383ed16b141183fc48bfd2e613d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13076: [SPS]: Cleanup work for HDFS-10285 merge. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "39ed3a66dbb01383ed16b141183fc48bfd2e613d",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "dfcb331ba3516264398121c9f23af3a79c0509cc",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,93 +1,90 @@\n   void startDataNode(List\u003cStorageLocation\u003e dataDirectories,\n                      SecureResources resources\n                      ) throws IOException {\n \n     // settings global for all BPs in the Data Node\n     this.secureResources \u003d resources;\n     synchronized (this) {\n       this.dataDirs \u003d dataDirectories;\n     }\n     this.dnConf \u003d new DNConf(this);\n     checkSecureConfig(dnConf, getConf(), resources);\n \n     if (dnConf.maxLockedMemory \u003e 0) {\n       if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {\n         throw new RuntimeException(String.format(\n             \"Cannot start datanode because the configured max locked memory\" +\n             \" size (%s) is greater than zero and native code is not available.\",\n             DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));\n       }\n       if (Path.WINDOWS) {\n         NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);\n       } else {\n         long ulimit \u003d NativeIO.POSIX.getCacheManipulator().getMemlockLimit();\n         if (dnConf.maxLockedMemory \u003e ulimit) {\n           throw new RuntimeException(String.format(\n             \"Cannot start datanode because the configured max locked memory\" +\n             \" size (%s) of %d bytes is more than the datanode\u0027s available\" +\n             \" RLIMIT_MEMLOCK ulimit of %d bytes.\",\n             DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,\n             dnConf.maxLockedMemory,\n             ulimit));\n         }\n       }\n     }\n     LOG.info(\"Starting DataNode with maxLockedMemory \u003d {}\",\n         dnConf.maxLockedMemory);\n \n     int volFailuresTolerated \u003d dnConf.getVolFailuresTolerated();\n     int volsConfigured \u003d dnConf.getVolsConfigured();\n     if (volFailuresTolerated \u003c MAX_VOLUME_FAILURE_TOLERATED_LIMIT\n         || volFailuresTolerated \u003e\u003d volsConfigured) {\n       throw new DiskErrorException(\"Invalid value configured for \"\n           + \"dfs.datanode.failed.volumes.tolerated - \" + volFailuresTolerated\n           + \". Value configured is either greater than -1 or \u003e\u003d \"\n           + \"to the number of configured volumes (\" + volsConfigured + \").\");\n     }\n \n     storage \u003d new DataStorage();\n     \n     // global DN settings\n     registerMXBean();\n     initDataXceiver();\n     startInfoServer();\n     pauseMonitor \u003d new JvmPauseMonitor();\n     pauseMonitor.init(getConf());\n     pauseMonitor.start();\n   \n     // BlockPoolTokenSecretManager is required to create ipc server.\n     this.blockPoolTokenSecretManager \u003d new BlockPoolTokenSecretManager();\n \n     // Login is done by now. Set the DN user name.\n     dnUserName \u003d UserGroupInformation.getCurrentUser().getUserName();\n     LOG.info(\"dnUserName \u003d {}\", dnUserName);\n     LOG.info(\"supergroup \u003d {}\", supergroup);\n     initIpcServer();\n \n     metrics \u003d DataNodeMetrics.create(getConf(), getDisplayName());\n     peerMetrics \u003d dnConf.peerStatsEnabled ?\n         DataNodePeerMetrics.create(getDisplayName()) : null;\n     metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n \n     ecWorker \u003d new ErasureCodingWorker(getConf(), this);\n     blockRecoveryWorker \u003d new BlockRecoveryWorker(this);\n-    storagePolicySatisfyWorker \u003d\n-        new StoragePolicySatisfyWorker(getConf(), this, null);\n-    storagePolicySatisfyWorker.start();\n \n     blockPoolManager \u003d new BlockPoolManager(this);\n     blockPoolManager.refreshNamenodes(getConf());\n \n     // Create the ReadaheadPool from the DataNode context so we can\n     // exit without having to explicitly shutdown its thread pool.\n     readaheadPool \u003d ReadaheadPool.getInstance();\n     saslClient \u003d new SaslDataTransferClient(dnConf.getConf(),\n         dnConf.saslPropsResolver, dnConf.trustedChannelResolver);\n     saslServer \u003d new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);\n     startMetricsLogger();\n \n     if (dnConf.diskStatsEnabled) {\n       diskMetrics \u003d new DataNodeDiskMetrics(this,\n           dnConf.outliersReportIntervalMs);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void startDataNode(List\u003cStorageLocation\u003e dataDirectories,\n                     SecureResources resources\n                     ) throws IOException {\n\n    // settings global for all BPs in the Data Node\n    this.secureResources \u003d resources;\n    synchronized (this) {\n      this.dataDirs \u003d dataDirectories;\n    }\n    this.dnConf \u003d new DNConf(this);\n    checkSecureConfig(dnConf, getConf(), resources);\n\n    if (dnConf.maxLockedMemory \u003e 0) {\n      if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {\n        throw new RuntimeException(String.format(\n            \"Cannot start datanode because the configured max locked memory\" +\n            \" size (%s) is greater than zero and native code is not available.\",\n            DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));\n      }\n      if (Path.WINDOWS) {\n        NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);\n      } else {\n        long ulimit \u003d NativeIO.POSIX.getCacheManipulator().getMemlockLimit();\n        if (dnConf.maxLockedMemory \u003e ulimit) {\n          throw new RuntimeException(String.format(\n            \"Cannot start datanode because the configured max locked memory\" +\n            \" size (%s) of %d bytes is more than the datanode\u0027s available\" +\n            \" RLIMIT_MEMLOCK ulimit of %d bytes.\",\n            DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,\n            dnConf.maxLockedMemory,\n            ulimit));\n        }\n      }\n    }\n    LOG.info(\"Starting DataNode with maxLockedMemory \u003d {}\",\n        dnConf.maxLockedMemory);\n\n    int volFailuresTolerated \u003d dnConf.getVolFailuresTolerated();\n    int volsConfigured \u003d dnConf.getVolsConfigured();\n    if (volFailuresTolerated \u003c MAX_VOLUME_FAILURE_TOLERATED_LIMIT\n        || volFailuresTolerated \u003e\u003d volsConfigured) {\n      throw new DiskErrorException(\"Invalid value configured for \"\n          + \"dfs.datanode.failed.volumes.tolerated - \" + volFailuresTolerated\n          + \". Value configured is either greater than -1 or \u003e\u003d \"\n          + \"to the number of configured volumes (\" + volsConfigured + \").\");\n    }\n\n    storage \u003d new DataStorage();\n    \n    // global DN settings\n    registerMXBean();\n    initDataXceiver();\n    startInfoServer();\n    pauseMonitor \u003d new JvmPauseMonitor();\n    pauseMonitor.init(getConf());\n    pauseMonitor.start();\n  \n    // BlockPoolTokenSecretManager is required to create ipc server.\n    this.blockPoolTokenSecretManager \u003d new BlockPoolTokenSecretManager();\n\n    // Login is done by now. Set the DN user name.\n    dnUserName \u003d UserGroupInformation.getCurrentUser().getUserName();\n    LOG.info(\"dnUserName \u003d {}\", dnUserName);\n    LOG.info(\"supergroup \u003d {}\", supergroup);\n    initIpcServer();\n\n    metrics \u003d DataNodeMetrics.create(getConf(), getDisplayName());\n    peerMetrics \u003d dnConf.peerStatsEnabled ?\n        DataNodePeerMetrics.create(getDisplayName()) : null;\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    ecWorker \u003d new ErasureCodingWorker(getConf(), this);\n    blockRecoveryWorker \u003d new BlockRecoveryWorker(this);\n\n    blockPoolManager \u003d new BlockPoolManager(this);\n    blockPoolManager.refreshNamenodes(getConf());\n\n    // Create the ReadaheadPool from the DataNode context so we can\n    // exit without having to explicitly shutdown its thread pool.\n    readaheadPool \u003d ReadaheadPool.getInstance();\n    saslClient \u003d new SaslDataTransferClient(dnConf.getConf(),\n        dnConf.saslPropsResolver, dnConf.trustedChannelResolver);\n    saslServer \u003d new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);\n    startMetricsLogger();\n\n    if (dnConf.diskStatsEnabled) {\n      diskMetrics \u003d new DataNodeDiskMetrics(this,\n          dnConf.outliersReportIntervalMs);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {}
    },
    "dfcb331ba3516264398121c9f23af3a79c0509cc": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13076: [SPS]: Addendum. Resolve conflicts after rebasing branch to trunk. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "dfcb331ba3516264398121c9f23af3a79c0509cc",
      "commitAuthor": "Rakesh Radhakrishnan",
      "diff": "@@ -0,0 +1,93 @@\n+  void startDataNode(List\u003cStorageLocation\u003e dataDirectories,\n+                     SecureResources resources\n+                     ) throws IOException {\n+\n+    // settings global for all BPs in the Data Node\n+    this.secureResources \u003d resources;\n+    synchronized (this) {\n+      this.dataDirs \u003d dataDirectories;\n+    }\n+    this.dnConf \u003d new DNConf(this);\n+    checkSecureConfig(dnConf, getConf(), resources);\n+\n+    if (dnConf.maxLockedMemory \u003e 0) {\n+      if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {\n+        throw new RuntimeException(String.format(\n+            \"Cannot start datanode because the configured max locked memory\" +\n+            \" size (%s) is greater than zero and native code is not available.\",\n+            DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));\n+      }\n+      if (Path.WINDOWS) {\n+        NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);\n+      } else {\n+        long ulimit \u003d NativeIO.POSIX.getCacheManipulator().getMemlockLimit();\n+        if (dnConf.maxLockedMemory \u003e ulimit) {\n+          throw new RuntimeException(String.format(\n+            \"Cannot start datanode because the configured max locked memory\" +\n+            \" size (%s) of %d bytes is more than the datanode\u0027s available\" +\n+            \" RLIMIT_MEMLOCK ulimit of %d bytes.\",\n+            DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,\n+            dnConf.maxLockedMemory,\n+            ulimit));\n+        }\n+      }\n+    }\n+    LOG.info(\"Starting DataNode with maxLockedMemory \u003d {}\",\n+        dnConf.maxLockedMemory);\n+\n+    int volFailuresTolerated \u003d dnConf.getVolFailuresTolerated();\n+    int volsConfigured \u003d dnConf.getVolsConfigured();\n+    if (volFailuresTolerated \u003c MAX_VOLUME_FAILURE_TOLERATED_LIMIT\n+        || volFailuresTolerated \u003e\u003d volsConfigured) {\n+      throw new DiskErrorException(\"Invalid value configured for \"\n+          + \"dfs.datanode.failed.volumes.tolerated - \" + volFailuresTolerated\n+          + \". Value configured is either greater than -1 or \u003e\u003d \"\n+          + \"to the number of configured volumes (\" + volsConfigured + \").\");\n+    }\n+\n+    storage \u003d new DataStorage();\n+    \n+    // global DN settings\n+    registerMXBean();\n+    initDataXceiver();\n+    startInfoServer();\n+    pauseMonitor \u003d new JvmPauseMonitor();\n+    pauseMonitor.init(getConf());\n+    pauseMonitor.start();\n+  \n+    // BlockPoolTokenSecretManager is required to create ipc server.\n+    this.blockPoolTokenSecretManager \u003d new BlockPoolTokenSecretManager();\n+\n+    // Login is done by now. Set the DN user name.\n+    dnUserName \u003d UserGroupInformation.getCurrentUser().getUserName();\n+    LOG.info(\"dnUserName \u003d {}\", dnUserName);\n+    LOG.info(\"supergroup \u003d {}\", supergroup);\n+    initIpcServer();\n+\n+    metrics \u003d DataNodeMetrics.create(getConf(), getDisplayName());\n+    peerMetrics \u003d dnConf.peerStatsEnabled ?\n+        DataNodePeerMetrics.create(getDisplayName()) : null;\n+    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n+\n+    ecWorker \u003d new ErasureCodingWorker(getConf(), this);\n+    blockRecoveryWorker \u003d new BlockRecoveryWorker(this);\n+    storagePolicySatisfyWorker \u003d\n+        new StoragePolicySatisfyWorker(getConf(), this, null);\n+    storagePolicySatisfyWorker.start();\n+\n+    blockPoolManager \u003d new BlockPoolManager(this);\n+    blockPoolManager.refreshNamenodes(getConf());\n+\n+    // Create the ReadaheadPool from the DataNode context so we can\n+    // exit without having to explicitly shutdown its thread pool.\n+    readaheadPool \u003d ReadaheadPool.getInstance();\n+    saslClient \u003d new SaslDataTransferClient(dnConf.getConf(),\n+        dnConf.saslPropsResolver, dnConf.trustedChannelResolver);\n+    saslServer \u003d new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);\n+    startMetricsLogger();\n+\n+    if (dnConf.diskStatsEnabled) {\n+      diskMetrics \u003d new DataNodeDiskMetrics(this,\n+          dnConf.outliersReportIntervalMs);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void startDataNode(List\u003cStorageLocation\u003e dataDirectories,\n                     SecureResources resources\n                     ) throws IOException {\n\n    // settings global for all BPs in the Data Node\n    this.secureResources \u003d resources;\n    synchronized (this) {\n      this.dataDirs \u003d dataDirectories;\n    }\n    this.dnConf \u003d new DNConf(this);\n    checkSecureConfig(dnConf, getConf(), resources);\n\n    if (dnConf.maxLockedMemory \u003e 0) {\n      if (!NativeIO.POSIX.getCacheManipulator().verifyCanMlock()) {\n        throw new RuntimeException(String.format(\n            \"Cannot start datanode because the configured max locked memory\" +\n            \" size (%s) is greater than zero and native code is not available.\",\n            DFS_DATANODE_MAX_LOCKED_MEMORY_KEY));\n      }\n      if (Path.WINDOWS) {\n        NativeIO.Windows.extendWorkingSetSize(dnConf.maxLockedMemory);\n      } else {\n        long ulimit \u003d NativeIO.POSIX.getCacheManipulator().getMemlockLimit();\n        if (dnConf.maxLockedMemory \u003e ulimit) {\n          throw new RuntimeException(String.format(\n            \"Cannot start datanode because the configured max locked memory\" +\n            \" size (%s) of %d bytes is more than the datanode\u0027s available\" +\n            \" RLIMIT_MEMLOCK ulimit of %d bytes.\",\n            DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,\n            dnConf.maxLockedMemory,\n            ulimit));\n        }\n      }\n    }\n    LOG.info(\"Starting DataNode with maxLockedMemory \u003d {}\",\n        dnConf.maxLockedMemory);\n\n    int volFailuresTolerated \u003d dnConf.getVolFailuresTolerated();\n    int volsConfigured \u003d dnConf.getVolsConfigured();\n    if (volFailuresTolerated \u003c MAX_VOLUME_FAILURE_TOLERATED_LIMIT\n        || volFailuresTolerated \u003e\u003d volsConfigured) {\n      throw new DiskErrorException(\"Invalid value configured for \"\n          + \"dfs.datanode.failed.volumes.tolerated - \" + volFailuresTolerated\n          + \". Value configured is either greater than -1 or \u003e\u003d \"\n          + \"to the number of configured volumes (\" + volsConfigured + \").\");\n    }\n\n    storage \u003d new DataStorage();\n    \n    // global DN settings\n    registerMXBean();\n    initDataXceiver();\n    startInfoServer();\n    pauseMonitor \u003d new JvmPauseMonitor();\n    pauseMonitor.init(getConf());\n    pauseMonitor.start();\n  \n    // BlockPoolTokenSecretManager is required to create ipc server.\n    this.blockPoolTokenSecretManager \u003d new BlockPoolTokenSecretManager();\n\n    // Login is done by now. Set the DN user name.\n    dnUserName \u003d UserGroupInformation.getCurrentUser().getUserName();\n    LOG.info(\"dnUserName \u003d {}\", dnUserName);\n    LOG.info(\"supergroup \u003d {}\", supergroup);\n    initIpcServer();\n\n    metrics \u003d DataNodeMetrics.create(getConf(), getDisplayName());\n    peerMetrics \u003d dnConf.peerStatsEnabled ?\n        DataNodePeerMetrics.create(getDisplayName()) : null;\n    metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);\n\n    ecWorker \u003d new ErasureCodingWorker(getConf(), this);\n    blockRecoveryWorker \u003d new BlockRecoveryWorker(this);\n    storagePolicySatisfyWorker \u003d\n        new StoragePolicySatisfyWorker(getConf(), this, null);\n    storagePolicySatisfyWorker.start();\n\n    blockPoolManager \u003d new BlockPoolManager(this);\n    blockPoolManager.refreshNamenodes(getConf());\n\n    // Create the ReadaheadPool from the DataNode context so we can\n    // exit without having to explicitly shutdown its thread pool.\n    readaheadPool \u003d ReadaheadPool.getInstance();\n    saslClient \u003d new SaslDataTransferClient(dnConf.getConf(),\n        dnConf.saslPropsResolver, dnConf.trustedChannelResolver);\n    saslServer \u003d new SaslDataTransferServer(dnConf, blockPoolTokenSecretManager);\n    startMetricsLogger();\n\n    if (dnConf.diskStatsEnabled) {\n      diskMetrics \u003d new DataNodeDiskMetrics(this,\n          dnConf.outliersReportIntervalMs);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
    }
  }
}