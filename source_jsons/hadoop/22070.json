{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "EventFetcher.java",
  "functionName": "getMapCompletionEvents",
  "functionId": "getMapCompletionEvents",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java",
  "functionStartLine": 113,
  "functionEndLine": 150,
  "numCommitsSeen": 8,
  "timeTaken": 9120,
  "changeHistory": [
    "1a389305b27ac1efec4d7923b87de3e703bf70e1",
    "895029b2f2535f1ba8275be29fda16d0f80be790",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "1a389305b27ac1efec4d7923b87de3e703bf70e1": "Ymultichange(Yexceptionschange,Ybodychange)",
    "895029b2f2535f1ba8275be29fda16d0f80be790": "Ymultichange(Ymodifierchange,Ybodychange)",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymovefromfile",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "1a389305b27ac1efec4d7923b87de3e703bf70e1": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "MAPREDUCE-5192. Allow for alternate resolutions of TaskCompletionEvents. Contributed by Chris Douglas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1493445 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/06/13 8:07 PM",
      "commitName": "1a389305b27ac1efec4d7923b87de3e703bf70e1",
      "commitAuthor": "Arun Murthy",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "MAPREDUCE-5192. Allow for alternate resolutions of TaskCompletionEvents. Contributed by Chris Douglas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1493445 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "15/06/13 8:07 PM",
          "commitName": "1a389305b27ac1efec4d7923b87de3e703bf70e1",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "24/10/12 6:26 PM",
          "commitNameOld": "895029b2f2535f1ba8275be29fda16d0f80be790",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 234.07,
          "commitsBetweenForRepo": 1306,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,38 @@\n-  protected int getMapCompletionEvents() throws IOException {\n+  protected int getMapCompletionEvents()\n+      throws IOException, InterruptedException {\n     \n     int numNewMaps \u003d 0;\n     TaskCompletionEvent events[] \u003d null;\n \n     do {\n       MapTaskCompletionEventsUpdate update \u003d\n           umbilical.getMapCompletionEvents(\n               (org.apache.hadoop.mapred.JobID)reduce.getJobID(),\n               fromEventIdx,\n               maxEventsToFetch,\n               (org.apache.hadoop.mapred.TaskAttemptID)reduce);\n       events \u003d update.getMapTaskCompletionEvents();\n       LOG.debug(\"Got \" + events.length + \" map completion events from \" +\n                fromEventIdx);\n \n-      // Check if the reset is required.\n-      // Since there is no ordering of the task completion events at the\n-      // reducer, the only option to sync with the new jobtracker is to reset\n-      // the events index\n-      if (update.shouldReset()) {\n-        fromEventIdx \u003d 0;\n-        scheduler.resetKnownMaps();\n-      }\n+      assert !update.shouldReset() : \"Unexpected legacy state\";\n \n       // Update the last seen event ID\n       fromEventIdx +\u003d events.length;\n \n       // Process the TaskCompletionEvents:\n       // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n       // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop\n       //    fetching from those maps.\n       // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n       //    outputs at all.\n       for (TaskCompletionEvent event : events) {\n-        switch (event.getTaskStatus()) {\n-        case SUCCEEDED:\n-          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n-          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-              u.toString(),\n-              event.getTaskAttemptId());\n-          numNewMaps ++;\n-          int duration \u003d event.getTaskRunTime();\n-          if (duration \u003e maxMapRuntime) {\n-            maxMapRuntime \u003d duration;\n-            scheduler.informMaxMapRunTime(maxMapRuntime);\n-          }\n-          break;\n-        case FAILED:\n-        case KILLED:\n-        case OBSOLETE:\n-          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n-          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-              \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n-          break;\n-        case TIPFAILED:\n-          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n-          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-              event.getTaskAttemptId() + \"\u0027\");\n-          break;\n+        scheduler.resolve(event);\n+        if (TaskCompletionEvent.Status.SUCCEEDED \u003d\u003d event.getTaskStatus()) {\n+          ++numNewMaps;\n         }\n       }\n     } while (events.length \u003d\u003d maxEventsToFetch);\n \n     return numNewMaps;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected int getMapCompletionEvents()\n      throws IOException, InterruptedException {\n    \n    int numNewMaps \u003d 0;\n    TaskCompletionEvent events[] \u003d null;\n\n    do {\n      MapTaskCompletionEventsUpdate update \u003d\n          umbilical.getMapCompletionEvents(\n              (org.apache.hadoop.mapred.JobID)reduce.getJobID(),\n              fromEventIdx,\n              maxEventsToFetch,\n              (org.apache.hadoop.mapred.TaskAttemptID)reduce);\n      events \u003d update.getMapTaskCompletionEvents();\n      LOG.debug(\"Got \" + events.length + \" map completion events from \" +\n               fromEventIdx);\n\n      assert !update.shouldReset() : \"Unexpected legacy state\";\n\n      // Update the last seen event ID\n      fromEventIdx +\u003d events.length;\n\n      // Process the TaskCompletionEvents:\n      // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n      // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop\n      //    fetching from those maps.\n      // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n      //    outputs at all.\n      for (TaskCompletionEvent event : events) {\n        scheduler.resolve(event);\n        if (TaskCompletionEvent.Status.SUCCEEDED \u003d\u003d event.getTaskStatus()) {\n          ++numNewMaps;\n        }\n      }\n    } while (events.length \u003d\u003d maxEventsToFetch);\n\n    return numNewMaps;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java",
          "extendedDetails": {
            "oldValue": "[IOException]",
            "newValue": "[IOException, InterruptedException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-5192. Allow for alternate resolutions of TaskCompletionEvents. Contributed by Chris Douglas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1493445 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "15/06/13 8:07 PM",
          "commitName": "1a389305b27ac1efec4d7923b87de3e703bf70e1",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "24/10/12 6:26 PM",
          "commitNameOld": "895029b2f2535f1ba8275be29fda16d0f80be790",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 234.07,
          "commitsBetweenForRepo": 1306,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,38 @@\n-  protected int getMapCompletionEvents() throws IOException {\n+  protected int getMapCompletionEvents()\n+      throws IOException, InterruptedException {\n     \n     int numNewMaps \u003d 0;\n     TaskCompletionEvent events[] \u003d null;\n \n     do {\n       MapTaskCompletionEventsUpdate update \u003d\n           umbilical.getMapCompletionEvents(\n               (org.apache.hadoop.mapred.JobID)reduce.getJobID(),\n               fromEventIdx,\n               maxEventsToFetch,\n               (org.apache.hadoop.mapred.TaskAttemptID)reduce);\n       events \u003d update.getMapTaskCompletionEvents();\n       LOG.debug(\"Got \" + events.length + \" map completion events from \" +\n                fromEventIdx);\n \n-      // Check if the reset is required.\n-      // Since there is no ordering of the task completion events at the\n-      // reducer, the only option to sync with the new jobtracker is to reset\n-      // the events index\n-      if (update.shouldReset()) {\n-        fromEventIdx \u003d 0;\n-        scheduler.resetKnownMaps();\n-      }\n+      assert !update.shouldReset() : \"Unexpected legacy state\";\n \n       // Update the last seen event ID\n       fromEventIdx +\u003d events.length;\n \n       // Process the TaskCompletionEvents:\n       // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n       // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop\n       //    fetching from those maps.\n       // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n       //    outputs at all.\n       for (TaskCompletionEvent event : events) {\n-        switch (event.getTaskStatus()) {\n-        case SUCCEEDED:\n-          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n-          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-              u.toString(),\n-              event.getTaskAttemptId());\n-          numNewMaps ++;\n-          int duration \u003d event.getTaskRunTime();\n-          if (duration \u003e maxMapRuntime) {\n-            maxMapRuntime \u003d duration;\n-            scheduler.informMaxMapRunTime(maxMapRuntime);\n-          }\n-          break;\n-        case FAILED:\n-        case KILLED:\n-        case OBSOLETE:\n-          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n-          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-              \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n-          break;\n-        case TIPFAILED:\n-          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n-          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-              event.getTaskAttemptId() + \"\u0027\");\n-          break;\n+        scheduler.resolve(event);\n+        if (TaskCompletionEvent.Status.SUCCEEDED \u003d\u003d event.getTaskStatus()) {\n+          ++numNewMaps;\n         }\n       }\n     } while (events.length \u003d\u003d maxEventsToFetch);\n \n     return numNewMaps;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected int getMapCompletionEvents()\n      throws IOException, InterruptedException {\n    \n    int numNewMaps \u003d 0;\n    TaskCompletionEvent events[] \u003d null;\n\n    do {\n      MapTaskCompletionEventsUpdate update \u003d\n          umbilical.getMapCompletionEvents(\n              (org.apache.hadoop.mapred.JobID)reduce.getJobID(),\n              fromEventIdx,\n              maxEventsToFetch,\n              (org.apache.hadoop.mapred.TaskAttemptID)reduce);\n      events \u003d update.getMapTaskCompletionEvents();\n      LOG.debug(\"Got \" + events.length + \" map completion events from \" +\n               fromEventIdx);\n\n      assert !update.shouldReset() : \"Unexpected legacy state\";\n\n      // Update the last seen event ID\n      fromEventIdx +\u003d events.length;\n\n      // Process the TaskCompletionEvents:\n      // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n      // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop\n      //    fetching from those maps.\n      // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n      //    outputs at all.\n      for (TaskCompletionEvent event : events) {\n        scheduler.resolve(event);\n        if (TaskCompletionEvent.Status.SUCCEEDED \u003d\u003d event.getTaskStatus()) {\n          ++numNewMaps;\n        }\n      }\n    } while (events.length \u003d\u003d maxEventsToFetch);\n\n    return numNewMaps;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java",
          "extendedDetails": {}
        }
      ]
    },
    "895029b2f2535f1ba8275be29fda16d0f80be790": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-4730. Fix Reducer\u0027s EventFetcher to scale the map-completion requests slowly to avoid HADOOP-8942. Contributed by Jason Lowe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1401941 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/10/12 6:26 PM",
      "commitName": "895029b2f2535f1ba8275be29fda16d0f80be790",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "MAPREDUCE-4730. Fix Reducer\u0027s EventFetcher to scale the map-completion requests slowly to avoid HADOOP-8942. Contributed by Jason Lowe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1401941 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "24/10/12 6:26 PM",
          "commitName": "895029b2f2535f1ba8275be29fda16d0f80be790",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "24/01/12 3:18 PM",
          "commitNameOld": "078ae89a4793eb6a153a88b106d330fd059a4933",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 274.09,
          "commitsBetweenForRepo": 1800,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,66 @@\n-  private int getMapCompletionEvents() throws IOException {\n+  protected int getMapCompletionEvents() throws IOException {\n     \n     int numNewMaps \u003d 0;\n-    \n-    MapTaskCompletionEventsUpdate update \u003d \n-      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n-                                       reduce.getJobID(), \n-                                       fromEventId, \n-                                       MAX_EVENTS_TO_FETCH,\n-                                       (org.apache.hadoop.mapred.TaskAttemptID)\n-                                         reduce);\n-    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n-    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n-             fromEventId);\n-      \n-    // Check if the reset is required.\n-    // Since there is no ordering of the task completion events at the \n-    // reducer, the only option to sync with the new jobtracker is to reset \n-    // the events index\n-    if (update.shouldReset()) {\n-      fromEventId \u003d 0;\n-      scheduler.resetKnownMaps();\n-    }\n-    \n-    // Update the last seen event ID\n-    fromEventId +\u003d events.length;\n-    \n-    // Process the TaskCompletionEvents:\n-    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n-    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n-    //    fetching from those maps.\n-    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n-    //    outputs at all.\n-    for (TaskCompletionEvent event : events) {\n-      switch (event.getTaskStatus()) {\n+    TaskCompletionEvent events[] \u003d null;\n+\n+    do {\n+      MapTaskCompletionEventsUpdate update \u003d\n+          umbilical.getMapCompletionEvents(\n+              (org.apache.hadoop.mapred.JobID)reduce.getJobID(),\n+              fromEventIdx,\n+              maxEventsToFetch,\n+              (org.apache.hadoop.mapred.TaskAttemptID)reduce);\n+      events \u003d update.getMapTaskCompletionEvents();\n+      LOG.debug(\"Got \" + events.length + \" map completion events from \" +\n+               fromEventIdx);\n+\n+      // Check if the reset is required.\n+      // Since there is no ordering of the task completion events at the\n+      // reducer, the only option to sync with the new jobtracker is to reset\n+      // the events index\n+      if (update.shouldReset()) {\n+        fromEventIdx \u003d 0;\n+        scheduler.resetKnownMaps();\n+      }\n+\n+      // Update the last seen event ID\n+      fromEventIdx +\u003d events.length;\n+\n+      // Process the TaskCompletionEvents:\n+      // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n+      // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop\n+      //    fetching from those maps.\n+      // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n+      //    outputs at all.\n+      for (TaskCompletionEvent event : events) {\n+        switch (event.getTaskStatus()) {\n         case SUCCEEDED:\n           URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n           scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-                                      u.toString(),\n-                                      event.getTaskAttemptId());\n+              u.toString(),\n+              event.getTaskAttemptId());\n           numNewMaps ++;\n           int duration \u003d event.getTaskRunTime();\n           if (duration \u003e maxMapRuntime) {\n             maxMapRuntime \u003d duration;\n             scheduler.informMaxMapRunTime(maxMapRuntime);\n           }\n           break;\n         case FAILED:\n         case KILLED:\n         case OBSOLETE:\n           scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n           LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n+              \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n           break;\n         case TIPFAILED:\n           scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n           LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-               event.getTaskAttemptId() + \"\u0027\");\n+              event.getTaskAttemptId() + \"\u0027\");\n           break;\n+        }\n       }\n-    }\n+    } while (events.length \u003d\u003d maxEventsToFetch);\n+\n     return numNewMaps;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected int getMapCompletionEvents() throws IOException {\n    \n    int numNewMaps \u003d 0;\n    TaskCompletionEvent events[] \u003d null;\n\n    do {\n      MapTaskCompletionEventsUpdate update \u003d\n          umbilical.getMapCompletionEvents(\n              (org.apache.hadoop.mapred.JobID)reduce.getJobID(),\n              fromEventIdx,\n              maxEventsToFetch,\n              (org.apache.hadoop.mapred.TaskAttemptID)reduce);\n      events \u003d update.getMapTaskCompletionEvents();\n      LOG.debug(\"Got \" + events.length + \" map completion events from \" +\n               fromEventIdx);\n\n      // Check if the reset is required.\n      // Since there is no ordering of the task completion events at the\n      // reducer, the only option to sync with the new jobtracker is to reset\n      // the events index\n      if (update.shouldReset()) {\n        fromEventIdx \u003d 0;\n        scheduler.resetKnownMaps();\n      }\n\n      // Update the last seen event ID\n      fromEventIdx +\u003d events.length;\n\n      // Process the TaskCompletionEvents:\n      // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n      // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop\n      //    fetching from those maps.\n      // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n      //    outputs at all.\n      for (TaskCompletionEvent event : events) {\n        switch (event.getTaskStatus()) {\n        case SUCCEEDED:\n          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n              u.toString(),\n              event.getTaskAttemptId());\n          numNewMaps ++;\n          int duration \u003d event.getTaskRunTime();\n          if (duration \u003e maxMapRuntime) {\n            maxMapRuntime \u003d duration;\n            scheduler.informMaxMapRunTime(maxMapRuntime);\n          }\n          break;\n        case FAILED:\n        case KILLED:\n        case OBSOLETE:\n          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n              \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n          break;\n        case TIPFAILED:\n          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n              event.getTaskAttemptId() + \"\u0027\");\n          break;\n        }\n      }\n    } while (events.length \u003d\u003d maxEventsToFetch);\n\n    return numNewMaps;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[protected]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-4730. Fix Reducer\u0027s EventFetcher to scale the map-completion requests slowly to avoid HADOOP-8942. Contributed by Jason Lowe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1401941 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "24/10/12 6:26 PM",
          "commitName": "895029b2f2535f1ba8275be29fda16d0f80be790",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "24/01/12 3:18 PM",
          "commitNameOld": "078ae89a4793eb6a153a88b106d330fd059a4933",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 274.09,
          "commitsBetweenForRepo": 1800,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,66 @@\n-  private int getMapCompletionEvents() throws IOException {\n+  protected int getMapCompletionEvents() throws IOException {\n     \n     int numNewMaps \u003d 0;\n-    \n-    MapTaskCompletionEventsUpdate update \u003d \n-      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n-                                       reduce.getJobID(), \n-                                       fromEventId, \n-                                       MAX_EVENTS_TO_FETCH,\n-                                       (org.apache.hadoop.mapred.TaskAttemptID)\n-                                         reduce);\n-    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n-    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n-             fromEventId);\n-      \n-    // Check if the reset is required.\n-    // Since there is no ordering of the task completion events at the \n-    // reducer, the only option to sync with the new jobtracker is to reset \n-    // the events index\n-    if (update.shouldReset()) {\n-      fromEventId \u003d 0;\n-      scheduler.resetKnownMaps();\n-    }\n-    \n-    // Update the last seen event ID\n-    fromEventId +\u003d events.length;\n-    \n-    // Process the TaskCompletionEvents:\n-    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n-    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n-    //    fetching from those maps.\n-    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n-    //    outputs at all.\n-    for (TaskCompletionEvent event : events) {\n-      switch (event.getTaskStatus()) {\n+    TaskCompletionEvent events[] \u003d null;\n+\n+    do {\n+      MapTaskCompletionEventsUpdate update \u003d\n+          umbilical.getMapCompletionEvents(\n+              (org.apache.hadoop.mapred.JobID)reduce.getJobID(),\n+              fromEventIdx,\n+              maxEventsToFetch,\n+              (org.apache.hadoop.mapred.TaskAttemptID)reduce);\n+      events \u003d update.getMapTaskCompletionEvents();\n+      LOG.debug(\"Got \" + events.length + \" map completion events from \" +\n+               fromEventIdx);\n+\n+      // Check if the reset is required.\n+      // Since there is no ordering of the task completion events at the\n+      // reducer, the only option to sync with the new jobtracker is to reset\n+      // the events index\n+      if (update.shouldReset()) {\n+        fromEventIdx \u003d 0;\n+        scheduler.resetKnownMaps();\n+      }\n+\n+      // Update the last seen event ID\n+      fromEventIdx +\u003d events.length;\n+\n+      // Process the TaskCompletionEvents:\n+      // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n+      // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop\n+      //    fetching from those maps.\n+      // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n+      //    outputs at all.\n+      for (TaskCompletionEvent event : events) {\n+        switch (event.getTaskStatus()) {\n         case SUCCEEDED:\n           URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n           scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n-                                      u.toString(),\n-                                      event.getTaskAttemptId());\n+              u.toString(),\n+              event.getTaskAttemptId());\n           numNewMaps ++;\n           int duration \u003d event.getTaskRunTime();\n           if (duration \u003e maxMapRuntime) {\n             maxMapRuntime \u003d duration;\n             scheduler.informMaxMapRunTime(maxMapRuntime);\n           }\n           break;\n         case FAILED:\n         case KILLED:\n         case OBSOLETE:\n           scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n           LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n-                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n+              \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n           break;\n         case TIPFAILED:\n           scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n           LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n-               event.getTaskAttemptId() + \"\u0027\");\n+              event.getTaskAttemptId() + \"\u0027\");\n           break;\n+        }\n       }\n-    }\n+    } while (events.length \u003d\u003d maxEventsToFetch);\n+\n     return numNewMaps;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected int getMapCompletionEvents() throws IOException {\n    \n    int numNewMaps \u003d 0;\n    TaskCompletionEvent events[] \u003d null;\n\n    do {\n      MapTaskCompletionEventsUpdate update \u003d\n          umbilical.getMapCompletionEvents(\n              (org.apache.hadoop.mapred.JobID)reduce.getJobID(),\n              fromEventIdx,\n              maxEventsToFetch,\n              (org.apache.hadoop.mapred.TaskAttemptID)reduce);\n      events \u003d update.getMapTaskCompletionEvents();\n      LOG.debug(\"Got \" + events.length + \" map completion events from \" +\n               fromEventIdx);\n\n      // Check if the reset is required.\n      // Since there is no ordering of the task completion events at the\n      // reducer, the only option to sync with the new jobtracker is to reset\n      // the events index\n      if (update.shouldReset()) {\n        fromEventIdx \u003d 0;\n        scheduler.resetKnownMaps();\n      }\n\n      // Update the last seen event ID\n      fromEventIdx +\u003d events.length;\n\n      // Process the TaskCompletionEvents:\n      // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n      // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop\n      //    fetching from those maps.\n      // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n      //    outputs at all.\n      for (TaskCompletionEvent event : events) {\n        switch (event.getTaskStatus()) {\n        case SUCCEEDED:\n          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n              u.toString(),\n              event.getTaskAttemptId());\n          numNewMaps ++;\n          int duration \u003d event.getTaskRunTime();\n          if (duration \u003e maxMapRuntime) {\n            maxMapRuntime \u003d duration;\n            scheduler.informMaxMapRunTime(maxMapRuntime);\n          }\n          break;\n        case FAILED:\n        case KILLED:\n        case OBSOLETE:\n          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n              \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n          break;\n        case TIPFAILED:\n          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n              event.getTaskAttemptId() + \"\u0027\");\n          break;\n        }\n      }\n    } while (events.length \u003d\u003d maxEventsToFetch);\n\n    return numNewMaps;\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java",
          "extendedDetails": {}
        }
      ]
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private int getMapCompletionEvents() throws IOException {\n    \n    int numNewMaps \u003d 0;\n    \n    MapTaskCompletionEventsUpdate update \u003d \n      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n                                       reduce.getJobID(), \n                                       fromEventId, \n                                       MAX_EVENTS_TO_FETCH,\n                                       (org.apache.hadoop.mapred.TaskAttemptID)\n                                         reduce);\n    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n             fromEventId);\n      \n    // Check if the reset is required.\n    // Since there is no ordering of the task completion events at the \n    // reducer, the only option to sync with the new jobtracker is to reset \n    // the events index\n    if (update.shouldReset()) {\n      fromEventId \u003d 0;\n      scheduler.resetKnownMaps();\n    }\n    \n    // Update the last seen event ID\n    fromEventId +\u003d events.length;\n    \n    // Process the TaskCompletionEvents:\n    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n    //    fetching from those maps.\n    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n    //    outputs at all.\n    for (TaskCompletionEvent event : events) {\n      switch (event.getTaskStatus()) {\n        case SUCCEEDED:\n          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n                                      u.toString(),\n                                      event.getTaskAttemptId());\n          numNewMaps ++;\n          int duration \u003d event.getTaskRunTime();\n          if (duration \u003e maxMapRuntime) {\n            maxMapRuntime \u003d duration;\n            scheduler.informMaxMapRunTime(maxMapRuntime);\n          }\n          break;\n        case FAILED:\n        case KILLED:\n        case OBSOLETE:\n          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n          break;\n        case TIPFAILED:\n          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n               event.getTaskAttemptId() + \"\u0027\");\n          break;\n      }\n    }\n    return numNewMaps;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymovefromfile",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private int getMapCompletionEvents() throws IOException {\n    \n    int numNewMaps \u003d 0;\n    \n    MapTaskCompletionEventsUpdate update \u003d \n      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n                                       reduce.getJobID(), \n                                       fromEventId, \n                                       MAX_EVENTS_TO_FETCH,\n                                       (org.apache.hadoop.mapred.TaskAttemptID)\n                                         reduce);\n    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n             fromEventId);\n      \n    // Check if the reset is required.\n    // Since there is no ordering of the task completion events at the \n    // reducer, the only option to sync with the new jobtracker is to reset \n    // the events index\n    if (update.shouldReset()) {\n      fromEventId \u003d 0;\n      scheduler.resetKnownMaps();\n    }\n    \n    // Update the last seen event ID\n    fromEventId +\u003d events.length;\n    \n    // Process the TaskCompletionEvents:\n    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n    //    fetching from those maps.\n    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n    //    outputs at all.\n    for (TaskCompletionEvent event : events) {\n      switch (event.getTaskStatus()) {\n        case SUCCEEDED:\n          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n                                      u.toString(),\n                                      event.getTaskAttemptId());\n          numNewMaps ++;\n          int duration \u003d event.getTaskRunTime();\n          if (duration \u003e maxMapRuntime) {\n            maxMapRuntime \u003d duration;\n            scheduler.informMaxMapRunTime(maxMapRuntime);\n          }\n          break;\n        case FAILED:\n        case KILLED:\n        case OBSOLETE:\n          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n          break;\n        case TIPFAILED:\n          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n               event.getTaskAttemptId() + \"\u0027\");\n          break;\n      }\n    }\n    return numNewMaps;\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java",
        "oldMethodName": "getMapCompletionEvents",
        "newMethodName": "getMapCompletionEvents"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,63 @@\n+  private int getMapCompletionEvents() throws IOException {\n+    \n+    int numNewMaps \u003d 0;\n+    \n+    MapTaskCompletionEventsUpdate update \u003d \n+      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n+                                       reduce.getJobID(), \n+                                       fromEventId, \n+                                       MAX_EVENTS_TO_FETCH,\n+                                       (org.apache.hadoop.mapred.TaskAttemptID)\n+                                         reduce);\n+    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n+    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n+             fromEventId);\n+      \n+    // Check if the reset is required.\n+    // Since there is no ordering of the task completion events at the \n+    // reducer, the only option to sync with the new jobtracker is to reset \n+    // the events index\n+    if (update.shouldReset()) {\n+      fromEventId \u003d 0;\n+      scheduler.resetKnownMaps();\n+    }\n+    \n+    // Update the last seen event ID\n+    fromEventId +\u003d events.length;\n+    \n+    // Process the TaskCompletionEvents:\n+    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n+    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n+    //    fetching from those maps.\n+    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n+    //    outputs at all.\n+    for (TaskCompletionEvent event : events) {\n+      switch (event.getTaskStatus()) {\n+        case SUCCEEDED:\n+          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n+          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n+                                      u.toString(),\n+                                      event.getTaskAttemptId());\n+          numNewMaps ++;\n+          int duration \u003d event.getTaskRunTime();\n+          if (duration \u003e maxMapRuntime) {\n+            maxMapRuntime \u003d duration;\n+            scheduler.informMaxMapRunTime(maxMapRuntime);\n+          }\n+          break;\n+        case FAILED:\n+        case KILLED:\n+        case OBSOLETE:\n+          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n+          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n+                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n+          break;\n+        case TIPFAILED:\n+          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n+          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n+               event.getTaskAttemptId() + \"\u0027\");\n+          break;\n+      }\n+    }\n+    return numNewMaps;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private int getMapCompletionEvents() throws IOException {\n    \n    int numNewMaps \u003d 0;\n    \n    MapTaskCompletionEventsUpdate update \u003d \n      umbilical.getMapCompletionEvents((org.apache.hadoop.mapred.JobID)\n                                       reduce.getJobID(), \n                                       fromEventId, \n                                       MAX_EVENTS_TO_FETCH,\n                                       (org.apache.hadoop.mapred.TaskAttemptID)\n                                         reduce);\n    TaskCompletionEvent events[] \u003d update.getMapTaskCompletionEvents();\n    LOG.debug(\"Got \" + events.length + \" map completion events from \" + \n             fromEventId);\n      \n    // Check if the reset is required.\n    // Since there is no ordering of the task completion events at the \n    // reducer, the only option to sync with the new jobtracker is to reset \n    // the events index\n    if (update.shouldReset()) {\n      fromEventId \u003d 0;\n      scheduler.resetKnownMaps();\n    }\n    \n    // Update the last seen event ID\n    fromEventId +\u003d events.length;\n    \n    // Process the TaskCompletionEvents:\n    // 1. Save the SUCCEEDED maps in knownOutputs to fetch the outputs.\n    // 2. Save the OBSOLETE/FAILED/KILLED maps in obsoleteOutputs to stop \n    //    fetching from those maps.\n    // 3. Remove TIPFAILED maps from neededOutputs since we don\u0027t need their\n    //    outputs at all.\n    for (TaskCompletionEvent event : events) {\n      switch (event.getTaskStatus()) {\n        case SUCCEEDED:\n          URI u \u003d getBaseURI(event.getTaskTrackerHttp());\n          scheduler.addKnownMapOutput(u.getHost() + \":\" + u.getPort(),\n                                      u.toString(),\n                                      event.getTaskAttemptId());\n          numNewMaps ++;\n          int duration \u003d event.getTaskRunTime();\n          if (duration \u003e maxMapRuntime) {\n            maxMapRuntime \u003d duration;\n            scheduler.informMaxMapRunTime(maxMapRuntime);\n          }\n          break;\n        case FAILED:\n        case KILLED:\n        case OBSOLETE:\n          scheduler.obsoleteMapOutput(event.getTaskAttemptId());\n          LOG.info(\"Ignoring obsolete output of \" + event.getTaskStatus() + \n                   \" map-task: \u0027\" + event.getTaskAttemptId() + \"\u0027\");\n          break;\n        case TIPFAILED:\n          scheduler.tipFailed(event.getTaskAttemptId().getTaskID());\n          LOG.info(\"Ignoring output of failed map TIP: \u0027\" +  \n               event.getTaskAttemptId() + \"\u0027\");\n          break;\n      }\n    }\n    return numNewMaps;\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java"
    }
  }
}