{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockDecompressorStream.java",
  "functionName": "decompress",
  "functionId": "decompress___b-byte[]__off-int__len-int",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
  "functionStartLine": 69,
  "functionEndLine": 112,
  "numCommitsSeen": 14,
  "timeTaken": 1382,
  "changeHistory": [
    "75291e6d53c13debf45493a870a898b63779914b",
    "1773893c9a7c71877391e0d2eddea2dd713bf010",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "0f6dfeeacbab65a31a33927a4eb84871d371fe52",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
    "4be1688c53f9b6ee23dbf5c05ef0b5c762c8b629",
    "3d27eaad25a999c403216107a95de9dd06143361",
    "17f9e57f7cb4333e4a7e6ceb27ae2289e6b7ef5a",
    "2a248dfc32e5061c1f80295f448ca525ade764c6",
    "5128a9a453d64bfe1ed978cf9ffed27985eeef36"
  ],
  "changeHistoryShort": {
    "75291e6d53c13debf45493a870a898b63779914b": "Ybodychange",
    "1773893c9a7c71877391e0d2eddea2dd713bf010": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "0f6dfeeacbab65a31a33927a4eb84871d371fe52": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yfilerename",
    "4be1688c53f9b6ee23dbf5c05ef0b5c762c8b629": "Ybodychange",
    "3d27eaad25a999c403216107a95de9dd06143361": "Ybodychange",
    "17f9e57f7cb4333e4a7e6ceb27ae2289e6b7ef5a": "Ybodychange",
    "2a248dfc32e5061c1f80295f448ca525ade764c6": "Ybodychange",
    "5128a9a453d64bfe1ed978cf9ffed27985eeef36": "Yintroduced"
  },
  "changeHistoryDetails": {
    "75291e6d53c13debf45493a870a898b63779914b": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15869. BlockDecompressorStream#decompress should not return -1 in case of IOException. Contributed by Surendra Singh Lilhore\n",
      "commitDate": "13/11/18 6:52 AM",
      "commitName": "75291e6d53c13debf45493a870a898b63779914b",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "28/06/13 2:05 PM",
      "commitNameOld": "1773893c9a7c71877391e0d2eddea2dd713bf010",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 1963.74,
      "commitsBetweenForRepo": 14907,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,44 @@\n   protected int decompress(byte[] b, int off, int len) throws IOException {\n     // Check if we are the beginning of a block\n     if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n       // Get original data size\n       try {\n-        originalBlockSize \u003d  rawReadInt();\n-      } catch (IOException ioe) {\n+        originalBlockSize \u003d rawReadInt();\n+      } catch (EOFException e) {\n         return -1;\n       }\n       noUncompressedBytes \u003d 0;\n       // EOF if originalBlockSize is 0\n       // This will occur only when decompressing previous compressed empty file\n       if (originalBlockSize \u003d\u003d 0) {\n         eof \u003d true;\n         return -1;\n       }\n     }\n \n     int n \u003d 0;\n     while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n       if (decompressor.finished() || decompressor.needsDictionary()) {\n         if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n           eof \u003d true;\n           return -1;\n         }\n       }\n       if (decompressor.needsInput()) {\n         int m;\n         try {\n           m \u003d getCompressedData();\n         } catch (EOFException e) {\n           eof \u003d true;\n           return -1;\n         }\n         // Send the read data to the decompressor\n         decompressor.setInput(buffer, 0, m);\n       }\n     }\n \n     // Note the no. of decompressed bytes read from \u0027current\u0027 block\n     noUncompressedBytes +\u003d n;\n \n     return n;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected int decompress(byte[] b, int off, int len) throws IOException {\n    // Check if we are the beginning of a block\n    if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n      // Get original data size\n      try {\n        originalBlockSize \u003d rawReadInt();\n      } catch (EOFException e) {\n        return -1;\n      }\n      noUncompressedBytes \u003d 0;\n      // EOF if originalBlockSize is 0\n      // This will occur only when decompressing previous compressed empty file\n      if (originalBlockSize \u003d\u003d 0) {\n        eof \u003d true;\n        return -1;\n      }\n    }\n\n    int n \u003d 0;\n    while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n      if (decompressor.finished() || decompressor.needsDictionary()) {\n        if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n          eof \u003d true;\n          return -1;\n        }\n      }\n      if (decompressor.needsInput()) {\n        int m;\n        try {\n          m \u003d getCompressedData();\n        } catch (EOFException e) {\n          eof \u003d true;\n          return -1;\n        }\n        // Send the read data to the decompressor\n        decompressor.setInput(buffer, 0, m);\n      }\n    }\n\n    // Note the no. of decompressed bytes read from \u0027current\u0027 block\n    noUncompressedBytes +\u003d n;\n\n    return n;\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
      "extendedDetails": {}
    },
    "1773893c9a7c71877391e0d2eddea2dd713bf010": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-9665. Fixed BlockDecompressorStream#decompress to return -1 rather than throw EOF at end of file. Contributed by Zhijie Shen.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1497922 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/13 2:05 PM",
      "commitName": "1773893c9a7c71877391e0d2eddea2dd713bf010",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/12 6:03 PM",
      "commitNameOld": "deead78e35b0cb81af875b5a8032cbd06c9a2dae",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 307.83,
      "commitsBetweenForRepo": 1778,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,44 @@\n   protected int decompress(byte[] b, int off, int len) throws IOException {\n     // Check if we are the beginning of a block\n     if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n       // Get original data size\n       try {\n         originalBlockSize \u003d  rawReadInt();\n       } catch (IOException ioe) {\n         return -1;\n       }\n       noUncompressedBytes \u003d 0;\n       // EOF if originalBlockSize is 0\n       // This will occur only when decompressing previous compressed empty file\n       if (originalBlockSize \u003d\u003d 0) {\n         eof \u003d true;\n         return -1;\n       }\n     }\n \n     int n \u003d 0;\n     while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n       if (decompressor.finished() || decompressor.needsDictionary()) {\n         if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n           eof \u003d true;\n           return -1;\n         }\n       }\n       if (decompressor.needsInput()) {\n-        int m \u003d getCompressedData();\n+        int m;\n+        try {\n+          m \u003d getCompressedData();\n+        } catch (EOFException e) {\n+          eof \u003d true;\n+          return -1;\n+        }\n         // Send the read data to the decompressor\n         decompressor.setInput(buffer, 0, m);\n       }\n     }\n \n     // Note the no. of decompressed bytes read from \u0027current\u0027 block\n     noUncompressedBytes +\u003d n;\n \n     return n;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected int decompress(byte[] b, int off, int len) throws IOException {\n    // Check if we are the beginning of a block\n    if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n      // Get original data size\n      try {\n        originalBlockSize \u003d  rawReadInt();\n      } catch (IOException ioe) {\n        return -1;\n      }\n      noUncompressedBytes \u003d 0;\n      // EOF if originalBlockSize is 0\n      // This will occur only when decompressing previous compressed empty file\n      if (originalBlockSize \u003d\u003d 0) {\n        eof \u003d true;\n        return -1;\n      }\n    }\n\n    int n \u003d 0;\n    while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n      if (decompressor.finished() || decompressor.needsDictionary()) {\n        if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n          eof \u003d true;\n          return -1;\n        }\n      }\n      if (decompressor.needsInput()) {\n        int m;\n        try {\n          m \u003d getCompressedData();\n        } catch (EOFException e) {\n          eof \u003d true;\n          return -1;\n        }\n        // Send the read data to the decompressor\n        decompressor.setInput(buffer, 0, m);\n      }\n    }\n\n    // Note the no. of decompressed bytes read from \u0027current\u0027 block\n    noUncompressedBytes +\u003d n;\n\n    return n;\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected int decompress(byte[] b, int off, int len) throws IOException {\n    // Check if we are the beginning of a block\n    if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n      // Get original data size\n      try {\n        originalBlockSize \u003d  rawReadInt();\n      } catch (IOException ioe) {\n        return -1;\n      }\n      noUncompressedBytes \u003d 0;\n      // EOF if originalBlockSize is 0\n      // This will occur only when decompressing previous compressed empty file\n      if (originalBlockSize \u003d\u003d 0) {\n        eof \u003d true;\n        return -1;\n      }\n    }\n\n    int n \u003d 0;\n    while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n      if (decompressor.finished() || decompressor.needsDictionary()) {\n        if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n          eof \u003d true;\n          return -1;\n        }\n      }\n      if (decompressor.needsInput()) {\n        int m \u003d getCompressedData();\n        // Send the read data to the decompressor\n        decompressor.setInput(buffer, 0, m);\n      }\n    }\n\n    // Note the no. of decompressed bytes read from \u0027current\u0027 block\n    noUncompressedBytes +\u003d n;\n\n    return n;\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
        "newPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java"
      }
    },
    "0f6dfeeacbab65a31a33927a4eb84871d371fe52": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-6671. Use maven for hadoop common builds. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1153184 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/08/11 9:37 AM",
      "commitName": "0f6dfeeacbab65a31a33927a4eb84871d371fe52",
      "commitAuthor": "Thomas White",
      "commitDateOld": "01/08/11 3:53 PM",
      "commitNameOld": "9bac807cedbcff34e1a144fb475eff267e5ed86d",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.74,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected int decompress(byte[] b, int off, int len) throws IOException {\n    // Check if we are the beginning of a block\n    if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n      // Get original data size\n      try {\n        originalBlockSize \u003d  rawReadInt();\n      } catch (IOException ioe) {\n        return -1;\n      }\n      noUncompressedBytes \u003d 0;\n      // EOF if originalBlockSize is 0\n      // This will occur only when decompressing previous compressed empty file\n      if (originalBlockSize \u003d\u003d 0) {\n        eof \u003d true;\n        return -1;\n      }\n    }\n\n    int n \u003d 0;\n    while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n      if (decompressor.finished() || decompressor.needsDictionary()) {\n        if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n          eof \u003d true;\n          return -1;\n        }\n      }\n      if (decompressor.needsInput()) {\n        int m \u003d getCompressedData();\n        // Send the read data to the decompressor\n        decompressor.setInput(buffer, 0, m);\n      }\n    }\n\n    // Note the no. of decompressed bytes read from \u0027current\u0027 block\n    noUncompressedBytes +\u003d n;\n\n    return n;\n  }",
      "path": "hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
      "extendedDetails": {
        "oldPath": "common/src/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
        "newPath": "hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "11/06/11 9:13 PM",
      "commitNameOld": "a285fb5effe9ba3be4ec5f942afaf5ddd1186151",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 0.74,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected int decompress(byte[] b, int off, int len) throws IOException {\n    // Check if we are the beginning of a block\n    if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n      // Get original data size\n      try {\n        originalBlockSize \u003d  rawReadInt();\n      } catch (IOException ioe) {\n        return -1;\n      }\n      noUncompressedBytes \u003d 0;\n      // EOF if originalBlockSize is 0\n      // This will occur only when decompressing previous compressed empty file\n      if (originalBlockSize \u003d\u003d 0) {\n        eof \u003d true;\n        return -1;\n      }\n    }\n\n    int n \u003d 0;\n    while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n      if (decompressor.finished() || decompressor.needsDictionary()) {\n        if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n          eof \u003d true;\n          return -1;\n        }\n      }\n      if (decompressor.needsInput()) {\n        int m \u003d getCompressedData();\n        // Send the read data to the decompressor\n        decompressor.setInput(buffer, 0, m);\n      }\n    }\n\n    // Note the no. of decompressed bytes read from \u0027current\u0027 block\n    noUncompressedBytes +\u003d n;\n\n    return n;\n  }",
      "path": "common/src/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
      "extendedDetails": {
        "oldPath": "src/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
        "newPath": "common/src/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java"
      }
    },
    "4be1688c53f9b6ee23dbf5c05ef0b5c762c8b629": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-6663.  BlockDecompressorStream get EOF exception when decompressing the file compressed from empty file.  Contributed by Kang Xiao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1028390 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/10/10 10:16 AM",
      "commitName": "4be1688c53f9b6ee23dbf5c05ef0b5c762c8b629",
      "commitAuthor": "Thomas White",
      "commitDateOld": "25/10/10 4:50 PM",
      "commitNameOld": "3d27eaad25a999c403216107a95de9dd06143361",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 2.73,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,38 @@\n   protected int decompress(byte[] b, int off, int len) throws IOException {\n     // Check if we are the beginning of a block\n     if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n       // Get original data size\n       try {\n         originalBlockSize \u003d  rawReadInt();\n       } catch (IOException ioe) {\n         return -1;\n       }\n       noUncompressedBytes \u003d 0;\n+      // EOF if originalBlockSize is 0\n+      // This will occur only when decompressing previous compressed empty file\n+      if (originalBlockSize \u003d\u003d 0) {\n+        eof \u003d true;\n+        return -1;\n+      }\n     }\n \n     int n \u003d 0;\n     while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n       if (decompressor.finished() || decompressor.needsDictionary()) {\n         if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n           eof \u003d true;\n           return -1;\n         }\n       }\n       if (decompressor.needsInput()) {\n         int m \u003d getCompressedData();\n         // Send the read data to the decompressor\n         decompressor.setInput(buffer, 0, m);\n       }\n     }\n \n     // Note the no. of decompressed bytes read from \u0027current\u0027 block\n     noUncompressedBytes +\u003d n;\n \n     return n;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected int decompress(byte[] b, int off, int len) throws IOException {\n    // Check if we are the beginning of a block\n    if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n      // Get original data size\n      try {\n        originalBlockSize \u003d  rawReadInt();\n      } catch (IOException ioe) {\n        return -1;\n      }\n      noUncompressedBytes \u003d 0;\n      // EOF if originalBlockSize is 0\n      // This will occur only when decompressing previous compressed empty file\n      if (originalBlockSize \u003d\u003d 0) {\n        eof \u003d true;\n        return -1;\n      }\n    }\n\n    int n \u003d 0;\n    while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n      if (decompressor.finished() || decompressor.needsDictionary()) {\n        if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n          eof \u003d true;\n          return -1;\n        }\n      }\n      if (decompressor.needsInput()) {\n        int m \u003d getCompressedData();\n        // Send the read data to the decompressor\n        decompressor.setInput(buffer, 0, m);\n      }\n    }\n\n    // Note the no. of decompressed bytes read from \u0027current\u0027 block\n    noUncompressedBytes +\u003d n;\n\n    return n;\n  }",
      "path": "src/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
      "extendedDetails": {}
    },
    "3d27eaad25a999c403216107a95de9dd06143361": {
      "type": "Ybodychange",
      "commitMessage": "Reverting HADOOP-6663.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1027316 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/10/10 4:50 PM",
      "commitName": "3d27eaad25a999c403216107a95de9dd06143361",
      "commitAuthor": "Thomas White",
      "commitDateOld": "25/10/10 4:34 PM",
      "commitNameOld": "17f9e57f7cb4333e4a7e6ceb27ae2289e6b7ef5a",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,32 @@\n   protected int decompress(byte[] b, int off, int len) throws IOException {\n     // Check if we are the beginning of a block\n     if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n       // Get original data size\n       try {\n         originalBlockSize \u003d  rawReadInt();\n       } catch (IOException ioe) {\n         return -1;\n       }\n       noUncompressedBytes \u003d 0;\n-      // EOF if originalBlockSize is 0\n-      // This will occur only when decompressing previous compressed empty file\n-      if (originalBlockSize \u003d\u003d 0) {\n-        eof \u003d true;\n-        return -1;\n-      }\n     }\n \n     int n \u003d 0;\n     while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n       if (decompressor.finished() || decompressor.needsDictionary()) {\n         if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n           eof \u003d true;\n           return -1;\n         }\n       }\n       if (decompressor.needsInput()) {\n         int m \u003d getCompressedData();\n         // Send the read data to the decompressor\n         decompressor.setInput(buffer, 0, m);\n       }\n     }\n \n     // Note the no. of decompressed bytes read from \u0027current\u0027 block\n     noUncompressedBytes +\u003d n;\n \n     return n;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected int decompress(byte[] b, int off, int len) throws IOException {\n    // Check if we are the beginning of a block\n    if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n      // Get original data size\n      try {\n        originalBlockSize \u003d  rawReadInt();\n      } catch (IOException ioe) {\n        return -1;\n      }\n      noUncompressedBytes \u003d 0;\n    }\n\n    int n \u003d 0;\n    while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n      if (decompressor.finished() || decompressor.needsDictionary()) {\n        if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n          eof \u003d true;\n          return -1;\n        }\n      }\n      if (decompressor.needsInput()) {\n        int m \u003d getCompressedData();\n        // Send the read data to the decompressor\n        decompressor.setInput(buffer, 0, m);\n      }\n    }\n\n    // Note the no. of decompressed bytes read from \u0027current\u0027 block\n    noUncompressedBytes +\u003d n;\n\n    return n;\n  }",
      "path": "src/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
      "extendedDetails": {}
    },
    "17f9e57f7cb4333e4a7e6ceb27ae2289e6b7ef5a": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-6663.  BlockDecompressorStream get EOF exception when decompressing the file compressed from empty file.  Contributed by Kang Xiao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1027312 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/10/10 4:34 PM",
      "commitName": "17f9e57f7cb4333e4a7e6ceb27ae2289e6b7ef5a",
      "commitAuthor": "Thomas White",
      "commitDateOld": "07/07/10 4:22 PM",
      "commitNameOld": "2a248dfc32e5061c1f80295f448ca525ade764c6",
      "commitAuthorOld": "Christopher Douglas",
      "daysBetweenCommits": 110.01,
      "commitsBetweenForRepo": 84,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,38 @@\n   protected int decompress(byte[] b, int off, int len) throws IOException {\n     // Check if we are the beginning of a block\n     if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n       // Get original data size\n       try {\n         originalBlockSize \u003d  rawReadInt();\n       } catch (IOException ioe) {\n         return -1;\n       }\n       noUncompressedBytes \u003d 0;\n+      // EOF if originalBlockSize is 0\n+      // This will occur only when decompressing previous compressed empty file\n+      if (originalBlockSize \u003d\u003d 0) {\n+        eof \u003d true;\n+        return -1;\n+      }\n     }\n \n     int n \u003d 0;\n     while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n       if (decompressor.finished() || decompressor.needsDictionary()) {\n         if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n           eof \u003d true;\n           return -1;\n         }\n       }\n       if (decompressor.needsInput()) {\n         int m \u003d getCompressedData();\n         // Send the read data to the decompressor\n         decompressor.setInput(buffer, 0, m);\n       }\n     }\n \n     // Note the no. of decompressed bytes read from \u0027current\u0027 block\n     noUncompressedBytes +\u003d n;\n \n     return n;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected int decompress(byte[] b, int off, int len) throws IOException {\n    // Check if we are the beginning of a block\n    if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n      // Get original data size\n      try {\n        originalBlockSize \u003d  rawReadInt();\n      } catch (IOException ioe) {\n        return -1;\n      }\n      noUncompressedBytes \u003d 0;\n      // EOF if originalBlockSize is 0\n      // This will occur only when decompressing previous compressed empty file\n      if (originalBlockSize \u003d\u003d 0) {\n        eof \u003d true;\n        return -1;\n      }\n    }\n\n    int n \u003d 0;\n    while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n      if (decompressor.finished() || decompressor.needsDictionary()) {\n        if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n          eof \u003d true;\n          return -1;\n        }\n      }\n      if (decompressor.needsInput()) {\n        int m \u003d getCompressedData();\n        // Send the read data to the decompressor\n        decompressor.setInput(buffer, 0, m);\n      }\n    }\n\n    // Note the no. of decompressed bytes read from \u0027current\u0027 block\n    noUncompressedBytes +\u003d n;\n\n    return n;\n  }",
      "path": "src/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
      "extendedDetails": {}
    },
    "2a248dfc32e5061c1f80295f448ca525ade764c6": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-6835. Add support for concatenated gzip input. Contributed by Greg Roelofs\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@961532 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/07/10 4:22 PM",
      "commitName": "2a248dfc32e5061c1f80295f448ca525ade764c6",
      "commitAuthor": "Christopher Douglas",
      "commitDateOld": "11/06/10 2:34 PM",
      "commitNameOld": "6378822a67c0baa502d22201f5c2b478cbe1261c",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 26.07,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,32 @@\n   protected int decompress(byte[] b, int off, int len) throws IOException {\n     // Check if we are the beginning of a block\n     if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n       // Get original data size\n       try {\n         originalBlockSize \u003d  rawReadInt();\n       } catch (IOException ioe) {\n         return -1;\n       }\n       noUncompressedBytes \u003d 0;\n     }\n-    \n+\n     int n \u003d 0;\n     while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n       if (decompressor.finished() || decompressor.needsDictionary()) {\n         if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n           eof \u003d true;\n           return -1;\n         }\n       }\n       if (decompressor.needsInput()) {\n-        getCompressedData();\n+        int m \u003d getCompressedData();\n+        // Send the read data to the decompressor\n+        decompressor.setInput(buffer, 0, m);\n       }\n     }\n-    \n+\n     // Note the no. of decompressed bytes read from \u0027current\u0027 block\n     noUncompressedBytes +\u003d n;\n \n     return n;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected int decompress(byte[] b, int off, int len) throws IOException {\n    // Check if we are the beginning of a block\n    if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n      // Get original data size\n      try {\n        originalBlockSize \u003d  rawReadInt();\n      } catch (IOException ioe) {\n        return -1;\n      }\n      noUncompressedBytes \u003d 0;\n    }\n\n    int n \u003d 0;\n    while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n      if (decompressor.finished() || decompressor.needsDictionary()) {\n        if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n          eof \u003d true;\n          return -1;\n        }\n      }\n      if (decompressor.needsInput()) {\n        int m \u003d getCompressedData();\n        // Send the read data to the decompressor\n        decompressor.setInput(buffer, 0, m);\n      }\n    }\n\n    // Note the no. of decompressed bytes read from \u0027current\u0027 block\n    noUncompressedBytes +\u003d n;\n\n    return n;\n  }",
      "path": "src/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
      "extendedDetails": {}
    },
    "5128a9a453d64bfe1ed978cf9ffed27985eeef36": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-4687 Moving src directories on branch\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/core/branches/HADOOP-4687/core@776174 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/05/09 9:20 PM",
      "commitName": "5128a9a453d64bfe1ed978cf9ffed27985eeef36",
      "commitAuthor": "Owen O\u0027Malley",
      "diff": "@@ -0,0 +1,30 @@\n+  protected int decompress(byte[] b, int off, int len) throws IOException {\n+    // Check if we are the beginning of a block\n+    if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n+      // Get original data size\n+      try {\n+        originalBlockSize \u003d  rawReadInt();\n+      } catch (IOException ioe) {\n+        return -1;\n+      }\n+      noUncompressedBytes \u003d 0;\n+    }\n+    \n+    int n \u003d 0;\n+    while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n+      if (decompressor.finished() || decompressor.needsDictionary()) {\n+        if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n+          eof \u003d true;\n+          return -1;\n+        }\n+      }\n+      if (decompressor.needsInput()) {\n+        getCompressedData();\n+      }\n+    }\n+    \n+    // Note the no. of decompressed bytes read from \u0027current\u0027 block\n+    noUncompressedBytes +\u003d n;\n+\n+    return n;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected int decompress(byte[] b, int off, int len) throws IOException {\n    // Check if we are the beginning of a block\n    if (noUncompressedBytes \u003d\u003d originalBlockSize) {\n      // Get original data size\n      try {\n        originalBlockSize \u003d  rawReadInt();\n      } catch (IOException ioe) {\n        return -1;\n      }\n      noUncompressedBytes \u003d 0;\n    }\n    \n    int n \u003d 0;\n    while ((n \u003d decompressor.decompress(b, off, len)) \u003d\u003d 0) {\n      if (decompressor.finished() || decompressor.needsDictionary()) {\n        if (noUncompressedBytes \u003e\u003d originalBlockSize) {\n          eof \u003d true;\n          return -1;\n        }\n      }\n      if (decompressor.needsInput()) {\n        getCompressedData();\n      }\n    }\n    \n    // Note the no. of decompressed bytes read from \u0027current\u0027 block\n    noUncompressedBytes +\u003d n;\n\n    return n;\n  }",
      "path": "src/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java"
    }
  }
}