{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CacheDirectiveIterator.java",
  "functionName": "makeRequest",
  "functionId": "makeRequest___prevKey-Long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveIterator.java",
  "functionStartLine": 93,
  "functionEndLine": 120,
  "numCommitsSeen": 386,
  "timeTaken": 6648,
  "changeHistory": [
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "892ade689f9bcce76daae8f66fc00a49bee8548e",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "c4ccbe62c0857261b197a68c1e03a02e92f21a38",
    "8f48760663070529ff09927d1772010fffe5f438",
    "f791e291ca39eac6aa0650319e8dd606d15d5804",
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
    "f79b3e6b17450e9d34c483046b7437b09dd72016",
    "e202d4d1548a0be2f5c61ff82be8b52bd0cfce04",
    "02e0e158a26f81ce8375426ba0ea56db09ee36be",
    "f41f8b8842c3f26d19f7fa928070c7c07f760e4c",
    "d56d0b46e1b82ae068083ddb99872d314684dc82",
    "97b7267977ef42201e5844df49bc37ec3d10ce16",
    "920b4cc06f1bc15809902bdd1968cc434a694a08"
  ],
  "changeHistoryShort": {
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "892ade689f9bcce76daae8f66fc00a49bee8548e": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "c4ccbe62c0857261b197a68c1e03a02e92f21a38": "Ybodychange",
    "8f48760663070529ff09927d1772010fffe5f438": "Ybodychange",
    "f791e291ca39eac6aa0650319e8dd606d15d5804": "Ymultichange(Ymovefromfile,Ybodychange,Yparameterchange)",
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde": "Ymultichange(Yreturntypechange,Ybodychange)",
    "f79b3e6b17450e9d34c483046b7437b09dd72016": "Ymultichange(Yreturntypechange,Ybodychange)",
    "e202d4d1548a0be2f5c61ff82be8b52bd0cfce04": "Ymultichange(Yreturntypechange,Ybodychange)",
    "02e0e158a26f81ce8375426ba0ea56db09ee36be": "Ymultichange(Yreturntypechange,Ybodychange)",
    "f41f8b8842c3f26d19f7fa928070c7c07f760e4c": "Ymultichange(Yparameterchange,Ybodychange)",
    "d56d0b46e1b82ae068083ddb99872d314684dc82": "Ymultichange(Yparameterchange,Ybodychange)",
    "97b7267977ef42201e5844df49bc37ec3d10ce16": "Ymultichange(Yparameterchange,Yreturntypechange,Ymodifierchange,Ybodychange)",
    "920b4cc06f1bc15809902bdd1968cc434a694a08": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 5.16,
      "commitsBetweenForRepo": 41,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,28 @@\n   public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n       throws IOException {\n-    BatchedEntries\u003cCacheDirectiveEntry\u003e entries \u003d null;\n-    TraceScope scope \u003d tracer.newScope(\"listCacheDirectives\");\n-    try {\n+    BatchedEntries\u003cCacheDirectiveEntry\u003e entries;\n+    try (TraceScope ignored \u003d tracer.newScope(\"listCacheDirectives\")) {\n       entries \u003d namenode.listCacheDirectives(prevKey, filter);\n     } catch (IOException e) {\n       if (e.getMessage().contains(\"Filtering by ID is unsupported\")) {\n         // Retry case for old servers, do the filtering client-side\n         long id \u003d filter.getId();\n         filter \u003d removeIdFromFilter(filter);\n         // Using id - 1 as prevId should get us a window containing the id\n         // This is somewhat brittle, since it depends on directives being\n         // returned in order of ascending ID.\n         entries \u003d namenode.listCacheDirectives(id - 1, filter);\n-        for (int i\u003d0; i\u003centries.size(); i++) {\n+        for (int i \u003d 0; i \u003c entries.size(); i++) {\n           CacheDirectiveEntry entry \u003d entries.get(i);\n-          if (entry.getInfo().getId().equals((Long)id)) {\n+          if (entry.getInfo().getId().equals(id)) {\n             return new SingleEntry(entry);\n           }\n         }\n         throw new RemoteException(InvalidRequestException.class.getName(),\n             \"Did not find requested id \" + id);\n       }\n       throw e;\n-    } finally {\n-      scope.close();\n     }\n     Preconditions.checkNotNull(entries);\n     return entries;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n      throws IOException {\n    BatchedEntries\u003cCacheDirectiveEntry\u003e entries;\n    try (TraceScope ignored \u003d tracer.newScope(\"listCacheDirectives\")) {\n      entries \u003d namenode.listCacheDirectives(prevKey, filter);\n    } catch (IOException e) {\n      if (e.getMessage().contains(\"Filtering by ID is unsupported\")) {\n        // Retry case for old servers, do the filtering client-side\n        long id \u003d filter.getId();\n        filter \u003d removeIdFromFilter(filter);\n        // Using id - 1 as prevId should get us a window containing the id\n        // This is somewhat brittle, since it depends on directives being\n        // returned in order of ascending ID.\n        entries \u003d namenode.listCacheDirectives(id - 1, filter);\n        for (int i \u003d 0; i \u003c entries.size(); i++) {\n          CacheDirectiveEntry entry \u003d entries.get(i);\n          if (entry.getInfo().getId().equals(id)) {\n            return new SingleEntry(entry);\n          }\n        }\n        throw new RemoteException(InvalidRequestException.class.getName(),\n            \"Did not find requested id \" + id);\n      }\n      throw e;\n    }\n    Preconditions.checkNotNull(entries);\n    return entries;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveIterator.java",
      "extendedDetails": {}
    },
    "892ade689f9bcce76daae8f66fc00a49bee8548e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
      "commitDate": "28/09/15 7:42 AM",
      "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "26/09/15 11:08 AM",
      "commitNameOld": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.86,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n       throws IOException {\n     BatchedEntries\u003cCacheDirectiveEntry\u003e entries \u003d null;\n-    TraceScope scope \u003d Trace.startSpan(\"listCacheDirectives\", traceSampler);\n+    TraceScope scope \u003d tracer.newScope(\"listCacheDirectives\");\n     try {\n       entries \u003d namenode.listCacheDirectives(prevKey, filter);\n     } catch (IOException e) {\n       if (e.getMessage().contains(\"Filtering by ID is unsupported\")) {\n         // Retry case for old servers, do the filtering client-side\n         long id \u003d filter.getId();\n         filter \u003d removeIdFromFilter(filter);\n         // Using id - 1 as prevId should get us a window containing the id\n         // This is somewhat brittle, since it depends on directives being\n         // returned in order of ascending ID.\n         entries \u003d namenode.listCacheDirectives(id - 1, filter);\n         for (int i\u003d0; i\u003centries.size(); i++) {\n           CacheDirectiveEntry entry \u003d entries.get(i);\n           if (entry.getInfo().getId().equals((Long)id)) {\n             return new SingleEntry(entry);\n           }\n         }\n         throw new RemoteException(InvalidRequestException.class.getName(),\n             \"Did not find requested id \" + id);\n       }\n       throw e;\n     } finally {\n       scope.close();\n     }\n     Preconditions.checkNotNull(entries);\n     return entries;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n      throws IOException {\n    BatchedEntries\u003cCacheDirectiveEntry\u003e entries \u003d null;\n    TraceScope scope \u003d tracer.newScope(\"listCacheDirectives\");\n    try {\n      entries \u003d namenode.listCacheDirectives(prevKey, filter);\n    } catch (IOException e) {\n      if (e.getMessage().contains(\"Filtering by ID is unsupported\")) {\n        // Retry case for old servers, do the filtering client-side\n        long id \u003d filter.getId();\n        filter \u003d removeIdFromFilter(filter);\n        // Using id - 1 as prevId should get us a window containing the id\n        // This is somewhat brittle, since it depends on directives being\n        // returned in order of ascending ID.\n        entries \u003d namenode.listCacheDirectives(id - 1, filter);\n        for (int i\u003d0; i\u003centries.size(); i++) {\n          CacheDirectiveEntry entry \u003d entries.get(i);\n          if (entry.getInfo().getId().equals((Long)id)) {\n            return new SingleEntry(entry);\n          }\n        }\n        throw new RemoteException(InvalidRequestException.class.getName(),\n            \"Did not find requested id \" + id);\n      }\n      throw e;\n    } finally {\n      scope.close();\n    }\n    Preconditions.checkNotNull(entries);\n    return entries;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveIterator.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n      throws IOException {\n    BatchedEntries\u003cCacheDirectiveEntry\u003e entries \u003d null;\n    TraceScope scope \u003d Trace.startSpan(\"listCacheDirectives\", traceSampler);\n    try {\n      entries \u003d namenode.listCacheDirectives(prevKey, filter);\n    } catch (IOException e) {\n      if (e.getMessage().contains(\"Filtering by ID is unsupported\")) {\n        // Retry case for old servers, do the filtering client-side\n        long id \u003d filter.getId();\n        filter \u003d removeIdFromFilter(filter);\n        // Using id - 1 as prevId should get us a window containing the id\n        // This is somewhat brittle, since it depends on directives being\n        // returned in order of ascending ID.\n        entries \u003d namenode.listCacheDirectives(id - 1, filter);\n        for (int i\u003d0; i\u003centries.size(); i++) {\n          CacheDirectiveEntry entry \u003d entries.get(i);\n          if (entry.getInfo().getId().equals((Long)id)) {\n            return new SingleEntry(entry);\n          }\n        }\n        throw new RemoteException(InvalidRequestException.class.getName(),\n            \"Did not find requested id \" + id);\n      }\n      throw e;\n    } finally {\n      scope.close();\n    }\n    Preconditions.checkNotNull(entries);\n    return entries;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveIterator.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveIterator.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveIterator.java"
      }
    },
    "c4ccbe62c0857261b197a68c1e03a02e92f21a38": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7189. Add trace spans for DFSClient metadata operations. (Colin P. McCabe via yliu)\n",
      "commitDate": "15/01/15 8:23 AM",
      "commitName": "c4ccbe62c0857261b197a68c1e03a02e92f21a38",
      "commitAuthor": "yliu",
      "commitDateOld": "15/05/14 6:18 PM",
      "commitNameOld": "8f48760663070529ff09927d1772010fffe5f438",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 244.63,
      "commitsBetweenForRepo": 1979,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,31 @@\n   public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n       throws IOException {\n     BatchedEntries\u003cCacheDirectiveEntry\u003e entries \u003d null;\n+    TraceScope scope \u003d Trace.startSpan(\"listCacheDirectives\", traceSampler);\n     try {\n       entries \u003d namenode.listCacheDirectives(prevKey, filter);\n     } catch (IOException e) {\n       if (e.getMessage().contains(\"Filtering by ID is unsupported\")) {\n         // Retry case for old servers, do the filtering client-side\n         long id \u003d filter.getId();\n         filter \u003d removeIdFromFilter(filter);\n         // Using id - 1 as prevId should get us a window containing the id\n         // This is somewhat brittle, since it depends on directives being\n         // returned in order of ascending ID.\n         entries \u003d namenode.listCacheDirectives(id - 1, filter);\n         for (int i\u003d0; i\u003centries.size(); i++) {\n           CacheDirectiveEntry entry \u003d entries.get(i);\n           if (entry.getInfo().getId().equals((Long)id)) {\n             return new SingleEntry(entry);\n           }\n         }\n         throw new RemoteException(InvalidRequestException.class.getName(),\n             \"Did not find requested id \" + id);\n       }\n       throw e;\n+    } finally {\n+      scope.close();\n     }\n     Preconditions.checkNotNull(entries);\n     return entries;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n      throws IOException {\n    BatchedEntries\u003cCacheDirectiveEntry\u003e entries \u003d null;\n    TraceScope scope \u003d Trace.startSpan(\"listCacheDirectives\", traceSampler);\n    try {\n      entries \u003d namenode.listCacheDirectives(prevKey, filter);\n    } catch (IOException e) {\n      if (e.getMessage().contains(\"Filtering by ID is unsupported\")) {\n        // Retry case for old servers, do the filtering client-side\n        long id \u003d filter.getId();\n        filter \u003d removeIdFromFilter(filter);\n        // Using id - 1 as prevId should get us a window containing the id\n        // This is somewhat brittle, since it depends on directives being\n        // returned in order of ascending ID.\n        entries \u003d namenode.listCacheDirectives(id - 1, filter);\n        for (int i\u003d0; i\u003centries.size(); i++) {\n          CacheDirectiveEntry entry \u003d entries.get(i);\n          if (entry.getInfo().getId().equals((Long)id)) {\n            return new SingleEntry(entry);\n          }\n        }\n        throw new RemoteException(InvalidRequestException.class.getName(),\n            \"Did not find requested id \" + id);\n      }\n      throw e;\n    } finally {\n      scope.close();\n    }\n    Preconditions.checkNotNull(entries);\n    return entries;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveIterator.java",
      "extendedDetails": {}
    },
    "8f48760663070529ff09927d1772010fffe5f438": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6345. DFS.listCacheDirectives() should allow filtering based on cache directive ID. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1595086 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/05/14 6:18 PM",
      "commitName": "8f48760663070529ff09927d1772010fffe5f438",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "26/03/14 2:27 PM",
      "commitNameOld": "14556cc5d8fee8f8a846e4f65572828553be386c",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 50.16,
      "commitsBetweenForRepo": 296,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,4 +1,28 @@\n   public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n       throws IOException {\n-    return namenode.listCacheDirectives(prevKey, filter);\n+    BatchedEntries\u003cCacheDirectiveEntry\u003e entries \u003d null;\n+    try {\n+      entries \u003d namenode.listCacheDirectives(prevKey, filter);\n+    } catch (IOException e) {\n+      if (e.getMessage().contains(\"Filtering by ID is unsupported\")) {\n+        // Retry case for old servers, do the filtering client-side\n+        long id \u003d filter.getId();\n+        filter \u003d removeIdFromFilter(filter);\n+        // Using id - 1 as prevId should get us a window containing the id\n+        // This is somewhat brittle, since it depends on directives being\n+        // returned in order of ascending ID.\n+        entries \u003d namenode.listCacheDirectives(id - 1, filter);\n+        for (int i\u003d0; i\u003centries.size(); i++) {\n+          CacheDirectiveEntry entry \u003d entries.get(i);\n+          if (entry.getInfo().getId().equals((Long)id)) {\n+            return new SingleEntry(entry);\n+          }\n+        }\n+        throw new RemoteException(InvalidRequestException.class.getName(),\n+            \"Did not find requested id \" + id);\n+      }\n+      throw e;\n+    }\n+    Preconditions.checkNotNull(entries);\n+    return entries;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n      throws IOException {\n    BatchedEntries\u003cCacheDirectiveEntry\u003e entries \u003d null;\n    try {\n      entries \u003d namenode.listCacheDirectives(prevKey, filter);\n    } catch (IOException e) {\n      if (e.getMessage().contains(\"Filtering by ID is unsupported\")) {\n        // Retry case for old servers, do the filtering client-side\n        long id \u003d filter.getId();\n        filter \u003d removeIdFromFilter(filter);\n        // Using id - 1 as prevId should get us a window containing the id\n        // This is somewhat brittle, since it depends on directives being\n        // returned in order of ascending ID.\n        entries \u003d namenode.listCacheDirectives(id - 1, filter);\n        for (int i\u003d0; i\u003centries.size(); i++) {\n          CacheDirectiveEntry entry \u003d entries.get(i);\n          if (entry.getInfo().getId().equals((Long)id)) {\n            return new SingleEntry(entry);\n          }\n        }\n        throw new RemoteException(InvalidRequestException.class.getName(),\n            \"Did not find requested id \" + id);\n      }\n      throw e;\n    }\n    Preconditions.checkNotNull(entries);\n    return entries;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveIterator.java",
      "extendedDetails": {}
    },
    "f791e291ca39eac6aa0650319e8dd606d15d5804": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-5555. CacheAdmin commands fail when first listed NameNode is in Standby (jxiang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1547895 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/12/13 12:06 PM",
      "commitName": "f791e291ca39eac6aa0650319e8dd606d15d5804",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-5555. CacheAdmin commands fail when first listed NameNode is in Standby (jxiang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1547895 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/12/13 12:06 PM",
          "commitName": "f791e291ca39eac6aa0650319e8dd606d15d5804",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "04/12/13 2:49 AM",
          "commitNameOld": "59a2139093971ed663b58370a03ff408078e8b85",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.39,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,4 +1,4 @@\n-    public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(\n-        Long nextKey) throws IOException {\n-      return namesystem.listCacheDirectives(nextKey, filter);\n-    }\n\\ No newline at end of file\n+  public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n+      throws IOException {\n+    return namenode.listCacheDirectives(prevKey, filter);\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n      throws IOException {\n    return namenode.listCacheDirectives(prevKey, filter);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveIterator.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveIterator.java",
            "oldMethodName": "makeRequest",
            "newMethodName": "makeRequest"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5555. CacheAdmin commands fail when first listed NameNode is in Standby (jxiang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1547895 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/12/13 12:06 PM",
          "commitName": "f791e291ca39eac6aa0650319e8dd606d15d5804",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "04/12/13 2:49 AM",
          "commitNameOld": "59a2139093971ed663b58370a03ff408078e8b85",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.39,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,4 +1,4 @@\n-    public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(\n-        Long nextKey) throws IOException {\n-      return namesystem.listCacheDirectives(nextKey, filter);\n-    }\n\\ No newline at end of file\n+  public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n+      throws IOException {\n+    return namenode.listCacheDirectives(prevKey, filter);\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n      throws IOException {\n    return namenode.listCacheDirectives(prevKey, filter);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveIterator.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5555. CacheAdmin commands fail when first listed NameNode is in Standby (jxiang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1547895 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/12/13 12:06 PM",
          "commitName": "f791e291ca39eac6aa0650319e8dd606d15d5804",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "04/12/13 2:49 AM",
          "commitNameOld": "59a2139093971ed663b58370a03ff408078e8b85",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.39,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,4 +1,4 @@\n-    public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(\n-        Long nextKey) throws IOException {\n-      return namesystem.listCacheDirectives(nextKey, filter);\n-    }\n\\ No newline at end of file\n+  public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n+      throws IOException {\n+    return namenode.listCacheDirectives(prevKey, filter);\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(Long prevKey)\n      throws IOException {\n    return namenode.listCacheDirectives(prevKey, filter);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/CacheDirectiveIterator.java",
          "extendedDetails": {
            "oldValue": "[nextKey-Long]",
            "newValue": "[prevKey-Long]"
          }
        }
      ]
    },
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-5473. Consistent naming of user-visible caching classes and methods (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544252 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/11/13 9:12 AM",
      "commitName": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-5473. Consistent naming of user-visible caching classes and methods (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544252 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/11/13 9:12 AM",
          "commitName": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "07/11/13 2:07 PM",
          "commitNameOld": "f79b3e6b17450e9d34c483046b7437b09dd72016",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 13.8,
          "commitsBetweenForRepo": 99,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,4 +1,4 @@\n-    public BatchedEntries\u003cPathBasedCacheDirective\u003e makeRequest(\n+    public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(\n         Long nextKey) throws IOException {\n-      return namesystem.listPathBasedCacheDirectives(nextKey, filter);\n+      return namesystem.listCacheDirectives(nextKey, filter);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(\n        Long nextKey) throws IOException {\n      return namesystem.listCacheDirectives(nextKey, filter);\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "BatchedEntries\u003cPathBasedCacheDirective\u003e",
            "newValue": "BatchedEntries\u003cCacheDirectiveEntry\u003e"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5473. Consistent naming of user-visible caching classes and methods (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544252 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/11/13 9:12 AM",
          "commitName": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "07/11/13 2:07 PM",
          "commitNameOld": "f79b3e6b17450e9d34c483046b7437b09dd72016",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 13.8,
          "commitsBetweenForRepo": 99,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,4 +1,4 @@\n-    public BatchedEntries\u003cPathBasedCacheDirective\u003e makeRequest(\n+    public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(\n         Long nextKey) throws IOException {\n-      return namesystem.listPathBasedCacheDirectives(nextKey, filter);\n+      return namesystem.listCacheDirectives(nextKey, filter);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cCacheDirectiveEntry\u003e makeRequest(\n        Long nextKey) throws IOException {\n      return namesystem.listCacheDirectives(nextKey, filter);\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {}
        }
      ]
    },
    "f79b3e6b17450e9d34c483046b7437b09dd72016": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-5326. add modifyDirective to cacheAdmin (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1539839 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/11/13 2:07 PM",
      "commitName": "f79b3e6b17450e9d34c483046b7437b09dd72016",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-5326. add modifyDirective to cacheAdmin (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1539839 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/11/13 2:07 PM",
          "commitName": "f79b3e6b17450e9d34c483046b7437b09dd72016",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "21/10/13 12:29 PM",
          "commitNameOld": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 17.11,
          "commitsBetweenForRepo": 76,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,4 +1,4 @@\n-    public BatchedEntries\u003cPathBasedCacheDescriptor\u003e makeRequest(\n+    public BatchedEntries\u003cPathBasedCacheDirective\u003e makeRequest(\n         Long nextKey) throws IOException {\n-      return namesystem.listPathBasedCacheDescriptors(nextKey, pool, path);\n+      return namesystem.listPathBasedCacheDirectives(nextKey, filter);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cPathBasedCacheDirective\u003e makeRequest(\n        Long nextKey) throws IOException {\n      return namesystem.listPathBasedCacheDirectives(nextKey, filter);\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "BatchedEntries\u003cPathBasedCacheDescriptor\u003e",
            "newValue": "BatchedEntries\u003cPathBasedCacheDirective\u003e"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5326. add modifyDirective to cacheAdmin (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1539839 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "07/11/13 2:07 PM",
          "commitName": "f79b3e6b17450e9d34c483046b7437b09dd72016",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "21/10/13 12:29 PM",
          "commitNameOld": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 17.11,
          "commitsBetweenForRepo": 76,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,4 +1,4 @@\n-    public BatchedEntries\u003cPathBasedCacheDescriptor\u003e makeRequest(\n+    public BatchedEntries\u003cPathBasedCacheDirective\u003e makeRequest(\n         Long nextKey) throws IOException {\n-      return namesystem.listPathBasedCacheDescriptors(nextKey, pool, path);\n+      return namesystem.listPathBasedCacheDirectives(nextKey, filter);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cPathBasedCacheDirective\u003e makeRequest(\n        Long nextKey) throws IOException {\n      return namesystem.listPathBasedCacheDirectives(nextKey, filter);\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {}
        }
      ]
    },
    "e202d4d1548a0be2f5c61ff82be8b52bd0cfce04": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-5213. Separate PathBasedCacheEntry and PathBasedCacheDirectiveWithId. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1524561 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/09/13 1:43 PM",
      "commitName": "e202d4d1548a0be2f5c61ff82be8b52bd0cfce04",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-5213. Separate PathBasedCacheEntry and PathBasedCacheDirectiveWithId. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1524561 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/09/13 1:43 PM",
          "commitName": "e202d4d1548a0be2f5c61ff82be8b52bd0cfce04",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "13/09/13 4:27 PM",
          "commitNameOld": "40eb94ade3161d93e7a762a839004748f6d0ae89",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 4.89,
          "commitsBetweenForRepo": 4,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,4 +1,4 @@\n-    public BatchedEntries\u003cPathBasedCacheEntry\u003e makeRequest(\n+    public BatchedEntries\u003cPathBasedCacheDescriptor\u003e makeRequest(\n         Long nextKey) throws IOException {\n-      return namesystem.listPathBasedCacheEntries(nextKey, pool, path);\n+      return namesystem.listPathBasedCacheDescriptors(nextKey, pool, path);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cPathBasedCacheDescriptor\u003e makeRequest(\n        Long nextKey) throws IOException {\n      return namesystem.listPathBasedCacheDescriptors(nextKey, pool, path);\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "BatchedEntries\u003cPathBasedCacheEntry\u003e",
            "newValue": "BatchedEntries\u003cPathBasedCacheDescriptor\u003e"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5213. Separate PathBasedCacheEntry and PathBasedCacheDirectiveWithId. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1524561 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/09/13 1:43 PM",
          "commitName": "e202d4d1548a0be2f5c61ff82be8b52bd0cfce04",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "13/09/13 4:27 PM",
          "commitNameOld": "40eb94ade3161d93e7a762a839004748f6d0ae89",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 4.89,
          "commitsBetweenForRepo": 4,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,4 +1,4 @@\n-    public BatchedEntries\u003cPathBasedCacheEntry\u003e makeRequest(\n+    public BatchedEntries\u003cPathBasedCacheDescriptor\u003e makeRequest(\n         Long nextKey) throws IOException {\n-      return namesystem.listPathBasedCacheEntries(nextKey, pool, path);\n+      return namesystem.listPathBasedCacheDescriptors(nextKey, pool, path);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cPathBasedCacheDescriptor\u003e makeRequest(\n        Long nextKey) throws IOException {\n      return namesystem.listPathBasedCacheDescriptors(nextKey, pool, path);\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {}
        }
      ]
    },
    "02e0e158a26f81ce8375426ba0ea56db09ee36be": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-5158. Add command-line support for manipulating cache directives\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1522272 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/09/13 8:55 PM",
      "commitName": "02e0e158a26f81ce8375426ba0ea56db09ee36be",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-5158. Add command-line support for manipulating cache directives\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1522272 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/09/13 8:55 PM",
          "commitName": "02e0e158a26f81ce8375426ba0ea56db09ee36be",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "06/09/13 11:52 AM",
          "commitNameOld": "f41f8b8842c3f26d19f7fa928070c7c07f760e4c",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 5.38,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,4 +1,4 @@\n-    public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n+    public BatchedEntries\u003cPathBasedCacheEntry\u003e makeRequest(\n         Long nextKey) throws IOException {\n-      return namesystem.listPathCacheEntries(nextKey, pool);\n+      return namesystem.listPathBasedCacheEntries(nextKey, pool, path);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cPathBasedCacheEntry\u003e makeRequest(\n        Long nextKey) throws IOException {\n      return namesystem.listPathBasedCacheEntries(nextKey, pool, path);\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "BatchedEntries\u003cPathCacheEntry\u003e",
            "newValue": "BatchedEntries\u003cPathBasedCacheEntry\u003e"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5158. Add command-line support for manipulating cache directives\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1522272 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/09/13 8:55 PM",
          "commitName": "02e0e158a26f81ce8375426ba0ea56db09ee36be",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "06/09/13 11:52 AM",
          "commitNameOld": "f41f8b8842c3f26d19f7fa928070c7c07f760e4c",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 5.38,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,4 +1,4 @@\n-    public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n+    public BatchedEntries\u003cPathBasedCacheEntry\u003e makeRequest(\n         Long nextKey) throws IOException {\n-      return namesystem.listPathCacheEntries(nextKey, pool);\n+      return namesystem.listPathBasedCacheEntries(nextKey, pool, path);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cPathBasedCacheEntry\u003e makeRequest(\n        Long nextKey) throws IOException {\n      return namesystem.listPathBasedCacheEntries(nextKey, pool, path);\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {}
        }
      ]
    },
    "f41f8b8842c3f26d19f7fa928070c7c07f760e4c": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5163. Miscellaneous cache pool RPC fixes (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1520665 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/09/13 11:52 AM",
      "commitName": "f41f8b8842c3f26d19f7fa928070c7c07f760e4c",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5163. Miscellaneous cache pool RPC fixes (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1520665 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/09/13 11:52 AM",
          "commitName": "f41f8b8842c3f26d19f7fa928070c7c07f760e4c",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "04/09/13 11:23 AM",
          "commitNameOld": "d56d0b46e1b82ae068083ddb99872d314684dc82",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 2.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,6 +1,4 @@\n     public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n-        Long prevKey, int maxRepliesPerRequest) throws IOException {\n-      return new BatchedListEntries\u003cPathCacheEntry\u003e(\n-          namesystem.listPathCacheEntries(prevKey, poolId,\n-              maxRepliesPerRequest));\n+        Long nextKey) throws IOException {\n+      return namesystem.listPathCacheEntries(nextKey, pool);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n        Long nextKey) throws IOException {\n      return namesystem.listPathCacheEntries(nextKey, pool);\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "[prevKey-Long, maxRepliesPerRequest-int]",
            "newValue": "[nextKey-Long]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5163. Miscellaneous cache pool RPC fixes (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1520665 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/09/13 11:52 AM",
          "commitName": "f41f8b8842c3f26d19f7fa928070c7c07f760e4c",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "04/09/13 11:23 AM",
          "commitNameOld": "d56d0b46e1b82ae068083ddb99872d314684dc82",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 2.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,6 +1,4 @@\n     public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n-        Long prevKey, int maxRepliesPerRequest) throws IOException {\n-      return new BatchedListEntries\u003cPathCacheEntry\u003e(\n-          namesystem.listPathCacheEntries(prevKey, poolId,\n-              maxRepliesPerRequest));\n+        Long nextKey) throws IOException {\n+      return namesystem.listPathCacheEntries(nextKey, pool);\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n        Long nextKey) throws IOException {\n      return namesystem.listPathCacheEntries(nextKey, pool);\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {}
        }
      ]
    },
    "d56d0b46e1b82ae068083ddb99872d314684dc82": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "commit correct version of HDFS-5121\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1520090 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/13 11:23 AM",
      "commitName": "d56d0b46e1b82ae068083ddb99872d314684dc82",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "commit correct version of HDFS-5121\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1520090 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/09/13 11:23 AM",
          "commitName": "d56d0b46e1b82ae068083ddb99872d314684dc82",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "03/09/13 1:38 PM",
          "commitNameOld": "97b7267977ef42201e5844df49bc37ec3d10ce16",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 0.91,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,6 +1,6 @@\n     public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n-        Long nextKey, int maxRepliesPerRequest) throws IOException {\n+        Long prevKey, int maxRepliesPerRequest) throws IOException {\n       return new BatchedListEntries\u003cPathCacheEntry\u003e(\n-          namesystem.listPathCacheEntries(nextKey, pool,\n+          namesystem.listPathCacheEntries(prevKey, poolId,\n               maxRepliesPerRequest));\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n        Long prevKey, int maxRepliesPerRequest) throws IOException {\n      return new BatchedListEntries\u003cPathCacheEntry\u003e(\n          namesystem.listPathCacheEntries(prevKey, poolId,\n              maxRepliesPerRequest));\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "[nextKey-Long, maxRepliesPerRequest-int]",
            "newValue": "[prevKey-Long, maxRepliesPerRequest-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "commit correct version of HDFS-5121\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1520090 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/09/13 11:23 AM",
          "commitName": "d56d0b46e1b82ae068083ddb99872d314684dc82",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "03/09/13 1:38 PM",
          "commitNameOld": "97b7267977ef42201e5844df49bc37ec3d10ce16",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 0.91,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,6 +1,6 @@\n     public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n-        Long nextKey, int maxRepliesPerRequest) throws IOException {\n+        Long prevKey, int maxRepliesPerRequest) throws IOException {\n       return new BatchedListEntries\u003cPathCacheEntry\u003e(\n-          namesystem.listPathCacheEntries(nextKey, pool,\n+          namesystem.listPathCacheEntries(prevKey, poolId,\n               maxRepliesPerRequest));\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n        Long prevKey, int maxRepliesPerRequest) throws IOException {\n      return new BatchedListEntries\u003cPathCacheEntry\u003e(\n          namesystem.listPathCacheEntries(prevKey, poolId,\n              maxRepliesPerRequest));\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {}
        }
      ]
    },
    "97b7267977ef42201e5844df49bc37ec3d10ce16": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-5121.  Add RPCs for creating and manipulating cache pools.  (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1519841 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/09/13 1:38 PM",
      "commitName": "97b7267977ef42201e5844df49bc37ec3d10ce16",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5121.  Add RPCs for creating and manipulating cache pools.  (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1519841 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "03/09/13 1:38 PM",
          "commitName": "97b7267977ef42201e5844df49bc37ec3d10ce16",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "30/08/13 3:15 PM",
          "commitNameOld": "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 3.93,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,9 +1,6 @@\n-    private void makeRequest() throws IOException {\n-      idx \u003d 0;\n-      entries \u003d null;\n-      entries \u003d namesystem.listPathCacheEntries(prevId, pool,\n-          repliesPerRequest);\n-      if (entries.isEmpty()) {\n-        entries \u003d null;\n-      }\n+    public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n+        Long nextKey, int maxRepliesPerRequest) throws IOException {\n+      return new BatchedListEntries\u003cPathCacheEntry\u003e(\n+          namesystem.listPathCacheEntries(nextKey, pool,\n+              maxRepliesPerRequest));\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n        Long nextKey, int maxRepliesPerRequest) throws IOException {\n      return new BatchedListEntries\u003cPathCacheEntry\u003e(\n          namesystem.listPathCacheEntries(nextKey, pool,\n              maxRepliesPerRequest));\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[nextKey-Long, maxRepliesPerRequest-int]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-5121.  Add RPCs for creating and manipulating cache pools.  (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1519841 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "03/09/13 1:38 PM",
          "commitName": "97b7267977ef42201e5844df49bc37ec3d10ce16",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "30/08/13 3:15 PM",
          "commitNameOld": "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 3.93,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,9 +1,6 @@\n-    private void makeRequest() throws IOException {\n-      idx \u003d 0;\n-      entries \u003d null;\n-      entries \u003d namesystem.listPathCacheEntries(prevId, pool,\n-          repliesPerRequest);\n-      if (entries.isEmpty()) {\n-        entries \u003d null;\n-      }\n+    public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n+        Long nextKey, int maxRepliesPerRequest) throws IOException {\n+      return new BatchedListEntries\u003cPathCacheEntry\u003e(\n+          namesystem.listPathCacheEntries(nextKey, pool,\n+              maxRepliesPerRequest));\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n        Long nextKey, int maxRepliesPerRequest) throws IOException {\n      return new BatchedListEntries\u003cPathCacheEntry\u003e(\n          namesystem.listPathCacheEntries(nextKey, pool,\n              maxRepliesPerRequest));\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "BatchedEntries\u003cPathCacheEntry\u003e"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5121.  Add RPCs for creating and manipulating cache pools.  (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1519841 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "03/09/13 1:38 PM",
          "commitName": "97b7267977ef42201e5844df49bc37ec3d10ce16",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "30/08/13 3:15 PM",
          "commitNameOld": "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 3.93,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,9 +1,6 @@\n-    private void makeRequest() throws IOException {\n-      idx \u003d 0;\n-      entries \u003d null;\n-      entries \u003d namesystem.listPathCacheEntries(prevId, pool,\n-          repliesPerRequest);\n-      if (entries.isEmpty()) {\n-        entries \u003d null;\n-      }\n+    public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n+        Long nextKey, int maxRepliesPerRequest) throws IOException {\n+      return new BatchedListEntries\u003cPathCacheEntry\u003e(\n+          namesystem.listPathCacheEntries(nextKey, pool,\n+              maxRepliesPerRequest));\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n        Long nextKey, int maxRepliesPerRequest) throws IOException {\n      return new BatchedListEntries\u003cPathCacheEntry\u003e(\n          namesystem.listPathCacheEntries(nextKey, pool,\n              maxRepliesPerRequest));\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5121.  Add RPCs for creating and manipulating cache pools.  (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1519841 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "03/09/13 1:38 PM",
          "commitName": "97b7267977ef42201e5844df49bc37ec3d10ce16",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "30/08/13 3:15 PM",
          "commitNameOld": "fc14a92c6b46cc435a8f33e6fa0512c70caa06e0",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 3.93,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,9 +1,6 @@\n-    private void makeRequest() throws IOException {\n-      idx \u003d 0;\n-      entries \u003d null;\n-      entries \u003d namesystem.listPathCacheEntries(prevId, pool,\n-          repliesPerRequest);\n-      if (entries.isEmpty()) {\n-        entries \u003d null;\n-      }\n+    public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n+        Long nextKey, int maxRepliesPerRequest) throws IOException {\n+      return new BatchedListEntries\u003cPathCacheEntry\u003e(\n+          namesystem.listPathCacheEntries(nextKey, pool,\n+              maxRepliesPerRequest));\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public BatchedEntries\u003cPathCacheEntry\u003e makeRequest(\n        Long nextKey, int maxRepliesPerRequest) throws IOException {\n      return new BatchedListEntries\u003cPathCacheEntry\u003e(\n          namesystem.listPathCacheEntries(nextKey, pool,\n              maxRepliesPerRequest));\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {}
        }
      ]
    },
    "920b4cc06f1bc15809902bdd1968cc434a694a08": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5052. Add cacheRequest/uncacheRequest support to NameNode.  (Contributed by Colin Patrick McCabe.)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1516669 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/08/13 4:37 PM",
      "commitName": "920b4cc06f1bc15809902bdd1968cc434a694a08",
      "commitAuthor": "Colin McCabe",
      "diff": "@@ -0,0 +1,9 @@\n+    private void makeRequest() throws IOException {\n+      idx \u003d 0;\n+      entries \u003d null;\n+      entries \u003d namesystem.listPathCacheEntries(prevId, pool,\n+          repliesPerRequest);\n+      if (entries.isEmpty()) {\n+        entries \u003d null;\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void makeRequest() throws IOException {\n      idx \u003d 0;\n      entries \u003d null;\n      entries \u003d namesystem.listPathCacheEntries(prevId, pool,\n          repliesPerRequest);\n      if (entries.isEmpty()) {\n        entries \u003d null;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java"
    }
  }
}