{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Balancer.java",
  "functionName": "init",
  "functionId": "init___reports-List__DatanodeStorageReport__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
  "functionStartLine": 365,
  "functionEndLine": 430,
  "numCommitsSeen": 199,
  "timeTaken": 9565,
  "changeHistory": [
    "7ecbfd44aa57f5f54c214b7fdedda2500be76f51",
    "51a00964da0e399718d1cec25ff692a32d7642b7",
    "b56daff6a186599764b046248565918b894ec116",
    "a7a7768341f1b7d3a8f2686e2f4d00c57f2e1d4f",
    "e8e7fbe81abc64a9ae3d2f3f62c088426073b2bf",
    "5d5aae0694bc27df5b9fa50819854cd3050a8658",
    "e60673697d5046c29c52bbabdfe80506f99773e4",
    "c3cf331dc91e2beef2afeed11105084843b02858",
    "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
    "b8b8f3f5e7214d6fcfc30e1b94ff097e52868f4f",
    "e3612e442809310c67bc2ed4376e028c4ab8d597",
    "907fb15ee8c150e5ecc0560b7374441c57a84122",
    "4551da302d94cffea0313eac79479ab6f9b7cb34",
    "abf09f090f77a7e54e331b7a07354e7926b60dc9",
    "e4df14f8f151413a8ec0972a21e31a0b51fe0fb0",
    "7dd869c2a98998aea457e522ad4b7c5403312482",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "7ecbfd44aa57f5f54c214b7fdedda2500be76f51": "Ybodychange",
    "51a00964da0e399718d1cec25ff692a32d7642b7": "Ybodychange",
    "b56daff6a186599764b046248565918b894ec116": "Ybodychange",
    "a7a7768341f1b7d3a8f2686e2f4d00c57f2e1d4f": "Ybodychange",
    "e8e7fbe81abc64a9ae3d2f3f62c088426073b2bf": "Ybodychange",
    "5d5aae0694bc27df5b9fa50819854cd3050a8658": "Ybodychange",
    "e60673697d5046c29c52bbabdfe80506f99773e4": "Ybodychange",
    "c3cf331dc91e2beef2afeed11105084843b02858": "Ymultichange(Yparameterchange,Ybodychange)",
    "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d": "Ymultichange(Yrename,Yparameterchange,Ybodychange)",
    "b8b8f3f5e7214d6fcfc30e1b94ff097e52868f4f": "Ybodychange",
    "e3612e442809310c67bc2ed4376e028c4ab8d597": "Ybodychange",
    "907fb15ee8c150e5ecc0560b7374441c57a84122": "Ybodychange",
    "4551da302d94cffea0313eac79479ab6f9b7cb34": "Ybodychange",
    "abf09f090f77a7e54e331b7a07354e7926b60dc9": "Ybodychange",
    "e4df14f8f151413a8ec0972a21e31a0b51fe0fb0": "Ybodychange",
    "7dd869c2a98998aea457e522ad4b7c5403312482": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7ecbfd44aa57f5f54c214b7fdedda2500be76f51": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8826. In Balancer, add an option to specify the source node list so that balancer only selects blocks to move from those nodes.\n",
      "commitDate": "18/08/15 7:25 PM",
      "commitName": "7ecbfd44aa57f5f54c214b7fdedda2500be76f51",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "17/08/15 5:55 PM",
      "commitNameOld": "51a00964da0e399718d1cec25ff692a32d7642b7",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 1.06,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,66 @@\n   private long init(List\u003cDatanodeStorageReport\u003e reports) {\n     // compute average utilization\n     for (DatanodeStorageReport r : reports) {\n       policy.accumulateSpaces(r);\n     }\n     policy.initAvgUtilization();\n \n     // create network topology and classify utilization collections: \n     //   over-utilized, above-average, below-average and under-utilized.\n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n     for(DatanodeStorageReport r : reports) {\n       final DDatanode dn \u003d dispatcher.newDatanode(r.getDatanodeInfo());\n+      final boolean isSource \u003d Util.isIncluded(sourceNodes, dn.getDatanodeInfo());\n       for(StorageType t : StorageType.getMovableTypes()) {\n         final Double utilization \u003d policy.getUtilization(r, t);\n         if (utilization \u003d\u003d null) { // datanode does not have such storage type \n           continue;\n         }\n         \n+        final double average \u003d policy.getAvgUtilization(t);\n+        if (utilization \u003e\u003d average \u0026\u0026 !isSource) {\n+          LOG.info(dn + \"[\" + t + \"] has utilization\u003d\" + utilization\n+              + \" \u003e\u003d average\u003d\" + average\n+              + \" but it is not specified as a source; skipping it.\");\n+          continue;\n+        }\n+\n+        final double utilizationDiff \u003d utilization - average;\n         final long capacity \u003d getCapacity(r, t);\n-        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n         final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n         final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n             getRemaining(r, t), utilizationDiff, maxSizeToMove);\n \n         final StorageGroup g;\n         if (utilizationDiff \u003e 0) {\n           final Source s \u003d dn.addSource(t, maxSize2Move, dispatcher);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             aboveAvgUtilized.add(s);\n           } else {\n             overLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n             overUtilized.add(s);\n           }\n           g \u003d s;\n         } else {\n           g \u003d dn.addTarget(t, maxSize2Move);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             belowAvgUtilized.add(g);\n           } else {\n             underLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n             underUtilized.add(g);\n           }\n         }\n         dispatcher.getStorageGroupMap().put(g);\n       }\n     }\n \n     logUtilizationCollections();\n     \n     Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n         \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n            + belowAvgUtilized.size(),\n         \"Mismatched number of storage groups\");\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long init(List\u003cDatanodeStorageReport\u003e reports) {\n    // compute average utilization\n    for (DatanodeStorageReport r : reports) {\n      policy.accumulateSpaces(r);\n    }\n    policy.initAvgUtilization();\n\n    // create network topology and classify utilization collections: \n    //   over-utilized, above-average, below-average and under-utilized.\n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for(DatanodeStorageReport r : reports) {\n      final DDatanode dn \u003d dispatcher.newDatanode(r.getDatanodeInfo());\n      final boolean isSource \u003d Util.isIncluded(sourceNodes, dn.getDatanodeInfo());\n      for(StorageType t : StorageType.getMovableTypes()) {\n        final Double utilization \u003d policy.getUtilization(r, t);\n        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n          continue;\n        }\n        \n        final double average \u003d policy.getAvgUtilization(t);\n        if (utilization \u003e\u003d average \u0026\u0026 !isSource) {\n          LOG.info(dn + \"[\" + t + \"] has utilization\u003d\" + utilization\n              + \" \u003e\u003d average\u003d\" + average\n              + \" but it is not specified as a source; skipping it.\");\n          continue;\n        }\n\n        final double utilizationDiff \u003d utilization - average;\n        final long capacity \u003d getCapacity(r, t);\n        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n            getRemaining(r, t), utilizationDiff, maxSizeToMove);\n\n        final StorageGroup g;\n        if (utilizationDiff \u003e 0) {\n          final Source s \u003d dn.addSource(t, maxSize2Move, dispatcher);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            aboveAvgUtilized.add(s);\n          } else {\n            overLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n            overUtilized.add(s);\n          }\n          g \u003d s;\n        } else {\n          g \u003d dn.addTarget(t, maxSize2Move);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            belowAvgUtilized.add(g);\n          } else {\n            underLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n            underUtilized.add(g);\n          }\n        }\n        dispatcher.getStorageGroupMap().put(g);\n      }\n    }\n\n    logUtilizationCollections();\n    \n    Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n        \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n           + belowAvgUtilized.size(),\n        \"Mismatched number of storage groups\");\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "51a00964da0e399718d1cec25ff692a32d7642b7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8278. When computing max-size-to-move in Balancer, count only the storage with remaining \u003e\u003d default block size.\n",
      "commitDate": "17/08/15 5:55 PM",
      "commitName": "51a00964da0e399718d1cec25ff692a32d7642b7",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "14/08/15 1:03 PM",
      "commitNameOld": "2bc0a4f299fbd8035e29f62ce9cd22e209a62805",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 3.2,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   private long init(List\u003cDatanodeStorageReport\u003e reports) {\n     // compute average utilization\n     for (DatanodeStorageReport r : reports) {\n       policy.accumulateSpaces(r);\n     }\n     policy.initAvgUtilization();\n \n     // create network topology and classify utilization collections: \n     //   over-utilized, above-average, below-average and under-utilized.\n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n     for(DatanodeStorageReport r : reports) {\n       final DDatanode dn \u003d dispatcher.newDatanode(r.getDatanodeInfo());\n       for(StorageType t : StorageType.getMovableTypes()) {\n         final Double utilization \u003d policy.getUtilization(r, t);\n         if (utilization \u003d\u003d null) { // datanode does not have such storage type \n           continue;\n         }\n         \n         final long capacity \u003d getCapacity(r, t);\n         final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n         final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n         final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n-            getRemaining(r, t), utilizationDiff, threshold, maxSizeToMove);\n+            getRemaining(r, t), utilizationDiff, maxSizeToMove);\n \n         final StorageGroup g;\n         if (utilizationDiff \u003e 0) {\n           final Source s \u003d dn.addSource(t, maxSize2Move, dispatcher);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             aboveAvgUtilized.add(s);\n           } else {\n             overLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n             overUtilized.add(s);\n           }\n           g \u003d s;\n         } else {\n           g \u003d dn.addTarget(t, maxSize2Move);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             belowAvgUtilized.add(g);\n           } else {\n             underLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n             underUtilized.add(g);\n           }\n         }\n         dispatcher.getStorageGroupMap().put(g);\n       }\n     }\n \n     logUtilizationCollections();\n     \n     Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n         \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n            + belowAvgUtilized.size(),\n         \"Mismatched number of storage groups\");\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long init(List\u003cDatanodeStorageReport\u003e reports) {\n    // compute average utilization\n    for (DatanodeStorageReport r : reports) {\n      policy.accumulateSpaces(r);\n    }\n    policy.initAvgUtilization();\n\n    // create network topology and classify utilization collections: \n    //   over-utilized, above-average, below-average and under-utilized.\n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for(DatanodeStorageReport r : reports) {\n      final DDatanode dn \u003d dispatcher.newDatanode(r.getDatanodeInfo());\n      for(StorageType t : StorageType.getMovableTypes()) {\n        final Double utilization \u003d policy.getUtilization(r, t);\n        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n          continue;\n        }\n        \n        final long capacity \u003d getCapacity(r, t);\n        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n            getRemaining(r, t), utilizationDiff, maxSizeToMove);\n\n        final StorageGroup g;\n        if (utilizationDiff \u003e 0) {\n          final Source s \u003d dn.addSource(t, maxSize2Move, dispatcher);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            aboveAvgUtilized.add(s);\n          } else {\n            overLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n            overUtilized.add(s);\n          }\n          g \u003d s;\n        } else {\n          g \u003d dn.addTarget(t, maxSize2Move);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            belowAvgUtilized.add(g);\n          } else {\n            underLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n            underUtilized.add(g);\n          }\n        }\n        dispatcher.getStorageGroupMap().put(g);\n      }\n    }\n\n    logUtilizationCollections();\n    \n    Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n        \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n           + belowAvgUtilized.size(),\n        \"Mismatched number of storage groups\");\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "b56daff6a186599764b046248565918b894ec116": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8818. Changes the global moveExecutor to per datanode executors and changes MAX_SIZE_TO_MOVE to be configurable.\n",
      "commitDate": "10/08/15 4:52 PM",
      "commitName": "b56daff6a186599764b046248565918b894ec116",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "10/06/15 1:42 PM",
      "commitNameOld": "a7a7768341f1b7d3a8f2686e2f4d00c57f2e1d4f",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 61.13,
      "commitsBetweenForRepo": 359,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   private long init(List\u003cDatanodeStorageReport\u003e reports) {\n     // compute average utilization\n     for (DatanodeStorageReport r : reports) {\n       policy.accumulateSpaces(r);\n     }\n     policy.initAvgUtilization();\n \n     // create network topology and classify utilization collections: \n     //   over-utilized, above-average, below-average and under-utilized.\n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n     for(DatanodeStorageReport r : reports) {\n       final DDatanode dn \u003d dispatcher.newDatanode(r.getDatanodeInfo());\n       for(StorageType t : StorageType.getMovableTypes()) {\n         final Double utilization \u003d policy.getUtilization(r, t);\n         if (utilization \u003d\u003d null) { // datanode does not have such storage type \n           continue;\n         }\n         \n         final long capacity \u003d getCapacity(r, t);\n         final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n         final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n         final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n-            getRemaining(r, t), utilizationDiff, threshold);\n+            getRemaining(r, t), utilizationDiff, threshold, maxSizeToMove);\n \n         final StorageGroup g;\n         if (utilizationDiff \u003e 0) {\n           final Source s \u003d dn.addSource(t, maxSize2Move, dispatcher);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             aboveAvgUtilized.add(s);\n           } else {\n             overLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n             overUtilized.add(s);\n           }\n           g \u003d s;\n         } else {\n           g \u003d dn.addTarget(t, maxSize2Move);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             belowAvgUtilized.add(g);\n           } else {\n             underLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n             underUtilized.add(g);\n           }\n         }\n         dispatcher.getStorageGroupMap().put(g);\n       }\n     }\n \n     logUtilizationCollections();\n     \n     Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n         \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n            + belowAvgUtilized.size(),\n         \"Mismatched number of storage groups\");\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long init(List\u003cDatanodeStorageReport\u003e reports) {\n    // compute average utilization\n    for (DatanodeStorageReport r : reports) {\n      policy.accumulateSpaces(r);\n    }\n    policy.initAvgUtilization();\n\n    // create network topology and classify utilization collections: \n    //   over-utilized, above-average, below-average and under-utilized.\n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for(DatanodeStorageReport r : reports) {\n      final DDatanode dn \u003d dispatcher.newDatanode(r.getDatanodeInfo());\n      for(StorageType t : StorageType.getMovableTypes()) {\n        final Double utilization \u003d policy.getUtilization(r, t);\n        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n          continue;\n        }\n        \n        final long capacity \u003d getCapacity(r, t);\n        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n            getRemaining(r, t), utilizationDiff, threshold, maxSizeToMove);\n\n        final StorageGroup g;\n        if (utilizationDiff \u003e 0) {\n          final Source s \u003d dn.addSource(t, maxSize2Move, dispatcher);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            aboveAvgUtilized.add(s);\n          } else {\n            overLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n            overUtilized.add(s);\n          }\n          g \u003d s;\n        } else {\n          g \u003d dn.addTarget(t, maxSize2Move);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            belowAvgUtilized.add(g);\n          } else {\n            underLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n            underUtilized.add(g);\n          }\n        }\n        dispatcher.getStorageGroupMap().put(g);\n      }\n    }\n\n    logUtilizationCollections();\n    \n    Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n        \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n           + belowAvgUtilized.size(),\n        \"Mismatched number of storage groups\");\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "a7a7768341f1b7d3a8f2686e2f4d00c57f2e1d4f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8549. Abort the balancer if an upgrade is in progress.\n",
      "commitDate": "10/06/15 1:42 PM",
      "commitName": "a7a7768341f1b7d3a8f2686e2f4d00c57f2e1d4f",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "20/03/15 12:02 PM",
      "commitNameOld": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 82.07,
      "commitsBetweenForRepo": 760,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   private long init(List\u003cDatanodeStorageReport\u003e reports) {\n     // compute average utilization\n     for (DatanodeStorageReport r : reports) {\n       policy.accumulateSpaces(r);\n     }\n     policy.initAvgUtilization();\n \n     // create network topology and classify utilization collections: \n     //   over-utilized, above-average, below-average and under-utilized.\n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n     for(DatanodeStorageReport r : reports) {\n       final DDatanode dn \u003d dispatcher.newDatanode(r.getDatanodeInfo());\n       for(StorageType t : StorageType.getMovableTypes()) {\n         final Double utilization \u003d policy.getUtilization(r, t);\n         if (utilization \u003d\u003d null) { // datanode does not have such storage type \n           continue;\n         }\n         \n         final long capacity \u003d getCapacity(r, t);\n         final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n         final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n         final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n             getRemaining(r, t), utilizationDiff, threshold);\n \n         final StorageGroup g;\n         if (utilizationDiff \u003e 0) {\n           final Source s \u003d dn.addSource(t, maxSize2Move, dispatcher);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             aboveAvgUtilized.add(s);\n           } else {\n-            overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n+            overLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n             overUtilized.add(s);\n           }\n           g \u003d s;\n         } else {\n           g \u003d dn.addTarget(t, maxSize2Move);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             belowAvgUtilized.add(g);\n           } else {\n-            underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n+            underLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n             underUtilized.add(g);\n           }\n         }\n         dispatcher.getStorageGroupMap().put(g);\n       }\n     }\n \n     logUtilizationCollections();\n     \n     Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n         \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n            + belowAvgUtilized.size(),\n         \"Mismatched number of storage groups\");\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long init(List\u003cDatanodeStorageReport\u003e reports) {\n    // compute average utilization\n    for (DatanodeStorageReport r : reports) {\n      policy.accumulateSpaces(r);\n    }\n    policy.initAvgUtilization();\n\n    // create network topology and classify utilization collections: \n    //   over-utilized, above-average, below-average and under-utilized.\n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for(DatanodeStorageReport r : reports) {\n      final DDatanode dn \u003d dispatcher.newDatanode(r.getDatanodeInfo());\n      for(StorageType t : StorageType.getMovableTypes()) {\n        final Double utilization \u003d policy.getUtilization(r, t);\n        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n          continue;\n        }\n        \n        final long capacity \u003d getCapacity(r, t);\n        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n            getRemaining(r, t), utilizationDiff, threshold);\n\n        final StorageGroup g;\n        if (utilizationDiff \u003e 0) {\n          final Source s \u003d dn.addSource(t, maxSize2Move, dispatcher);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            aboveAvgUtilized.add(s);\n          } else {\n            overLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n            overUtilized.add(s);\n          }\n          g \u003d s;\n        } else {\n          g \u003d dn.addTarget(t, maxSize2Move);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            belowAvgUtilized.add(g);\n          } else {\n            underLoadedBytes +\u003d percentage2bytes(thresholdDiff, capacity);\n            underUtilized.add(g);\n          }\n        }\n        dispatcher.getStorageGroupMap().put(g);\n      }\n    }\n\n    logUtilizationCollections();\n    \n    Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n        \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n           + belowAvgUtilized.size(),\n        \"Mismatched number of storage groups\");\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "e8e7fbe81abc64a9ae3d2f3f62c088426073b2bf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6932. Balancer and Mover tools should ignore replicas on RAM_DISK. (Contributed by Xiaoyu Yao)\n",
      "commitDate": "24/09/14 9:08 PM",
      "commitName": "e8e7fbe81abc64a9ae3d2f3f62c088426073b2bf",
      "commitAuthor": "arp",
      "commitDateOld": "09/09/14 10:47 PM",
      "commitNameOld": "db41a1b7b944185e5e60a7fdf8cf43172b006a73",
      "commitAuthorOld": "",
      "daysBetweenCommits": 14.93,
      "commitsBetweenForRepo": 177,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   private long init(List\u003cDatanodeStorageReport\u003e reports) {\n     // compute average utilization\n     for (DatanodeStorageReport r : reports) {\n       policy.accumulateSpaces(r);\n     }\n     policy.initAvgUtilization();\n \n     // create network topology and classify utilization collections: \n     //   over-utilized, above-average, below-average and under-utilized.\n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n     for(DatanodeStorageReport r : reports) {\n       final DDatanode dn \u003d dispatcher.newDatanode(r.getDatanodeInfo());\n-      for(StorageType t : StorageType.asList()) {\n+      for(StorageType t : StorageType.getMovableTypes()) {\n         final Double utilization \u003d policy.getUtilization(r, t);\n         if (utilization \u003d\u003d null) { // datanode does not have such storage type \n           continue;\n         }\n         \n         final long capacity \u003d getCapacity(r, t);\n         final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n         final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n         final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n             getRemaining(r, t), utilizationDiff, threshold);\n \n         final StorageGroup g;\n         if (utilizationDiff \u003e 0) {\n           final Source s \u003d dn.addSource(t, maxSize2Move, dispatcher);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             aboveAvgUtilized.add(s);\n           } else {\n             overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n             overUtilized.add(s);\n           }\n           g \u003d s;\n         } else {\n           g \u003d dn.addTarget(t, maxSize2Move);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             belowAvgUtilized.add(g);\n           } else {\n             underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n             underUtilized.add(g);\n           }\n         }\n         dispatcher.getStorageGroupMap().put(g);\n       }\n     }\n \n     logUtilizationCollections();\n     \n     Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n         \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n            + belowAvgUtilized.size(),\n         \"Mismatched number of storage groups\");\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long init(List\u003cDatanodeStorageReport\u003e reports) {\n    // compute average utilization\n    for (DatanodeStorageReport r : reports) {\n      policy.accumulateSpaces(r);\n    }\n    policy.initAvgUtilization();\n\n    // create network topology and classify utilization collections: \n    //   over-utilized, above-average, below-average and under-utilized.\n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for(DatanodeStorageReport r : reports) {\n      final DDatanode dn \u003d dispatcher.newDatanode(r.getDatanodeInfo());\n      for(StorageType t : StorageType.getMovableTypes()) {\n        final Double utilization \u003d policy.getUtilization(r, t);\n        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n          continue;\n        }\n        \n        final long capacity \u003d getCapacity(r, t);\n        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n            getRemaining(r, t), utilizationDiff, threshold);\n\n        final StorageGroup g;\n        if (utilizationDiff \u003e 0) {\n          final Source s \u003d dn.addSource(t, maxSize2Move, dispatcher);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            aboveAvgUtilized.add(s);\n          } else {\n            overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            overUtilized.add(s);\n          }\n          g \u003d s;\n        } else {\n          g \u003d dn.addTarget(t, maxSize2Move);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            belowAvgUtilized.add(g);\n          } else {\n            underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            underUtilized.add(g);\n          }\n        }\n        dispatcher.getStorageGroupMap().put(g);\n      }\n    }\n\n    logUtilizationCollections();\n    \n    Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n        \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n           + belowAvgUtilized.size(),\n        \"Mismatched number of storage groups\");\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "5d5aae0694bc27df5b9fa50819854cd3050a8658": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6801. Archival Storage: Add a new data migration tool. Contributed by Tsz Wo Nicholas Sze.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-6584@1618675 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/14 10:51 AM",
      "commitName": "5d5aae0694bc27df5b9fa50819854cd3050a8658",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "11/08/14 11:01 AM",
      "commitNameOld": "e60673697d5046c29c52bbabdfe80506f99773e4",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 6.99,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   private long init(List\u003cDatanodeStorageReport\u003e reports) {\n     // compute average utilization\n     for (DatanodeStorageReport r : reports) {\n       policy.accumulateSpaces(r);\n     }\n     policy.initAvgUtilization();\n \n     // create network topology and classify utilization collections: \n     //   over-utilized, above-average, below-average and under-utilized.\n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n     for(DatanodeStorageReport r : reports) {\n-      final DDatanode dn \u003d dispatcher.newDatanode(r);\n+      final DDatanode dn \u003d dispatcher.newDatanode(r.getDatanodeInfo());\n       for(StorageType t : StorageType.asList()) {\n         final Double utilization \u003d policy.getUtilization(r, t);\n         if (utilization \u003d\u003d null) { // datanode does not have such storage type \n           continue;\n         }\n         \n         final long capacity \u003d getCapacity(r, t);\n         final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n         final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n         final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n             getRemaining(r, t), utilizationDiff, threshold);\n \n         final StorageGroup g;\n         if (utilizationDiff \u003e 0) {\n           final Source s \u003d dn.addSource(t, maxSize2Move, dispatcher);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             aboveAvgUtilized.add(s);\n           } else {\n             overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n             overUtilized.add(s);\n           }\n           g \u003d s;\n         } else {\n-          g \u003d dn.addStorageGroup(t, maxSize2Move);\n+          g \u003d dn.addTarget(t, maxSize2Move);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             belowAvgUtilized.add(g);\n           } else {\n             underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n             underUtilized.add(g);\n           }\n         }\n         dispatcher.getStorageGroupMap().put(g);\n       }\n     }\n \n     logUtilizationCollections();\n     \n     Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n         \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n            + belowAvgUtilized.size(),\n         \"Mismatched number of storage groups\");\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long init(List\u003cDatanodeStorageReport\u003e reports) {\n    // compute average utilization\n    for (DatanodeStorageReport r : reports) {\n      policy.accumulateSpaces(r);\n    }\n    policy.initAvgUtilization();\n\n    // create network topology and classify utilization collections: \n    //   over-utilized, above-average, below-average and under-utilized.\n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for(DatanodeStorageReport r : reports) {\n      final DDatanode dn \u003d dispatcher.newDatanode(r.getDatanodeInfo());\n      for(StorageType t : StorageType.asList()) {\n        final Double utilization \u003d policy.getUtilization(r, t);\n        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n          continue;\n        }\n        \n        final long capacity \u003d getCapacity(r, t);\n        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n            getRemaining(r, t), utilizationDiff, threshold);\n\n        final StorageGroup g;\n        if (utilizationDiff \u003e 0) {\n          final Source s \u003d dn.addSource(t, maxSize2Move, dispatcher);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            aboveAvgUtilized.add(s);\n          } else {\n            overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            overUtilized.add(s);\n          }\n          g \u003d s;\n        } else {\n          g \u003d dn.addTarget(t, maxSize2Move);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            belowAvgUtilized.add(g);\n          } else {\n            underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            underUtilized.add(g);\n          }\n        }\n        dispatcher.getStorageGroupMap().put(g);\n      }\n    }\n\n    logUtilizationCollections();\n    \n    Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n        \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n           + belowAvgUtilized.size(),\n        \"Mismatched number of storage groups\");\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "e60673697d5046c29c52bbabdfe80506f99773e4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6837. Code cleanup for Balancer and Dispatcher. Contributed by Tsz Wo Nicholas Sze.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1617337 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/08/14 11:01 AM",
      "commitName": "e60673697d5046c29c52bbabdfe80506f99773e4",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "08/08/14 2:33 PM",
      "commitNameOld": "c3cf331dc91e2beef2afeed11105084843b02858",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.85,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   private long init(List\u003cDatanodeStorageReport\u003e reports) {\n     // compute average utilization\n     for (DatanodeStorageReport r : reports) {\n       policy.accumulateSpaces(r);\n     }\n     policy.initAvgUtilization();\n \n     // create network topology and classify utilization collections: \n     //   over-utilized, above-average, below-average and under-utilized.\n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n     for(DatanodeStorageReport r : reports) {\n-      final BalancerDatanode dn \u003d dispatcher.newDatanode(r);\n+      final DDatanode dn \u003d dispatcher.newDatanode(r);\n       for(StorageType t : StorageType.asList()) {\n         final Double utilization \u003d policy.getUtilization(r, t);\n         if (utilization \u003d\u003d null) { // datanode does not have such storage type \n           continue;\n         }\n         \n         final long capacity \u003d getCapacity(r, t);\n         final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n         final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n         final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n             getRemaining(r, t), utilizationDiff, threshold);\n \n-        final BalancerDatanode.StorageGroup g;\n+        final StorageGroup g;\n         if (utilizationDiff \u003e 0) {\n-          final Source s \u003d dn.addSource(t, utilization, maxSize2Move, dispatcher);\n+          final Source s \u003d dn.addSource(t, maxSize2Move, dispatcher);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             aboveAvgUtilized.add(s);\n           } else {\n             overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n             overUtilized.add(s);\n           }\n           g \u003d s;\n         } else {\n-          g \u003d dn.addStorageGroup(t, utilization, maxSize2Move);\n+          g \u003d dn.addStorageGroup(t, maxSize2Move);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             belowAvgUtilized.add(g);\n           } else {\n             underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n             underUtilized.add(g);\n           }\n         }\n         dispatcher.getStorageGroupMap().put(g);\n       }\n     }\n \n     logUtilizationCollections();\n     \n     Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n         \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n            + belowAvgUtilized.size(),\n         \"Mismatched number of storage groups\");\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long init(List\u003cDatanodeStorageReport\u003e reports) {\n    // compute average utilization\n    for (DatanodeStorageReport r : reports) {\n      policy.accumulateSpaces(r);\n    }\n    policy.initAvgUtilization();\n\n    // create network topology and classify utilization collections: \n    //   over-utilized, above-average, below-average and under-utilized.\n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for(DatanodeStorageReport r : reports) {\n      final DDatanode dn \u003d dispatcher.newDatanode(r);\n      for(StorageType t : StorageType.asList()) {\n        final Double utilization \u003d policy.getUtilization(r, t);\n        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n          continue;\n        }\n        \n        final long capacity \u003d getCapacity(r, t);\n        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n            getRemaining(r, t), utilizationDiff, threshold);\n\n        final StorageGroup g;\n        if (utilizationDiff \u003e 0) {\n          final Source s \u003d dn.addSource(t, maxSize2Move, dispatcher);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            aboveAvgUtilized.add(s);\n          } else {\n            overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            overUtilized.add(s);\n          }\n          g \u003d s;\n        } else {\n          g \u003d dn.addStorageGroup(t, maxSize2Move);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            belowAvgUtilized.add(g);\n          } else {\n            underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            underUtilized.add(g);\n          }\n        }\n        dispatcher.getStorageGroupMap().put(g);\n      }\n    }\n\n    logUtilizationCollections();\n    \n    Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n        \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n           + belowAvgUtilized.size(),\n        \"Mismatched number of storage groups\");\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "c3cf331dc91e2beef2afeed11105084843b02858": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6828. Separate block replica dispatching from Balancer. Contributed by Tsz Wo Nicholas Sze.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1616889 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/08/14 2:33 PM",
      "commitName": "c3cf331dc91e2beef2afeed11105084843b02858",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6828. Separate block replica dispatching from Balancer. Contributed by Tsz Wo Nicholas Sze.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1616889 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/08/14 2:33 PM",
          "commitName": "c3cf331dc91e2beef2afeed11105084843b02858",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "07/08/14 12:18 AM",
          "commitNameOld": "83b9933db3349e6a6faf23bce35c9d4ce3f7bcf2",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 1.59,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,57 @@\n-  private long init(DatanodeStorageReport[] reports) {\n+  private long init(List\u003cDatanodeStorageReport\u003e reports) {\n     // compute average utilization\n     for (DatanodeStorageReport r : reports) {\n-      if (shouldIgnore(r.getDatanodeInfo())) {\n-        continue; \n-      }\n       policy.accumulateSpaces(r);\n     }\n     policy.initAvgUtilization();\n \n     // create network topology and classify utilization collections: \n     //   over-utilized, above-average, below-average and under-utilized.\n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n-    for(DatanodeStorageReport r : DFSUtil.shuffle(reports)) {\n-      final DatanodeInfo datanode \u003d r.getDatanodeInfo();\n-      if (shouldIgnore(datanode)) {\n-        continue; // ignore decommissioning or decommissioned nodes\n-      }\n-      cluster.add(datanode);\n-\n-      final BalancerDatanode dn \u003d new BalancerDatanode(r, underLoadedBytes,\n-          maxConcurrentMovesPerNode);\n+    for(DatanodeStorageReport r : reports) {\n+      final BalancerDatanode dn \u003d dispatcher.newDatanode(r);\n       for(StorageType t : StorageType.asList()) {\n         final Double utilization \u003d policy.getUtilization(r, t);\n         if (utilization \u003d\u003d null) { // datanode does not have such storage type \n           continue;\n         }\n         \n         final long capacity \u003d getCapacity(r, t);\n         final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n         final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n         final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n             getRemaining(r, t), utilizationDiff, threshold);\n \n         final BalancerDatanode.StorageGroup g;\n         if (utilizationDiff \u003e 0) {\n-          final Source s \u003d dn.addSource(t, utilization, maxSize2Move, this);\n+          final Source s \u003d dn.addSource(t, utilization, maxSize2Move, dispatcher);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             aboveAvgUtilized.add(s);\n           } else {\n             overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n             overUtilized.add(s);\n           }\n           g \u003d s;\n         } else {\n           g \u003d dn.addStorageGroup(t, utilization, maxSize2Move);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             belowAvgUtilized.add(g);\n           } else {\n             underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n             underUtilized.add(g);\n           }\n         }\n-        storageGroupMap.put(g);\n+        dispatcher.getStorageGroupMap().put(g);\n       }\n     }\n \n     logUtilizationCollections();\n     \n-    Preconditions.checkState(storageGroupMap.size() \u003d\u003d overUtilized.size()\n-        + underUtilized.size() + aboveAvgUtilized.size() + belowAvgUtilized.size(),\n+    Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n+        \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n+           + belowAvgUtilized.size(),\n         \"Mismatched number of storage groups\");\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long init(List\u003cDatanodeStorageReport\u003e reports) {\n    // compute average utilization\n    for (DatanodeStorageReport r : reports) {\n      policy.accumulateSpaces(r);\n    }\n    policy.initAvgUtilization();\n\n    // create network topology and classify utilization collections: \n    //   over-utilized, above-average, below-average and under-utilized.\n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for(DatanodeStorageReport r : reports) {\n      final BalancerDatanode dn \u003d dispatcher.newDatanode(r);\n      for(StorageType t : StorageType.asList()) {\n        final Double utilization \u003d policy.getUtilization(r, t);\n        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n          continue;\n        }\n        \n        final long capacity \u003d getCapacity(r, t);\n        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n            getRemaining(r, t), utilizationDiff, threshold);\n\n        final BalancerDatanode.StorageGroup g;\n        if (utilizationDiff \u003e 0) {\n          final Source s \u003d dn.addSource(t, utilization, maxSize2Move, dispatcher);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            aboveAvgUtilized.add(s);\n          } else {\n            overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            overUtilized.add(s);\n          }\n          g \u003d s;\n        } else {\n          g \u003d dn.addStorageGroup(t, utilization, maxSize2Move);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            belowAvgUtilized.add(g);\n          } else {\n            underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            underUtilized.add(g);\n          }\n        }\n        dispatcher.getStorageGroupMap().put(g);\n      }\n    }\n\n    logUtilizationCollections();\n    \n    Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n        \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n           + belowAvgUtilized.size(),\n        \"Mismatched number of storage groups\");\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
          "extendedDetails": {
            "oldValue": "[reports-DatanodeStorageReport[]]",
            "newValue": "[reports-List\u003cDatanodeStorageReport\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6828. Separate block replica dispatching from Balancer. Contributed by Tsz Wo Nicholas Sze.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1616889 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/08/14 2:33 PM",
          "commitName": "c3cf331dc91e2beef2afeed11105084843b02858",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "07/08/14 12:18 AM",
          "commitNameOld": "83b9933db3349e6a6faf23bce35c9d4ce3f7bcf2",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 1.59,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,66 +1,57 @@\n-  private long init(DatanodeStorageReport[] reports) {\n+  private long init(List\u003cDatanodeStorageReport\u003e reports) {\n     // compute average utilization\n     for (DatanodeStorageReport r : reports) {\n-      if (shouldIgnore(r.getDatanodeInfo())) {\n-        continue; \n-      }\n       policy.accumulateSpaces(r);\n     }\n     policy.initAvgUtilization();\n \n     // create network topology and classify utilization collections: \n     //   over-utilized, above-average, below-average and under-utilized.\n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n-    for(DatanodeStorageReport r : DFSUtil.shuffle(reports)) {\n-      final DatanodeInfo datanode \u003d r.getDatanodeInfo();\n-      if (shouldIgnore(datanode)) {\n-        continue; // ignore decommissioning or decommissioned nodes\n-      }\n-      cluster.add(datanode);\n-\n-      final BalancerDatanode dn \u003d new BalancerDatanode(r, underLoadedBytes,\n-          maxConcurrentMovesPerNode);\n+    for(DatanodeStorageReport r : reports) {\n+      final BalancerDatanode dn \u003d dispatcher.newDatanode(r);\n       for(StorageType t : StorageType.asList()) {\n         final Double utilization \u003d policy.getUtilization(r, t);\n         if (utilization \u003d\u003d null) { // datanode does not have such storage type \n           continue;\n         }\n         \n         final long capacity \u003d getCapacity(r, t);\n         final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n         final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n         final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n             getRemaining(r, t), utilizationDiff, threshold);\n \n         final BalancerDatanode.StorageGroup g;\n         if (utilizationDiff \u003e 0) {\n-          final Source s \u003d dn.addSource(t, utilization, maxSize2Move, this);\n+          final Source s \u003d dn.addSource(t, utilization, maxSize2Move, dispatcher);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             aboveAvgUtilized.add(s);\n           } else {\n             overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n             overUtilized.add(s);\n           }\n           g \u003d s;\n         } else {\n           g \u003d dn.addStorageGroup(t, utilization, maxSize2Move);\n           if (thresholdDiff \u003c\u003d 0) { // within threshold\n             belowAvgUtilized.add(g);\n           } else {\n             underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n             underUtilized.add(g);\n           }\n         }\n-        storageGroupMap.put(g);\n+        dispatcher.getStorageGroupMap().put(g);\n       }\n     }\n \n     logUtilizationCollections();\n     \n-    Preconditions.checkState(storageGroupMap.size() \u003d\u003d overUtilized.size()\n-        + underUtilized.size() + aboveAvgUtilized.size() + belowAvgUtilized.size(),\n+    Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n+        \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n+           + belowAvgUtilized.size(),\n         \"Mismatched number of storage groups\");\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long init(List\u003cDatanodeStorageReport\u003e reports) {\n    // compute average utilization\n    for (DatanodeStorageReport r : reports) {\n      policy.accumulateSpaces(r);\n    }\n    policy.initAvgUtilization();\n\n    // create network topology and classify utilization collections: \n    //   over-utilized, above-average, below-average and under-utilized.\n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for(DatanodeStorageReport r : reports) {\n      final BalancerDatanode dn \u003d dispatcher.newDatanode(r);\n      for(StorageType t : StorageType.asList()) {\n        final Double utilization \u003d policy.getUtilization(r, t);\n        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n          continue;\n        }\n        \n        final long capacity \u003d getCapacity(r, t);\n        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n            getRemaining(r, t), utilizationDiff, threshold);\n\n        final BalancerDatanode.StorageGroup g;\n        if (utilizationDiff \u003e 0) {\n          final Source s \u003d dn.addSource(t, utilization, maxSize2Move, dispatcher);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            aboveAvgUtilized.add(s);\n          } else {\n            overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            overUtilized.add(s);\n          }\n          g \u003d s;\n        } else {\n          g \u003d dn.addStorageGroup(t, utilization, maxSize2Move);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            belowAvgUtilized.add(g);\n          } else {\n            underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            underUtilized.add(g);\n          }\n        }\n        dispatcher.getStorageGroupMap().put(g);\n      }\n    }\n\n    logUtilizationCollections();\n    \n    Preconditions.checkState(dispatcher.getStorageGroupMap().size()\n        \u003d\u003d overUtilized.size() + underUtilized.size() + aboveAvgUtilized.size()\n           + belowAvgUtilized.size(),\n        \"Mismatched number of storage groups\");\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
          "extendedDetails": {}
        }
      ]
    },
    "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d": {
      "type": "Ymultichange(Yrename,Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6685. Balancer should preserve storage type of replicas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615015 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/07/14 6:05 PM",
      "commitName": "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-6685. Balancer should preserve storage type of replicas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615015 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "31/07/14 6:05 PM",
          "commitName": "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "30/07/14 11:02 PM",
          "commitNameOld": "b8b8f3f5e7214d6fcfc30e1b94ff097e52868f4f",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.79,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,75 +1,66 @@\n-  private long initNodes(DatanodeInfo[] datanodes) {\n+  private long init(DatanodeStorageReport[] reports) {\n     // compute average utilization\n-    for (DatanodeInfo datanode : datanodes) {\n-     // ignore decommissioning or decommissioned nodes or\n-      // ignore nodes in exclude list\n-      // or nodes not in the include list (if include list is not empty)\n-      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress() ||\n-          Util.shouldBeExcluded(nodesToBeExcluded, datanode) ||\n-          !Util.shouldBeIncluded(nodesToBeIncluded, datanode)) {\n-        continue;\n+    for (DatanodeStorageReport r : reports) {\n+      if (shouldIgnore(r.getDatanodeInfo())) {\n+        continue; \n       }\n-      policy.accumulateSpaces(datanode);\n+      policy.accumulateSpaces(r);\n     }\n     policy.initAvgUtilization();\n \n-    /*create network topology and all data node lists: \n-     * overloaded, above-average, below-average, and underloaded\n-     * we alternates the accessing of the given datanodes array either by\n-     * an increasing order or a decreasing order.\n-     */  \n+    // create network topology and classify utilization collections: \n+    //   over-utilized, above-average, below-average and under-utilized.\n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n-    for (DatanodeInfo datanode : DFSUtil.shuffle(datanodes)) {\n-      // ignore decommissioning or decommissioned nodes or\n-      // ignore nodes in exclude list\n-      // or nodes not in the include list (if include list is not empty)\n-      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress() ||\n-          Util.shouldBeExcluded(nodesToBeExcluded, datanode) ||\n-          !Util.shouldBeIncluded(nodesToBeIncluded, datanode)) {\n-        if (LOG.isTraceEnabled()) {\n-          LOG.trace(\"Excluding datanode \" + datanode);\n-        }\n-        continue;\n+    for(DatanodeStorageReport r : DFSUtil.shuffle(reports)) {\n+      final DatanodeInfo datanode \u003d r.getDatanodeInfo();\n+      if (shouldIgnore(datanode)) {\n+        continue; // ignore decommissioning or decommissioned nodes\n       }\n       cluster.add(datanode);\n-      BalancerDatanode datanodeS;\n-      final double avg \u003d policy.getAvgUtilization();\n-      if (policy.getUtilization(datanode) \u003e\u003d avg) {\n-        datanodeS \u003d new Source(datanode, policy, threshold, maxConcurrentMovesPerNode);\n-        if (isAboveAvgUtilized(datanodeS)) {\n-          this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n-        } else {\n-          assert(isOverUtilized(datanodeS)) :\n-            datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n-          this.overUtilizedDatanodes.add((Source)datanodeS);\n-          overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n-              -threshold)*datanodeS.datanode.getCapacity()/100.0);\n+\n+      final BalancerDatanode dn \u003d new BalancerDatanode(r, underLoadedBytes,\n+          maxConcurrentMovesPerNode);\n+      for(StorageType t : StorageType.asList()) {\n+        final Double utilization \u003d policy.getUtilization(r, t);\n+        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n+          continue;\n         }\n-      } else {\n-        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold,\n-            maxConcurrentMovesPerNode);\n-        if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n-          this.belowAvgUtilizedDatanodes.add(datanodeS);\n+        \n+        final long capacity \u003d getCapacity(r, t);\n+        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n+        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n+        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n+            getRemaining(r, t), utilizationDiff, threshold);\n+\n+        final BalancerDatanode.StorageGroup g;\n+        if (utilizationDiff \u003e 0) {\n+          final Source s \u003d dn.addSource(t, utilization, maxSize2Move, this);\n+          if (thresholdDiff \u003c\u003d 0) { // within threshold\n+            aboveAvgUtilized.add(s);\n+          } else {\n+            overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n+            overUtilized.add(s);\n+          }\n+          g \u003d s;\n         } else {\n-          assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n-              + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n-              + \", utilization\u003d\" + datanodeS.utilization; \n-          this.underUtilizedDatanodes.add(datanodeS);\n-          underLoadedBytes +\u003d (long)((avg-threshold-\n-              datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n+          g \u003d dn.addStorageGroup(t, utilization, maxSize2Move);\n+          if (thresholdDiff \u003c\u003d 0) { // within threshold\n+            belowAvgUtilized.add(g);\n+          } else {\n+            underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n+            underUtilized.add(g);\n+          }\n         }\n+        storageGroupMap.put(g);\n       }\n-      datanodeMap.put(datanode.getDatanodeUuid(), datanodeS);\n     }\n \n-    //logging\n-    logNodes();\n+    logUtilizationCollections();\n     \n-    assert (this.datanodeMap.size() \u003d\u003d \n-      overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n-      aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n-      : \"Mismatched number of datanodes\";\n+    Preconditions.checkState(storageGroupMap.size() \u003d\u003d overUtilized.size()\n+        + underUtilized.size() + aboveAvgUtilized.size() + belowAvgUtilized.size(),\n+        \"Mismatched number of storage groups\");\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long init(DatanodeStorageReport[] reports) {\n    // compute average utilization\n    for (DatanodeStorageReport r : reports) {\n      if (shouldIgnore(r.getDatanodeInfo())) {\n        continue; \n      }\n      policy.accumulateSpaces(r);\n    }\n    policy.initAvgUtilization();\n\n    // create network topology and classify utilization collections: \n    //   over-utilized, above-average, below-average and under-utilized.\n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for(DatanodeStorageReport r : DFSUtil.shuffle(reports)) {\n      final DatanodeInfo datanode \u003d r.getDatanodeInfo();\n      if (shouldIgnore(datanode)) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      cluster.add(datanode);\n\n      final BalancerDatanode dn \u003d new BalancerDatanode(r, underLoadedBytes,\n          maxConcurrentMovesPerNode);\n      for(StorageType t : StorageType.asList()) {\n        final Double utilization \u003d policy.getUtilization(r, t);\n        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n          continue;\n        }\n        \n        final long capacity \u003d getCapacity(r, t);\n        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n            getRemaining(r, t), utilizationDiff, threshold);\n\n        final BalancerDatanode.StorageGroup g;\n        if (utilizationDiff \u003e 0) {\n          final Source s \u003d dn.addSource(t, utilization, maxSize2Move, this);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            aboveAvgUtilized.add(s);\n          } else {\n            overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            overUtilized.add(s);\n          }\n          g \u003d s;\n        } else {\n          g \u003d dn.addStorageGroup(t, utilization, maxSize2Move);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            belowAvgUtilized.add(g);\n          } else {\n            underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            underUtilized.add(g);\n          }\n        }\n        storageGroupMap.put(g);\n      }\n    }\n\n    logUtilizationCollections();\n    \n    Preconditions.checkState(storageGroupMap.size() \u003d\u003d overUtilized.size()\n        + underUtilized.size() + aboveAvgUtilized.size() + belowAvgUtilized.size(),\n        \"Mismatched number of storage groups\");\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
          "extendedDetails": {
            "oldValue": "initNodes",
            "newValue": "init"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6685. Balancer should preserve storage type of replicas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615015 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "31/07/14 6:05 PM",
          "commitName": "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "30/07/14 11:02 PM",
          "commitNameOld": "b8b8f3f5e7214d6fcfc30e1b94ff097e52868f4f",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.79,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,75 +1,66 @@\n-  private long initNodes(DatanodeInfo[] datanodes) {\n+  private long init(DatanodeStorageReport[] reports) {\n     // compute average utilization\n-    for (DatanodeInfo datanode : datanodes) {\n-     // ignore decommissioning or decommissioned nodes or\n-      // ignore nodes in exclude list\n-      // or nodes not in the include list (if include list is not empty)\n-      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress() ||\n-          Util.shouldBeExcluded(nodesToBeExcluded, datanode) ||\n-          !Util.shouldBeIncluded(nodesToBeIncluded, datanode)) {\n-        continue;\n+    for (DatanodeStorageReport r : reports) {\n+      if (shouldIgnore(r.getDatanodeInfo())) {\n+        continue; \n       }\n-      policy.accumulateSpaces(datanode);\n+      policy.accumulateSpaces(r);\n     }\n     policy.initAvgUtilization();\n \n-    /*create network topology and all data node lists: \n-     * overloaded, above-average, below-average, and underloaded\n-     * we alternates the accessing of the given datanodes array either by\n-     * an increasing order or a decreasing order.\n-     */  \n+    // create network topology and classify utilization collections: \n+    //   over-utilized, above-average, below-average and under-utilized.\n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n-    for (DatanodeInfo datanode : DFSUtil.shuffle(datanodes)) {\n-      // ignore decommissioning or decommissioned nodes or\n-      // ignore nodes in exclude list\n-      // or nodes not in the include list (if include list is not empty)\n-      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress() ||\n-          Util.shouldBeExcluded(nodesToBeExcluded, datanode) ||\n-          !Util.shouldBeIncluded(nodesToBeIncluded, datanode)) {\n-        if (LOG.isTraceEnabled()) {\n-          LOG.trace(\"Excluding datanode \" + datanode);\n-        }\n-        continue;\n+    for(DatanodeStorageReport r : DFSUtil.shuffle(reports)) {\n+      final DatanodeInfo datanode \u003d r.getDatanodeInfo();\n+      if (shouldIgnore(datanode)) {\n+        continue; // ignore decommissioning or decommissioned nodes\n       }\n       cluster.add(datanode);\n-      BalancerDatanode datanodeS;\n-      final double avg \u003d policy.getAvgUtilization();\n-      if (policy.getUtilization(datanode) \u003e\u003d avg) {\n-        datanodeS \u003d new Source(datanode, policy, threshold, maxConcurrentMovesPerNode);\n-        if (isAboveAvgUtilized(datanodeS)) {\n-          this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n-        } else {\n-          assert(isOverUtilized(datanodeS)) :\n-            datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n-          this.overUtilizedDatanodes.add((Source)datanodeS);\n-          overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n-              -threshold)*datanodeS.datanode.getCapacity()/100.0);\n+\n+      final BalancerDatanode dn \u003d new BalancerDatanode(r, underLoadedBytes,\n+          maxConcurrentMovesPerNode);\n+      for(StorageType t : StorageType.asList()) {\n+        final Double utilization \u003d policy.getUtilization(r, t);\n+        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n+          continue;\n         }\n-      } else {\n-        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold,\n-            maxConcurrentMovesPerNode);\n-        if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n-          this.belowAvgUtilizedDatanodes.add(datanodeS);\n+        \n+        final long capacity \u003d getCapacity(r, t);\n+        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n+        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n+        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n+            getRemaining(r, t), utilizationDiff, threshold);\n+\n+        final BalancerDatanode.StorageGroup g;\n+        if (utilizationDiff \u003e 0) {\n+          final Source s \u003d dn.addSource(t, utilization, maxSize2Move, this);\n+          if (thresholdDiff \u003c\u003d 0) { // within threshold\n+            aboveAvgUtilized.add(s);\n+          } else {\n+            overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n+            overUtilized.add(s);\n+          }\n+          g \u003d s;\n         } else {\n-          assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n-              + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n-              + \", utilization\u003d\" + datanodeS.utilization; \n-          this.underUtilizedDatanodes.add(datanodeS);\n-          underLoadedBytes +\u003d (long)((avg-threshold-\n-              datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n+          g \u003d dn.addStorageGroup(t, utilization, maxSize2Move);\n+          if (thresholdDiff \u003c\u003d 0) { // within threshold\n+            belowAvgUtilized.add(g);\n+          } else {\n+            underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n+            underUtilized.add(g);\n+          }\n         }\n+        storageGroupMap.put(g);\n       }\n-      datanodeMap.put(datanode.getDatanodeUuid(), datanodeS);\n     }\n \n-    //logging\n-    logNodes();\n+    logUtilizationCollections();\n     \n-    assert (this.datanodeMap.size() \u003d\u003d \n-      overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n-      aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n-      : \"Mismatched number of datanodes\";\n+    Preconditions.checkState(storageGroupMap.size() \u003d\u003d overUtilized.size()\n+        + underUtilized.size() + aboveAvgUtilized.size() + belowAvgUtilized.size(),\n+        \"Mismatched number of storage groups\");\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long init(DatanodeStorageReport[] reports) {\n    // compute average utilization\n    for (DatanodeStorageReport r : reports) {\n      if (shouldIgnore(r.getDatanodeInfo())) {\n        continue; \n      }\n      policy.accumulateSpaces(r);\n    }\n    policy.initAvgUtilization();\n\n    // create network topology and classify utilization collections: \n    //   over-utilized, above-average, below-average and under-utilized.\n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for(DatanodeStorageReport r : DFSUtil.shuffle(reports)) {\n      final DatanodeInfo datanode \u003d r.getDatanodeInfo();\n      if (shouldIgnore(datanode)) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      cluster.add(datanode);\n\n      final BalancerDatanode dn \u003d new BalancerDatanode(r, underLoadedBytes,\n          maxConcurrentMovesPerNode);\n      for(StorageType t : StorageType.asList()) {\n        final Double utilization \u003d policy.getUtilization(r, t);\n        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n          continue;\n        }\n        \n        final long capacity \u003d getCapacity(r, t);\n        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n            getRemaining(r, t), utilizationDiff, threshold);\n\n        final BalancerDatanode.StorageGroup g;\n        if (utilizationDiff \u003e 0) {\n          final Source s \u003d dn.addSource(t, utilization, maxSize2Move, this);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            aboveAvgUtilized.add(s);\n          } else {\n            overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            overUtilized.add(s);\n          }\n          g \u003d s;\n        } else {\n          g \u003d dn.addStorageGroup(t, utilization, maxSize2Move);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            belowAvgUtilized.add(g);\n          } else {\n            underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            underUtilized.add(g);\n          }\n        }\n        storageGroupMap.put(g);\n      }\n    }\n\n    logUtilizationCollections();\n    \n    Preconditions.checkState(storageGroupMap.size() \u003d\u003d overUtilized.size()\n        + underUtilized.size() + aboveAvgUtilized.size() + belowAvgUtilized.size(),\n        \"Mismatched number of storage groups\");\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
          "extendedDetails": {
            "oldValue": "[datanodes-DatanodeInfo[]]",
            "newValue": "[reports-DatanodeStorageReport[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6685. Balancer should preserve storage type of replicas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1615015 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "31/07/14 6:05 PM",
          "commitName": "b8597e6a10b2e8df1bee4e8ce0c8be345f7e007d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "30/07/14 11:02 PM",
          "commitNameOld": "b8b8f3f5e7214d6fcfc30e1b94ff097e52868f4f",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 0.79,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,75 +1,66 @@\n-  private long initNodes(DatanodeInfo[] datanodes) {\n+  private long init(DatanodeStorageReport[] reports) {\n     // compute average utilization\n-    for (DatanodeInfo datanode : datanodes) {\n-     // ignore decommissioning or decommissioned nodes or\n-      // ignore nodes in exclude list\n-      // or nodes not in the include list (if include list is not empty)\n-      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress() ||\n-          Util.shouldBeExcluded(nodesToBeExcluded, datanode) ||\n-          !Util.shouldBeIncluded(nodesToBeIncluded, datanode)) {\n-        continue;\n+    for (DatanodeStorageReport r : reports) {\n+      if (shouldIgnore(r.getDatanodeInfo())) {\n+        continue; \n       }\n-      policy.accumulateSpaces(datanode);\n+      policy.accumulateSpaces(r);\n     }\n     policy.initAvgUtilization();\n \n-    /*create network topology and all data node lists: \n-     * overloaded, above-average, below-average, and underloaded\n-     * we alternates the accessing of the given datanodes array either by\n-     * an increasing order or a decreasing order.\n-     */  \n+    // create network topology and classify utilization collections: \n+    //   over-utilized, above-average, below-average and under-utilized.\n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n-    for (DatanodeInfo datanode : DFSUtil.shuffle(datanodes)) {\n-      // ignore decommissioning or decommissioned nodes or\n-      // ignore nodes in exclude list\n-      // or nodes not in the include list (if include list is not empty)\n-      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress() ||\n-          Util.shouldBeExcluded(nodesToBeExcluded, datanode) ||\n-          !Util.shouldBeIncluded(nodesToBeIncluded, datanode)) {\n-        if (LOG.isTraceEnabled()) {\n-          LOG.trace(\"Excluding datanode \" + datanode);\n-        }\n-        continue;\n+    for(DatanodeStorageReport r : DFSUtil.shuffle(reports)) {\n+      final DatanodeInfo datanode \u003d r.getDatanodeInfo();\n+      if (shouldIgnore(datanode)) {\n+        continue; // ignore decommissioning or decommissioned nodes\n       }\n       cluster.add(datanode);\n-      BalancerDatanode datanodeS;\n-      final double avg \u003d policy.getAvgUtilization();\n-      if (policy.getUtilization(datanode) \u003e\u003d avg) {\n-        datanodeS \u003d new Source(datanode, policy, threshold, maxConcurrentMovesPerNode);\n-        if (isAboveAvgUtilized(datanodeS)) {\n-          this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n-        } else {\n-          assert(isOverUtilized(datanodeS)) :\n-            datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n-          this.overUtilizedDatanodes.add((Source)datanodeS);\n-          overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n-              -threshold)*datanodeS.datanode.getCapacity()/100.0);\n+\n+      final BalancerDatanode dn \u003d new BalancerDatanode(r, underLoadedBytes,\n+          maxConcurrentMovesPerNode);\n+      for(StorageType t : StorageType.asList()) {\n+        final Double utilization \u003d policy.getUtilization(r, t);\n+        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n+          continue;\n         }\n-      } else {\n-        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold,\n-            maxConcurrentMovesPerNode);\n-        if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n-          this.belowAvgUtilizedDatanodes.add(datanodeS);\n+        \n+        final long capacity \u003d getCapacity(r, t);\n+        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n+        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n+        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n+            getRemaining(r, t), utilizationDiff, threshold);\n+\n+        final BalancerDatanode.StorageGroup g;\n+        if (utilizationDiff \u003e 0) {\n+          final Source s \u003d dn.addSource(t, utilization, maxSize2Move, this);\n+          if (thresholdDiff \u003c\u003d 0) { // within threshold\n+            aboveAvgUtilized.add(s);\n+          } else {\n+            overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n+            overUtilized.add(s);\n+          }\n+          g \u003d s;\n         } else {\n-          assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n-              + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n-              + \", utilization\u003d\" + datanodeS.utilization; \n-          this.underUtilizedDatanodes.add(datanodeS);\n-          underLoadedBytes +\u003d (long)((avg-threshold-\n-              datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n+          g \u003d dn.addStorageGroup(t, utilization, maxSize2Move);\n+          if (thresholdDiff \u003c\u003d 0) { // within threshold\n+            belowAvgUtilized.add(g);\n+          } else {\n+            underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n+            underUtilized.add(g);\n+          }\n         }\n+        storageGroupMap.put(g);\n       }\n-      datanodeMap.put(datanode.getDatanodeUuid(), datanodeS);\n     }\n \n-    //logging\n-    logNodes();\n+    logUtilizationCollections();\n     \n-    assert (this.datanodeMap.size() \u003d\u003d \n-      overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n-      aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n-      : \"Mismatched number of datanodes\";\n+    Preconditions.checkState(storageGroupMap.size() \u003d\u003d overUtilized.size()\n+        + underUtilized.size() + aboveAvgUtilized.size() + belowAvgUtilized.size(),\n+        \"Mismatched number of storage groups\");\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long init(DatanodeStorageReport[] reports) {\n    // compute average utilization\n    for (DatanodeStorageReport r : reports) {\n      if (shouldIgnore(r.getDatanodeInfo())) {\n        continue; \n      }\n      policy.accumulateSpaces(r);\n    }\n    policy.initAvgUtilization();\n\n    // create network topology and classify utilization collections: \n    //   over-utilized, above-average, below-average and under-utilized.\n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for(DatanodeStorageReport r : DFSUtil.shuffle(reports)) {\n      final DatanodeInfo datanode \u003d r.getDatanodeInfo();\n      if (shouldIgnore(datanode)) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      cluster.add(datanode);\n\n      final BalancerDatanode dn \u003d new BalancerDatanode(r, underLoadedBytes,\n          maxConcurrentMovesPerNode);\n      for(StorageType t : StorageType.asList()) {\n        final Double utilization \u003d policy.getUtilization(r, t);\n        if (utilization \u003d\u003d null) { // datanode does not have such storage type \n          continue;\n        }\n        \n        final long capacity \u003d getCapacity(r, t);\n        final double utilizationDiff \u003d utilization - policy.getAvgUtilization(t);\n        final double thresholdDiff \u003d Math.abs(utilizationDiff) - threshold;\n        final long maxSize2Move \u003d computeMaxSize2Move(capacity,\n            getRemaining(r, t), utilizationDiff, threshold);\n\n        final BalancerDatanode.StorageGroup g;\n        if (utilizationDiff \u003e 0) {\n          final Source s \u003d dn.addSource(t, utilization, maxSize2Move, this);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            aboveAvgUtilized.add(s);\n          } else {\n            overLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            overUtilized.add(s);\n          }\n          g \u003d s;\n        } else {\n          g \u003d dn.addStorageGroup(t, utilization, maxSize2Move);\n          if (thresholdDiff \u003c\u003d 0) { // within threshold\n            belowAvgUtilized.add(g);\n          } else {\n            underLoadedBytes +\u003d precentage2bytes(thresholdDiff, capacity);\n            underUtilized.add(g);\n          }\n        }\n        storageGroupMap.put(g);\n      }\n    }\n\n    logUtilizationCollections();\n    \n    Preconditions.checkState(storageGroupMap.size() \u003d\u003d overUtilized.size()\n        + underUtilized.size() + aboveAvgUtilized.size() + belowAvgUtilized.size(),\n        \"Mismatched number of storage groups\");\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
          "extendedDetails": {}
        }
      ]
    },
    "b8b8f3f5e7214d6fcfc30e1b94ff097e52868f4f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6441. Add ability to exclude/include specific datanodes while balancing. (Contributed by Benoy Antony and Yu Li)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1614812 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/07/14 11:02 PM",
      "commitName": "b8b8f3f5e7214d6fcfc30e1b94ff097e52868f4f",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "22/07/14 12:41 AM",
      "commitNameOld": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 8.93,
      "commitsBetweenForRepo": 65,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,75 @@\n   private long initNodes(DatanodeInfo[] datanodes) {\n     // compute average utilization\n     for (DatanodeInfo datanode : datanodes) {\n-      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n-        continue; // ignore decommissioning or decommissioned nodes\n+     // ignore decommissioning or decommissioned nodes or\n+      // ignore nodes in exclude list\n+      // or nodes not in the include list (if include list is not empty)\n+      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress() ||\n+          Util.shouldBeExcluded(nodesToBeExcluded, datanode) ||\n+          !Util.shouldBeIncluded(nodesToBeIncluded, datanode)) {\n+        continue;\n       }\n       policy.accumulateSpaces(datanode);\n     }\n     policy.initAvgUtilization();\n \n     /*create network topology and all data node lists: \n      * overloaded, above-average, below-average, and underloaded\n      * we alternates the accessing of the given datanodes array either by\n      * an increasing order or a decreasing order.\n      */  \n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n     for (DatanodeInfo datanode : DFSUtil.shuffle(datanodes)) {\n-      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n-        continue; // ignore decommissioning or decommissioned nodes\n+      // ignore decommissioning or decommissioned nodes or\n+      // ignore nodes in exclude list\n+      // or nodes not in the include list (if include list is not empty)\n+      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress() ||\n+          Util.shouldBeExcluded(nodesToBeExcluded, datanode) ||\n+          !Util.shouldBeIncluded(nodesToBeIncluded, datanode)) {\n+        if (LOG.isTraceEnabled()) {\n+          LOG.trace(\"Excluding datanode \" + datanode);\n+        }\n+        continue;\n       }\n       cluster.add(datanode);\n       BalancerDatanode datanodeS;\n       final double avg \u003d policy.getAvgUtilization();\n       if (policy.getUtilization(datanode) \u003e\u003d avg) {\n         datanodeS \u003d new Source(datanode, policy, threshold, maxConcurrentMovesPerNode);\n         if (isAboveAvgUtilized(datanodeS)) {\n           this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n         } else {\n           assert(isOverUtilized(datanodeS)) :\n             datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n           this.overUtilizedDatanodes.add((Source)datanodeS);\n           overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n               -threshold)*datanodeS.datanode.getCapacity()/100.0);\n         }\n       } else {\n         datanodeS \u003d new BalancerDatanode(datanode, policy, threshold,\n             maxConcurrentMovesPerNode);\n         if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n           this.belowAvgUtilizedDatanodes.add(datanodeS);\n         } else {\n           assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n               + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n               + \", utilization\u003d\" + datanodeS.utilization; \n           this.underUtilizedDatanodes.add(datanodeS);\n           underLoadedBytes +\u003d (long)((avg-threshold-\n               datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n         }\n       }\n       datanodeMap.put(datanode.getDatanodeUuid(), datanodeS);\n     }\n \n     //logging\n     logNodes();\n     \n     assert (this.datanodeMap.size() \u003d\u003d \n       overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n       aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n       : \"Mismatched number of datanodes\";\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long initNodes(DatanodeInfo[] datanodes) {\n    // compute average utilization\n    for (DatanodeInfo datanode : datanodes) {\n     // ignore decommissioning or decommissioned nodes or\n      // ignore nodes in exclude list\n      // or nodes not in the include list (if include list is not empty)\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress() ||\n          Util.shouldBeExcluded(nodesToBeExcluded, datanode) ||\n          !Util.shouldBeIncluded(nodesToBeIncluded, datanode)) {\n        continue;\n      }\n      policy.accumulateSpaces(datanode);\n    }\n    policy.initAvgUtilization();\n\n    /*create network topology and all data node lists: \n     * overloaded, above-average, below-average, and underloaded\n     * we alternates the accessing of the given datanodes array either by\n     * an increasing order or a decreasing order.\n     */  \n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for (DatanodeInfo datanode : DFSUtil.shuffle(datanodes)) {\n      // ignore decommissioning or decommissioned nodes or\n      // ignore nodes in exclude list\n      // or nodes not in the include list (if include list is not empty)\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress() ||\n          Util.shouldBeExcluded(nodesToBeExcluded, datanode) ||\n          !Util.shouldBeIncluded(nodesToBeIncluded, datanode)) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Excluding datanode \" + datanode);\n        }\n        continue;\n      }\n      cluster.add(datanode);\n      BalancerDatanode datanodeS;\n      final double avg \u003d policy.getAvgUtilization();\n      if (policy.getUtilization(datanode) \u003e\u003d avg) {\n        datanodeS \u003d new Source(datanode, policy, threshold, maxConcurrentMovesPerNode);\n        if (isAboveAvgUtilized(datanodeS)) {\n          this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n        } else {\n          assert(isOverUtilized(datanodeS)) :\n            datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n          this.overUtilizedDatanodes.add((Source)datanodeS);\n          overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n              -threshold)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      } else {\n        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold,\n            maxConcurrentMovesPerNode);\n        if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n          this.belowAvgUtilizedDatanodes.add(datanodeS);\n        } else {\n          assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n              + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n              + \", utilization\u003d\" + datanodeS.utilization; \n          this.underUtilizedDatanodes.add(datanodeS);\n          underLoadedBytes +\u003d (long)((avg-threshold-\n              datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      }\n      datanodeMap.put(datanode.getDatanodeUuid(), datanodeS);\n    }\n\n    //logging\n    logNodes();\n    \n    assert (this.datanodeMap.size() \u003d\u003d \n      overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n      aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n      : \"Mismatched number of datanodes\";\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "e3612e442809310c67bc2ed4376e028c4ab8d597": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6595. Allow the maximum threads for balancing on datanodes to be configurable. Contributed by Benoy Antony\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1605565 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/06/14 12:27 PM",
      "commitName": "e3612e442809310c67bc2ed4376e028c4ab8d597",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "10/06/14 2:02 PM",
      "commitNameOld": "c802ca28fd2c9fd35a662b8dd6b675e2aaa515d1",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 14.93,
      "commitsBetweenForRepo": 117,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,62 @@\n   private long initNodes(DatanodeInfo[] datanodes) {\n     // compute average utilization\n     for (DatanodeInfo datanode : datanodes) {\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue; // ignore decommissioning or decommissioned nodes\n       }\n       policy.accumulateSpaces(datanode);\n     }\n     policy.initAvgUtilization();\n \n     /*create network topology and all data node lists: \n      * overloaded, above-average, below-average, and underloaded\n      * we alternates the accessing of the given datanodes array either by\n      * an increasing order or a decreasing order.\n      */  \n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n     for (DatanodeInfo datanode : DFSUtil.shuffle(datanodes)) {\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue; // ignore decommissioning or decommissioned nodes\n       }\n       cluster.add(datanode);\n       BalancerDatanode datanodeS;\n       final double avg \u003d policy.getAvgUtilization();\n       if (policy.getUtilization(datanode) \u003e\u003d avg) {\n-        datanodeS \u003d new Source(datanode, policy, threshold);\n+        datanodeS \u003d new Source(datanode, policy, threshold, maxConcurrentMovesPerNode);\n         if (isAboveAvgUtilized(datanodeS)) {\n           this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n         } else {\n           assert(isOverUtilized(datanodeS)) :\n             datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n           this.overUtilizedDatanodes.add((Source)datanodeS);\n           overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n               -threshold)*datanodeS.datanode.getCapacity()/100.0);\n         }\n       } else {\n-        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n+        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold,\n+            maxConcurrentMovesPerNode);\n         if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n           this.belowAvgUtilizedDatanodes.add(datanodeS);\n         } else {\n           assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n               + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n               + \", utilization\u003d\" + datanodeS.utilization; \n           this.underUtilizedDatanodes.add(datanodeS);\n           underLoadedBytes +\u003d (long)((avg-threshold-\n               datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n         }\n       }\n       datanodeMap.put(datanode.getDatanodeUuid(), datanodeS);\n     }\n \n     //logging\n     logNodes();\n     \n     assert (this.datanodeMap.size() \u003d\u003d \n       overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n       aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n       : \"Mismatched number of datanodes\";\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long initNodes(DatanodeInfo[] datanodes) {\n    // compute average utilization\n    for (DatanodeInfo datanode : datanodes) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      policy.accumulateSpaces(datanode);\n    }\n    policy.initAvgUtilization();\n\n    /*create network topology and all data node lists: \n     * overloaded, above-average, below-average, and underloaded\n     * we alternates the accessing of the given datanodes array either by\n     * an increasing order or a decreasing order.\n     */  \n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for (DatanodeInfo datanode : DFSUtil.shuffle(datanodes)) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      cluster.add(datanode);\n      BalancerDatanode datanodeS;\n      final double avg \u003d policy.getAvgUtilization();\n      if (policy.getUtilization(datanode) \u003e\u003d avg) {\n        datanodeS \u003d new Source(datanode, policy, threshold, maxConcurrentMovesPerNode);\n        if (isAboveAvgUtilized(datanodeS)) {\n          this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n        } else {\n          assert(isOverUtilized(datanodeS)) :\n            datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n          this.overUtilizedDatanodes.add((Source)datanodeS);\n          overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n              -threshold)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      } else {\n        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold,\n            maxConcurrentMovesPerNode);\n        if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n          this.belowAvgUtilizedDatanodes.add(datanodeS);\n        } else {\n          assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n              + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n              + \", utilization\u003d\" + datanodeS.utilization; \n          this.underUtilizedDatanodes.add(datanodeS);\n          underLoadedBytes +\u003d (long)((avg-threshold-\n              datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      }\n      datanodeMap.put(datanode.getDatanodeUuid(), datanodeS);\n    }\n\n    //logging\n    logNodes();\n    \n    assert (this.datanodeMap.size() \u003d\u003d \n      overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n      aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n      : \"Mismatched number of datanodes\";\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "907fb15ee8c150e5ecc0560b7374441c57a84122": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5480. Update Balancer for HDFS-2832. (Contributed by szetszwo)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1540547 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/11/13 12:59 PM",
      "commitName": "907fb15ee8c150e5ecc0560b7374441c57a84122",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "17/10/13 11:54 AM",
      "commitNameOld": "f8d5755a69d7b4f230adbbfd88ea73df7a83b4f0",
      "commitAuthorOld": "",
      "daysBetweenCommits": 24.09,
      "commitsBetweenForRepo": 121,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   private long initNodes(DatanodeInfo[] datanodes) {\n     // compute average utilization\n     for (DatanodeInfo datanode : datanodes) {\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue; // ignore decommissioning or decommissioned nodes\n       }\n       policy.accumulateSpaces(datanode);\n     }\n     policy.initAvgUtilization();\n \n     /*create network topology and all data node lists: \n      * overloaded, above-average, below-average, and underloaded\n      * we alternates the accessing of the given datanodes array either by\n      * an increasing order or a decreasing order.\n      */  \n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n     for (DatanodeInfo datanode : DFSUtil.shuffle(datanodes)) {\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue; // ignore decommissioning or decommissioned nodes\n       }\n       cluster.add(datanode);\n       BalancerDatanode datanodeS;\n       final double avg \u003d policy.getAvgUtilization();\n       if (policy.getUtilization(datanode) \u003e\u003d avg) {\n         datanodeS \u003d new Source(datanode, policy, threshold);\n         if (isAboveAvgUtilized(datanodeS)) {\n           this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n         } else {\n           assert(isOverUtilized(datanodeS)) :\n             datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n           this.overUtilizedDatanodes.add((Source)datanodeS);\n           overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n               -threshold)*datanodeS.datanode.getCapacity()/100.0);\n         }\n       } else {\n         datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n         if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n           this.belowAvgUtilizedDatanodes.add(datanodeS);\n         } else {\n           assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n               + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n               + \", utilization\u003d\" + datanodeS.utilization; \n           this.underUtilizedDatanodes.add(datanodeS);\n           underLoadedBytes +\u003d (long)((avg-threshold-\n               datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n         }\n       }\n-      this.datanodes.put(datanode.getDatanodeUuid(), datanodeS);\n+      datanodeMap.put(datanode.getDatanodeUuid(), datanodeS);\n     }\n \n     //logging\n     logNodes();\n     \n-    assert (this.datanodes.size() \u003d\u003d \n+    assert (this.datanodeMap.size() \u003d\u003d \n       overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n       aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n       : \"Mismatched number of datanodes\";\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long initNodes(DatanodeInfo[] datanodes) {\n    // compute average utilization\n    for (DatanodeInfo datanode : datanodes) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      policy.accumulateSpaces(datanode);\n    }\n    policy.initAvgUtilization();\n\n    /*create network topology and all data node lists: \n     * overloaded, above-average, below-average, and underloaded\n     * we alternates the accessing of the given datanodes array either by\n     * an increasing order or a decreasing order.\n     */  \n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for (DatanodeInfo datanode : DFSUtil.shuffle(datanodes)) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      cluster.add(datanode);\n      BalancerDatanode datanodeS;\n      final double avg \u003d policy.getAvgUtilization();\n      if (policy.getUtilization(datanode) \u003e\u003d avg) {\n        datanodeS \u003d new Source(datanode, policy, threshold);\n        if (isAboveAvgUtilized(datanodeS)) {\n          this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n        } else {\n          assert(isOverUtilized(datanodeS)) :\n            datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n          this.overUtilizedDatanodes.add((Source)datanodeS);\n          overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n              -threshold)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      } else {\n        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n        if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n          this.belowAvgUtilizedDatanodes.add(datanodeS);\n        } else {\n          assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n              + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n              + \", utilization\u003d\" + datanodeS.utilization; \n          this.underUtilizedDatanodes.add(datanodeS);\n          underLoadedBytes +\u003d (long)((avg-threshold-\n              datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      }\n      datanodeMap.put(datanode.getDatanodeUuid(), datanodeS);\n    }\n\n    //logging\n    logNodes();\n    \n    assert (this.datanodeMap.size() \u003d\u003d \n      overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n      aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n      : \"Mismatched number of datanodes\";\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "4551da302d94cffea0313eac79479ab6f9b7cb34": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5233. Use Datanode UUID to identify Datanodes.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1525407 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/09/13 11:03 AM",
      "commitName": "4551da302d94cffea0313eac79479ab6f9b7cb34",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "18/09/13 8:12 AM",
      "commitNameOld": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.12,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,61 @@\n   private long initNodes(DatanodeInfo[] datanodes) {\n     // compute average utilization\n     for (DatanodeInfo datanode : datanodes) {\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue; // ignore decommissioning or decommissioned nodes\n       }\n       policy.accumulateSpaces(datanode);\n     }\n     policy.initAvgUtilization();\n \n     /*create network topology and all data node lists: \n      * overloaded, above-average, below-average, and underloaded\n      * we alternates the accessing of the given datanodes array either by\n      * an increasing order or a decreasing order.\n      */  \n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n     for (DatanodeInfo datanode : DFSUtil.shuffle(datanodes)) {\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue; // ignore decommissioning or decommissioned nodes\n       }\n       cluster.add(datanode);\n       BalancerDatanode datanodeS;\n       final double avg \u003d policy.getAvgUtilization();\n       if (policy.getUtilization(datanode) \u003e\u003d avg) {\n         datanodeS \u003d new Source(datanode, policy, threshold);\n         if (isAboveAvgUtilized(datanodeS)) {\n           this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n         } else {\n           assert(isOverUtilized(datanodeS)) :\n             datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n           this.overUtilizedDatanodes.add((Source)datanodeS);\n           overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n               -threshold)*datanodeS.datanode.getCapacity()/100.0);\n         }\n       } else {\n         datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n         if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n           this.belowAvgUtilizedDatanodes.add(datanodeS);\n         } else {\n           assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n               + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n               + \", utilization\u003d\" + datanodeS.utilization; \n           this.underUtilizedDatanodes.add(datanodeS);\n           underLoadedBytes +\u003d (long)((avg-threshold-\n               datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n         }\n       }\n-      this.datanodes.put(datanode.getStorageID(), datanodeS);\n+      this.datanodes.put(datanode.getDatanodeUuid(), datanodeS);\n     }\n \n     //logging\n     logNodes();\n     \n     assert (this.datanodes.size() \u003d\u003d \n       overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n       aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n       : \"Mismatched number of datanodes\";\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long initNodes(DatanodeInfo[] datanodes) {\n    // compute average utilization\n    for (DatanodeInfo datanode : datanodes) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      policy.accumulateSpaces(datanode);\n    }\n    policy.initAvgUtilization();\n\n    /*create network topology and all data node lists: \n     * overloaded, above-average, below-average, and underloaded\n     * we alternates the accessing of the given datanodes array either by\n     * an increasing order or a decreasing order.\n     */  \n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for (DatanodeInfo datanode : DFSUtil.shuffle(datanodes)) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      cluster.add(datanode);\n      BalancerDatanode datanodeS;\n      final double avg \u003d policy.getAvgUtilization();\n      if (policy.getUtilization(datanode) \u003e\u003d avg) {\n        datanodeS \u003d new Source(datanode, policy, threshold);\n        if (isAboveAvgUtilized(datanodeS)) {\n          this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n        } else {\n          assert(isOverUtilized(datanodeS)) :\n            datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n          this.overUtilizedDatanodes.add((Source)datanodeS);\n          overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n              -threshold)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      } else {\n        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n        if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n          this.belowAvgUtilizedDatanodes.add(datanodeS);\n        } else {\n          assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n              + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n              + \", utilization\u003d\" + datanodeS.utilization; \n          this.underUtilizedDatanodes.add(datanodeS);\n          underLoadedBytes +\u003d (long)((avg-threshold-\n              datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      }\n      this.datanodes.put(datanode.getDatanodeUuid(), datanodeS);\n    }\n\n    //logging\n    logNodes();\n    \n    assert (this.datanodes.size() \u003d\u003d \n      overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n      aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n      : \"Mismatched number of datanodes\";\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "abf09f090f77a7e54e331b7a07354e7926b60dc9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4990. Change BlockPlacementPolicy to choose storages instead of datanodes.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1524444 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/09/13 8:12 AM",
      "commitName": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "16/07/13 7:04 AM",
      "commitNameOld": "2d6049f1d683529c9e6372f33ee9b91fcfdbd3bd",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 64.05,
      "commitsBetweenForRepo": 382,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,61 @@\n   private long initNodes(DatanodeInfo[] datanodes) {\n     // compute average utilization\n     for (DatanodeInfo datanode : datanodes) {\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue; // ignore decommissioning or decommissioned nodes\n       }\n       policy.accumulateSpaces(datanode);\n     }\n     policy.initAvgUtilization();\n \n     /*create network topology and all data node lists: \n      * overloaded, above-average, below-average, and underloaded\n      * we alternates the accessing of the given datanodes array either by\n      * an increasing order or a decreasing order.\n      */  \n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n-    shuffleArray(datanodes);\n-    for (DatanodeInfo datanode : datanodes) {\n+    for (DatanodeInfo datanode : DFSUtil.shuffle(datanodes)) {\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue; // ignore decommissioning or decommissioned nodes\n       }\n       cluster.add(datanode);\n       BalancerDatanode datanodeS;\n       final double avg \u003d policy.getAvgUtilization();\n       if (policy.getUtilization(datanode) \u003e\u003d avg) {\n         datanodeS \u003d new Source(datanode, policy, threshold);\n         if (isAboveAvgUtilized(datanodeS)) {\n           this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n         } else {\n           assert(isOverUtilized(datanodeS)) :\n             datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n           this.overUtilizedDatanodes.add((Source)datanodeS);\n           overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n               -threshold)*datanodeS.datanode.getCapacity()/100.0);\n         }\n       } else {\n         datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n         if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n           this.belowAvgUtilizedDatanodes.add(datanodeS);\n         } else {\n           assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n               + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n               + \", utilization\u003d\" + datanodeS.utilization; \n           this.underUtilizedDatanodes.add(datanodeS);\n           underLoadedBytes +\u003d (long)((avg-threshold-\n               datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n         }\n       }\n       this.datanodes.put(datanode.getStorageID(), datanodeS);\n     }\n \n     //logging\n     logNodes();\n     \n     assert (this.datanodes.size() \u003d\u003d \n       overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n       aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n       : \"Mismatched number of datanodes\";\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long initNodes(DatanodeInfo[] datanodes) {\n    // compute average utilization\n    for (DatanodeInfo datanode : datanodes) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      policy.accumulateSpaces(datanode);\n    }\n    policy.initAvgUtilization();\n\n    /*create network topology and all data node lists: \n     * overloaded, above-average, below-average, and underloaded\n     * we alternates the accessing of the given datanodes array either by\n     * an increasing order or a decreasing order.\n     */  \n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    for (DatanodeInfo datanode : DFSUtil.shuffle(datanodes)) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      cluster.add(datanode);\n      BalancerDatanode datanodeS;\n      final double avg \u003d policy.getAvgUtilization();\n      if (policy.getUtilization(datanode) \u003e\u003d avg) {\n        datanodeS \u003d new Source(datanode, policy, threshold);\n        if (isAboveAvgUtilized(datanodeS)) {\n          this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n        } else {\n          assert(isOverUtilized(datanodeS)) :\n            datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n          this.overUtilizedDatanodes.add((Source)datanodeS);\n          overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n              -threshold)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      } else {\n        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n        if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n          this.belowAvgUtilizedDatanodes.add(datanodeS);\n        } else {\n          assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n              + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n              + \", utilization\u003d\" + datanodeS.utilization; \n          this.underUtilizedDatanodes.add(datanodeS);\n          underLoadedBytes +\u003d (long)((avg-threshold-\n              datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      }\n      this.datanodes.put(datanode.getStorageID(), datanodeS);\n    }\n\n    //logging\n    logNodes();\n    \n    assert (this.datanodes.size() \u003d\u003d \n      overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n      aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n      : \"Mismatched number of datanodes\";\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "e4df14f8f151413a8ec0972a21e31a0b51fe0fb0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3417. Rename BalancerDatanode#getName to getDisplayName to be consistent with Datanode. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1338767 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/05/12 9:01 AM",
      "commitName": "e4df14f8f151413a8ec0972a21e31a0b51fe0fb0",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "30/04/12 11:44 PM",
      "commitNameOld": "d37ec9d09e02f16396c296e72ff8c1b7c3e6ed10",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 14.39,
      "commitsBetweenForRepo": 96,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,62 @@\n   private long initNodes(DatanodeInfo[] datanodes) {\n     // compute average utilization\n     for (DatanodeInfo datanode : datanodes) {\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue; // ignore decommissioning or decommissioned nodes\n       }\n       policy.accumulateSpaces(datanode);\n     }\n     policy.initAvgUtilization();\n \n     /*create network topology and all data node lists: \n      * overloaded, above-average, below-average, and underloaded\n      * we alternates the accessing of the given datanodes array either by\n      * an increasing order or a decreasing order.\n      */  \n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n     shuffleArray(datanodes);\n     for (DatanodeInfo datanode : datanodes) {\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue; // ignore decommissioning or decommissioned nodes\n       }\n       cluster.add(datanode);\n       BalancerDatanode datanodeS;\n       final double avg \u003d policy.getAvgUtilization();\n       if (policy.getUtilization(datanode) \u003e\u003d avg) {\n         datanodeS \u003d new Source(datanode, policy, threshold);\n         if (isAboveAvgUtilized(datanodeS)) {\n           this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n         } else {\n           assert(isOverUtilized(datanodeS)) :\n-            datanodeS.getName()+ \"is not an overUtilized node\";\n+            datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n           this.overUtilizedDatanodes.add((Source)datanodeS);\n           overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n               -threshold)*datanodeS.datanode.getCapacity()/100.0);\n         }\n       } else {\n         datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n         if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n           this.belowAvgUtilizedDatanodes.add(datanodeS);\n         } else {\n           assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n-              + datanodeS.getName() + \")\u003d\" + isUnderUtilized(datanodeS)\n+              + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n               + \", utilization\u003d\" + datanodeS.utilization; \n           this.underUtilizedDatanodes.add(datanodeS);\n           underLoadedBytes +\u003d (long)((avg-threshold-\n               datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n         }\n       }\n       this.datanodes.put(datanode.getStorageID(), datanodeS);\n     }\n \n     //logging\n     logNodes();\n     \n     assert (this.datanodes.size() \u003d\u003d \n       overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n       aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n       : \"Mismatched number of datanodes\";\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long initNodes(DatanodeInfo[] datanodes) {\n    // compute average utilization\n    for (DatanodeInfo datanode : datanodes) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      policy.accumulateSpaces(datanode);\n    }\n    policy.initAvgUtilization();\n\n    /*create network topology and all data node lists: \n     * overloaded, above-average, below-average, and underloaded\n     * we alternates the accessing of the given datanodes array either by\n     * an increasing order or a decreasing order.\n     */  \n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    shuffleArray(datanodes);\n    for (DatanodeInfo datanode : datanodes) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      cluster.add(datanode);\n      BalancerDatanode datanodeS;\n      final double avg \u003d policy.getAvgUtilization();\n      if (policy.getUtilization(datanode) \u003e\u003d avg) {\n        datanodeS \u003d new Source(datanode, policy, threshold);\n        if (isAboveAvgUtilized(datanodeS)) {\n          this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n        } else {\n          assert(isOverUtilized(datanodeS)) :\n            datanodeS.getDisplayName()+ \"is not an overUtilized node\";\n          this.overUtilizedDatanodes.add((Source)datanodeS);\n          overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n              -threshold)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      } else {\n        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n        if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n          this.belowAvgUtilizedDatanodes.add(datanodeS);\n        } else {\n          assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n              + datanodeS.getDisplayName() + \")\u003d\" + isUnderUtilized(datanodeS)\n              + \", utilization\u003d\" + datanodeS.utilization; \n          this.underUtilizedDatanodes.add(datanodeS);\n          underLoadedBytes +\u003d (long)((avg-threshold-\n              datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      }\n      this.datanodes.put(datanode.getStorageID(), datanodeS);\n    }\n\n    //logging\n    logNodes();\n    \n    assert (this.datanodes.size() \u003d\u003d \n      overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n      aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n      : \"Mismatched number of datanodes\";\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "7dd869c2a98998aea457e522ad4b7c5403312482": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2491. TestBalancer can fail when datanode utilization and avgUtilization is exactly same. Contributed by Uma Maheswara Rao G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1187837 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/10/11 5:35 PM",
      "commitName": "7dd869c2a98998aea457e522ad4b7c5403312482",
      "commitAuthor": "Konstantin Shvachko",
      "commitDateOld": "14/10/11 11:31 AM",
      "commitNameOld": "4cb7a4854d9b24f01bb0fe20ec3eae32c36aa8c6",
      "commitAuthorOld": "Konstantin Shvachko",
      "daysBetweenCommits": 8.25,
      "commitsBetweenForRepo": 62,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,62 @@\n   private long initNodes(DatanodeInfo[] datanodes) {\n     // compute average utilization\n     for (DatanodeInfo datanode : datanodes) {\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue; // ignore decommissioning or decommissioned nodes\n       }\n       policy.accumulateSpaces(datanode);\n     }\n     policy.initAvgUtilization();\n \n     /*create network topology and all data node lists: \n      * overloaded, above-average, below-average, and underloaded\n      * we alternates the accessing of the given datanodes array either by\n      * an increasing order or a decreasing order.\n      */  \n     long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n     shuffleArray(datanodes);\n     for (DatanodeInfo datanode : datanodes) {\n       if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n         continue; // ignore decommissioning or decommissioned nodes\n       }\n       cluster.add(datanode);\n       BalancerDatanode datanodeS;\n       final double avg \u003d policy.getAvgUtilization();\n-      if (policy.getUtilization(datanode) \u003e avg) {\n+      if (policy.getUtilization(datanode) \u003e\u003d avg) {\n         datanodeS \u003d new Source(datanode, policy, threshold);\n         if (isAboveAvgUtilized(datanodeS)) {\n           this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n         } else {\n           assert(isOverUtilized(datanodeS)) :\n             datanodeS.getName()+ \"is not an overUtilized node\";\n           this.overUtilizedDatanodes.add((Source)datanodeS);\n           overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n               -threshold)*datanodeS.datanode.getCapacity()/100.0);\n         }\n       } else {\n         datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n         if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n           this.belowAvgUtilizedDatanodes.add(datanodeS);\n         } else {\n           assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n               + datanodeS.getName() + \")\u003d\" + isUnderUtilized(datanodeS)\n               + \", utilization\u003d\" + datanodeS.utilization; \n           this.underUtilizedDatanodes.add(datanodeS);\n           underLoadedBytes +\u003d (long)((avg-threshold-\n               datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n         }\n       }\n       this.datanodes.put(datanode.getStorageID(), datanodeS);\n     }\n \n     //logging\n     logNodes();\n     \n     assert (this.datanodes.size() \u003d\u003d \n       overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n       aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n       : \"Mismatched number of datanodes\";\n     \n     // return number of bytes to be moved in order to make the cluster balanced\n     return Math.max(overLoadedBytes, underLoadedBytes);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private long initNodes(DatanodeInfo[] datanodes) {\n    // compute average utilization\n    for (DatanodeInfo datanode : datanodes) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      policy.accumulateSpaces(datanode);\n    }\n    policy.initAvgUtilization();\n\n    /*create network topology and all data node lists: \n     * overloaded, above-average, below-average, and underloaded\n     * we alternates the accessing of the given datanodes array either by\n     * an increasing order or a decreasing order.\n     */  \n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    shuffleArray(datanodes);\n    for (DatanodeInfo datanode : datanodes) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      cluster.add(datanode);\n      BalancerDatanode datanodeS;\n      final double avg \u003d policy.getAvgUtilization();\n      if (policy.getUtilization(datanode) \u003e\u003d avg) {\n        datanodeS \u003d new Source(datanode, policy, threshold);\n        if (isAboveAvgUtilized(datanodeS)) {\n          this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n        } else {\n          assert(isOverUtilized(datanodeS)) :\n            datanodeS.getName()+ \"is not an overUtilized node\";\n          this.overUtilizedDatanodes.add((Source)datanodeS);\n          overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n              -threshold)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      } else {\n        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n        if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n          this.belowAvgUtilizedDatanodes.add(datanodeS);\n        } else {\n          assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n              + datanodeS.getName() + \")\u003d\" + isUnderUtilized(datanodeS)\n              + \", utilization\u003d\" + datanodeS.utilization; \n          this.underUtilizedDatanodes.add(datanodeS);\n          underLoadedBytes +\u003d (long)((avg-threshold-\n              datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      }\n      this.datanodes.put(datanode.getStorageID(), datanodeS);\n    }\n\n    //logging\n    logNodes();\n    \n    assert (this.datanodes.size() \u003d\u003d \n      overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n      aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n      : \"Mismatched number of datanodes\";\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private long initNodes(DatanodeInfo[] datanodes) {\n    // compute average utilization\n    for (DatanodeInfo datanode : datanodes) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      policy.accumulateSpaces(datanode);\n    }\n    policy.initAvgUtilization();\n\n    /*create network topology and all data node lists: \n     * overloaded, above-average, below-average, and underloaded\n     * we alternates the accessing of the given datanodes array either by\n     * an increasing order or a decreasing order.\n     */  \n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    shuffleArray(datanodes);\n    for (DatanodeInfo datanode : datanodes) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      cluster.add(datanode);\n      BalancerDatanode datanodeS;\n      final double avg \u003d policy.getAvgUtilization();\n      if (policy.getUtilization(datanode) \u003e avg) {\n        datanodeS \u003d new Source(datanode, policy, threshold);\n        if (isAboveAvgUtilized(datanodeS)) {\n          this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n        } else {\n          assert(isOverUtilized(datanodeS)) :\n            datanodeS.getName()+ \"is not an overUtilized node\";\n          this.overUtilizedDatanodes.add((Source)datanodeS);\n          overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n              -threshold)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      } else {\n        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n        if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n          this.belowAvgUtilizedDatanodes.add(datanodeS);\n        } else {\n          assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n              + datanodeS.getName() + \")\u003d\" + isUnderUtilized(datanodeS)\n              + \", utilization\u003d\" + datanodeS.utilization; \n          this.underUtilizedDatanodes.add(datanodeS);\n          underLoadedBytes +\u003d (long)((avg-threshold-\n              datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      }\n      this.datanodes.put(datanode.getStorageID(), datanodeS);\n    }\n\n    //logging\n    logNodes();\n    \n    assert (this.datanodes.size() \u003d\u003d \n      overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n      aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n      : \"Mismatched number of datanodes\";\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private long initNodes(DatanodeInfo[] datanodes) {\n    // compute average utilization\n    for (DatanodeInfo datanode : datanodes) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      policy.accumulateSpaces(datanode);\n    }\n    policy.initAvgUtilization();\n\n    /*create network topology and all data node lists: \n     * overloaded, above-average, below-average, and underloaded\n     * we alternates the accessing of the given datanodes array either by\n     * an increasing order or a decreasing order.\n     */  \n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    shuffleArray(datanodes);\n    for (DatanodeInfo datanode : datanodes) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      cluster.add(datanode);\n      BalancerDatanode datanodeS;\n      final double avg \u003d policy.getAvgUtilization();\n      if (policy.getUtilization(datanode) \u003e avg) {\n        datanodeS \u003d new Source(datanode, policy, threshold);\n        if (isAboveAvgUtilized(datanodeS)) {\n          this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n        } else {\n          assert(isOverUtilized(datanodeS)) :\n            datanodeS.getName()+ \"is not an overUtilized node\";\n          this.overUtilizedDatanodes.add((Source)datanodeS);\n          overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n              -threshold)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      } else {\n        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n        if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n          this.belowAvgUtilizedDatanodes.add(datanodeS);\n        } else {\n          assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n              + datanodeS.getName() + \")\u003d\" + isUnderUtilized(datanodeS)\n              + \", utilization\u003d\" + datanodeS.utilization; \n          this.underUtilizedDatanodes.add(datanodeS);\n          underLoadedBytes +\u003d (long)((avg-threshold-\n              datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      }\n      this.datanodes.put(datanode.getStorageID(), datanodeS);\n    }\n\n    //logging\n    logNodes();\n    \n    assert (this.datanodes.size() \u003d\u003d \n      overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n      aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n      : \"Mismatched number of datanodes\";\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,62 @@\n+  private long initNodes(DatanodeInfo[] datanodes) {\n+    // compute average utilization\n+    for (DatanodeInfo datanode : datanodes) {\n+      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n+        continue; // ignore decommissioning or decommissioned nodes\n+      }\n+      policy.accumulateSpaces(datanode);\n+    }\n+    policy.initAvgUtilization();\n+\n+    /*create network topology and all data node lists: \n+     * overloaded, above-average, below-average, and underloaded\n+     * we alternates the accessing of the given datanodes array either by\n+     * an increasing order or a decreasing order.\n+     */  \n+    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n+    shuffleArray(datanodes);\n+    for (DatanodeInfo datanode : datanodes) {\n+      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n+        continue; // ignore decommissioning or decommissioned nodes\n+      }\n+      cluster.add(datanode);\n+      BalancerDatanode datanodeS;\n+      final double avg \u003d policy.getAvgUtilization();\n+      if (policy.getUtilization(datanode) \u003e avg) {\n+        datanodeS \u003d new Source(datanode, policy, threshold);\n+        if (isAboveAvgUtilized(datanodeS)) {\n+          this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n+        } else {\n+          assert(isOverUtilized(datanodeS)) :\n+            datanodeS.getName()+ \"is not an overUtilized node\";\n+          this.overUtilizedDatanodes.add((Source)datanodeS);\n+          overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n+              -threshold)*datanodeS.datanode.getCapacity()/100.0);\n+        }\n+      } else {\n+        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n+        if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n+          this.belowAvgUtilizedDatanodes.add(datanodeS);\n+        } else {\n+          assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n+              + datanodeS.getName() + \")\u003d\" + isUnderUtilized(datanodeS)\n+              + \", utilization\u003d\" + datanodeS.utilization; \n+          this.underUtilizedDatanodes.add(datanodeS);\n+          underLoadedBytes +\u003d (long)((avg-threshold-\n+              datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n+        }\n+      }\n+      this.datanodes.put(datanode.getStorageID(), datanodeS);\n+    }\n+\n+    //logging\n+    logNodes();\n+    \n+    assert (this.datanodes.size() \u003d\u003d \n+      overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n+      aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n+      : \"Mismatched number of datanodes\";\n+    \n+    // return number of bytes to be moved in order to make the cluster balanced\n+    return Math.max(overLoadedBytes, underLoadedBytes);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private long initNodes(DatanodeInfo[] datanodes) {\n    // compute average utilization\n    for (DatanodeInfo datanode : datanodes) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      policy.accumulateSpaces(datanode);\n    }\n    policy.initAvgUtilization();\n\n    /*create network topology and all data node lists: \n     * overloaded, above-average, below-average, and underloaded\n     * we alternates the accessing of the given datanodes array either by\n     * an increasing order or a decreasing order.\n     */  \n    long overLoadedBytes \u003d 0L, underLoadedBytes \u003d 0L;\n    shuffleArray(datanodes);\n    for (DatanodeInfo datanode : datanodes) {\n      if (datanode.isDecommissioned() || datanode.isDecommissionInProgress()) {\n        continue; // ignore decommissioning or decommissioned nodes\n      }\n      cluster.add(datanode);\n      BalancerDatanode datanodeS;\n      final double avg \u003d policy.getAvgUtilization();\n      if (policy.getUtilization(datanode) \u003e avg) {\n        datanodeS \u003d new Source(datanode, policy, threshold);\n        if (isAboveAvgUtilized(datanodeS)) {\n          this.aboveAvgUtilizedDatanodes.add((Source)datanodeS);\n        } else {\n          assert(isOverUtilized(datanodeS)) :\n            datanodeS.getName()+ \"is not an overUtilized node\";\n          this.overUtilizedDatanodes.add((Source)datanodeS);\n          overLoadedBytes +\u003d (long)((datanodeS.utilization-avg\n              -threshold)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      } else {\n        datanodeS \u003d new BalancerDatanode(datanode, policy, threshold);\n        if ( isBelowOrEqualAvgUtilized(datanodeS)) {\n          this.belowAvgUtilizedDatanodes.add(datanodeS);\n        } else {\n          assert isUnderUtilized(datanodeS) : \"isUnderUtilized(\"\n              + datanodeS.getName() + \")\u003d\" + isUnderUtilized(datanodeS)\n              + \", utilization\u003d\" + datanodeS.utilization; \n          this.underUtilizedDatanodes.add(datanodeS);\n          underLoadedBytes +\u003d (long)((avg-threshold-\n              datanodeS.utilization)*datanodeS.datanode.getCapacity()/100.0);\n        }\n      }\n      this.datanodes.put(datanode.getStorageID(), datanodeS);\n    }\n\n    //logging\n    logNodes();\n    \n    assert (this.datanodes.size() \u003d\u003d \n      overUtilizedDatanodes.size()+underUtilizedDatanodes.size()+\n      aboveAvgUtilizedDatanodes.size()+belowAvgUtilizedDatanodes.size())\n      : \"Mismatched number of datanodes\";\n    \n    // return number of bytes to be moved in order to make the cluster balanced\n    return Math.max(overLoadedBytes, underLoadedBytes);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java"
    }
  }
}