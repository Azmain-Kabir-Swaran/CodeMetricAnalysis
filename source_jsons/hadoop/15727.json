{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "HttpFSFileSystem.java",
  "functionName": "concat",
  "functionId": "concat___f-Path__psrcs-Path[]",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/client/HttpFSFileSystem.java",
  "functionStartLine": 664,
  "functionEndLine": 677,
  "numCommitsSeen": 51,
  "timeTaken": 1774,
  "changeHistory": [
    "70b218748badf079c859c3af2b468a0b7b49c333",
    "bbdae834d2ec26b329b48b4c9343ebb182a63242"
  ],
  "changeHistoryShort": {
    "70b218748badf079c859c3af2b468a0b7b49c333": "Ybodychange",
    "bbdae834d2ec26b329b48b4c9343ebb182a63242": "Yintroduced"
  },
  "changeHistoryDetails": {
    "70b218748badf079c859c3af2b468a0b7b49c333": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11015. Http server/client utils to propagate and recreate Exceptions from server to client. (tucu)\n",
      "commitDate": "04/09/14 9:11 AM",
      "commitName": "70b218748badf079c859c3af2b468a0b7b49c333",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "07/08/14 9:58 PM",
      "commitNameOld": "be9c67930b57c516723d566625f9036a88a84055",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 27.47,
      "commitsBetweenForRepo": 206,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n   public void concat(Path f, Path[] psrcs) throws IOException {\n     List\u003cString\u003e strPaths \u003d new ArrayList\u003cString\u003e(psrcs.length);\n     for(Path psrc : psrcs) {\n       strPaths.add(psrc.toUri().getPath());\n     }\n     String srcs \u003d StringUtils.join(\",\", strPaths);\n \n     Map\u003cString, String\u003e params \u003d new HashMap\u003cString, String\u003e();\n     params.put(OP_PARAM, Operation.CONCAT.toString());\n     params.put(SOURCES_PARAM, srcs);\n     HttpURLConnection conn \u003d getConnection(Operation.CONCAT.getMethod(),\n         params, f, true);\n-    HttpFSUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\n+    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void concat(Path f, Path[] psrcs) throws IOException {\n    List\u003cString\u003e strPaths \u003d new ArrayList\u003cString\u003e(psrcs.length);\n    for(Path psrc : psrcs) {\n      strPaths.add(psrc.toUri().getPath());\n    }\n    String srcs \u003d StringUtils.join(\",\", strPaths);\n\n    Map\u003cString, String\u003e params \u003d new HashMap\u003cString, String\u003e();\n    params.put(OP_PARAM, Operation.CONCAT.toString());\n    params.put(SOURCES_PARAM, srcs);\n    HttpURLConnection conn \u003d getConnection(Operation.CONCAT.getMethod(),\n        params, f, true);\n    HttpExceptionUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/client/HttpFSFileSystem.java",
      "extendedDetails": {}
    },
    "bbdae834d2ec26b329b48b4c9343ebb182a63242": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4456. Add concat to HttpFS and WebHDFS REST API docs. (plamenj2003 via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1441603 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/02/13 11:42 AM",
      "commitName": "bbdae834d2ec26b329b48b4c9343ebb182a63242",
      "commitAuthor": "Alejandro Abdelnur",
      "diff": "@@ -0,0 +1,14 @@\n+  public void concat(Path f, Path[] psrcs) throws IOException {\n+    List\u003cString\u003e strPaths \u003d new ArrayList\u003cString\u003e(psrcs.length);\n+    for(Path psrc : psrcs) {\n+      strPaths.add(psrc.toUri().getPath());\n+    }\n+    String srcs \u003d StringUtils.join(\",\", strPaths);\n+\n+    Map\u003cString, String\u003e params \u003d new HashMap\u003cString, String\u003e();\n+    params.put(OP_PARAM, Operation.CONCAT.toString());\n+    params.put(SOURCES_PARAM, srcs);\n+    HttpURLConnection conn \u003d getConnection(Operation.CONCAT.getMethod(),\n+        params, f, true);\n+    HttpFSUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void concat(Path f, Path[] psrcs) throws IOException {\n    List\u003cString\u003e strPaths \u003d new ArrayList\u003cString\u003e(psrcs.length);\n    for(Path psrc : psrcs) {\n      strPaths.add(psrc.toUri().getPath());\n    }\n    String srcs \u003d StringUtils.join(\",\", strPaths);\n\n    Map\u003cString, String\u003e params \u003d new HashMap\u003cString, String\u003e();\n    params.put(OP_PARAM, Operation.CONCAT.toString());\n    params.put(SOURCES_PARAM, srcs);\n    HttpURLConnection conn \u003d getConnection(Operation.CONCAT.getMethod(),\n        params, f, true);\n    HttpFSUtils.validateResponse(conn, HttpURLConnection.HTTP_OK);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/client/HttpFSFileSystem.java"
    }
  }
}