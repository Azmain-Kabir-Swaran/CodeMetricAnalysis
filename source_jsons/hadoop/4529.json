{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PBHelper.java",
  "functionName": "convert",
  "functionId": "convert___cmd-BlockCommand",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
  "functionStartLine": 502,
  "functionEndLine": 529,
  "numCommitsSeen": 195,
  "timeTaken": 6945,
  "changeHistory": [
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "3e2a0b5446bce51871ab3e1262a0ac6bd365e94f",
    "15d08c4778350a86d7bae0174aeb48f8d8f61cce",
    "eb9f1b670726e1af03f2e940ce2696b880964972",
    "abf09f090f77a7e54e331b7a07354e7926b60dc9",
    "40eb94ade3161d93e7a762a839004748f6d0ae89",
    "5cdb7e5ce7f0c3129749be8f29e2f11c0e0f2269",
    "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e",
    "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c",
    "48da033901d3471ef176a94104158546152353e9",
    "38a19bc293dec6221ae96e304fc6ab660d94e706"
  ],
  "changeHistoryShort": {
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ybodychange",
    "3e2a0b5446bce51871ab3e1262a0ac6bd365e94f": "Ybodychange",
    "15d08c4778350a86d7bae0174aeb48f8d8f61cce": "Ybodychange",
    "eb9f1b670726e1af03f2e940ce2696b880964972": "Ybodychange",
    "abf09f090f77a7e54e331b7a07354e7926b60dc9": "Ybodychange",
    "40eb94ade3161d93e7a762a839004748f6d0ae89": "Ybodychange",
    "5cdb7e5ce7f0c3129749be8f29e2f11c0e0f2269": "Ybodychange",
    "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e": "Ybodychange",
    "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c": "Ybodychange",
    "48da033901d3471ef176a94104158546152353e9": "Ybodychange",
    "38a19bc293dec6221ae96e304fc6ab660d94e706": "Yintroduced"
  },
  "changeHistoryDetails": {
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "19/09/15 6:08 PM",
      "commitNameOld": "3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 2.03,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   public static BlockCommandProto convert(BlockCommand cmd) {\n     BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n         .setBlockPoolId(cmd.getBlockPoolId());\n     switch (cmd.getAction()) {\n     case DatanodeProtocol.DNA_TRANSFER:\n       builder.setAction(BlockCommandProto.Action.TRANSFER);\n       break;\n     case DatanodeProtocol.DNA_INVALIDATE:\n       builder.setAction(BlockCommandProto.Action.INVALIDATE);\n       break;\n     case DatanodeProtocol.DNA_SHUTDOWN:\n       builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n       break;\n     default:\n       throw new AssertionError(\"Invalid action\");\n     }\n     Block[] blocks \u003d cmd.getBlocks();\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n-      builder.addBlocks(PBHelper.convert(blocks[i]));\n+      builder.addBlocks(PBHelperClient.convert(blocks[i]));\n     }\n-    builder.addAllTargets(convert(cmd.getTargets()))\n+    builder.addAllTargets(PBHelperClient.convert(cmd.getTargets()))\n            .addAllTargetStorageUuids(convert(cmd.getTargetStorageIDs()));\n     StorageType[][] types \u003d cmd.getTargetStorageTypes();\n     if (types !\u003d null) {\n-      builder.addAllTargetStorageTypes(convert(types));\n+      builder.addAllTargetStorageTypes(PBHelperClient.convert(types));\n     }\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockCommandProto convert(BlockCommand cmd) {\n    BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n        .setBlockPoolId(cmd.getBlockPoolId());\n    switch (cmd.getAction()) {\n    case DatanodeProtocol.DNA_TRANSFER:\n      builder.setAction(BlockCommandProto.Action.TRANSFER);\n      break;\n    case DatanodeProtocol.DNA_INVALIDATE:\n      builder.setAction(BlockCommandProto.Action.INVALIDATE);\n      break;\n    case DatanodeProtocol.DNA_SHUTDOWN:\n      builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n      break;\n    default:\n      throw new AssertionError(\"Invalid action\");\n    }\n    Block[] blocks \u003d cmd.getBlocks();\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      builder.addBlocks(PBHelperClient.convert(blocks[i]));\n    }\n    builder.addAllTargets(PBHelperClient.convert(cmd.getTargets()))\n           .addAllTargetStorageUuids(convert(cmd.getTargetStorageIDs()));\n    StorageType[][] types \u003d cmd.getTargetStorageTypes();\n    if (types !\u003d null) {\n      builder.addAllTargetStorageTypes(PBHelperClient.convert(types));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "3e2a0b5446bce51871ab3e1262a0ac6bd365e94f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6969. Archival Storage: INode#getStoragePolicyID should always return the latest storage policy. Contributed by Jing Zhao.\n",
      "commitDate": "01/09/14 5:56 PM",
      "commitName": "3e2a0b5446bce51871ab3e1262a0ac6bd365e94f",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "20/08/14 10:22 PM",
      "commitNameOld": "c92d869d02af4d7870649317474134ba94b1dd0c",
      "commitAuthorOld": "",
      "daysBetweenCommits": 11.82,
      "commitsBetweenForRepo": 34,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,28 @@\n   public static BlockCommandProto convert(BlockCommand cmd) {\n     BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n         .setBlockPoolId(cmd.getBlockPoolId());\n     switch (cmd.getAction()) {\n     case DatanodeProtocol.DNA_TRANSFER:\n       builder.setAction(BlockCommandProto.Action.TRANSFER);\n       break;\n     case DatanodeProtocol.DNA_INVALIDATE:\n       builder.setAction(BlockCommandProto.Action.INVALIDATE);\n       break;\n     case DatanodeProtocol.DNA_SHUTDOWN:\n       builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n       break;\n     default:\n       throw new AssertionError(\"Invalid action\");\n     }\n     Block[] blocks \u003d cmd.getBlocks();\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       builder.addBlocks(PBHelper.convert(blocks[i]));\n     }\n     builder.addAllTargets(convert(cmd.getTargets()))\n            .addAllTargetStorageUuids(convert(cmd.getTargetStorageIDs()));\n+    StorageType[][] types \u003d cmd.getTargetStorageTypes();\n+    if (types !\u003d null) {\n+      builder.addAllTargetStorageTypes(convert(types));\n+    }\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockCommandProto convert(BlockCommand cmd) {\n    BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n        .setBlockPoolId(cmd.getBlockPoolId());\n    switch (cmd.getAction()) {\n    case DatanodeProtocol.DNA_TRANSFER:\n      builder.setAction(BlockCommandProto.Action.TRANSFER);\n      break;\n    case DatanodeProtocol.DNA_INVALIDATE:\n      builder.setAction(BlockCommandProto.Action.INVALIDATE);\n      break;\n    case DatanodeProtocol.DNA_SHUTDOWN:\n      builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n      break;\n    default:\n      throw new AssertionError(\"Invalid action\");\n    }\n    Block[] blocks \u003d cmd.getBlocks();\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      builder.addBlocks(PBHelper.convert(blocks[i]));\n    }\n    builder.addAllTargets(convert(cmd.getTargets()))\n           .addAllTargetStorageUuids(convert(cmd.getTargetStorageIDs()));\n    StorageType[][] types \u003d cmd.getTargetStorageTypes();\n    if (types !\u003d null) {\n      builder.addAllTargetStorageTypes(convert(types));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "15d08c4778350a86d7bae0174aeb48f8d8f61cce": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5349. DNA_CACHE and DNA_UNCACHE should be by blockId only (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532116 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/10/13 3:19 PM",
      "commitName": "15d08c4778350a86d7bae0174aeb48f8d8f61cce",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "09/10/13 2:30 PM",
      "commitNameOld": "3fc8792b5c75fca9fc4f6cf4b95fb2927c62e624",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 5.03,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,23 @@\n   public static BlockCommandProto convert(BlockCommand cmd) {\n     BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n         .setBlockPoolId(cmd.getBlockPoolId());\n     switch (cmd.getAction()) {\n     case DatanodeProtocol.DNA_TRANSFER:\n       builder.setAction(BlockCommandProto.Action.TRANSFER);\n       break;\n     case DatanodeProtocol.DNA_INVALIDATE:\n       builder.setAction(BlockCommandProto.Action.INVALIDATE);\n       break;\n     case DatanodeProtocol.DNA_SHUTDOWN:\n       builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n       break;\n-    case DatanodeProtocol.DNA_CACHE:\n-      builder.setAction(BlockCommandProto.Action.CACHE);\n-      break;\n-    case DatanodeProtocol.DNA_UNCACHE:\n-      builder.setAction(BlockCommandProto.Action.UNCACHE);\n-      break;\n     default:\n       throw new AssertionError(\"Invalid action\");\n     }\n     Block[] blocks \u003d cmd.getBlocks();\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       builder.addBlocks(PBHelper.convert(blocks[i]));\n     }\n     builder.addAllTargets(PBHelper.convert(cmd.getTargets()));\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockCommandProto convert(BlockCommand cmd) {\n    BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n        .setBlockPoolId(cmd.getBlockPoolId());\n    switch (cmd.getAction()) {\n    case DatanodeProtocol.DNA_TRANSFER:\n      builder.setAction(BlockCommandProto.Action.TRANSFER);\n      break;\n    case DatanodeProtocol.DNA_INVALIDATE:\n      builder.setAction(BlockCommandProto.Action.INVALIDATE);\n      break;\n    case DatanodeProtocol.DNA_SHUTDOWN:\n      builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n      break;\n    default:\n      throw new AssertionError(\"Invalid action\");\n    }\n    Block[] blocks \u003d cmd.getBlocks();\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      builder.addBlocks(PBHelper.convert(blocks[i]));\n    }\n    builder.addAllTargets(PBHelper.convert(cmd.getTargets()));\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "eb9f1b670726e1af03f2e940ce2696b880964972": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5232. Protocol changes to transmit StorageUuid.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1525153 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/09/13 3:06 PM",
      "commitName": "eb9f1b670726e1af03f2e940ce2696b880964972",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "18/09/13 8:12 AM",
      "commitNameOld": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 2.29,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   public static BlockCommandProto convert(BlockCommand cmd) {\n     BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n         .setBlockPoolId(cmd.getBlockPoolId());\n     switch (cmd.getAction()) {\n     case DatanodeProtocol.DNA_TRANSFER:\n       builder.setAction(BlockCommandProto.Action.TRANSFER);\n       break;\n     case DatanodeProtocol.DNA_INVALIDATE:\n       builder.setAction(BlockCommandProto.Action.INVALIDATE);\n       break;\n     case DatanodeProtocol.DNA_SHUTDOWN:\n       builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n       break;\n     default:\n       throw new AssertionError(\"Invalid action\");\n     }\n     Block[] blocks \u003d cmd.getBlocks();\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       builder.addBlocks(PBHelper.convert(blocks[i]));\n     }\n     builder.addAllTargets(convert(cmd.getTargets()))\n-           .addAllTargetStorageIDs(convert(cmd.getTargetStorageIDs()));\n+           .addAllTargetStorageUuids(convert(cmd.getTargetStorageIDs()));\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockCommandProto convert(BlockCommand cmd) {\n    BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n        .setBlockPoolId(cmd.getBlockPoolId());\n    switch (cmd.getAction()) {\n    case DatanodeProtocol.DNA_TRANSFER:\n      builder.setAction(BlockCommandProto.Action.TRANSFER);\n      break;\n    case DatanodeProtocol.DNA_INVALIDATE:\n      builder.setAction(BlockCommandProto.Action.INVALIDATE);\n      break;\n    case DatanodeProtocol.DNA_SHUTDOWN:\n      builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n      break;\n    default:\n      throw new AssertionError(\"Invalid action\");\n    }\n    Block[] blocks \u003d cmd.getBlocks();\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      builder.addBlocks(PBHelper.convert(blocks[i]));\n    }\n    builder.addAllTargets(convert(cmd.getTargets()))\n           .addAllTargetStorageUuids(convert(cmd.getTargetStorageIDs()));\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "abf09f090f77a7e54e331b7a07354e7926b60dc9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4990. Change BlockPlacementPolicy to choose storages instead of datanodes.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1524444 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/09/13 8:12 AM",
      "commitName": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "03/09/13 7:03 AM",
      "commitNameOld": "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 15.05,
      "commitsBetweenForRepo": 72,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,24 @@\n   public static BlockCommandProto convert(BlockCommand cmd) {\n     BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n         .setBlockPoolId(cmd.getBlockPoolId());\n     switch (cmd.getAction()) {\n     case DatanodeProtocol.DNA_TRANSFER:\n       builder.setAction(BlockCommandProto.Action.TRANSFER);\n       break;\n     case DatanodeProtocol.DNA_INVALIDATE:\n       builder.setAction(BlockCommandProto.Action.INVALIDATE);\n       break;\n     case DatanodeProtocol.DNA_SHUTDOWN:\n       builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n       break;\n     default:\n       throw new AssertionError(\"Invalid action\");\n     }\n     Block[] blocks \u003d cmd.getBlocks();\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       builder.addBlocks(PBHelper.convert(blocks[i]));\n     }\n-    builder.addAllTargets(PBHelper.convert(cmd.getTargets()));\n+    builder.addAllTargets(convert(cmd.getTargets()))\n+           .addAllTargetStorageIDs(convert(cmd.getTargetStorageIDs()));\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockCommandProto convert(BlockCommand cmd) {\n    BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n        .setBlockPoolId(cmd.getBlockPoolId());\n    switch (cmd.getAction()) {\n    case DatanodeProtocol.DNA_TRANSFER:\n      builder.setAction(BlockCommandProto.Action.TRANSFER);\n      break;\n    case DatanodeProtocol.DNA_INVALIDATE:\n      builder.setAction(BlockCommandProto.Action.INVALIDATE);\n      break;\n    case DatanodeProtocol.DNA_SHUTDOWN:\n      builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n      break;\n    default:\n      throw new AssertionError(\"Invalid action\");\n    }\n    Block[] blocks \u003d cmd.getBlocks();\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      builder.addBlocks(PBHelper.convert(blocks[i]));\n    }\n    builder.addAllTargets(convert(cmd.getTargets()))\n           .addAllTargetStorageIDs(convert(cmd.getTargetStorageIDs()));\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "40eb94ade3161d93e7a762a839004748f6d0ae89": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5053. NameNode should invoke DataNode APIs to coordinate caching. (Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1523145 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/09/13 4:27 PM",
      "commitName": "40eb94ade3161d93e7a762a839004748f6d0ae89",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "06/09/13 11:52 AM",
      "commitNameOld": "f41f8b8842c3f26d19f7fa928070c7c07f760e4c",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 7.19,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,29 @@\n   public static BlockCommandProto convert(BlockCommand cmd) {\n     BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n         .setBlockPoolId(cmd.getBlockPoolId());\n     switch (cmd.getAction()) {\n     case DatanodeProtocol.DNA_TRANSFER:\n       builder.setAction(BlockCommandProto.Action.TRANSFER);\n       break;\n     case DatanodeProtocol.DNA_INVALIDATE:\n       builder.setAction(BlockCommandProto.Action.INVALIDATE);\n       break;\n     case DatanodeProtocol.DNA_SHUTDOWN:\n       builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n       break;\n+    case DatanodeProtocol.DNA_CACHE:\n+      builder.setAction(BlockCommandProto.Action.CACHE);\n+      break;\n+    case DatanodeProtocol.DNA_UNCACHE:\n+      builder.setAction(BlockCommandProto.Action.UNCACHE);\n+      break;\n     default:\n       throw new AssertionError(\"Invalid action\");\n     }\n     Block[] blocks \u003d cmd.getBlocks();\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       builder.addBlocks(PBHelper.convert(blocks[i]));\n     }\n     builder.addAllTargets(PBHelper.convert(cmd.getTargets()));\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockCommandProto convert(BlockCommand cmd) {\n    BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n        .setBlockPoolId(cmd.getBlockPoolId());\n    switch (cmd.getAction()) {\n    case DatanodeProtocol.DNA_TRANSFER:\n      builder.setAction(BlockCommandProto.Action.TRANSFER);\n      break;\n    case DatanodeProtocol.DNA_INVALIDATE:\n      builder.setAction(BlockCommandProto.Action.INVALIDATE);\n      break;\n    case DatanodeProtocol.DNA_SHUTDOWN:\n      builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n      break;\n    case DatanodeProtocol.DNA_CACHE:\n      builder.setAction(BlockCommandProto.Action.CACHE);\n      break;\n    case DatanodeProtocol.DNA_UNCACHE:\n      builder.setAction(BlockCommandProto.Action.UNCACHE);\n      break;\n    default:\n      throw new AssertionError(\"Invalid action\");\n    }\n    Block[] blocks \u003d cmd.getBlocks();\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      builder.addBlocks(PBHelper.convert(blocks[i]));\n    }\n    builder.addAllTargets(PBHelper.convert(cmd.getTargets()));\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "5cdb7e5ce7f0c3129749be8f29e2f11c0e0f2269": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4033. Miscellaneous findbugs 2 fixes. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430534 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 1:05 PM",
      "commitName": "5cdb7e5ce7f0c3129749be8f29e2f11c0e0f2269",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "04/01/13 8:49 AM",
      "commitNameOld": "251b485af5599d2cd8ba241388b5bc2713cb6645",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 4.18,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,23 @@\n   public static BlockCommandProto convert(BlockCommand cmd) {\n     BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n         .setBlockPoolId(cmd.getBlockPoolId());\n     switch (cmd.getAction()) {\n     case DatanodeProtocol.DNA_TRANSFER:\n       builder.setAction(BlockCommandProto.Action.TRANSFER);\n       break;\n     case DatanodeProtocol.DNA_INVALIDATE:\n       builder.setAction(BlockCommandProto.Action.INVALIDATE);\n       break;\n     case DatanodeProtocol.DNA_SHUTDOWN:\n       builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n       break;\n+    default:\n+      throw new AssertionError(\"Invalid action\");\n     }\n     Block[] blocks \u003d cmd.getBlocks();\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       builder.addBlocks(PBHelper.convert(blocks[i]));\n     }\n     builder.addAllTargets(PBHelper.convert(cmd.getTargets()));\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockCommandProto convert(BlockCommand cmd) {\n    BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n        .setBlockPoolId(cmd.getBlockPoolId());\n    switch (cmd.getAction()) {\n    case DatanodeProtocol.DNA_TRANSFER:\n      builder.setAction(BlockCommandProto.Action.TRANSFER);\n      break;\n    case DatanodeProtocol.DNA_INVALIDATE:\n      builder.setAction(BlockCommandProto.Action.INVALIDATE);\n      break;\n    case DatanodeProtocol.DNA_SHUTDOWN:\n      builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n      break;\n    default:\n      throw new AssertionError(\"Invalid action\");\n    }\n    Block[] blocks \u003d cmd.getBlocks();\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      builder.addBlocks(PBHelper.convert(blocks[i]));\n    }\n    builder.addAllTargets(PBHelper.convert(cmd.getTargets()));\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e": {
      "type": "Ybodychange",
      "commitMessage": "Merge trunk into HA branch.\n\nSeveral conflicts around introduction of protobuf translator for DatanodeProtocol - mostly trivial resolutions.\n\nNB: this does not successfully pass any tests since the HAStatus field needs\nto be integrated into the HeartbeatResponse Protobuf implementation.\nThat will be a separate commit for clearer history.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1214518 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/12/11 2:47 PM",
      "commitName": "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "13/12/11 11:02 AM",
      "commitNameOld": "a0fe4f476ae907c9c070af48a250739a4fb33362",
      "commitAuthorOld": "",
      "daysBetweenCommits": 1.16,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,21 @@\n   public static BlockCommandProto convert(BlockCommand cmd) {\n     BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n         .setBlockPoolId(cmd.getBlockPoolId());\n     switch (cmd.getAction()) {\n     case DatanodeProtocol.DNA_TRANSFER:\n       builder.setAction(BlockCommandProto.Action.TRANSFER);\n       break;\n     case DatanodeProtocol.DNA_INVALIDATE:\n       builder.setAction(BlockCommandProto.Action.INVALIDATE);\n       break;\n+    case DatanodeProtocol.DNA_SHUTDOWN:\n+      builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n+      break;\n     }\n     Block[] blocks \u003d cmd.getBlocks();\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       builder.addBlocks(PBHelper.convert(blocks[i]));\n     }\n     builder.addAllTargets(PBHelper.convert(cmd.getTargets()));\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockCommandProto convert(BlockCommand cmd) {\n    BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n        .setBlockPoolId(cmd.getBlockPoolId());\n    switch (cmd.getAction()) {\n    case DatanodeProtocol.DNA_TRANSFER:\n      builder.setAction(BlockCommandProto.Action.TRANSFER);\n      break;\n    case DatanodeProtocol.DNA_INVALIDATE:\n      builder.setAction(BlockCommandProto.Action.INVALIDATE);\n      break;\n    case DatanodeProtocol.DNA_SHUTDOWN:\n      builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n      break;\n    }\n    Block[] blocks \u003d cmd.getBlocks();\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      builder.addBlocks(PBHelper.convert(blocks[i]));\n    }\n    builder.addAllTargets(PBHelper.convert(cmd.getTargets()));\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2661. Enable protobuf RPC for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1214033 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 6:15 PM",
      "commitName": "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c",
      "commitAuthor": "Jitendra Nath Pandey",
      "commitDateOld": "13/12/11 3:31 PM",
      "commitNameOld": "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.11,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,21 @@\n   public static BlockCommandProto convert(BlockCommand cmd) {\n     BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n         .setBlockPoolId(cmd.getBlockPoolId());\n     switch (cmd.getAction()) {\n     case DatanodeProtocol.DNA_TRANSFER:\n       builder.setAction(BlockCommandProto.Action.TRANSFER);\n       break;\n     case DatanodeProtocol.DNA_INVALIDATE:\n       builder.setAction(BlockCommandProto.Action.INVALIDATE);\n       break;\n+    case DatanodeProtocol.DNA_SHUTDOWN:\n+      builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n+      break;\n     }\n     Block[] blocks \u003d cmd.getBlocks();\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       builder.addBlocks(PBHelper.convert(blocks[i]));\n     }\n     builder.addAllTargets(PBHelper.convert(cmd.getTargets()));\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockCommandProto convert(BlockCommand cmd) {\n    BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n        .setBlockPoolId(cmd.getBlockPoolId());\n    switch (cmd.getAction()) {\n    case DatanodeProtocol.DNA_TRANSFER:\n      builder.setAction(BlockCommandProto.Action.TRANSFER);\n      break;\n    case DatanodeProtocol.DNA_INVALIDATE:\n      builder.setAction(BlockCommandProto.Action.INVALIDATE);\n      break;\n    case DatanodeProtocol.DNA_SHUTDOWN:\n      builder.setAction(BlockCommandProto.Action.SHUTDOWN);\n      break;\n    }\n    Block[] blocks \u003d cmd.getBlocks();\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      builder.addBlocks(PBHelper.convert(blocks[i]));\n    }\n    builder.addAllTargets(PBHelper.convert(cmd.getTargets()));\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "48da033901d3471ef176a94104158546152353e9": {
      "type": "Ybodychange",
      "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/12/11 9:36 PM",
      "commitName": "48da033901d3471ef176a94104158546152353e9",
      "commitAuthor": "Sanjay Radia",
      "commitDateOld": "11/12/11 10:53 AM",
      "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.45,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,18 @@\n   public static BlockCommandProto convert(BlockCommand cmd) {\n     BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n         .setBlockPoolId(cmd.getBlockPoolId());\n     switch (cmd.getAction()) {\n     case DatanodeProtocol.DNA_TRANSFER:\n       builder.setAction(BlockCommandProto.Action.TRANSFER);\n       break;\n     case DatanodeProtocol.DNA_INVALIDATE:\n       builder.setAction(BlockCommandProto.Action.INVALIDATE);\n       break;\n     }\n     Block[] blocks \u003d cmd.getBlocks();\n     for (int i \u003d 0; i \u003c blocks.length; i++) {\n       builder.addBlocks(PBHelper.convert(blocks[i]));\n     }\n-    DatanodeInfo[][] infos \u003d cmd.getTargets();\n-    for (int i \u003d 0; i \u003c infos.length; i++) {\n-      builder.addTargets(PBHelper.convert(infos[i]));\n-    }\n+    builder.addAllTargets(PBHelper.convert(cmd.getTargets()));\n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockCommandProto convert(BlockCommand cmd) {\n    BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n        .setBlockPoolId(cmd.getBlockPoolId());\n    switch (cmd.getAction()) {\n    case DatanodeProtocol.DNA_TRANSFER:\n      builder.setAction(BlockCommandProto.Action.TRANSFER);\n      break;\n    case DatanodeProtocol.DNA_INVALIDATE:\n      builder.setAction(BlockCommandProto.Action.INVALIDATE);\n      break;\n    }\n    Block[] blocks \u003d cmd.getBlocks();\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      builder.addBlocks(PBHelper.convert(blocks[i]));\n    }\n    builder.addAllTargets(PBHelper.convert(cmd.getTargets()));\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "38a19bc293dec6221ae96e304fc6ab660d94e706": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/12/11 12:02 PM",
      "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
      "commitAuthor": "Jitendra Nath Pandey",
      "diff": "@@ -0,0 +1,21 @@\n+  public static BlockCommandProto convert(BlockCommand cmd) {\n+    BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n+        .setBlockPoolId(cmd.getBlockPoolId());\n+    switch (cmd.getAction()) {\n+    case DatanodeProtocol.DNA_TRANSFER:\n+      builder.setAction(BlockCommandProto.Action.TRANSFER);\n+      break;\n+    case DatanodeProtocol.DNA_INVALIDATE:\n+      builder.setAction(BlockCommandProto.Action.INVALIDATE);\n+      break;\n+    }\n+    Block[] blocks \u003d cmd.getBlocks();\n+    for (int i \u003d 0; i \u003c blocks.length; i++) {\n+      builder.addBlocks(PBHelper.convert(blocks[i]));\n+    }\n+    DatanodeInfo[][] infos \u003d cmd.getTargets();\n+    for (int i \u003d 0; i \u003c infos.length; i++) {\n+      builder.addTargets(PBHelper.convert(infos[i]));\n+    }\n+    return builder.build();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockCommandProto convert(BlockCommand cmd) {\n    BlockCommandProto.Builder builder \u003d BlockCommandProto.newBuilder()\n        .setBlockPoolId(cmd.getBlockPoolId());\n    switch (cmd.getAction()) {\n    case DatanodeProtocol.DNA_TRANSFER:\n      builder.setAction(BlockCommandProto.Action.TRANSFER);\n      break;\n    case DatanodeProtocol.DNA_INVALIDATE:\n      builder.setAction(BlockCommandProto.Action.INVALIDATE);\n      break;\n    }\n    Block[] blocks \u003d cmd.getBlocks();\n    for (int i \u003d 0; i \u003c blocks.length; i++) {\n      builder.addBlocks(PBHelper.convert(blocks[i]));\n    }\n    DatanodeInfo[][] infos \u003d cmd.getTargets();\n    for (int i \u003d 0; i \u003c infos.length; i++) {\n      builder.addTargets(PBHelper.convert(infos[i]));\n    }\n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java"
    }
  }
}