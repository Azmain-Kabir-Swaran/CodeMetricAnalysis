{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSAdmin.java",
  "functionName": "report",
  "functionId": "report___argv-String[]__i-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
  "functionStartLine": 500,
  "functionEndLine": 622,
  "numCommitsSeen": 153,
  "timeTaken": 11051,
  "changeHistory": [
    "9499df7b81b55b488a32fd59798a543dafef4ef8",
    "4e50dc976a92a9560630c87cfc4e4513916e5735",
    "40c2f31f8dd45bc94291535ad41ffe3cc30b5536",
    "b89ffcff362a872013f5d96c1fb76e0731402db4",
    "770cc462281518545e3d1c0f8c21cf9ec9673200",
    "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
    "8d9084eb62f4593d4dfeb618abacf6ae89019109",
    "10a2bc0dffaece216eb9a6bac3236a086b9ece31",
    "86c92227fc56b6e06d879d250728e8dc8cbe98fe",
    "8c5b23b5473e447384f818d69d907d5c35ed6d6a",
    "9445859930b8653cb0b9a0e1abf38cc05dbe2658",
    "e28edbffe15e9d176d14ea2af8d9460d807b3fc4",
    "6c0ccb5989c2053f5a1ebab0dd9fdb7b4019fda8",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "c163455df487f99171e5045cdf0c2e1be1c4f99e",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "9499df7b81b55b488a32fd59798a543dafef4ef8": "Ybodychange",
    "4e50dc976a92a9560630c87cfc4e4513916e5735": "Ybodychange",
    "40c2f31f8dd45bc94291535ad41ffe3cc30b5536": "Ybodychange",
    "b89ffcff362a872013f5d96c1fb76e0731402db4": "Ybodychange",
    "770cc462281518545e3d1c0f8c21cf9ec9673200": "Ybodychange",
    "999c8fcbefc876d9c26c23c5b87a64a81e4f113e": "Ybodychange",
    "8d9084eb62f4593d4dfeb618abacf6ae89019109": "Ybodychange",
    "10a2bc0dffaece216eb9a6bac3236a086b9ece31": "Ybodychange",
    "86c92227fc56b6e06d879d250728e8dc8cbe98fe": "Ybodychange",
    "8c5b23b5473e447384f818d69d907d5c35ed6d6a": "Ybodychange",
    "9445859930b8653cb0b9a0e1abf38cc05dbe2658": "Ymultichange(Yparameterchange,Ybodychange)",
    "e28edbffe15e9d176d14ea2af8d9460d807b3fc4": "Ybodychange",
    "6c0ccb5989c2053f5a1ebab0dd9fdb7b4019fda8": "Ybodychange",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "c163455df487f99171e5045cdf0c2e1be1c4f99e": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "9499df7b81b55b488a32fd59798a543dafef4ef8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13658. Expose HighestPriorityLowRedundancy blocks statistics. Contributed by Kitti Nanasi.\n",
      "commitDate": "08/08/18 10:40 AM",
      "commitName": "9499df7b81b55b488a32fd59798a543dafef4ef8",
      "commitAuthor": "Xiao Chen",
      "commitDateOld": "31/05/18 3:26 PM",
      "commitNameOld": "3f4a29813beccd85191886f4d7421c4f33180594",
      "commitAuthorOld": "Wei-Chiu Chuang",
      "daysBetweenCommits": 68.8,
      "commitsBetweenForRepo": 447,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,113 +1,123 @@\n   public void report(String[] argv, int i) throws IOException {\n     DistributedFileSystem dfs \u003d getDFS();\n     FsStatus ds \u003d dfs.getStatus();\n     long capacity \u003d ds.getCapacity();\n     long used \u003d ds.getUsed();\n     long remaining \u003d ds.getRemaining();\n     long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n     long presentCapacity \u003d used + remaining;\n     boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n     if (mode) {\n       System.out.println(\"Safe mode is ON\");\n       if (bytesInFuture \u003e 0) {\n         System.out.println(\"\\nWARNING: \");\n         System.out.println(\"Name node has detected blocks with generation \" +\n             \"stamps in future.\");\n         System.out.println(\"Forcing exit from safemode will cause \" +\n             bytesInFuture + \" byte(s) to be deleted.\");\n         System.out.println(\"If you are sure that the NameNode was started with\"\n             + \" the correct metadata files then you may proceed with \" +\n             \"\u0027-safemode forceExit\u0027\\n\");\n       }\n     }\n     System.out.println(\"Configured Capacity: \" + capacity\n                        + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n     System.out.println(\"Present Capacity: \" + presentCapacity\n         + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n     System.out.println(\"DFS Remaining: \" + remaining\n         + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n     System.out.println(\"DFS Used: \" + used\n                        + \" (\" + StringUtils.byteDesc(used) + \")\");\n     double dfsUsedPercent \u003d 0;\n     if (presentCapacity !\u003d 0) {\n       dfsUsedPercent \u003d used/(double)presentCapacity;\n     }\n     System.out.println(\"DFS Used%: \"\n         + StringUtils.formatPercent(dfsUsedPercent, 2));\n \n     /* These counts are not always upto date. They are updated after  \n      * iteration of an internal list. Should be updated in a few seconds to \n      * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n      * counts.\n      */\n     ReplicatedBlockStats replicatedBlockStats \u003d\n         dfs.getClient().getNamenode().getReplicatedBlockStats();\n     System.out.println(\"Replicated Blocks:\");\n     System.out.println(\"\\tUnder replicated blocks: \" +\n         replicatedBlockStats.getLowRedundancyBlocks());\n     System.out.println(\"\\tBlocks with corrupt replicas: \" +\n         replicatedBlockStats.getCorruptBlocks());\n     System.out.println(\"\\tMissing blocks: \" +\n         replicatedBlockStats.getMissingReplicaBlocks());\n     System.out.println(\"\\tMissing blocks (with replication factor 1): \" +\n         replicatedBlockStats.getMissingReplicationOneBlocks());\n+    if (replicatedBlockStats.hasHighestPriorityLowRedundancyBlocks()) {\n+      System.out.println(\"\\tLow redundancy blocks with highest priority \" +\n+          \"to recover: \" +\n+          replicatedBlockStats.getHighestPriorityLowRedundancyBlocks());\n+    }\n     System.out.println(\"\\tPending deletion blocks: \" +\n         replicatedBlockStats.getPendingDeletionBlocks());\n \n     ECBlockGroupStats ecBlockGroupStats \u003d\n         dfs.getClient().getNamenode().getECBlockGroupStats();\n     System.out.println(\"Erasure Coded Block Groups: \");\n     System.out.println(\"\\tLow redundancy block groups: \" +\n         ecBlockGroupStats.getLowRedundancyBlockGroups());\n     System.out.println(\"\\tBlock groups with corrupt internal blocks: \" +\n         ecBlockGroupStats.getCorruptBlockGroups());\n     System.out.println(\"\\tMissing block groups: \" +\n         ecBlockGroupStats.getMissingBlockGroups());\n+    if (ecBlockGroupStats.hasHighestPriorityLowRedundancyBlocks()) {\n+      System.out.println(\"\\tLow redundancy blocks with highest priority \" +\n+          \"to recover: \" +\n+          ecBlockGroupStats.getHighestPriorityLowRedundancyBlocks());\n+    }\n     System.out.println(\"\\tPending deletion blocks: \" +\n         ecBlockGroupStats.getPendingDeletionBlocks());\n \n     System.out.println();\n \n     System.out.println(\"-------------------------------------------------\");\n     \n     // Parse arguments for filtering the node list\n     List\u003cString\u003e args \u003d Arrays.asList(argv);\n     // Truncate already handled arguments before parsing report()-specific ones\n     args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n     final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n     final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n     final boolean listDecommissioning \u003d\n         StringUtils.popOption(\"-decommissioning\", args);\n     final boolean listEnteringMaintenance \u003d\n         StringUtils.popOption(\"-enteringmaintenance\", args);\n     final boolean listInMaintenance \u003d\n         StringUtils.popOption(\"-inmaintenance\", args);\n \n \n     // If no filter flags are found, then list all DN types\n     boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning\n         \u0026\u0026 !listEnteringMaintenance \u0026\u0026 !listInMaintenance);\n \n     if (listAll || listLive) {\n       printDataNodeReports(dfs, DatanodeReportType.LIVE, listLive, \"Live\");\n     }\n \n     if (listAll || listDead) {\n       printDataNodeReports(dfs, DatanodeReportType.DEAD, listDead, \"Dead\");\n     }\n \n     if (listAll || listDecommissioning) {\n       printDataNodeReports(dfs, DatanodeReportType.DECOMMISSIONING,\n           listDecommissioning, \"Decommissioning\");\n     }\n \n     if (listAll || listEnteringMaintenance) {\n       printDataNodeReports(dfs, DatanodeReportType.ENTERING_MAINTENANCE,\n           listEnteringMaintenance, \"Entering maintenance\");\n     }\n \n     if (listAll || listInMaintenance) {\n       printDataNodeReports(dfs, DatanodeReportType.IN_MAINTENANCE,\n           listInMaintenance, \"In maintenance\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void report(String[] argv, int i) throws IOException {\n    DistributedFileSystem dfs \u003d getDFS();\n    FsStatus ds \u003d dfs.getStatus();\n    long capacity \u003d ds.getCapacity();\n    long used \u003d ds.getUsed();\n    long remaining \u003d ds.getRemaining();\n    long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n    long presentCapacity \u003d used + remaining;\n    boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n    if (mode) {\n      System.out.println(\"Safe mode is ON\");\n      if (bytesInFuture \u003e 0) {\n        System.out.println(\"\\nWARNING: \");\n        System.out.println(\"Name node has detected blocks with generation \" +\n            \"stamps in future.\");\n        System.out.println(\"Forcing exit from safemode will cause \" +\n            bytesInFuture + \" byte(s) to be deleted.\");\n        System.out.println(\"If you are sure that the NameNode was started with\"\n            + \" the correct metadata files then you may proceed with \" +\n            \"\u0027-safemode forceExit\u0027\\n\");\n      }\n    }\n    System.out.println(\"Configured Capacity: \" + capacity\n                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n    System.out.println(\"Present Capacity: \" + presentCapacity\n        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n    System.out.println(\"DFS Remaining: \" + remaining\n        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n    System.out.println(\"DFS Used: \" + used\n                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n    double dfsUsedPercent \u003d 0;\n    if (presentCapacity !\u003d 0) {\n      dfsUsedPercent \u003d used/(double)presentCapacity;\n    }\n    System.out.println(\"DFS Used%: \"\n        + StringUtils.formatPercent(dfsUsedPercent, 2));\n\n    /* These counts are not always upto date. They are updated after  \n     * iteration of an internal list. Should be updated in a few seconds to \n     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n     * counts.\n     */\n    ReplicatedBlockStats replicatedBlockStats \u003d\n        dfs.getClient().getNamenode().getReplicatedBlockStats();\n    System.out.println(\"Replicated Blocks:\");\n    System.out.println(\"\\tUnder replicated blocks: \" +\n        replicatedBlockStats.getLowRedundancyBlocks());\n    System.out.println(\"\\tBlocks with corrupt replicas: \" +\n        replicatedBlockStats.getCorruptBlocks());\n    System.out.println(\"\\tMissing blocks: \" +\n        replicatedBlockStats.getMissingReplicaBlocks());\n    System.out.println(\"\\tMissing blocks (with replication factor 1): \" +\n        replicatedBlockStats.getMissingReplicationOneBlocks());\n    if (replicatedBlockStats.hasHighestPriorityLowRedundancyBlocks()) {\n      System.out.println(\"\\tLow redundancy blocks with highest priority \" +\n          \"to recover: \" +\n          replicatedBlockStats.getHighestPriorityLowRedundancyBlocks());\n    }\n    System.out.println(\"\\tPending deletion blocks: \" +\n        replicatedBlockStats.getPendingDeletionBlocks());\n\n    ECBlockGroupStats ecBlockGroupStats \u003d\n        dfs.getClient().getNamenode().getECBlockGroupStats();\n    System.out.println(\"Erasure Coded Block Groups: \");\n    System.out.println(\"\\tLow redundancy block groups: \" +\n        ecBlockGroupStats.getLowRedundancyBlockGroups());\n    System.out.println(\"\\tBlock groups with corrupt internal blocks: \" +\n        ecBlockGroupStats.getCorruptBlockGroups());\n    System.out.println(\"\\tMissing block groups: \" +\n        ecBlockGroupStats.getMissingBlockGroups());\n    if (ecBlockGroupStats.hasHighestPriorityLowRedundancyBlocks()) {\n      System.out.println(\"\\tLow redundancy blocks with highest priority \" +\n          \"to recover: \" +\n          ecBlockGroupStats.getHighestPriorityLowRedundancyBlocks());\n    }\n    System.out.println(\"\\tPending deletion blocks: \" +\n        ecBlockGroupStats.getPendingDeletionBlocks());\n\n    System.out.println();\n\n    System.out.println(\"-------------------------------------------------\");\n    \n    // Parse arguments for filtering the node list\n    List\u003cString\u003e args \u003d Arrays.asList(argv);\n    // Truncate already handled arguments before parsing report()-specific ones\n    args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n    final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n    final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n    final boolean listDecommissioning \u003d\n        StringUtils.popOption(\"-decommissioning\", args);\n    final boolean listEnteringMaintenance \u003d\n        StringUtils.popOption(\"-enteringmaintenance\", args);\n    final boolean listInMaintenance \u003d\n        StringUtils.popOption(\"-inmaintenance\", args);\n\n\n    // If no filter flags are found, then list all DN types\n    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning\n        \u0026\u0026 !listEnteringMaintenance \u0026\u0026 !listInMaintenance);\n\n    if (listAll || listLive) {\n      printDataNodeReports(dfs, DatanodeReportType.LIVE, listLive, \"Live\");\n    }\n\n    if (listAll || listDead) {\n      printDataNodeReports(dfs, DatanodeReportType.DEAD, listDead, \"Dead\");\n    }\n\n    if (listAll || listDecommissioning) {\n      printDataNodeReports(dfs, DatanodeReportType.DECOMMISSIONING,\n          listDecommissioning, \"Decommissioning\");\n    }\n\n    if (listAll || listEnteringMaintenance) {\n      printDataNodeReports(dfs, DatanodeReportType.ENTERING_MAINTENANCE,\n          listEnteringMaintenance, \"Entering maintenance\");\n    }\n\n    if (listAll || listInMaintenance) {\n      printDataNodeReports(dfs, DatanodeReportType.IN_MAINTENANCE,\n          listInMaintenance, \"In maintenance\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "4e50dc976a92a9560630c87cfc4e4513916e5735": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12218. Addendum. Rename split EC / replicated block metrics in BlockManager.\n",
      "commitDate": "07/09/17 4:57 PM",
      "commitName": "4e50dc976a92a9560630c87cfc4e4513916e5735",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "07/09/17 4:56 PM",
      "commitNameOld": "40c2f31f8dd45bc94291535ad41ffe3cc30b5536",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,112 +1,113 @@\n   public void report(String[] argv, int i) throws IOException {\n     DistributedFileSystem dfs \u003d getDFS();\n     FsStatus ds \u003d dfs.getStatus();\n     long capacity \u003d ds.getCapacity();\n     long used \u003d ds.getUsed();\n     long remaining \u003d ds.getRemaining();\n     long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n     long presentCapacity \u003d used + remaining;\n     boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n     if (mode) {\n       System.out.println(\"Safe mode is ON\");\n       if (bytesInFuture \u003e 0) {\n         System.out.println(\"\\nWARNING: \");\n         System.out.println(\"Name node has detected blocks with generation \" +\n             \"stamps in future.\");\n         System.out.println(\"Forcing exit from safemode will cause \" +\n             bytesInFuture + \" byte(s) to be deleted.\");\n         System.out.println(\"If you are sure that the NameNode was started with\"\n             + \" the correct metadata files then you may proceed with \" +\n             \"\u0027-safemode forceExit\u0027\\n\");\n       }\n     }\n     System.out.println(\"Configured Capacity: \" + capacity\n                        + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n     System.out.println(\"Present Capacity: \" + presentCapacity\n         + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n     System.out.println(\"DFS Remaining: \" + remaining\n         + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n     System.out.println(\"DFS Used: \" + used\n                        + \" (\" + StringUtils.byteDesc(used) + \")\");\n     double dfsUsedPercent \u003d 0;\n     if (presentCapacity !\u003d 0) {\n       dfsUsedPercent \u003d used/(double)presentCapacity;\n     }\n     System.out.println(\"DFS Used%: \"\n         + StringUtils.formatPercent(dfsUsedPercent, 2));\n \n     /* These counts are not always upto date. They are updated after  \n      * iteration of an internal list. Should be updated in a few seconds to \n      * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n      * counts.\n      */\n-    ReplicatedBlockStats replicatedBlockStats \u003d dfs.getClient().getNamenode().getBlocksStats();\n+    ReplicatedBlockStats replicatedBlockStats \u003d\n+        dfs.getClient().getNamenode().getReplicatedBlockStats();\n     System.out.println(\"Replicated Blocks:\");\n     System.out.println(\"\\tUnder replicated blocks: \" +\n-        replicatedBlockStats.getLowRedundancyBlocksStat());\n+        replicatedBlockStats.getLowRedundancyBlocks());\n     System.out.println(\"\\tBlocks with corrupt replicas: \" +\n-        replicatedBlockStats.getCorruptBlocksStat());\n+        replicatedBlockStats.getCorruptBlocks());\n     System.out.println(\"\\tMissing blocks: \" +\n-        replicatedBlockStats.getMissingReplicaBlocksStat());\n+        replicatedBlockStats.getMissingReplicaBlocks());\n     System.out.println(\"\\tMissing blocks (with replication factor 1): \" +\n-        replicatedBlockStats.getMissingReplicationOneBlocksStat());\n+        replicatedBlockStats.getMissingReplicationOneBlocks());\n     System.out.println(\"\\tPending deletion blocks: \" +\n-        replicatedBlockStats.getPendingDeletionBlocksStat());\n+        replicatedBlockStats.getPendingDeletionBlocks());\n \n     ECBlockGroupStats ecBlockGroupStats \u003d\n-        dfs.getClient().getNamenode().getECBlockGroupsStats();\n+        dfs.getClient().getNamenode().getECBlockGroupStats();\n     System.out.println(\"Erasure Coded Block Groups: \");\n     System.out.println(\"\\tLow redundancy block groups: \" +\n-        ecBlockGroupStats.getLowRedundancyBlockGroupsStat());\n+        ecBlockGroupStats.getLowRedundancyBlockGroups());\n     System.out.println(\"\\tBlock groups with corrupt internal blocks: \" +\n-        ecBlockGroupStats.getCorruptBlockGroupsStat());\n+        ecBlockGroupStats.getCorruptBlockGroups());\n     System.out.println(\"\\tMissing block groups: \" +\n-        ecBlockGroupStats.getMissingBlockGroupsStat());\n-    System.out.println(\"\\tPending deletion block groups: \" +\n-        ecBlockGroupStats.getPendingDeletionBlockGroupsStat());\n+        ecBlockGroupStats.getMissingBlockGroups());\n+    System.out.println(\"\\tPending deletion blocks: \" +\n+        ecBlockGroupStats.getPendingDeletionBlocks());\n \n     System.out.println();\n \n     System.out.println(\"-------------------------------------------------\");\n     \n     // Parse arguments for filtering the node list\n     List\u003cString\u003e args \u003d Arrays.asList(argv);\n     // Truncate already handled arguments before parsing report()-specific ones\n     args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n     final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n     final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n     final boolean listDecommissioning \u003d\n         StringUtils.popOption(\"-decommissioning\", args);\n     final boolean listEnteringMaintenance \u003d\n         StringUtils.popOption(\"-enteringmaintenance\", args);\n     final boolean listInMaintenance \u003d\n         StringUtils.popOption(\"-inmaintenance\", args);\n \n \n     // If no filter flags are found, then list all DN types\n     boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning\n         \u0026\u0026 !listEnteringMaintenance \u0026\u0026 !listInMaintenance);\n \n     if (listAll || listLive) {\n       printDataNodeReports(dfs, DatanodeReportType.LIVE, listLive, \"Live\");\n     }\n \n     if (listAll || listDead) {\n       printDataNodeReports(dfs, DatanodeReportType.DEAD, listDead, \"Dead\");\n     }\n \n     if (listAll || listDecommissioning) {\n       printDataNodeReports(dfs, DatanodeReportType.DECOMMISSIONING,\n           listDecommissioning, \"Decommissioning\");\n     }\n \n     if (listAll || listEnteringMaintenance) {\n       printDataNodeReports(dfs, DatanodeReportType.ENTERING_MAINTENANCE,\n           listEnteringMaintenance, \"Entering maintenance\");\n     }\n \n     if (listAll || listInMaintenance) {\n       printDataNodeReports(dfs, DatanodeReportType.IN_MAINTENANCE,\n           listInMaintenance, \"In maintenance\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void report(String[] argv, int i) throws IOException {\n    DistributedFileSystem dfs \u003d getDFS();\n    FsStatus ds \u003d dfs.getStatus();\n    long capacity \u003d ds.getCapacity();\n    long used \u003d ds.getUsed();\n    long remaining \u003d ds.getRemaining();\n    long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n    long presentCapacity \u003d used + remaining;\n    boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n    if (mode) {\n      System.out.println(\"Safe mode is ON\");\n      if (bytesInFuture \u003e 0) {\n        System.out.println(\"\\nWARNING: \");\n        System.out.println(\"Name node has detected blocks with generation \" +\n            \"stamps in future.\");\n        System.out.println(\"Forcing exit from safemode will cause \" +\n            bytesInFuture + \" byte(s) to be deleted.\");\n        System.out.println(\"If you are sure that the NameNode was started with\"\n            + \" the correct metadata files then you may proceed with \" +\n            \"\u0027-safemode forceExit\u0027\\n\");\n      }\n    }\n    System.out.println(\"Configured Capacity: \" + capacity\n                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n    System.out.println(\"Present Capacity: \" + presentCapacity\n        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n    System.out.println(\"DFS Remaining: \" + remaining\n        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n    System.out.println(\"DFS Used: \" + used\n                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n    double dfsUsedPercent \u003d 0;\n    if (presentCapacity !\u003d 0) {\n      dfsUsedPercent \u003d used/(double)presentCapacity;\n    }\n    System.out.println(\"DFS Used%: \"\n        + StringUtils.formatPercent(dfsUsedPercent, 2));\n\n    /* These counts are not always upto date. They are updated after  \n     * iteration of an internal list. Should be updated in a few seconds to \n     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n     * counts.\n     */\n    ReplicatedBlockStats replicatedBlockStats \u003d\n        dfs.getClient().getNamenode().getReplicatedBlockStats();\n    System.out.println(\"Replicated Blocks:\");\n    System.out.println(\"\\tUnder replicated blocks: \" +\n        replicatedBlockStats.getLowRedundancyBlocks());\n    System.out.println(\"\\tBlocks with corrupt replicas: \" +\n        replicatedBlockStats.getCorruptBlocks());\n    System.out.println(\"\\tMissing blocks: \" +\n        replicatedBlockStats.getMissingReplicaBlocks());\n    System.out.println(\"\\tMissing blocks (with replication factor 1): \" +\n        replicatedBlockStats.getMissingReplicationOneBlocks());\n    System.out.println(\"\\tPending deletion blocks: \" +\n        replicatedBlockStats.getPendingDeletionBlocks());\n\n    ECBlockGroupStats ecBlockGroupStats \u003d\n        dfs.getClient().getNamenode().getECBlockGroupStats();\n    System.out.println(\"Erasure Coded Block Groups: \");\n    System.out.println(\"\\tLow redundancy block groups: \" +\n        ecBlockGroupStats.getLowRedundancyBlockGroups());\n    System.out.println(\"\\tBlock groups with corrupt internal blocks: \" +\n        ecBlockGroupStats.getCorruptBlockGroups());\n    System.out.println(\"\\tMissing block groups: \" +\n        ecBlockGroupStats.getMissingBlockGroups());\n    System.out.println(\"\\tPending deletion blocks: \" +\n        ecBlockGroupStats.getPendingDeletionBlocks());\n\n    System.out.println();\n\n    System.out.println(\"-------------------------------------------------\");\n    \n    // Parse arguments for filtering the node list\n    List\u003cString\u003e args \u003d Arrays.asList(argv);\n    // Truncate already handled arguments before parsing report()-specific ones\n    args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n    final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n    final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n    final boolean listDecommissioning \u003d\n        StringUtils.popOption(\"-decommissioning\", args);\n    final boolean listEnteringMaintenance \u003d\n        StringUtils.popOption(\"-enteringmaintenance\", args);\n    final boolean listInMaintenance \u003d\n        StringUtils.popOption(\"-inmaintenance\", args);\n\n\n    // If no filter flags are found, then list all DN types\n    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning\n        \u0026\u0026 !listEnteringMaintenance \u0026\u0026 !listInMaintenance);\n\n    if (listAll || listLive) {\n      printDataNodeReports(dfs, DatanodeReportType.LIVE, listLive, \"Live\");\n    }\n\n    if (listAll || listDead) {\n      printDataNodeReports(dfs, DatanodeReportType.DEAD, listDead, \"Dead\");\n    }\n\n    if (listAll || listDecommissioning) {\n      printDataNodeReports(dfs, DatanodeReportType.DECOMMISSIONING,\n          listDecommissioning, \"Decommissioning\");\n    }\n\n    if (listAll || listEnteringMaintenance) {\n      printDataNodeReports(dfs, DatanodeReportType.ENTERING_MAINTENANCE,\n          listEnteringMaintenance, \"Entering maintenance\");\n    }\n\n    if (listAll || listInMaintenance) {\n      printDataNodeReports(dfs, DatanodeReportType.IN_MAINTENANCE,\n          listInMaintenance, \"In maintenance\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "40c2f31f8dd45bc94291535ad41ffe3cc30b5536": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12218. Rename split EC / replicated block metrics in BlockManager.\n",
      "commitDate": "07/09/17 4:56 PM",
      "commitName": "40c2f31f8dd45bc94291535ad41ffe3cc30b5536",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "25/08/17 5:21 PM",
      "commitNameOld": "b89ffcff362a872013f5d96c1fb76e0731402db4",
      "commitAuthorOld": "Manoj Govindassamy",
      "daysBetweenCommits": 12.98,
      "commitsBetweenForRepo": 142,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,112 +1,112 @@\n   public void report(String[] argv, int i) throws IOException {\n     DistributedFileSystem dfs \u003d getDFS();\n     FsStatus ds \u003d dfs.getStatus();\n     long capacity \u003d ds.getCapacity();\n     long used \u003d ds.getUsed();\n     long remaining \u003d ds.getRemaining();\n     long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n     long presentCapacity \u003d used + remaining;\n     boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n     if (mode) {\n       System.out.println(\"Safe mode is ON\");\n       if (bytesInFuture \u003e 0) {\n         System.out.println(\"\\nWARNING: \");\n         System.out.println(\"Name node has detected blocks with generation \" +\n             \"stamps in future.\");\n         System.out.println(\"Forcing exit from safemode will cause \" +\n             bytesInFuture + \" byte(s) to be deleted.\");\n         System.out.println(\"If you are sure that the NameNode was started with\"\n             + \" the correct metadata files then you may proceed with \" +\n             \"\u0027-safemode forceExit\u0027\\n\");\n       }\n     }\n     System.out.println(\"Configured Capacity: \" + capacity\n                        + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n     System.out.println(\"Present Capacity: \" + presentCapacity\n         + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n     System.out.println(\"DFS Remaining: \" + remaining\n         + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n     System.out.println(\"DFS Used: \" + used\n                        + \" (\" + StringUtils.byteDesc(used) + \")\");\n     double dfsUsedPercent \u003d 0;\n     if (presentCapacity !\u003d 0) {\n       dfsUsedPercent \u003d used/(double)presentCapacity;\n     }\n     System.out.println(\"DFS Used%: \"\n         + StringUtils.formatPercent(dfsUsedPercent, 2));\n \n     /* These counts are not always upto date. They are updated after  \n      * iteration of an internal list. Should be updated in a few seconds to \n      * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n      * counts.\n      */\n-    BlocksStats blocksStats \u003d dfs.getClient().getNamenode().getBlocksStats();\n+    ReplicatedBlockStats replicatedBlockStats \u003d dfs.getClient().getNamenode().getBlocksStats();\n     System.out.println(\"Replicated Blocks:\");\n     System.out.println(\"\\tUnder replicated blocks: \" +\n-        blocksStats.getLowRedundancyBlocksStat());\n+        replicatedBlockStats.getLowRedundancyBlocksStat());\n     System.out.println(\"\\tBlocks with corrupt replicas: \" +\n-        blocksStats.getCorruptBlocksStat());\n+        replicatedBlockStats.getCorruptBlocksStat());\n     System.out.println(\"\\tMissing blocks: \" +\n-        blocksStats.getMissingReplicaBlocksStat());\n+        replicatedBlockStats.getMissingReplicaBlocksStat());\n     System.out.println(\"\\tMissing blocks (with replication factor 1): \" +\n-        blocksStats.getMissingReplicationOneBlocksStat());\n+        replicatedBlockStats.getMissingReplicationOneBlocksStat());\n     System.out.println(\"\\tPending deletion blocks: \" +\n-        blocksStats.getPendingDeletionBlocksStat());\n+        replicatedBlockStats.getPendingDeletionBlocksStat());\n \n-    ECBlockGroupsStats ecBlockGroupsStats \u003d\n+    ECBlockGroupStats ecBlockGroupStats \u003d\n         dfs.getClient().getNamenode().getECBlockGroupsStats();\n     System.out.println(\"Erasure Coded Block Groups: \");\n     System.out.println(\"\\tLow redundancy block groups: \" +\n-        ecBlockGroupsStats.getLowRedundancyBlockGroupsStat());\n+        ecBlockGroupStats.getLowRedundancyBlockGroupsStat());\n     System.out.println(\"\\tBlock groups with corrupt internal blocks: \" +\n-        ecBlockGroupsStats.getCorruptBlockGroupsStat());\n+        ecBlockGroupStats.getCorruptBlockGroupsStat());\n     System.out.println(\"\\tMissing block groups: \" +\n-        ecBlockGroupsStats.getMissingBlockGroupsStat());\n+        ecBlockGroupStats.getMissingBlockGroupsStat());\n     System.out.println(\"\\tPending deletion block groups: \" +\n-        ecBlockGroupsStats.getPendingDeletionBlockGroupsStat());\n+        ecBlockGroupStats.getPendingDeletionBlockGroupsStat());\n \n     System.out.println();\n \n     System.out.println(\"-------------------------------------------------\");\n     \n     // Parse arguments for filtering the node list\n     List\u003cString\u003e args \u003d Arrays.asList(argv);\n     // Truncate already handled arguments before parsing report()-specific ones\n     args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n     final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n     final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n     final boolean listDecommissioning \u003d\n         StringUtils.popOption(\"-decommissioning\", args);\n     final boolean listEnteringMaintenance \u003d\n         StringUtils.popOption(\"-enteringmaintenance\", args);\n     final boolean listInMaintenance \u003d\n         StringUtils.popOption(\"-inmaintenance\", args);\n \n \n     // If no filter flags are found, then list all DN types\n     boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning\n         \u0026\u0026 !listEnteringMaintenance \u0026\u0026 !listInMaintenance);\n \n     if (listAll || listLive) {\n       printDataNodeReports(dfs, DatanodeReportType.LIVE, listLive, \"Live\");\n     }\n \n     if (listAll || listDead) {\n       printDataNodeReports(dfs, DatanodeReportType.DEAD, listDead, \"Dead\");\n     }\n \n     if (listAll || listDecommissioning) {\n       printDataNodeReports(dfs, DatanodeReportType.DECOMMISSIONING,\n           listDecommissioning, \"Decommissioning\");\n     }\n \n     if (listAll || listEnteringMaintenance) {\n       printDataNodeReports(dfs, DatanodeReportType.ENTERING_MAINTENANCE,\n           listEnteringMaintenance, \"Entering maintenance\");\n     }\n \n     if (listAll || listInMaintenance) {\n       printDataNodeReports(dfs, DatanodeReportType.IN_MAINTENANCE,\n           listInMaintenance, \"In maintenance\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void report(String[] argv, int i) throws IOException {\n    DistributedFileSystem dfs \u003d getDFS();\n    FsStatus ds \u003d dfs.getStatus();\n    long capacity \u003d ds.getCapacity();\n    long used \u003d ds.getUsed();\n    long remaining \u003d ds.getRemaining();\n    long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n    long presentCapacity \u003d used + remaining;\n    boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n    if (mode) {\n      System.out.println(\"Safe mode is ON\");\n      if (bytesInFuture \u003e 0) {\n        System.out.println(\"\\nWARNING: \");\n        System.out.println(\"Name node has detected blocks with generation \" +\n            \"stamps in future.\");\n        System.out.println(\"Forcing exit from safemode will cause \" +\n            bytesInFuture + \" byte(s) to be deleted.\");\n        System.out.println(\"If you are sure that the NameNode was started with\"\n            + \" the correct metadata files then you may proceed with \" +\n            \"\u0027-safemode forceExit\u0027\\n\");\n      }\n    }\n    System.out.println(\"Configured Capacity: \" + capacity\n                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n    System.out.println(\"Present Capacity: \" + presentCapacity\n        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n    System.out.println(\"DFS Remaining: \" + remaining\n        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n    System.out.println(\"DFS Used: \" + used\n                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n    double dfsUsedPercent \u003d 0;\n    if (presentCapacity !\u003d 0) {\n      dfsUsedPercent \u003d used/(double)presentCapacity;\n    }\n    System.out.println(\"DFS Used%: \"\n        + StringUtils.formatPercent(dfsUsedPercent, 2));\n\n    /* These counts are not always upto date. They are updated after  \n     * iteration of an internal list. Should be updated in a few seconds to \n     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n     * counts.\n     */\n    ReplicatedBlockStats replicatedBlockStats \u003d dfs.getClient().getNamenode().getBlocksStats();\n    System.out.println(\"Replicated Blocks:\");\n    System.out.println(\"\\tUnder replicated blocks: \" +\n        replicatedBlockStats.getLowRedundancyBlocksStat());\n    System.out.println(\"\\tBlocks with corrupt replicas: \" +\n        replicatedBlockStats.getCorruptBlocksStat());\n    System.out.println(\"\\tMissing blocks: \" +\n        replicatedBlockStats.getMissingReplicaBlocksStat());\n    System.out.println(\"\\tMissing blocks (with replication factor 1): \" +\n        replicatedBlockStats.getMissingReplicationOneBlocksStat());\n    System.out.println(\"\\tPending deletion blocks: \" +\n        replicatedBlockStats.getPendingDeletionBlocksStat());\n\n    ECBlockGroupStats ecBlockGroupStats \u003d\n        dfs.getClient().getNamenode().getECBlockGroupsStats();\n    System.out.println(\"Erasure Coded Block Groups: \");\n    System.out.println(\"\\tLow redundancy block groups: \" +\n        ecBlockGroupStats.getLowRedundancyBlockGroupsStat());\n    System.out.println(\"\\tBlock groups with corrupt internal blocks: \" +\n        ecBlockGroupStats.getCorruptBlockGroupsStat());\n    System.out.println(\"\\tMissing block groups: \" +\n        ecBlockGroupStats.getMissingBlockGroupsStat());\n    System.out.println(\"\\tPending deletion block groups: \" +\n        ecBlockGroupStats.getPendingDeletionBlockGroupsStat());\n\n    System.out.println();\n\n    System.out.println(\"-------------------------------------------------\");\n    \n    // Parse arguments for filtering the node list\n    List\u003cString\u003e args \u003d Arrays.asList(argv);\n    // Truncate already handled arguments before parsing report()-specific ones\n    args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n    final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n    final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n    final boolean listDecommissioning \u003d\n        StringUtils.popOption(\"-decommissioning\", args);\n    final boolean listEnteringMaintenance \u003d\n        StringUtils.popOption(\"-enteringmaintenance\", args);\n    final boolean listInMaintenance \u003d\n        StringUtils.popOption(\"-inmaintenance\", args);\n\n\n    // If no filter flags are found, then list all DN types\n    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning\n        \u0026\u0026 !listEnteringMaintenance \u0026\u0026 !listInMaintenance);\n\n    if (listAll || listLive) {\n      printDataNodeReports(dfs, DatanodeReportType.LIVE, listLive, \"Live\");\n    }\n\n    if (listAll || listDead) {\n      printDataNodeReports(dfs, DatanodeReportType.DEAD, listDead, \"Dead\");\n    }\n\n    if (listAll || listDecommissioning) {\n      printDataNodeReports(dfs, DatanodeReportType.DECOMMISSIONING,\n          listDecommissioning, \"Decommissioning\");\n    }\n\n    if (listAll || listEnteringMaintenance) {\n      printDataNodeReports(dfs, DatanodeReportType.ENTERING_MAINTENANCE,\n          listEnteringMaintenance, \"Entering maintenance\");\n    }\n\n    if (listAll || listInMaintenance) {\n      printDataNodeReports(dfs, DatanodeReportType.IN_MAINTENANCE,\n          listInMaintenance, \"In maintenance\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "b89ffcff362a872013f5d96c1fb76e0731402db4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11986. Dfsadmin should report erasure coding related information separately.\n",
      "commitDate": "25/08/17 5:21 PM",
      "commitName": "b89ffcff362a872013f5d96c1fb76e0731402db4",
      "commitAuthor": "Manoj Govindassamy",
      "commitDateOld": "15/08/17 1:48 AM",
      "commitNameOld": "2e43c28e01fe006210e71aab179527669f6412ed",
      "commitAuthorOld": "Yiqun Lin",
      "daysBetweenCommits": 10.65,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,98 +1,112 @@\n   public void report(String[] argv, int i) throws IOException {\n     DistributedFileSystem dfs \u003d getDFS();\n     FsStatus ds \u003d dfs.getStatus();\n     long capacity \u003d ds.getCapacity();\n     long used \u003d ds.getUsed();\n     long remaining \u003d ds.getRemaining();\n     long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n     long presentCapacity \u003d used + remaining;\n     boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n     if (mode) {\n       System.out.println(\"Safe mode is ON\");\n       if (bytesInFuture \u003e 0) {\n         System.out.println(\"\\nWARNING: \");\n         System.out.println(\"Name node has detected blocks with generation \" +\n             \"stamps in future.\");\n         System.out.println(\"Forcing exit from safemode will cause \" +\n             bytesInFuture + \" byte(s) to be deleted.\");\n         System.out.println(\"If you are sure that the NameNode was started with\"\n             + \" the correct metadata files then you may proceed with \" +\n             \"\u0027-safemode forceExit\u0027\\n\");\n       }\n     }\n     System.out.println(\"Configured Capacity: \" + capacity\n                        + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n     System.out.println(\"Present Capacity: \" + presentCapacity\n         + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n     System.out.println(\"DFS Remaining: \" + remaining\n         + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n     System.out.println(\"DFS Used: \" + used\n                        + \" (\" + StringUtils.byteDesc(used) + \")\");\n     double dfsUsedPercent \u003d 0;\n     if (presentCapacity !\u003d 0) {\n       dfsUsedPercent \u003d used/(double)presentCapacity;\n     }\n     System.out.println(\"DFS Used%: \"\n         + StringUtils.formatPercent(dfsUsedPercent, 2));\n \n     /* These counts are not always upto date. They are updated after  \n      * iteration of an internal list. Should be updated in a few seconds to \n      * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n      * counts.\n      */\n-    System.out.println(\"Under replicated blocks: \" + \n-                       dfs.getLowRedundancyBlocksCount());\n-    System.out.println(\"Blocks with corrupt replicas: \" + \n-                       dfs.getCorruptBlocksCount());\n-    System.out.println(\"Missing blocks: \" + \n-                       dfs.getMissingBlocksCount());\n-    System.out.println(\"Missing blocks (with replication factor 1): \" +\n-                      dfs.getMissingReplOneBlocksCount());\n-    System.out.println(\"Pending deletion blocks: \" +\n-        dfs.getPendingDeletionBlocksCount());\n+    BlocksStats blocksStats \u003d dfs.getClient().getNamenode().getBlocksStats();\n+    System.out.println(\"Replicated Blocks:\");\n+    System.out.println(\"\\tUnder replicated blocks: \" +\n+        blocksStats.getLowRedundancyBlocksStat());\n+    System.out.println(\"\\tBlocks with corrupt replicas: \" +\n+        blocksStats.getCorruptBlocksStat());\n+    System.out.println(\"\\tMissing blocks: \" +\n+        blocksStats.getMissingReplicaBlocksStat());\n+    System.out.println(\"\\tMissing blocks (with replication factor 1): \" +\n+        blocksStats.getMissingReplicationOneBlocksStat());\n+    System.out.println(\"\\tPending deletion blocks: \" +\n+        blocksStats.getPendingDeletionBlocksStat());\n+\n+    ECBlockGroupsStats ecBlockGroupsStats \u003d\n+        dfs.getClient().getNamenode().getECBlockGroupsStats();\n+    System.out.println(\"Erasure Coded Block Groups: \");\n+    System.out.println(\"\\tLow redundancy block groups: \" +\n+        ecBlockGroupsStats.getLowRedundancyBlockGroupsStat());\n+    System.out.println(\"\\tBlock groups with corrupt internal blocks: \" +\n+        ecBlockGroupsStats.getCorruptBlockGroupsStat());\n+    System.out.println(\"\\tMissing block groups: \" +\n+        ecBlockGroupsStats.getMissingBlockGroupsStat());\n+    System.out.println(\"\\tPending deletion block groups: \" +\n+        ecBlockGroupsStats.getPendingDeletionBlockGroupsStat());\n \n     System.out.println();\n \n     System.out.println(\"-------------------------------------------------\");\n     \n     // Parse arguments for filtering the node list\n     List\u003cString\u003e args \u003d Arrays.asList(argv);\n     // Truncate already handled arguments before parsing report()-specific ones\n     args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n     final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n     final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n     final boolean listDecommissioning \u003d\n         StringUtils.popOption(\"-decommissioning\", args);\n     final boolean listEnteringMaintenance \u003d\n         StringUtils.popOption(\"-enteringmaintenance\", args);\n     final boolean listInMaintenance \u003d\n         StringUtils.popOption(\"-inmaintenance\", args);\n \n \n     // If no filter flags are found, then list all DN types\n     boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning\n         \u0026\u0026 !listEnteringMaintenance \u0026\u0026 !listInMaintenance);\n \n     if (listAll || listLive) {\n       printDataNodeReports(dfs, DatanodeReportType.LIVE, listLive, \"Live\");\n     }\n \n     if (listAll || listDead) {\n       printDataNodeReports(dfs, DatanodeReportType.DEAD, listDead, \"Dead\");\n     }\n \n     if (listAll || listDecommissioning) {\n       printDataNodeReports(dfs, DatanodeReportType.DECOMMISSIONING,\n           listDecommissioning, \"Decommissioning\");\n     }\n \n     if (listAll || listEnteringMaintenance) {\n       printDataNodeReports(dfs, DatanodeReportType.ENTERING_MAINTENANCE,\n           listEnteringMaintenance, \"Entering maintenance\");\n     }\n \n     if (listAll || listInMaintenance) {\n       printDataNodeReports(dfs, DatanodeReportType.IN_MAINTENANCE,\n           listInMaintenance, \"In maintenance\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void report(String[] argv, int i) throws IOException {\n    DistributedFileSystem dfs \u003d getDFS();\n    FsStatus ds \u003d dfs.getStatus();\n    long capacity \u003d ds.getCapacity();\n    long used \u003d ds.getUsed();\n    long remaining \u003d ds.getRemaining();\n    long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n    long presentCapacity \u003d used + remaining;\n    boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n    if (mode) {\n      System.out.println(\"Safe mode is ON\");\n      if (bytesInFuture \u003e 0) {\n        System.out.println(\"\\nWARNING: \");\n        System.out.println(\"Name node has detected blocks with generation \" +\n            \"stamps in future.\");\n        System.out.println(\"Forcing exit from safemode will cause \" +\n            bytesInFuture + \" byte(s) to be deleted.\");\n        System.out.println(\"If you are sure that the NameNode was started with\"\n            + \" the correct metadata files then you may proceed with \" +\n            \"\u0027-safemode forceExit\u0027\\n\");\n      }\n    }\n    System.out.println(\"Configured Capacity: \" + capacity\n                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n    System.out.println(\"Present Capacity: \" + presentCapacity\n        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n    System.out.println(\"DFS Remaining: \" + remaining\n        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n    System.out.println(\"DFS Used: \" + used\n                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n    double dfsUsedPercent \u003d 0;\n    if (presentCapacity !\u003d 0) {\n      dfsUsedPercent \u003d used/(double)presentCapacity;\n    }\n    System.out.println(\"DFS Used%: \"\n        + StringUtils.formatPercent(dfsUsedPercent, 2));\n\n    /* These counts are not always upto date. They are updated after  \n     * iteration of an internal list. Should be updated in a few seconds to \n     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n     * counts.\n     */\n    BlocksStats blocksStats \u003d dfs.getClient().getNamenode().getBlocksStats();\n    System.out.println(\"Replicated Blocks:\");\n    System.out.println(\"\\tUnder replicated blocks: \" +\n        blocksStats.getLowRedundancyBlocksStat());\n    System.out.println(\"\\tBlocks with corrupt replicas: \" +\n        blocksStats.getCorruptBlocksStat());\n    System.out.println(\"\\tMissing blocks: \" +\n        blocksStats.getMissingReplicaBlocksStat());\n    System.out.println(\"\\tMissing blocks (with replication factor 1): \" +\n        blocksStats.getMissingReplicationOneBlocksStat());\n    System.out.println(\"\\tPending deletion blocks: \" +\n        blocksStats.getPendingDeletionBlocksStat());\n\n    ECBlockGroupsStats ecBlockGroupsStats \u003d\n        dfs.getClient().getNamenode().getECBlockGroupsStats();\n    System.out.println(\"Erasure Coded Block Groups: \");\n    System.out.println(\"\\tLow redundancy block groups: \" +\n        ecBlockGroupsStats.getLowRedundancyBlockGroupsStat());\n    System.out.println(\"\\tBlock groups with corrupt internal blocks: \" +\n        ecBlockGroupsStats.getCorruptBlockGroupsStat());\n    System.out.println(\"\\tMissing block groups: \" +\n        ecBlockGroupsStats.getMissingBlockGroupsStat());\n    System.out.println(\"\\tPending deletion block groups: \" +\n        ecBlockGroupsStats.getPendingDeletionBlockGroupsStat());\n\n    System.out.println();\n\n    System.out.println(\"-------------------------------------------------\");\n    \n    // Parse arguments for filtering the node list\n    List\u003cString\u003e args \u003d Arrays.asList(argv);\n    // Truncate already handled arguments before parsing report()-specific ones\n    args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n    final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n    final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n    final boolean listDecommissioning \u003d\n        StringUtils.popOption(\"-decommissioning\", args);\n    final boolean listEnteringMaintenance \u003d\n        StringUtils.popOption(\"-enteringmaintenance\", args);\n    final boolean listInMaintenance \u003d\n        StringUtils.popOption(\"-inmaintenance\", args);\n\n\n    // If no filter flags are found, then list all DN types\n    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning\n        \u0026\u0026 !listEnteringMaintenance \u0026\u0026 !listInMaintenance);\n\n    if (listAll || listLive) {\n      printDataNodeReports(dfs, DatanodeReportType.LIVE, listLive, \"Live\");\n    }\n\n    if (listAll || listDead) {\n      printDataNodeReports(dfs, DatanodeReportType.DEAD, listDead, \"Dead\");\n    }\n\n    if (listAll || listDecommissioning) {\n      printDataNodeReports(dfs, DatanodeReportType.DECOMMISSIONING,\n          listDecommissioning, \"Decommissioning\");\n    }\n\n    if (listAll || listEnteringMaintenance) {\n      printDataNodeReports(dfs, DatanodeReportType.ENTERING_MAINTENANCE,\n          listEnteringMaintenance, \"Entering maintenance\");\n    }\n\n    if (listAll || listInMaintenance) {\n      printDataNodeReports(dfs, DatanodeReportType.IN_MAINTENANCE,\n          listInMaintenance, \"In maintenance\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "770cc462281518545e3d1c0f8c21cf9ec9673200": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12176. dfsadmin shows DFS Used%: NaN% if the cluster has zero block. Contributed by Weiwei Yang.\n",
      "commitDate": "24/07/17 12:24 AM",
      "commitName": "770cc462281518545e3d1c0f8c21cf9ec9673200",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "19/07/17 8:21 AM",
      "commitNameOld": "f8cd55fe33665faf2d1b14df231516fc891118fc",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 4.67,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,94 +1,98 @@\n   public void report(String[] argv, int i) throws IOException {\n     DistributedFileSystem dfs \u003d getDFS();\n     FsStatus ds \u003d dfs.getStatus();\n     long capacity \u003d ds.getCapacity();\n     long used \u003d ds.getUsed();\n     long remaining \u003d ds.getRemaining();\n     long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n     long presentCapacity \u003d used + remaining;\n     boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n     if (mode) {\n       System.out.println(\"Safe mode is ON\");\n       if (bytesInFuture \u003e 0) {\n         System.out.println(\"\\nWARNING: \");\n         System.out.println(\"Name node has detected blocks with generation \" +\n             \"stamps in future.\");\n         System.out.println(\"Forcing exit from safemode will cause \" +\n             bytesInFuture + \" byte(s) to be deleted.\");\n         System.out.println(\"If you are sure that the NameNode was started with\"\n             + \" the correct metadata files then you may proceed with \" +\n             \"\u0027-safemode forceExit\u0027\\n\");\n       }\n     }\n     System.out.println(\"Configured Capacity: \" + capacity\n                        + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n     System.out.println(\"Present Capacity: \" + presentCapacity\n         + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n     System.out.println(\"DFS Remaining: \" + remaining\n         + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n     System.out.println(\"DFS Used: \" + used\n                        + \" (\" + StringUtils.byteDesc(used) + \")\");\n+    double dfsUsedPercent \u003d 0;\n+    if (presentCapacity !\u003d 0) {\n+      dfsUsedPercent \u003d used/(double)presentCapacity;\n+    }\n     System.out.println(\"DFS Used%: \"\n-        + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n-    \n+        + StringUtils.formatPercent(dfsUsedPercent, 2));\n+\n     /* These counts are not always upto date. They are updated after  \n      * iteration of an internal list. Should be updated in a few seconds to \n      * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n      * counts.\n      */\n     System.out.println(\"Under replicated blocks: \" + \n                        dfs.getLowRedundancyBlocksCount());\n     System.out.println(\"Blocks with corrupt replicas: \" + \n                        dfs.getCorruptBlocksCount());\n     System.out.println(\"Missing blocks: \" + \n                        dfs.getMissingBlocksCount());\n     System.out.println(\"Missing blocks (with replication factor 1): \" +\n                       dfs.getMissingReplOneBlocksCount());\n     System.out.println(\"Pending deletion blocks: \" +\n         dfs.getPendingDeletionBlocksCount());\n \n     System.out.println();\n \n     System.out.println(\"-------------------------------------------------\");\n     \n     // Parse arguments for filtering the node list\n     List\u003cString\u003e args \u003d Arrays.asList(argv);\n     // Truncate already handled arguments before parsing report()-specific ones\n     args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n     final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n     final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n     final boolean listDecommissioning \u003d\n         StringUtils.popOption(\"-decommissioning\", args);\n     final boolean listEnteringMaintenance \u003d\n         StringUtils.popOption(\"-enteringmaintenance\", args);\n     final boolean listInMaintenance \u003d\n         StringUtils.popOption(\"-inmaintenance\", args);\n \n \n     // If no filter flags are found, then list all DN types\n     boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning\n         \u0026\u0026 !listEnteringMaintenance \u0026\u0026 !listInMaintenance);\n \n     if (listAll || listLive) {\n       printDataNodeReports(dfs, DatanodeReportType.LIVE, listLive, \"Live\");\n     }\n \n     if (listAll || listDead) {\n       printDataNodeReports(dfs, DatanodeReportType.DEAD, listDead, \"Dead\");\n     }\n \n     if (listAll || listDecommissioning) {\n       printDataNodeReports(dfs, DatanodeReportType.DECOMMISSIONING,\n           listDecommissioning, \"Decommissioning\");\n     }\n \n     if (listAll || listEnteringMaintenance) {\n       printDataNodeReports(dfs, DatanodeReportType.ENTERING_MAINTENANCE,\n           listEnteringMaintenance, \"Entering maintenance\");\n     }\n \n     if (listAll || listInMaintenance) {\n       printDataNodeReports(dfs, DatanodeReportType.IN_MAINTENANCE,\n           listInMaintenance, \"In maintenance\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void report(String[] argv, int i) throws IOException {\n    DistributedFileSystem dfs \u003d getDFS();\n    FsStatus ds \u003d dfs.getStatus();\n    long capacity \u003d ds.getCapacity();\n    long used \u003d ds.getUsed();\n    long remaining \u003d ds.getRemaining();\n    long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n    long presentCapacity \u003d used + remaining;\n    boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n    if (mode) {\n      System.out.println(\"Safe mode is ON\");\n      if (bytesInFuture \u003e 0) {\n        System.out.println(\"\\nWARNING: \");\n        System.out.println(\"Name node has detected blocks with generation \" +\n            \"stamps in future.\");\n        System.out.println(\"Forcing exit from safemode will cause \" +\n            bytesInFuture + \" byte(s) to be deleted.\");\n        System.out.println(\"If you are sure that the NameNode was started with\"\n            + \" the correct metadata files then you may proceed with \" +\n            \"\u0027-safemode forceExit\u0027\\n\");\n      }\n    }\n    System.out.println(\"Configured Capacity: \" + capacity\n                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n    System.out.println(\"Present Capacity: \" + presentCapacity\n        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n    System.out.println(\"DFS Remaining: \" + remaining\n        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n    System.out.println(\"DFS Used: \" + used\n                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n    double dfsUsedPercent \u003d 0;\n    if (presentCapacity !\u003d 0) {\n      dfsUsedPercent \u003d used/(double)presentCapacity;\n    }\n    System.out.println(\"DFS Used%: \"\n        + StringUtils.formatPercent(dfsUsedPercent, 2));\n\n    /* These counts are not always upto date. They are updated after  \n     * iteration of an internal list. Should be updated in a few seconds to \n     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n     * counts.\n     */\n    System.out.println(\"Under replicated blocks: \" + \n                       dfs.getLowRedundancyBlocksCount());\n    System.out.println(\"Blocks with corrupt replicas: \" + \n                       dfs.getCorruptBlocksCount());\n    System.out.println(\"Missing blocks: \" + \n                       dfs.getMissingBlocksCount());\n    System.out.println(\"Missing blocks (with replication factor 1): \" +\n                      dfs.getMissingReplOneBlocksCount());\n    System.out.println(\"Pending deletion blocks: \" +\n        dfs.getPendingDeletionBlocksCount());\n\n    System.out.println();\n\n    System.out.println(\"-------------------------------------------------\");\n    \n    // Parse arguments for filtering the node list\n    List\u003cString\u003e args \u003d Arrays.asList(argv);\n    // Truncate already handled arguments before parsing report()-specific ones\n    args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n    final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n    final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n    final boolean listDecommissioning \u003d\n        StringUtils.popOption(\"-decommissioning\", args);\n    final boolean listEnteringMaintenance \u003d\n        StringUtils.popOption(\"-enteringmaintenance\", args);\n    final boolean listInMaintenance \u003d\n        StringUtils.popOption(\"-inmaintenance\", args);\n\n\n    // If no filter flags are found, then list all DN types\n    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning\n        \u0026\u0026 !listEnteringMaintenance \u0026\u0026 !listInMaintenance);\n\n    if (listAll || listLive) {\n      printDataNodeReports(dfs, DatanodeReportType.LIVE, listLive, \"Live\");\n    }\n\n    if (listAll || listDead) {\n      printDataNodeReports(dfs, DatanodeReportType.DEAD, listDead, \"Dead\");\n    }\n\n    if (listAll || listDecommissioning) {\n      printDataNodeReports(dfs, DatanodeReportType.DECOMMISSIONING,\n          listDecommissioning, \"Decommissioning\");\n    }\n\n    if (listAll || listEnteringMaintenance) {\n      printDataNodeReports(dfs, DatanodeReportType.ENTERING_MAINTENANCE,\n          listEnteringMaintenance, \"Entering maintenance\");\n    }\n\n    if (listAll || listInMaintenance) {\n      printDataNodeReports(dfs, DatanodeReportType.IN_MAINTENANCE,\n          listInMaintenance, \"In maintenance\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "999c8fcbefc876d9c26c23c5b87a64a81e4f113e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10999. Introduce separate stats for Replicated and Erasure Coded Blocks apart from the current Aggregated stats. (Manoj Govindassamy via lei)\n",
      "commitDate": "14/06/17 10:44 AM",
      "commitName": "999c8fcbefc876d9c26c23c5b87a64a81e4f113e",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "01/06/17 9:48 PM",
      "commitNameOld": "8d9084eb62f4593d4dfeb618abacf6ae89019109",
      "commitAuthorOld": "Yiqun Lin",
      "daysBetweenCommits": 12.54,
      "commitsBetweenForRepo": 48,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,94 +1,94 @@\n   public void report(String[] argv, int i) throws IOException {\n     DistributedFileSystem dfs \u003d getDFS();\n     FsStatus ds \u003d dfs.getStatus();\n     long capacity \u003d ds.getCapacity();\n     long used \u003d ds.getUsed();\n     long remaining \u003d ds.getRemaining();\n     long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n     long presentCapacity \u003d used + remaining;\n     boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n     if (mode) {\n       System.out.println(\"Safe mode is ON\");\n       if (bytesInFuture \u003e 0) {\n         System.out.println(\"\\nWARNING: \");\n         System.out.println(\"Name node has detected blocks with generation \" +\n             \"stamps in future.\");\n         System.out.println(\"Forcing exit from safemode will cause \" +\n             bytesInFuture + \" byte(s) to be deleted.\");\n         System.out.println(\"If you are sure that the NameNode was started with\"\n             + \" the correct metadata files then you may proceed with \" +\n             \"\u0027-safemode forceExit\u0027\\n\");\n       }\n     }\n     System.out.println(\"Configured Capacity: \" + capacity\n                        + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n     System.out.println(\"Present Capacity: \" + presentCapacity\n         + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n     System.out.println(\"DFS Remaining: \" + remaining\n         + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n     System.out.println(\"DFS Used: \" + used\n                        + \" (\" + StringUtils.byteDesc(used) + \")\");\n     System.out.println(\"DFS Used%: \"\n         + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n     \n     /* These counts are not always upto date. They are updated after  \n      * iteration of an internal list. Should be updated in a few seconds to \n      * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n      * counts.\n      */\n     System.out.println(\"Under replicated blocks: \" + \n-                       dfs.getUnderReplicatedBlocksCount());\n+                       dfs.getLowRedundancyBlocksCount());\n     System.out.println(\"Blocks with corrupt replicas: \" + \n                        dfs.getCorruptBlocksCount());\n     System.out.println(\"Missing blocks: \" + \n                        dfs.getMissingBlocksCount());\n     System.out.println(\"Missing blocks (with replication factor 1): \" +\n                       dfs.getMissingReplOneBlocksCount());\n     System.out.println(\"Pending deletion blocks: \" +\n         dfs.getPendingDeletionBlocksCount());\n \n     System.out.println();\n \n     System.out.println(\"-------------------------------------------------\");\n     \n     // Parse arguments for filtering the node list\n     List\u003cString\u003e args \u003d Arrays.asList(argv);\n     // Truncate already handled arguments before parsing report()-specific ones\n     args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n     final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n     final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n     final boolean listDecommissioning \u003d\n         StringUtils.popOption(\"-decommissioning\", args);\n     final boolean listEnteringMaintenance \u003d\n         StringUtils.popOption(\"-enteringmaintenance\", args);\n     final boolean listInMaintenance \u003d\n         StringUtils.popOption(\"-inmaintenance\", args);\n \n \n     // If no filter flags are found, then list all DN types\n     boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning\n         \u0026\u0026 !listEnteringMaintenance \u0026\u0026 !listInMaintenance);\n \n     if (listAll || listLive) {\n       printDataNodeReports(dfs, DatanodeReportType.LIVE, listLive, \"Live\");\n     }\n \n     if (listAll || listDead) {\n       printDataNodeReports(dfs, DatanodeReportType.DEAD, listDead, \"Dead\");\n     }\n \n     if (listAll || listDecommissioning) {\n       printDataNodeReports(dfs, DatanodeReportType.DECOMMISSIONING,\n           listDecommissioning, \"Decommissioning\");\n     }\n \n     if (listAll || listEnteringMaintenance) {\n       printDataNodeReports(dfs, DatanodeReportType.ENTERING_MAINTENANCE,\n           listEnteringMaintenance, \"Entering maintenance\");\n     }\n \n     if (listAll || listInMaintenance) {\n       printDataNodeReports(dfs, DatanodeReportType.IN_MAINTENANCE,\n           listInMaintenance, \"In maintenance\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void report(String[] argv, int i) throws IOException {\n    DistributedFileSystem dfs \u003d getDFS();\n    FsStatus ds \u003d dfs.getStatus();\n    long capacity \u003d ds.getCapacity();\n    long used \u003d ds.getUsed();\n    long remaining \u003d ds.getRemaining();\n    long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n    long presentCapacity \u003d used + remaining;\n    boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n    if (mode) {\n      System.out.println(\"Safe mode is ON\");\n      if (bytesInFuture \u003e 0) {\n        System.out.println(\"\\nWARNING: \");\n        System.out.println(\"Name node has detected blocks with generation \" +\n            \"stamps in future.\");\n        System.out.println(\"Forcing exit from safemode will cause \" +\n            bytesInFuture + \" byte(s) to be deleted.\");\n        System.out.println(\"If you are sure that the NameNode was started with\"\n            + \" the correct metadata files then you may proceed with \" +\n            \"\u0027-safemode forceExit\u0027\\n\");\n      }\n    }\n    System.out.println(\"Configured Capacity: \" + capacity\n                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n    System.out.println(\"Present Capacity: \" + presentCapacity\n        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n    System.out.println(\"DFS Remaining: \" + remaining\n        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n    System.out.println(\"DFS Used: \" + used\n                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n    System.out.println(\"DFS Used%: \"\n        + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n    \n    /* These counts are not always upto date. They are updated after  \n     * iteration of an internal list. Should be updated in a few seconds to \n     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n     * counts.\n     */\n    System.out.println(\"Under replicated blocks: \" + \n                       dfs.getLowRedundancyBlocksCount());\n    System.out.println(\"Blocks with corrupt replicas: \" + \n                       dfs.getCorruptBlocksCount());\n    System.out.println(\"Missing blocks: \" + \n                       dfs.getMissingBlocksCount());\n    System.out.println(\"Missing blocks (with replication factor 1): \" +\n                      dfs.getMissingReplOneBlocksCount());\n    System.out.println(\"Pending deletion blocks: \" +\n        dfs.getPendingDeletionBlocksCount());\n\n    System.out.println();\n\n    System.out.println(\"-------------------------------------------------\");\n    \n    // Parse arguments for filtering the node list\n    List\u003cString\u003e args \u003d Arrays.asList(argv);\n    // Truncate already handled arguments before parsing report()-specific ones\n    args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n    final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n    final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n    final boolean listDecommissioning \u003d\n        StringUtils.popOption(\"-decommissioning\", args);\n    final boolean listEnteringMaintenance \u003d\n        StringUtils.popOption(\"-enteringmaintenance\", args);\n    final boolean listInMaintenance \u003d\n        StringUtils.popOption(\"-inmaintenance\", args);\n\n\n    // If no filter flags are found, then list all DN types\n    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning\n        \u0026\u0026 !listEnteringMaintenance \u0026\u0026 !listInMaintenance);\n\n    if (listAll || listLive) {\n      printDataNodeReports(dfs, DatanodeReportType.LIVE, listLive, \"Live\");\n    }\n\n    if (listAll || listDead) {\n      printDataNodeReports(dfs, DatanodeReportType.DEAD, listDead, \"Dead\");\n    }\n\n    if (listAll || listDecommissioning) {\n      printDataNodeReports(dfs, DatanodeReportType.DECOMMISSIONING,\n          listDecommissioning, \"Decommissioning\");\n    }\n\n    if (listAll || listEnteringMaintenance) {\n      printDataNodeReports(dfs, DatanodeReportType.ENTERING_MAINTENANCE,\n          listEnteringMaintenance, \"Entering maintenance\");\n    }\n\n    if (listAll || listInMaintenance) {\n      printDataNodeReports(dfs, DatanodeReportType.IN_MAINTENANCE,\n          listInMaintenance, \"In maintenance\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "8d9084eb62f4593d4dfeb618abacf6ae89019109": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11359. DFSAdmin report command supports displaying maintenance state datanodes. Contributed by Yiqun Lin.\n",
      "commitDate": "01/06/17 9:48 PM",
      "commitName": "8d9084eb62f4593d4dfeb618abacf6ae89019109",
      "commitAuthor": "Yiqun Lin",
      "commitDateOld": "16/05/17 9:41 AM",
      "commitNameOld": "89a8edc0149e3f31a5ade9a0927c4b6332cf6b1a",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 16.5,
      "commitsBetweenForRepo": 93,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,106 +1,94 @@\n   public void report(String[] argv, int i) throws IOException {\n     DistributedFileSystem dfs \u003d getDFS();\n     FsStatus ds \u003d dfs.getStatus();\n     long capacity \u003d ds.getCapacity();\n     long used \u003d ds.getUsed();\n     long remaining \u003d ds.getRemaining();\n     long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n     long presentCapacity \u003d used + remaining;\n     boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n     if (mode) {\n       System.out.println(\"Safe mode is ON\");\n       if (bytesInFuture \u003e 0) {\n         System.out.println(\"\\nWARNING: \");\n         System.out.println(\"Name node has detected blocks with generation \" +\n             \"stamps in future.\");\n         System.out.println(\"Forcing exit from safemode will cause \" +\n             bytesInFuture + \" byte(s) to be deleted.\");\n         System.out.println(\"If you are sure that the NameNode was started with\"\n             + \" the correct metadata files then you may proceed with \" +\n             \"\u0027-safemode forceExit\u0027\\n\");\n       }\n     }\n     System.out.println(\"Configured Capacity: \" + capacity\n                        + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n     System.out.println(\"Present Capacity: \" + presentCapacity\n         + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n     System.out.println(\"DFS Remaining: \" + remaining\n         + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n     System.out.println(\"DFS Used: \" + used\n                        + \" (\" + StringUtils.byteDesc(used) + \")\");\n     System.out.println(\"DFS Used%: \"\n         + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n     \n     /* These counts are not always upto date. They are updated after  \n      * iteration of an internal list. Should be updated in a few seconds to \n      * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n      * counts.\n      */\n     System.out.println(\"Under replicated blocks: \" + \n                        dfs.getUnderReplicatedBlocksCount());\n     System.out.println(\"Blocks with corrupt replicas: \" + \n                        dfs.getCorruptBlocksCount());\n     System.out.println(\"Missing blocks: \" + \n                        dfs.getMissingBlocksCount());\n     System.out.println(\"Missing blocks (with replication factor 1): \" +\n                       dfs.getMissingReplOneBlocksCount());\n     System.out.println(\"Pending deletion blocks: \" +\n         dfs.getPendingDeletionBlocksCount());\n \n     System.out.println();\n \n     System.out.println(\"-------------------------------------------------\");\n     \n     // Parse arguments for filtering the node list\n     List\u003cString\u003e args \u003d Arrays.asList(argv);\n     // Truncate already handled arguments before parsing report()-specific ones\n     args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n     final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n     final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n     final boolean listDecommissioning \u003d\n         StringUtils.popOption(\"-decommissioning\", args);\n+    final boolean listEnteringMaintenance \u003d\n+        StringUtils.popOption(\"-enteringmaintenance\", args);\n+    final boolean listInMaintenance \u003d\n+        StringUtils.popOption(\"-inmaintenance\", args);\n+\n \n     // If no filter flags are found, then list all DN types\n-    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning);\n+    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning\n+        \u0026\u0026 !listEnteringMaintenance \u0026\u0026 !listInMaintenance);\n \n     if (listAll || listLive) {\n-      DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n-      if (live.length \u003e 0 || listLive) {\n-        System.out.println(\"Live datanodes (\" + live.length + \"):\\n\");\n-      }\n-      if (live.length \u003e 0) {\n-        for (DatanodeInfo dn : live) {\n-          System.out.println(dn.getDatanodeReport());\n-          System.out.println();\n-        }\n-      }\n+      printDataNodeReports(dfs, DatanodeReportType.LIVE, listLive, \"Live\");\n     }\n \n     if (listAll || listDead) {\n-      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n-      if (dead.length \u003e 0 || listDead) {\n-        System.out.println(\"Dead datanodes (\" + dead.length + \"):\\n\");\n-      }\n-      if (dead.length \u003e 0) {\n-        for (DatanodeInfo dn : dead) {\n-          System.out.println(dn.getDatanodeReport());\n-          System.out.println();\n-        }\n-      }\n+      printDataNodeReports(dfs, DatanodeReportType.DEAD, listDead, \"Dead\");\n     }\n \n     if (listAll || listDecommissioning) {\n-      DatanodeInfo[] decom \u003d\n-          dfs.getDataNodeStats(DatanodeReportType.DECOMMISSIONING);\n-      if (decom.length \u003e 0 || listDecommissioning) {\n-        System.out.println(\"Decommissioning datanodes (\" + decom.length\n-            + \"):\\n\");\n-      }\n-      if (decom.length \u003e 0) {\n-        for (DatanodeInfo dn : decom) {\n-          System.out.println(dn.getDatanodeReport());\n-          System.out.println();\n-        }\n-      }\n+      printDataNodeReports(dfs, DatanodeReportType.DECOMMISSIONING,\n+          listDecommissioning, \"Decommissioning\");\n+    }\n+\n+    if (listAll || listEnteringMaintenance) {\n+      printDataNodeReports(dfs, DatanodeReportType.ENTERING_MAINTENANCE,\n+          listEnteringMaintenance, \"Entering maintenance\");\n+    }\n+\n+    if (listAll || listInMaintenance) {\n+      printDataNodeReports(dfs, DatanodeReportType.IN_MAINTENANCE,\n+          listInMaintenance, \"In maintenance\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void report(String[] argv, int i) throws IOException {\n    DistributedFileSystem dfs \u003d getDFS();\n    FsStatus ds \u003d dfs.getStatus();\n    long capacity \u003d ds.getCapacity();\n    long used \u003d ds.getUsed();\n    long remaining \u003d ds.getRemaining();\n    long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n    long presentCapacity \u003d used + remaining;\n    boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n    if (mode) {\n      System.out.println(\"Safe mode is ON\");\n      if (bytesInFuture \u003e 0) {\n        System.out.println(\"\\nWARNING: \");\n        System.out.println(\"Name node has detected blocks with generation \" +\n            \"stamps in future.\");\n        System.out.println(\"Forcing exit from safemode will cause \" +\n            bytesInFuture + \" byte(s) to be deleted.\");\n        System.out.println(\"If you are sure that the NameNode was started with\"\n            + \" the correct metadata files then you may proceed with \" +\n            \"\u0027-safemode forceExit\u0027\\n\");\n      }\n    }\n    System.out.println(\"Configured Capacity: \" + capacity\n                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n    System.out.println(\"Present Capacity: \" + presentCapacity\n        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n    System.out.println(\"DFS Remaining: \" + remaining\n        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n    System.out.println(\"DFS Used: \" + used\n                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n    System.out.println(\"DFS Used%: \"\n        + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n    \n    /* These counts are not always upto date. They are updated after  \n     * iteration of an internal list. Should be updated in a few seconds to \n     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n     * counts.\n     */\n    System.out.println(\"Under replicated blocks: \" + \n                       dfs.getUnderReplicatedBlocksCount());\n    System.out.println(\"Blocks with corrupt replicas: \" + \n                       dfs.getCorruptBlocksCount());\n    System.out.println(\"Missing blocks: \" + \n                       dfs.getMissingBlocksCount());\n    System.out.println(\"Missing blocks (with replication factor 1): \" +\n                      dfs.getMissingReplOneBlocksCount());\n    System.out.println(\"Pending deletion blocks: \" +\n        dfs.getPendingDeletionBlocksCount());\n\n    System.out.println();\n\n    System.out.println(\"-------------------------------------------------\");\n    \n    // Parse arguments for filtering the node list\n    List\u003cString\u003e args \u003d Arrays.asList(argv);\n    // Truncate already handled arguments before parsing report()-specific ones\n    args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n    final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n    final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n    final boolean listDecommissioning \u003d\n        StringUtils.popOption(\"-decommissioning\", args);\n    final boolean listEnteringMaintenance \u003d\n        StringUtils.popOption(\"-enteringmaintenance\", args);\n    final boolean listInMaintenance \u003d\n        StringUtils.popOption(\"-inmaintenance\", args);\n\n\n    // If no filter flags are found, then list all DN types\n    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning\n        \u0026\u0026 !listEnteringMaintenance \u0026\u0026 !listInMaintenance);\n\n    if (listAll || listLive) {\n      printDataNodeReports(dfs, DatanodeReportType.LIVE, listLive, \"Live\");\n    }\n\n    if (listAll || listDead) {\n      printDataNodeReports(dfs, DatanodeReportType.DEAD, listDead, \"Dead\");\n    }\n\n    if (listAll || listDecommissioning) {\n      printDataNodeReports(dfs, DatanodeReportType.DECOMMISSIONING,\n          listDecommissioning, \"Decommissioning\");\n    }\n\n    if (listAll || listEnteringMaintenance) {\n      printDataNodeReports(dfs, DatanodeReportType.ENTERING_MAINTENANCE,\n          listEnteringMaintenance, \"Entering maintenance\");\n    }\n\n    if (listAll || listInMaintenance) {\n      printDataNodeReports(dfs, DatanodeReportType.IN_MAINTENANCE,\n          listInMaintenance, \"In maintenance\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "10a2bc0dffaece216eb9a6bac3236a086b9ece31": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9653.  Added blocks pending deletion report to dfsadmin.\n(Weiwei Yang via eyang)\n",
      "commitDate": "24/01/16 2:19 PM",
      "commitName": "10a2bc0dffaece216eb9a6bac3236a086b9ece31",
      "commitAuthor": "Eric Yang",
      "commitDateOld": "06/01/16 9:40 AM",
      "commitNameOld": "b9936689c9ea37bf0050e7970643bcddfc9cfdbe",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 18.19,
      "commitsBetweenForRepo": 139,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,106 @@\n   public void report(String[] argv, int i) throws IOException {\n     DistributedFileSystem dfs \u003d getDFS();\n     FsStatus ds \u003d dfs.getStatus();\n     long capacity \u003d ds.getCapacity();\n     long used \u003d ds.getUsed();\n     long remaining \u003d ds.getRemaining();\n     long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n     long presentCapacity \u003d used + remaining;\n     boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n     if (mode) {\n       System.out.println(\"Safe mode is ON\");\n       if (bytesInFuture \u003e 0) {\n         System.out.println(\"\\nWARNING: \");\n         System.out.println(\"Name node has detected blocks with generation \" +\n             \"stamps in future.\");\n         System.out.println(\"Forcing exit from safemode will cause \" +\n             bytesInFuture + \" byte(s) to be deleted.\");\n         System.out.println(\"If you are sure that the NameNode was started with\"\n             + \" the correct metadata files then you may proceed with \" +\n             \"\u0027-safemode forceExit\u0027\\n\");\n       }\n     }\n     System.out.println(\"Configured Capacity: \" + capacity\n                        + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n     System.out.println(\"Present Capacity: \" + presentCapacity\n         + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n     System.out.println(\"DFS Remaining: \" + remaining\n         + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n     System.out.println(\"DFS Used: \" + used\n                        + \" (\" + StringUtils.byteDesc(used) + \")\");\n     System.out.println(\"DFS Used%: \"\n         + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n     \n     /* These counts are not always upto date. They are updated after  \n      * iteration of an internal list. Should be updated in a few seconds to \n      * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n      * counts.\n      */\n     System.out.println(\"Under replicated blocks: \" + \n                        dfs.getUnderReplicatedBlocksCount());\n     System.out.println(\"Blocks with corrupt replicas: \" + \n                        dfs.getCorruptBlocksCount());\n     System.out.println(\"Missing blocks: \" + \n                        dfs.getMissingBlocksCount());\n     System.out.println(\"Missing blocks (with replication factor 1): \" +\n                       dfs.getMissingReplOneBlocksCount());\n+    System.out.println(\"Pending deletion blocks: \" +\n+        dfs.getPendingDeletionBlocksCount());\n \n     System.out.println();\n \n     System.out.println(\"-------------------------------------------------\");\n     \n     // Parse arguments for filtering the node list\n     List\u003cString\u003e args \u003d Arrays.asList(argv);\n     // Truncate already handled arguments before parsing report()-specific ones\n     args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n     final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n     final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n     final boolean listDecommissioning \u003d\n         StringUtils.popOption(\"-decommissioning\", args);\n \n     // If no filter flags are found, then list all DN types\n     boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning);\n \n     if (listAll || listLive) {\n       DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n       if (live.length \u003e 0 || listLive) {\n         System.out.println(\"Live datanodes (\" + live.length + \"):\\n\");\n       }\n       if (live.length \u003e 0) {\n         for (DatanodeInfo dn : live) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n     }\n \n     if (listAll || listDead) {\n       DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n       if (dead.length \u003e 0 || listDead) {\n         System.out.println(\"Dead datanodes (\" + dead.length + \"):\\n\");\n       }\n       if (dead.length \u003e 0) {\n         for (DatanodeInfo dn : dead) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n     }\n \n     if (listAll || listDecommissioning) {\n       DatanodeInfo[] decom \u003d\n           dfs.getDataNodeStats(DatanodeReportType.DECOMMISSIONING);\n       if (decom.length \u003e 0 || listDecommissioning) {\n         System.out.println(\"Decommissioning datanodes (\" + decom.length\n             + \"):\\n\");\n       }\n       if (decom.length \u003e 0) {\n         for (DatanodeInfo dn : decom) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void report(String[] argv, int i) throws IOException {\n    DistributedFileSystem dfs \u003d getDFS();\n    FsStatus ds \u003d dfs.getStatus();\n    long capacity \u003d ds.getCapacity();\n    long used \u003d ds.getUsed();\n    long remaining \u003d ds.getRemaining();\n    long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n    long presentCapacity \u003d used + remaining;\n    boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n    if (mode) {\n      System.out.println(\"Safe mode is ON\");\n      if (bytesInFuture \u003e 0) {\n        System.out.println(\"\\nWARNING: \");\n        System.out.println(\"Name node has detected blocks with generation \" +\n            \"stamps in future.\");\n        System.out.println(\"Forcing exit from safemode will cause \" +\n            bytesInFuture + \" byte(s) to be deleted.\");\n        System.out.println(\"If you are sure that the NameNode was started with\"\n            + \" the correct metadata files then you may proceed with \" +\n            \"\u0027-safemode forceExit\u0027\\n\");\n      }\n    }\n    System.out.println(\"Configured Capacity: \" + capacity\n                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n    System.out.println(\"Present Capacity: \" + presentCapacity\n        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n    System.out.println(\"DFS Remaining: \" + remaining\n        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n    System.out.println(\"DFS Used: \" + used\n                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n    System.out.println(\"DFS Used%: \"\n        + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n    \n    /* These counts are not always upto date. They are updated after  \n     * iteration of an internal list. Should be updated in a few seconds to \n     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n     * counts.\n     */\n    System.out.println(\"Under replicated blocks: \" + \n                       dfs.getUnderReplicatedBlocksCount());\n    System.out.println(\"Blocks with corrupt replicas: \" + \n                       dfs.getCorruptBlocksCount());\n    System.out.println(\"Missing blocks: \" + \n                       dfs.getMissingBlocksCount());\n    System.out.println(\"Missing blocks (with replication factor 1): \" +\n                      dfs.getMissingReplOneBlocksCount());\n    System.out.println(\"Pending deletion blocks: \" +\n        dfs.getPendingDeletionBlocksCount());\n\n    System.out.println();\n\n    System.out.println(\"-------------------------------------------------\");\n    \n    // Parse arguments for filtering the node list\n    List\u003cString\u003e args \u003d Arrays.asList(argv);\n    // Truncate already handled arguments before parsing report()-specific ones\n    args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n    final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n    final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n    final boolean listDecommissioning \u003d\n        StringUtils.popOption(\"-decommissioning\", args);\n\n    // If no filter flags are found, then list all DN types\n    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning);\n\n    if (listAll || listLive) {\n      DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n      if (live.length \u003e 0 || listLive) {\n        System.out.println(\"Live datanodes (\" + live.length + \"):\\n\");\n      }\n      if (live.length \u003e 0) {\n        for (DatanodeInfo dn : live) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n\n    if (listAll || listDead) {\n      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n      if (dead.length \u003e 0 || listDead) {\n        System.out.println(\"Dead datanodes (\" + dead.length + \"):\\n\");\n      }\n      if (dead.length \u003e 0) {\n        for (DatanodeInfo dn : dead) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n\n    if (listAll || listDecommissioning) {\n      DatanodeInfo[] decom \u003d\n          dfs.getDataNodeStats(DatanodeReportType.DECOMMISSIONING);\n      if (decom.length \u003e 0 || listDecommissioning) {\n        System.out.println(\"Decommissioning datanodes (\" + decom.length\n            + \"):\\n\");\n      }\n      if (decom.length \u003e 0) {\n        for (DatanodeInfo dn : decom) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "86c92227fc56b6e06d879d250728e8dc8cbe98fe": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4015. Safemode should count and report orphaned blocks. (Contributed by Anu Engineer)\n",
      "commitDate": "23/10/15 6:07 PM",
      "commitName": "86c92227fc56b6e06d879d250728e8dc8cbe98fe",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "22/09/15 8:52 PM",
      "commitNameOld": "63d9f1596c92206cce3b72e3214d2fb5f6242b90",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 30.89,
      "commitsBetweenForRepo": 255,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,93 +1,104 @@\n   public void report(String[] argv, int i) throws IOException {\n     DistributedFileSystem dfs \u003d getDFS();\n     FsStatus ds \u003d dfs.getStatus();\n     long capacity \u003d ds.getCapacity();\n     long used \u003d ds.getUsed();\n     long remaining \u003d ds.getRemaining();\n+    long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n     long presentCapacity \u003d used + remaining;\n     boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n     if (mode) {\n       System.out.println(\"Safe mode is ON\");\n+      if (bytesInFuture \u003e 0) {\n+        System.out.println(\"\\nWARNING: \");\n+        System.out.println(\"Name node has detected blocks with generation \" +\n+            \"stamps in future.\");\n+        System.out.println(\"Forcing exit from safemode will cause \" +\n+            bytesInFuture + \" byte(s) to be deleted.\");\n+        System.out.println(\"If you are sure that the NameNode was started with\"\n+            + \" the correct metadata files then you may proceed with \" +\n+            \"\u0027-safemode forceExit\u0027\\n\");\n+      }\n     }\n     System.out.println(\"Configured Capacity: \" + capacity\n                        + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n     System.out.println(\"Present Capacity: \" + presentCapacity\n         + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n     System.out.println(\"DFS Remaining: \" + remaining\n         + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n     System.out.println(\"DFS Used: \" + used\n                        + \" (\" + StringUtils.byteDesc(used) + \")\");\n     System.out.println(\"DFS Used%: \"\n         + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n     \n     /* These counts are not always upto date. They are updated after  \n      * iteration of an internal list. Should be updated in a few seconds to \n      * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n      * counts.\n      */\n     System.out.println(\"Under replicated blocks: \" + \n                        dfs.getUnderReplicatedBlocksCount());\n     System.out.println(\"Blocks with corrupt replicas: \" + \n                        dfs.getCorruptBlocksCount());\n     System.out.println(\"Missing blocks: \" + \n                        dfs.getMissingBlocksCount());\n     System.out.println(\"Missing blocks (with replication factor 1): \" +\n                       dfs.getMissingReplOneBlocksCount());\n \n     System.out.println();\n \n     System.out.println(\"-------------------------------------------------\");\n     \n     // Parse arguments for filtering the node list\n     List\u003cString\u003e args \u003d Arrays.asList(argv);\n     // Truncate already handled arguments before parsing report()-specific ones\n     args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n     final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n     final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n     final boolean listDecommissioning \u003d\n         StringUtils.popOption(\"-decommissioning\", args);\n \n     // If no filter flags are found, then list all DN types\n     boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning);\n \n     if (listAll || listLive) {\n       DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n       if (live.length \u003e 0 || listLive) {\n         System.out.println(\"Live datanodes (\" + live.length + \"):\\n\");\n       }\n       if (live.length \u003e 0) {\n         for (DatanodeInfo dn : live) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n     }\n \n     if (listAll || listDead) {\n       DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n       if (dead.length \u003e 0 || listDead) {\n         System.out.println(\"Dead datanodes (\" + dead.length + \"):\\n\");\n       }\n       if (dead.length \u003e 0) {\n         for (DatanodeInfo dn : dead) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n     }\n \n     if (listAll || listDecommissioning) {\n       DatanodeInfo[] decom \u003d\n           dfs.getDataNodeStats(DatanodeReportType.DECOMMISSIONING);\n       if (decom.length \u003e 0 || listDecommissioning) {\n         System.out.println(\"Decommissioning datanodes (\" + decom.length\n             + \"):\\n\");\n       }\n       if (decom.length \u003e 0) {\n         for (DatanodeInfo dn : decom) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void report(String[] argv, int i) throws IOException {\n    DistributedFileSystem dfs \u003d getDFS();\n    FsStatus ds \u003d dfs.getStatus();\n    long capacity \u003d ds.getCapacity();\n    long used \u003d ds.getUsed();\n    long remaining \u003d ds.getRemaining();\n    long bytesInFuture \u003d dfs.getBytesWithFutureGenerationStamps();\n    long presentCapacity \u003d used + remaining;\n    boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n    if (mode) {\n      System.out.println(\"Safe mode is ON\");\n      if (bytesInFuture \u003e 0) {\n        System.out.println(\"\\nWARNING: \");\n        System.out.println(\"Name node has detected blocks with generation \" +\n            \"stamps in future.\");\n        System.out.println(\"Forcing exit from safemode will cause \" +\n            bytesInFuture + \" byte(s) to be deleted.\");\n        System.out.println(\"If you are sure that the NameNode was started with\"\n            + \" the correct metadata files then you may proceed with \" +\n            \"\u0027-safemode forceExit\u0027\\n\");\n      }\n    }\n    System.out.println(\"Configured Capacity: \" + capacity\n                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n    System.out.println(\"Present Capacity: \" + presentCapacity\n        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n    System.out.println(\"DFS Remaining: \" + remaining\n        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n    System.out.println(\"DFS Used: \" + used\n                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n    System.out.println(\"DFS Used%: \"\n        + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n    \n    /* These counts are not always upto date. They are updated after  \n     * iteration of an internal list. Should be updated in a few seconds to \n     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n     * counts.\n     */\n    System.out.println(\"Under replicated blocks: \" + \n                       dfs.getUnderReplicatedBlocksCount());\n    System.out.println(\"Blocks with corrupt replicas: \" + \n                       dfs.getCorruptBlocksCount());\n    System.out.println(\"Missing blocks: \" + \n                       dfs.getMissingBlocksCount());\n    System.out.println(\"Missing blocks (with replication factor 1): \" +\n                      dfs.getMissingReplOneBlocksCount());\n\n    System.out.println();\n\n    System.out.println(\"-------------------------------------------------\");\n    \n    // Parse arguments for filtering the node list\n    List\u003cString\u003e args \u003d Arrays.asList(argv);\n    // Truncate already handled arguments before parsing report()-specific ones\n    args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n    final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n    final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n    final boolean listDecommissioning \u003d\n        StringUtils.popOption(\"-decommissioning\", args);\n\n    // If no filter flags are found, then list all DN types\n    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning);\n\n    if (listAll || listLive) {\n      DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n      if (live.length \u003e 0 || listLive) {\n        System.out.println(\"Live datanodes (\" + live.length + \"):\\n\");\n      }\n      if (live.length \u003e 0) {\n        for (DatanodeInfo dn : live) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n\n    if (listAll || listDead) {\n      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n      if (dead.length \u003e 0 || listDead) {\n        System.out.println(\"Dead datanodes (\" + dead.length + \"):\\n\");\n      }\n      if (dead.length \u003e 0) {\n        for (DatanodeInfo dn : dead) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n\n    if (listAll || listDecommissioning) {\n      DatanodeInfo[] decom \u003d\n          dfs.getDataNodeStats(DatanodeReportType.DECOMMISSIONING);\n      if (decom.length \u003e 0 || listDecommissioning) {\n        System.out.println(\"Decommissioning datanodes (\" + decom.length\n            + \"):\\n\");\n      }\n      if (decom.length \u003e 0) {\n        for (DatanodeInfo dn : decom) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "8c5b23b5473e447384f818d69d907d5c35ed6d6a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7165. Separate block metrics for files with replication count 1. (Zhe Zhang via wang)\n",
      "commitDate": "23/10/14 12:28 PM",
      "commitName": "8c5b23b5473e447384f818d69d907d5c35ed6d6a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "20/10/14 6:33 PM",
      "commitNameOld": "7aab5fa1bd9386b036af45cd8206622a4555d74a",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 2.75,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,91 +1,93 @@\n   public void report(String[] argv, int i) throws IOException {\n     DistributedFileSystem dfs \u003d getDFS();\n     FsStatus ds \u003d dfs.getStatus();\n     long capacity \u003d ds.getCapacity();\n     long used \u003d ds.getUsed();\n     long remaining \u003d ds.getRemaining();\n     long presentCapacity \u003d used + remaining;\n     boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n     if (mode) {\n       System.out.println(\"Safe mode is ON\");\n     }\n     System.out.println(\"Configured Capacity: \" + capacity\n                        + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n     System.out.println(\"Present Capacity: \" + presentCapacity\n         + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n     System.out.println(\"DFS Remaining: \" + remaining\n         + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n     System.out.println(\"DFS Used: \" + used\n                        + \" (\" + StringUtils.byteDesc(used) + \")\");\n     System.out.println(\"DFS Used%: \"\n         + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n     \n     /* These counts are not always upto date. They are updated after  \n      * iteration of an internal list. Should be updated in a few seconds to \n      * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n      * counts.\n      */\n     System.out.println(\"Under replicated blocks: \" + \n                        dfs.getUnderReplicatedBlocksCount());\n     System.out.println(\"Blocks with corrupt replicas: \" + \n                        dfs.getCorruptBlocksCount());\n     System.out.println(\"Missing blocks: \" + \n                        dfs.getMissingBlocksCount());\n+    System.out.println(\"Missing blocks (with replication factor 1): \" +\n+                      dfs.getMissingReplOneBlocksCount());\n \n     System.out.println();\n \n     System.out.println(\"-------------------------------------------------\");\n     \n     // Parse arguments for filtering the node list\n     List\u003cString\u003e args \u003d Arrays.asList(argv);\n     // Truncate already handled arguments before parsing report()-specific ones\n     args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n     final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n     final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n     final boolean listDecommissioning \u003d\n         StringUtils.popOption(\"-decommissioning\", args);\n \n     // If no filter flags are found, then list all DN types\n     boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning);\n \n     if (listAll || listLive) {\n       DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n       if (live.length \u003e 0 || listLive) {\n         System.out.println(\"Live datanodes (\" + live.length + \"):\\n\");\n       }\n       if (live.length \u003e 0) {\n         for (DatanodeInfo dn : live) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n     }\n \n     if (listAll || listDead) {\n       DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n       if (dead.length \u003e 0 || listDead) {\n         System.out.println(\"Dead datanodes (\" + dead.length + \"):\\n\");\n       }\n       if (dead.length \u003e 0) {\n         for (DatanodeInfo dn : dead) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n     }\n \n     if (listAll || listDecommissioning) {\n       DatanodeInfo[] decom \u003d\n           dfs.getDataNodeStats(DatanodeReportType.DECOMMISSIONING);\n       if (decom.length \u003e 0 || listDecommissioning) {\n         System.out.println(\"Decommissioning datanodes (\" + decom.length\n             + \"):\\n\");\n       }\n       if (decom.length \u003e 0) {\n         for (DatanodeInfo dn : decom) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void report(String[] argv, int i) throws IOException {\n    DistributedFileSystem dfs \u003d getDFS();\n    FsStatus ds \u003d dfs.getStatus();\n    long capacity \u003d ds.getCapacity();\n    long used \u003d ds.getUsed();\n    long remaining \u003d ds.getRemaining();\n    long presentCapacity \u003d used + remaining;\n    boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n    if (mode) {\n      System.out.println(\"Safe mode is ON\");\n    }\n    System.out.println(\"Configured Capacity: \" + capacity\n                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n    System.out.println(\"Present Capacity: \" + presentCapacity\n        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n    System.out.println(\"DFS Remaining: \" + remaining\n        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n    System.out.println(\"DFS Used: \" + used\n                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n    System.out.println(\"DFS Used%: \"\n        + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n    \n    /* These counts are not always upto date. They are updated after  \n     * iteration of an internal list. Should be updated in a few seconds to \n     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n     * counts.\n     */\n    System.out.println(\"Under replicated blocks: \" + \n                       dfs.getUnderReplicatedBlocksCount());\n    System.out.println(\"Blocks with corrupt replicas: \" + \n                       dfs.getCorruptBlocksCount());\n    System.out.println(\"Missing blocks: \" + \n                       dfs.getMissingBlocksCount());\n    System.out.println(\"Missing blocks (with replication factor 1): \" +\n                      dfs.getMissingReplOneBlocksCount());\n\n    System.out.println();\n\n    System.out.println(\"-------------------------------------------------\");\n    \n    // Parse arguments for filtering the node list\n    List\u003cString\u003e args \u003d Arrays.asList(argv);\n    // Truncate already handled arguments before parsing report()-specific ones\n    args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n    final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n    final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n    final boolean listDecommissioning \u003d\n        StringUtils.popOption(\"-decommissioning\", args);\n\n    // If no filter flags are found, then list all DN types\n    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning);\n\n    if (listAll || listLive) {\n      DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n      if (live.length \u003e 0 || listLive) {\n        System.out.println(\"Live datanodes (\" + live.length + \"):\\n\");\n      }\n      if (live.length \u003e 0) {\n        for (DatanodeInfo dn : live) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n\n    if (listAll || listDead) {\n      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n      if (dead.length \u003e 0 || listDead) {\n        System.out.println(\"Dead datanodes (\" + dead.length + \"):\\n\");\n      }\n      if (dead.length \u003e 0) {\n        for (DatanodeInfo dn : dead) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n\n    if (listAll || listDecommissioning) {\n      DatanodeInfo[] decom \u003d\n          dfs.getDataNodeStats(DatanodeReportType.DECOMMISSIONING);\n      if (decom.length \u003e 0 || listDecommissioning) {\n        System.out.println(\"Decommissioning datanodes (\" + decom.length\n            + \"):\\n\");\n      }\n      if (decom.length \u003e 0) {\n        for (DatanodeInfo dn : decom) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "9445859930b8653cb0b9a0e1abf38cc05dbe2658": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6295. Add decommissioning state and node state filtering to dfsadmin. Contributed by Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1592438 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/05/14 8:38 PM",
      "commitName": "9445859930b8653cb0b9a0e1abf38cc05dbe2658",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6295. Add decommissioning state and node state filtering to dfsadmin. Contributed by Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1592438 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/05/14 8:38 PM",
          "commitName": "9445859930b8653cb0b9a0e1abf38cc05dbe2658",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "28/04/14 11:04 PM",
          "commitNameOld": "02d28907beab7110abf768fd4006b076a6bf2bd2",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 5.9,
          "commitsBetweenForRepo": 21,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,91 @@\n-  public void report() throws IOException {\n-      DistributedFileSystem dfs \u003d getDFS();\n-      FsStatus ds \u003d dfs.getStatus();\n-      long capacity \u003d ds.getCapacity();\n-      long used \u003d ds.getUsed();\n-      long remaining \u003d ds.getRemaining();\n-      long presentCapacity \u003d used + remaining;\n-      boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n-      if (mode) {\n-        System.out.println(\"Safe mode is ON\");\n-      }\n-      System.out.println(\"Configured Capacity: \" + capacity\n-                         + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n-      System.out.println(\"Present Capacity: \" + presentCapacity\n-          + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n-      System.out.println(\"DFS Remaining: \" + remaining\n-          + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n-      System.out.println(\"DFS Used: \" + used\n-                         + \" (\" + StringUtils.byteDesc(used) + \")\");\n-      System.out.println(\"DFS Used%: \"\n-          + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n-      \n-      /* These counts are not always upto date. They are updated after  \n-       * iteration of an internal list. Should be updated in a few seconds to \n-       * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n-       * counts.\n-       */\n-      System.out.println(\"Under replicated blocks: \" + \n-                         dfs.getUnderReplicatedBlocksCount());\n-      System.out.println(\"Blocks with corrupt replicas: \" + \n-                         dfs.getCorruptBlocksCount());\n-      System.out.println(\"Missing blocks: \" + \n-                         dfs.getMissingBlocksCount());\n-                           \n-      System.out.println();\n+  public void report(String[] argv, int i) throws IOException {\n+    DistributedFileSystem dfs \u003d getDFS();\n+    FsStatus ds \u003d dfs.getStatus();\n+    long capacity \u003d ds.getCapacity();\n+    long used \u003d ds.getUsed();\n+    long remaining \u003d ds.getRemaining();\n+    long presentCapacity \u003d used + remaining;\n+    boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n+    if (mode) {\n+      System.out.println(\"Safe mode is ON\");\n+    }\n+    System.out.println(\"Configured Capacity: \" + capacity\n+                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n+    System.out.println(\"Present Capacity: \" + presentCapacity\n+        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n+    System.out.println(\"DFS Remaining: \" + remaining\n+        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n+    System.out.println(\"DFS Used: \" + used\n+                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n+    System.out.println(\"DFS Used%: \"\n+        + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n+    \n+    /* These counts are not always upto date. They are updated after  \n+     * iteration of an internal list. Should be updated in a few seconds to \n+     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n+     * counts.\n+     */\n+    System.out.println(\"Under replicated blocks: \" + \n+                       dfs.getUnderReplicatedBlocksCount());\n+    System.out.println(\"Blocks with corrupt replicas: \" + \n+                       dfs.getCorruptBlocksCount());\n+    System.out.println(\"Missing blocks: \" + \n+                       dfs.getMissingBlocksCount());\n \n-      System.out.println(\"-------------------------------------------------\");\n-      \n+    System.out.println();\n+\n+    System.out.println(\"-------------------------------------------------\");\n+    \n+    // Parse arguments for filtering the node list\n+    List\u003cString\u003e args \u003d Arrays.asList(argv);\n+    // Truncate already handled arguments before parsing report()-specific ones\n+    args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n+    final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n+    final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n+    final boolean listDecommissioning \u003d\n+        StringUtils.popOption(\"-decommissioning\", args);\n+\n+    // If no filter flags are found, then list all DN types\n+    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning);\n+\n+    if (listAll || listLive) {\n       DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n-      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n-      System.out.println(\"Datanodes available: \" + live.length +\n-                         \" (\" + (live.length + dead.length) + \" total, \" + \n-                         dead.length + \" dead)\\n\");\n-      \n-      if(live.length \u003e 0) {\n-        System.out.println(\"Live datanodes:\");\n+      if (live.length \u003e 0 || listLive) {\n+        System.out.println(\"Live datanodes (\" + live.length + \"):\\n\");\n+      }\n+      if (live.length \u003e 0) {\n         for (DatanodeInfo dn : live) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n-      \n-      if(dead.length \u003e 0) {\n-        System.out.println(\"Dead datanodes:\");\n+    }\n+\n+    if (listAll || listDead) {\n+      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n+      if (dead.length \u003e 0 || listDead) {\n+        System.out.println(\"Dead datanodes (\" + dead.length + \"):\\n\");\n+      }\n+      if (dead.length \u003e 0) {\n         for (DatanodeInfo dn : dead) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n-        }     \n+        }\n       }\n+    }\n+\n+    if (listAll || listDecommissioning) {\n+      DatanodeInfo[] decom \u003d\n+          dfs.getDataNodeStats(DatanodeReportType.DECOMMISSIONING);\n+      if (decom.length \u003e 0 || listDecommissioning) {\n+        System.out.println(\"Decommissioning datanodes (\" + decom.length\n+            + \"):\\n\");\n+      }\n+      if (decom.length \u003e 0) {\n+        for (DatanodeInfo dn : decom) {\n+          System.out.println(dn.getDatanodeReport());\n+          System.out.println();\n+        }\n+      }\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void report(String[] argv, int i) throws IOException {\n    DistributedFileSystem dfs \u003d getDFS();\n    FsStatus ds \u003d dfs.getStatus();\n    long capacity \u003d ds.getCapacity();\n    long used \u003d ds.getUsed();\n    long remaining \u003d ds.getRemaining();\n    long presentCapacity \u003d used + remaining;\n    boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n    if (mode) {\n      System.out.println(\"Safe mode is ON\");\n    }\n    System.out.println(\"Configured Capacity: \" + capacity\n                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n    System.out.println(\"Present Capacity: \" + presentCapacity\n        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n    System.out.println(\"DFS Remaining: \" + remaining\n        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n    System.out.println(\"DFS Used: \" + used\n                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n    System.out.println(\"DFS Used%: \"\n        + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n    \n    /* These counts are not always upto date. They are updated after  \n     * iteration of an internal list. Should be updated in a few seconds to \n     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n     * counts.\n     */\n    System.out.println(\"Under replicated blocks: \" + \n                       dfs.getUnderReplicatedBlocksCount());\n    System.out.println(\"Blocks with corrupt replicas: \" + \n                       dfs.getCorruptBlocksCount());\n    System.out.println(\"Missing blocks: \" + \n                       dfs.getMissingBlocksCount());\n\n    System.out.println();\n\n    System.out.println(\"-------------------------------------------------\");\n    \n    // Parse arguments for filtering the node list\n    List\u003cString\u003e args \u003d Arrays.asList(argv);\n    // Truncate already handled arguments before parsing report()-specific ones\n    args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n    final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n    final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n    final boolean listDecommissioning \u003d\n        StringUtils.popOption(\"-decommissioning\", args);\n\n    // If no filter flags are found, then list all DN types\n    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning);\n\n    if (listAll || listLive) {\n      DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n      if (live.length \u003e 0 || listLive) {\n        System.out.println(\"Live datanodes (\" + live.length + \"):\\n\");\n      }\n      if (live.length \u003e 0) {\n        for (DatanodeInfo dn : live) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n\n    if (listAll || listDead) {\n      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n      if (dead.length \u003e 0 || listDead) {\n        System.out.println(\"Dead datanodes (\" + dead.length + \"):\\n\");\n      }\n      if (dead.length \u003e 0) {\n        for (DatanodeInfo dn : dead) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n\n    if (listAll || listDecommissioning) {\n      DatanodeInfo[] decom \u003d\n          dfs.getDataNodeStats(DatanodeReportType.DECOMMISSIONING);\n      if (decom.length \u003e 0 || listDecommissioning) {\n        System.out.println(\"Decommissioning datanodes (\" + decom.length\n            + \"):\\n\");\n      }\n      if (decom.length \u003e 0) {\n        for (DatanodeInfo dn : decom) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[argv-String[], i-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6295. Add decommissioning state and node state filtering to dfsadmin. Contributed by Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1592438 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/05/14 8:38 PM",
          "commitName": "9445859930b8653cb0b9a0e1abf38cc05dbe2658",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "28/04/14 11:04 PM",
          "commitNameOld": "02d28907beab7110abf768fd4006b076a6bf2bd2",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 5.9,
          "commitsBetweenForRepo": 21,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,91 @@\n-  public void report() throws IOException {\n-      DistributedFileSystem dfs \u003d getDFS();\n-      FsStatus ds \u003d dfs.getStatus();\n-      long capacity \u003d ds.getCapacity();\n-      long used \u003d ds.getUsed();\n-      long remaining \u003d ds.getRemaining();\n-      long presentCapacity \u003d used + remaining;\n-      boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n-      if (mode) {\n-        System.out.println(\"Safe mode is ON\");\n-      }\n-      System.out.println(\"Configured Capacity: \" + capacity\n-                         + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n-      System.out.println(\"Present Capacity: \" + presentCapacity\n-          + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n-      System.out.println(\"DFS Remaining: \" + remaining\n-          + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n-      System.out.println(\"DFS Used: \" + used\n-                         + \" (\" + StringUtils.byteDesc(used) + \")\");\n-      System.out.println(\"DFS Used%: \"\n-          + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n-      \n-      /* These counts are not always upto date. They are updated after  \n-       * iteration of an internal list. Should be updated in a few seconds to \n-       * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n-       * counts.\n-       */\n-      System.out.println(\"Under replicated blocks: \" + \n-                         dfs.getUnderReplicatedBlocksCount());\n-      System.out.println(\"Blocks with corrupt replicas: \" + \n-                         dfs.getCorruptBlocksCount());\n-      System.out.println(\"Missing blocks: \" + \n-                         dfs.getMissingBlocksCount());\n-                           \n-      System.out.println();\n+  public void report(String[] argv, int i) throws IOException {\n+    DistributedFileSystem dfs \u003d getDFS();\n+    FsStatus ds \u003d dfs.getStatus();\n+    long capacity \u003d ds.getCapacity();\n+    long used \u003d ds.getUsed();\n+    long remaining \u003d ds.getRemaining();\n+    long presentCapacity \u003d used + remaining;\n+    boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n+    if (mode) {\n+      System.out.println(\"Safe mode is ON\");\n+    }\n+    System.out.println(\"Configured Capacity: \" + capacity\n+                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n+    System.out.println(\"Present Capacity: \" + presentCapacity\n+        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n+    System.out.println(\"DFS Remaining: \" + remaining\n+        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n+    System.out.println(\"DFS Used: \" + used\n+                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n+    System.out.println(\"DFS Used%: \"\n+        + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n+    \n+    /* These counts are not always upto date. They are updated after  \n+     * iteration of an internal list. Should be updated in a few seconds to \n+     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n+     * counts.\n+     */\n+    System.out.println(\"Under replicated blocks: \" + \n+                       dfs.getUnderReplicatedBlocksCount());\n+    System.out.println(\"Blocks with corrupt replicas: \" + \n+                       dfs.getCorruptBlocksCount());\n+    System.out.println(\"Missing blocks: \" + \n+                       dfs.getMissingBlocksCount());\n \n-      System.out.println(\"-------------------------------------------------\");\n-      \n+    System.out.println();\n+\n+    System.out.println(\"-------------------------------------------------\");\n+    \n+    // Parse arguments for filtering the node list\n+    List\u003cString\u003e args \u003d Arrays.asList(argv);\n+    // Truncate already handled arguments before parsing report()-specific ones\n+    args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n+    final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n+    final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n+    final boolean listDecommissioning \u003d\n+        StringUtils.popOption(\"-decommissioning\", args);\n+\n+    // If no filter flags are found, then list all DN types\n+    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning);\n+\n+    if (listAll || listLive) {\n       DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n-      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n-      System.out.println(\"Datanodes available: \" + live.length +\n-                         \" (\" + (live.length + dead.length) + \" total, \" + \n-                         dead.length + \" dead)\\n\");\n-      \n-      if(live.length \u003e 0) {\n-        System.out.println(\"Live datanodes:\");\n+      if (live.length \u003e 0 || listLive) {\n+        System.out.println(\"Live datanodes (\" + live.length + \"):\\n\");\n+      }\n+      if (live.length \u003e 0) {\n         for (DatanodeInfo dn : live) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n-      \n-      if(dead.length \u003e 0) {\n-        System.out.println(\"Dead datanodes:\");\n+    }\n+\n+    if (listAll || listDead) {\n+      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n+      if (dead.length \u003e 0 || listDead) {\n+        System.out.println(\"Dead datanodes (\" + dead.length + \"):\\n\");\n+      }\n+      if (dead.length \u003e 0) {\n         for (DatanodeInfo dn : dead) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n-        }     \n+        }\n       }\n+    }\n+\n+    if (listAll || listDecommissioning) {\n+      DatanodeInfo[] decom \u003d\n+          dfs.getDataNodeStats(DatanodeReportType.DECOMMISSIONING);\n+      if (decom.length \u003e 0 || listDecommissioning) {\n+        System.out.println(\"Decommissioning datanodes (\" + decom.length\n+            + \"):\\n\");\n+      }\n+      if (decom.length \u003e 0) {\n+        for (DatanodeInfo dn : decom) {\n+          System.out.println(dn.getDatanodeReport());\n+          System.out.println();\n+        }\n+      }\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void report(String[] argv, int i) throws IOException {\n    DistributedFileSystem dfs \u003d getDFS();\n    FsStatus ds \u003d dfs.getStatus();\n    long capacity \u003d ds.getCapacity();\n    long used \u003d ds.getUsed();\n    long remaining \u003d ds.getRemaining();\n    long presentCapacity \u003d used + remaining;\n    boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n    if (mode) {\n      System.out.println(\"Safe mode is ON\");\n    }\n    System.out.println(\"Configured Capacity: \" + capacity\n                       + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n    System.out.println(\"Present Capacity: \" + presentCapacity\n        + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n    System.out.println(\"DFS Remaining: \" + remaining\n        + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n    System.out.println(\"DFS Used: \" + used\n                       + \" (\" + StringUtils.byteDesc(used) + \")\");\n    System.out.println(\"DFS Used%: \"\n        + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n    \n    /* These counts are not always upto date. They are updated after  \n     * iteration of an internal list. Should be updated in a few seconds to \n     * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n     * counts.\n     */\n    System.out.println(\"Under replicated blocks: \" + \n                       dfs.getUnderReplicatedBlocksCount());\n    System.out.println(\"Blocks with corrupt replicas: \" + \n                       dfs.getCorruptBlocksCount());\n    System.out.println(\"Missing blocks: \" + \n                       dfs.getMissingBlocksCount());\n\n    System.out.println();\n\n    System.out.println(\"-------------------------------------------------\");\n    \n    // Parse arguments for filtering the node list\n    List\u003cString\u003e args \u003d Arrays.asList(argv);\n    // Truncate already handled arguments before parsing report()-specific ones\n    args \u003d new ArrayList\u003cString\u003e(args.subList(i, args.size()));\n    final boolean listLive \u003d StringUtils.popOption(\"-live\", args);\n    final boolean listDead \u003d StringUtils.popOption(\"-dead\", args);\n    final boolean listDecommissioning \u003d\n        StringUtils.popOption(\"-decommissioning\", args);\n\n    // If no filter flags are found, then list all DN types\n    boolean listAll \u003d (!listLive \u0026\u0026 !listDead \u0026\u0026 !listDecommissioning);\n\n    if (listAll || listLive) {\n      DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n      if (live.length \u003e 0 || listLive) {\n        System.out.println(\"Live datanodes (\" + live.length + \"):\\n\");\n      }\n      if (live.length \u003e 0) {\n        for (DatanodeInfo dn : live) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n\n    if (listAll || listDead) {\n      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n      if (dead.length \u003e 0 || listDead) {\n        System.out.println(\"Dead datanodes (\" + dead.length + \"):\\n\");\n      }\n      if (dead.length \u003e 0) {\n        for (DatanodeInfo dn : dead) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n\n    if (listAll || listDecommissioning) {\n      DatanodeInfo[] decom \u003d\n          dfs.getDataNodeStats(DatanodeReportType.DECOMMISSIONING);\n      if (decom.length \u003e 0 || listDecommissioning) {\n        System.out.println(\"Decommissioning datanodes (\" + decom.length\n            + \"):\\n\");\n      }\n      if (decom.length \u003e 0) {\n        for (DatanodeInfo dn : decom) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
          "extendedDetails": {}
        }
      ]
    },
    "e28edbffe15e9d176d14ea2af8d9460d807b3fc4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4468.  Use the new StringUtils methods added by HADOOP-9252 and fix TestHDFSCLI and TestQuota.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1442824 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/02/13 5:13 PM",
      "commitName": "e28edbffe15e9d176d14ea2af8d9460d807b3fc4",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "16/11/12 1:51 PM",
      "commitNameOld": "320c32a2895e0e63b9f4d55da8c11ffbb3e9227a",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 81.14,
      "commitsBetweenForRepo": 335,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,61 +1,60 @@\n   public void report() throws IOException {\n       DistributedFileSystem dfs \u003d getDFS();\n       FsStatus ds \u003d dfs.getStatus();\n       long capacity \u003d ds.getCapacity();\n       long used \u003d ds.getUsed();\n       long remaining \u003d ds.getRemaining();\n       long presentCapacity \u003d used + remaining;\n       boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n       if (mode) {\n         System.out.println(\"Safe mode is ON\");\n       }\n       System.out.println(\"Configured Capacity: \" + capacity\n                          + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n       System.out.println(\"Present Capacity: \" + presentCapacity\n           + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n       System.out.println(\"DFS Remaining: \" + remaining\n           + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n       System.out.println(\"DFS Used: \" + used\n                          + \" (\" + StringUtils.byteDesc(used) + \")\");\n       System.out.println(\"DFS Used%: \"\n-                         + StringUtils.limitDecimalTo2(((1.0 * used) / presentCapacity) * 100)\n-                         + \"%\");\n+          + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n       \n       /* These counts are not always upto date. They are updated after  \n        * iteration of an internal list. Should be updated in a few seconds to \n        * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n        * counts.\n        */\n       System.out.println(\"Under replicated blocks: \" + \n                          dfs.getUnderReplicatedBlocksCount());\n       System.out.println(\"Blocks with corrupt replicas: \" + \n                          dfs.getCorruptBlocksCount());\n       System.out.println(\"Missing blocks: \" + \n                          dfs.getMissingBlocksCount());\n                            \n       System.out.println();\n \n       System.out.println(\"-------------------------------------------------\");\n       \n       DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n       DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n       System.out.println(\"Datanodes available: \" + live.length +\n                          \" (\" + (live.length + dead.length) + \" total, \" + \n                          dead.length + \" dead)\\n\");\n       \n       if(live.length \u003e 0) {\n         System.out.println(\"Live datanodes:\");\n         for (DatanodeInfo dn : live) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n       \n       if(dead.length \u003e 0) {\n         System.out.println(\"Dead datanodes:\");\n         for (DatanodeInfo dn : dead) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }     \n       }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void report() throws IOException {\n      DistributedFileSystem dfs \u003d getDFS();\n      FsStatus ds \u003d dfs.getStatus();\n      long capacity \u003d ds.getCapacity();\n      long used \u003d ds.getUsed();\n      long remaining \u003d ds.getRemaining();\n      long presentCapacity \u003d used + remaining;\n      boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n      if (mode) {\n        System.out.println(\"Safe mode is ON\");\n      }\n      System.out.println(\"Configured Capacity: \" + capacity\n                         + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n      System.out.println(\"Present Capacity: \" + presentCapacity\n          + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n      System.out.println(\"DFS Remaining: \" + remaining\n          + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n      System.out.println(\"DFS Used: \" + used\n                         + \" (\" + StringUtils.byteDesc(used) + \")\");\n      System.out.println(\"DFS Used%: \"\n          + StringUtils.formatPercent(used/(double)presentCapacity, 2));\n      \n      /* These counts are not always upto date. They are updated after  \n       * iteration of an internal list. Should be updated in a few seconds to \n       * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n       * counts.\n       */\n      System.out.println(\"Under replicated blocks: \" + \n                         dfs.getUnderReplicatedBlocksCount());\n      System.out.println(\"Blocks with corrupt replicas: \" + \n                         dfs.getCorruptBlocksCount());\n      System.out.println(\"Missing blocks: \" + \n                         dfs.getMissingBlocksCount());\n                           \n      System.out.println();\n\n      System.out.println(\"-------------------------------------------------\");\n      \n      DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n      System.out.println(\"Datanodes available: \" + live.length +\n                         \" (\" + (live.length + dead.length) + \" total, \" + \n                         dead.length + \" dead)\\n\");\n      \n      if(live.length \u003e 0) {\n        System.out.println(\"Live datanodes:\");\n        for (DatanodeInfo dn : live) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n      \n      if(dead.length \u003e 0) {\n        System.out.println(\"Dead datanodes:\");\n        for (DatanodeInfo dn : dead) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }     \n      }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "6c0ccb5989c2053f5a1ebab0dd9fdb7b4019fda8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2686. Remove DistributedUpgrade related code. Contributed by Suresh Srinivas\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1375800 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/08/12 2:18 PM",
      "commitName": "6c0ccb5989c2053f5a1ebab0dd9fdb7b4019fda8",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "16/08/12 1:35 PM",
      "commitNameOld": "cf93dfba4e5b7849a3917caa78b29b8a4fb5ef12",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 5.03,
      "commitsBetweenForRepo": 28,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,61 @@\n   public void report() throws IOException {\n       DistributedFileSystem dfs \u003d getDFS();\n       FsStatus ds \u003d dfs.getStatus();\n       long capacity \u003d ds.getCapacity();\n       long used \u003d ds.getUsed();\n       long remaining \u003d ds.getRemaining();\n       long presentCapacity \u003d used + remaining;\n       boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n-      UpgradeStatusReport status \u003d \n-                      dfs.distributedUpgradeProgress(UpgradeAction.GET_STATUS);\n-\n       if (mode) {\n         System.out.println(\"Safe mode is ON\");\n       }\n-      if (status !\u003d null) {\n-        System.out.println(status.getStatusText(false));\n-      }\n       System.out.println(\"Configured Capacity: \" + capacity\n                          + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n       System.out.println(\"Present Capacity: \" + presentCapacity\n           + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n       System.out.println(\"DFS Remaining: \" + remaining\n           + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n       System.out.println(\"DFS Used: \" + used\n                          + \" (\" + StringUtils.byteDesc(used) + \")\");\n       System.out.println(\"DFS Used%: \"\n                          + StringUtils.limitDecimalTo2(((1.0 * used) / presentCapacity) * 100)\n                          + \"%\");\n       \n       /* These counts are not always upto date. They are updated after  \n        * iteration of an internal list. Should be updated in a few seconds to \n        * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n        * counts.\n        */\n       System.out.println(\"Under replicated blocks: \" + \n                          dfs.getUnderReplicatedBlocksCount());\n       System.out.println(\"Blocks with corrupt replicas: \" + \n                          dfs.getCorruptBlocksCount());\n       System.out.println(\"Missing blocks: \" + \n                          dfs.getMissingBlocksCount());\n                            \n       System.out.println();\n \n       System.out.println(\"-------------------------------------------------\");\n       \n       DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n       DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n       System.out.println(\"Datanodes available: \" + live.length +\n                          \" (\" + (live.length + dead.length) + \" total, \" + \n                          dead.length + \" dead)\\n\");\n       \n       if(live.length \u003e 0) {\n         System.out.println(\"Live datanodes:\");\n         for (DatanodeInfo dn : live) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n       \n       if(dead.length \u003e 0) {\n         System.out.println(\"Dead datanodes:\");\n         for (DatanodeInfo dn : dead) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }     \n       }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void report() throws IOException {\n      DistributedFileSystem dfs \u003d getDFS();\n      FsStatus ds \u003d dfs.getStatus();\n      long capacity \u003d ds.getCapacity();\n      long used \u003d ds.getUsed();\n      long remaining \u003d ds.getRemaining();\n      long presentCapacity \u003d used + remaining;\n      boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n      if (mode) {\n        System.out.println(\"Safe mode is ON\");\n      }\n      System.out.println(\"Configured Capacity: \" + capacity\n                         + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n      System.out.println(\"Present Capacity: \" + presentCapacity\n          + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n      System.out.println(\"DFS Remaining: \" + remaining\n          + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n      System.out.println(\"DFS Used: \" + used\n                         + \" (\" + StringUtils.byteDesc(used) + \")\");\n      System.out.println(\"DFS Used%: \"\n                         + StringUtils.limitDecimalTo2(((1.0 * used) / presentCapacity) * 100)\n                         + \"%\");\n      \n      /* These counts are not always upto date. They are updated after  \n       * iteration of an internal list. Should be updated in a few seconds to \n       * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n       * counts.\n       */\n      System.out.println(\"Under replicated blocks: \" + \n                         dfs.getUnderReplicatedBlocksCount());\n      System.out.println(\"Blocks with corrupt replicas: \" + \n                         dfs.getCorruptBlocksCount());\n      System.out.println(\"Missing blocks: \" + \n                         dfs.getMissingBlocksCount());\n                           \n      System.out.println();\n\n      System.out.println(\"-------------------------------------------------\");\n      \n      DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n      System.out.println(\"Datanodes available: \" + live.length +\n                         \" (\" + (live.length + dead.length) + \" total, \" + \n                         dead.length + \" dead)\\n\");\n      \n      if(live.length \u003e 0) {\n        System.out.println(\"Live datanodes:\");\n        for (DatanodeInfo dn : live) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n      \n      if(dead.length \u003e 0) {\n        System.out.println(\"Dead datanodes:\");\n        for (DatanodeInfo dn : dead) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }     \n      }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1620. Rename HdfsConstants -\u003e HdfsServerConstants, FSConstants -\u003e HdfsConstants. (Harsh J Chouraria via atm)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165096 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/11 12:30 PM",
      "commitName": "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 10.8,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,67 @@\n   public void report() throws IOException {\n       DistributedFileSystem dfs \u003d getDFS();\n       FsStatus ds \u003d dfs.getStatus();\n       long capacity \u003d ds.getCapacity();\n       long used \u003d ds.getUsed();\n       long remaining \u003d ds.getRemaining();\n       long presentCapacity \u003d used + remaining;\n-      boolean mode \u003d dfs.setSafeMode(FSConstants.SafeModeAction.SAFEMODE_GET);\n+      boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n       UpgradeStatusReport status \u003d \n                       dfs.distributedUpgradeProgress(UpgradeAction.GET_STATUS);\n \n       if (mode) {\n         System.out.println(\"Safe mode is ON\");\n       }\n       if (status !\u003d null) {\n         System.out.println(status.getStatusText(false));\n       }\n       System.out.println(\"Configured Capacity: \" + capacity\n                          + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n       System.out.println(\"Present Capacity: \" + presentCapacity\n           + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n       System.out.println(\"DFS Remaining: \" + remaining\n           + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n       System.out.println(\"DFS Used: \" + used\n                          + \" (\" + StringUtils.byteDesc(used) + \")\");\n       System.out.println(\"DFS Used%: \"\n                          + StringUtils.limitDecimalTo2(((1.0 * used) / presentCapacity) * 100)\n                          + \"%\");\n       \n       /* These counts are not always upto date. They are updated after  \n        * iteration of an internal list. Should be updated in a few seconds to \n        * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n        * counts.\n        */\n       System.out.println(\"Under replicated blocks: \" + \n                          dfs.getUnderReplicatedBlocksCount());\n       System.out.println(\"Blocks with corrupt replicas: \" + \n                          dfs.getCorruptBlocksCount());\n       System.out.println(\"Missing blocks: \" + \n                          dfs.getMissingBlocksCount());\n                            \n       System.out.println();\n \n       System.out.println(\"-------------------------------------------------\");\n       \n       DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n       DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n       System.out.println(\"Datanodes available: \" + live.length +\n                          \" (\" + (live.length + dead.length) + \" total, \" + \n                          dead.length + \" dead)\\n\");\n       \n       if(live.length \u003e 0) {\n         System.out.println(\"Live datanodes:\");\n         for (DatanodeInfo dn : live) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n       \n       if(dead.length \u003e 0) {\n         System.out.println(\"Dead datanodes:\");\n         for (DatanodeInfo dn : dead) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }     \n       }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void report() throws IOException {\n      DistributedFileSystem dfs \u003d getDFS();\n      FsStatus ds \u003d dfs.getStatus();\n      long capacity \u003d ds.getCapacity();\n      long used \u003d ds.getUsed();\n      long remaining \u003d ds.getRemaining();\n      long presentCapacity \u003d used + remaining;\n      boolean mode \u003d dfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_GET);\n      UpgradeStatusReport status \u003d \n                      dfs.distributedUpgradeProgress(UpgradeAction.GET_STATUS);\n\n      if (mode) {\n        System.out.println(\"Safe mode is ON\");\n      }\n      if (status !\u003d null) {\n        System.out.println(status.getStatusText(false));\n      }\n      System.out.println(\"Configured Capacity: \" + capacity\n                         + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n      System.out.println(\"Present Capacity: \" + presentCapacity\n          + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n      System.out.println(\"DFS Remaining: \" + remaining\n          + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n      System.out.println(\"DFS Used: \" + used\n                         + \" (\" + StringUtils.byteDesc(used) + \")\");\n      System.out.println(\"DFS Used%: \"\n                         + StringUtils.limitDecimalTo2(((1.0 * used) / presentCapacity) * 100)\n                         + \"%\");\n      \n      /* These counts are not always upto date. They are updated after  \n       * iteration of an internal list. Should be updated in a few seconds to \n       * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n       * counts.\n       */\n      System.out.println(\"Under replicated blocks: \" + \n                         dfs.getUnderReplicatedBlocksCount());\n      System.out.println(\"Blocks with corrupt replicas: \" + \n                         dfs.getCorruptBlocksCount());\n      System.out.println(\"Missing blocks: \" + \n                         dfs.getMissingBlocksCount());\n                           \n      System.out.println();\n\n      System.out.println(\"-------------------------------------------------\");\n      \n      DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n      System.out.println(\"Datanodes available: \" + live.length +\n                         \" (\" + (live.length + dead.length) + \" total, \" + \n                         dead.length + \" dead)\\n\");\n      \n      if(live.length \u003e 0) {\n        System.out.println(\"Live datanodes:\");\n        for (DatanodeInfo dn : live) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n      \n      if(dead.length \u003e 0) {\n        System.out.println(\"Dead datanodes:\");\n        for (DatanodeInfo dn : dead) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }     \n      }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void report() throws IOException {\n      DistributedFileSystem dfs \u003d getDFS();\n      FsStatus ds \u003d dfs.getStatus();\n      long capacity \u003d ds.getCapacity();\n      long used \u003d ds.getUsed();\n      long remaining \u003d ds.getRemaining();\n      long presentCapacity \u003d used + remaining;\n      boolean mode \u003d dfs.setSafeMode(FSConstants.SafeModeAction.SAFEMODE_GET);\n      UpgradeStatusReport status \u003d \n                      dfs.distributedUpgradeProgress(UpgradeAction.GET_STATUS);\n\n      if (mode) {\n        System.out.println(\"Safe mode is ON\");\n      }\n      if (status !\u003d null) {\n        System.out.println(status.getStatusText(false));\n      }\n      System.out.println(\"Configured Capacity: \" + capacity\n                         + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n      System.out.println(\"Present Capacity: \" + presentCapacity\n          + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n      System.out.println(\"DFS Remaining: \" + remaining\n          + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n      System.out.println(\"DFS Used: \" + used\n                         + \" (\" + StringUtils.byteDesc(used) + \")\");\n      System.out.println(\"DFS Used%: \"\n                         + StringUtils.limitDecimalTo2(((1.0 * used) / presentCapacity) * 100)\n                         + \"%\");\n      \n      /* These counts are not always upto date. They are updated after  \n       * iteration of an internal list. Should be updated in a few seconds to \n       * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n       * counts.\n       */\n      System.out.println(\"Under replicated blocks: \" + \n                         dfs.getUnderReplicatedBlocksCount());\n      System.out.println(\"Blocks with corrupt replicas: \" + \n                         dfs.getCorruptBlocksCount());\n      System.out.println(\"Missing blocks: \" + \n                         dfs.getMissingBlocksCount());\n                           \n      System.out.println();\n\n      System.out.println(\"-------------------------------------------------\");\n      \n      DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n      System.out.println(\"Datanodes available: \" + live.length +\n                         \" (\" + (live.length + dead.length) + \" total, \" + \n                         dead.length + \" dead)\\n\");\n      \n      if(live.length \u003e 0) {\n        System.out.println(\"Live datanodes:\");\n        for (DatanodeInfo dn : live) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n      \n      if(dead.length \u003e 0) {\n        System.out.println(\"Dead datanodes:\");\n        for (DatanodeInfo dn : dead) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }     \n      }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void report() throws IOException {\n      DistributedFileSystem dfs \u003d getDFS();\n      FsStatus ds \u003d dfs.getStatus();\n      long capacity \u003d ds.getCapacity();\n      long used \u003d ds.getUsed();\n      long remaining \u003d ds.getRemaining();\n      long presentCapacity \u003d used + remaining;\n      boolean mode \u003d dfs.setSafeMode(FSConstants.SafeModeAction.SAFEMODE_GET);\n      UpgradeStatusReport status \u003d \n                      dfs.distributedUpgradeProgress(UpgradeAction.GET_STATUS);\n\n      if (mode) {\n        System.out.println(\"Safe mode is ON\");\n      }\n      if (status !\u003d null) {\n        System.out.println(status.getStatusText(false));\n      }\n      System.out.println(\"Configured Capacity: \" + capacity\n                         + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n      System.out.println(\"Present Capacity: \" + presentCapacity\n          + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n      System.out.println(\"DFS Remaining: \" + remaining\n          + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n      System.out.println(\"DFS Used: \" + used\n                         + \" (\" + StringUtils.byteDesc(used) + \")\");\n      System.out.println(\"DFS Used%: \"\n                         + StringUtils.limitDecimalTo2(((1.0 * used) / presentCapacity) * 100)\n                         + \"%\");\n      \n      /* These counts are not always upto date. They are updated after  \n       * iteration of an internal list. Should be updated in a few seconds to \n       * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n       * counts.\n       */\n      System.out.println(\"Under replicated blocks: \" + \n                         dfs.getUnderReplicatedBlocksCount());\n      System.out.println(\"Blocks with corrupt replicas: \" + \n                         dfs.getCorruptBlocksCount());\n      System.out.println(\"Missing blocks: \" + \n                         dfs.getMissingBlocksCount());\n                           \n      System.out.println();\n\n      System.out.println(\"-------------------------------------------------\");\n      \n      DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n      System.out.println(\"Datanodes available: \" + live.length +\n                         \" (\" + (live.length + dead.length) + \" total, \" + \n                         dead.length + \" dead)\\n\");\n      \n      if(live.length \u003e 0) {\n        System.out.println(\"Live datanodes:\");\n        for (DatanodeInfo dn : live) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n      \n      if(dead.length \u003e 0) {\n        System.out.println(\"Dead datanodes:\");\n        for (DatanodeInfo dn : dead) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }     \n      }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java"
      }
    },
    "c163455df487f99171e5045cdf0c2e1be1c4f99e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2159. Deprecate DistributedFileSystem.getClient() and fixed the deprecated warnings in DFSAdmin.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1147359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/07/11 10:12 PM",
      "commitName": "c163455df487f99171e5045cdf0c2e1be1c4f99e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 33.3,
      "commitsBetweenForRepo": 111,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,67 @@\n   public void report() throws IOException {\n       DistributedFileSystem dfs \u003d getDFS();\n       FsStatus ds \u003d dfs.getStatus();\n       long capacity \u003d ds.getCapacity();\n       long used \u003d ds.getUsed();\n       long remaining \u003d ds.getRemaining();\n       long presentCapacity \u003d used + remaining;\n       boolean mode \u003d dfs.setSafeMode(FSConstants.SafeModeAction.SAFEMODE_GET);\n       UpgradeStatusReport status \u003d \n                       dfs.distributedUpgradeProgress(UpgradeAction.GET_STATUS);\n \n       if (mode) {\n         System.out.println(\"Safe mode is ON\");\n       }\n       if (status !\u003d null) {\n         System.out.println(status.getStatusText(false));\n       }\n       System.out.println(\"Configured Capacity: \" + capacity\n                          + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n       System.out.println(\"Present Capacity: \" + presentCapacity\n           + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n       System.out.println(\"DFS Remaining: \" + remaining\n           + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n       System.out.println(\"DFS Used: \" + used\n                          + \" (\" + StringUtils.byteDesc(used) + \")\");\n       System.out.println(\"DFS Used%: \"\n                          + StringUtils.limitDecimalTo2(((1.0 * used) / presentCapacity) * 100)\n                          + \"%\");\n       \n       /* These counts are not always upto date. They are updated after  \n        * iteration of an internal list. Should be updated in a few seconds to \n        * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n        * counts.\n        */\n       System.out.println(\"Under replicated blocks: \" + \n                          dfs.getUnderReplicatedBlocksCount());\n       System.out.println(\"Blocks with corrupt replicas: \" + \n                          dfs.getCorruptBlocksCount());\n       System.out.println(\"Missing blocks: \" + \n                          dfs.getMissingBlocksCount());\n                            \n       System.out.println();\n \n       System.out.println(\"-------------------------------------------------\");\n       \n-      DatanodeInfo[] live \u003d dfs.getClient().datanodeReport(\n-                                                   DatanodeReportType.LIVE);\n-      DatanodeInfo[] dead \u003d dfs.getClient().datanodeReport(\n-                                                   DatanodeReportType.DEAD);\n+      DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n+      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n       System.out.println(\"Datanodes available: \" + live.length +\n                          \" (\" + (live.length + dead.length) + \" total, \" + \n                          dead.length + \" dead)\\n\");\n       \n       if(live.length \u003e 0) {\n         System.out.println(\"Live datanodes:\");\n         for (DatanodeInfo dn : live) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }\n       }\n       \n       if(dead.length \u003e 0) {\n         System.out.println(\"Dead datanodes:\");\n         for (DatanodeInfo dn : dead) {\n           System.out.println(dn.getDatanodeReport());\n           System.out.println();\n         }     \n       }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void report() throws IOException {\n      DistributedFileSystem dfs \u003d getDFS();\n      FsStatus ds \u003d dfs.getStatus();\n      long capacity \u003d ds.getCapacity();\n      long used \u003d ds.getUsed();\n      long remaining \u003d ds.getRemaining();\n      long presentCapacity \u003d used + remaining;\n      boolean mode \u003d dfs.setSafeMode(FSConstants.SafeModeAction.SAFEMODE_GET);\n      UpgradeStatusReport status \u003d \n                      dfs.distributedUpgradeProgress(UpgradeAction.GET_STATUS);\n\n      if (mode) {\n        System.out.println(\"Safe mode is ON\");\n      }\n      if (status !\u003d null) {\n        System.out.println(status.getStatusText(false));\n      }\n      System.out.println(\"Configured Capacity: \" + capacity\n                         + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n      System.out.println(\"Present Capacity: \" + presentCapacity\n          + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n      System.out.println(\"DFS Remaining: \" + remaining\n          + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n      System.out.println(\"DFS Used: \" + used\n                         + \" (\" + StringUtils.byteDesc(used) + \")\");\n      System.out.println(\"DFS Used%: \"\n                         + StringUtils.limitDecimalTo2(((1.0 * used) / presentCapacity) * 100)\n                         + \"%\");\n      \n      /* These counts are not always upto date. They are updated after  \n       * iteration of an internal list. Should be updated in a few seconds to \n       * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n       * counts.\n       */\n      System.out.println(\"Under replicated blocks: \" + \n                         dfs.getUnderReplicatedBlocksCount());\n      System.out.println(\"Blocks with corrupt replicas: \" + \n                         dfs.getCorruptBlocksCount());\n      System.out.println(\"Missing blocks: \" + \n                         dfs.getMissingBlocksCount());\n                           \n      System.out.println();\n\n      System.out.println(\"-------------------------------------------------\");\n      \n      DatanodeInfo[] live \u003d dfs.getDataNodeStats(DatanodeReportType.LIVE);\n      DatanodeInfo[] dead \u003d dfs.getDataNodeStats(DatanodeReportType.DEAD);\n      System.out.println(\"Datanodes available: \" + live.length +\n                         \" (\" + (live.length + dead.length) + \" total, \" + \n                         dead.length + \" dead)\\n\");\n      \n      if(live.length \u003e 0) {\n        System.out.println(\"Live datanodes:\");\n        for (DatanodeInfo dn : live) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n      \n      if(dead.length \u003e 0) {\n        System.out.println(\"Dead datanodes:\");\n        for (DatanodeInfo dn : dead) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }     \n      }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,69 @@\n+  public void report() throws IOException {\n+      DistributedFileSystem dfs \u003d getDFS();\n+      FsStatus ds \u003d dfs.getStatus();\n+      long capacity \u003d ds.getCapacity();\n+      long used \u003d ds.getUsed();\n+      long remaining \u003d ds.getRemaining();\n+      long presentCapacity \u003d used + remaining;\n+      boolean mode \u003d dfs.setSafeMode(FSConstants.SafeModeAction.SAFEMODE_GET);\n+      UpgradeStatusReport status \u003d \n+                      dfs.distributedUpgradeProgress(UpgradeAction.GET_STATUS);\n+\n+      if (mode) {\n+        System.out.println(\"Safe mode is ON\");\n+      }\n+      if (status !\u003d null) {\n+        System.out.println(status.getStatusText(false));\n+      }\n+      System.out.println(\"Configured Capacity: \" + capacity\n+                         + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n+      System.out.println(\"Present Capacity: \" + presentCapacity\n+          + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n+      System.out.println(\"DFS Remaining: \" + remaining\n+          + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n+      System.out.println(\"DFS Used: \" + used\n+                         + \" (\" + StringUtils.byteDesc(used) + \")\");\n+      System.out.println(\"DFS Used%: \"\n+                         + StringUtils.limitDecimalTo2(((1.0 * used) / presentCapacity) * 100)\n+                         + \"%\");\n+      \n+      /* These counts are not always upto date. They are updated after  \n+       * iteration of an internal list. Should be updated in a few seconds to \n+       * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n+       * counts.\n+       */\n+      System.out.println(\"Under replicated blocks: \" + \n+                         dfs.getUnderReplicatedBlocksCount());\n+      System.out.println(\"Blocks with corrupt replicas: \" + \n+                         dfs.getCorruptBlocksCount());\n+      System.out.println(\"Missing blocks: \" + \n+                         dfs.getMissingBlocksCount());\n+                           \n+      System.out.println();\n+\n+      System.out.println(\"-------------------------------------------------\");\n+      \n+      DatanodeInfo[] live \u003d dfs.getClient().datanodeReport(\n+                                                   DatanodeReportType.LIVE);\n+      DatanodeInfo[] dead \u003d dfs.getClient().datanodeReport(\n+                                                   DatanodeReportType.DEAD);\n+      System.out.println(\"Datanodes available: \" + live.length +\n+                         \" (\" + (live.length + dead.length) + \" total, \" + \n+                         dead.length + \" dead)\\n\");\n+      \n+      if(live.length \u003e 0) {\n+        System.out.println(\"Live datanodes:\");\n+        for (DatanodeInfo dn : live) {\n+          System.out.println(dn.getDatanodeReport());\n+          System.out.println();\n+        }\n+      }\n+      \n+      if(dead.length \u003e 0) {\n+        System.out.println(\"Dead datanodes:\");\n+        for (DatanodeInfo dn : dead) {\n+          System.out.println(dn.getDatanodeReport());\n+          System.out.println();\n+        }     \n+      }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void report() throws IOException {\n      DistributedFileSystem dfs \u003d getDFS();\n      FsStatus ds \u003d dfs.getStatus();\n      long capacity \u003d ds.getCapacity();\n      long used \u003d ds.getUsed();\n      long remaining \u003d ds.getRemaining();\n      long presentCapacity \u003d used + remaining;\n      boolean mode \u003d dfs.setSafeMode(FSConstants.SafeModeAction.SAFEMODE_GET);\n      UpgradeStatusReport status \u003d \n                      dfs.distributedUpgradeProgress(UpgradeAction.GET_STATUS);\n\n      if (mode) {\n        System.out.println(\"Safe mode is ON\");\n      }\n      if (status !\u003d null) {\n        System.out.println(status.getStatusText(false));\n      }\n      System.out.println(\"Configured Capacity: \" + capacity\n                         + \" (\" + StringUtils.byteDesc(capacity) + \")\");\n      System.out.println(\"Present Capacity: \" + presentCapacity\n          + \" (\" + StringUtils.byteDesc(presentCapacity) + \")\");\n      System.out.println(\"DFS Remaining: \" + remaining\n          + \" (\" + StringUtils.byteDesc(remaining) + \")\");\n      System.out.println(\"DFS Used: \" + used\n                         + \" (\" + StringUtils.byteDesc(used) + \")\");\n      System.out.println(\"DFS Used%: \"\n                         + StringUtils.limitDecimalTo2(((1.0 * used) / presentCapacity) * 100)\n                         + \"%\");\n      \n      /* These counts are not always upto date. They are updated after  \n       * iteration of an internal list. Should be updated in a few seconds to \n       * minutes. Use \"-metaSave\" to list of all such blocks and accurate \n       * counts.\n       */\n      System.out.println(\"Under replicated blocks: \" + \n                         dfs.getUnderReplicatedBlocksCount());\n      System.out.println(\"Blocks with corrupt replicas: \" + \n                         dfs.getCorruptBlocksCount());\n      System.out.println(\"Missing blocks: \" + \n                         dfs.getMissingBlocksCount());\n                           \n      System.out.println();\n\n      System.out.println(\"-------------------------------------------------\");\n      \n      DatanodeInfo[] live \u003d dfs.getClient().datanodeReport(\n                                                   DatanodeReportType.LIVE);\n      DatanodeInfo[] dead \u003d dfs.getClient().datanodeReport(\n                                                   DatanodeReportType.DEAD);\n      System.out.println(\"Datanodes available: \" + live.length +\n                         \" (\" + (live.length + dead.length) + \" total, \" + \n                         dead.length + \" dead)\\n\");\n      \n      if(live.length \u003e 0) {\n        System.out.println(\"Live datanodes:\");\n        for (DatanodeInfo dn : live) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }\n      }\n      \n      if(dead.length \u003e 0) {\n        System.out.println(\"Dead datanodes:\");\n        for (DatanodeInfo dn : dead) {\n          System.out.println(dn.getDatanodeReport());\n          System.out.println();\n        }     \n      }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java"
    }
  }
}