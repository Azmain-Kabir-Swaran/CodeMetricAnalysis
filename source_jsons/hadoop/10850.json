{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "StripedBlockReconstructor.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
  "functionStartLine": 52,
  "functionEndLine": 84,
  "numCommitsSeen": 18,
  "timeTaken": 6576,
  "changeHistory": [
    "d6fc482a541310d83d9cf1393e8f6ed220ef4c1e",
    "c9393dd17f25ee62ceac0c66b96ce26b2789cc8a",
    "1cb2eb0df30d4fbaa090c68022833063f3d225cc",
    "77791e4c36ddc9305306c83806bf486d4d32575d",
    "84d787b9d51196010495d51dc5ebf66c01c340ab",
    "56a13a6a59cb128cf6fdac78a074faf7e5603967",
    "1f14f6d038aecad55a5398c6fa4137c9d2f44729",
    "b5af9be72c72734d668f817c99d889031922a951",
    "d749cf65e1ab0e0daf5be86931507183f189e855",
    "ad9441122f31547fcab29f50e64d52a8895906b6",
    "3c18a53cbd2efabb2ad108d63a0b0b558424115f"
  ],
  "changeHistoryShort": {
    "d6fc482a541310d83d9cf1393e8f6ed220ef4c1e": "Ybodychange",
    "c9393dd17f25ee62ceac0c66b96ce26b2789cc8a": "Ybodychange",
    "1cb2eb0df30d4fbaa090c68022833063f3d225cc": "Ybodychange",
    "77791e4c36ddc9305306c83806bf486d4d32575d": "Ybodychange",
    "84d787b9d51196010495d51dc5ebf66c01c340ab": "Ybodychange",
    "56a13a6a59cb128cf6fdac78a074faf7e5603967": "Ybodychange",
    "1f14f6d038aecad55a5398c6fa4137c9d2f44729": "Ybodychange",
    "b5af9be72c72734d668f817c99d889031922a951": "Ybodychange",
    "d749cf65e1ab0e0daf5be86931507183f189e855": "Ymultichange(Ymovefromfile,Ybodychange)",
    "ad9441122f31547fcab29f50e64d52a8895906b6": "Ybodychange",
    "3c18a53cbd2efabb2ad108d63a0b0b558424115f": "Yintroduced"
  },
  "changeHistoryDetails": {
    "d6fc482a541310d83d9cf1393e8f6ed220ef4c1e": {
      "type": "Ybodychange",
      "commitMessage": "Erasure Coding: metrics xmitsInProgress become to negative. Contributed by maobaolong and Toshihiko Uchida.\n",
      "commitDate": "03/05/20 6:39 AM",
      "commitName": "d6fc482a541310d83d9cf1393e8f6ed220ef4c1e",
      "commitAuthor": "Ayush Saxena",
      "commitDateOld": "24/05/19 10:23 AM",
      "commitNameOld": "c9393dd17f25ee62ceac0c66b96ce26b2789cc8a",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 344.84,
      "commitsBetweenForRepo": 1904,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,33 @@\n   public void run() {\n     try {\n       initDecoderIfNecessary();\n \n       getStripedReader().init();\n \n       stripedWriter.init();\n \n       reconstruct();\n \n       stripedWriter.endTargetBlocks();\n \n       // Currently we don\u0027t check the acks for packets, this is similar as\n       // block replication.\n     } catch (Throwable e) {\n       LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n       getDatanode().getMetrics().incrECFailedReconstructionTasks();\n     } finally {\n-      getDatanode().decrementXmitsInProgress(getXmits());\n+      float xmitWeight \u003d getErasureCodingWorker().getXmitWeight();\n+      // if the xmits is smaller than 1, the xmitsSubmitted should be set to 1\n+      // because if it set to zero, we cannot to measure the xmits submitted\n+      int xmitsSubmitted \u003d Math.max((int) (getXmits() * xmitWeight), 1);\n+      getDatanode().decrementXmitsInProgress(xmitsSubmitted);\n       final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n       metrics.incrECReconstructionTasks();\n       metrics.incrECReconstructionBytesRead(getBytesRead());\n       metrics.incrECReconstructionRemoteBytesRead(getRemoteBytesRead());\n       metrics.incrECReconstructionBytesWritten(getBytesWritten());\n       getStripedReader().close();\n       stripedWriter.close();\n       cleanup();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    try {\n      initDecoderIfNecessary();\n\n      getStripedReader().init();\n\n      stripedWriter.init();\n\n      reconstruct();\n\n      stripedWriter.endTargetBlocks();\n\n      // Currently we don\u0027t check the acks for packets, this is similar as\n      // block replication.\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n      getDatanode().getMetrics().incrECFailedReconstructionTasks();\n    } finally {\n      float xmitWeight \u003d getErasureCodingWorker().getXmitWeight();\n      // if the xmits is smaller than 1, the xmitsSubmitted should be set to 1\n      // because if it set to zero, we cannot to measure the xmits submitted\n      int xmitsSubmitted \u003d Math.max((int) (getXmits() * xmitWeight), 1);\n      getDatanode().decrementXmitsInProgress(xmitsSubmitted);\n      final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n      metrics.incrECReconstructionTasks();\n      metrics.incrECReconstructionBytesRead(getBytesRead());\n      metrics.incrECReconstructionRemoteBytesRead(getRemoteBytesRead());\n      metrics.incrECReconstructionBytesWritten(getBytesWritten());\n      getStripedReader().close();\n      stripedWriter.close();\n      cleanup();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
      "extendedDetails": {}
    },
    "c9393dd17f25ee62ceac0c66b96ce26b2789cc8a": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-14353. Erasure Coding: metrics xmitsInProgress become to negative. Contributed by maobaolong.\"\n\nThis reverts commit 1cb2eb0df30d4fbaa090c68022833063f3d225cc.\n",
      "commitDate": "24/05/19 10:23 AM",
      "commitName": "c9393dd17f25ee62ceac0c66b96ce26b2789cc8a",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "20/05/19 5:22 PM",
      "commitNameOld": "1cb2eb0df30d4fbaa090c68022833063f3d225cc",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 3.71,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,29 @@\n   public void run() {\n     try {\n       initDecoderIfNecessary();\n \n       getStripedReader().init();\n \n       stripedWriter.init();\n \n       reconstruct();\n \n       stripedWriter.endTargetBlocks();\n \n       // Currently we don\u0027t check the acks for packets, this is similar as\n       // block replication.\n     } catch (Throwable e) {\n       LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n       getDatanode().getMetrics().incrECFailedReconstructionTasks();\n     } finally {\n-      float xmitWeight \u003d getErasureCodingWorker().getXmitWeight();\n-      // if the xmits is smaller than 1, the xmitsSubmitted should be set to 1\n-      // because if it set to zero, we cannot to measure the xmits submitted\n-      int xmitsSubmitted \u003d Math.max((int) (getXmits() * xmitWeight), 1);\n-      getDatanode().decrementXmitsInProgress(xmitsSubmitted);\n+      getDatanode().decrementXmitsInProgress(getXmits());\n       final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n       metrics.incrECReconstructionTasks();\n       metrics.incrECReconstructionBytesRead(getBytesRead());\n       metrics.incrECReconstructionRemoteBytesRead(getRemoteBytesRead());\n       metrics.incrECReconstructionBytesWritten(getBytesWritten());\n       getStripedReader().close();\n       stripedWriter.close();\n       cleanup();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    try {\n      initDecoderIfNecessary();\n\n      getStripedReader().init();\n\n      stripedWriter.init();\n\n      reconstruct();\n\n      stripedWriter.endTargetBlocks();\n\n      // Currently we don\u0027t check the acks for packets, this is similar as\n      // block replication.\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n      getDatanode().getMetrics().incrECFailedReconstructionTasks();\n    } finally {\n      getDatanode().decrementXmitsInProgress(getXmits());\n      final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n      metrics.incrECReconstructionTasks();\n      metrics.incrECReconstructionBytesRead(getBytesRead());\n      metrics.incrECReconstructionRemoteBytesRead(getRemoteBytesRead());\n      metrics.incrECReconstructionBytesWritten(getBytesWritten());\n      getStripedReader().close();\n      stripedWriter.close();\n      cleanup();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
      "extendedDetails": {}
    },
    "1cb2eb0df30d4fbaa090c68022833063f3d225cc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14353. Erasure Coding: metrics xmitsInProgress become to negative. Contributed by maobaolong.\n",
      "commitDate": "20/05/19 5:22 PM",
      "commitName": "1cb2eb0df30d4fbaa090c68022833063f3d225cc",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "10/04/18 9:31 PM",
      "commitNameOld": "7c9cdad6d04c98db5a83e2108219bf6e6c903daf",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 404.83,
      "commitsBetweenForRepo": 3532,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,33 @@\n   public void run() {\n     try {\n       initDecoderIfNecessary();\n \n       getStripedReader().init();\n \n       stripedWriter.init();\n \n       reconstruct();\n \n       stripedWriter.endTargetBlocks();\n \n       // Currently we don\u0027t check the acks for packets, this is similar as\n       // block replication.\n     } catch (Throwable e) {\n       LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n       getDatanode().getMetrics().incrECFailedReconstructionTasks();\n     } finally {\n-      getDatanode().decrementXmitsInProgress(getXmits());\n+      float xmitWeight \u003d getErasureCodingWorker().getXmitWeight();\n+      // if the xmits is smaller than 1, the xmitsSubmitted should be set to 1\n+      // because if it set to zero, we cannot to measure the xmits submitted\n+      int xmitsSubmitted \u003d Math.max((int) (getXmits() * xmitWeight), 1);\n+      getDatanode().decrementXmitsInProgress(xmitsSubmitted);\n       final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n       metrics.incrECReconstructionTasks();\n       metrics.incrECReconstructionBytesRead(getBytesRead());\n       metrics.incrECReconstructionRemoteBytesRead(getRemoteBytesRead());\n       metrics.incrECReconstructionBytesWritten(getBytesWritten());\n       getStripedReader().close();\n       stripedWriter.close();\n       cleanup();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    try {\n      initDecoderIfNecessary();\n\n      getStripedReader().init();\n\n      stripedWriter.init();\n\n      reconstruct();\n\n      stripedWriter.endTargetBlocks();\n\n      // Currently we don\u0027t check the acks for packets, this is similar as\n      // block replication.\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n      getDatanode().getMetrics().incrECFailedReconstructionTasks();\n    } finally {\n      float xmitWeight \u003d getErasureCodingWorker().getXmitWeight();\n      // if the xmits is smaller than 1, the xmitsSubmitted should be set to 1\n      // because if it set to zero, we cannot to measure the xmits submitted\n      int xmitsSubmitted \u003d Math.max((int) (getXmits() * xmitWeight), 1);\n      getDatanode().decrementXmitsInProgress(xmitsSubmitted);\n      final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n      metrics.incrECReconstructionTasks();\n      metrics.incrECReconstructionBytesRead(getBytesRead());\n      metrics.incrECReconstructionRemoteBytesRead(getRemoteBytesRead());\n      metrics.incrECReconstructionBytesWritten(getBytesWritten());\n      getStripedReader().close();\n      stripedWriter.close();\n      cleanup();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
      "extendedDetails": {}
    },
    "77791e4c36ddc9305306c83806bf486d4d32575d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12044. Mismatch between BlockManager.maxReplicationStreams and ErasureCodingWorker.stripedReconstructionPool pool size causes slow and bursty recovery. (Contributed by Lei (Eddy) Xu)\n",
      "commitDate": "28/07/17 10:50 AM",
      "commitName": "77791e4c36ddc9305306c83806bf486d4d32575d",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "28/03/17 11:11 PM",
      "commitNameOld": "84d787b9d51196010495d51dc5ebf66c01c340ab",
      "commitAuthorOld": "Rakesh Radhakrishnan",
      "daysBetweenCommits": 121.49,
      "commitsBetweenForRepo": 623,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,29 @@\n   public void run() {\n-    getDatanode().incrementXmitsInProgress();\n     try {\n       initDecoderIfNecessary();\n \n       getStripedReader().init();\n \n       stripedWriter.init();\n \n       reconstruct();\n \n       stripedWriter.endTargetBlocks();\n \n       // Currently we don\u0027t check the acks for packets, this is similar as\n       // block replication.\n     } catch (Throwable e) {\n       LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n       getDatanode().getMetrics().incrECFailedReconstructionTasks();\n     } finally {\n-      getDatanode().decrementXmitsInProgress();\n+      getDatanode().decrementXmitsInProgress(getXmits());\n       final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n       metrics.incrECReconstructionTasks();\n       metrics.incrECReconstructionBytesRead(getBytesRead());\n       metrics.incrECReconstructionRemoteBytesRead(getRemoteBytesRead());\n       metrics.incrECReconstructionBytesWritten(getBytesWritten());\n       getStripedReader().close();\n       stripedWriter.close();\n       cleanup();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    try {\n      initDecoderIfNecessary();\n\n      getStripedReader().init();\n\n      stripedWriter.init();\n\n      reconstruct();\n\n      stripedWriter.endTargetBlocks();\n\n      // Currently we don\u0027t check the acks for packets, this is similar as\n      // block replication.\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n      getDatanode().getMetrics().incrECFailedReconstructionTasks();\n    } finally {\n      getDatanode().decrementXmitsInProgress(getXmits());\n      final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n      metrics.incrECReconstructionTasks();\n      metrics.incrECReconstructionBytesRead(getBytesRead());\n      metrics.incrECReconstructionRemoteBytesRead(getRemoteBytesRead());\n      metrics.incrECReconstructionBytesWritten(getBytesWritten());\n      getStripedReader().close();\n      stripedWriter.close();\n      cleanup();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
      "extendedDetails": {}
    },
    "84d787b9d51196010495d51dc5ebf66c01c340ab": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11541. Call RawErasureEncoder and RawErasureDecoder release() methods. Contributed by SammiChen.\n",
      "commitDate": "28/03/17 11:11 PM",
      "commitName": "84d787b9d51196010495d51dc5ebf66c01c340ab",
      "commitAuthor": "Rakesh Radhakrishnan",
      "commitDateOld": "21/12/16 10:18 PM",
      "commitNameOld": "56a13a6a59cb128cf6fdac78a074faf7e5603967",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 97.0,
      "commitsBetweenForRepo": 502,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,30 @@\n   public void run() {\n     getDatanode().incrementXmitsInProgress();\n     try {\n       initDecoderIfNecessary();\n \n       getStripedReader().init();\n \n       stripedWriter.init();\n \n       reconstruct();\n \n       stripedWriter.endTargetBlocks();\n \n       // Currently we don\u0027t check the acks for packets, this is similar as\n       // block replication.\n     } catch (Throwable e) {\n       LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n       getDatanode().getMetrics().incrECFailedReconstructionTasks();\n     } finally {\n       getDatanode().decrementXmitsInProgress();\n       final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n       metrics.incrECReconstructionTasks();\n       metrics.incrECReconstructionBytesRead(getBytesRead());\n       metrics.incrECReconstructionRemoteBytesRead(getRemoteBytesRead());\n       metrics.incrECReconstructionBytesWritten(getBytesWritten());\n       getStripedReader().close();\n       stripedWriter.close();\n+      cleanup();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    getDatanode().incrementXmitsInProgress();\n    try {\n      initDecoderIfNecessary();\n\n      getStripedReader().init();\n\n      stripedWriter.init();\n\n      reconstruct();\n\n      stripedWriter.endTargetBlocks();\n\n      // Currently we don\u0027t check the acks for packets, this is similar as\n      // block replication.\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n      getDatanode().getMetrics().incrECFailedReconstructionTasks();\n    } finally {\n      getDatanode().decrementXmitsInProgress();\n      final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n      metrics.incrECReconstructionTasks();\n      metrics.incrECReconstructionBytesRead(getBytesRead());\n      metrics.incrECReconstructionRemoteBytesRead(getRemoteBytesRead());\n      metrics.incrECReconstructionBytesWritten(getBytesWritten());\n      getStripedReader().close();\n      stripedWriter.close();\n      cleanup();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
      "extendedDetails": {}
    },
    "56a13a6a59cb128cf6fdac78a074faf7e5603967": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11216. Add remoteBytesRead counter metrics for erasure coding reconstruction task. Contributed by Sammi Chen\n",
      "commitDate": "21/12/16 10:18 PM",
      "commitName": "56a13a6a59cb128cf6fdac78a074faf7e5603967",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "13/12/16 10:50 PM",
      "commitNameOld": "1f14f6d038aecad55a5398c6fa4137c9d2f44729",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 7.98,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,29 @@\n   public void run() {\n     getDatanode().incrementXmitsInProgress();\n     try {\n       initDecoderIfNecessary();\n \n       getStripedReader().init();\n \n       stripedWriter.init();\n \n       reconstruct();\n \n       stripedWriter.endTargetBlocks();\n \n       // Currently we don\u0027t check the acks for packets, this is similar as\n       // block replication.\n     } catch (Throwable e) {\n       LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n       getDatanode().getMetrics().incrECFailedReconstructionTasks();\n     } finally {\n       getDatanode().decrementXmitsInProgress();\n       final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n       metrics.incrECReconstructionTasks();\n       metrics.incrECReconstructionBytesRead(getBytesRead());\n+      metrics.incrECReconstructionRemoteBytesRead(getRemoteBytesRead());\n       metrics.incrECReconstructionBytesWritten(getBytesWritten());\n       getStripedReader().close();\n       stripedWriter.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    getDatanode().incrementXmitsInProgress();\n    try {\n      initDecoderIfNecessary();\n\n      getStripedReader().init();\n\n      stripedWriter.init();\n\n      reconstruct();\n\n      stripedWriter.endTargetBlocks();\n\n      // Currently we don\u0027t check the acks for packets, this is similar as\n      // block replication.\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n      getDatanode().getMetrics().incrECFailedReconstructionTasks();\n    } finally {\n      getDatanode().decrementXmitsInProgress();\n      final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n      metrics.incrECReconstructionTasks();\n      metrics.incrECReconstructionBytesRead(getBytesRead());\n      metrics.incrECReconstructionRemoteBytesRead(getRemoteBytesRead());\n      metrics.incrECReconstructionBytesWritten(getBytesWritten());\n      getStripedReader().close();\n      stripedWriter.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
      "extendedDetails": {}
    },
    "1f14f6d038aecad55a5398c6fa4137c9d2f44729": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8411. Add bytes count metrics to datanode for ECWorker. Contributed by Sammi Chen and Andrew Wang\n",
      "commitDate": "13/12/16 10:50 PM",
      "commitName": "1f14f6d038aecad55a5398c6fa4137c9d2f44729",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "21/10/16 1:12 PM",
      "commitNameOld": "61e30cf83ca78529603d9b4c6732418da7e4d0c8",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 53.44,
      "commitsBetweenForRepo": 424,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,28 @@\n   public void run() {\n     getDatanode().incrementXmitsInProgress();\n     try {\n       initDecoderIfNecessary();\n \n       getStripedReader().init();\n \n       stripedWriter.init();\n \n       reconstruct();\n \n       stripedWriter.endTargetBlocks();\n \n       // Currently we don\u0027t check the acks for packets, this is similar as\n       // block replication.\n     } catch (Throwable e) {\n       LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n       getDatanode().getMetrics().incrECFailedReconstructionTasks();\n     } finally {\n       getDatanode().decrementXmitsInProgress();\n-      getDatanode().getMetrics().incrECReconstructionTasks();\n+      final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n+      metrics.incrECReconstructionTasks();\n+      metrics.incrECReconstructionBytesRead(getBytesRead());\n+      metrics.incrECReconstructionBytesWritten(getBytesWritten());\n       getStripedReader().close();\n       stripedWriter.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    getDatanode().incrementXmitsInProgress();\n    try {\n      initDecoderIfNecessary();\n\n      getStripedReader().init();\n\n      stripedWriter.init();\n\n      reconstruct();\n\n      stripedWriter.endTargetBlocks();\n\n      // Currently we don\u0027t check the acks for packets, this is similar as\n      // block replication.\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n      getDatanode().getMetrics().incrECFailedReconstructionTasks();\n    } finally {\n      getDatanode().decrementXmitsInProgress();\n      final DataNodeMetrics metrics \u003d getDatanode().getMetrics();\n      metrics.incrECReconstructionTasks();\n      metrics.incrECReconstructionBytesRead(getBytesRead());\n      metrics.incrECReconstructionBytesWritten(getBytesWritten());\n      getStripedReader().close();\n      stripedWriter.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
      "extendedDetails": {}
    },
    "b5af9be72c72734d668f817c99d889031922a951": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8668. Erasure Coding: revisit buffer used for encoding and decoding. Contributed by Sammi Chen\n",
      "commitDate": "12/08/16 10:52 PM",
      "commitName": "b5af9be72c72734d668f817c99d889031922a951",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "01/06/16 9:56 PM",
      "commitNameOld": "d749cf65e1ab0e0daf5be86931507183f189e855",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 72.04,
      "commitsBetweenForRepo": 616,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,25 @@\n   public void run() {\n     getDatanode().incrementXmitsInProgress();\n     try {\n+      initDecoderIfNecessary();\n+\n       getStripedReader().init();\n \n       stripedWriter.init();\n \n       reconstruct();\n \n       stripedWriter.endTargetBlocks();\n \n       // Currently we don\u0027t check the acks for packets, this is similar as\n       // block replication.\n     } catch (Throwable e) {\n       LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n       getDatanode().getMetrics().incrECFailedReconstructionTasks();\n     } finally {\n       getDatanode().decrementXmitsInProgress();\n       getDatanode().getMetrics().incrECReconstructionTasks();\n       getStripedReader().close();\n       stripedWriter.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    getDatanode().incrementXmitsInProgress();\n    try {\n      initDecoderIfNecessary();\n\n      getStripedReader().init();\n\n      stripedWriter.init();\n\n      reconstruct();\n\n      stripedWriter.endTargetBlocks();\n\n      // Currently we don\u0027t check the acks for packets, this is similar as\n      // block replication.\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n      getDatanode().getMetrics().incrECFailedReconstructionTasks();\n    } finally {\n      getDatanode().decrementXmitsInProgress();\n      getDatanode().getMetrics().incrECReconstructionTasks();\n      getStripedReader().close();\n      stripedWriter.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
      "extendedDetails": {}
    },
    "d749cf65e1ab0e0daf5be86931507183f189e855": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-9833. Erasure coding: recomputing block checksum on the fly by reconstructing the missed/corrupt block data. Contributed by Rakesh R.\n",
      "commitDate": "01/06/16 9:56 PM",
      "commitName": "d749cf65e1ab0e0daf5be86931507183f189e855",
      "commitAuthor": "Kai Zheng",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-9833. Erasure coding: recomputing block checksum on the fly by reconstructing the missed/corrupt block data. Contributed by Rakesh R.\n",
          "commitDate": "01/06/16 9:56 PM",
          "commitName": "d749cf65e1ab0e0daf5be86931507183f189e855",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "31/05/16 5:54 PM",
          "commitNameOld": "8ceb06e2392763726210f96bb1c176e6a9fe7b53",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 1.17,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n   public void run() {\n-    datanode.incrementXmitsInProgress();\n+    getDatanode().incrementXmitsInProgress();\n     try {\n-      stripedReader.init();\n+      getStripedReader().init();\n \n       stripedWriter.init();\n \n-      reconstructAndTransfer();\n+      reconstruct();\n \n       stripedWriter.endTargetBlocks();\n \n       // Currently we don\u0027t check the acks for packets, this is similar as\n       // block replication.\n     } catch (Throwable e) {\n-      LOG.warn(\"Failed to reconstruct striped block: {}\", blockGroup, e);\n-      datanode.getMetrics().incrECFailedReconstructionTasks();\n+      LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n+      getDatanode().getMetrics().incrECFailedReconstructionTasks();\n     } finally {\n-      datanode.decrementXmitsInProgress();\n-      datanode.getMetrics().incrECReconstructionTasks();\n-      stripedReader.close();\n+      getDatanode().decrementXmitsInProgress();\n+      getDatanode().getMetrics().incrECReconstructionTasks();\n+      getStripedReader().close();\n       stripedWriter.close();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void run() {\n    getDatanode().incrementXmitsInProgress();\n    try {\n      getStripedReader().init();\n\n      stripedWriter.init();\n\n      reconstruct();\n\n      stripedWriter.endTargetBlocks();\n\n      // Currently we don\u0027t check the acks for packets, this is similar as\n      // block replication.\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n      getDatanode().getMetrics().incrECFailedReconstructionTasks();\n    } finally {\n      getDatanode().decrementXmitsInProgress();\n      getDatanode().getMetrics().incrECReconstructionTasks();\n      getStripedReader().close();\n      stripedWriter.close();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructor.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
            "oldMethodName": "run",
            "newMethodName": "run"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9833. Erasure coding: recomputing block checksum on the fly by reconstructing the missed/corrupt block data. Contributed by Rakesh R.\n",
          "commitDate": "01/06/16 9:56 PM",
          "commitName": "d749cf65e1ab0e0daf5be86931507183f189e855",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "31/05/16 5:54 PM",
          "commitNameOld": "8ceb06e2392763726210f96bb1c176e6a9fe7b53",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 1.17,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n   public void run() {\n-    datanode.incrementXmitsInProgress();\n+    getDatanode().incrementXmitsInProgress();\n     try {\n-      stripedReader.init();\n+      getStripedReader().init();\n \n       stripedWriter.init();\n \n-      reconstructAndTransfer();\n+      reconstruct();\n \n       stripedWriter.endTargetBlocks();\n \n       // Currently we don\u0027t check the acks for packets, this is similar as\n       // block replication.\n     } catch (Throwable e) {\n-      LOG.warn(\"Failed to reconstruct striped block: {}\", blockGroup, e);\n-      datanode.getMetrics().incrECFailedReconstructionTasks();\n+      LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n+      getDatanode().getMetrics().incrECFailedReconstructionTasks();\n     } finally {\n-      datanode.decrementXmitsInProgress();\n-      datanode.getMetrics().incrECReconstructionTasks();\n-      stripedReader.close();\n+      getDatanode().decrementXmitsInProgress();\n+      getDatanode().getMetrics().incrECReconstructionTasks();\n+      getStripedReader().close();\n       stripedWriter.close();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void run() {\n    getDatanode().incrementXmitsInProgress();\n    try {\n      getStripedReader().init();\n\n      stripedWriter.init();\n\n      reconstruct();\n\n      stripedWriter.endTargetBlocks();\n\n      // Currently we don\u0027t check the acks for packets, this is similar as\n      // block replication.\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to reconstruct striped block: {}\", getBlockGroup(), e);\n      getDatanode().getMetrics().incrECFailedReconstructionTasks();\n    } finally {\n      getDatanode().decrementXmitsInProgress();\n      getDatanode().getMetrics().incrECReconstructionTasks();\n      getStripedReader().close();\n      stripedWriter.close();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockReconstructor.java",
          "extendedDetails": {}
        }
      ]
    },
    "ad9441122f31547fcab29f50e64d52a8895906b6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8449. Add tasks count metrics to datanode for ECWorker. Contributed by Bo Li.\n",
      "commitDate": "15/05/16 11:39 PM",
      "commitName": "ad9441122f31547fcab29f50e64d52a8895906b6",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "20/04/16 12:47 AM",
      "commitNameOld": "b5d4c7dc76ddb3e0af95d792c2cbc0f99353a42a",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 25.95,
      "commitsBetweenForRepo": 158,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public void run() {\n     datanode.incrementXmitsInProgress();\n     try {\n       stripedReader.init();\n \n       stripedWriter.init();\n \n       reconstructAndTransfer();\n \n       stripedWriter.endTargetBlocks();\n \n       // Currently we don\u0027t check the acks for packets, this is similar as\n       // block replication.\n     } catch (Throwable e) {\n       LOG.warn(\"Failed to reconstruct striped block: {}\", blockGroup, e);\n+      datanode.getMetrics().incrECFailedReconstructionTasks();\n     } finally {\n       datanode.decrementXmitsInProgress();\n-\n+      datanode.getMetrics().incrECReconstructionTasks();\n       stripedReader.close();\n-\n       stripedWriter.close();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    datanode.incrementXmitsInProgress();\n    try {\n      stripedReader.init();\n\n      stripedWriter.init();\n\n      reconstructAndTransfer();\n\n      stripedWriter.endTargetBlocks();\n\n      // Currently we don\u0027t check the acks for packets, this is similar as\n      // block replication.\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to reconstruct striped block: {}\", blockGroup, e);\n      datanode.getMetrics().incrECFailedReconstructionTasks();\n    } finally {\n      datanode.decrementXmitsInProgress();\n      datanode.getMetrics().incrECReconstructionTasks();\n      stripedReader.close();\n      stripedWriter.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructor.java",
      "extendedDetails": {}
    },
    "3c18a53cbd2efabb2ad108d63a0b0b558424115f": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9719. Refactoring ErasureCodingWorker into smaller reusable constructs. Contributed by Kai Zheng.\n",
      "commitDate": "06/04/16 10:50 PM",
      "commitName": "3c18a53cbd2efabb2ad108d63a0b0b558424115f",
      "commitAuthor": "Uma Maheswara Rao G",
      "diff": "@@ -0,0 +1,23 @@\n+  public void run() {\n+    datanode.incrementXmitsInProgress();\n+    try {\n+      stripedReader.init();\n+\n+      stripedWriter.init();\n+\n+      reconstructAndTransfer();\n+\n+      stripedWriter.endTargetBlocks();\n+\n+      // Currently we don\u0027t check the acks for packets, this is similar as\n+      // block replication.\n+    } catch (Throwable e) {\n+      LOG.warn(\"Failed to reconstruct striped block: {}\", blockGroup, e);\n+    } finally {\n+      datanode.decrementXmitsInProgress();\n+\n+      stripedReader.close();\n+\n+      stripedWriter.close();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    datanode.incrementXmitsInProgress();\n    try {\n      stripedReader.init();\n\n      stripedWriter.init();\n\n      reconstructAndTransfer();\n\n      stripedWriter.endTargetBlocks();\n\n      // Currently we don\u0027t check the acks for packets, this is similar as\n      // block replication.\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to reconstruct striped block: {}\", blockGroup, e);\n    } finally {\n      datanode.decrementXmitsInProgress();\n\n      stripedReader.close();\n\n      stripedWriter.close();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructor.java"
    }
  }
}