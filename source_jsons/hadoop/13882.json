{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "removeStoredBlock",
  "functionId": "removeStoredBlock___storedBlock-BlockInfo__node-DatanodeDescriptor",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 4050,
  "functionEndLine": 4087,
  "numCommitsSeen": 477,
  "timeTaken": 10160,
  "changeHistory": [
    "5865fe2bf01284993572ea60b3ec3bf8b4492818",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
    "6979cbfc1f4c28440816b56f5624765872b0be49",
    "972782d9568e0849484c027f27c1638ba50ec56e",
    "a49cc74b4c72195dee1dfb6f9548e5e411dff553",
    "73b86a5046fe3262dde7b05be46b18575e35fd5f"
  ],
  "changeHistoryShort": {
    "5865fe2bf01284993572ea60b3ec3bf8b4492818": "Ybodychange",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": "Ybodychange",
    "6979cbfc1f4c28440816b56f5624765872b0be49": "Ybodychange",
    "972782d9568e0849484c027f27c1638ba50ec56e": "Ybodychange",
    "a49cc74b4c72195dee1dfb6f9548e5e411dff553": "Ybodychange",
    "73b86a5046fe3262dde7b05be46b18575e35fd5f": "Ybodychange"
  },
  "changeHistoryDetails": {
    "5865fe2bf01284993572ea60b3ec3bf8b4492818": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9869. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-2]. Contributed by Rakesh R.\n",
      "commitDate": "25/04/16 10:01 PM",
      "commitName": "5865fe2bf01284993572ea60b3ec3bf8b4492818",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "17/04/16 6:28 PM",
      "commitNameOld": "67523ffcf491f4f2db5335899c00a174d0caaa9b",
      "commitAuthorOld": "Walter Su",
      "daysBetweenCommits": 8.15,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   public void removeStoredBlock(BlockInfo storedBlock, DatanodeDescriptor node) {\n     blockLog.debug(\"BLOCK* removeStoredBlock: {} from {}\", storedBlock, node);\n     assert (namesystem.hasWriteLock());\n     {\n       if (storedBlock \u003d\u003d null || !blocksMap.removeNode(storedBlock, node)) {\n         blockLog.debug(\"BLOCK* removeStoredBlock: {} has already been\" +\n             \" removed from node {}\", storedBlock, node);\n         return;\n       }\n \n       CachedBlock cblock \u003d namesystem.getCacheManager().getCachedBlocks()\n           .get(new CachedBlock(storedBlock.getBlockId(), (short) 0, false));\n       if (cblock !\u003d null) {\n         boolean removed \u003d false;\n         removed |\u003d node.getPendingCached().remove(cblock);\n         removed |\u003d node.getCached().remove(cblock);\n         removed |\u003d node.getPendingUncached().remove(cblock);\n         if (removed) {\n           blockLog.debug(\"BLOCK* removeStoredBlock: {} removed from caching \"\n               + \"related lists on node {}\", storedBlock, node);\n         }\n       }\n \n       //\n       // It\u0027s possible that the block was removed because of a datanode\n       // failure. If the block is still valid, check if replication is\n       // necessary. In that case, put block on a possibly-will-\n       // be-replicated list.\n       //\n       if (!storedBlock.isDeleted()) {\n         bmSafeMode.decrementSafeBlockCount(storedBlock);\n         updateNeededReconstructions(storedBlock, -1, 0);\n       }\n \n-      excessReplicas.remove(node, storedBlock);\n+      excessRedundancyMap.remove(node, storedBlock);\n       corruptReplicas.removeFromCorruptReplicasMap(storedBlock, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeStoredBlock(BlockInfo storedBlock, DatanodeDescriptor node) {\n    blockLog.debug(\"BLOCK* removeStoredBlock: {} from {}\", storedBlock, node);\n    assert (namesystem.hasWriteLock());\n    {\n      if (storedBlock \u003d\u003d null || !blocksMap.removeNode(storedBlock, node)) {\n        blockLog.debug(\"BLOCK* removeStoredBlock: {} has already been\" +\n            \" removed from node {}\", storedBlock, node);\n        return;\n      }\n\n      CachedBlock cblock \u003d namesystem.getCacheManager().getCachedBlocks()\n          .get(new CachedBlock(storedBlock.getBlockId(), (short) 0, false));\n      if (cblock !\u003d null) {\n        boolean removed \u003d false;\n        removed |\u003d node.getPendingCached().remove(cblock);\n        removed |\u003d node.getCached().remove(cblock);\n        removed |\u003d node.getPendingUncached().remove(cblock);\n        if (removed) {\n          blockLog.debug(\"BLOCK* removeStoredBlock: {} removed from caching \"\n              + \"related lists on node {}\", storedBlock, node);\n        }\n      }\n\n      //\n      // It\u0027s possible that the block was removed because of a datanode\n      // failure. If the block is still valid, check if replication is\n      // necessary. In that case, put block on a possibly-will-\n      // be-replicated list.\n      //\n      if (!storedBlock.isDeleted()) {\n        bmSafeMode.decrementSafeBlockCount(storedBlock);\n        updateNeededReconstructions(storedBlock, -1, 0);\n      }\n\n      excessRedundancyMap.remove(node, storedBlock);\n      corruptReplicas.removeFromCorruptReplicasMap(storedBlock, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
      "commitDate": "16/03/16 4:53 PM",
      "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "10/03/16 7:03 PM",
      "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 5.87,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,38 @@\n   public void removeStoredBlock(BlockInfo storedBlock, DatanodeDescriptor node) {\n     blockLog.debug(\"BLOCK* removeStoredBlock: {} from {}\", storedBlock, node);\n     assert (namesystem.hasWriteLock());\n     {\n       if (storedBlock \u003d\u003d null || !blocksMap.removeNode(storedBlock, node)) {\n         blockLog.debug(\"BLOCK* removeStoredBlock: {} has already been\" +\n             \" removed from node {}\", storedBlock, node);\n         return;\n       }\n \n       CachedBlock cblock \u003d namesystem.getCacheManager().getCachedBlocks()\n           .get(new CachedBlock(storedBlock.getBlockId(), (short) 0, false));\n       if (cblock !\u003d null) {\n         boolean removed \u003d false;\n         removed |\u003d node.getPendingCached().remove(cblock);\n         removed |\u003d node.getCached().remove(cblock);\n         removed |\u003d node.getPendingUncached().remove(cblock);\n         if (removed) {\n           blockLog.debug(\"BLOCK* removeStoredBlock: {} removed from caching \"\n               + \"related lists on node {}\", storedBlock, node);\n         }\n       }\n \n       //\n       // It\u0027s possible that the block was removed because of a datanode\n       // failure. If the block is still valid, check if replication is\n       // necessary. In that case, put block on a possibly-will-\n       // be-replicated list.\n       //\n       if (!storedBlock.isDeleted()) {\n         bmSafeMode.decrementSafeBlockCount(storedBlock);\n-        updateNeededReplications(storedBlock, -1, 0);\n+        updateNeededReconstructions(storedBlock, -1, 0);\n       }\n \n       excessReplicas.remove(node, storedBlock);\n       corruptReplicas.removeFromCorruptReplicasMap(storedBlock, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeStoredBlock(BlockInfo storedBlock, DatanodeDescriptor node) {\n    blockLog.debug(\"BLOCK* removeStoredBlock: {} from {}\", storedBlock, node);\n    assert (namesystem.hasWriteLock());\n    {\n      if (storedBlock \u003d\u003d null || !blocksMap.removeNode(storedBlock, node)) {\n        blockLog.debug(\"BLOCK* removeStoredBlock: {} has already been\" +\n            \" removed from node {}\", storedBlock, node);\n        return;\n      }\n\n      CachedBlock cblock \u003d namesystem.getCacheManager().getCachedBlocks()\n          .get(new CachedBlock(storedBlock.getBlockId(), (short) 0, false));\n      if (cblock !\u003d null) {\n        boolean removed \u003d false;\n        removed |\u003d node.getPendingCached().remove(cblock);\n        removed |\u003d node.getCached().remove(cblock);\n        removed |\u003d node.getPendingUncached().remove(cblock);\n        if (removed) {\n          blockLog.debug(\"BLOCK* removeStoredBlock: {} removed from caching \"\n              + \"related lists on node {}\", storedBlock, node);\n        }\n      }\n\n      //\n      // It\u0027s possible that the block was removed because of a datanode\n      // failure. If the block is still valid, check if replication is\n      // necessary. In that case, put block on a possibly-will-\n      // be-replicated list.\n      //\n      if (!storedBlock.isDeleted()) {\n        bmSafeMode.decrementSafeBlockCount(storedBlock);\n        updateNeededReconstructions(storedBlock, -1, 0);\n      }\n\n      excessReplicas.remove(node, storedBlock);\n      corruptReplicas.removeFromCorruptReplicasMap(storedBlock, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "6979cbfc1f4c28440816b56f5624765872b0be49": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9838. Refactor the excessReplicateMap to a class.\n",
      "commitDate": "24/02/16 7:42 PM",
      "commitName": "6979cbfc1f4c28440816b56f5624765872b0be49",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "24/02/16 3:13 PM",
      "commitNameOld": "47b92f2b6f2dafc129a41b247f35e77c8e47ffba",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.19,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,38 @@\n   public void removeStoredBlock(BlockInfo storedBlock, DatanodeDescriptor node) {\n     blockLog.debug(\"BLOCK* removeStoredBlock: {} from {}\", storedBlock, node);\n     assert (namesystem.hasWriteLock());\n     {\n       if (storedBlock \u003d\u003d null || !blocksMap.removeNode(storedBlock, node)) {\n         blockLog.debug(\"BLOCK* removeStoredBlock: {} has already been\" +\n             \" removed from node {}\", storedBlock, node);\n         return;\n       }\n \n       CachedBlock cblock \u003d namesystem.getCacheManager().getCachedBlocks()\n           .get(new CachedBlock(storedBlock.getBlockId(), (short) 0, false));\n       if (cblock !\u003d null) {\n         boolean removed \u003d false;\n         removed |\u003d node.getPendingCached().remove(cblock);\n         removed |\u003d node.getCached().remove(cblock);\n         removed |\u003d node.getPendingUncached().remove(cblock);\n         if (removed) {\n           blockLog.debug(\"BLOCK* removeStoredBlock: {} removed from caching \"\n               + \"related lists on node {}\", storedBlock, node);\n         }\n       }\n \n       //\n       // It\u0027s possible that the block was removed because of a datanode\n       // failure. If the block is still valid, check if replication is\n       // necessary. In that case, put block on a possibly-will-\n       // be-replicated list.\n       //\n       if (!storedBlock.isDeleted()) {\n         bmSafeMode.decrementSafeBlockCount(storedBlock);\n         updateNeededReplications(storedBlock, -1, 0);\n       }\n \n-      //\n-      // We\u0027ve removed a block from a node, so it\u0027s definitely no longer\n-      // in \"excess\" there.\n-      //\n-      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n-          node.getDatanodeUuid());\n-      if (excessBlocks !\u003d null) {\n-        if (excessBlocks.remove(storedBlock)) {\n-          excessBlocksCount.decrementAndGet();\n-          blockLog.debug(\"BLOCK* removeStoredBlock: {} is removed from \" +\n-              \"excessBlocks\", storedBlock);\n-          if (excessBlocks.size() \u003d\u003d 0) {\n-            excessReplicateMap.remove(node.getDatanodeUuid());\n-          }\n-        }\n-      }\n-\n-      // Remove the replica from corruptReplicas\n+      excessReplicas.remove(node, storedBlock);\n       corruptReplicas.removeFromCorruptReplicasMap(storedBlock, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeStoredBlock(BlockInfo storedBlock, DatanodeDescriptor node) {\n    blockLog.debug(\"BLOCK* removeStoredBlock: {} from {}\", storedBlock, node);\n    assert (namesystem.hasWriteLock());\n    {\n      if (storedBlock \u003d\u003d null || !blocksMap.removeNode(storedBlock, node)) {\n        blockLog.debug(\"BLOCK* removeStoredBlock: {} has already been\" +\n            \" removed from node {}\", storedBlock, node);\n        return;\n      }\n\n      CachedBlock cblock \u003d namesystem.getCacheManager().getCachedBlocks()\n          .get(new CachedBlock(storedBlock.getBlockId(), (short) 0, false));\n      if (cblock !\u003d null) {\n        boolean removed \u003d false;\n        removed |\u003d node.getPendingCached().remove(cblock);\n        removed |\u003d node.getCached().remove(cblock);\n        removed |\u003d node.getPendingUncached().remove(cblock);\n        if (removed) {\n          blockLog.debug(\"BLOCK* removeStoredBlock: {} removed from caching \"\n              + \"related lists on node {}\", storedBlock, node);\n        }\n      }\n\n      //\n      // It\u0027s possible that the block was removed because of a datanode\n      // failure. If the block is still valid, check if replication is\n      // necessary. In that case, put block on a possibly-will-\n      // be-replicated list.\n      //\n      if (!storedBlock.isDeleted()) {\n        bmSafeMode.decrementSafeBlockCount(storedBlock);\n        updateNeededReplications(storedBlock, -1, 0);\n      }\n\n      excessReplicas.remove(node, storedBlock);\n      corruptReplicas.removeFromCorruptReplicasMap(storedBlock, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "972782d9568e0849484c027f27c1638ba50ec56e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9754. Avoid unnecessary getBlockCollection calls in BlockManager. Contributed by Jing Zhao.\n",
      "commitDate": "12/02/16 11:07 AM",
      "commitName": "972782d9568e0849484c027f27c1638ba50ec56e",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "10/02/16 9:24 PM",
      "commitNameOld": "19adb2bc641999b83e25ff0e107ba8c6edbad399",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.57,
      "commitsBetweenForRepo": 17,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,55 @@\n   public void removeStoredBlock(BlockInfo storedBlock, DatanodeDescriptor node) {\n     blockLog.debug(\"BLOCK* removeStoredBlock: {} from {}\", storedBlock, node);\n     assert (namesystem.hasWriteLock());\n     {\n       if (storedBlock \u003d\u003d null || !blocksMap.removeNode(storedBlock, node)) {\n         blockLog.debug(\"BLOCK* removeStoredBlock: {} has already been\" +\n             \" removed from node {}\", storedBlock, node);\n         return;\n       }\n \n       CachedBlock cblock \u003d namesystem.getCacheManager().getCachedBlocks()\n           .get(new CachedBlock(storedBlock.getBlockId(), (short) 0, false));\n       if (cblock !\u003d null) {\n         boolean removed \u003d false;\n         removed |\u003d node.getPendingCached().remove(cblock);\n         removed |\u003d node.getCached().remove(cblock);\n         removed |\u003d node.getPendingUncached().remove(cblock);\n         if (removed) {\n           blockLog.debug(\"BLOCK* removeStoredBlock: {} removed from caching \"\n               + \"related lists on node {}\", storedBlock, node);\n         }\n       }\n \n       //\n       // It\u0027s possible that the block was removed because of a datanode\n       // failure. If the block is still valid, check if replication is\n       // necessary. In that case, put block on a possibly-will-\n       // be-replicated list.\n       //\n-      BlockCollection bc \u003d getBlockCollection(storedBlock);\n-      if (bc !\u003d null) {\n+      if (!storedBlock.isDeleted()) {\n         bmSafeMode.decrementSafeBlockCount(storedBlock);\n         updateNeededReplications(storedBlock, -1, 0);\n       }\n \n       //\n       // We\u0027ve removed a block from a node, so it\u0027s definitely no longer\n       // in \"excess\" there.\n       //\n       LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n           node.getDatanodeUuid());\n       if (excessBlocks !\u003d null) {\n         if (excessBlocks.remove(storedBlock)) {\n           excessBlocksCount.decrementAndGet();\n           blockLog.debug(\"BLOCK* removeStoredBlock: {} is removed from \" +\n               \"excessBlocks\", storedBlock);\n           if (excessBlocks.size() \u003d\u003d 0) {\n             excessReplicateMap.remove(node.getDatanodeUuid());\n           }\n         }\n       }\n \n       // Remove the replica from corruptReplicas\n       corruptReplicas.removeFromCorruptReplicasMap(storedBlock, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeStoredBlock(BlockInfo storedBlock, DatanodeDescriptor node) {\n    blockLog.debug(\"BLOCK* removeStoredBlock: {} from {}\", storedBlock, node);\n    assert (namesystem.hasWriteLock());\n    {\n      if (storedBlock \u003d\u003d null || !blocksMap.removeNode(storedBlock, node)) {\n        blockLog.debug(\"BLOCK* removeStoredBlock: {} has already been\" +\n            \" removed from node {}\", storedBlock, node);\n        return;\n      }\n\n      CachedBlock cblock \u003d namesystem.getCacheManager().getCachedBlocks()\n          .get(new CachedBlock(storedBlock.getBlockId(), (short) 0, false));\n      if (cblock !\u003d null) {\n        boolean removed \u003d false;\n        removed |\u003d node.getPendingCached().remove(cblock);\n        removed |\u003d node.getCached().remove(cblock);\n        removed |\u003d node.getPendingUncached().remove(cblock);\n        if (removed) {\n          blockLog.debug(\"BLOCK* removeStoredBlock: {} removed from caching \"\n              + \"related lists on node {}\", storedBlock, node);\n        }\n      }\n\n      //\n      // It\u0027s possible that the block was removed because of a datanode\n      // failure. If the block is still valid, check if replication is\n      // necessary. In that case, put block on a possibly-will-\n      // be-replicated list.\n      //\n      if (!storedBlock.isDeleted()) {\n        bmSafeMode.decrementSafeBlockCount(storedBlock);\n        updateNeededReplications(storedBlock, -1, 0);\n      }\n\n      //\n      // We\u0027ve removed a block from a node, so it\u0027s definitely no longer\n      // in \"excess\" there.\n      //\n      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n          node.getDatanodeUuid());\n      if (excessBlocks !\u003d null) {\n        if (excessBlocks.remove(storedBlock)) {\n          excessBlocksCount.decrementAndGet();\n          blockLog.debug(\"BLOCK* removeStoredBlock: {} is removed from \" +\n              \"excessBlocks\", storedBlock);\n          if (excessBlocks.size() \u003d\u003d 0) {\n            excessReplicateMap.remove(node.getDatanodeUuid());\n          }\n        }\n      }\n\n      // Remove the replica from corruptReplicas\n      corruptReplicas.removeFromCorruptReplicasMap(storedBlock, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "a49cc74b4c72195dee1dfb6f9548e5e411dff553": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9129. Move the safemode block count into BlockManager. Contributed by Mingliang Liu.\n",
      "commitDate": "01/12/15 4:09 PM",
      "commitName": "a49cc74b4c72195dee1dfb6f9548e5e411dff553",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "01/12/15 1:05 PM",
      "commitNameOld": "830eb252aaa4fec7ef2ec38cb66f669e8e1ecaa5",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.13,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n   public void removeStoredBlock(BlockInfo storedBlock, DatanodeDescriptor node) {\n     blockLog.debug(\"BLOCK* removeStoredBlock: {} from {}\", storedBlock, node);\n     assert (namesystem.hasWriteLock());\n     {\n       if (storedBlock \u003d\u003d null || !blocksMap.removeNode(storedBlock, node)) {\n         blockLog.debug(\"BLOCK* removeStoredBlock: {} has already been\" +\n             \" removed from node {}\", storedBlock, node);\n         return;\n       }\n \n       CachedBlock cblock \u003d namesystem.getCacheManager().getCachedBlocks()\n           .get(new CachedBlock(storedBlock.getBlockId(), (short) 0, false));\n       if (cblock !\u003d null) {\n         boolean removed \u003d false;\n         removed |\u003d node.getPendingCached().remove(cblock);\n         removed |\u003d node.getCached().remove(cblock);\n         removed |\u003d node.getPendingUncached().remove(cblock);\n         if (removed) {\n           blockLog.debug(\"BLOCK* removeStoredBlock: {} removed from caching \"\n               + \"related lists on node {}\", storedBlock, node);\n         }\n       }\n \n       //\n       // It\u0027s possible that the block was removed because of a datanode\n       // failure. If the block is still valid, check if replication is\n       // necessary. In that case, put block on a possibly-will-\n       // be-replicated list.\n       //\n       BlockCollection bc \u003d getBlockCollection(storedBlock);\n       if (bc !\u003d null) {\n-        namesystem.decrementSafeBlockCount(storedBlock);\n+        bmSafeMode.decrementSafeBlockCount(storedBlock);\n         updateNeededReplications(storedBlock, -1, 0);\n       }\n \n       //\n       // We\u0027ve removed a block from a node, so it\u0027s definitely no longer\n       // in \"excess\" there.\n       //\n       LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n           node.getDatanodeUuid());\n       if (excessBlocks !\u003d null) {\n         if (excessBlocks.remove(storedBlock)) {\n           excessBlocksCount.decrementAndGet();\n           blockLog.debug(\"BLOCK* removeStoredBlock: {} is removed from \" +\n               \"excessBlocks\", storedBlock);\n           if (excessBlocks.size() \u003d\u003d 0) {\n             excessReplicateMap.remove(node.getDatanodeUuid());\n           }\n         }\n       }\n \n       // Remove the replica from corruptReplicas\n       corruptReplicas.removeFromCorruptReplicasMap(storedBlock, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeStoredBlock(BlockInfo storedBlock, DatanodeDescriptor node) {\n    blockLog.debug(\"BLOCK* removeStoredBlock: {} from {}\", storedBlock, node);\n    assert (namesystem.hasWriteLock());\n    {\n      if (storedBlock \u003d\u003d null || !blocksMap.removeNode(storedBlock, node)) {\n        blockLog.debug(\"BLOCK* removeStoredBlock: {} has already been\" +\n            \" removed from node {}\", storedBlock, node);\n        return;\n      }\n\n      CachedBlock cblock \u003d namesystem.getCacheManager().getCachedBlocks()\n          .get(new CachedBlock(storedBlock.getBlockId(), (short) 0, false));\n      if (cblock !\u003d null) {\n        boolean removed \u003d false;\n        removed |\u003d node.getPendingCached().remove(cblock);\n        removed |\u003d node.getCached().remove(cblock);\n        removed |\u003d node.getPendingUncached().remove(cblock);\n        if (removed) {\n          blockLog.debug(\"BLOCK* removeStoredBlock: {} removed from caching \"\n              + \"related lists on node {}\", storedBlock, node);\n        }\n      }\n\n      //\n      // It\u0027s possible that the block was removed because of a datanode\n      // failure. If the block is still valid, check if replication is\n      // necessary. In that case, put block on a possibly-will-\n      // be-replicated list.\n      //\n      BlockCollection bc \u003d getBlockCollection(storedBlock);\n      if (bc !\u003d null) {\n        bmSafeMode.decrementSafeBlockCount(storedBlock);\n        updateNeededReplications(storedBlock, -1, 0);\n      }\n\n      //\n      // We\u0027ve removed a block from a node, so it\u0027s definitely no longer\n      // in \"excess\" there.\n      //\n      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n          node.getDatanodeUuid());\n      if (excessBlocks !\u003d null) {\n        if (excessBlocks.remove(storedBlock)) {\n          excessBlocksCount.decrementAndGet();\n          blockLog.debug(\"BLOCK* removeStoredBlock: {} is removed from \" +\n              \"excessBlocks\", storedBlock);\n          if (excessBlocks.size() \u003d\u003d 0) {\n            excessReplicateMap.remove(node.getDatanodeUuid());\n          }\n        }\n      }\n\n      // Remove the replica from corruptReplicas\n      corruptReplicas.removeFromCorruptReplicasMap(storedBlock, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "73b86a5046fe3262dde7b05be46b18575e35fd5f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8988. Use LightWeightHashSet instead of LightWeightLinkedSet in BlockManager#excessReplicateMap. (yliu)\n",
      "commitDate": "11/10/15 11:40 PM",
      "commitName": "73b86a5046fe3262dde7b05be46b18575e35fd5f",
      "commitAuthor": "yliu",
      "commitDateOld": "23/09/15 1:34 PM",
      "commitNameOld": "c09dc258a8f64fab852bf6f26187163480dbee3c",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 18.42,
      "commitsBetweenForRepo": 126,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n   public void removeStoredBlock(BlockInfo storedBlock, DatanodeDescriptor node) {\n     blockLog.debug(\"BLOCK* removeStoredBlock: {} from {}\", storedBlock, node);\n     assert (namesystem.hasWriteLock());\n     {\n       if (storedBlock \u003d\u003d null || !blocksMap.removeNode(storedBlock, node)) {\n         blockLog.debug(\"BLOCK* removeStoredBlock: {} has already been\" +\n             \" removed from node {}\", storedBlock, node);\n         return;\n       }\n \n       CachedBlock cblock \u003d namesystem.getCacheManager().getCachedBlocks()\n           .get(new CachedBlock(storedBlock.getBlockId(), (short) 0, false));\n       if (cblock !\u003d null) {\n         boolean removed \u003d false;\n         removed |\u003d node.getPendingCached().remove(cblock);\n         removed |\u003d node.getCached().remove(cblock);\n         removed |\u003d node.getPendingUncached().remove(cblock);\n         if (removed) {\n           blockLog.debug(\"BLOCK* removeStoredBlock: {} removed from caching \"\n               + \"related lists on node {}\", storedBlock, node);\n         }\n       }\n \n       //\n       // It\u0027s possible that the block was removed because of a datanode\n       // failure. If the block is still valid, check if replication is\n       // necessary. In that case, put block on a possibly-will-\n       // be-replicated list.\n       //\n       BlockCollection bc \u003d getBlockCollection(storedBlock);\n       if (bc !\u003d null) {\n         namesystem.decrementSafeBlockCount(storedBlock);\n         updateNeededReplications(storedBlock, -1, 0);\n       }\n \n       //\n       // We\u0027ve removed a block from a node, so it\u0027s definitely no longer\n       // in \"excess\" there.\n       //\n-      LightWeightLinkedSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n+      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n           node.getDatanodeUuid());\n       if (excessBlocks !\u003d null) {\n         if (excessBlocks.remove(storedBlock)) {\n           excessBlocksCount.decrementAndGet();\n           blockLog.debug(\"BLOCK* removeStoredBlock: {} is removed from \" +\n               \"excessBlocks\", storedBlock);\n           if (excessBlocks.size() \u003d\u003d 0) {\n             excessReplicateMap.remove(node.getDatanodeUuid());\n           }\n         }\n       }\n \n       // Remove the replica from corruptReplicas\n       corruptReplicas.removeFromCorruptReplicasMap(storedBlock, node);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void removeStoredBlock(BlockInfo storedBlock, DatanodeDescriptor node) {\n    blockLog.debug(\"BLOCK* removeStoredBlock: {} from {}\", storedBlock, node);\n    assert (namesystem.hasWriteLock());\n    {\n      if (storedBlock \u003d\u003d null || !blocksMap.removeNode(storedBlock, node)) {\n        blockLog.debug(\"BLOCK* removeStoredBlock: {} has already been\" +\n            \" removed from node {}\", storedBlock, node);\n        return;\n      }\n\n      CachedBlock cblock \u003d namesystem.getCacheManager().getCachedBlocks()\n          .get(new CachedBlock(storedBlock.getBlockId(), (short) 0, false));\n      if (cblock !\u003d null) {\n        boolean removed \u003d false;\n        removed |\u003d node.getPendingCached().remove(cblock);\n        removed |\u003d node.getCached().remove(cblock);\n        removed |\u003d node.getPendingUncached().remove(cblock);\n        if (removed) {\n          blockLog.debug(\"BLOCK* removeStoredBlock: {} removed from caching \"\n              + \"related lists on node {}\", storedBlock, node);\n        }\n      }\n\n      //\n      // It\u0027s possible that the block was removed because of a datanode\n      // failure. If the block is still valid, check if replication is\n      // necessary. In that case, put block on a possibly-will-\n      // be-replicated list.\n      //\n      BlockCollection bc \u003d getBlockCollection(storedBlock);\n      if (bc !\u003d null) {\n        namesystem.decrementSafeBlockCount(storedBlock);\n        updateNeededReplications(storedBlock, -1, 0);\n      }\n\n      //\n      // We\u0027ve removed a block from a node, so it\u0027s definitely no longer\n      // in \"excess\" there.\n      //\n      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n          node.getDatanodeUuid());\n      if (excessBlocks !\u003d null) {\n        if (excessBlocks.remove(storedBlock)) {\n          excessBlocksCount.decrementAndGet();\n          blockLog.debug(\"BLOCK* removeStoredBlock: {} is removed from \" +\n              \"excessBlocks\", storedBlock);\n          if (excessBlocks.size() \u003d\u003d 0) {\n            excessReplicateMap.remove(node.getDatanodeUuid());\n          }\n        }\n      }\n\n      // Remove the replica from corruptReplicas\n      corruptReplicas.removeFromCorruptReplicasMap(storedBlock, node);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    }
  }
}