{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "AbstractFuture.java",
  "functionName": "removeWaiter",
  "functionId": "removeWaiter___node-Waiter",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java",
  "functionStartLine": 221,
  "functionEndLine": 251,
  "numCommitsSeen": 4,
  "timeTaken": 730,
  "changeHistory": [
    "d69a82c89c1e857a2a86ff614ae11fb5df993614"
  ],
  "changeHistoryShort": {
    "d69a82c89c1e857a2a86ff614ae11fb5df993614": "Yintroduced"
  },
  "changeHistoryDetails": {
    "d69a82c89c1e857a2a86ff614ae11fb5df993614": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11511. Support Timeout when checking single disk. Contributed by Hanisha Koneru.\n",
      "commitDate": "15/03/17 6:01 PM",
      "commitName": "d69a82c89c1e857a2a86ff614ae11fb5df993614",
      "commitAuthor": "Hanisha Koneru",
      "diff": "@@ -0,0 +1,31 @@\n+  private void removeWaiter(Waiter node) {\n+    node.thread \u003d null; // mark as \u0027deleted\u0027\n+    restart:\n+    while (true) {\n+      Waiter pred \u003d null;\n+      Waiter curr \u003d waiters;\n+      if (curr \u003d\u003d Waiter.TOMBSTONE) {\n+        return; // give up if someone is calling complete\n+      }\n+      Waiter succ;\n+      while (curr !\u003d null) {\n+        succ \u003d curr.next;\n+        if (curr.thread !\u003d null) { // we aren\u0027t unlinking this node, update\n+          // pred.\n+          pred \u003d curr;\n+        } else if (pred !\u003d null) { // We are unlinking this node and it has a\n+          // predecessor.\n+          pred.next \u003d succ;\n+          if (pred.thread \u003d\u003d null) { // We raced with another node that\n+            // unlinked pred. Restart.\n+            continue restart;\n+          }\n+        } else if (!ATOMIC_HELPER\n+            .casWaiters(this, curr, succ)) { // We are unlinking head\n+          continue restart; // We raced with an add or complete\n+        }\n+        curr \u003d succ;\n+      }\n+      break;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void removeWaiter(Waiter node) {\n    node.thread \u003d null; // mark as \u0027deleted\u0027\n    restart:\n    while (true) {\n      Waiter pred \u003d null;\n      Waiter curr \u003d waiters;\n      if (curr \u003d\u003d Waiter.TOMBSTONE) {\n        return; // give up if someone is calling complete\n      }\n      Waiter succ;\n      while (curr !\u003d null) {\n        succ \u003d curr.next;\n        if (curr.thread !\u003d null) { // we aren\u0027t unlinking this node, update\n          // pred.\n          pred \u003d curr;\n        } else if (pred !\u003d null) { // We are unlinking this node and it has a\n          // predecessor.\n          pred.next \u003d succ;\n          if (pred.thread \u003d\u003d null) { // We raced with another node that\n            // unlinked pred. Restart.\n            continue restart;\n          }\n        } else if (!ATOMIC_HELPER\n            .casWaiters(this, curr, succ)) { // We are unlinking head\n          continue restart; // We raced with an add or complete\n        }\n        curr \u003d succ;\n      }\n      break;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/AbstractFuture.java"
    }
  }
}