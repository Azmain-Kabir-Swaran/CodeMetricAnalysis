{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSInputStream.java",
  "functionName": "read",
  "functionId": "read___bufferPool-ByteBufferPool__maxLength-int__opts-EnumSet__ReadOption__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
  "functionStartLine": 1810,
  "functionEndLine": 1846,
  "numCommitsSeen": 136,
  "timeTaken": 4479,
  "changeHistory": [
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "871cb56152e6039ff56c6fabfcd45451029471c3",
    "173c1159519b6a1885c604b9891a31011b0bcc85",
    "c94e43c6df096682138c9030274bbdd6d0d91817",
    "124e507674c0d396f8494585e64226957199097b",
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036"
  ],
  "changeHistoryShort": {
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ybodychange",
    "871cb56152e6039ff56c6fabfcd45451029471c3": "Ybodychange",
    "173c1159519b6a1885c604b9891a31011b0bcc85": "Ybodychange",
    "c94e43c6df096682138c9030274bbdd6d0d91817": "Ybodychange",
    "124e507674c0d396f8494585e64226957199097b": "Ybodychange",
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036": "Yintroduced"
  },
  "changeHistoryDetails": {
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized ByteBuffer read(ByteBufferPool bufferPool,\n      int maxLength, EnumSet\u003cReadOption\u003e opts) \n          throws IOException, UnsupportedOperationException {\n    if (maxLength \u003d\u003d 0) {\n      return EMPTY_BUFFER;\n    } else if (maxLength \u003c 0) {\n      throw new IllegalArgumentException(\"can\u0027t read a negative \" +\n          \"number of bytes.\");\n    }\n    if ((blockReader \u003d\u003d null) || (blockEnd \u003d\u003d -1)) {\n      if (pos \u003e\u003d getFileLength()) {\n        return null;\n      }\n      /*\n       * If we don\u0027t have a blockReader, or the one we have has no more bytes\n       * left to read, we call seekToBlockSource to get a new blockReader and\n       * recalculate blockEnd.  Note that we assume we\u0027re not at EOF here\n       * (we check this above).\n       */\n      if ((!seekToBlockSource(pos)) || (blockReader \u003d\u003d null)) {\n        throw new IOException(\"failed to allocate new BlockReader \" +\n            \"at position \" + pos);\n      }\n    }\n    ByteBuffer buffer \u003d null;\n    if (dfsClient.getConf().getShortCircuitConf().isShortCircuitMmapEnabled()) {\n      buffer \u003d tryReadZeroCopy(maxLength, opts);\n    }\n    if (buffer !\u003d null) {\n      return buffer;\n    }\n    buffer \u003d ByteBufferUtil.fallbackRead(this, bufferPool, maxLength);\n    if (buffer !\u003d null) {\n      getExtendedReadBuffers().put(buffer, bufferPool);\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "09/04/15 11:22 AM",
      "commitNameOld": "30acb7372ab97adf9bc86ead529c96cfe36e2396",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.14,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,37 @@\n   public synchronized ByteBuffer read(ByteBufferPool bufferPool,\n       int maxLength, EnumSet\u003cReadOption\u003e opts) \n           throws IOException, UnsupportedOperationException {\n     if (maxLength \u003d\u003d 0) {\n       return EMPTY_BUFFER;\n     } else if (maxLength \u003c 0) {\n       throw new IllegalArgumentException(\"can\u0027t read a negative \" +\n           \"number of bytes.\");\n     }\n     if ((blockReader \u003d\u003d null) || (blockEnd \u003d\u003d -1)) {\n       if (pos \u003e\u003d getFileLength()) {\n         return null;\n       }\n       /*\n        * If we don\u0027t have a blockReader, or the one we have has no more bytes\n        * left to read, we call seekToBlockSource to get a new blockReader and\n        * recalculate blockEnd.  Note that we assume we\u0027re not at EOF here\n        * (we check this above).\n        */\n       if ((!seekToBlockSource(pos)) || (blockReader \u003d\u003d null)) {\n         throw new IOException(\"failed to allocate new BlockReader \" +\n             \"at position \" + pos);\n       }\n     }\n     ByteBuffer buffer \u003d null;\n-    if (dfsClient.getConf().shortCircuitMmapEnabled) {\n+    if (dfsClient.getConf().getShortCircuitConf().isShortCircuitMmapEnabled()) {\n       buffer \u003d tryReadZeroCopy(maxLength, opts);\n     }\n     if (buffer !\u003d null) {\n       return buffer;\n     }\n     buffer \u003d ByteBufferUtil.fallbackRead(this, bufferPool, maxLength);\n     if (buffer !\u003d null) {\n       getExtendedReadBuffers().put(buffer, bufferPool);\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ByteBuffer read(ByteBufferPool bufferPool,\n      int maxLength, EnumSet\u003cReadOption\u003e opts) \n          throws IOException, UnsupportedOperationException {\n    if (maxLength \u003d\u003d 0) {\n      return EMPTY_BUFFER;\n    } else if (maxLength \u003c 0) {\n      throw new IllegalArgumentException(\"can\u0027t read a negative \" +\n          \"number of bytes.\");\n    }\n    if ((blockReader \u003d\u003d null) || (blockEnd \u003d\u003d -1)) {\n      if (pos \u003e\u003d getFileLength()) {\n        return null;\n      }\n      /*\n       * If we don\u0027t have a blockReader, or the one we have has no more bytes\n       * left to read, we call seekToBlockSource to get a new blockReader and\n       * recalculate blockEnd.  Note that we assume we\u0027re not at EOF here\n       * (we check this above).\n       */\n      if ((!seekToBlockSource(pos)) || (blockReader \u003d\u003d null)) {\n        throw new IOException(\"failed to allocate new BlockReader \" +\n            \"at position \" + pos);\n      }\n    }\n    ByteBuffer buffer \u003d null;\n    if (dfsClient.getConf().getShortCircuitConf().isShortCircuitMmapEnabled()) {\n      buffer \u003d tryReadZeroCopy(maxLength, opts);\n    }\n    if (buffer !\u003d null) {\n      return buffer;\n    }\n    buffer \u003d ByteBufferUtil.fallbackRead(this, bufferPool, maxLength);\n    if (buffer !\u003d null) {\n      getExtendedReadBuffers().put(buffer, bufferPool);\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "871cb56152e6039ff56c6fabfcd45451029471c3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7790. Do not create optional fields in DFSInputStream unless they are needed (cmccabe)\n",
      "commitDate": "12/02/15 5:48 PM",
      "commitName": "871cb56152e6039ff56c6fabfcd45451029471c3",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "12/02/15 10:40 AM",
      "commitNameOld": "6b39ad0865cb2a7960dd59d68178f0bf28865ce2",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.3,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,37 @@\n   public synchronized ByteBuffer read(ByteBufferPool bufferPool,\n       int maxLength, EnumSet\u003cReadOption\u003e opts) \n           throws IOException, UnsupportedOperationException {\n     if (maxLength \u003d\u003d 0) {\n       return EMPTY_BUFFER;\n     } else if (maxLength \u003c 0) {\n       throw new IllegalArgumentException(\"can\u0027t read a negative \" +\n           \"number of bytes.\");\n     }\n     if ((blockReader \u003d\u003d null) || (blockEnd \u003d\u003d -1)) {\n       if (pos \u003e\u003d getFileLength()) {\n         return null;\n       }\n       /*\n        * If we don\u0027t have a blockReader, or the one we have has no more bytes\n        * left to read, we call seekToBlockSource to get a new blockReader and\n        * recalculate blockEnd.  Note that we assume we\u0027re not at EOF here\n        * (we check this above).\n        */\n       if ((!seekToBlockSource(pos)) || (blockReader \u003d\u003d null)) {\n         throw new IOException(\"failed to allocate new BlockReader \" +\n             \"at position \" + pos);\n       }\n     }\n     ByteBuffer buffer \u003d null;\n     if (dfsClient.getConf().shortCircuitMmapEnabled) {\n       buffer \u003d tryReadZeroCopy(maxLength, opts);\n     }\n     if (buffer !\u003d null) {\n       return buffer;\n     }\n     buffer \u003d ByteBufferUtil.fallbackRead(this, bufferPool, maxLength);\n     if (buffer !\u003d null) {\n-      extendedReadBuffers.put(buffer, bufferPool);\n+      getExtendedReadBuffers().put(buffer, bufferPool);\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ByteBuffer read(ByteBufferPool bufferPool,\n      int maxLength, EnumSet\u003cReadOption\u003e opts) \n          throws IOException, UnsupportedOperationException {\n    if (maxLength \u003d\u003d 0) {\n      return EMPTY_BUFFER;\n    } else if (maxLength \u003c 0) {\n      throw new IllegalArgumentException(\"can\u0027t read a negative \" +\n          \"number of bytes.\");\n    }\n    if ((blockReader \u003d\u003d null) || (blockEnd \u003d\u003d -1)) {\n      if (pos \u003e\u003d getFileLength()) {\n        return null;\n      }\n      /*\n       * If we don\u0027t have a blockReader, or the one we have has no more bytes\n       * left to read, we call seekToBlockSource to get a new blockReader and\n       * recalculate blockEnd.  Note that we assume we\u0027re not at EOF here\n       * (we check this above).\n       */\n      if ((!seekToBlockSource(pos)) || (blockReader \u003d\u003d null)) {\n        throw new IOException(\"failed to allocate new BlockReader \" +\n            \"at position \" + pos);\n      }\n    }\n    ByteBuffer buffer \u003d null;\n    if (dfsClient.getConf().shortCircuitMmapEnabled) {\n      buffer \u003d tryReadZeroCopy(maxLength, opts);\n    }\n    if (buffer !\u003d null) {\n      return buffer;\n    }\n    buffer \u003d ByteBufferUtil.fallbackRead(this, bufferPool, maxLength);\n    if (buffer !\u003d null) {\n      getExtendedReadBuffers().put(buffer, bufferPool);\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "173c1159519b6a1885c604b9891a31011b0bcc85": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6065. HDFS zero-copy reads should return null on EOF when doing ZCR (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1575109 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/03/14 5:18 PM",
      "commitName": "173c1159519b6a1885c604b9891a31011b0bcc85",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "04/03/14 1:07 PM",
      "commitNameOld": "c0a903da22c65294b232c7530a6a684ee93daba4",
      "commitAuthorOld": "Michael Stack",
      "daysBetweenCommits": 2.17,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,37 @@\n   public synchronized ByteBuffer read(ByteBufferPool bufferPool,\n       int maxLength, EnumSet\u003cReadOption\u003e opts) \n           throws IOException, UnsupportedOperationException {\n-    assert(maxLength \u003e 0);\n-    if (((blockReader \u003d\u003d null) || (blockEnd \u003d\u003d -1)) \u0026\u0026\n-          (pos \u003c getFileLength())) {\n+    if (maxLength \u003d\u003d 0) {\n+      return EMPTY_BUFFER;\n+    } else if (maxLength \u003c 0) {\n+      throw new IllegalArgumentException(\"can\u0027t read a negative \" +\n+          \"number of bytes.\");\n+    }\n+    if ((blockReader \u003d\u003d null) || (blockEnd \u003d\u003d -1)) {\n+      if (pos \u003e\u003d getFileLength()) {\n+        return null;\n+      }\n       /*\n        * If we don\u0027t have a blockReader, or the one we have has no more bytes\n        * left to read, we call seekToBlockSource to get a new blockReader and\n        * recalculate blockEnd.  Note that we assume we\u0027re not at EOF here\n        * (we check this above).\n        */\n       if ((!seekToBlockSource(pos)) || (blockReader \u003d\u003d null)) {\n         throw new IOException(\"failed to allocate new BlockReader \" +\n             \"at position \" + pos);\n       }\n     }\n     ByteBuffer buffer \u003d null;\n     if (dfsClient.getConf().shortCircuitMmapEnabled) {\n       buffer \u003d tryReadZeroCopy(maxLength, opts);\n     }\n     if (buffer !\u003d null) {\n       return buffer;\n     }\n     buffer \u003d ByteBufferUtil.fallbackRead(this, bufferPool, maxLength);\n     if (buffer !\u003d null) {\n       extendedReadBuffers.put(buffer, bufferPool);\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ByteBuffer read(ByteBufferPool bufferPool,\n      int maxLength, EnumSet\u003cReadOption\u003e opts) \n          throws IOException, UnsupportedOperationException {\n    if (maxLength \u003d\u003d 0) {\n      return EMPTY_BUFFER;\n    } else if (maxLength \u003c 0) {\n      throw new IllegalArgumentException(\"can\u0027t read a negative \" +\n          \"number of bytes.\");\n    }\n    if ((blockReader \u003d\u003d null) || (blockEnd \u003d\u003d -1)) {\n      if (pos \u003e\u003d getFileLength()) {\n        return null;\n      }\n      /*\n       * If we don\u0027t have a blockReader, or the one we have has no more bytes\n       * left to read, we call seekToBlockSource to get a new blockReader and\n       * recalculate blockEnd.  Note that we assume we\u0027re not at EOF here\n       * (we check this above).\n       */\n      if ((!seekToBlockSource(pos)) || (blockReader \u003d\u003d null)) {\n        throw new IOException(\"failed to allocate new BlockReader \" +\n            \"at position \" + pos);\n      }\n    }\n    ByteBuffer buffer \u003d null;\n    if (dfsClient.getConf().shortCircuitMmapEnabled) {\n      buffer \u003d tryReadZeroCopy(maxLength, opts);\n    }\n    if (buffer !\u003d null) {\n      return buffer;\n    }\n    buffer \u003d ByteBufferUtil.fallbackRead(this, bufferPool, maxLength);\n    if (buffer !\u003d null) {\n      extendedReadBuffers.put(buffer, bufferPool);\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "c94e43c6df096682138c9030274bbdd6d0d91817": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6046. add dfs.client.mmap.enabled (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1573887 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/03/14 10:46 PM",
      "commitName": "c94e43c6df096682138c9030274bbdd6d0d91817",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "02/03/14 7:58 PM",
      "commitNameOld": "dd049a2f6097da189ccce2f5890a2b9bc77fa73f",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 1.12,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,30 @@\n   public synchronized ByteBuffer read(ByteBufferPool bufferPool,\n       int maxLength, EnumSet\u003cReadOption\u003e opts) \n           throws IOException, UnsupportedOperationException {\n     assert(maxLength \u003e 0);\n     if (((blockReader \u003d\u003d null) || (blockEnd \u003d\u003d -1)) \u0026\u0026\n           (pos \u003c getFileLength())) {\n       /*\n        * If we don\u0027t have a blockReader, or the one we have has no more bytes\n        * left to read, we call seekToBlockSource to get a new blockReader and\n        * recalculate blockEnd.  Note that we assume we\u0027re not at EOF here\n        * (we check this above).\n        */\n       if ((!seekToBlockSource(pos)) || (blockReader \u003d\u003d null)) {\n         throw new IOException(\"failed to allocate new BlockReader \" +\n             \"at position \" + pos);\n       }\n     }\n-    ByteBuffer buffer \u003d tryReadZeroCopy(maxLength, opts);\n+    ByteBuffer buffer \u003d null;\n+    if (dfsClient.getConf().shortCircuitMmapEnabled) {\n+      buffer \u003d tryReadZeroCopy(maxLength, opts);\n+    }\n     if (buffer !\u003d null) {\n       return buffer;\n     }\n     buffer \u003d ByteBufferUtil.fallbackRead(this, bufferPool, maxLength);\n     if (buffer !\u003d null) {\n       extendedReadBuffers.put(buffer, bufferPool);\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ByteBuffer read(ByteBufferPool bufferPool,\n      int maxLength, EnumSet\u003cReadOption\u003e opts) \n          throws IOException, UnsupportedOperationException {\n    assert(maxLength \u003e 0);\n    if (((blockReader \u003d\u003d null) || (blockEnd \u003d\u003d -1)) \u0026\u0026\n          (pos \u003c getFileLength())) {\n      /*\n       * If we don\u0027t have a blockReader, or the one we have has no more bytes\n       * left to read, we call seekToBlockSource to get a new blockReader and\n       * recalculate blockEnd.  Note that we assume we\u0027re not at EOF here\n       * (we check this above).\n       */\n      if ((!seekToBlockSource(pos)) || (blockReader \u003d\u003d null)) {\n        throw new IOException(\"failed to allocate new BlockReader \" +\n            \"at position \" + pos);\n      }\n    }\n    ByteBuffer buffer \u003d null;\n    if (dfsClient.getConf().shortCircuitMmapEnabled) {\n      buffer \u003d tryReadZeroCopy(maxLength, opts);\n    }\n    if (buffer !\u003d null) {\n      return buffer;\n    }\n    buffer \u003d ByteBufferUtil.fallbackRead(this, bufferPool, maxLength);\n    if (buffer !\u003d null) {\n      extendedReadBuffers.put(buffer, bufferPool);\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "124e507674c0d396f8494585e64226957199097b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5634. Allow BlockReaderLocal to switch between checksumming and not (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551701 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 12:57 PM",
      "commitName": "124e507674c0d396f8494585e64226957199097b",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "12/12/13 6:47 PM",
      "commitNameOld": "a31a3a68ead2773f8daa5b3ac1041e271b442789",
      "commitAuthorOld": "Junping Du",
      "daysBetweenCommits": 4.76,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,27 @@\n   public synchronized ByteBuffer read(ByteBufferPool bufferPool,\n       int maxLength, EnumSet\u003cReadOption\u003e opts) \n           throws IOException, UnsupportedOperationException {\n     assert(maxLength \u003e 0);\n     if (((blockReader \u003d\u003d null) || (blockEnd \u003d\u003d -1)) \u0026\u0026\n           (pos \u003c getFileLength())) {\n       /*\n        * If we don\u0027t have a blockReader, or the one we have has no more bytes\n        * left to read, we call seekToBlockSource to get a new blockReader and\n        * recalculate blockEnd.  Note that we assume we\u0027re not at EOF here\n        * (we check this above).\n        */\n       if ((!seekToBlockSource(pos)) || (blockReader \u003d\u003d null)) {\n         throw new IOException(\"failed to allocate new BlockReader \" +\n             \"at position \" + pos);\n       }\n     }\n-    boolean canSkipChecksums \u003d opts.contains(ReadOption.SKIP_CHECKSUMS);\n-    if (canSkipChecksums) {\n-      ByteBuffer buffer \u003d tryReadZeroCopy(maxLength);\n-      if (buffer !\u003d null) {\n-        return buffer;\n-      }\n+    ByteBuffer buffer \u003d tryReadZeroCopy(maxLength, opts);\n+    if (buffer !\u003d null) {\n+      return buffer;\n     }\n-    ByteBuffer buffer \u003d ByteBufferUtil.\n-        fallbackRead(this, bufferPool, maxLength);\n+    buffer \u003d ByteBufferUtil.fallbackRead(this, bufferPool, maxLength);\n     if (buffer !\u003d null) {\n       extendedReadBuffers.put(buffer, bufferPool);\n     }\n     return buffer;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ByteBuffer read(ByteBufferPool bufferPool,\n      int maxLength, EnumSet\u003cReadOption\u003e opts) \n          throws IOException, UnsupportedOperationException {\n    assert(maxLength \u003e 0);\n    if (((blockReader \u003d\u003d null) || (blockEnd \u003d\u003d -1)) \u0026\u0026\n          (pos \u003c getFileLength())) {\n      /*\n       * If we don\u0027t have a blockReader, or the one we have has no more bytes\n       * left to read, we call seekToBlockSource to get a new blockReader and\n       * recalculate blockEnd.  Note that we assume we\u0027re not at EOF here\n       * (we check this above).\n       */\n      if ((!seekToBlockSource(pos)) || (blockReader \u003d\u003d null)) {\n        throw new IOException(\"failed to allocate new BlockReader \" +\n            \"at position \" + pos);\n      }\n    }\n    ByteBuffer buffer \u003d tryReadZeroCopy(maxLength, opts);\n    if (buffer !\u003d null) {\n      return buffer;\n    }\n    buffer \u003d ByteBufferUtil.fallbackRead(this, bufferPool, maxLength);\n    if (buffer !\u003d null) {\n      extendedReadBuffers.put(buffer, bufferPool);\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5260. Merge zero-copy memory-mapped HDFS client reads to trunk and branch-2. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1527113 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/13 3:51 PM",
      "commitName": "eccdb9aa8bcdee750583d16a1253f1c5faabd036",
      "commitAuthor": "Chris Nauroth",
      "diff": "@@ -0,0 +1,31 @@\n+  public synchronized ByteBuffer read(ByteBufferPool bufferPool,\n+      int maxLength, EnumSet\u003cReadOption\u003e opts) \n+          throws IOException, UnsupportedOperationException {\n+    assert(maxLength \u003e 0);\n+    if (((blockReader \u003d\u003d null) || (blockEnd \u003d\u003d -1)) \u0026\u0026\n+          (pos \u003c getFileLength())) {\n+      /*\n+       * If we don\u0027t have a blockReader, or the one we have has no more bytes\n+       * left to read, we call seekToBlockSource to get a new blockReader and\n+       * recalculate blockEnd.  Note that we assume we\u0027re not at EOF here\n+       * (we check this above).\n+       */\n+      if ((!seekToBlockSource(pos)) || (blockReader \u003d\u003d null)) {\n+        throw new IOException(\"failed to allocate new BlockReader \" +\n+            \"at position \" + pos);\n+      }\n+    }\n+    boolean canSkipChecksums \u003d opts.contains(ReadOption.SKIP_CHECKSUMS);\n+    if (canSkipChecksums) {\n+      ByteBuffer buffer \u003d tryReadZeroCopy(maxLength);\n+      if (buffer !\u003d null) {\n+        return buffer;\n+      }\n+    }\n+    ByteBuffer buffer \u003d ByteBufferUtil.\n+        fallbackRead(this, bufferPool, maxLength);\n+    if (buffer !\u003d null) {\n+      extendedReadBuffers.put(buffer, bufferPool);\n+    }\n+    return buffer;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ByteBuffer read(ByteBufferPool bufferPool,\n      int maxLength, EnumSet\u003cReadOption\u003e opts) \n          throws IOException, UnsupportedOperationException {\n    assert(maxLength \u003e 0);\n    if (((blockReader \u003d\u003d null) || (blockEnd \u003d\u003d -1)) \u0026\u0026\n          (pos \u003c getFileLength())) {\n      /*\n       * If we don\u0027t have a blockReader, or the one we have has no more bytes\n       * left to read, we call seekToBlockSource to get a new blockReader and\n       * recalculate blockEnd.  Note that we assume we\u0027re not at EOF here\n       * (we check this above).\n       */\n      if ((!seekToBlockSource(pos)) || (blockReader \u003d\u003d null)) {\n        throw new IOException(\"failed to allocate new BlockReader \" +\n            \"at position \" + pos);\n      }\n    }\n    boolean canSkipChecksums \u003d opts.contains(ReadOption.SKIP_CHECKSUMS);\n    if (canSkipChecksums) {\n      ByteBuffer buffer \u003d tryReadZeroCopy(maxLength);\n      if (buffer !\u003d null) {\n        return buffer;\n      }\n    }\n    ByteBuffer buffer \u003d ByteBufferUtil.\n        fallbackRead(this, bufferPool, maxLength);\n    if (buffer !\u003d null) {\n      extendedReadBuffers.put(buffer, bufferPool);\n    }\n    return buffer;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
    }
  }
}