{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ShortCircuitReplica.java",
  "functionName": "close",
  "functionId": "close",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java",
  "functionStartLine": 233,
  "functionEndLine": 255,
  "numCommitsSeen": 18,
  "timeTaken": 4563,
  "changeHistory": [
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "c992bcf9c136d3df686655a80e636bb7bb0664da",
    "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f",
    "f93d99990a9a02ce693cd74466c2e5f127c1f560",
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
    "9fdb11747628f2fd010bcda4398043eb4eb380ce",
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
    "9a4030e0e84a688c12daa21fe9a165808c3eca70"
  ],
  "changeHistoryShort": {
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": "Ybodychange",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "c992bcf9c136d3df686655a80e636bb7bb0664da": "Ymultichange(Yfilerename,Ybodychange)",
    "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f": "Ybodychange",
    "f93d99990a9a02ce693cd74466c2e5f127c1f560": "Yfilerename",
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f": "Ybodychange",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange)",
    "9fdb11747628f2fd010bcda4398043eb4eb380ce": "Ybodychange",
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f": "Ybodychange",
    "9a4030e0e84a688c12daa21fe9a165808c3eca70": "Yintroduced"
  },
  "changeHistoryDetails": {
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13695. Move logging to slf4j in HDFS package. Contributed by Ian Pickering.\n",
      "commitDate": "06/09/18 2:48 PM",
      "commitName": "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "03/10/15 11:38 AM",
      "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1069.13,
      "commitsBetweenForRepo": 7869,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   void close() {\n     String suffix \u003d \"\";\n \n     Preconditions.checkState(refCount \u003d\u003d 0,\n         \"tried to close replica with refCount %d: %s\", refCount, this);\n     refCount \u003d -1;\n     Preconditions.checkState(purged,\n         \"tried to close unpurged replica %s\", this);\n     if (hasMmap()) {\n       munmap();\n       if (LOG.isTraceEnabled()) {\n         suffix +\u003d \"  munmapped.\";\n       }\n     }\n-    IOUtilsClient.cleanup(LOG, dataStream, metaStream);\n+    IOUtilsClient.cleanupWithLogger(LOG, dataStream, metaStream);\n     if (slot !\u003d null) {\n       cache.scheduleSlotReleaser(slot);\n       if (LOG.isTraceEnabled()) {\n         suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n       }\n     }\n     LOG.trace(\"closed {}{}\", this, suffix);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void close() {\n    String suffix \u003d \"\";\n\n    Preconditions.checkState(refCount \u003d\u003d 0,\n        \"tried to close replica with refCount %d: %s\", refCount, this);\n    refCount \u003d -1;\n    Preconditions.checkState(purged,\n        \"tried to close unpurged replica %s\", this);\n    if (hasMmap()) {\n      munmap();\n      if (LOG.isTraceEnabled()) {\n        suffix +\u003d \"  munmapped.\";\n      }\n    }\n    IOUtilsClient.cleanupWithLogger(LOG, dataStream, metaStream);\n    if (slot !\u003d null) {\n      cache.scheduleSlotReleaser(slot);\n      if (LOG.isTraceEnabled()) {\n        suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n      }\n    }\n    LOG.trace(\"closed {}{}\", this, suffix);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java",
      "extendedDetails": {}
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,23 @@\n   void close() {\n     String suffix \u003d \"\";\n     \n     Preconditions.checkState(refCount \u003d\u003d 0,\n         \"tried to close replica with refCount %d: %s\", refCount, this);\n     refCount \u003d -1;\n     Preconditions.checkState(purged,\n         \"tried to close unpurged replica %s\", this);\n     if (hasMmap()) {\n       munmap();\n       if (LOG.isTraceEnabled()) {\n         suffix +\u003d \"  munmapped.\";\n       }\n     }\n     IOUtilsClient.cleanup(LOG, dataStream, metaStream);\n     if (slot !\u003d null) {\n       cache.scheduleSlotReleaser(slot);\n       if (LOG.isTraceEnabled()) {\n         suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n       }\n     }\n-    if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"closed \" + this + suffix);\n-    }\n+    LOG.trace(\"closed {}{}\", this, suffix);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void close() {\n    String suffix \u003d \"\";\n    \n    Preconditions.checkState(refCount \u003d\u003d 0,\n        \"tried to close replica with refCount %d: %s\", refCount, this);\n    refCount \u003d -1;\n    Preconditions.checkState(purged,\n        \"tried to close unpurged replica %s\", this);\n    if (hasMmap()) {\n      munmap();\n      if (LOG.isTraceEnabled()) {\n        suffix +\u003d \"  munmapped.\";\n      }\n    }\n    IOUtilsClient.cleanup(LOG, dataStream, metaStream);\n    if (slot !\u003d null) {\n      cache.scheduleSlotReleaser(slot);\n      if (LOG.isTraceEnabled()) {\n        suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n      }\n    }\n    LOG.trace(\"closed {}{}\", this, suffix);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,25 @@\n   void close() {\n     String suffix \u003d \"\";\n     \n     Preconditions.checkState(refCount \u003d\u003d 0,\n         \"tried to close replica with refCount %d: %s\", refCount, this);\n     refCount \u003d -1;\n     Preconditions.checkState(purged,\n         \"tried to close unpurged replica %s\", this);\n     if (hasMmap()) {\n       munmap();\n       if (LOG.isTraceEnabled()) {\n         suffix +\u003d \"  munmapped.\";\n       }\n     }\n     IOUtilsClient.cleanup(LOG, dataStream, metaStream);\n     if (slot !\u003d null) {\n       cache.scheduleSlotReleaser(slot);\n       if (LOG.isTraceEnabled()) {\n         suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n       }\n     }\n-    LOG.trace(\"closed {}{}\", this, suffix);\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"closed \" + this + suffix);\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void close() {\n    String suffix \u003d \"\";\n    \n    Preconditions.checkState(refCount \u003d\u003d 0,\n        \"tried to close replica with refCount %d: %s\", refCount, this);\n    refCount \u003d -1;\n    Preconditions.checkState(purged,\n        \"tried to close unpurged replica %s\", this);\n    if (hasMmap()) {\n      munmap();\n      if (LOG.isTraceEnabled()) {\n        suffix +\u003d \"  munmapped.\";\n      }\n    }\n    IOUtilsClient.cleanup(LOG, dataStream, metaStream);\n    if (slot !\u003d null) {\n      cache.scheduleSlotReleaser(slot);\n      if (LOG.isTraceEnabled()) {\n        suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n      }\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"closed \" + this + suffix);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/08/15 2:02 PM",
      "commitNameOld": "c992bcf9c136d3df686655a80e636bb7bb0664da",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 34.16,
      "commitsBetweenForRepo": 233,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,23 @@\n   void close() {\n     String suffix \u003d \"\";\n     \n     Preconditions.checkState(refCount \u003d\u003d 0,\n         \"tried to close replica with refCount %d: %s\", refCount, this);\n     refCount \u003d -1;\n     Preconditions.checkState(purged,\n         \"tried to close unpurged replica %s\", this);\n     if (hasMmap()) {\n       munmap();\n       if (LOG.isTraceEnabled()) {\n         suffix +\u003d \"  munmapped.\";\n       }\n     }\n     IOUtilsClient.cleanup(LOG, dataStream, metaStream);\n     if (slot !\u003d null) {\n       cache.scheduleSlotReleaser(slot);\n       if (LOG.isTraceEnabled()) {\n         suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n       }\n     }\n-    if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"closed \" + this + suffix);\n-    }\n+    LOG.trace(\"closed {}{}\", this, suffix);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void close() {\n    String suffix \u003d \"\";\n    \n    Preconditions.checkState(refCount \u003d\u003d 0,\n        \"tried to close replica with refCount %d: %s\", refCount, this);\n    refCount \u003d -1;\n    Preconditions.checkState(purged,\n        \"tried to close unpurged replica %s\", this);\n    if (hasMmap()) {\n      munmap();\n      if (LOG.isTraceEnabled()) {\n        suffix +\u003d \"  munmapped.\";\n      }\n    }\n    IOUtilsClient.cleanup(LOG, dataStream, metaStream);\n    if (slot !\u003d null) {\n      cache.scheduleSlotReleaser(slot);\n      if (LOG.isTraceEnabled()) {\n        suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n      }\n    }\n    LOG.trace(\"closed {}{}\", this, suffix);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java",
      "extendedDetails": {}
    },
    "c992bcf9c136d3df686655a80e636bb7bb0664da": {
      "type": "Ymultichange(Yfilerename,Ybodychange)",
      "commitMessage": "HDFS-8951. Move the shortcircuit package to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/08/15 2:02 PM",
      "commitName": "c992bcf9c136d3df686655a80e636bb7bb0664da",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-8951. Move the shortcircuit package to hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "26/08/15 2:02 PM",
          "commitName": "c992bcf9c136d3df686655a80e636bb7bb0664da",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "25/08/15 2:29 PM",
          "commitNameOld": "a4d9acc51d1a977bc333da17780c00c72e8546f1",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.98,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,25 @@\n   void close() {\n     String suffix \u003d \"\";\n     \n     Preconditions.checkState(refCount \u003d\u003d 0,\n         \"tried to close replica with refCount %d: %s\", refCount, this);\n     refCount \u003d -1;\n     Preconditions.checkState(purged,\n         \"tried to close unpurged replica %s\", this);\n     if (hasMmap()) {\n       munmap();\n       if (LOG.isTraceEnabled()) {\n         suffix +\u003d \"  munmapped.\";\n       }\n     }\n-    IOUtils.cleanup(LOG, dataStream, metaStream);\n+    IOUtilsClient.cleanup(LOG, dataStream, metaStream);\n     if (slot !\u003d null) {\n       cache.scheduleSlotReleaser(slot);\n       if (LOG.isTraceEnabled()) {\n         suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n       }\n     }\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"closed \" + this + suffix);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void close() {\n    String suffix \u003d \"\";\n    \n    Preconditions.checkState(refCount \u003d\u003d 0,\n        \"tried to close replica with refCount %d: %s\", refCount, this);\n    refCount \u003d -1;\n    Preconditions.checkState(purged,\n        \"tried to close unpurged replica %s\", this);\n    if (hasMmap()) {\n      munmap();\n      if (LOG.isTraceEnabled()) {\n        suffix +\u003d \"  munmapped.\";\n      }\n    }\n    IOUtilsClient.cleanup(LOG, dataStream, metaStream);\n    if (slot !\u003d null) {\n      cache.scheduleSlotReleaser(slot);\n      if (LOG.isTraceEnabled()) {\n        suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n      }\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"closed \" + this + suffix);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8951. Move the shortcircuit package to hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "26/08/15 2:02 PM",
          "commitName": "c992bcf9c136d3df686655a80e636bb7bb0664da",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "25/08/15 2:29 PM",
          "commitNameOld": "a4d9acc51d1a977bc333da17780c00c72e8546f1",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.98,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,25 @@\n   void close() {\n     String suffix \u003d \"\";\n     \n     Preconditions.checkState(refCount \u003d\u003d 0,\n         \"tried to close replica with refCount %d: %s\", refCount, this);\n     refCount \u003d -1;\n     Preconditions.checkState(purged,\n         \"tried to close unpurged replica %s\", this);\n     if (hasMmap()) {\n       munmap();\n       if (LOG.isTraceEnabled()) {\n         suffix +\u003d \"  munmapped.\";\n       }\n     }\n-    IOUtils.cleanup(LOG, dataStream, metaStream);\n+    IOUtilsClient.cleanup(LOG, dataStream, metaStream);\n     if (slot !\u003d null) {\n       cache.scheduleSlotReleaser(slot);\n       if (LOG.isTraceEnabled()) {\n         suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n       }\n     }\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"closed \" + this + suffix);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void close() {\n    String suffix \u003d \"\";\n    \n    Preconditions.checkState(refCount \u003d\u003d 0,\n        \"tried to close replica with refCount %d: %s\", refCount, this);\n    refCount \u003d -1;\n    Preconditions.checkState(purged,\n        \"tried to close unpurged replica %s\", this);\n    if (hasMmap()) {\n      munmap();\n      if (LOG.isTraceEnabled()) {\n        suffix +\u003d \"  munmapped.\";\n      }\n    }\n    IOUtilsClient.cleanup(LOG, dataStream, metaStream);\n    if (slot !\u003d null) {\n      cache.scheduleSlotReleaser(slot);\n      if (LOG.isTraceEnabled()) {\n        suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n      }\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"closed \" + this + suffix);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java",
          "extendedDetails": {}
        }
      ]
    },
    "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6735. A minor optimization to avoid pread() be blocked by read() inside the same DFSInputStream (Lars Hofhansl via stack)\n",
      "commitDate": "02/12/14 8:57 PM",
      "commitName": "7caa3bc98e6880f98c5c32c486a0c539f9fd3f5f",
      "commitAuthor": "stack",
      "commitDateOld": "01/04/14 10:09 PM",
      "commitNameOld": "f93d99990a9a02ce693cd74466c2e5f127c1f560",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 244.99,
      "commitsBetweenForRepo": 1984,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,25 @@\n   void close() {\n     String suffix \u003d \"\";\n     \n     Preconditions.checkState(refCount \u003d\u003d 0,\n-        \"tried to close replica with refCount \" + refCount + \": \" + this);\n+        \"tried to close replica with refCount %d: %s\", refCount, this);\n     refCount \u003d -1;\n     Preconditions.checkState(purged,\n-        \"tried to close unpurged replica \" + this);\n+        \"tried to close unpurged replica %s\", this);\n     if (hasMmap()) {\n       munmap();\n-      suffix +\u003d \"  munmapped.\";\n+      if (LOG.isTraceEnabled()) {\n+        suffix +\u003d \"  munmapped.\";\n+      }\n     }\n     IOUtils.cleanup(LOG, dataStream, metaStream);\n     if (slot !\u003d null) {\n       cache.scheduleSlotReleaser(slot);\n-      suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n+      if (LOG.isTraceEnabled()) {\n+        suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n+      }\n     }\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"closed \" + this + suffix);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void close() {\n    String suffix \u003d \"\";\n    \n    Preconditions.checkState(refCount \u003d\u003d 0,\n        \"tried to close replica with refCount %d: %s\", refCount, this);\n    refCount \u003d -1;\n    Preconditions.checkState(purged,\n        \"tried to close unpurged replica %s\", this);\n    if (hasMmap()) {\n      munmap();\n      if (LOG.isTraceEnabled()) {\n        suffix +\u003d \"  munmapped.\";\n      }\n    }\n    IOUtils.cleanup(LOG, dataStream, metaStream);\n    if (slot !\u003d null) {\n      cache.scheduleSlotReleaser(slot);\n      if (LOG.isTraceEnabled()) {\n        suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n      }\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"closed \" + this + suffix);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java",
      "extendedDetails": {}
    },
    "f93d99990a9a02ce693cd74466c2e5f127c1f560": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-6167. Relocate the non-public API classes in the hdfs.client package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1583878 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/14 10:09 PM",
      "commitName": "f93d99990a9a02ce693cd74466c2e5f127c1f560",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "01/04/14 6:00 PM",
      "commitNameOld": "5c7cb51775bd3d4a6e3e1bd501b3a8d747733fe3",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.17,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void close() {\n    String suffix \u003d \"\";\n    \n    Preconditions.checkState(refCount \u003d\u003d 0,\n        \"tried to close replica with refCount \" + refCount + \": \" + this);\n    refCount \u003d -1;\n    Preconditions.checkState(purged,\n        \"tried to close unpurged replica \" + this);\n    if (hasMmap()) {\n      munmap();\n      suffix +\u003d \"  munmapped.\";\n    }\n    IOUtils.cleanup(LOG, dataStream, metaStream);\n    if (slot !\u003d null) {\n      cache.scheduleSlotReleaser(slot);\n      suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"closed \" + this + suffix);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/client/ShortCircuitReplica.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitReplica.java"
      }
    },
    "dd049a2f6097da189ccce2f5890a2b9bc77fa73f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5950. The DFSClient and DataNode should use shared memory segments to communicate short-circuit information (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1573433 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/03/14 7:58 PM",
      "commitName": "dd049a2f6097da189ccce2f5890a2b9bc77fa73f",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "12/02/14 7:10 PM",
      "commitNameOld": "f0d64a078da7e932b9509734f75170e3e525e68c",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 18.03,
      "commitsBetweenForRepo": 129,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,21 @@\n   void close() {\n+    String suffix \u003d \"\";\n+    \n     Preconditions.checkState(refCount \u003d\u003d 0,\n         \"tried to close replica with refCount \" + refCount + \": \" + this);\n+    refCount \u003d -1;\n     Preconditions.checkState(purged,\n         \"tried to close unpurged replica \" + this);\n-    if (hasMmap()) munmap();\n+    if (hasMmap()) {\n+      munmap();\n+      suffix +\u003d \"  munmapped.\";\n+    }\n     IOUtils.cleanup(LOG, dataStream, metaStream);\n+    if (slot !\u003d null) {\n+      cache.scheduleSlotReleaser(slot);\n+      suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n+    }\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"closed \" + this + suffix);\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void close() {\n    String suffix \u003d \"\";\n    \n    Preconditions.checkState(refCount \u003d\u003d 0,\n        \"tried to close replica with refCount \" + refCount + \": \" + this);\n    refCount \u003d -1;\n    Preconditions.checkState(purged,\n        \"tried to close unpurged replica \" + this);\n    if (hasMmap()) {\n      munmap();\n      suffix +\u003d \"  munmapped.\";\n    }\n    IOUtils.cleanup(LOG, dataStream, metaStream);\n    if (slot !\u003d null) {\n      cache.scheduleSlotReleaser(slot);\n      suffix +\u003d \"  scheduling \" + slot + \" for later release.\";\n    }\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"closed \" + this + suffix);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/client/ShortCircuitReplica.java",
      "extendedDetails": {}
    },
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-5810. Unify mmap cache and short-circuit file descriptor cache (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567720 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 11:08 AM",
      "commitName": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-5810. Unify mmap cache and short-circuit file descriptor cache (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567720 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "12/02/14 11:08 AM",
          "commitName": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "12/02/14 10:02 AM",
          "commitNameOld": "5efc9978ddf35f8f4e194e34a102a729dae69992",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.05,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,11 +1,8 @@\n-  public synchronized void close() {\n-    if (closed) return;\n-    closed \u003d true;\n-    IOUtils.cleanup(LOG, cacheCleaner);\n-    for (Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d map.entries().iterator();\n-          iter.hasNext();) {\n-      Entry\u003cKey, Value\u003e entry \u003d iter.next();\n-      entry.getValue().close();\n-      iter.remove();\n-    }\n+  void close() {\n+    Preconditions.checkState(refCount \u003d\u003d 0,\n+        \"tried to close replica with refCount \" + refCount + \": \" + this);\n+    Preconditions.checkState(purged,\n+        \"tried to close unpurged replica \" + this);\n+    if (hasMmap()) munmap();\n+    IOUtils.cleanup(LOG, dataStream, metaStream);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void close() {\n    Preconditions.checkState(refCount \u003d\u003d 0,\n        \"tried to close replica with refCount \" + refCount + \": \" + this);\n    Preconditions.checkState(purged,\n        \"tried to close unpurged replica \" + this);\n    if (hasMmap()) munmap();\n    IOUtils.cleanup(LOG, dataStream, metaStream);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/client/ShortCircuitReplica.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/FileInputStreamCache.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/client/ShortCircuitReplica.java",
            "oldMethodName": "close",
            "newMethodName": "close"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5810. Unify mmap cache and short-circuit file descriptor cache (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567720 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "12/02/14 11:08 AM",
          "commitName": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "12/02/14 10:02 AM",
          "commitNameOld": "5efc9978ddf35f8f4e194e34a102a729dae69992",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.05,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,11 +1,8 @@\n-  public synchronized void close() {\n-    if (closed) return;\n-    closed \u003d true;\n-    IOUtils.cleanup(LOG, cacheCleaner);\n-    for (Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d map.entries().iterator();\n-          iter.hasNext();) {\n-      Entry\u003cKey, Value\u003e entry \u003d iter.next();\n-      entry.getValue().close();\n-      iter.remove();\n-    }\n+  void close() {\n+    Preconditions.checkState(refCount \u003d\u003d 0,\n+        \"tried to close replica with refCount \" + refCount + \": \" + this);\n+    Preconditions.checkState(purged,\n+        \"tried to close unpurged replica \" + this);\n+    if (hasMmap()) munmap();\n+    IOUtils.cleanup(LOG, dataStream, metaStream);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void close() {\n    Preconditions.checkState(refCount \u003d\u003d 0,\n        \"tried to close replica with refCount \" + refCount + \": \" + this);\n    Preconditions.checkState(purged,\n        \"tried to close unpurged replica \" + this);\n    if (hasMmap()) munmap();\n    IOUtils.cleanup(LOG, dataStream, metaStream);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/client/ShortCircuitReplica.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5810. Unify mmap cache and short-circuit file descriptor cache (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567720 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "12/02/14 11:08 AM",
          "commitName": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "12/02/14 10:02 AM",
          "commitNameOld": "5efc9978ddf35f8f4e194e34a102a729dae69992",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.05,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,11 +1,8 @@\n-  public synchronized void close() {\n-    if (closed) return;\n-    closed \u003d true;\n-    IOUtils.cleanup(LOG, cacheCleaner);\n-    for (Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d map.entries().iterator();\n-          iter.hasNext();) {\n-      Entry\u003cKey, Value\u003e entry \u003d iter.next();\n-      entry.getValue().close();\n-      iter.remove();\n-    }\n+  void close() {\n+    Preconditions.checkState(refCount \u003d\u003d 0,\n+        \"tried to close replica with refCount \" + refCount + \": \" + this);\n+    Preconditions.checkState(purged,\n+        \"tried to close unpurged replica \" + this);\n+    if (hasMmap()) munmap();\n+    IOUtils.cleanup(LOG, dataStream, metaStream);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void close() {\n    Preconditions.checkState(refCount \u003d\u003d 0,\n        \"tried to close replica with refCount \" + refCount + \": \" + this);\n    Preconditions.checkState(purged,\n        \"tried to close unpurged replica \" + this);\n    if (hasMmap()) munmap();\n    IOUtils.cleanup(LOG, dataStream, metaStream);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/client/ShortCircuitReplica.java",
          "extendedDetails": {}
        }
      ]
    },
    "9fdb11747628f2fd010bcda4398043eb4eb380ce": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4824. FileInputStreamCache.close leaves dangling reference to FileInputStreamCache.cacheCleaner. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1483641 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/05/13 8:39 PM",
      "commitName": "9fdb11747628f2fd010bcda4398043eb4eb380ce",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "09/05/13 5:03 PM",
      "commitNameOld": "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 7.15,
      "commitsBetweenForRepo": 41,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,11 @@\n   public synchronized void close() {\n     if (closed) return;\n     closed \u003d true;\n-    if (cacheCleaner !\u003d null) {\n-      executor.remove(cacheCleaner);\n-    }\n+    IOUtils.cleanup(LOG, cacheCleaner);\n     for (Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d map.entries().iterator();\n           iter.hasNext();) {\n       Entry\u003cKey, Value\u003e entry \u003d iter.next();\n       entry.getValue().close();\n       iter.remove();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() {\n    if (closed) return;\n    closed \u003d true;\n    IOUtils.cleanup(LOG, cacheCleaner);\n    for (Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d map.entries().iterator();\n          iter.hasNext();) {\n      Entry\u003cKey, Value\u003e entry \u003d iter.next();\n      entry.getValue().close();\n      iter.remove();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/FileInputStreamCache.java",
      "extendedDetails": {}
    },
    "a18fd620d070cf8e84aaf80d93807ac9ee207a0f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4661. A few little code cleanups of some HDFS-347-related code. Contributed by Colin Patrick McCabe.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1480839 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/05/13 5:03 PM",
      "commitName": "a18fd620d070cf8e84aaf80d93807ac9ee207a0f",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "14/01/13 4:31 PM",
      "commitNameOld": "2fd41b3b429775a7151e84d971f593d99ef8de14",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 114.98,
      "commitsBetweenForRepo": 669,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,13 @@\n   public synchronized void close() {\n     if (closed) return;\n     closed \u003d true;\n     if (cacheCleaner !\u003d null) {\n       executor.remove(cacheCleaner);\n     }\n     for (Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d map.entries().iterator();\n-          iter.hasNext();\n-          iter \u003d map.entries().iterator()) {\n+          iter.hasNext();) {\n       Entry\u003cKey, Value\u003e entry \u003d iter.next();\n       entry.getValue().close();\n       iter.remove();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() {\n    if (closed) return;\n    closed \u003d true;\n    if (cacheCleaner !\u003d null) {\n      executor.remove(cacheCleaner);\n    }\n    for (Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d map.entries().iterator();\n          iter.hasNext();) {\n      Entry\u003cKey, Value\u003e entry \u003d iter.next();\n      entry.getValue().close();\n      iter.remove();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/FileInputStreamCache.java",
      "extendedDetails": {}
    },
    "9a4030e0e84a688c12daa21fe9a165808c3eca70": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4356. BlockReaderLocal should use passed file descriptors rather than paths. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1432335 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/01/13 3:52 PM",
      "commitName": "9a4030e0e84a688c12daa21fe9a165808c3eca70",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,14 @@\n+  public synchronized void close() {\n+    if (closed) return;\n+    closed \u003d true;\n+    if (cacheCleaner !\u003d null) {\n+      executor.remove(cacheCleaner);\n+    }\n+    for (Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d map.entries().iterator();\n+          iter.hasNext();\n+          iter \u003d map.entries().iterator()) {\n+      Entry\u003cKey, Value\u003e entry \u003d iter.next();\n+      entry.getValue().close();\n+      iter.remove();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void close() {\n    if (closed) return;\n    closed \u003d true;\n    if (cacheCleaner !\u003d null) {\n      executor.remove(cacheCleaner);\n    }\n    for (Iterator\u003cEntry\u003cKey, Value\u003e\u003e iter \u003d map.entries().iterator();\n          iter.hasNext();\n          iter \u003d map.entries().iterator()) {\n      Entry\u003cKey, Value\u003e entry \u003d iter.next();\n      entry.getValue().close();\n      iter.remove();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/FileInputStreamCache.java"
    }
  }
}