{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Journal.java",
  "functionName": "doUpgrade",
  "functionId": "doUpgrade___sInfo-StorageInfo",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
  "functionStartLine": 1119,
  "functionEndLine": 1159,
  "numCommitsSeen": 64,
  "timeTaken": 3860,
  "changeHistory": [
    "7217494f40dd99068a3f3b155261b1dac6c67828",
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
    "6beb25ab7e4f5454dba0315a296081e61753f301",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "e9c37de485f8d4dcb04afb0d4cb887cc09d317c9",
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de"
  ],
  "changeHistoryShort": {
    "7217494f40dd99068a3f3b155261b1dac6c67828": "Ybodychange",
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": "Ybodychange",
    "6beb25ab7e4f5454dba0315a296081e61753f301": "Ybodychange",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "e9c37de485f8d4dcb04afb0d4cb887cc09d317c9": "Ybodychange",
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7217494f40dd99068a3f3b155261b1dac6c67828": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10659. Namenode crashes after Journalnode re-installation in an HA cluster due to missing paxos directory. Contributed by star, Hanisha Koneru.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "10/06/19 1:45 PM",
      "commitName": "7217494f40dd99068a3f3b155261b1dac6c67828",
      "commitAuthor": "Hanisha Koneru",
      "commitDateOld": "02/05/19 12:58 PM",
      "commitNameOld": "7a3188d054481b9bd563e337901e93476303ce7f",
      "commitAuthorOld": "Giovanni Matteo Fumarola",
      "daysBetweenCommits": 39.03,
      "commitsBetweenForRepo": 231,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   public synchronized void doUpgrade(StorageInfo sInfo) throws IOException {\n     long oldCTime \u003d storage.getCTime();\n     storage.cTime \u003d sInfo.cTime;\n     int oldLV \u003d storage.getLayoutVersion();\n     storage.layoutVersion \u003d sInfo.layoutVersion;\n     LOG.info(\"Starting upgrade of edits directory: \" + storage.getRoot()\n         + \".\\n   old LV \u003d \" + oldLV\n         + \"; old CTime \u003d \" + oldCTime\n         + \".\\n   new LV \u003d \" + storage.getLayoutVersion()\n         + \"; new CTime \u003d \" + storage.getCTime());\n     storage.getJournalManager().doUpgrade(storage);\n-    storage.createPaxosDir();\n+    storage.getOrCreatePaxosDir();\n     \n     // Copy over the contents of the epoch data files to the new dir.\n     File currentDir \u003d storage.getSingularStorageDir().getCurrentDir();\n     File previousDir \u003d storage.getSingularStorageDir().getPreviousDir();\n     \n     PersistentLongFile prevLastPromisedEpoch \u003d new PersistentLongFile(\n         new File(previousDir, LAST_PROMISED_FILENAME), 0);\n     PersistentLongFile prevLastWriterEpoch \u003d new PersistentLongFile(\n         new File(previousDir, LAST_WRITER_EPOCH), 0);\n     BestEffortLongFile prevCommittedTxnId \u003d new BestEffortLongFile(\n         new File(previousDir, COMMITTED_TXID_FILENAME),\n         HdfsServerConstants.INVALID_TXID);\n \n     lastPromisedEpoch \u003d new PersistentLongFile(\n         new File(currentDir, LAST_PROMISED_FILENAME), 0);\n     lastWriterEpoch \u003d new PersistentLongFile(\n         new File(currentDir, LAST_WRITER_EPOCH), 0);\n     committedTxnId \u003d new BestEffortLongFile(\n         new File(currentDir, COMMITTED_TXID_FILENAME),\n         HdfsServerConstants.INVALID_TXID);\n \n     try {\n       lastPromisedEpoch.set(prevLastPromisedEpoch.get());\n       lastWriterEpoch.set(prevLastWriterEpoch.get());\n       committedTxnId.set(prevCommittedTxnId.get());\n     } finally {\n       IOUtils.cleanupWithLogger(LOG, prevCommittedTxnId);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void doUpgrade(StorageInfo sInfo) throws IOException {\n    long oldCTime \u003d storage.getCTime();\n    storage.cTime \u003d sInfo.cTime;\n    int oldLV \u003d storage.getLayoutVersion();\n    storage.layoutVersion \u003d sInfo.layoutVersion;\n    LOG.info(\"Starting upgrade of edits directory: \" + storage.getRoot()\n        + \".\\n   old LV \u003d \" + oldLV\n        + \"; old CTime \u003d \" + oldCTime\n        + \".\\n   new LV \u003d \" + storage.getLayoutVersion()\n        + \"; new CTime \u003d \" + storage.getCTime());\n    storage.getJournalManager().doUpgrade(storage);\n    storage.getOrCreatePaxosDir();\n    \n    // Copy over the contents of the epoch data files to the new dir.\n    File currentDir \u003d storage.getSingularStorageDir().getCurrentDir();\n    File previousDir \u003d storage.getSingularStorageDir().getPreviousDir();\n    \n    PersistentLongFile prevLastPromisedEpoch \u003d new PersistentLongFile(\n        new File(previousDir, LAST_PROMISED_FILENAME), 0);\n    PersistentLongFile prevLastWriterEpoch \u003d new PersistentLongFile(\n        new File(previousDir, LAST_WRITER_EPOCH), 0);\n    BestEffortLongFile prevCommittedTxnId \u003d new BestEffortLongFile(\n        new File(previousDir, COMMITTED_TXID_FILENAME),\n        HdfsServerConstants.INVALID_TXID);\n\n    lastPromisedEpoch \u003d new PersistentLongFile(\n        new File(currentDir, LAST_PROMISED_FILENAME), 0);\n    lastWriterEpoch \u003d new PersistentLongFile(\n        new File(currentDir, LAST_WRITER_EPOCH), 0);\n    committedTxnId \u003d new BestEffortLongFile(\n        new File(currentDir, COMMITTED_TXID_FILENAME),\n        HdfsServerConstants.INVALID_TXID);\n\n    try {\n      lastPromisedEpoch.set(prevLastPromisedEpoch.get());\n      lastWriterEpoch.set(prevLastWriterEpoch.get());\n      committedTxnId.set(prevCommittedTxnId.get());\n    } finally {\n      IOUtils.cleanupWithLogger(LOG, prevCommittedTxnId);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "eca1a4bfe952fc184fe90dde50bac9b0e5293568": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13695. Move logging to slf4j in HDFS package. Contributed by Ian Pickering.\n",
      "commitDate": "06/09/18 2:48 PM",
      "commitName": "eca1a4bfe952fc184fe90dde50bac9b0e5293568",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "23/08/18 7:44 PM",
      "commitNameOld": "96c4575d7373079becfa3e3db29ba98e6fb86388",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 13.79,
      "commitsBetweenForRepo": 84,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   public synchronized void doUpgrade(StorageInfo sInfo) throws IOException {\n     long oldCTime \u003d storage.getCTime();\n     storage.cTime \u003d sInfo.cTime;\n     int oldLV \u003d storage.getLayoutVersion();\n     storage.layoutVersion \u003d sInfo.layoutVersion;\n     LOG.info(\"Starting upgrade of edits directory: \" + storage.getRoot()\n         + \".\\n   old LV \u003d \" + oldLV\n         + \"; old CTime \u003d \" + oldCTime\n         + \".\\n   new LV \u003d \" + storage.getLayoutVersion()\n         + \"; new CTime \u003d \" + storage.getCTime());\n     storage.getJournalManager().doUpgrade(storage);\n     storage.createPaxosDir();\n     \n     // Copy over the contents of the epoch data files to the new dir.\n     File currentDir \u003d storage.getSingularStorageDir().getCurrentDir();\n     File previousDir \u003d storage.getSingularStorageDir().getPreviousDir();\n     \n     PersistentLongFile prevLastPromisedEpoch \u003d new PersistentLongFile(\n         new File(previousDir, LAST_PROMISED_FILENAME), 0);\n     PersistentLongFile prevLastWriterEpoch \u003d new PersistentLongFile(\n         new File(previousDir, LAST_WRITER_EPOCH), 0);\n     BestEffortLongFile prevCommittedTxnId \u003d new BestEffortLongFile(\n         new File(previousDir, COMMITTED_TXID_FILENAME),\n         HdfsServerConstants.INVALID_TXID);\n \n     lastPromisedEpoch \u003d new PersistentLongFile(\n         new File(currentDir, LAST_PROMISED_FILENAME), 0);\n     lastWriterEpoch \u003d new PersistentLongFile(\n         new File(currentDir, LAST_WRITER_EPOCH), 0);\n     committedTxnId \u003d new BestEffortLongFile(\n         new File(currentDir, COMMITTED_TXID_FILENAME),\n         HdfsServerConstants.INVALID_TXID);\n \n     try {\n       lastPromisedEpoch.set(prevLastPromisedEpoch.get());\n       lastWriterEpoch.set(prevLastWriterEpoch.get());\n       committedTxnId.set(prevCommittedTxnId.get());\n     } finally {\n-      IOUtils.cleanup(LOG, prevCommittedTxnId);\n+      IOUtils.cleanupWithLogger(LOG, prevCommittedTxnId);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void doUpgrade(StorageInfo sInfo) throws IOException {\n    long oldCTime \u003d storage.getCTime();\n    storage.cTime \u003d sInfo.cTime;\n    int oldLV \u003d storage.getLayoutVersion();\n    storage.layoutVersion \u003d sInfo.layoutVersion;\n    LOG.info(\"Starting upgrade of edits directory: \" + storage.getRoot()\n        + \".\\n   old LV \u003d \" + oldLV\n        + \"; old CTime \u003d \" + oldCTime\n        + \".\\n   new LV \u003d \" + storage.getLayoutVersion()\n        + \"; new CTime \u003d \" + storage.getCTime());\n    storage.getJournalManager().doUpgrade(storage);\n    storage.createPaxosDir();\n    \n    // Copy over the contents of the epoch data files to the new dir.\n    File currentDir \u003d storage.getSingularStorageDir().getCurrentDir();\n    File previousDir \u003d storage.getSingularStorageDir().getPreviousDir();\n    \n    PersistentLongFile prevLastPromisedEpoch \u003d new PersistentLongFile(\n        new File(previousDir, LAST_PROMISED_FILENAME), 0);\n    PersistentLongFile prevLastWriterEpoch \u003d new PersistentLongFile(\n        new File(previousDir, LAST_WRITER_EPOCH), 0);\n    BestEffortLongFile prevCommittedTxnId \u003d new BestEffortLongFile(\n        new File(previousDir, COMMITTED_TXID_FILENAME),\n        HdfsServerConstants.INVALID_TXID);\n\n    lastPromisedEpoch \u003d new PersistentLongFile(\n        new File(currentDir, LAST_PROMISED_FILENAME), 0);\n    lastWriterEpoch \u003d new PersistentLongFile(\n        new File(currentDir, LAST_WRITER_EPOCH), 0);\n    committedTxnId \u003d new BestEffortLongFile(\n        new File(currentDir, COMMITTED_TXID_FILENAME),\n        HdfsServerConstants.INVALID_TXID);\n\n    try {\n      lastPromisedEpoch.set(prevLastPromisedEpoch.get());\n      lastWriterEpoch.set(prevLastWriterEpoch.get());\n      committedTxnId.set(prevCommittedTxnId.get());\n    } finally {\n      IOUtils.cleanupWithLogger(LOG, prevCommittedTxnId);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "6beb25ab7e4f5454dba0315a296081e61753f301": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13544. Improve logging for JournalNode in federated cluster.\n",
      "commitDate": "14/05/18 10:12 AM",
      "commitName": "6beb25ab7e4f5454dba0315a296081e61753f301",
      "commitAuthor": "Hanisha Koneru",
      "commitDateOld": "13/10/17 2:22 PM",
      "commitNameOld": "8dd1eeb94fef59feaf19182dd8f1fcf1389c7f34",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 212.83,
      "commitsBetweenForRepo": 2051,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   public synchronized void doUpgrade(StorageInfo sInfo) throws IOException {\n     long oldCTime \u003d storage.getCTime();\n     storage.cTime \u003d sInfo.cTime;\n     int oldLV \u003d storage.getLayoutVersion();\n     storage.layoutVersion \u003d sInfo.layoutVersion;\n-    LOG.info(\"Starting upgrade of edits directory: \"\n+    LOG.info(\"Starting upgrade of edits directory: \" + storage.getRoot()\n         + \".\\n   old LV \u003d \" + oldLV\n         + \"; old CTime \u003d \" + oldCTime\n         + \".\\n   new LV \u003d \" + storage.getLayoutVersion()\n         + \"; new CTime \u003d \" + storage.getCTime());\n     storage.getJournalManager().doUpgrade(storage);\n     storage.createPaxosDir();\n     \n     // Copy over the contents of the epoch data files to the new dir.\n     File currentDir \u003d storage.getSingularStorageDir().getCurrentDir();\n     File previousDir \u003d storage.getSingularStorageDir().getPreviousDir();\n     \n     PersistentLongFile prevLastPromisedEpoch \u003d new PersistentLongFile(\n         new File(previousDir, LAST_PROMISED_FILENAME), 0);\n     PersistentLongFile prevLastWriterEpoch \u003d new PersistentLongFile(\n         new File(previousDir, LAST_WRITER_EPOCH), 0);\n     BestEffortLongFile prevCommittedTxnId \u003d new BestEffortLongFile(\n         new File(previousDir, COMMITTED_TXID_FILENAME),\n         HdfsServerConstants.INVALID_TXID);\n \n     lastPromisedEpoch \u003d new PersistentLongFile(\n         new File(currentDir, LAST_PROMISED_FILENAME), 0);\n     lastWriterEpoch \u003d new PersistentLongFile(\n         new File(currentDir, LAST_WRITER_EPOCH), 0);\n     committedTxnId \u003d new BestEffortLongFile(\n         new File(currentDir, COMMITTED_TXID_FILENAME),\n         HdfsServerConstants.INVALID_TXID);\n \n     try {\n       lastPromisedEpoch.set(prevLastPromisedEpoch.get());\n       lastWriterEpoch.set(prevLastWriterEpoch.get());\n       committedTxnId.set(prevCommittedTxnId.get());\n     } finally {\n       IOUtils.cleanup(LOG, prevCommittedTxnId);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void doUpgrade(StorageInfo sInfo) throws IOException {\n    long oldCTime \u003d storage.getCTime();\n    storage.cTime \u003d sInfo.cTime;\n    int oldLV \u003d storage.getLayoutVersion();\n    storage.layoutVersion \u003d sInfo.layoutVersion;\n    LOG.info(\"Starting upgrade of edits directory: \" + storage.getRoot()\n        + \".\\n   old LV \u003d \" + oldLV\n        + \"; old CTime \u003d \" + oldCTime\n        + \".\\n   new LV \u003d \" + storage.getLayoutVersion()\n        + \"; new CTime \u003d \" + storage.getCTime());\n    storage.getJournalManager().doUpgrade(storage);\n    storage.createPaxosDir();\n    \n    // Copy over the contents of the epoch data files to the new dir.\n    File currentDir \u003d storage.getSingularStorageDir().getCurrentDir();\n    File previousDir \u003d storage.getSingularStorageDir().getPreviousDir();\n    \n    PersistentLongFile prevLastPromisedEpoch \u003d new PersistentLongFile(\n        new File(previousDir, LAST_PROMISED_FILENAME), 0);\n    PersistentLongFile prevLastWriterEpoch \u003d new PersistentLongFile(\n        new File(previousDir, LAST_WRITER_EPOCH), 0);\n    BestEffortLongFile prevCommittedTxnId \u003d new BestEffortLongFile(\n        new File(previousDir, COMMITTED_TXID_FILENAME),\n        HdfsServerConstants.INVALID_TXID);\n\n    lastPromisedEpoch \u003d new PersistentLongFile(\n        new File(currentDir, LAST_PROMISED_FILENAME), 0);\n    lastWriterEpoch \u003d new PersistentLongFile(\n        new File(currentDir, LAST_WRITER_EPOCH), 0);\n    committedTxnId \u003d new BestEffortLongFile(\n        new File(currentDir, COMMITTED_TXID_FILENAME),\n        HdfsServerConstants.INVALID_TXID);\n\n    try {\n      lastPromisedEpoch.set(prevLastPromisedEpoch.get());\n      lastWriterEpoch.set(prevLastWriterEpoch.get());\n      committedTxnId.set(prevCommittedTxnId.get());\n    } finally {\n      IOUtils.cleanup(LOG, prevCommittedTxnId);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "08/01/15 4:09 PM",
      "commitNameOld": "ae91b13a4b1896b893268253104f935c3078d345",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 113.7,
      "commitsBetweenForRepo": 1004,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,41 +1,41 @@\n   public synchronized void doUpgrade(StorageInfo sInfo) throws IOException {\n     long oldCTime \u003d storage.getCTime();\n     storage.cTime \u003d sInfo.cTime;\n     int oldLV \u003d storage.getLayoutVersion();\n     storage.layoutVersion \u003d sInfo.layoutVersion;\n     LOG.info(\"Starting upgrade of edits directory: \"\n         + \".\\n   old LV \u003d \" + oldLV\n         + \"; old CTime \u003d \" + oldCTime\n         + \".\\n   new LV \u003d \" + storage.getLayoutVersion()\n         + \"; new CTime \u003d \" + storage.getCTime());\n     storage.getJournalManager().doUpgrade(storage);\n     storage.createPaxosDir();\n     \n     // Copy over the contents of the epoch data files to the new dir.\n     File currentDir \u003d storage.getSingularStorageDir().getCurrentDir();\n     File previousDir \u003d storage.getSingularStorageDir().getPreviousDir();\n     \n     PersistentLongFile prevLastPromisedEpoch \u003d new PersistentLongFile(\n         new File(previousDir, LAST_PROMISED_FILENAME), 0);\n     PersistentLongFile prevLastWriterEpoch \u003d new PersistentLongFile(\n         new File(previousDir, LAST_WRITER_EPOCH), 0);\n     BestEffortLongFile prevCommittedTxnId \u003d new BestEffortLongFile(\n         new File(previousDir, COMMITTED_TXID_FILENAME),\n-        HdfsConstants.INVALID_TXID);\n+        HdfsServerConstants.INVALID_TXID);\n \n     lastPromisedEpoch \u003d new PersistentLongFile(\n         new File(currentDir, LAST_PROMISED_FILENAME), 0);\n     lastWriterEpoch \u003d new PersistentLongFile(\n         new File(currentDir, LAST_WRITER_EPOCH), 0);\n     committedTxnId \u003d new BestEffortLongFile(\n         new File(currentDir, COMMITTED_TXID_FILENAME),\n-        HdfsConstants.INVALID_TXID);\n+        HdfsServerConstants.INVALID_TXID);\n \n     try {\n       lastPromisedEpoch.set(prevLastPromisedEpoch.get());\n       lastWriterEpoch.set(prevLastWriterEpoch.get());\n       committedTxnId.set(prevCommittedTxnId.get());\n     } finally {\n       IOUtils.cleanup(LOG, prevCommittedTxnId);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void doUpgrade(StorageInfo sInfo) throws IOException {\n    long oldCTime \u003d storage.getCTime();\n    storage.cTime \u003d sInfo.cTime;\n    int oldLV \u003d storage.getLayoutVersion();\n    storage.layoutVersion \u003d sInfo.layoutVersion;\n    LOG.info(\"Starting upgrade of edits directory: \"\n        + \".\\n   old LV \u003d \" + oldLV\n        + \"; old CTime \u003d \" + oldCTime\n        + \".\\n   new LV \u003d \" + storage.getLayoutVersion()\n        + \"; new CTime \u003d \" + storage.getCTime());\n    storage.getJournalManager().doUpgrade(storage);\n    storage.createPaxosDir();\n    \n    // Copy over the contents of the epoch data files to the new dir.\n    File currentDir \u003d storage.getSingularStorageDir().getCurrentDir();\n    File previousDir \u003d storage.getSingularStorageDir().getPreviousDir();\n    \n    PersistentLongFile prevLastPromisedEpoch \u003d new PersistentLongFile(\n        new File(previousDir, LAST_PROMISED_FILENAME), 0);\n    PersistentLongFile prevLastWriterEpoch \u003d new PersistentLongFile(\n        new File(previousDir, LAST_WRITER_EPOCH), 0);\n    BestEffortLongFile prevCommittedTxnId \u003d new BestEffortLongFile(\n        new File(previousDir, COMMITTED_TXID_FILENAME),\n        HdfsServerConstants.INVALID_TXID);\n\n    lastPromisedEpoch \u003d new PersistentLongFile(\n        new File(currentDir, LAST_PROMISED_FILENAME), 0);\n    lastWriterEpoch \u003d new PersistentLongFile(\n        new File(currentDir, LAST_WRITER_EPOCH), 0);\n    committedTxnId \u003d new BestEffortLongFile(\n        new File(currentDir, COMMITTED_TXID_FILENAME),\n        HdfsServerConstants.INVALID_TXID);\n\n    try {\n      lastPromisedEpoch.set(prevLastPromisedEpoch.get());\n      lastWriterEpoch.set(prevLastWriterEpoch.get());\n      committedTxnId.set(prevCommittedTxnId.get());\n    } finally {\n      IOUtils.cleanup(LOG, prevCommittedTxnId);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "e9c37de485f8d4dcb04afb0d4cb887cc09d317c9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7131. During HA upgrade, JournalNode should create a new committedTxnId file in the current directory. Contributed by Jing Zhao.\n",
      "commitDate": "25/09/14 5:15 PM",
      "commitName": "e9c37de485f8d4dcb04afb0d4cb887cc09d317c9",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "11/09/14 12:16 PM",
      "commitNameOld": "80ac6aabcea9f808fd55504cdaef2da7b50da7f1",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 14.21,
      "commitsBetweenForRepo": 156,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,41 @@\n   public synchronized void doUpgrade(StorageInfo sInfo) throws IOException {\n     long oldCTime \u003d storage.getCTime();\n     storage.cTime \u003d sInfo.cTime;\n     int oldLV \u003d storage.getLayoutVersion();\n     storage.layoutVersion \u003d sInfo.layoutVersion;\n     LOG.info(\"Starting upgrade of edits directory: \"\n         + \".\\n   old LV \u003d \" + oldLV\n         + \"; old CTime \u003d \" + oldCTime\n         + \".\\n   new LV \u003d \" + storage.getLayoutVersion()\n         + \"; new CTime \u003d \" + storage.getCTime());\n     storage.getJournalManager().doUpgrade(storage);\n     storage.createPaxosDir();\n     \n     // Copy over the contents of the epoch data files to the new dir.\n     File currentDir \u003d storage.getSingularStorageDir().getCurrentDir();\n     File previousDir \u003d storage.getSingularStorageDir().getPreviousDir();\n     \n     PersistentLongFile prevLastPromisedEpoch \u003d new PersistentLongFile(\n         new File(previousDir, LAST_PROMISED_FILENAME), 0);\n     PersistentLongFile prevLastWriterEpoch \u003d new PersistentLongFile(\n         new File(previousDir, LAST_WRITER_EPOCH), 0);\n-    \n+    BestEffortLongFile prevCommittedTxnId \u003d new BestEffortLongFile(\n+        new File(previousDir, COMMITTED_TXID_FILENAME),\n+        HdfsConstants.INVALID_TXID);\n+\n     lastPromisedEpoch \u003d new PersistentLongFile(\n         new File(currentDir, LAST_PROMISED_FILENAME), 0);\n     lastWriterEpoch \u003d new PersistentLongFile(\n         new File(currentDir, LAST_WRITER_EPOCH), 0);\n-    \n-    lastPromisedEpoch.set(prevLastPromisedEpoch.get());\n-    lastWriterEpoch.set(prevLastWriterEpoch.get());\n+    committedTxnId \u003d new BestEffortLongFile(\n+        new File(currentDir, COMMITTED_TXID_FILENAME),\n+        HdfsConstants.INVALID_TXID);\n+\n+    try {\n+      lastPromisedEpoch.set(prevLastPromisedEpoch.get());\n+      lastWriterEpoch.set(prevLastWriterEpoch.get());\n+      committedTxnId.set(prevCommittedTxnId.get());\n+    } finally {\n+      IOUtils.cleanup(LOG, prevCommittedTxnId);\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void doUpgrade(StorageInfo sInfo) throws IOException {\n    long oldCTime \u003d storage.getCTime();\n    storage.cTime \u003d sInfo.cTime;\n    int oldLV \u003d storage.getLayoutVersion();\n    storage.layoutVersion \u003d sInfo.layoutVersion;\n    LOG.info(\"Starting upgrade of edits directory: \"\n        + \".\\n   old LV \u003d \" + oldLV\n        + \"; old CTime \u003d \" + oldCTime\n        + \".\\n   new LV \u003d \" + storage.getLayoutVersion()\n        + \"; new CTime \u003d \" + storage.getCTime());\n    storage.getJournalManager().doUpgrade(storage);\n    storage.createPaxosDir();\n    \n    // Copy over the contents of the epoch data files to the new dir.\n    File currentDir \u003d storage.getSingularStorageDir().getCurrentDir();\n    File previousDir \u003d storage.getSingularStorageDir().getPreviousDir();\n    \n    PersistentLongFile prevLastPromisedEpoch \u003d new PersistentLongFile(\n        new File(previousDir, LAST_PROMISED_FILENAME), 0);\n    PersistentLongFile prevLastWriterEpoch \u003d new PersistentLongFile(\n        new File(previousDir, LAST_WRITER_EPOCH), 0);\n    BestEffortLongFile prevCommittedTxnId \u003d new BestEffortLongFile(\n        new File(previousDir, COMMITTED_TXID_FILENAME),\n        HdfsConstants.INVALID_TXID);\n\n    lastPromisedEpoch \u003d new PersistentLongFile(\n        new File(currentDir, LAST_PROMISED_FILENAME), 0);\n    lastWriterEpoch \u003d new PersistentLongFile(\n        new File(currentDir, LAST_WRITER_EPOCH), 0);\n    committedTxnId \u003d new BestEffortLongFile(\n        new File(currentDir, COMMITTED_TXID_FILENAME),\n        HdfsConstants.INVALID_TXID);\n\n    try {\n      lastPromisedEpoch.set(prevLastPromisedEpoch.get());\n      lastWriterEpoch.set(prevLastWriterEpoch.get());\n      committedTxnId.set(prevCommittedTxnId.get());\n    } finally {\n      IOUtils.cleanup(LOG, prevCommittedTxnId);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5138. Support HDFS upgrade in HA. Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1561381 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/01/14 12:01 PM",
      "commitName": "edb6dc5f303093c2604cd07b0c0dacf12dbce5de",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,30 @@\n+  public synchronized void doUpgrade(StorageInfo sInfo) throws IOException {\n+    long oldCTime \u003d storage.getCTime();\n+    storage.cTime \u003d sInfo.cTime;\n+    int oldLV \u003d storage.getLayoutVersion();\n+    storage.layoutVersion \u003d sInfo.layoutVersion;\n+    LOG.info(\"Starting upgrade of edits directory: \"\n+        + \".\\n   old LV \u003d \" + oldLV\n+        + \"; old CTime \u003d \" + oldCTime\n+        + \".\\n   new LV \u003d \" + storage.getLayoutVersion()\n+        + \"; new CTime \u003d \" + storage.getCTime());\n+    storage.getJournalManager().doUpgrade(storage);\n+    storage.createPaxosDir();\n+    \n+    // Copy over the contents of the epoch data files to the new dir.\n+    File currentDir \u003d storage.getSingularStorageDir().getCurrentDir();\n+    File previousDir \u003d storage.getSingularStorageDir().getPreviousDir();\n+    \n+    PersistentLongFile prevLastPromisedEpoch \u003d new PersistentLongFile(\n+        new File(previousDir, LAST_PROMISED_FILENAME), 0);\n+    PersistentLongFile prevLastWriterEpoch \u003d new PersistentLongFile(\n+        new File(previousDir, LAST_WRITER_EPOCH), 0);\n+    \n+    lastPromisedEpoch \u003d new PersistentLongFile(\n+        new File(currentDir, LAST_PROMISED_FILENAME), 0);\n+    lastWriterEpoch \u003d new PersistentLongFile(\n+        new File(currentDir, LAST_WRITER_EPOCH), 0);\n+    \n+    lastPromisedEpoch.set(prevLastPromisedEpoch.get());\n+    lastWriterEpoch.set(prevLastWriterEpoch.get());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void doUpgrade(StorageInfo sInfo) throws IOException {\n    long oldCTime \u003d storage.getCTime();\n    storage.cTime \u003d sInfo.cTime;\n    int oldLV \u003d storage.getLayoutVersion();\n    storage.layoutVersion \u003d sInfo.layoutVersion;\n    LOG.info(\"Starting upgrade of edits directory: \"\n        + \".\\n   old LV \u003d \" + oldLV\n        + \"; old CTime \u003d \" + oldCTime\n        + \".\\n   new LV \u003d \" + storage.getLayoutVersion()\n        + \"; new CTime \u003d \" + storage.getCTime());\n    storage.getJournalManager().doUpgrade(storage);\n    storage.createPaxosDir();\n    \n    // Copy over the contents of the epoch data files to the new dir.\n    File currentDir \u003d storage.getSingularStorageDir().getCurrentDir();\n    File previousDir \u003d storage.getSingularStorageDir().getPreviousDir();\n    \n    PersistentLongFile prevLastPromisedEpoch \u003d new PersistentLongFile(\n        new File(previousDir, LAST_PROMISED_FILENAME), 0);\n    PersistentLongFile prevLastWriterEpoch \u003d new PersistentLongFile(\n        new File(previousDir, LAST_WRITER_EPOCH), 0);\n    \n    lastPromisedEpoch \u003d new PersistentLongFile(\n        new File(currentDir, LAST_PROMISED_FILENAME), 0);\n    lastWriterEpoch \u003d new PersistentLongFile(\n        new File(currentDir, LAST_WRITER_EPOCH), 0);\n    \n    lastPromisedEpoch.set(prevLastPromisedEpoch.get());\n    lastWriterEpoch.set(prevLastWriterEpoch.get());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java"
    }
  }
}