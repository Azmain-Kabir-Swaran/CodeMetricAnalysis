{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirXAttrOp.java",
  "functionName": "checkXAttrSize",
  "functionId": "checkXAttrSize___fsd-FSDirectory__xAttr-XAttr",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java",
  "functionStartLine": 446,
  "functionEndLine": 457,
  "numCommitsSeen": 509,
  "timeTaken": 14265,
  "changeHistory": [
    "6ae39199dac6ac7be6802b31452552c76da16e24",
    "eee0d4563c62647cfaaed6605ee713aaf69add78",
    "6c5bbd7a42d1e8b4416fd8870fd60c67867b35c9",
    "6a5596e3b4443462fc86f800b3c2eb839d44c3bd",
    "ac23a55547716df29b3e25c98a113399e184d9d1"
  ],
  "changeHistoryShort": {
    "6ae39199dac6ac7be6802b31452552c76da16e24": "Ybodychange",
    "eee0d4563c62647cfaaed6605ee713aaf69add78": "Ybodychange",
    "6c5bbd7a42d1e8b4416fd8870fd60c67867b35c9": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "6a5596e3b4443462fc86f800b3c2eb839d44c3bd": "Ybodychange",
    "ac23a55547716df29b3e25c98a113399e184d9d1": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6ae39199dac6ac7be6802b31452552c76da16e24": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10662. Optimize UTF8 string/byte conversions. Contributed by Daryn Sharp.\n",
      "commitDate": "04/08/16 7:07 AM",
      "commitName": "6ae39199dac6ac7be6802b31452552c76da16e24",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "01/03/16 9:43 PM",
      "commitNameOld": "27e0681f28ee896ada163bbbc08fd44d113e7d15",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 155.35,
      "commitsBetweenForRepo": 1114,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,12 @@\n   private static void checkXAttrSize(FSDirectory fsd, XAttr xAttr) {\n-    int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n+    int size \u003d DFSUtil.string2Bytes(xAttr.getName()).length;\n     if (xAttr.getValue() !\u003d null) {\n       size +\u003d xAttr.getValue().length;\n     }\n     if (size \u003e fsd.getXattrMaxSize()) {\n       throw new HadoopIllegalArgumentException(\n           \"The XAttr is too big. The maximum combined size of the\"\n           + \" name and value is \" + fsd.getXattrMaxSize()\n           + \", but the total size is \" + size);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static void checkXAttrSize(FSDirectory fsd, XAttr xAttr) {\n    int size \u003d DFSUtil.string2Bytes(xAttr.getName()).length;\n    if (xAttr.getValue() !\u003d null) {\n      size +\u003d xAttr.getValue().length;\n    }\n    if (size \u003e fsd.getXattrMaxSize()) {\n      throw new HadoopIllegalArgumentException(\n          \"The XAttr is too big. The maximum combined size of the\"\n          + \" name and value is \" + fsd.getXattrMaxSize()\n          + \", but the total size is \" + size);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java",
      "extendedDetails": {}
    },
    "eee0d4563c62647cfaaed6605ee713aaf69add78": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8900. Compact XAttrs to optimize memory footprint. (yliu)\n",
      "commitDate": "25/08/15 1:16 AM",
      "commitName": "eee0d4563c62647cfaaed6605ee713aaf69add78",
      "commitAuthor": "yliu",
      "commitDateOld": "09/04/15 12:38 PM",
      "commitNameOld": "922b7ed21d1f1460263ca42f709bb9f415d189c5",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 137.53,
      "commitsBetweenForRepo": 1026,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,12 @@\n   private static void checkXAttrSize(FSDirectory fsd, XAttr xAttr) {\n-    if (fsd.getXattrMaxSize() \u003d\u003d 0) {\n-      return;\n-    }\n     int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n     if (xAttr.getValue() !\u003d null) {\n       size +\u003d xAttr.getValue().length;\n     }\n     if (size \u003e fsd.getXattrMaxSize()) {\n       throw new HadoopIllegalArgumentException(\n           \"The XAttr is too big. The maximum combined size of the\"\n           + \" name and value is \" + fsd.getXattrMaxSize()\n           + \", but the total size is \" + size);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static void checkXAttrSize(FSDirectory fsd, XAttr xAttr) {\n    int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n    if (xAttr.getValue() !\u003d null) {\n      size +\u003d xAttr.getValue().length;\n    }\n    if (size \u003e fsd.getXattrMaxSize()) {\n      throw new HadoopIllegalArgumentException(\n          \"The XAttr is too big. The maximum combined size of the\"\n          + \" name and value is \" + fsd.getXattrMaxSize()\n          + \", but the total size is \" + size);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java",
      "extendedDetails": {}
    },
    "6c5bbd7a42d1e8b4416fd8870fd60c67867b35c9": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-7486. Consolidate XAttr-related implementation into a single class. Contributed by Haohui Mai.\n",
      "commitDate": "08/12/14 11:52 AM",
      "commitName": "6c5bbd7a42d1e8b4416fd8870fd60c67867b35c9",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7486. Consolidate XAttr-related implementation into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "08/12/14 11:52 AM",
          "commitName": "6c5bbd7a42d1e8b4416fd8870fd60c67867b35c9",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "08/12/14 11:08 AM",
          "commitNameOld": "57cb43be50c81daad8da34d33a45f396d9c1c35b",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,15 @@\n-  private void checkXAttrSize(XAttr xAttr) {\n-    if (xattrMaxSize \u003d\u003d 0) {\n+  private static void checkXAttrSize(FSDirectory fsd, XAttr xAttr) {\n+    if (fsd.getXattrMaxSize() \u003d\u003d 0) {\n       return;\n     }\n     int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n     if (xAttr.getValue() !\u003d null) {\n       size +\u003d xAttr.getValue().length;\n     }\n-    if (size \u003e xattrMaxSize) {\n+    if (size \u003e fsd.getXattrMaxSize()) {\n       throw new HadoopIllegalArgumentException(\n           \"The XAttr is too big. The maximum combined size of the\"\n-          + \" name and value is \" + xattrMaxSize\n+          + \" name and value is \" + fsd.getXattrMaxSize()\n           + \", but the total size is \" + size);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void checkXAttrSize(FSDirectory fsd, XAttr xAttr) {\n    if (fsd.getXattrMaxSize() \u003d\u003d 0) {\n      return;\n    }\n    int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n    if (xAttr.getValue() !\u003d null) {\n      size +\u003d xAttr.getValue().length;\n    }\n    if (size \u003e fsd.getXattrMaxSize()) {\n      throw new HadoopIllegalArgumentException(\n          \"The XAttr is too big. The maximum combined size of the\"\n          + \" name and value is \" + fsd.getXattrMaxSize()\n          + \", but the total size is \" + size);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java",
            "oldMethodName": "checkXAttrSize",
            "newMethodName": "checkXAttrSize"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7486. Consolidate XAttr-related implementation into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "08/12/14 11:52 AM",
          "commitName": "6c5bbd7a42d1e8b4416fd8870fd60c67867b35c9",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "08/12/14 11:08 AM",
          "commitNameOld": "57cb43be50c81daad8da34d33a45f396d9c1c35b",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,15 @@\n-  private void checkXAttrSize(XAttr xAttr) {\n-    if (xattrMaxSize \u003d\u003d 0) {\n+  private static void checkXAttrSize(FSDirectory fsd, XAttr xAttr) {\n+    if (fsd.getXattrMaxSize() \u003d\u003d 0) {\n       return;\n     }\n     int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n     if (xAttr.getValue() !\u003d null) {\n       size +\u003d xAttr.getValue().length;\n     }\n-    if (size \u003e xattrMaxSize) {\n+    if (size \u003e fsd.getXattrMaxSize()) {\n       throw new HadoopIllegalArgumentException(\n           \"The XAttr is too big. The maximum combined size of the\"\n-          + \" name and value is \" + xattrMaxSize\n+          + \" name and value is \" + fsd.getXattrMaxSize()\n           + \", but the total size is \" + size);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void checkXAttrSize(FSDirectory fsd, XAttr xAttr) {\n    if (fsd.getXattrMaxSize() \u003d\u003d 0) {\n      return;\n    }\n    int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n    if (xAttr.getValue() !\u003d null) {\n      size +\u003d xAttr.getValue().length;\n    }\n    if (size \u003e fsd.getXattrMaxSize()) {\n      throw new HadoopIllegalArgumentException(\n          \"The XAttr is too big. The maximum combined size of the\"\n          + \" name and value is \" + fsd.getXattrMaxSize()\n          + \", but the total size is \" + size);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[private, static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7486. Consolidate XAttr-related implementation into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "08/12/14 11:52 AM",
          "commitName": "6c5bbd7a42d1e8b4416fd8870fd60c67867b35c9",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "08/12/14 11:08 AM",
          "commitNameOld": "57cb43be50c81daad8da34d33a45f396d9c1c35b",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,15 @@\n-  private void checkXAttrSize(XAttr xAttr) {\n-    if (xattrMaxSize \u003d\u003d 0) {\n+  private static void checkXAttrSize(FSDirectory fsd, XAttr xAttr) {\n+    if (fsd.getXattrMaxSize() \u003d\u003d 0) {\n       return;\n     }\n     int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n     if (xAttr.getValue() !\u003d null) {\n       size +\u003d xAttr.getValue().length;\n     }\n-    if (size \u003e xattrMaxSize) {\n+    if (size \u003e fsd.getXattrMaxSize()) {\n       throw new HadoopIllegalArgumentException(\n           \"The XAttr is too big. The maximum combined size of the\"\n-          + \" name and value is \" + xattrMaxSize\n+          + \" name and value is \" + fsd.getXattrMaxSize()\n           + \", but the total size is \" + size);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void checkXAttrSize(FSDirectory fsd, XAttr xAttr) {\n    if (fsd.getXattrMaxSize() \u003d\u003d 0) {\n      return;\n    }\n    int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n    if (xAttr.getValue() !\u003d null) {\n      size +\u003d xAttr.getValue().length;\n    }\n    if (size \u003e fsd.getXattrMaxSize()) {\n      throw new HadoopIllegalArgumentException(\n          \"The XAttr is too big. The maximum combined size of the\"\n          + \" name and value is \" + fsd.getXattrMaxSize()\n          + \", but the total size is \" + size);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7486. Consolidate XAttr-related implementation into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "08/12/14 11:52 AM",
          "commitName": "6c5bbd7a42d1e8b4416fd8870fd60c67867b35c9",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "08/12/14 11:08 AM",
          "commitNameOld": "57cb43be50c81daad8da34d33a45f396d9c1c35b",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,15 @@\n-  private void checkXAttrSize(XAttr xAttr) {\n-    if (xattrMaxSize \u003d\u003d 0) {\n+  private static void checkXAttrSize(FSDirectory fsd, XAttr xAttr) {\n+    if (fsd.getXattrMaxSize() \u003d\u003d 0) {\n       return;\n     }\n     int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n     if (xAttr.getValue() !\u003d null) {\n       size +\u003d xAttr.getValue().length;\n     }\n-    if (size \u003e xattrMaxSize) {\n+    if (size \u003e fsd.getXattrMaxSize()) {\n       throw new HadoopIllegalArgumentException(\n           \"The XAttr is too big. The maximum combined size of the\"\n-          + \" name and value is \" + xattrMaxSize\n+          + \" name and value is \" + fsd.getXattrMaxSize()\n           + \", but the total size is \" + size);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void checkXAttrSize(FSDirectory fsd, XAttr xAttr) {\n    if (fsd.getXattrMaxSize() \u003d\u003d 0) {\n      return;\n    }\n    int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n    if (xAttr.getValue() !\u003d null) {\n      size +\u003d xAttr.getValue().length;\n    }\n    if (size \u003e fsd.getXattrMaxSize()) {\n      throw new HadoopIllegalArgumentException(\n          \"The XAttr is too big. The maximum combined size of the\"\n          + \" name and value is \" + fsd.getXattrMaxSize()\n          + \", but the total size is \" + size);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java",
          "extendedDetails": {
            "oldValue": "[xAttr-XAttr]",
            "newValue": "[fsd-FSDirectory, xAttr-XAttr]"
          }
        }
      ]
    },
    "6a5596e3b4443462fc86f800b3c2eb839d44c3bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7478. Move org.apache.hadoop.hdfs.server.namenode.NNConf to FSNamesystem. Contributed by Li Lu.\n",
      "commitDate": "05/12/14 10:55 AM",
      "commitName": "6a5596e3b4443462fc86f800b3c2eb839d44c3bd",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/12/14 2:53 PM",
      "commitNameOld": "185e0c7b4c056b88f606362c71e4a22aae7076e0",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 2.83,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,15 @@\n   private void checkXAttrSize(XAttr xAttr) {\n-    if (nnConf.xattrMaxSize \u003d\u003d 0) {\n+    if (xattrMaxSize \u003d\u003d 0) {\n       return;\n     }\n     int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n     if (xAttr.getValue() !\u003d null) {\n       size +\u003d xAttr.getValue().length;\n     }\n-    if (size \u003e nnConf.xattrMaxSize) {\n+    if (size \u003e xattrMaxSize) {\n       throw new HadoopIllegalArgumentException(\n           \"The XAttr is too big. The maximum combined size of the\"\n-          + \" name and value is \" + nnConf.xattrMaxSize\n+          + \" name and value is \" + xattrMaxSize\n           + \", but the total size is \" + size);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void checkXAttrSize(XAttr xAttr) {\n    if (xattrMaxSize \u003d\u003d 0) {\n      return;\n    }\n    int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n    if (xAttr.getValue() !\u003d null) {\n      size +\u003d xAttr.getValue().length;\n    }\n    if (size \u003e xattrMaxSize) {\n      throw new HadoopIllegalArgumentException(\n          \"The XAttr is too big. The maximum combined size of the\"\n          + \" name and value is \" + xattrMaxSize\n          + \", but the total size is \" + size);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "ac23a55547716df29b3e25c98a113399e184d9d1": {
      "type": "Yintroduced",
      "commitMessage": "Merge HDFS-2006 HDFS XAttrs branch to Trunk\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1596575 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/05/14 6:57 AM",
      "commitName": "ac23a55547716df29b3e25c98a113399e184d9d1",
      "commitAuthor": "Uma Maheswara Rao G",
      "diff": "@@ -0,0 +1,15 @@\n+  private void checkXAttrSize(XAttr xAttr) {\n+    if (nnConf.xattrMaxSize \u003d\u003d 0) {\n+      return;\n+    }\n+    int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n+    if (xAttr.getValue() !\u003d null) {\n+      size +\u003d xAttr.getValue().length;\n+    }\n+    if (size \u003e nnConf.xattrMaxSize) {\n+      throw new HadoopIllegalArgumentException(\n+          \"The XAttr is too big. The maximum combined size of the\"\n+          + \" name and value is \" + nnConf.xattrMaxSize\n+          + \", but the total size is \" + size);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void checkXAttrSize(XAttr xAttr) {\n    if (nnConf.xattrMaxSize \u003d\u003d 0) {\n      return;\n    }\n    int size \u003d xAttr.getName().getBytes(Charsets.UTF_8).length;\n    if (xAttr.getValue() !\u003d null) {\n      size +\u003d xAttr.getValue().length;\n    }\n    if (size \u003e nnConf.xattrMaxSize) {\n      throw new HadoopIllegalArgumentException(\n          \"The XAttr is too big. The maximum combined size of the\"\n          + \" name and value is \" + nnConf.xattrMaxSize\n          + \", but the total size is \" + size);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
    }
  }
}