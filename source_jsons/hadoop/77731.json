{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ByteBufferUtil.java",
  "functionName": "fallbackRead",
  "functionId": "fallbackRead___stream-InputStream__bufferPool-ByteBufferPool__maxLength-int",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ByteBufferUtil.java",
  "functionStartLine": 52,
  "functionEndLine": 112,
  "numCommitsSeen": 1,
  "timeTaken": 938,
  "changeHistory": [
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036"
  ],
  "changeHistoryShort": {
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036": "Yintroduced"
  },
  "changeHistoryDetails": {
    "eccdb9aa8bcdee750583d16a1253f1c5faabd036": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5260. Merge zero-copy memory-mapped HDFS client reads to trunk and branch-2. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1527113 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/13 3:51 PM",
      "commitName": "eccdb9aa8bcdee750583d16a1253f1c5faabd036",
      "commitAuthor": "Chris Nauroth",
      "diff": "@@ -0,0 +1,61 @@\n+  public static ByteBuffer fallbackRead(\n+      InputStream stream, ByteBufferPool bufferPool, int maxLength)\n+          throws IOException {\n+    if (bufferPool \u003d\u003d null) {\n+      throw new UnsupportedOperationException(\"zero-copy reads \" +\n+          \"were not available, and you did not provide a fallback \" +\n+          \"ByteBufferPool.\");\n+    }\n+    boolean useDirect \u003d streamHasByteBufferRead(stream);\n+    ByteBuffer buffer \u003d bufferPool.getBuffer(useDirect, maxLength);\n+    if (buffer \u003d\u003d null) {\n+      throw new UnsupportedOperationException(\"zero-copy reads \" +\n+          \"were not available, and the ByteBufferPool did not provide \" +\n+          \"us with \" + (useDirect ? \"a direct\" : \"an indirect\") +\n+          \"buffer.\");\n+    }\n+    Preconditions.checkState(buffer.capacity() \u003e 0);\n+    Preconditions.checkState(buffer.isDirect() \u003d\u003d useDirect);\n+    maxLength \u003d Math.min(maxLength, buffer.capacity());\n+    boolean success \u003d false;\n+    try {\n+      if (useDirect) {\n+        buffer.clear();\n+        buffer.limit(maxLength);\n+        ByteBufferReadable readable \u003d (ByteBufferReadable)stream;\n+        int totalRead \u003d 0;\n+        while (true) {\n+          if (totalRead \u003e\u003d maxLength) {\n+            success \u003d true;\n+            break;\n+          }\n+          int nRead \u003d readable.read(buffer);\n+          if (nRead \u003c 0) {\n+            if (totalRead \u003e 0) {\n+              success \u003d true;\n+            }\n+            break;\n+          }\n+          totalRead +\u003d nRead;\n+        }\n+        buffer.flip();\n+      } else {\n+        buffer.clear();\n+        int nRead \u003d stream.read(buffer.array(),\n+            buffer.arrayOffset(), maxLength);\n+        if (nRead \u003e\u003d 0) {\n+          buffer.limit(nRead);\n+          success \u003d true;\n+        }\n+      }\n+    } finally {\n+      if (!success) {\n+        // If we got an error while reading, or if we are at EOF, we \n+        // don\u0027t need the buffer any more.  We can give it back to the\n+        // bufferPool.\n+        bufferPool.putBuffer(buffer);\n+        buffer \u003d null;\n+      }\n+    }\n+    return buffer;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static ByteBuffer fallbackRead(\n      InputStream stream, ByteBufferPool bufferPool, int maxLength)\n          throws IOException {\n    if (bufferPool \u003d\u003d null) {\n      throw new UnsupportedOperationException(\"zero-copy reads \" +\n          \"were not available, and you did not provide a fallback \" +\n          \"ByteBufferPool.\");\n    }\n    boolean useDirect \u003d streamHasByteBufferRead(stream);\n    ByteBuffer buffer \u003d bufferPool.getBuffer(useDirect, maxLength);\n    if (buffer \u003d\u003d null) {\n      throw new UnsupportedOperationException(\"zero-copy reads \" +\n          \"were not available, and the ByteBufferPool did not provide \" +\n          \"us with \" + (useDirect ? \"a direct\" : \"an indirect\") +\n          \"buffer.\");\n    }\n    Preconditions.checkState(buffer.capacity() \u003e 0);\n    Preconditions.checkState(buffer.isDirect() \u003d\u003d useDirect);\n    maxLength \u003d Math.min(maxLength, buffer.capacity());\n    boolean success \u003d false;\n    try {\n      if (useDirect) {\n        buffer.clear();\n        buffer.limit(maxLength);\n        ByteBufferReadable readable \u003d (ByteBufferReadable)stream;\n        int totalRead \u003d 0;\n        while (true) {\n          if (totalRead \u003e\u003d maxLength) {\n            success \u003d true;\n            break;\n          }\n          int nRead \u003d readable.read(buffer);\n          if (nRead \u003c 0) {\n            if (totalRead \u003e 0) {\n              success \u003d true;\n            }\n            break;\n          }\n          totalRead +\u003d nRead;\n        }\n        buffer.flip();\n      } else {\n        buffer.clear();\n        int nRead \u003d stream.read(buffer.array(),\n            buffer.arrayOffset(), maxLength);\n        if (nRead \u003e\u003d 0) {\n          buffer.limit(nRead);\n          success \u003d true;\n        }\n      }\n    } finally {\n      if (!success) {\n        // If we got an error while reading, or if we are at EOF, we \n        // don\u0027t need the buffer any more.  We can give it back to the\n        // bufferPool.\n        bufferPool.putBuffer(buffer);\n        buffer \u003d null;\n      }\n    }\n    return buffer;\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ByteBufferUtil.java"
    }
  }
}