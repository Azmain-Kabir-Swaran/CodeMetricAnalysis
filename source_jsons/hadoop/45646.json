{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "AggregatedLogFormat.java",
  "functionName": "readAcontainerLogs",
  "functionId": "readAcontainerLogs___valueStream-DataInputStream__writer-Writer",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogFormat.java",
  "functionStartLine": 800,
  "functionEndLine": 803,
  "numCommitsSeen": 56,
  "timeTaken": 2562,
  "changeHistory": [
    "58e9f24e0f06efede21085b7ffe36af042fa7b38",
    "f81dc3f995579c1b94b11d60e9fc6da56c8a9496",
    "e1fdf62123625e4ba399af02f8aad500637d29d1",
    "f58e3c8b121859194185fa12bfbd0ff57e6bcc7f",
    "c27601fefebd0af887a12d684bfc6f90d9fc0321",
    "670fa24b48acb407c22fbfdde87ae3123dcbf449"
  ],
  "changeHistoryShort": {
    "58e9f24e0f06efede21085b7ffe36af042fa7b38": "Ybodychange",
    "f81dc3f995579c1b94b11d60e9fc6da56c8a9496": "Ybodychange",
    "e1fdf62123625e4ba399af02f8aad500637d29d1": "Yfilerename",
    "f58e3c8b121859194185fa12bfbd0ff57e6bcc7f": "Ybodychange",
    "c27601fefebd0af887a12d684bfc6f90d9fc0321": "Yfilerename",
    "670fa24b48acb407c22fbfdde87ae3123dcbf449": "Yintroduced"
  },
  "changeHistoryDetails": {
    "58e9f24e0f06efede21085b7ffe36af042fa7b38": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2788. Fixed backwards compatiblity issues with log-aggregation feature that were caused when adding log-upload-time via YARN-2703. Contributed by Xuan Gong.\n",
      "commitDate": "03/11/14 1:16 PM",
      "commitName": "58e9f24e0f06efede21085b7ffe36af042fa7b38",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "26/10/14 12:57 PM",
      "commitNameOld": "caecd9fffe7c6216be31f3ab65349182045451fa",
      "commitAuthorOld": "Zhijie Shen",
      "daysBetweenCommits": 8.05,
      "commitsBetweenForRepo": 84,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,4 @@\n     public static void readAcontainerLogs(DataInputStream valueStream,\n         Writer writer) throws IOException {\n-      int bufferSize \u003d 65536;\n-      char[] cbuf \u003d new char[bufferSize];\n-      String fileType;\n-      long uploadTime;\n-      String fileLengthStr;\n-      long fileLength;\n-\n-      while (true) {\n-        try {\n-          fileType \u003d valueStream.readUTF();\n-        } catch (EOFException e) {\n-          // EndOfFile\n-          return;\n-        }\n-        uploadTime \u003d valueStream.readLong();\n-        fileLengthStr \u003d valueStream.readUTF();\n-        fileLength \u003d Long.parseLong(fileLengthStr);\n-        writer.write(\"\\n\\nLogType:\");\n-        writer.write(fileType);\n-        writer.write(\"\\nLogUploadTime:\");\n-        writer.write(String.valueOf(uploadTime));\n-        writer.write(\"\\nLogLength:\");\n-        writer.write(fileLengthStr);\n-        writer.write(\"\\nLog Contents:\\n\");\n-        // ByteLevel\n-        BoundedInputStream bis \u003d\n-            new BoundedInputStream(valueStream, fileLength);\n-        InputStreamReader reader \u003d new InputStreamReader(bis);\n-        int currentRead \u003d 0;\n-        int totalRead \u003d 0;\n-        while ((currentRead \u003d reader.read(cbuf, 0, bufferSize)) !\u003d -1) {\n-          writer.write(cbuf, 0, currentRead);\n-          totalRead +\u003d currentRead;\n-        }\n-      }\n+      readAcontainerLogs(valueStream, writer, -1);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public static void readAcontainerLogs(DataInputStream valueStream,\n        Writer writer) throws IOException {\n      readAcontainerLogs(valueStream, writer, -1);\n    }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogFormat.java",
      "extendedDetails": {}
    },
    "f81dc3f995579c1b94b11d60e9fc6da56c8a9496": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2703. Added logUploadedTime into LogValue for better display. Contributed by Xuan Gong.\n",
      "commitDate": "24/10/14 2:10 PM",
      "commitName": "f81dc3f995579c1b94b11d60e9fc6da56c8a9496",
      "commitAuthor": "Zhijie Shen",
      "commitDateOld": "24/10/14 11:13 AM",
      "commitNameOld": "e31f0a6558b106662c83e1f797216e412b6689a9",
      "commitAuthorOld": "Zhijie Shen",
      "daysBetweenCommits": 0.12,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,38 @@\n     public static void readAcontainerLogs(DataInputStream valueStream,\n         Writer writer) throws IOException {\n       int bufferSize \u003d 65536;\n       char[] cbuf \u003d new char[bufferSize];\n       String fileType;\n+      long uploadTime;\n       String fileLengthStr;\n       long fileLength;\n \n       while (true) {\n         try {\n           fileType \u003d valueStream.readUTF();\n         } catch (EOFException e) {\n           // EndOfFile\n           return;\n         }\n+        uploadTime \u003d valueStream.readLong();\n         fileLengthStr \u003d valueStream.readUTF();\n         fileLength \u003d Long.parseLong(fileLengthStr);\n         writer.write(\"\\n\\nLogType:\");\n         writer.write(fileType);\n+        writer.write(\"\\nLogUploadTime:\");\n+        writer.write(String.valueOf(uploadTime));\n         writer.write(\"\\nLogLength:\");\n         writer.write(fileLengthStr);\n         writer.write(\"\\nLog Contents:\\n\");\n         // ByteLevel\n         BoundedInputStream bis \u003d\n             new BoundedInputStream(valueStream, fileLength);\n         InputStreamReader reader \u003d new InputStreamReader(bis);\n         int currentRead \u003d 0;\n         int totalRead \u003d 0;\n         while ((currentRead \u003d reader.read(cbuf, 0, bufferSize)) !\u003d -1) {\n           writer.write(cbuf, 0, currentRead);\n           totalRead +\u003d currentRead;\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public static void readAcontainerLogs(DataInputStream valueStream,\n        Writer writer) throws IOException {\n      int bufferSize \u003d 65536;\n      char[] cbuf \u003d new char[bufferSize];\n      String fileType;\n      long uploadTime;\n      String fileLengthStr;\n      long fileLength;\n\n      while (true) {\n        try {\n          fileType \u003d valueStream.readUTF();\n        } catch (EOFException e) {\n          // EndOfFile\n          return;\n        }\n        uploadTime \u003d valueStream.readLong();\n        fileLengthStr \u003d valueStream.readUTF();\n        fileLength \u003d Long.parseLong(fileLengthStr);\n        writer.write(\"\\n\\nLogType:\");\n        writer.write(fileType);\n        writer.write(\"\\nLogUploadTime:\");\n        writer.write(String.valueOf(uploadTime));\n        writer.write(\"\\nLogLength:\");\n        writer.write(fileLengthStr);\n        writer.write(\"\\nLog Contents:\\n\");\n        // ByteLevel\n        BoundedInputStream bis \u003d\n            new BoundedInputStream(valueStream, fileLength);\n        InputStreamReader reader \u003d new InputStreamReader(bis);\n        int currentRead \u003d 0;\n        int totalRead \u003d 0;\n        while ((currentRead \u003d reader.read(cbuf, 0, bufferSize)) !\u003d -1) {\n          writer.write(cbuf, 0, currentRead);\n          totalRead +\u003d currentRead;\n        }\n      }\n    }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogFormat.java",
      "extendedDetails": {}
    },
    "e1fdf62123625e4ba399af02f8aad500637d29d1": {
      "type": "Yfilerename",
      "commitMessage": "YARN-1. Promote YARN to be a sub-project of Apache Hadoop.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370666 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 10:22 PM",
      "commitName": "e1fdf62123625e4ba399af02f8aad500637d29d1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "07/08/12 7:53 PM",
      "commitNameOld": "34554d1e11ee1d5b564d7d9ed3e6d55931d72749",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public static void readAcontainerLogs(DataInputStream valueStream,\n        Writer writer) throws IOException {\n      int bufferSize \u003d 65536;\n      char[] cbuf \u003d new char[bufferSize];\n      String fileType;\n      String fileLengthStr;\n      long fileLength;\n\n      while (true) {\n        try {\n          fileType \u003d valueStream.readUTF();\n        } catch (EOFException e) {\n          // EndOfFile\n          return;\n        }\n        fileLengthStr \u003d valueStream.readUTF();\n        fileLength \u003d Long.parseLong(fileLengthStr);\n        writer.write(\"\\n\\nLogType:\");\n        writer.write(fileType);\n        writer.write(\"\\nLogLength:\");\n        writer.write(fileLengthStr);\n        writer.write(\"\\nLog Contents:\\n\");\n        // ByteLevel\n        BoundedInputStream bis \u003d\n            new BoundedInputStream(valueStream, fileLength);\n        InputStreamReader reader \u003d new InputStreamReader(bis);\n        int currentRead \u003d 0;\n        int totalRead \u003d 0;\n        while ((currentRead \u003d reader.read(cbuf, 0, bufferSize)) !\u003d -1) {\n          writer.write(cbuf, 0, currentRead);\n          totalRead +\u003d currentRead;\n        }\n      }\n    }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogFormat.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogFormat.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogFormat.java"
      }
    },
    "f58e3c8b121859194185fa12bfbd0ff57e6bcc7f": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4006. History server container log web UI sometimes combines stderr/stdout/syslog contents together (Siddharth Seth via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1301731 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/03/12 1:10 PM",
      "commitName": "f58e3c8b121859194185fa12bfbd0ff57e6bcc7f",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "29/11/11 3:17 PM",
      "commitNameOld": "ea17da82f7fc4b7fcc05bba82d141e27289fd7cb",
      "commitAuthorOld": "Mahadev Konar",
      "daysBetweenCommits": 107.87,
      "commitsBetweenForRepo": 781,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n     public static void readAcontainerLogs(DataInputStream valueStream,\n         Writer writer) throws IOException {\n       int bufferSize \u003d 65536;\n       char[] cbuf \u003d new char[bufferSize];\n       String fileType;\n       String fileLengthStr;\n       long fileLength;\n \n       while (true) {\n         try {\n           fileType \u003d valueStream.readUTF();\n         } catch (EOFException e) {\n           // EndOfFile\n           return;\n         }\n         fileLengthStr \u003d valueStream.readUTF();\n         fileLength \u003d Long.parseLong(fileLengthStr);\n         writer.write(\"\\n\\nLogType:\");\n         writer.write(fileType);\n         writer.write(\"\\nLogLength:\");\n         writer.write(fileLengthStr);\n         writer.write(\"\\nLog Contents:\\n\");\n         // ByteLevel\n         BoundedInputStream bis \u003d\n             new BoundedInputStream(valueStream, fileLength);\n         InputStreamReader reader \u003d new InputStreamReader(bis);\n         int currentRead \u003d 0;\n         int totalRead \u003d 0;\n         while ((currentRead \u003d reader.read(cbuf, 0, bufferSize)) !\u003d -1) {\n-          writer.write(cbuf);\n+          writer.write(cbuf, 0, currentRead);\n           totalRead +\u003d currentRead;\n         }\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public static void readAcontainerLogs(DataInputStream valueStream,\n        Writer writer) throws IOException {\n      int bufferSize \u003d 65536;\n      char[] cbuf \u003d new char[bufferSize];\n      String fileType;\n      String fileLengthStr;\n      long fileLength;\n\n      while (true) {\n        try {\n          fileType \u003d valueStream.readUTF();\n        } catch (EOFException e) {\n          // EndOfFile\n          return;\n        }\n        fileLengthStr \u003d valueStream.readUTF();\n        fileLength \u003d Long.parseLong(fileLengthStr);\n        writer.write(\"\\n\\nLogType:\");\n        writer.write(fileType);\n        writer.write(\"\\nLogLength:\");\n        writer.write(fileLengthStr);\n        writer.write(\"\\nLog Contents:\\n\");\n        // ByteLevel\n        BoundedInputStream bis \u003d\n            new BoundedInputStream(valueStream, fileLength);\n        InputStreamReader reader \u003d new InputStreamReader(bis);\n        int currentRead \u003d 0;\n        int totalRead \u003d 0;\n        while ((currentRead \u003d reader.read(cbuf, 0, bufferSize)) !\u003d -1) {\n          writer.write(cbuf, 0, currentRead);\n          totalRead +\u003d currentRead;\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogFormat.java",
      "extendedDetails": {}
    },
    "c27601fefebd0af887a12d684bfc6f90d9fc0321": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-3297. Moved log related components into yarn-common so that HistoryServer and clients can use them without depending on the yarn-server-nodemanager module. Contributed by Siddharth Seth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1196986 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/11/11 1:02 AM",
      "commitName": "c27601fefebd0af887a12d684bfc6f90d9fc0321",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "03/11/11 12:06 AM",
      "commitNameOld": "0df4878033b797b9313c887ca9d75f8ea104d029",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public static void readAcontainerLogs(DataInputStream valueStream,\n        Writer writer) throws IOException {\n      int bufferSize \u003d 65536;\n      char[] cbuf \u003d new char[bufferSize];\n      String fileType;\n      String fileLengthStr;\n      long fileLength;\n\n      while (true) {\n        try {\n          fileType \u003d valueStream.readUTF();\n        } catch (EOFException e) {\n          // EndOfFile\n          return;\n        }\n        fileLengthStr \u003d valueStream.readUTF();\n        fileLength \u003d Long.parseLong(fileLengthStr);\n        writer.write(\"\\n\\nLogType:\");\n        writer.write(fileType);\n        writer.write(\"\\nLogLength:\");\n        writer.write(fileLengthStr);\n        writer.write(\"\\nLog Contents:\\n\");\n        // ByteLevel\n        BoundedInputStream bis \u003d\n            new BoundedInputStream(valueStream, fileLength);\n        InputStreamReader reader \u003d new InputStreamReader(bis);\n        int currentRead \u003d 0;\n        int totalRead \u003d 0;\n        while ((currentRead \u003d reader.read(cbuf, 0, bufferSize)) !\u003d -1) {\n          writer.write(cbuf);\n          totalRead +\u003d currentRead;\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogFormat.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AggregatedLogFormat.java",
        "newPath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/AggregatedLogFormat.java"
      }
    },
    "670fa24b48acb407c22fbfdde87ae3123dcbf449": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-2989. Modified JobHistory to link to task and AM logs from the JobHistoryServer. Contributed by Siddharth Seth.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1190174 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/10/11 11:45 PM",
      "commitName": "670fa24b48acb407c22fbfdde87ae3123dcbf449",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "diff": "@@ -0,0 +1,34 @@\n+    public static void readAcontainerLogs(DataInputStream valueStream,\n+        Writer writer) throws IOException {\n+      int bufferSize \u003d 65536;\n+      char[] cbuf \u003d new char[bufferSize];\n+      String fileType;\n+      String fileLengthStr;\n+      long fileLength;\n+\n+      while (true) {\n+        try {\n+          fileType \u003d valueStream.readUTF();\n+        } catch (EOFException e) {\n+          // EndOfFile\n+          return;\n+        }\n+        fileLengthStr \u003d valueStream.readUTF();\n+        fileLength \u003d Long.parseLong(fileLengthStr);\n+        writer.write(\"\\n\\nLogType:\");\n+        writer.write(fileType);\n+        writer.write(\"\\nLogLength:\");\n+        writer.write(fileLengthStr);\n+        writer.write(\"\\nLog Contents:\\n\");\n+        // ByteLevel\n+        BoundedInputStream bis \u003d\n+            new BoundedInputStream(valueStream, fileLength);\n+        InputStreamReader reader \u003d new InputStreamReader(bis);\n+        int currentRead \u003d 0;\n+        int totalRead \u003d 0;\n+        while ((currentRead \u003d reader.read(cbuf, 0, bufferSize)) !\u003d -1) {\n+          writer.write(cbuf);\n+          totalRead +\u003d currentRead;\n+        }\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public static void readAcontainerLogs(DataInputStream valueStream,\n        Writer writer) throws IOException {\n      int bufferSize \u003d 65536;\n      char[] cbuf \u003d new char[bufferSize];\n      String fileType;\n      String fileLengthStr;\n      long fileLength;\n\n      while (true) {\n        try {\n          fileType \u003d valueStream.readUTF();\n        } catch (EOFException e) {\n          // EndOfFile\n          return;\n        }\n        fileLengthStr \u003d valueStream.readUTF();\n        fileLength \u003d Long.parseLong(fileLengthStr);\n        writer.write(\"\\n\\nLogType:\");\n        writer.write(fileType);\n        writer.write(\"\\nLogLength:\");\n        writer.write(fileLengthStr);\n        writer.write(\"\\nLog Contents:\\n\");\n        // ByteLevel\n        BoundedInputStream bis \u003d\n            new BoundedInputStream(valueStream, fileLength);\n        InputStreamReader reader \u003d new InputStreamReader(bis);\n        int currentRead \u003d 0;\n        int totalRead \u003d 0;\n        while ((currentRead \u003d reader.read(cbuf, 0, bufferSize)) !\u003d -1) {\n          writer.write(cbuf);\n          totalRead +\u003d currentRead;\n        }\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AggregatedLogFormat.java"
    }
  }
}