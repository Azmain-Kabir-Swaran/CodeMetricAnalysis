{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LoadJob.java",
  "functionName": "setup",
  "functionId": "setup___context-Context",
  "sourceFilePath": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
  "functionStartLine": 453,
  "functionEndLine": 504,
  "numCommitsSeen": 11,
  "timeTaken": 4629,
  "changeHistory": [
    "dcf84707ab50662add112bd6b01c0bfd63374853",
    "3edc40e3777a4cf226fff1be0bdc0ac4c2f49f34",
    "c1c0e8c9eaa12043faad985ac5d7e1b5949544cd",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "3fd40ae8d0b45d7bf6186fe14851ca87eb9ee3ef",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "dcf84707ab50662add112bd6b01c0bfd63374853": "Yfilerename",
    "3edc40e3777a4cf226fff1be0bdc0ac4c2f49f34": "Ybodychange",
    "c1c0e8c9eaa12043faad985ac5d7e1b5949544cd": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "3fd40ae8d0b45d7bf6186fe14851ca87eb9ee3ef": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "dcf84707ab50662add112bd6b01c0bfd63374853": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-3543. Mavenize Gridmix. (tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1339629 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/05/12 8:06 AM",
      "commitName": "dcf84707ab50662add112bd6b01c0bfd63374853",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "17/05/12 7:20 AM",
      "commitNameOld": "e1f09365ca0bee093f849fcf2e546dd6e2c0a965",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 0.03,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    protected void setup(Context context)\n    throws IOException, InterruptedException {\n      if (!context.nextKey() \n          || context.getCurrentKey().getType() !\u003d GridmixKey.REDUCE_SPEC) {\n        throw new IOException(\"Missing reduce spec\");\n      }\n      long outBytes \u003d 0L;\n      long outRecords \u003d 0L;\n      long inRecords \u003d 0L;\n      ResourceUsageMetrics metrics \u003d new ResourceUsageMetrics();\n      for (GridmixRecord ignored : context.getValues()) {\n        final GridmixKey spec \u003d context.getCurrentKey();\n        inRecords +\u003d spec.getReduceInputRecords();\n        outBytes +\u003d spec.getReduceOutputBytes();\n        outRecords +\u003d spec.getReduceOutputRecords();\n        if (spec.getReduceResourceUsageMetrics() !\u003d null) {\n          metrics \u003d spec.getReduceResourceUsageMetrics();\n        }\n      }\n      if (0 \u003d\u003d outRecords \u0026\u0026 inRecords \u003e 0) {\n        LOG.info(\"Spec output bytes w/o records. Using input record count\");\n        outRecords \u003d inRecords;\n      }\n      \n      // enable gridmix reduce output record for compression\n      Configuration conf \u003d context.getConfiguration();\n      if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n          \u0026\u0026 FileOutputFormat.getCompressOutput(context)) {\n        float compressionRatio \u003d \n          CompressionEmulationUtil\n            .getJobOutputCompressionEmulationRatio(conf);\n        LOG.info(\"GridMix is configured to use a compression ratio of \" \n                 + compressionRatio + \" for the reduce output data.\");\n        val.setCompressibility(true, compressionRatio);\n        \n        // Set the actual output data size to make sure that the actual output \n        // data size is same after compression\n        outBytes /\u003d compressionRatio;\n      }\n      \n      factory \u003d\n        new AvgRecordFactory(outBytes, outRecords, \n                             context.getConfiguration(), 5*1024);\n      ratio \u003d outRecords / (1.0 * inRecords);\n      acc \u003d 0.0;\n      \n      matcher \u003d new ResourceUsageMatcherRunner(context, metrics);\n      \n      // start the status reporter thread\n      reporter \u003d new StatusReporter(context, matcher);\n      reporter.start();\n    }",
      "path": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
        "newPath": "hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java"
      }
    },
    "3edc40e3777a4cf226fff1be0bdc0ac4c2f49f34": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4100. [Gridmix] Bug fixed in compression emulation feature for map only jobs. (amarrk)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1327816 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/04/12 9:26 PM",
      "commitName": "3edc40e3777a4cf226fff1be0bdc0ac4c2f49f34",
      "commitAuthor": "Amar Kamat",
      "commitDateOld": "05/03/12 5:44 AM",
      "commitNameOld": "231e39462dbfe60f66710e0425dbf16069382dbe",
      "commitAuthorOld": "Ravi Gummadi",
      "daysBetweenCommits": 44.61,
      "commitsBetweenForRepo": 333,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,52 @@\n     protected void setup(Context context)\n     throws IOException, InterruptedException {\n       if (!context.nextKey() \n           || context.getCurrentKey().getType() !\u003d GridmixKey.REDUCE_SPEC) {\n         throw new IOException(\"Missing reduce spec\");\n       }\n       long outBytes \u003d 0L;\n       long outRecords \u003d 0L;\n       long inRecords \u003d 0L;\n       ResourceUsageMetrics metrics \u003d new ResourceUsageMetrics();\n       for (GridmixRecord ignored : context.getValues()) {\n         final GridmixKey spec \u003d context.getCurrentKey();\n         inRecords +\u003d spec.getReduceInputRecords();\n         outBytes +\u003d spec.getReduceOutputBytes();\n         outRecords +\u003d spec.getReduceOutputRecords();\n         if (spec.getReduceResourceUsageMetrics() !\u003d null) {\n           metrics \u003d spec.getReduceResourceUsageMetrics();\n         }\n       }\n       if (0 \u003d\u003d outRecords \u0026\u0026 inRecords \u003e 0) {\n         LOG.info(\"Spec output bytes w/o records. Using input record count\");\n         outRecords \u003d inRecords;\n       }\n       \n       // enable gridmix reduce output record for compression\n       Configuration conf \u003d context.getConfiguration();\n       if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n           \u0026\u0026 FileOutputFormat.getCompressOutput(context)) {\n         float compressionRatio \u003d \n           CompressionEmulationUtil\n-            .getReduceOutputCompressionEmulationRatio(conf);\n+            .getJobOutputCompressionEmulationRatio(conf);\n         LOG.info(\"GridMix is configured to use a compression ratio of \" \n                  + compressionRatio + \" for the reduce output data.\");\n         val.setCompressibility(true, compressionRatio);\n         \n         // Set the actual output data size to make sure that the actual output \n         // data size is same after compression\n         outBytes /\u003d compressionRatio;\n       }\n       \n       factory \u003d\n         new AvgRecordFactory(outBytes, outRecords, \n                              context.getConfiguration(), 5*1024);\n       ratio \u003d outRecords / (1.0 * inRecords);\n       acc \u003d 0.0;\n       \n       matcher \u003d new ResourceUsageMatcherRunner(context, metrics);\n       \n       // start the status reporter thread\n       reporter \u003d new StatusReporter(context, matcher);\n       reporter.start();\n     }\n\\ No newline at end of file\n",
      "actualSource": "    protected void setup(Context context)\n    throws IOException, InterruptedException {\n      if (!context.nextKey() \n          || context.getCurrentKey().getType() !\u003d GridmixKey.REDUCE_SPEC) {\n        throw new IOException(\"Missing reduce spec\");\n      }\n      long outBytes \u003d 0L;\n      long outRecords \u003d 0L;\n      long inRecords \u003d 0L;\n      ResourceUsageMetrics metrics \u003d new ResourceUsageMetrics();\n      for (GridmixRecord ignored : context.getValues()) {\n        final GridmixKey spec \u003d context.getCurrentKey();\n        inRecords +\u003d spec.getReduceInputRecords();\n        outBytes +\u003d spec.getReduceOutputBytes();\n        outRecords +\u003d spec.getReduceOutputRecords();\n        if (spec.getReduceResourceUsageMetrics() !\u003d null) {\n          metrics \u003d spec.getReduceResourceUsageMetrics();\n        }\n      }\n      if (0 \u003d\u003d outRecords \u0026\u0026 inRecords \u003e 0) {\n        LOG.info(\"Spec output bytes w/o records. Using input record count\");\n        outRecords \u003d inRecords;\n      }\n      \n      // enable gridmix reduce output record for compression\n      Configuration conf \u003d context.getConfiguration();\n      if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n          \u0026\u0026 FileOutputFormat.getCompressOutput(context)) {\n        float compressionRatio \u003d \n          CompressionEmulationUtil\n            .getJobOutputCompressionEmulationRatio(conf);\n        LOG.info(\"GridMix is configured to use a compression ratio of \" \n                 + compressionRatio + \" for the reduce output data.\");\n        val.setCompressibility(true, compressionRatio);\n        \n        // Set the actual output data size to make sure that the actual output \n        // data size is same after compression\n        outBytes /\u003d compressionRatio;\n      }\n      \n      factory \u003d\n        new AvgRecordFactory(outBytes, outRecords, \n                             context.getConfiguration(), 5*1024);\n      ratio \u003d outRecords / (1.0 * inRecords);\n      acc \u003d 0.0;\n      \n      matcher \u003d new ResourceUsageMatcherRunner(context, metrics);\n      \n      // start the status reporter thread\n      reporter \u003d new StatusReporter(context, matcher);\n      reporter.start();\n    }",
      "path": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {}
    },
    "c1c0e8c9eaa12043faad985ac5d7e1b5949544cd": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3008. Improvements to cumulative CPU emulation for short running tasks in Gridmix. (amarrk)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179933 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/10/11 9:59 PM",
      "commitName": "c1c0e8c9eaa12043faad985ac5d7e1b5949544cd",
      "commitAuthor": "Amar Kamat",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 43.2,
      "commitsBetweenForRepo": 284,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,52 @@\n     protected void setup(Context context)\n     throws IOException, InterruptedException {\n       if (!context.nextKey() \n           || context.getCurrentKey().getType() !\u003d GridmixKey.REDUCE_SPEC) {\n         throw new IOException(\"Missing reduce spec\");\n       }\n       long outBytes \u003d 0L;\n       long outRecords \u003d 0L;\n       long inRecords \u003d 0L;\n       ResourceUsageMetrics metrics \u003d new ResourceUsageMetrics();\n       for (GridmixRecord ignored : context.getValues()) {\n         final GridmixKey spec \u003d context.getCurrentKey();\n         inRecords +\u003d spec.getReduceInputRecords();\n         outBytes +\u003d spec.getReduceOutputBytes();\n         outRecords +\u003d spec.getReduceOutputRecords();\n         if (spec.getReduceResourceUsageMetrics() !\u003d null) {\n           metrics \u003d spec.getReduceResourceUsageMetrics();\n         }\n       }\n       if (0 \u003d\u003d outRecords \u0026\u0026 inRecords \u003e 0) {\n         LOG.info(\"Spec output bytes w/o records. Using input record count\");\n         outRecords \u003d inRecords;\n       }\n       \n       // enable gridmix reduce output record for compression\n       Configuration conf \u003d context.getConfiguration();\n       if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n           \u0026\u0026 FileOutputFormat.getCompressOutput(context)) {\n         float compressionRatio \u003d \n           CompressionEmulationUtil\n             .getReduceOutputCompressionEmulationRatio(conf);\n         LOG.info(\"GridMix is configured to use a compression ratio of \" \n                  + compressionRatio + \" for the reduce output data.\");\n         val.setCompressibility(true, compressionRatio);\n         \n         // Set the actual output data size to make sure that the actual output \n         // data size is same after compression\n         outBytes /\u003d compressionRatio;\n       }\n       \n       factory \u003d\n         new AvgRecordFactory(outBytes, outRecords, \n                              context.getConfiguration(), 5*1024);\n       ratio \u003d outRecords / (1.0 * inRecords);\n       acc \u003d 0.0;\n       \n       matcher \u003d new ResourceUsageMatcherRunner(context, metrics);\n       \n       // start the status reporter thread\n-      reporter \u003d new StatusReporter(context);\n+      reporter \u003d new StatusReporter(context, matcher);\n       reporter.start();\n     }\n\\ No newline at end of file\n",
      "actualSource": "    protected void setup(Context context)\n    throws IOException, InterruptedException {\n      if (!context.nextKey() \n          || context.getCurrentKey().getType() !\u003d GridmixKey.REDUCE_SPEC) {\n        throw new IOException(\"Missing reduce spec\");\n      }\n      long outBytes \u003d 0L;\n      long outRecords \u003d 0L;\n      long inRecords \u003d 0L;\n      ResourceUsageMetrics metrics \u003d new ResourceUsageMetrics();\n      for (GridmixRecord ignored : context.getValues()) {\n        final GridmixKey spec \u003d context.getCurrentKey();\n        inRecords +\u003d spec.getReduceInputRecords();\n        outBytes +\u003d spec.getReduceOutputBytes();\n        outRecords +\u003d spec.getReduceOutputRecords();\n        if (spec.getReduceResourceUsageMetrics() !\u003d null) {\n          metrics \u003d spec.getReduceResourceUsageMetrics();\n        }\n      }\n      if (0 \u003d\u003d outRecords \u0026\u0026 inRecords \u003e 0) {\n        LOG.info(\"Spec output bytes w/o records. Using input record count\");\n        outRecords \u003d inRecords;\n      }\n      \n      // enable gridmix reduce output record for compression\n      Configuration conf \u003d context.getConfiguration();\n      if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n          \u0026\u0026 FileOutputFormat.getCompressOutput(context)) {\n        float compressionRatio \u003d \n          CompressionEmulationUtil\n            .getReduceOutputCompressionEmulationRatio(conf);\n        LOG.info(\"GridMix is configured to use a compression ratio of \" \n                 + compressionRatio + \" for the reduce output data.\");\n        val.setCompressibility(true, compressionRatio);\n        \n        // Set the actual output data size to make sure that the actual output \n        // data size is same after compression\n        outBytes /\u003d compressionRatio;\n      }\n      \n      factory \u003d\n        new AvgRecordFactory(outBytes, outRecords, \n                             context.getConfiguration(), 5*1024);\n      ratio \u003d outRecords / (1.0 * inRecords);\n      acc \u003d 0.0;\n      \n      matcher \u003d new ResourceUsageMatcherRunner(context, metrics);\n      \n      // start the status reporter thread\n      reporter \u003d new StatusReporter(context, matcher);\n      reporter.start();\n    }",
      "path": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    protected void setup(Context context)\n    throws IOException, InterruptedException {\n      if (!context.nextKey() \n          || context.getCurrentKey().getType() !\u003d GridmixKey.REDUCE_SPEC) {\n        throw new IOException(\"Missing reduce spec\");\n      }\n      long outBytes \u003d 0L;\n      long outRecords \u003d 0L;\n      long inRecords \u003d 0L;\n      ResourceUsageMetrics metrics \u003d new ResourceUsageMetrics();\n      for (GridmixRecord ignored : context.getValues()) {\n        final GridmixKey spec \u003d context.getCurrentKey();\n        inRecords +\u003d spec.getReduceInputRecords();\n        outBytes +\u003d spec.getReduceOutputBytes();\n        outRecords +\u003d spec.getReduceOutputRecords();\n        if (spec.getReduceResourceUsageMetrics() !\u003d null) {\n          metrics \u003d spec.getReduceResourceUsageMetrics();\n        }\n      }\n      if (0 \u003d\u003d outRecords \u0026\u0026 inRecords \u003e 0) {\n        LOG.info(\"Spec output bytes w/o records. Using input record count\");\n        outRecords \u003d inRecords;\n      }\n      \n      // enable gridmix reduce output record for compression\n      Configuration conf \u003d context.getConfiguration();\n      if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n          \u0026\u0026 FileOutputFormat.getCompressOutput(context)) {\n        float compressionRatio \u003d \n          CompressionEmulationUtil\n            .getReduceOutputCompressionEmulationRatio(conf);\n        LOG.info(\"GridMix is configured to use a compression ratio of \" \n                 + compressionRatio + \" for the reduce output data.\");\n        val.setCompressibility(true, compressionRatio);\n        \n        // Set the actual output data size to make sure that the actual output \n        // data size is same after compression\n        outBytes /\u003d compressionRatio;\n      }\n      \n      factory \u003d\n        new AvgRecordFactory(outBytes, outRecords, \n                             context.getConfiguration(), 5*1024);\n      ratio \u003d outRecords / (1.0 * inRecords);\n      acc \u003d 0.0;\n      \n      matcher \u003d new ResourceUsageMatcherRunner(context, metrics);\n      \n      // start the status reporter thread\n      reporter \u003d new StatusReporter(context);\n      reporter.start();\n    }",
      "path": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
        "newPath": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    protected void setup(Context context)\n    throws IOException, InterruptedException {\n      if (!context.nextKey() \n          || context.getCurrentKey().getType() !\u003d GridmixKey.REDUCE_SPEC) {\n        throw new IOException(\"Missing reduce spec\");\n      }\n      long outBytes \u003d 0L;\n      long outRecords \u003d 0L;\n      long inRecords \u003d 0L;\n      ResourceUsageMetrics metrics \u003d new ResourceUsageMetrics();\n      for (GridmixRecord ignored : context.getValues()) {\n        final GridmixKey spec \u003d context.getCurrentKey();\n        inRecords +\u003d spec.getReduceInputRecords();\n        outBytes +\u003d spec.getReduceOutputBytes();\n        outRecords +\u003d spec.getReduceOutputRecords();\n        if (spec.getReduceResourceUsageMetrics() !\u003d null) {\n          metrics \u003d spec.getReduceResourceUsageMetrics();\n        }\n      }\n      if (0 \u003d\u003d outRecords \u0026\u0026 inRecords \u003e 0) {\n        LOG.info(\"Spec output bytes w/o records. Using input record count\");\n        outRecords \u003d inRecords;\n      }\n      \n      // enable gridmix reduce output record for compression\n      Configuration conf \u003d context.getConfiguration();\n      if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n          \u0026\u0026 FileOutputFormat.getCompressOutput(context)) {\n        float compressionRatio \u003d \n          CompressionEmulationUtil\n            .getReduceOutputCompressionEmulationRatio(conf);\n        LOG.info(\"GridMix is configured to use a compression ratio of \" \n                 + compressionRatio + \" for the reduce output data.\");\n        val.setCompressibility(true, compressionRatio);\n        \n        // Set the actual output data size to make sure that the actual output \n        // data size is same after compression\n        outBytes /\u003d compressionRatio;\n      }\n      \n      factory \u003d\n        new AvgRecordFactory(outBytes, outRecords, \n                             context.getConfiguration(), 5*1024);\n      ratio \u003d outRecords / (1.0 * inRecords);\n      acc \u003d 0.0;\n      \n      matcher \u003d new ResourceUsageMatcherRunner(context, metrics);\n      \n      // start the status reporter thread\n      reporter \u003d new StatusReporter(context);\n      reporter.start();\n    }",
      "path": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
        "newPath": "hadoop-mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java"
      }
    },
    "3fd40ae8d0b45d7bf6186fe14851ca87eb9ee3ef": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2106. [Gridmix] Cumulative CPU usage emulation in Gridmix. (amarrk)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1135396 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/06/11 12:44 AM",
      "commitName": "3fd40ae8d0b45d7bf6186fe14851ca87eb9ee3ef",
      "commitAuthor": "Amar Kamat",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 1.41,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,52 @@\n     protected void setup(Context context)\n     throws IOException, InterruptedException {\n       if (!context.nextKey() \n           || context.getCurrentKey().getType() !\u003d GridmixKey.REDUCE_SPEC) {\n         throw new IOException(\"Missing reduce spec\");\n       }\n       long outBytes \u003d 0L;\n       long outRecords \u003d 0L;\n       long inRecords \u003d 0L;\n+      ResourceUsageMetrics metrics \u003d new ResourceUsageMetrics();\n       for (GridmixRecord ignored : context.getValues()) {\n         final GridmixKey spec \u003d context.getCurrentKey();\n         inRecords +\u003d spec.getReduceInputRecords();\n         outBytes +\u003d spec.getReduceOutputBytes();\n         outRecords +\u003d spec.getReduceOutputRecords();\n+        if (spec.getReduceResourceUsageMetrics() !\u003d null) {\n+          metrics \u003d spec.getReduceResourceUsageMetrics();\n+        }\n       }\n       if (0 \u003d\u003d outRecords \u0026\u0026 inRecords \u003e 0) {\n         LOG.info(\"Spec output bytes w/o records. Using input record count\");\n         outRecords \u003d inRecords;\n       }\n       \n       // enable gridmix reduce output record for compression\n       Configuration conf \u003d context.getConfiguration();\n       if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n           \u0026\u0026 FileOutputFormat.getCompressOutput(context)) {\n         float compressionRatio \u003d \n           CompressionEmulationUtil\n             .getReduceOutputCompressionEmulationRatio(conf);\n         LOG.info(\"GridMix is configured to use a compression ratio of \" \n                  + compressionRatio + \" for the reduce output data.\");\n         val.setCompressibility(true, compressionRatio);\n         \n         // Set the actual output data size to make sure that the actual output \n         // data size is same after compression\n         outBytes /\u003d compressionRatio;\n       }\n       \n       factory \u003d\n         new AvgRecordFactory(outBytes, outRecords, \n                              context.getConfiguration(), 5*1024);\n       ratio \u003d outRecords / (1.0 * inRecords);\n       acc \u003d 0.0;\n+      \n+      matcher \u003d new ResourceUsageMatcherRunner(context, metrics);\n+      \n+      // start the status reporter thread\n+      reporter \u003d new StatusReporter(context);\n+      reporter.start();\n     }\n\\ No newline at end of file\n",
      "actualSource": "    protected void setup(Context context)\n    throws IOException, InterruptedException {\n      if (!context.nextKey() \n          || context.getCurrentKey().getType() !\u003d GridmixKey.REDUCE_SPEC) {\n        throw new IOException(\"Missing reduce spec\");\n      }\n      long outBytes \u003d 0L;\n      long outRecords \u003d 0L;\n      long inRecords \u003d 0L;\n      ResourceUsageMetrics metrics \u003d new ResourceUsageMetrics();\n      for (GridmixRecord ignored : context.getValues()) {\n        final GridmixKey spec \u003d context.getCurrentKey();\n        inRecords +\u003d spec.getReduceInputRecords();\n        outBytes +\u003d spec.getReduceOutputBytes();\n        outRecords +\u003d spec.getReduceOutputRecords();\n        if (spec.getReduceResourceUsageMetrics() !\u003d null) {\n          metrics \u003d spec.getReduceResourceUsageMetrics();\n        }\n      }\n      if (0 \u003d\u003d outRecords \u0026\u0026 inRecords \u003e 0) {\n        LOG.info(\"Spec output bytes w/o records. Using input record count\");\n        outRecords \u003d inRecords;\n      }\n      \n      // enable gridmix reduce output record for compression\n      Configuration conf \u003d context.getConfiguration();\n      if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n          \u0026\u0026 FileOutputFormat.getCompressOutput(context)) {\n        float compressionRatio \u003d \n          CompressionEmulationUtil\n            .getReduceOutputCompressionEmulationRatio(conf);\n        LOG.info(\"GridMix is configured to use a compression ratio of \" \n                 + compressionRatio + \" for the reduce output data.\");\n        val.setCompressibility(true, compressionRatio);\n        \n        // Set the actual output data size to make sure that the actual output \n        // data size is same after compression\n        outBytes /\u003d compressionRatio;\n      }\n      \n      factory \u003d\n        new AvgRecordFactory(outBytes, outRecords, \n                             context.getConfiguration(), 5*1024);\n      ratio \u003d outRecords / (1.0 * inRecords);\n      acc \u003d 0.0;\n      \n      matcher \u003d new ResourceUsageMatcherRunner(context, metrics);\n      \n      // start the status reporter thread\n      reporter \u003d new StatusReporter(context);\n      reporter.start();\n    }",
      "path": "mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,42 @@\n+    protected void setup(Context context)\n+    throws IOException, InterruptedException {\n+      if (!context.nextKey() \n+          || context.getCurrentKey().getType() !\u003d GridmixKey.REDUCE_SPEC) {\n+        throw new IOException(\"Missing reduce spec\");\n+      }\n+      long outBytes \u003d 0L;\n+      long outRecords \u003d 0L;\n+      long inRecords \u003d 0L;\n+      for (GridmixRecord ignored : context.getValues()) {\n+        final GridmixKey spec \u003d context.getCurrentKey();\n+        inRecords +\u003d spec.getReduceInputRecords();\n+        outBytes +\u003d spec.getReduceOutputBytes();\n+        outRecords +\u003d spec.getReduceOutputRecords();\n+      }\n+      if (0 \u003d\u003d outRecords \u0026\u0026 inRecords \u003e 0) {\n+        LOG.info(\"Spec output bytes w/o records. Using input record count\");\n+        outRecords \u003d inRecords;\n+      }\n+      \n+      // enable gridmix reduce output record for compression\n+      Configuration conf \u003d context.getConfiguration();\n+      if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n+          \u0026\u0026 FileOutputFormat.getCompressOutput(context)) {\n+        float compressionRatio \u003d \n+          CompressionEmulationUtil\n+            .getReduceOutputCompressionEmulationRatio(conf);\n+        LOG.info(\"GridMix is configured to use a compression ratio of \" \n+                 + compressionRatio + \" for the reduce output data.\");\n+        val.setCompressibility(true, compressionRatio);\n+        \n+        // Set the actual output data size to make sure that the actual output \n+        // data size is same after compression\n+        outBytes /\u003d compressionRatio;\n+      }\n+      \n+      factory \u003d\n+        new AvgRecordFactory(outBytes, outRecords, \n+                             context.getConfiguration(), 5*1024);\n+      ratio \u003d outRecords / (1.0 * inRecords);\n+      acc \u003d 0.0;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    protected void setup(Context context)\n    throws IOException, InterruptedException {\n      if (!context.nextKey() \n          || context.getCurrentKey().getType() !\u003d GridmixKey.REDUCE_SPEC) {\n        throw new IOException(\"Missing reduce spec\");\n      }\n      long outBytes \u003d 0L;\n      long outRecords \u003d 0L;\n      long inRecords \u003d 0L;\n      for (GridmixRecord ignored : context.getValues()) {\n        final GridmixKey spec \u003d context.getCurrentKey();\n        inRecords +\u003d spec.getReduceInputRecords();\n        outBytes +\u003d spec.getReduceOutputBytes();\n        outRecords +\u003d spec.getReduceOutputRecords();\n      }\n      if (0 \u003d\u003d outRecords \u0026\u0026 inRecords \u003e 0) {\n        LOG.info(\"Spec output bytes w/o records. Using input record count\");\n        outRecords \u003d inRecords;\n      }\n      \n      // enable gridmix reduce output record for compression\n      Configuration conf \u003d context.getConfiguration();\n      if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)\n          \u0026\u0026 FileOutputFormat.getCompressOutput(context)) {\n        float compressionRatio \u003d \n          CompressionEmulationUtil\n            .getReduceOutputCompressionEmulationRatio(conf);\n        LOG.info(\"GridMix is configured to use a compression ratio of \" \n                 + compressionRatio + \" for the reduce output data.\");\n        val.setCompressibility(true, compressionRatio);\n        \n        // Set the actual output data size to make sure that the actual output \n        // data size is same after compression\n        outBytes /\u003d compressionRatio;\n      }\n      \n      factory \u003d\n        new AvgRecordFactory(outBytes, outRecords, \n                             context.getConfiguration(), 5*1024);\n      ratio \u003d outRecords / (1.0 * inRecords);\n      acc \u003d 0.0;\n    }",
      "path": "mapreduce/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/LoadJob.java"
    }
  }
}