{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "EntityTableRW.java",
  "functionName": "createTable",
  "functionId": "createTable___admin-Admin__hbaseConf-Configuration",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityTableRW.java",
  "functionStartLine": 83,
  "functionEndLine": 125,
  "numCommitsSeen": 14,
  "timeTaken": 2366,
  "changeHistory": [
    "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2",
    "461ee44d287b1fcf0bf15d662aebd3e6f2b83a72",
    "b01514f65bc6090a50a583f67d1ecb5d74b6d276",
    "9e5155be363c6610ccf41fe08b7f1394f353ea65",
    "92d90c3a243134177f192d1d473dd606c79a91fc"
  ],
  "changeHistoryShort": {
    "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2": "Yfilerename",
    "461ee44d287b1fcf0bf15d662aebd3e6f2b83a72": "Ybodychange",
    "b01514f65bc6090a50a583f67d1ecb5d74b6d276": "Yfilerename",
    "9e5155be363c6610ccf41fe08b7f1394f353ea65": "Ybodychange",
    "92d90c3a243134177f192d1d473dd606c79a91fc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2": {
      "type": "Yfilerename",
      "commitMessage": "YARN-7919. Refactor timelineservice-hbase module into submodules. Contributed by Haibo Chen.\n",
      "commitDate": "17/02/18 7:00 AM",
      "commitName": "9af30d46c6e82332a8eda20cb3eb5f987e25e7a2",
      "commitAuthor": "Rohith Sharma K S",
      "commitDateOld": "17/02/18 3:24 AM",
      "commitNameOld": "a1e56a62863d8d494af309ec5f476c4b7e4d5ef9",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 0.15,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void createTable(Admin admin, Configuration hbaseConf)\n      throws IOException {\n\n    TableName table \u003d getTableName(hbaseConf);\n    if (admin.tableExists(table)) {\n      // do not disable / delete existing table\n      // similar to the approach taken by map-reduce jobs when\n      // output directory exists\n      throw new IOException(\"Table \" + table.getNameAsString()\n          + \" already exists.\");\n    }\n\n    HTableDescriptor entityTableDescp \u003d new HTableDescriptor(table);\n    HColumnDescriptor infoCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.INFO.getBytes());\n    infoCF.setBloomFilterType(BloomType.ROWCOL);\n    entityTableDescp.addFamily(infoCF);\n\n    HColumnDescriptor configCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.CONFIGS.getBytes());\n    configCF.setBloomFilterType(BloomType.ROWCOL);\n    configCF.setBlockCacheEnabled(true);\n    entityTableDescp.addFamily(configCF);\n\n    HColumnDescriptor metricsCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.METRICS.getBytes());\n    entityTableDescp.addFamily(metricsCF);\n    metricsCF.setBlockCacheEnabled(true);\n    // always keep 1 version (the latest)\n    metricsCF.setMinVersions(1);\n    metricsCF.setMaxVersions(\n        hbaseConf.getInt(METRICS_MAX_VERSIONS, DEFAULT_METRICS_MAX_VERSIONS));\n    metricsCF.setTimeToLive(hbaseConf.getInt(METRICS_TTL_CONF_NAME,\n        DEFAULT_METRICS_TTL));\n    entityTableDescp.setRegionSplitPolicyClassName(\n        \"org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy\");\n    entityTableDescp.setValue(\"KeyPrefixRegionSplitPolicy.prefix_length\",\n        TimelineHBaseSchemaConstants.USERNAME_SPLIT_KEY_PREFIX_LENGTH);\n    admin.createTable(entityTableDescp,\n        TimelineHBaseSchemaConstants.getUsernameSplits());\n    LOG.info(\"Status of table creation for \" + table.getNameAsString() + \"\u003d\"\n        + admin.tableExists(table));\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityTableRW.java",
      "extendedDetails": {
        "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityTable.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/hadoop-yarn-server-timelineservice-hbase-client/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityTableRW.java"
      }
    },
    "461ee44d287b1fcf0bf15d662aebd3e6f2b83a72": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6435. [ATSv2] Can\u0027t retrieve more than 1000 versions of metrics in time series. (Rohith Sharma K S via Haibo Chen)\n",
      "commitDate": "09/05/17 9:12 PM",
      "commitName": "461ee44d287b1fcf0bf15d662aebd3e6f2b83a72",
      "commitAuthor": "Haibo Chen",
      "commitDateOld": "19/01/17 8:52 PM",
      "commitNameOld": "b01514f65bc6090a50a583f67d1ecb5d74b6d276",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 109.97,
      "commitsBetweenForRepo": 602,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,43 @@\n   public void createTable(Admin admin, Configuration hbaseConf)\n       throws IOException {\n \n     TableName table \u003d getTableName(hbaseConf);\n     if (admin.tableExists(table)) {\n       // do not disable / delete existing table\n       // similar to the approach taken by map-reduce jobs when\n       // output directory exists\n       throw new IOException(\"Table \" + table.getNameAsString()\n           + \" already exists.\");\n     }\n \n     HTableDescriptor entityTableDescp \u003d new HTableDescriptor(table);\n     HColumnDescriptor infoCF \u003d\n         new HColumnDescriptor(EntityColumnFamily.INFO.getBytes());\n     infoCF.setBloomFilterType(BloomType.ROWCOL);\n     entityTableDescp.addFamily(infoCF);\n \n     HColumnDescriptor configCF \u003d\n         new HColumnDescriptor(EntityColumnFamily.CONFIGS.getBytes());\n     configCF.setBloomFilterType(BloomType.ROWCOL);\n     configCF.setBlockCacheEnabled(true);\n     entityTableDescp.addFamily(configCF);\n \n     HColumnDescriptor metricsCF \u003d\n         new HColumnDescriptor(EntityColumnFamily.METRICS.getBytes());\n     entityTableDescp.addFamily(metricsCF);\n     metricsCF.setBlockCacheEnabled(true);\n     // always keep 1 version (the latest)\n     metricsCF.setMinVersions(1);\n-    metricsCF.setMaxVersions(DEFAULT_METRICS_MAX_VERSIONS);\n+    metricsCF.setMaxVersions(\n+        hbaseConf.getInt(METRICS_MAX_VERSIONS, DEFAULT_METRICS_MAX_VERSIONS));\n     metricsCF.setTimeToLive(hbaseConf.getInt(METRICS_TTL_CONF_NAME,\n         DEFAULT_METRICS_TTL));\n     entityTableDescp.setRegionSplitPolicyClassName(\n         \"org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy\");\n     entityTableDescp.setValue(\"KeyPrefixRegionSplitPolicy.prefix_length\",\n         TimelineHBaseSchemaConstants.USERNAME_SPLIT_KEY_PREFIX_LENGTH);\n     admin.createTable(entityTableDescp,\n         TimelineHBaseSchemaConstants.getUsernameSplits());\n     LOG.info(\"Status of table creation for \" + table.getNameAsString() + \"\u003d\"\n         + admin.tableExists(table));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void createTable(Admin admin, Configuration hbaseConf)\n      throws IOException {\n\n    TableName table \u003d getTableName(hbaseConf);\n    if (admin.tableExists(table)) {\n      // do not disable / delete existing table\n      // similar to the approach taken by map-reduce jobs when\n      // output directory exists\n      throw new IOException(\"Table \" + table.getNameAsString()\n          + \" already exists.\");\n    }\n\n    HTableDescriptor entityTableDescp \u003d new HTableDescriptor(table);\n    HColumnDescriptor infoCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.INFO.getBytes());\n    infoCF.setBloomFilterType(BloomType.ROWCOL);\n    entityTableDescp.addFamily(infoCF);\n\n    HColumnDescriptor configCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.CONFIGS.getBytes());\n    configCF.setBloomFilterType(BloomType.ROWCOL);\n    configCF.setBlockCacheEnabled(true);\n    entityTableDescp.addFamily(configCF);\n\n    HColumnDescriptor metricsCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.METRICS.getBytes());\n    entityTableDescp.addFamily(metricsCF);\n    metricsCF.setBlockCacheEnabled(true);\n    // always keep 1 version (the latest)\n    metricsCF.setMinVersions(1);\n    metricsCF.setMaxVersions(\n        hbaseConf.getInt(METRICS_MAX_VERSIONS, DEFAULT_METRICS_MAX_VERSIONS));\n    metricsCF.setTimeToLive(hbaseConf.getInt(METRICS_TTL_CONF_NAME,\n        DEFAULT_METRICS_TTL));\n    entityTableDescp.setRegionSplitPolicyClassName(\n        \"org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy\");\n    entityTableDescp.setValue(\"KeyPrefixRegionSplitPolicy.prefix_length\",\n        TimelineHBaseSchemaConstants.USERNAME_SPLIT_KEY_PREFIX_LENGTH);\n    admin.createTable(entityTableDescp,\n        TimelineHBaseSchemaConstants.getUsernameSplits());\n    LOG.info(\"Status of table creation for \" + table.getNameAsString() + \"\u003d\"\n        + admin.tableExists(table));\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityTable.java",
      "extendedDetails": {}
    },
    "b01514f65bc6090a50a583f67d1ecb5d74b6d276": {
      "type": "Yfilerename",
      "commitMessage": "YARN-5928. Move ATSv2 HBase backend code into a new module that is only dependent at runtime by yarn servers. Contributed by Haibo Chen.\n",
      "commitDate": "19/01/17 8:52 PM",
      "commitName": "b01514f65bc6090a50a583f67d1ecb5d74b6d276",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "19/01/17 5:32 PM",
      "commitNameOld": "60865c8ea08053f3d6ac23f81c3376a3de3ca996",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.14,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void createTable(Admin admin, Configuration hbaseConf)\n      throws IOException {\n\n    TableName table \u003d getTableName(hbaseConf);\n    if (admin.tableExists(table)) {\n      // do not disable / delete existing table\n      // similar to the approach taken by map-reduce jobs when\n      // output directory exists\n      throw new IOException(\"Table \" + table.getNameAsString()\n          + \" already exists.\");\n    }\n\n    HTableDescriptor entityTableDescp \u003d new HTableDescriptor(table);\n    HColumnDescriptor infoCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.INFO.getBytes());\n    infoCF.setBloomFilterType(BloomType.ROWCOL);\n    entityTableDescp.addFamily(infoCF);\n\n    HColumnDescriptor configCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.CONFIGS.getBytes());\n    configCF.setBloomFilterType(BloomType.ROWCOL);\n    configCF.setBlockCacheEnabled(true);\n    entityTableDescp.addFamily(configCF);\n\n    HColumnDescriptor metricsCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.METRICS.getBytes());\n    entityTableDescp.addFamily(metricsCF);\n    metricsCF.setBlockCacheEnabled(true);\n    // always keep 1 version (the latest)\n    metricsCF.setMinVersions(1);\n    metricsCF.setMaxVersions(DEFAULT_METRICS_MAX_VERSIONS);\n    metricsCF.setTimeToLive(hbaseConf.getInt(METRICS_TTL_CONF_NAME,\n        DEFAULT_METRICS_TTL));\n    entityTableDescp.setRegionSplitPolicyClassName(\n        \"org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy\");\n    entityTableDescp.setValue(\"KeyPrefixRegionSplitPolicy.prefix_length\",\n        TimelineHBaseSchemaConstants.USERNAME_SPLIT_KEY_PREFIX_LENGTH);\n    admin.createTable(entityTableDescp,\n        TimelineHBaseSchemaConstants.getUsernameSplits());\n    LOG.info(\"Status of table creation for \" + table.getNameAsString() + \"\u003d\"\n        + admin.tableExists(table));\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityTable.java",
      "extendedDetails": {
        "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityTable.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityTable.java"
      }
    },
    "9e5155be363c6610ccf41fe08b7f1394f353ea65": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3049. [Storage Implementation] Implement storage reader interface to fetch raw data from HBase backend (Zhijie Shen via sjlee)\n\n(cherry picked from commit 07433c2ad52df9e844dbd90020c277d3df844dcd)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "9e5155be363c6610ccf41fe08b7f1394f353ea65",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "a9fab9b644e636c1f1b2632130d4eaea70111f16",
      "commitAuthorOld": "Zhijie Shen",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,42 @@\n   public void createTable(Admin admin, Configuration hbaseConf)\n       throws IOException {\n \n     TableName table \u003d getTableName(hbaseConf);\n     if (admin.tableExists(table)) {\n       // do not disable / delete existing table\n       // similar to the approach taken by map-reduce jobs when\n       // output directory exists\n       throw new IOException(\"Table \" + table.getNameAsString()\n           + \" already exists.\");\n     }\n \n     HTableDescriptor entityTableDescp \u003d new HTableDescriptor(table);\n     HColumnDescriptor infoCF \u003d\n         new HColumnDescriptor(EntityColumnFamily.INFO.getBytes());\n     infoCF.setBloomFilterType(BloomType.ROWCOL);\n     entityTableDescp.addFamily(infoCF);\n \n     HColumnDescriptor configCF \u003d\n         new HColumnDescriptor(EntityColumnFamily.CONFIGS.getBytes());\n     configCF.setBloomFilterType(BloomType.ROWCOL);\n     configCF.setBlockCacheEnabled(true);\n     entityTableDescp.addFamily(configCF);\n \n     HColumnDescriptor metricsCF \u003d\n         new HColumnDescriptor(EntityColumnFamily.METRICS.getBytes());\n     entityTableDescp.addFamily(metricsCF);\n     metricsCF.setBlockCacheEnabled(true);\n     // always keep 1 version (the latest)\n     metricsCF.setMinVersions(1);\n     metricsCF.setMaxVersions(DEFAULT_METRICS_MAX_VERSIONS);\n     metricsCF.setTimeToLive(hbaseConf.getInt(METRICS_TTL_CONF_NAME,\n         DEFAULT_METRICS_TTL));\n     entityTableDescp\n         .setRegionSplitPolicyClassName(\"org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy\");\n     entityTableDescp.setValue(\"KeyPrefixRegionSplitPolicy.prefix_length\",\n-        TimelineEntitySchemaConstants.USERNAME_SPLIT_KEY_PREFIX_LENGTH);\n+        TimelineHBaseSchemaConstants.USERNAME_SPLIT_KEY_PREFIX_LENGTH);\n     admin.createTable(entityTableDescp,\n-        TimelineEntitySchemaConstants.getUsernameSplits());\n+        TimelineHBaseSchemaConstants.getUsernameSplits());\n     LOG.info(\"Status of table creation for \" + table.getNameAsString() + \"\u003d\"\n         + admin.tableExists(table));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void createTable(Admin admin, Configuration hbaseConf)\n      throws IOException {\n\n    TableName table \u003d getTableName(hbaseConf);\n    if (admin.tableExists(table)) {\n      // do not disable / delete existing table\n      // similar to the approach taken by map-reduce jobs when\n      // output directory exists\n      throw new IOException(\"Table \" + table.getNameAsString()\n          + \" already exists.\");\n    }\n\n    HTableDescriptor entityTableDescp \u003d new HTableDescriptor(table);\n    HColumnDescriptor infoCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.INFO.getBytes());\n    infoCF.setBloomFilterType(BloomType.ROWCOL);\n    entityTableDescp.addFamily(infoCF);\n\n    HColumnDescriptor configCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.CONFIGS.getBytes());\n    configCF.setBloomFilterType(BloomType.ROWCOL);\n    configCF.setBlockCacheEnabled(true);\n    entityTableDescp.addFamily(configCF);\n\n    HColumnDescriptor metricsCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.METRICS.getBytes());\n    entityTableDescp.addFamily(metricsCF);\n    metricsCF.setBlockCacheEnabled(true);\n    // always keep 1 version (the latest)\n    metricsCF.setMinVersions(1);\n    metricsCF.setMaxVersions(DEFAULT_METRICS_MAX_VERSIONS);\n    metricsCF.setTimeToLive(hbaseConf.getInt(METRICS_TTL_CONF_NAME,\n        DEFAULT_METRICS_TTL));\n    entityTableDescp\n        .setRegionSplitPolicyClassName(\"org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy\");\n    entityTableDescp.setValue(\"KeyPrefixRegionSplitPolicy.prefix_length\",\n        TimelineHBaseSchemaConstants.USERNAME_SPLIT_KEY_PREFIX_LENGTH);\n    admin.createTable(entityTableDescp,\n        TimelineHBaseSchemaConstants.getUsernameSplits());\n    LOG.info(\"Status of table creation for \" + table.getNameAsString() + \"\u003d\"\n        + admin.tableExists(table));\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityTable.java",
      "extendedDetails": {}
    },
    "92d90c3a243134177f192d1d473dd606c79a91fc": {
      "type": "Yintroduced",
      "commitMessage": "YARN-3706. Generalize native HBase writer for additional tables (Joep Rottinghuis via sjlee)\n\n(cherry picked from commit 9137aeae0dec83f9eff40d12cae712dfd508c0c5)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "92d90c3a243134177f192d1d473dd606c79a91fc",
      "commitAuthor": "Sangjin Lee",
      "diff": "@@ -0,0 +1,42 @@\n+  public void createTable(Admin admin, Configuration hbaseConf)\n+      throws IOException {\n+\n+    TableName table \u003d getTableName(hbaseConf);\n+    if (admin.tableExists(table)) {\n+      // do not disable / delete existing table\n+      // similar to the approach taken by map-reduce jobs when\n+      // output directory exists\n+      throw new IOException(\"Table \" + table.getNameAsString()\n+          + \" already exists.\");\n+    }\n+\n+    HTableDescriptor entityTableDescp \u003d new HTableDescriptor(table);\n+    HColumnDescriptor infoCF \u003d\n+        new HColumnDescriptor(EntityColumnFamily.INFO.getBytes());\n+    infoCF.setBloomFilterType(BloomType.ROWCOL);\n+    entityTableDescp.addFamily(infoCF);\n+\n+    HColumnDescriptor configCF \u003d\n+        new HColumnDescriptor(EntityColumnFamily.CONFIGS.getBytes());\n+    configCF.setBloomFilterType(BloomType.ROWCOL);\n+    configCF.setBlockCacheEnabled(true);\n+    entityTableDescp.addFamily(configCF);\n+\n+    HColumnDescriptor metricsCF \u003d\n+        new HColumnDescriptor(EntityColumnFamily.METRICS.getBytes());\n+    entityTableDescp.addFamily(metricsCF);\n+    metricsCF.setBlockCacheEnabled(true);\n+    // always keep 1 version (the latest)\n+    metricsCF.setMinVersions(1);\n+    metricsCF.setMaxVersions(DEFAULT_METRICS_MAX_VERSIONS);\n+    metricsCF.setTimeToLive(hbaseConf.getInt(METRICS_TTL_CONF_NAME,\n+        DEFAULT_METRICS_TTL));\n+    entityTableDescp\n+        .setRegionSplitPolicyClassName(\"org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy\");\n+    entityTableDescp.setValue(\"KeyPrefixRegionSplitPolicy.prefix_length\",\n+        TimelineEntitySchemaConstants.USERNAME_SPLIT_KEY_PREFIX_LENGTH);\n+    admin.createTable(entityTableDescp,\n+        TimelineEntitySchemaConstants.getUsernameSplits());\n+    LOG.info(\"Status of table creation for \" + table.getNameAsString() + \"\u003d\"\n+        + admin.tableExists(table));\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void createTable(Admin admin, Configuration hbaseConf)\n      throws IOException {\n\n    TableName table \u003d getTableName(hbaseConf);\n    if (admin.tableExists(table)) {\n      // do not disable / delete existing table\n      // similar to the approach taken by map-reduce jobs when\n      // output directory exists\n      throw new IOException(\"Table \" + table.getNameAsString()\n          + \" already exists.\");\n    }\n\n    HTableDescriptor entityTableDescp \u003d new HTableDescriptor(table);\n    HColumnDescriptor infoCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.INFO.getBytes());\n    infoCF.setBloomFilterType(BloomType.ROWCOL);\n    entityTableDescp.addFamily(infoCF);\n\n    HColumnDescriptor configCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.CONFIGS.getBytes());\n    configCF.setBloomFilterType(BloomType.ROWCOL);\n    configCF.setBlockCacheEnabled(true);\n    entityTableDescp.addFamily(configCF);\n\n    HColumnDescriptor metricsCF \u003d\n        new HColumnDescriptor(EntityColumnFamily.METRICS.getBytes());\n    entityTableDescp.addFamily(metricsCF);\n    metricsCF.setBlockCacheEnabled(true);\n    // always keep 1 version (the latest)\n    metricsCF.setMinVersions(1);\n    metricsCF.setMaxVersions(DEFAULT_METRICS_MAX_VERSIONS);\n    metricsCF.setTimeToLive(hbaseConf.getInt(METRICS_TTL_CONF_NAME,\n        DEFAULT_METRICS_TTL));\n    entityTableDescp\n        .setRegionSplitPolicyClassName(\"org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy\");\n    entityTableDescp.setValue(\"KeyPrefixRegionSplitPolicy.prefix_length\",\n        TimelineEntitySchemaConstants.USERNAME_SPLIT_KEY_PREFIX_LENGTH);\n    admin.createTable(entityTableDescp,\n        TimelineEntitySchemaConstants.getUsernameSplits());\n    LOG.info(\"Status of table creation for \" + table.getNameAsString() + \"\u003d\"\n        + admin.tableExists(table));\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/entity/EntityTable.java"
    }
  }
}