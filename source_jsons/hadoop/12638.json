{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ExternalSPSFilePathCollector.java",
  "functionName": "processPath",
  "functionId": "processPath___startID-Long__childPath-String",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFilePathCollector.java",
  "functionStartLine": 76,
  "functionEndLine": 119,
  "numCommitsSeen": 6,
  "timeTaken": 5301,
  "changeHistory": [
    "66e8f9b31529226309c924226a53dead3e6fcf11",
    "8467ec24fb74f30371d5a13e893fc56309ee9372",
    "3b83110d5ed582b9f913ecf3f62ce410535f8fca",
    "3159b39cf8ef704835325263154fb1a1cecc109d"
  ],
  "changeHistoryShort": {
    "66e8f9b31529226309c924226a53dead3e6fcf11": "Ymultichange(Yparameterchange,Ybodychange)",
    "8467ec24fb74f30371d5a13e893fc56309ee9372": "Ymultichange(Yfilerename,Ybodychange,Yparameterchange)",
    "3b83110d5ed582b9f913ecf3f62ce410535f8fca": "Ymultichange(Yreturntypechange,Ybodychange)",
    "3159b39cf8ef704835325263154fb1a1cecc109d": "Yintroduced"
  },
  "changeHistoryDetails": {
    "66e8f9b31529226309c924226a53dead3e6fcf11": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-13381 : [SPS]: Use DFSUtilClient#makePathFromFileId() to prepare satisfier file path. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "66e8f9b31529226309c924226a53dead3e6fcf11",
      "commitAuthor": "Uma Maheswara Rao G",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13381 : [SPS]: Use DFSUtilClient#makePathFromFileId() to prepare satisfier file path. Contributed by Rakesh R.\n",
          "commitDate": "12/08/18 3:06 AM",
          "commitName": "66e8f9b31529226309c924226a53dead3e6fcf11",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "12/08/18 3:06 AM",
          "commitNameOld": "8467ec24fb74f30371d5a13e893fc56309ee9372",
          "commitAuthorOld": "Rakesh Radhakrishnan",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,44 @@\n-  private long processPath(String startID, String childPath) {\n+  private long processPath(Long startID, String childPath) {\n     long pendingWorkCount \u003d 0; // to be satisfied file counter\n     for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n       final DirectoryListing children;\n       try {\n-        children \u003d dfs.getClient().listPaths(childPath, lastReturnedName,\n-            false);\n+        children \u003d dfs.getClient().listPaths(childPath,\n+            lastReturnedName, false);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to list directory \" + childPath\n             + \". Ignore the directory and continue.\", e);\n         return pendingWorkCount;\n       }\n       if (children \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"The scanning start dir/sub dir \" + childPath\n               + \" does not have childrens.\");\n         }\n         return pendingWorkCount;\n       }\n \n       for (HdfsFileStatus child : children.getPartialListing()) {\n-        String childFullPath \u003d child.getFullName(childPath);\n         if (child.isFile()) {\n-          service.addFileToProcess(\n-              new ItemInfo\u003cString\u003e(startID, childFullPath), false);\n+          service.addFileToProcess(new ItemInfo(startID, child.getFileId()),\n+              false);\n           checkProcessingQueuesFree();\n           pendingWorkCount++; // increment to be satisfied file count\n         } else {\n+          String childFullPathName \u003d child.getFullName(childPath);\n           if (child.isDirectory()) {\n-            if (!childFullPath.endsWith(Path.SEPARATOR)) {\n-              childFullPath \u003d childFullPath + Path.SEPARATOR;\n+            if (!childFullPathName.endsWith(Path.SEPARATOR)) {\n+              childFullPathName \u003d childFullPathName + Path.SEPARATOR;\n             }\n-            pendingWorkCount +\u003d processPath(startID, childFullPath);\n+            pendingWorkCount +\u003d processPath(startID, childFullPathName);\n           }\n         }\n       }\n \n       if (children.hasMore()) {\n         lastReturnedName \u003d children.getLastName();\n       } else {\n         return pendingWorkCount;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long processPath(Long startID, String childPath) {\n    long pendingWorkCount \u003d 0; // to be satisfied file counter\n    for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n      final DirectoryListing children;\n      try {\n        children \u003d dfs.getClient().listPaths(childPath,\n            lastReturnedName, false);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to list directory \" + childPath\n            + \". Ignore the directory and continue.\", e);\n        return pendingWorkCount;\n      }\n      if (children \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The scanning start dir/sub dir \" + childPath\n              + \" does not have childrens.\");\n        }\n        return pendingWorkCount;\n      }\n\n      for (HdfsFileStatus child : children.getPartialListing()) {\n        if (child.isFile()) {\n          service.addFileToProcess(new ItemInfo(startID, child.getFileId()),\n              false);\n          checkProcessingQueuesFree();\n          pendingWorkCount++; // increment to be satisfied file count\n        } else {\n          String childFullPathName \u003d child.getFullName(childPath);\n          if (child.isDirectory()) {\n            if (!childFullPathName.endsWith(Path.SEPARATOR)) {\n              childFullPathName \u003d childFullPathName + Path.SEPARATOR;\n            }\n            pendingWorkCount +\u003d processPath(startID, childFullPathName);\n          }\n        }\n      }\n\n      if (children.hasMore()) {\n        lastReturnedName \u003d children.getLastName();\n      } else {\n        return pendingWorkCount;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFilePathCollector.java",
          "extendedDetails": {
            "oldValue": "[startID-String, childPath-String]",
            "newValue": "[startID-Long, childPath-String]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13381 : [SPS]: Use DFSUtilClient#makePathFromFileId() to prepare satisfier file path. Contributed by Rakesh R.\n",
          "commitDate": "12/08/18 3:06 AM",
          "commitName": "66e8f9b31529226309c924226a53dead3e6fcf11",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "12/08/18 3:06 AM",
          "commitNameOld": "8467ec24fb74f30371d5a13e893fc56309ee9372",
          "commitAuthorOld": "Rakesh Radhakrishnan",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,44 +1,44 @@\n-  private long processPath(String startID, String childPath) {\n+  private long processPath(Long startID, String childPath) {\n     long pendingWorkCount \u003d 0; // to be satisfied file counter\n     for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n       final DirectoryListing children;\n       try {\n-        children \u003d dfs.getClient().listPaths(childPath, lastReturnedName,\n-            false);\n+        children \u003d dfs.getClient().listPaths(childPath,\n+            lastReturnedName, false);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to list directory \" + childPath\n             + \". Ignore the directory and continue.\", e);\n         return pendingWorkCount;\n       }\n       if (children \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"The scanning start dir/sub dir \" + childPath\n               + \" does not have childrens.\");\n         }\n         return pendingWorkCount;\n       }\n \n       for (HdfsFileStatus child : children.getPartialListing()) {\n-        String childFullPath \u003d child.getFullName(childPath);\n         if (child.isFile()) {\n-          service.addFileToProcess(\n-              new ItemInfo\u003cString\u003e(startID, childFullPath), false);\n+          service.addFileToProcess(new ItemInfo(startID, child.getFileId()),\n+              false);\n           checkProcessingQueuesFree();\n           pendingWorkCount++; // increment to be satisfied file count\n         } else {\n+          String childFullPathName \u003d child.getFullName(childPath);\n           if (child.isDirectory()) {\n-            if (!childFullPath.endsWith(Path.SEPARATOR)) {\n-              childFullPath \u003d childFullPath + Path.SEPARATOR;\n+            if (!childFullPathName.endsWith(Path.SEPARATOR)) {\n+              childFullPathName \u003d childFullPathName + Path.SEPARATOR;\n             }\n-            pendingWorkCount +\u003d processPath(startID, childFullPath);\n+            pendingWorkCount +\u003d processPath(startID, childFullPathName);\n           }\n         }\n       }\n \n       if (children.hasMore()) {\n         lastReturnedName \u003d children.getLastName();\n       } else {\n         return pendingWorkCount;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long processPath(Long startID, String childPath) {\n    long pendingWorkCount \u003d 0; // to be satisfied file counter\n    for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n      final DirectoryListing children;\n      try {\n        children \u003d dfs.getClient().listPaths(childPath,\n            lastReturnedName, false);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to list directory \" + childPath\n            + \". Ignore the directory and continue.\", e);\n        return pendingWorkCount;\n      }\n      if (children \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The scanning start dir/sub dir \" + childPath\n              + \" does not have childrens.\");\n        }\n        return pendingWorkCount;\n      }\n\n      for (HdfsFileStatus child : children.getPartialListing()) {\n        if (child.isFile()) {\n          service.addFileToProcess(new ItemInfo(startID, child.getFileId()),\n              false);\n          checkProcessingQueuesFree();\n          pendingWorkCount++; // increment to be satisfied file count\n        } else {\n          String childFullPathName \u003d child.getFullName(childPath);\n          if (child.isDirectory()) {\n            if (!childFullPathName.endsWith(Path.SEPARATOR)) {\n              childFullPathName \u003d childFullPathName + Path.SEPARATOR;\n            }\n            pendingWorkCount +\u003d processPath(startID, childFullPathName);\n          }\n        }\n      }\n\n      if (children.hasMore()) {\n        lastReturnedName \u003d children.getLastName();\n      } else {\n        return pendingWorkCount;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFilePathCollector.java",
          "extendedDetails": {}
        }
      ]
    },
    "8467ec24fb74f30371d5a13e893fc56309ee9372": {
      "type": "Ymultichange(Yfilerename,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-13110: [SPS]: Reduce the number of APIs in NamenodeProtocol used by external satisfier. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "8467ec24fb74f30371d5a13e893fc56309ee9372",
      "commitAuthor": "Rakesh Radhakrishnan",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-13110: [SPS]: Reduce the number of APIs in NamenodeProtocol used by external satisfier. Contributed by Rakesh R.\n",
          "commitDate": "12/08/18 3:06 AM",
          "commitName": "8467ec24fb74f30371d5a13e893fc56309ee9372",
          "commitAuthor": "Rakesh Radhakrishnan",
          "commitDateOld": "12/08/18 3:06 AM",
          "commitNameOld": "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private long processPath(long startID, String fullPath) {\n+  private long processPath(String startID, String childPath) {\n     long pendingWorkCount \u003d 0; // to be satisfied file counter\n     for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n       final DirectoryListing children;\n       try {\n-        children \u003d dfs.getClient().listPaths(fullPath, lastReturnedName, false);\n+        children \u003d dfs.getClient().listPaths(childPath, lastReturnedName,\n+            false);\n       } catch (IOException e) {\n-        LOG.warn(\"Failed to list directory \" + fullPath\n+        LOG.warn(\"Failed to list directory \" + childPath\n             + \". Ignore the directory and continue.\", e);\n         return pendingWorkCount;\n       }\n       if (children \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"The scanning start dir/sub dir \" + fullPath\n+          LOG.debug(\"The scanning start dir/sub dir \" + childPath\n               + \" does not have childrens.\");\n         }\n         return pendingWorkCount;\n       }\n \n       for (HdfsFileStatus child : children.getPartialListing()) {\n+        String childFullPath \u003d child.getFullName(childPath);\n         if (child.isFile()) {\n-          service.addFileIdToProcess(new ItemInfo(startID, child.getFileId()),\n-              false);\n+          service.addFileToProcess(\n+              new ItemInfo\u003cString\u003e(startID, childFullPath), false);\n           checkProcessingQueuesFree();\n           pendingWorkCount++; // increment to be satisfied file count\n         } else {\n-          String fullPathStr \u003d child.getFullName(fullPath);\n           if (child.isDirectory()) {\n-            if (!fullPathStr.endsWith(Path.SEPARATOR)) {\n-              fullPathStr \u003d fullPathStr + Path.SEPARATOR;\n+            if (!childFullPath.endsWith(Path.SEPARATOR)) {\n+              childFullPath \u003d childFullPath + Path.SEPARATOR;\n             }\n-            pendingWorkCount +\u003d processPath(startID, fullPathStr);\n+            pendingWorkCount +\u003d processPath(startID, childFullPath);\n           }\n         }\n       }\n \n       if (children.hasMore()) {\n         lastReturnedName \u003d children.getLastName();\n       } else {\n         return pendingWorkCount;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long processPath(String startID, String childPath) {\n    long pendingWorkCount \u003d 0; // to be satisfied file counter\n    for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n      final DirectoryListing children;\n      try {\n        children \u003d dfs.getClient().listPaths(childPath, lastReturnedName,\n            false);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to list directory \" + childPath\n            + \". Ignore the directory and continue.\", e);\n        return pendingWorkCount;\n      }\n      if (children \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The scanning start dir/sub dir \" + childPath\n              + \" does not have childrens.\");\n        }\n        return pendingWorkCount;\n      }\n\n      for (HdfsFileStatus child : children.getPartialListing()) {\n        String childFullPath \u003d child.getFullName(childPath);\n        if (child.isFile()) {\n          service.addFileToProcess(\n              new ItemInfo\u003cString\u003e(startID, childFullPath), false);\n          checkProcessingQueuesFree();\n          pendingWorkCount++; // increment to be satisfied file count\n        } else {\n          if (child.isDirectory()) {\n            if (!childFullPath.endsWith(Path.SEPARATOR)) {\n              childFullPath \u003d childFullPath + Path.SEPARATOR;\n            }\n            pendingWorkCount +\u003d processPath(startID, childFullPath);\n          }\n        }\n      }\n\n      if (children.hasMore()) {\n        lastReturnedName \u003d children.getLastName();\n      } else {\n        return pendingWorkCount;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFilePathCollector.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFileIDCollector.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFilePathCollector.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13110: [SPS]: Reduce the number of APIs in NamenodeProtocol used by external satisfier. Contributed by Rakesh R.\n",
          "commitDate": "12/08/18 3:06 AM",
          "commitName": "8467ec24fb74f30371d5a13e893fc56309ee9372",
          "commitAuthor": "Rakesh Radhakrishnan",
          "commitDateOld": "12/08/18 3:06 AM",
          "commitNameOld": "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private long processPath(long startID, String fullPath) {\n+  private long processPath(String startID, String childPath) {\n     long pendingWorkCount \u003d 0; // to be satisfied file counter\n     for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n       final DirectoryListing children;\n       try {\n-        children \u003d dfs.getClient().listPaths(fullPath, lastReturnedName, false);\n+        children \u003d dfs.getClient().listPaths(childPath, lastReturnedName,\n+            false);\n       } catch (IOException e) {\n-        LOG.warn(\"Failed to list directory \" + fullPath\n+        LOG.warn(\"Failed to list directory \" + childPath\n             + \". Ignore the directory and continue.\", e);\n         return pendingWorkCount;\n       }\n       if (children \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"The scanning start dir/sub dir \" + fullPath\n+          LOG.debug(\"The scanning start dir/sub dir \" + childPath\n               + \" does not have childrens.\");\n         }\n         return pendingWorkCount;\n       }\n \n       for (HdfsFileStatus child : children.getPartialListing()) {\n+        String childFullPath \u003d child.getFullName(childPath);\n         if (child.isFile()) {\n-          service.addFileIdToProcess(new ItemInfo(startID, child.getFileId()),\n-              false);\n+          service.addFileToProcess(\n+              new ItemInfo\u003cString\u003e(startID, childFullPath), false);\n           checkProcessingQueuesFree();\n           pendingWorkCount++; // increment to be satisfied file count\n         } else {\n-          String fullPathStr \u003d child.getFullName(fullPath);\n           if (child.isDirectory()) {\n-            if (!fullPathStr.endsWith(Path.SEPARATOR)) {\n-              fullPathStr \u003d fullPathStr + Path.SEPARATOR;\n+            if (!childFullPath.endsWith(Path.SEPARATOR)) {\n+              childFullPath \u003d childFullPath + Path.SEPARATOR;\n             }\n-            pendingWorkCount +\u003d processPath(startID, fullPathStr);\n+            pendingWorkCount +\u003d processPath(startID, childFullPath);\n           }\n         }\n       }\n \n       if (children.hasMore()) {\n         lastReturnedName \u003d children.getLastName();\n       } else {\n         return pendingWorkCount;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long processPath(String startID, String childPath) {\n    long pendingWorkCount \u003d 0; // to be satisfied file counter\n    for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n      final DirectoryListing children;\n      try {\n        children \u003d dfs.getClient().listPaths(childPath, lastReturnedName,\n            false);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to list directory \" + childPath\n            + \". Ignore the directory and continue.\", e);\n        return pendingWorkCount;\n      }\n      if (children \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The scanning start dir/sub dir \" + childPath\n              + \" does not have childrens.\");\n        }\n        return pendingWorkCount;\n      }\n\n      for (HdfsFileStatus child : children.getPartialListing()) {\n        String childFullPath \u003d child.getFullName(childPath);\n        if (child.isFile()) {\n          service.addFileToProcess(\n              new ItemInfo\u003cString\u003e(startID, childFullPath), false);\n          checkProcessingQueuesFree();\n          pendingWorkCount++; // increment to be satisfied file count\n        } else {\n          if (child.isDirectory()) {\n            if (!childFullPath.endsWith(Path.SEPARATOR)) {\n              childFullPath \u003d childFullPath + Path.SEPARATOR;\n            }\n            pendingWorkCount +\u003d processPath(startID, childFullPath);\n          }\n        }\n      }\n\n      if (children.hasMore()) {\n        lastReturnedName \u003d children.getLastName();\n      } else {\n        return pendingWorkCount;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFilePathCollector.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13110: [SPS]: Reduce the number of APIs in NamenodeProtocol used by external satisfier. Contributed by Rakesh R.\n",
          "commitDate": "12/08/18 3:06 AM",
          "commitName": "8467ec24fb74f30371d5a13e893fc56309ee9372",
          "commitAuthor": "Rakesh Radhakrishnan",
          "commitDateOld": "12/08/18 3:06 AM",
          "commitNameOld": "4402f3f8557527d5c6cdad6f5bdcbd707b8cbf52",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,43 +1,44 @@\n-  private long processPath(long startID, String fullPath) {\n+  private long processPath(String startID, String childPath) {\n     long pendingWorkCount \u003d 0; // to be satisfied file counter\n     for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n       final DirectoryListing children;\n       try {\n-        children \u003d dfs.getClient().listPaths(fullPath, lastReturnedName, false);\n+        children \u003d dfs.getClient().listPaths(childPath, lastReturnedName,\n+            false);\n       } catch (IOException e) {\n-        LOG.warn(\"Failed to list directory \" + fullPath\n+        LOG.warn(\"Failed to list directory \" + childPath\n             + \". Ignore the directory and continue.\", e);\n         return pendingWorkCount;\n       }\n       if (children \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"The scanning start dir/sub dir \" + fullPath\n+          LOG.debug(\"The scanning start dir/sub dir \" + childPath\n               + \" does not have childrens.\");\n         }\n         return pendingWorkCount;\n       }\n \n       for (HdfsFileStatus child : children.getPartialListing()) {\n+        String childFullPath \u003d child.getFullName(childPath);\n         if (child.isFile()) {\n-          service.addFileIdToProcess(new ItemInfo(startID, child.getFileId()),\n-              false);\n+          service.addFileToProcess(\n+              new ItemInfo\u003cString\u003e(startID, childFullPath), false);\n           checkProcessingQueuesFree();\n           pendingWorkCount++; // increment to be satisfied file count\n         } else {\n-          String fullPathStr \u003d child.getFullName(fullPath);\n           if (child.isDirectory()) {\n-            if (!fullPathStr.endsWith(Path.SEPARATOR)) {\n-              fullPathStr \u003d fullPathStr + Path.SEPARATOR;\n+            if (!childFullPath.endsWith(Path.SEPARATOR)) {\n+              childFullPath \u003d childFullPath + Path.SEPARATOR;\n             }\n-            pendingWorkCount +\u003d processPath(startID, fullPathStr);\n+            pendingWorkCount +\u003d processPath(startID, childFullPath);\n           }\n         }\n       }\n \n       if (children.hasMore()) {\n         lastReturnedName \u003d children.getLastName();\n       } else {\n         return pendingWorkCount;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long processPath(String startID, String childPath) {\n    long pendingWorkCount \u003d 0; // to be satisfied file counter\n    for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n      final DirectoryListing children;\n      try {\n        children \u003d dfs.getClient().listPaths(childPath, lastReturnedName,\n            false);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to list directory \" + childPath\n            + \". Ignore the directory and continue.\", e);\n        return pendingWorkCount;\n      }\n      if (children \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The scanning start dir/sub dir \" + childPath\n              + \" does not have childrens.\");\n        }\n        return pendingWorkCount;\n      }\n\n      for (HdfsFileStatus child : children.getPartialListing()) {\n        String childFullPath \u003d child.getFullName(childPath);\n        if (child.isFile()) {\n          service.addFileToProcess(\n              new ItemInfo\u003cString\u003e(startID, childFullPath), false);\n          checkProcessingQueuesFree();\n          pendingWorkCount++; // increment to be satisfied file count\n        } else {\n          if (child.isDirectory()) {\n            if (!childFullPath.endsWith(Path.SEPARATOR)) {\n              childFullPath \u003d childFullPath + Path.SEPARATOR;\n            }\n            pendingWorkCount +\u003d processPath(startID, childFullPath);\n          }\n        }\n      }\n\n      if (children.hasMore()) {\n        lastReturnedName \u003d children.getLastName();\n      } else {\n        return pendingWorkCount;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFilePathCollector.java",
          "extendedDetails": {
            "oldValue": "[startID-long, fullPath-String]",
            "newValue": "[startID-String, childPath-String]"
          }
        }
      ]
    },
    "3b83110d5ed582b9f913ecf3f62ce410535f8fca": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-13057: [SPS]: Revisit configurations to make SPS service modes internal/external/none. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "3b83110d5ed582b9f913ecf3f62ce410535f8fca",
      "commitAuthor": "Uma Maheswara Rao G",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-13057: [SPS]: Revisit configurations to make SPS service modes internal/external/none. Contributed by Rakesh R.\n",
          "commitDate": "12/08/18 3:06 AM",
          "commitName": "3b83110d5ed582b9f913ecf3f62ce410535f8fca",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "12/08/18 3:06 AM",
          "commitNameOld": "3159b39cf8ef704835325263154fb1a1cecc109d",
          "commitAuthorOld": "Rakesh Radhakrishnan",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,43 @@\n-  private void processPath(long startID, String fullPath) {\n+  private long processPath(long startID, String fullPath) {\n+    long pendingWorkCount \u003d 0; // to be satisfied file counter\n     for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n       final DirectoryListing children;\n       try {\n         children \u003d dfs.getClient().listPaths(fullPath, lastReturnedName, false);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to list directory \" + fullPath\n             + \". Ignore the directory and continue.\", e);\n-        return;\n+        return pendingWorkCount;\n       }\n       if (children \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"The scanning start dir/sub dir \" + fullPath\n               + \" does not have childrens.\");\n         }\n-        return;\n+        return pendingWorkCount;\n       }\n \n       for (HdfsFileStatus child : children.getPartialListing()) {\n         if (child.isFile()) {\n           service.addFileIdToProcess(new ItemInfo(startID, child.getFileId()),\n               false);\n           checkProcessingQueuesFree();\n+          pendingWorkCount++; // increment to be satisfied file count\n         } else {\n           String fullPathStr \u003d child.getFullName(fullPath);\n           if (child.isDirectory()) {\n             if (!fullPathStr.endsWith(Path.SEPARATOR)) {\n               fullPathStr \u003d fullPathStr + Path.SEPARATOR;\n             }\n-            processPath(startID, fullPathStr);\n+            pendingWorkCount +\u003d processPath(startID, fullPathStr);\n           }\n         }\n       }\n \n       if (children.hasMore()) {\n         lastReturnedName \u003d children.getLastName();\n       } else {\n-        return;\n+        return pendingWorkCount;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long processPath(long startID, String fullPath) {\n    long pendingWorkCount \u003d 0; // to be satisfied file counter\n    for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n      final DirectoryListing children;\n      try {\n        children \u003d dfs.getClient().listPaths(fullPath, lastReturnedName, false);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to list directory \" + fullPath\n            + \". Ignore the directory and continue.\", e);\n        return pendingWorkCount;\n      }\n      if (children \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The scanning start dir/sub dir \" + fullPath\n              + \" does not have childrens.\");\n        }\n        return pendingWorkCount;\n      }\n\n      for (HdfsFileStatus child : children.getPartialListing()) {\n        if (child.isFile()) {\n          service.addFileIdToProcess(new ItemInfo(startID, child.getFileId()),\n              false);\n          checkProcessingQueuesFree();\n          pendingWorkCount++; // increment to be satisfied file count\n        } else {\n          String fullPathStr \u003d child.getFullName(fullPath);\n          if (child.isDirectory()) {\n            if (!fullPathStr.endsWith(Path.SEPARATOR)) {\n              fullPathStr \u003d fullPathStr + Path.SEPARATOR;\n            }\n            pendingWorkCount +\u003d processPath(startID, fullPathStr);\n          }\n        }\n      }\n\n      if (children.hasMore()) {\n        lastReturnedName \u003d children.getLastName();\n      } else {\n        return pendingWorkCount;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFileIDCollector.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "long"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13057: [SPS]: Revisit configurations to make SPS service modes internal/external/none. Contributed by Rakesh R.\n",
          "commitDate": "12/08/18 3:06 AM",
          "commitName": "3b83110d5ed582b9f913ecf3f62ce410535f8fca",
          "commitAuthor": "Uma Maheswara Rao G",
          "commitDateOld": "12/08/18 3:06 AM",
          "commitNameOld": "3159b39cf8ef704835325263154fb1a1cecc109d",
          "commitAuthorOld": "Rakesh Radhakrishnan",
          "daysBetweenCommits": 0.0,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,41 +1,43 @@\n-  private void processPath(long startID, String fullPath) {\n+  private long processPath(long startID, String fullPath) {\n+    long pendingWorkCount \u003d 0; // to be satisfied file counter\n     for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n       final DirectoryListing children;\n       try {\n         children \u003d dfs.getClient().listPaths(fullPath, lastReturnedName, false);\n       } catch (IOException e) {\n         LOG.warn(\"Failed to list directory \" + fullPath\n             + \". Ignore the directory and continue.\", e);\n-        return;\n+        return pendingWorkCount;\n       }\n       if (children \u003d\u003d null) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"The scanning start dir/sub dir \" + fullPath\n               + \" does not have childrens.\");\n         }\n-        return;\n+        return pendingWorkCount;\n       }\n \n       for (HdfsFileStatus child : children.getPartialListing()) {\n         if (child.isFile()) {\n           service.addFileIdToProcess(new ItemInfo(startID, child.getFileId()),\n               false);\n           checkProcessingQueuesFree();\n+          pendingWorkCount++; // increment to be satisfied file count\n         } else {\n           String fullPathStr \u003d child.getFullName(fullPath);\n           if (child.isDirectory()) {\n             if (!fullPathStr.endsWith(Path.SEPARATOR)) {\n               fullPathStr \u003d fullPathStr + Path.SEPARATOR;\n             }\n-            processPath(startID, fullPathStr);\n+            pendingWorkCount +\u003d processPath(startID, fullPathStr);\n           }\n         }\n       }\n \n       if (children.hasMore()) {\n         lastReturnedName \u003d children.getLastName();\n       } else {\n-        return;\n+        return pendingWorkCount;\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private long processPath(long startID, String fullPath) {\n    long pendingWorkCount \u003d 0; // to be satisfied file counter\n    for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n      final DirectoryListing children;\n      try {\n        children \u003d dfs.getClient().listPaths(fullPath, lastReturnedName, false);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to list directory \" + fullPath\n            + \". Ignore the directory and continue.\", e);\n        return pendingWorkCount;\n      }\n      if (children \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The scanning start dir/sub dir \" + fullPath\n              + \" does not have childrens.\");\n        }\n        return pendingWorkCount;\n      }\n\n      for (HdfsFileStatus child : children.getPartialListing()) {\n        if (child.isFile()) {\n          service.addFileIdToProcess(new ItemInfo(startID, child.getFileId()),\n              false);\n          checkProcessingQueuesFree();\n          pendingWorkCount++; // increment to be satisfied file count\n        } else {\n          String fullPathStr \u003d child.getFullName(fullPath);\n          if (child.isDirectory()) {\n            if (!fullPathStr.endsWith(Path.SEPARATOR)) {\n              fullPathStr \u003d fullPathStr + Path.SEPARATOR;\n            }\n            pendingWorkCount +\u003d processPath(startID, fullPathStr);\n          }\n        }\n      }\n\n      if (children.hasMore()) {\n        lastReturnedName \u003d children.getLastName();\n      } else {\n        return pendingWorkCount;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFileIDCollector.java",
          "extendedDetails": {}
        }
      ]
    },
    "3159b39cf8ef704835325263154fb1a1cecc109d": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13025. [SPS]: Implement a mechanism to scan the files for external SPS. Contributed by Uma Maheswara Rao G.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "3159b39cf8ef704835325263154fb1a1cecc109d",
      "commitAuthor": "Rakesh Radhakrishnan",
      "diff": "@@ -0,0 +1,41 @@\n+  private void processPath(long startID, String fullPath) {\n+    for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n+      final DirectoryListing children;\n+      try {\n+        children \u003d dfs.getClient().listPaths(fullPath, lastReturnedName, false);\n+      } catch (IOException e) {\n+        LOG.warn(\"Failed to list directory \" + fullPath\n+            + \". Ignore the directory and continue.\", e);\n+        return;\n+      }\n+      if (children \u003d\u003d null) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"The scanning start dir/sub dir \" + fullPath\n+              + \" does not have childrens.\");\n+        }\n+        return;\n+      }\n+\n+      for (HdfsFileStatus child : children.getPartialListing()) {\n+        if (child.isFile()) {\n+          service.addFileIdToProcess(new ItemInfo(startID, child.getFileId()),\n+              false);\n+          checkProcessingQueuesFree();\n+        } else {\n+          String fullPathStr \u003d child.getFullName(fullPath);\n+          if (child.isDirectory()) {\n+            if (!fullPathStr.endsWith(Path.SEPARATOR)) {\n+              fullPathStr \u003d fullPathStr + Path.SEPARATOR;\n+            }\n+            processPath(startID, fullPathStr);\n+          }\n+        }\n+      }\n+\n+      if (children.hasMore()) {\n+        lastReturnedName \u003d children.getLastName();\n+      } else {\n+        return;\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void processPath(long startID, String fullPath) {\n    for (byte[] lastReturnedName \u003d HdfsFileStatus.EMPTY_NAME;;) {\n      final DirectoryListing children;\n      try {\n        children \u003d dfs.getClient().listPaths(fullPath, lastReturnedName, false);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to list directory \" + fullPath\n            + \". Ignore the directory and continue.\", e);\n        return;\n      }\n      if (children \u003d\u003d null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The scanning start dir/sub dir \" + fullPath\n              + \" does not have childrens.\");\n        }\n        return;\n      }\n\n      for (HdfsFileStatus child : children.getPartialListing()) {\n        if (child.isFile()) {\n          service.addFileIdToProcess(new ItemInfo(startID, child.getFileId()),\n              false);\n          checkProcessingQueuesFree();\n        } else {\n          String fullPathStr \u003d child.getFullName(fullPath);\n          if (child.isDirectory()) {\n            if (!fullPathStr.endsWith(Path.SEPARATOR)) {\n              fullPathStr \u003d fullPathStr + Path.SEPARATOR;\n            }\n            processPath(startID, fullPathStr);\n          }\n        }\n      }\n\n      if (children.hasMore()) {\n        lastReturnedName \u003d children.getLastName();\n      } else {\n        return;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/sps/ExternalSPSFileIDCollector.java"
    }
  }
}