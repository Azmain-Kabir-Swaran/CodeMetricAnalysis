{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DatanodeManager.java",
  "functionName": "getBlockRecoveryCommand",
  "functionId": "getBlockRecoveryCommand___blockPoolId-String__nodeinfo-DatanodeDescriptor",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
  "functionStartLine": 1584,
  "functionEndLine": 1643,
  "numCommitsSeen": 172,
  "timeTaken": 4219,
  "changeHistory": [
    "61a9b4f58b639e71c564d84b529ac66aaae7f8ef",
    "61ab0440f7eaff0f631cbae0378403912f88d7ad",
    "8602692338d6f493647205e0241e4116211fab75"
  ],
  "changeHistoryShort": {
    "61a9b4f58b639e71c564d84b529ac66aaae7f8ef": "Ymultichange(Yexceptionschange,Ybodychange)",
    "61ab0440f7eaff0f631cbae0378403912f88d7ad": "Ybodychange",
    "8602692338d6f493647205e0241e4116211fab75": "Yintroduced"
  },
  "changeHistoryDetails": {
    "61a9b4f58b639e71c564d84b529ac66aaae7f8ef": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-13758. DatanodeManager should throw exception if it has BlockRecoveryCommand but the block is not under construction. Contributed by chencan.\n",
      "commitDate": "14/08/18 11:51 AM",
      "commitName": "61a9b4f58b639e71c564d84b529ac66aaae7f8ef",
      "commitAuthor": "Wei-Chiu Chuang",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-13758. DatanodeManager should throw exception if it has BlockRecoveryCommand but the block is not under construction. Contributed by chencan.\n",
          "commitDate": "14/08/18 11:51 AM",
          "commitName": "61a9b4f58b639e71c564d84b529ac66aaae7f8ef",
          "commitAuthor": "Wei-Chiu Chuang",
          "commitDateOld": "12/08/18 3:06 AM",
          "commitNameOld": "3ac07b720b7839a7fe6c83f4ccfe319b6a892501",
          "commitAuthorOld": "Uma Maheswara Rao Gangumalla",
          "daysBetweenCommits": 2.36,
          "commitsBetweenForRepo": 15,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,60 @@\n   private BlockRecoveryCommand getBlockRecoveryCommand(String blockPoolId,\n-      DatanodeDescriptor nodeinfo) {\n+      DatanodeDescriptor nodeinfo) throws IOException {\n     BlockInfo[] blocks \u003d nodeinfo.getLeaseRecoveryCommand(Integer.MAX_VALUE);\n     if (blocks \u003d\u003d null) {\n       return null;\n     }\n     BlockRecoveryCommand brCommand \u003d new BlockRecoveryCommand(blocks.length);\n     for (BlockInfo b : blocks) {\n       BlockUnderConstructionFeature uc \u003d b.getUnderConstructionFeature();\n-      assert uc !\u003d null;\n+      if(uc \u003d\u003d null) {\n+        throw new IOException(\"Recovery block \" + b +\n+            \"where it is not under construction.\");\n+      }\n       final DatanodeStorageInfo[] storages \u003d uc.getExpectedStorageLocations();\n       // Skip stale nodes during recovery\n       final List\u003cDatanodeStorageInfo\u003e recoveryLocations \u003d\n           new ArrayList\u003c\u003e(storages.length);\n       for (DatanodeStorageInfo storage : storages) {\n         if (!storage.getDatanodeDescriptor().isStale(staleInterval)) {\n           recoveryLocations.add(storage);\n         }\n       }\n       // If we are performing a truncate recovery than set recovery fields\n       // to old block.\n       boolean truncateRecovery \u003d uc.getTruncateBlock() !\u003d null;\n       boolean copyOnTruncateRecovery \u003d truncateRecovery \u0026\u0026\n           uc.getTruncateBlock().getBlockId() !\u003d b.getBlockId();\n       ExtendedBlock primaryBlock \u003d (copyOnTruncateRecovery) ?\n           new ExtendedBlock(blockPoolId, uc.getTruncateBlock()) :\n           new ExtendedBlock(blockPoolId, b);\n       // If we only get 1 replica after eliminating stale nodes, choose all\n       // replicas for recovery and let the primary data node handle failures.\n       DatanodeInfo[] recoveryInfos;\n       if (recoveryLocations.size() \u003e 1) {\n         if (recoveryLocations.size() !\u003d storages.length) {\n           LOG.info(\"Skipped stale nodes for recovery : \"\n               + (storages.length - recoveryLocations.size()));\n         }\n         recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(recoveryLocations);\n       } else {\n         // If too many replicas are stale, then choose all replicas to\n         // participate in block recovery.\n         recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(storages);\n       }\n       RecoveringBlock rBlock;\n       if (truncateRecovery) {\n         Block recoveryBlock \u003d (copyOnTruncateRecovery) ? b : uc.getTruncateBlock();\n         rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos, recoveryBlock);\n       } else {\n         rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos,\n             uc.getBlockRecoveryId());\n         if (b.isStriped()) {\n           rBlock \u003d new RecoveringStripedBlock(rBlock, uc.getBlockIndices(),\n               ((BlockInfoStriped) b).getErasureCodingPolicy());\n         }\n       }\n       brCommand.add(rBlock);\n     }\n     return brCommand;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private BlockRecoveryCommand getBlockRecoveryCommand(String blockPoolId,\n      DatanodeDescriptor nodeinfo) throws IOException {\n    BlockInfo[] blocks \u003d nodeinfo.getLeaseRecoveryCommand(Integer.MAX_VALUE);\n    if (blocks \u003d\u003d null) {\n      return null;\n    }\n    BlockRecoveryCommand brCommand \u003d new BlockRecoveryCommand(blocks.length);\n    for (BlockInfo b : blocks) {\n      BlockUnderConstructionFeature uc \u003d b.getUnderConstructionFeature();\n      if(uc \u003d\u003d null) {\n        throw new IOException(\"Recovery block \" + b +\n            \"where it is not under construction.\");\n      }\n      final DatanodeStorageInfo[] storages \u003d uc.getExpectedStorageLocations();\n      // Skip stale nodes during recovery\n      final List\u003cDatanodeStorageInfo\u003e recoveryLocations \u003d\n          new ArrayList\u003c\u003e(storages.length);\n      for (DatanodeStorageInfo storage : storages) {\n        if (!storage.getDatanodeDescriptor().isStale(staleInterval)) {\n          recoveryLocations.add(storage);\n        }\n      }\n      // If we are performing a truncate recovery than set recovery fields\n      // to old block.\n      boolean truncateRecovery \u003d uc.getTruncateBlock() !\u003d null;\n      boolean copyOnTruncateRecovery \u003d truncateRecovery \u0026\u0026\n          uc.getTruncateBlock().getBlockId() !\u003d b.getBlockId();\n      ExtendedBlock primaryBlock \u003d (copyOnTruncateRecovery) ?\n          new ExtendedBlock(blockPoolId, uc.getTruncateBlock()) :\n          new ExtendedBlock(blockPoolId, b);\n      // If we only get 1 replica after eliminating stale nodes, choose all\n      // replicas for recovery and let the primary data node handle failures.\n      DatanodeInfo[] recoveryInfos;\n      if (recoveryLocations.size() \u003e 1) {\n        if (recoveryLocations.size() !\u003d storages.length) {\n          LOG.info(\"Skipped stale nodes for recovery : \"\n              + (storages.length - recoveryLocations.size()));\n        }\n        recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(recoveryLocations);\n      } else {\n        // If too many replicas are stale, then choose all replicas to\n        // participate in block recovery.\n        recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(storages);\n      }\n      RecoveringBlock rBlock;\n      if (truncateRecovery) {\n        Block recoveryBlock \u003d (copyOnTruncateRecovery) ? b : uc.getTruncateBlock();\n        rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos, recoveryBlock);\n      } else {\n        rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos,\n            uc.getBlockRecoveryId());\n        if (b.isStriped()) {\n          rBlock \u003d new RecoveringStripedBlock(rBlock, uc.getBlockIndices(),\n              ((BlockInfoStriped) b).getErasureCodingPolicy());\n        }\n      }\n      brCommand.add(rBlock);\n    }\n    return brCommand;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13758. DatanodeManager should throw exception if it has BlockRecoveryCommand but the block is not under construction. Contributed by chencan.\n",
          "commitDate": "14/08/18 11:51 AM",
          "commitName": "61a9b4f58b639e71c564d84b529ac66aaae7f8ef",
          "commitAuthor": "Wei-Chiu Chuang",
          "commitDateOld": "12/08/18 3:06 AM",
          "commitNameOld": "3ac07b720b7839a7fe6c83f4ccfe319b6a892501",
          "commitAuthorOld": "Uma Maheswara Rao Gangumalla",
          "daysBetweenCommits": 2.36,
          "commitsBetweenForRepo": 15,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,57 +1,60 @@\n   private BlockRecoveryCommand getBlockRecoveryCommand(String blockPoolId,\n-      DatanodeDescriptor nodeinfo) {\n+      DatanodeDescriptor nodeinfo) throws IOException {\n     BlockInfo[] blocks \u003d nodeinfo.getLeaseRecoveryCommand(Integer.MAX_VALUE);\n     if (blocks \u003d\u003d null) {\n       return null;\n     }\n     BlockRecoveryCommand brCommand \u003d new BlockRecoveryCommand(blocks.length);\n     for (BlockInfo b : blocks) {\n       BlockUnderConstructionFeature uc \u003d b.getUnderConstructionFeature();\n-      assert uc !\u003d null;\n+      if(uc \u003d\u003d null) {\n+        throw new IOException(\"Recovery block \" + b +\n+            \"where it is not under construction.\");\n+      }\n       final DatanodeStorageInfo[] storages \u003d uc.getExpectedStorageLocations();\n       // Skip stale nodes during recovery\n       final List\u003cDatanodeStorageInfo\u003e recoveryLocations \u003d\n           new ArrayList\u003c\u003e(storages.length);\n       for (DatanodeStorageInfo storage : storages) {\n         if (!storage.getDatanodeDescriptor().isStale(staleInterval)) {\n           recoveryLocations.add(storage);\n         }\n       }\n       // If we are performing a truncate recovery than set recovery fields\n       // to old block.\n       boolean truncateRecovery \u003d uc.getTruncateBlock() !\u003d null;\n       boolean copyOnTruncateRecovery \u003d truncateRecovery \u0026\u0026\n           uc.getTruncateBlock().getBlockId() !\u003d b.getBlockId();\n       ExtendedBlock primaryBlock \u003d (copyOnTruncateRecovery) ?\n           new ExtendedBlock(blockPoolId, uc.getTruncateBlock()) :\n           new ExtendedBlock(blockPoolId, b);\n       // If we only get 1 replica after eliminating stale nodes, choose all\n       // replicas for recovery and let the primary data node handle failures.\n       DatanodeInfo[] recoveryInfos;\n       if (recoveryLocations.size() \u003e 1) {\n         if (recoveryLocations.size() !\u003d storages.length) {\n           LOG.info(\"Skipped stale nodes for recovery : \"\n               + (storages.length - recoveryLocations.size()));\n         }\n         recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(recoveryLocations);\n       } else {\n         // If too many replicas are stale, then choose all replicas to\n         // participate in block recovery.\n         recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(storages);\n       }\n       RecoveringBlock rBlock;\n       if (truncateRecovery) {\n         Block recoveryBlock \u003d (copyOnTruncateRecovery) ? b : uc.getTruncateBlock();\n         rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos, recoveryBlock);\n       } else {\n         rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos,\n             uc.getBlockRecoveryId());\n         if (b.isStriped()) {\n           rBlock \u003d new RecoveringStripedBlock(rBlock, uc.getBlockIndices(),\n               ((BlockInfoStriped) b).getErasureCodingPolicy());\n         }\n       }\n       brCommand.add(rBlock);\n     }\n     return brCommand;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private BlockRecoveryCommand getBlockRecoveryCommand(String blockPoolId,\n      DatanodeDescriptor nodeinfo) throws IOException {\n    BlockInfo[] blocks \u003d nodeinfo.getLeaseRecoveryCommand(Integer.MAX_VALUE);\n    if (blocks \u003d\u003d null) {\n      return null;\n    }\n    BlockRecoveryCommand brCommand \u003d new BlockRecoveryCommand(blocks.length);\n    for (BlockInfo b : blocks) {\n      BlockUnderConstructionFeature uc \u003d b.getUnderConstructionFeature();\n      if(uc \u003d\u003d null) {\n        throw new IOException(\"Recovery block \" + b +\n            \"where it is not under construction.\");\n      }\n      final DatanodeStorageInfo[] storages \u003d uc.getExpectedStorageLocations();\n      // Skip stale nodes during recovery\n      final List\u003cDatanodeStorageInfo\u003e recoveryLocations \u003d\n          new ArrayList\u003c\u003e(storages.length);\n      for (DatanodeStorageInfo storage : storages) {\n        if (!storage.getDatanodeDescriptor().isStale(staleInterval)) {\n          recoveryLocations.add(storage);\n        }\n      }\n      // If we are performing a truncate recovery than set recovery fields\n      // to old block.\n      boolean truncateRecovery \u003d uc.getTruncateBlock() !\u003d null;\n      boolean copyOnTruncateRecovery \u003d truncateRecovery \u0026\u0026\n          uc.getTruncateBlock().getBlockId() !\u003d b.getBlockId();\n      ExtendedBlock primaryBlock \u003d (copyOnTruncateRecovery) ?\n          new ExtendedBlock(blockPoolId, uc.getTruncateBlock()) :\n          new ExtendedBlock(blockPoolId, b);\n      // If we only get 1 replica after eliminating stale nodes, choose all\n      // replicas for recovery and let the primary data node handle failures.\n      DatanodeInfo[] recoveryInfos;\n      if (recoveryLocations.size() \u003e 1) {\n        if (recoveryLocations.size() !\u003d storages.length) {\n          LOG.info(\"Skipped stale nodes for recovery : \"\n              + (storages.length - recoveryLocations.size()));\n        }\n        recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(recoveryLocations);\n      } else {\n        // If too many replicas are stale, then choose all replicas to\n        // participate in block recovery.\n        recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(storages);\n      }\n      RecoveringBlock rBlock;\n      if (truncateRecovery) {\n        Block recoveryBlock \u003d (copyOnTruncateRecovery) ? b : uc.getTruncateBlock();\n        rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos, recoveryBlock);\n      } else {\n        rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos,\n            uc.getBlockRecoveryId());\n        if (b.isStriped()) {\n          rBlock \u003d new RecoveringStripedBlock(rBlock, uc.getBlockIndices(),\n              ((BlockInfoStriped) b).getErasureCodingPolicy());\n        }\n      }\n      brCommand.add(rBlock);\n    }\n    return brCommand;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "61ab0440f7eaff0f631cbae0378403912f88d7ad": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9173. Erasure Coding: Lease recovery for striped file. Contributed by Walter Su and Jing Zhao.\n\nChange-Id: I51703a61c9d8454f883028f3f6acb5729fde1b15\n",
      "commitDate": "18/12/15 3:57 PM",
      "commitName": "61ab0440f7eaff0f631cbae0378403912f88d7ad",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "16/12/15 6:16 PM",
      "commitNameOld": "f741476146574550a1a208d58ef8be76639e5ddc",
      "commitAuthorOld": "Uma Mahesh",
      "daysBetweenCommits": 1.9,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,57 @@\n   private BlockRecoveryCommand getBlockRecoveryCommand(String blockPoolId,\n       DatanodeDescriptor nodeinfo) {\n     BlockInfo[] blocks \u003d nodeinfo.getLeaseRecoveryCommand(Integer.MAX_VALUE);\n     if (blocks \u003d\u003d null) {\n       return null;\n     }\n     BlockRecoveryCommand brCommand \u003d new BlockRecoveryCommand(blocks.length);\n     for (BlockInfo b : blocks) {\n       BlockUnderConstructionFeature uc \u003d b.getUnderConstructionFeature();\n       assert uc !\u003d null;\n       final DatanodeStorageInfo[] storages \u003d uc.getExpectedStorageLocations();\n       // Skip stale nodes during recovery\n       final List\u003cDatanodeStorageInfo\u003e recoveryLocations \u003d\n           new ArrayList\u003c\u003e(storages.length);\n       for (DatanodeStorageInfo storage : storages) {\n         if (!storage.getDatanodeDescriptor().isStale(staleInterval)) {\n           recoveryLocations.add(storage);\n         }\n       }\n       // If we are performing a truncate recovery than set recovery fields\n       // to old block.\n       boolean truncateRecovery \u003d uc.getTruncateBlock() !\u003d null;\n       boolean copyOnTruncateRecovery \u003d truncateRecovery \u0026\u0026\n           uc.getTruncateBlock().getBlockId() !\u003d b.getBlockId();\n       ExtendedBlock primaryBlock \u003d (copyOnTruncateRecovery) ?\n           new ExtendedBlock(blockPoolId, uc.getTruncateBlock()) :\n           new ExtendedBlock(blockPoolId, b);\n       // If we only get 1 replica after eliminating stale nodes, choose all\n       // replicas for recovery and let the primary data node handle failures.\n       DatanodeInfo[] recoveryInfos;\n       if (recoveryLocations.size() \u003e 1) {\n         if (recoveryLocations.size() !\u003d storages.length) {\n           LOG.info(\"Skipped stale nodes for recovery : \"\n               + (storages.length - recoveryLocations.size()));\n         }\n         recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(recoveryLocations);\n       } else {\n         // If too many replicas are stale, then choose all replicas to\n         // participate in block recovery.\n         recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(storages);\n       }\n       RecoveringBlock rBlock;\n       if (truncateRecovery) {\n         Block recoveryBlock \u003d (copyOnTruncateRecovery) ? b : uc.getTruncateBlock();\n         rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos, recoveryBlock);\n       } else {\n         rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos,\n             uc.getBlockRecoveryId());\n+        if (b.isStriped()) {\n+          rBlock \u003d new RecoveringStripedBlock(rBlock, uc.getBlockIndices(),\n+              ((BlockInfoStriped) b).getErasureCodingPolicy());\n+        }\n       }\n       brCommand.add(rBlock);\n     }\n     return brCommand;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockRecoveryCommand getBlockRecoveryCommand(String blockPoolId,\n      DatanodeDescriptor nodeinfo) {\n    BlockInfo[] blocks \u003d nodeinfo.getLeaseRecoveryCommand(Integer.MAX_VALUE);\n    if (blocks \u003d\u003d null) {\n      return null;\n    }\n    BlockRecoveryCommand brCommand \u003d new BlockRecoveryCommand(blocks.length);\n    for (BlockInfo b : blocks) {\n      BlockUnderConstructionFeature uc \u003d b.getUnderConstructionFeature();\n      assert uc !\u003d null;\n      final DatanodeStorageInfo[] storages \u003d uc.getExpectedStorageLocations();\n      // Skip stale nodes during recovery\n      final List\u003cDatanodeStorageInfo\u003e recoveryLocations \u003d\n          new ArrayList\u003c\u003e(storages.length);\n      for (DatanodeStorageInfo storage : storages) {\n        if (!storage.getDatanodeDescriptor().isStale(staleInterval)) {\n          recoveryLocations.add(storage);\n        }\n      }\n      // If we are performing a truncate recovery than set recovery fields\n      // to old block.\n      boolean truncateRecovery \u003d uc.getTruncateBlock() !\u003d null;\n      boolean copyOnTruncateRecovery \u003d truncateRecovery \u0026\u0026\n          uc.getTruncateBlock().getBlockId() !\u003d b.getBlockId();\n      ExtendedBlock primaryBlock \u003d (copyOnTruncateRecovery) ?\n          new ExtendedBlock(blockPoolId, uc.getTruncateBlock()) :\n          new ExtendedBlock(blockPoolId, b);\n      // If we only get 1 replica after eliminating stale nodes, choose all\n      // replicas for recovery and let the primary data node handle failures.\n      DatanodeInfo[] recoveryInfos;\n      if (recoveryLocations.size() \u003e 1) {\n        if (recoveryLocations.size() !\u003d storages.length) {\n          LOG.info(\"Skipped stale nodes for recovery : \"\n              + (storages.length - recoveryLocations.size()));\n        }\n        recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(recoveryLocations);\n      } else {\n        // If too many replicas are stale, then choose all replicas to\n        // participate in block recovery.\n        recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(storages);\n      }\n      RecoveringBlock rBlock;\n      if (truncateRecovery) {\n        Block recoveryBlock \u003d (copyOnTruncateRecovery) ? b : uc.getTruncateBlock();\n        rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos, recoveryBlock);\n      } else {\n        rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos,\n            uc.getBlockRecoveryId());\n        if (b.isStriped()) {\n          rBlock \u003d new RecoveringStripedBlock(rBlock, uc.getBlockIndices(),\n              ((BlockInfoStriped) b).getErasureCodingPolicy());\n        }\n      }\n      brCommand.add(rBlock);\n    }\n    return brCommand;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
      "extendedDetails": {}
    },
    "8602692338d6f493647205e0241e4116211fab75": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9371. Code cleanup for DatanodeManager. Contributed by Jing Zhao.\n",
      "commitDate": "15/12/15 10:47 AM",
      "commitName": "8602692338d6f493647205e0241e4116211fab75",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,53 @@\n+  private BlockRecoveryCommand getBlockRecoveryCommand(String blockPoolId,\n+      DatanodeDescriptor nodeinfo) {\n+    BlockInfo[] blocks \u003d nodeinfo.getLeaseRecoveryCommand(Integer.MAX_VALUE);\n+    if (blocks \u003d\u003d null) {\n+      return null;\n+    }\n+    BlockRecoveryCommand brCommand \u003d new BlockRecoveryCommand(blocks.length);\n+    for (BlockInfo b : blocks) {\n+      BlockUnderConstructionFeature uc \u003d b.getUnderConstructionFeature();\n+      assert uc !\u003d null;\n+      final DatanodeStorageInfo[] storages \u003d uc.getExpectedStorageLocations();\n+      // Skip stale nodes during recovery\n+      final List\u003cDatanodeStorageInfo\u003e recoveryLocations \u003d\n+          new ArrayList\u003c\u003e(storages.length);\n+      for (DatanodeStorageInfo storage : storages) {\n+        if (!storage.getDatanodeDescriptor().isStale(staleInterval)) {\n+          recoveryLocations.add(storage);\n+        }\n+      }\n+      // If we are performing a truncate recovery than set recovery fields\n+      // to old block.\n+      boolean truncateRecovery \u003d uc.getTruncateBlock() !\u003d null;\n+      boolean copyOnTruncateRecovery \u003d truncateRecovery \u0026\u0026\n+          uc.getTruncateBlock().getBlockId() !\u003d b.getBlockId();\n+      ExtendedBlock primaryBlock \u003d (copyOnTruncateRecovery) ?\n+          new ExtendedBlock(blockPoolId, uc.getTruncateBlock()) :\n+          new ExtendedBlock(blockPoolId, b);\n+      // If we only get 1 replica after eliminating stale nodes, choose all\n+      // replicas for recovery and let the primary data node handle failures.\n+      DatanodeInfo[] recoveryInfos;\n+      if (recoveryLocations.size() \u003e 1) {\n+        if (recoveryLocations.size() !\u003d storages.length) {\n+          LOG.info(\"Skipped stale nodes for recovery : \"\n+              + (storages.length - recoveryLocations.size()));\n+        }\n+        recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(recoveryLocations);\n+      } else {\n+        // If too many replicas are stale, then choose all replicas to\n+        // participate in block recovery.\n+        recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(storages);\n+      }\n+      RecoveringBlock rBlock;\n+      if (truncateRecovery) {\n+        Block recoveryBlock \u003d (copyOnTruncateRecovery) ? b : uc.getTruncateBlock();\n+        rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos, recoveryBlock);\n+      } else {\n+        rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos,\n+            uc.getBlockRecoveryId());\n+      }\n+      brCommand.add(rBlock);\n+    }\n+    return brCommand;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private BlockRecoveryCommand getBlockRecoveryCommand(String blockPoolId,\n      DatanodeDescriptor nodeinfo) {\n    BlockInfo[] blocks \u003d nodeinfo.getLeaseRecoveryCommand(Integer.MAX_VALUE);\n    if (blocks \u003d\u003d null) {\n      return null;\n    }\n    BlockRecoveryCommand brCommand \u003d new BlockRecoveryCommand(blocks.length);\n    for (BlockInfo b : blocks) {\n      BlockUnderConstructionFeature uc \u003d b.getUnderConstructionFeature();\n      assert uc !\u003d null;\n      final DatanodeStorageInfo[] storages \u003d uc.getExpectedStorageLocations();\n      // Skip stale nodes during recovery\n      final List\u003cDatanodeStorageInfo\u003e recoveryLocations \u003d\n          new ArrayList\u003c\u003e(storages.length);\n      for (DatanodeStorageInfo storage : storages) {\n        if (!storage.getDatanodeDescriptor().isStale(staleInterval)) {\n          recoveryLocations.add(storage);\n        }\n      }\n      // If we are performing a truncate recovery than set recovery fields\n      // to old block.\n      boolean truncateRecovery \u003d uc.getTruncateBlock() !\u003d null;\n      boolean copyOnTruncateRecovery \u003d truncateRecovery \u0026\u0026\n          uc.getTruncateBlock().getBlockId() !\u003d b.getBlockId();\n      ExtendedBlock primaryBlock \u003d (copyOnTruncateRecovery) ?\n          new ExtendedBlock(blockPoolId, uc.getTruncateBlock()) :\n          new ExtendedBlock(blockPoolId, b);\n      // If we only get 1 replica after eliminating stale nodes, choose all\n      // replicas for recovery and let the primary data node handle failures.\n      DatanodeInfo[] recoveryInfos;\n      if (recoveryLocations.size() \u003e 1) {\n        if (recoveryLocations.size() !\u003d storages.length) {\n          LOG.info(\"Skipped stale nodes for recovery : \"\n              + (storages.length - recoveryLocations.size()));\n        }\n        recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(recoveryLocations);\n      } else {\n        // If too many replicas are stale, then choose all replicas to\n        // participate in block recovery.\n        recoveryInfos \u003d DatanodeStorageInfo.toDatanodeInfos(storages);\n      }\n      RecoveringBlock rBlock;\n      if (truncateRecovery) {\n        Block recoveryBlock \u003d (copyOnTruncateRecovery) ? b : uc.getTruncateBlock();\n        rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos, recoveryBlock);\n      } else {\n        rBlock \u003d new RecoveringBlock(primaryBlock, recoveryInfos,\n            uc.getBlockRecoveryId());\n      }\n      brCommand.add(rBlock);\n    }\n    return brCommand;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java"
    }
  }
}