{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Merger.java",
  "functionName": "next",
  "functionId": "next",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Merger.java",
  "functionStartLine": 537,
  "functionEndLine": 576,
  "numCommitsSeen": 14,
  "timeTaken": 5533,
  "changeHistory": [
    "8c1adeaa26a7eaaca891dccdb71ff085e598a7de",
    "90194ca1cbd695d48c3705121c2ac9a8554578a2",
    "8f701ae07a0b1dc70b8e1eb8d4a5c35c0a1e76da",
    "5bf7ef839bbe4f7d5d673ae43e795ee4050fad88",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "8c1adeaa26a7eaaca891dccdb71ff085e598a7de": "Ybodychange",
    "90194ca1cbd695d48c3705121c2ac9a8554578a2": "Ybodychange",
    "8f701ae07a0b1dc70b8e1eb8d4a5c35c0a1e76da": "Ybodychange",
    "5bf7ef839bbe4f7d5d673ae43e795ee4050fad88": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "8c1adeaa26a7eaaca891dccdb71ff085e598a7de": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6068. Illegal progress value warnings in map tasks. Contributed by Binglin Chang.\n",
      "commitDate": "11/01/16 6:12 AM",
      "commitName": "8c1adeaa26a7eaaca891dccdb71ff085e598a7de",
      "commitAuthor": "Junping Du",
      "commitDateOld": "20/11/14 3:36 PM",
      "commitNameOld": "90194ca1cbd695d48c3705121c2ac9a8554578a2",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 416.61,
      "commitsBetweenForRepo": 3243,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n     public boolean next() throws IOException {\n       if (size() \u003d\u003d 0) {\n         resetKeyValue();\n         return false;\n       }\n \n       if (minSegment !\u003d null) {\n         //minSegment is non-null for all invocations of next except the first\n         //one. For the first invocation, the priority queue is ready for use\n         //but for the subsequent invocations, first adjust the queue \n         adjustPriorityQueue(minSegment);\n         if (size() \u003d\u003d 0) {\n           minSegment \u003d null;\n           resetKeyValue();\n           return false;\n         }\n       }\n       minSegment \u003d top();\n       long startPos \u003d minSegment.getReader().bytesRead;\n       key \u003d minSegment.getKey();\n       if (!minSegment.inMemory()) {\n         //When we load the value from an inmemory segment, we reset\n         //the \"value\" DIB in this class to the inmem segment\u0027s byte[].\n         //When we load the value bytes from disk, we shouldn\u0027t use\n         //the same byte[] since it would corrupt the data in the inmem\n         //segment. So we maintain an explicit DIB for value bytes\n         //obtained from disk, and if the current segment is a disk\n         //segment, we reset the \"value\" DIB to the byte[] in that (so \n         //we reuse the disk segment DIB whenever we consider\n         //a disk segment).\n         minSegment.getValue(diskIFileValue);\n         value.reset(diskIFileValue.getData(), diskIFileValue.getLength());\n       } else {\n         minSegment.getValue(value);\n       }\n       long endPos \u003d minSegment.getReader().bytesRead;\n       totalBytesProcessed +\u003d endPos - startPos;\n-      mergeProgress.set(totalBytesProcessed * progPerByte);\n+      mergeProgress.set(Math.min(1.0f, totalBytesProcessed * progPerByte));\n       return true;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public boolean next() throws IOException {\n      if (size() \u003d\u003d 0) {\n        resetKeyValue();\n        return false;\n      }\n\n      if (minSegment !\u003d null) {\n        //minSegment is non-null for all invocations of next except the first\n        //one. For the first invocation, the priority queue is ready for use\n        //but for the subsequent invocations, first adjust the queue \n        adjustPriorityQueue(minSegment);\n        if (size() \u003d\u003d 0) {\n          minSegment \u003d null;\n          resetKeyValue();\n          return false;\n        }\n      }\n      minSegment \u003d top();\n      long startPos \u003d minSegment.getReader().bytesRead;\n      key \u003d minSegment.getKey();\n      if (!minSegment.inMemory()) {\n        //When we load the value from an inmemory segment, we reset\n        //the \"value\" DIB in this class to the inmem segment\u0027s byte[].\n        //When we load the value bytes from disk, we shouldn\u0027t use\n        //the same byte[] since it would corrupt the data in the inmem\n        //segment. So we maintain an explicit DIB for value bytes\n        //obtained from disk, and if the current segment is a disk\n        //segment, we reset the \"value\" DIB to the byte[] in that (so \n        //we reuse the disk segment DIB whenever we consider\n        //a disk segment).\n        minSegment.getValue(diskIFileValue);\n        value.reset(diskIFileValue.getData(), diskIFileValue.getLength());\n      } else {\n        minSegment.getValue(value);\n      }\n      long endPos \u003d minSegment.getReader().bytesRead;\n      totalBytesProcessed +\u003d endPos - startPos;\n      mergeProgress.set(Math.min(1.0f, totalBytesProcessed * progPerByte));\n      return true;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Merger.java",
      "extendedDetails": {}
    },
    "90194ca1cbd695d48c3705121c2ac9a8554578a2": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6169. MergeQueue should release reference to the current item from key and value at the end of the iteration to save memory. (Zhihai Xu via kasha)\n",
      "commitDate": "20/11/14 3:36 PM",
      "commitName": "90194ca1cbd695d48c3705121c2ac9a8554578a2",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "06/11/14 1:53 PM",
      "commitNameOld": "8f701ae07a0b1dc70b8e1eb8d4a5c35c0a1e76da",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 14.07,
      "commitsBetweenForRepo": 120,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,40 @@\n     public boolean next() throws IOException {\n-      if (size() \u003d\u003d 0)\n+      if (size() \u003d\u003d 0) {\n+        resetKeyValue();\n         return false;\n+      }\n \n       if (minSegment !\u003d null) {\n         //minSegment is non-null for all invocations of next except the first\n         //one. For the first invocation, the priority queue is ready for use\n         //but for the subsequent invocations, first adjust the queue \n         adjustPriorityQueue(minSegment);\n         if (size() \u003d\u003d 0) {\n           minSegment \u003d null;\n+          resetKeyValue();\n           return false;\n         }\n       }\n       minSegment \u003d top();\n       long startPos \u003d minSegment.getReader().bytesRead;\n       key \u003d minSegment.getKey();\n       if (!minSegment.inMemory()) {\n         //When we load the value from an inmemory segment, we reset\n         //the \"value\" DIB in this class to the inmem segment\u0027s byte[].\n         //When we load the value bytes from disk, we shouldn\u0027t use\n         //the same byte[] since it would corrupt the data in the inmem\n         //segment. So we maintain an explicit DIB for value bytes\n         //obtained from disk, and if the current segment is a disk\n         //segment, we reset the \"value\" DIB to the byte[] in that (so \n         //we reuse the disk segment DIB whenever we consider\n         //a disk segment).\n         minSegment.getValue(diskIFileValue);\n         value.reset(diskIFileValue.getData(), diskIFileValue.getLength());\n       } else {\n         minSegment.getValue(value);\n       }\n       long endPos \u003d minSegment.getReader().bytesRead;\n       totalBytesProcessed +\u003d endPos - startPos;\n       mergeProgress.set(totalBytesProcessed * progPerByte);\n       return true;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public boolean next() throws IOException {\n      if (size() \u003d\u003d 0) {\n        resetKeyValue();\n        return false;\n      }\n\n      if (minSegment !\u003d null) {\n        //minSegment is non-null for all invocations of next except the first\n        //one. For the first invocation, the priority queue is ready for use\n        //but for the subsequent invocations, first adjust the queue \n        adjustPriorityQueue(minSegment);\n        if (size() \u003d\u003d 0) {\n          minSegment \u003d null;\n          resetKeyValue();\n          return false;\n        }\n      }\n      minSegment \u003d top();\n      long startPos \u003d minSegment.getReader().bytesRead;\n      key \u003d minSegment.getKey();\n      if (!minSegment.inMemory()) {\n        //When we load the value from an inmemory segment, we reset\n        //the \"value\" DIB in this class to the inmem segment\u0027s byte[].\n        //When we load the value bytes from disk, we shouldn\u0027t use\n        //the same byte[] since it would corrupt the data in the inmem\n        //segment. So we maintain an explicit DIB for value bytes\n        //obtained from disk, and if the current segment is a disk\n        //segment, we reset the \"value\" DIB to the byte[] in that (so \n        //we reuse the disk segment DIB whenever we consider\n        //a disk segment).\n        minSegment.getValue(diskIFileValue);\n        value.reset(diskIFileValue.getData(), diskIFileValue.getLength());\n      } else {\n        minSegment.getValue(value);\n      }\n      long endPos \u003d minSegment.getReader().bytesRead;\n      totalBytesProcessed +\u003d endPos - startPos;\n      mergeProgress.set(totalBytesProcessed * progPerByte);\n      return true;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Merger.java",
      "extendedDetails": {}
    },
    "8f701ae07a0b1dc70b8e1eb8d4a5c35c0a1e76da": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5958. Wrong reduce task progress if map output is compressed. Contributed by Emilio Coppa and Jason Lowe.\n",
      "commitDate": "06/11/14 1:53 PM",
      "commitName": "8f701ae07a0b1dc70b8e1eb8d4a5c35c0a1e76da",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "10/07/14 5:43 PM",
      "commitNameOld": "95986dd2fb4527c43fa4c088c61fb7b4bd794d23",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 118.88,
      "commitsBetweenForRepo": 1169,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,37 @@\n     public boolean next() throws IOException {\n       if (size() \u003d\u003d 0)\n         return false;\n \n       if (minSegment !\u003d null) {\n         //minSegment is non-null for all invocations of next except the first\n         //one. For the first invocation, the priority queue is ready for use\n         //but for the subsequent invocations, first adjust the queue \n         adjustPriorityQueue(minSegment);\n         if (size() \u003d\u003d 0) {\n           minSegment \u003d null;\n           return false;\n         }\n       }\n       minSegment \u003d top();\n-      long startPos \u003d minSegment.getPosition();\n+      long startPos \u003d minSegment.getReader().bytesRead;\n       key \u003d minSegment.getKey();\n       if (!minSegment.inMemory()) {\n         //When we load the value from an inmemory segment, we reset\n         //the \"value\" DIB in this class to the inmem segment\u0027s byte[].\n         //When we load the value bytes from disk, we shouldn\u0027t use\n         //the same byte[] since it would corrupt the data in the inmem\n         //segment. So we maintain an explicit DIB for value bytes\n         //obtained from disk, and if the current segment is a disk\n         //segment, we reset the \"value\" DIB to the byte[] in that (so \n         //we reuse the disk segment DIB whenever we consider\n         //a disk segment).\n         minSegment.getValue(diskIFileValue);\n         value.reset(diskIFileValue.getData(), diskIFileValue.getLength());\n       } else {\n         minSegment.getValue(value);\n       }\n-      long endPos \u003d minSegment.getPosition();\n+      long endPos \u003d minSegment.getReader().bytesRead;\n       totalBytesProcessed +\u003d endPos - startPos;\n       mergeProgress.set(totalBytesProcessed * progPerByte);\n       return true;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public boolean next() throws IOException {\n      if (size() \u003d\u003d 0)\n        return false;\n\n      if (minSegment !\u003d null) {\n        //minSegment is non-null for all invocations of next except the first\n        //one. For the first invocation, the priority queue is ready for use\n        //but for the subsequent invocations, first adjust the queue \n        adjustPriorityQueue(minSegment);\n        if (size() \u003d\u003d 0) {\n          minSegment \u003d null;\n          return false;\n        }\n      }\n      minSegment \u003d top();\n      long startPos \u003d minSegment.getReader().bytesRead;\n      key \u003d minSegment.getKey();\n      if (!minSegment.inMemory()) {\n        //When we load the value from an inmemory segment, we reset\n        //the \"value\" DIB in this class to the inmem segment\u0027s byte[].\n        //When we load the value bytes from disk, we shouldn\u0027t use\n        //the same byte[] since it would corrupt the data in the inmem\n        //segment. So we maintain an explicit DIB for value bytes\n        //obtained from disk, and if the current segment is a disk\n        //segment, we reset the \"value\" DIB to the byte[] in that (so \n        //we reuse the disk segment DIB whenever we consider\n        //a disk segment).\n        minSegment.getValue(diskIFileValue);\n        value.reset(diskIFileValue.getData(), diskIFileValue.getLength());\n      } else {\n        minSegment.getValue(value);\n      }\n      long endPos \u003d minSegment.getReader().bytesRead;\n      totalBytesProcessed +\u003d endPos - startPos;\n      mergeProgress.set(totalBytesProcessed * progPerByte);\n      return true;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Merger.java",
      "extendedDetails": {}
    },
    "5bf7ef839bbe4f7d5d673ae43e795ee4050fad88": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5821. Avoid unintentional reallocation of byte arrays in segments\nduring merge. Contributed by Todd Lipcon\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594654 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/05/14 10:51 AM",
      "commitName": "5bf7ef839bbe4f7d5d673ae43e795ee4050fad88",
      "commitAuthor": "Christopher Douglas",
      "commitDateOld": "05/08/13 11:36 PM",
      "commitNameOld": "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
      "commitAuthorOld": "Sanford Ryza",
      "daysBetweenCommits": 281.47,
      "commitsBetweenForRepo": 1905,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,37 @@\n     public boolean next() throws IOException {\n       if (size() \u003d\u003d 0)\n         return false;\n \n       if (minSegment !\u003d null) {\n         //minSegment is non-null for all invocations of next except the first\n         //one. For the first invocation, the priority queue is ready for use\n         //but for the subsequent invocations, first adjust the queue \n         adjustPriorityQueue(minSegment);\n         if (size() \u003d\u003d 0) {\n           minSegment \u003d null;\n           return false;\n         }\n       }\n       minSegment \u003d top();\n+      long startPos \u003d minSegment.getPosition();\n+      key \u003d minSegment.getKey();\n       if (!minSegment.inMemory()) {\n         //When we load the value from an inmemory segment, we reset\n         //the \"value\" DIB in this class to the inmem segment\u0027s byte[].\n         //When we load the value bytes from disk, we shouldn\u0027t use\n         //the same byte[] since it would corrupt the data in the inmem\n         //segment. So we maintain an explicit DIB for value bytes\n         //obtained from disk, and if the current segment is a disk\n         //segment, we reset the \"value\" DIB to the byte[] in that (so \n         //we reuse the disk segment DIB whenever we consider\n         //a disk segment).\n+        minSegment.getValue(diskIFileValue);\n         value.reset(diskIFileValue.getData(), diskIFileValue.getLength());\n+      } else {\n+        minSegment.getValue(value);\n       }\n-      long startPos \u003d minSegment.getPosition();\n-      key \u003d minSegment.getKey();\n-      minSegment.getValue(value);\n       long endPos \u003d minSegment.getPosition();\n       totalBytesProcessed +\u003d endPos - startPos;\n       mergeProgress.set(totalBytesProcessed * progPerByte);\n       return true;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public boolean next() throws IOException {\n      if (size() \u003d\u003d 0)\n        return false;\n\n      if (minSegment !\u003d null) {\n        //minSegment is non-null for all invocations of next except the first\n        //one. For the first invocation, the priority queue is ready for use\n        //but for the subsequent invocations, first adjust the queue \n        adjustPriorityQueue(minSegment);\n        if (size() \u003d\u003d 0) {\n          minSegment \u003d null;\n          return false;\n        }\n      }\n      minSegment \u003d top();\n      long startPos \u003d minSegment.getPosition();\n      key \u003d minSegment.getKey();\n      if (!minSegment.inMemory()) {\n        //When we load the value from an inmemory segment, we reset\n        //the \"value\" DIB in this class to the inmem segment\u0027s byte[].\n        //When we load the value bytes from disk, we shouldn\u0027t use\n        //the same byte[] since it would corrupt the data in the inmem\n        //segment. So we maintain an explicit DIB for value bytes\n        //obtained from disk, and if the current segment is a disk\n        //segment, we reset the \"value\" DIB to the byte[] in that (so \n        //we reuse the disk segment DIB whenever we consider\n        //a disk segment).\n        minSegment.getValue(diskIFileValue);\n        value.reset(diskIFileValue.getData(), diskIFileValue.getLength());\n      } else {\n        minSegment.getValue(value);\n      }\n      long endPos \u003d minSegment.getPosition();\n      totalBytesProcessed +\u003d endPos - startPos;\n      mergeProgress.set(totalBytesProcessed * progPerByte);\n      return true;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Merger.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public boolean next() throws IOException {\n      if (size() \u003d\u003d 0)\n        return false;\n\n      if (minSegment !\u003d null) {\n        //minSegment is non-null for all invocations of next except the first\n        //one. For the first invocation, the priority queue is ready for use\n        //but for the subsequent invocations, first adjust the queue \n        adjustPriorityQueue(minSegment);\n        if (size() \u003d\u003d 0) {\n          minSegment \u003d null;\n          return false;\n        }\n      }\n      minSegment \u003d top();\n      if (!minSegment.inMemory()) {\n        //When we load the value from an inmemory segment, we reset\n        //the \"value\" DIB in this class to the inmem segment\u0027s byte[].\n        //When we load the value bytes from disk, we shouldn\u0027t use\n        //the same byte[] since it would corrupt the data in the inmem\n        //segment. So we maintain an explicit DIB for value bytes\n        //obtained from disk, and if the current segment is a disk\n        //segment, we reset the \"value\" DIB to the byte[] in that (so \n        //we reuse the disk segment DIB whenever we consider\n        //a disk segment).\n        value.reset(diskIFileValue.getData(), diskIFileValue.getLength());\n      }\n      long startPos \u003d minSegment.getPosition();\n      key \u003d minSegment.getKey();\n      minSegment.getValue(value);\n      long endPos \u003d minSegment.getPosition();\n      totalBytesProcessed +\u003d endPos - startPos;\n      mergeProgress.set(totalBytesProcessed * progPerByte);\n      return true;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Merger.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Merger.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Merger.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public boolean next() throws IOException {\n      if (size() \u003d\u003d 0)\n        return false;\n\n      if (minSegment !\u003d null) {\n        //minSegment is non-null for all invocations of next except the first\n        //one. For the first invocation, the priority queue is ready for use\n        //but for the subsequent invocations, first adjust the queue \n        adjustPriorityQueue(minSegment);\n        if (size() \u003d\u003d 0) {\n          minSegment \u003d null;\n          return false;\n        }\n      }\n      minSegment \u003d top();\n      if (!minSegment.inMemory()) {\n        //When we load the value from an inmemory segment, we reset\n        //the \"value\" DIB in this class to the inmem segment\u0027s byte[].\n        //When we load the value bytes from disk, we shouldn\u0027t use\n        //the same byte[] since it would corrupt the data in the inmem\n        //segment. So we maintain an explicit DIB for value bytes\n        //obtained from disk, and if the current segment is a disk\n        //segment, we reset the \"value\" DIB to the byte[] in that (so \n        //we reuse the disk segment DIB whenever we consider\n        //a disk segment).\n        value.reset(diskIFileValue.getData(), diskIFileValue.getLength());\n      }\n      long startPos \u003d minSegment.getPosition();\n      key \u003d minSegment.getKey();\n      minSegment.getValue(value);\n      long endPos \u003d minSegment.getPosition();\n      totalBytesProcessed +\u003d endPos - startPos;\n      mergeProgress.set(totalBytesProcessed * progPerByte);\n      return true;\n    }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Merger.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/Merger.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Merger.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,35 @@\n+    public boolean next() throws IOException {\n+      if (size() \u003d\u003d 0)\n+        return false;\n+\n+      if (minSegment !\u003d null) {\n+        //minSegment is non-null for all invocations of next except the first\n+        //one. For the first invocation, the priority queue is ready for use\n+        //but for the subsequent invocations, first adjust the queue \n+        adjustPriorityQueue(minSegment);\n+        if (size() \u003d\u003d 0) {\n+          minSegment \u003d null;\n+          return false;\n+        }\n+      }\n+      minSegment \u003d top();\n+      if (!minSegment.inMemory()) {\n+        //When we load the value from an inmemory segment, we reset\n+        //the \"value\" DIB in this class to the inmem segment\u0027s byte[].\n+        //When we load the value bytes from disk, we shouldn\u0027t use\n+        //the same byte[] since it would corrupt the data in the inmem\n+        //segment. So we maintain an explicit DIB for value bytes\n+        //obtained from disk, and if the current segment is a disk\n+        //segment, we reset the \"value\" DIB to the byte[] in that (so \n+        //we reuse the disk segment DIB whenever we consider\n+        //a disk segment).\n+        value.reset(diskIFileValue.getData(), diskIFileValue.getLength());\n+      }\n+      long startPos \u003d minSegment.getPosition();\n+      key \u003d minSegment.getKey();\n+      minSegment.getValue(value);\n+      long endPos \u003d minSegment.getPosition();\n+      totalBytesProcessed +\u003d endPos - startPos;\n+      mergeProgress.set(totalBytesProcessed * progPerByte);\n+      return true;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    public boolean next() throws IOException {\n      if (size() \u003d\u003d 0)\n        return false;\n\n      if (minSegment !\u003d null) {\n        //minSegment is non-null for all invocations of next except the first\n        //one. For the first invocation, the priority queue is ready for use\n        //but for the subsequent invocations, first adjust the queue \n        adjustPriorityQueue(minSegment);\n        if (size() \u003d\u003d 0) {\n          minSegment \u003d null;\n          return false;\n        }\n      }\n      minSegment \u003d top();\n      if (!minSegment.inMemory()) {\n        //When we load the value from an inmemory segment, we reset\n        //the \"value\" DIB in this class to the inmem segment\u0027s byte[].\n        //When we load the value bytes from disk, we shouldn\u0027t use\n        //the same byte[] since it would corrupt the data in the inmem\n        //segment. So we maintain an explicit DIB for value bytes\n        //obtained from disk, and if the current segment is a disk\n        //segment, we reset the \"value\" DIB to the byte[] in that (so \n        //we reuse the disk segment DIB whenever we consider\n        //a disk segment).\n        value.reset(diskIFileValue.getData(), diskIFileValue.getLength());\n      }\n      long startPos \u003d minSegment.getPosition();\n      key \u003d minSegment.getKey();\n      minSegment.getValue(value);\n      long endPos \u003d minSegment.getPosition();\n      totalBytesProcessed +\u003d endPos - startPos;\n      mergeProgress.set(totalBytesProcessed * progPerByte);\n      return true;\n    }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/Merger.java"
    }
  }
}