{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DynamoDBMetadataStore.java",
  "functionName": "processBatchWriteRequest",
  "functionId": "processBatchWriteRequest___ancestorState-AncestorState(annotations-@Nullable)__keysToDelete-PrimaryKey[]__itemsToPut-Item[]",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
  "functionStartLine": 1163,
  "functionEndLine": 1236,
  "numCommitsSeen": 72,
  "timeTaken": 4433,
  "changeHistory": [
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0",
    "e02eb24e0a9139418120027b694492e0738df20a",
    "d7c0a08a1c077752918a8cf1b4f1900ce2721899",
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c",
    "621b43e254afaff708cd6fc4698b29628f6abc33"
  ],
  "changeHistoryShort": {
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0": "Ymultichange(Yparameterchange,Ybodychange)",
    "e02eb24e0a9139418120027b694492e0738df20a": "Ybodychange",
    "d7c0a08a1c077752918a8cf1b4f1900ce2721899": "Ymultichange(Yreturntypechange,Ybodychange)",
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c": "Ybodychange",
    "621b43e254afaff708cd6fc4698b29628f6abc33": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b15ef7dc3d91c6d50fa515158104fba29f43e6b0": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-16384: S3A: Avoid inconsistencies between DDB and S3.\n\nContributed by Steve Loughran\n\nContains\n\n- HADOOP-16397. Hadoop S3Guard Prune command to support a -tombstone option.\n- HADOOP-16406. ITestDynamoDBMetadataStore.testProvisionTable times out intermittently\n\nThis patch doesn\u0027t fix the underlying problem but it\n\n* changes some tests to clean up better\n* does a lot more in logging operations in against DDB, if enabled\n* adds an entry point to dump the state of the metastore and s3 tables (precursor to fsck)\n* adds a purge entry point to help clean up after a test run has got a store into a mess\n* s3guard prune command adds -tombstone option to only clear tombstones\n\nThe outcome is that tests should pass consistently and if problems occur we have better diagnostics.\n\nChange-Id: I3eca3f5529d7f6fec398c0ff0472919f08f054eb\n",
      "commitDate": "12/07/19 5:02 AM",
      "commitName": "b15ef7dc3d91c6d50fa515158104fba29f43e6b0",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-16384: S3A: Avoid inconsistencies between DDB and S3.\n\nContributed by Steve Loughran\n\nContains\n\n- HADOOP-16397. Hadoop S3Guard Prune command to support a -tombstone option.\n- HADOOP-16406. ITestDynamoDBMetadataStore.testProvisionTable times out intermittently\n\nThis patch doesn\u0027t fix the underlying problem but it\n\n* changes some tests to clean up better\n* does a lot more in logging operations in against DDB, if enabled\n* adds an entry point to dump the state of the metastore and s3 tables (precursor to fsck)\n* adds a purge entry point to help clean up after a test run has got a store into a mess\n* s3guard prune command adds -tombstone option to only clear tombstones\n\nThe outcome is that tests should pass consistently and if problems occur we have better diagnostics.\n\nChange-Id: I3eca3f5529d7f6fec398c0ff0472919f08f054eb\n",
          "commitDate": "12/07/19 5:02 AM",
          "commitName": "b15ef7dc3d91c6d50fa515158104fba29f43e6b0",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "08/07/19 10:27 AM",
          "commitNameOld": "de6b7bc67ace7744adb0320ee7de79cf28259d2d",
          "commitAuthorOld": "Sean Mackrory",
          "daysBetweenCommits": 3.77,
          "commitsBetweenForRepo": 38,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,67 +1,74 @@\n-  private int processBatchWriteRequest(PrimaryKey[] keysToDelete,\n+  private int processBatchWriteRequest(\n+      @Nullable AncestorState ancestorState,\n+      PrimaryKey[] keysToDelete,\n       Item[] itemsToPut) throws IOException {\n     final int totalToDelete \u003d (keysToDelete \u003d\u003d null ? 0 : keysToDelete.length);\n     final int totalToPut \u003d (itemsToPut \u003d\u003d null ? 0 : itemsToPut.length);\n     if (totalToPut \u003d\u003d 0 \u0026\u0026 totalToDelete \u003d\u003d 0) {\n       LOG.debug(\"Ignoring empty batch write request\");\n       return 0;\n     }\n     int count \u003d 0;\n     int batches \u003d 0;\n     while (count \u003c totalToDelete + totalToPut) {\n       final TableWriteItems writeItems \u003d new TableWriteItems(tableName);\n       int numToDelete \u003d 0;\n       if (keysToDelete !\u003d null\n           \u0026\u0026 count \u003c totalToDelete) {\n         numToDelete \u003d Math.min(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT,\n             totalToDelete - count);\n-        writeItems.withPrimaryKeysToDelete(\n-            Arrays.copyOfRange(keysToDelete, count, count + numToDelete));\n+        PrimaryKey[] toDelete \u003d Arrays.copyOfRange(keysToDelete,\n+            count, count + numToDelete);\n+        LOG.debug(\"Deleting {} entries: {}\", toDelete.length, toDelete);\n+        writeItems.withPrimaryKeysToDelete(toDelete);\n         count +\u003d numToDelete;\n       }\n \n       if (numToDelete \u003c S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT\n           \u0026\u0026 itemsToPut !\u003d null\n           \u0026\u0026 count \u003c totalToDelete + totalToPut) {\n         final int numToPut \u003d Math.min(\n             S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT - numToDelete,\n             totalToDelete + totalToPut - count);\n         final int index \u003d count - totalToDelete;\n         writeItems.withItemsToPut(\n             Arrays.copyOfRange(itemsToPut, index, index + numToPut));\n         count +\u003d numToPut;\n       }\n \n       // if there\u0027s a retry and another process updates things then it\u0027s not\n       // quite idempotent, but this was the case anyway\n       batches++;\n       BatchWriteItemOutcome res \u003d writeOp.retry(\n           \"batch write\",\n           \"\",\n           true,\n           () -\u003e dynamoDB.batchWriteItem(writeItems));\n       // Check for unprocessed keys in case of exceeding provisioned throughput\n       Map\u003cString, List\u003cWriteRequest\u003e\u003e unprocessed \u003d res.getUnprocessedItems();\n       int retryCount \u003d 0;\n       while (!unprocessed.isEmpty()) {\n         batchWriteCapacityExceededEvents.incrementAndGet();\n         batches++;\n         retryBackoffOnBatchWrite(retryCount++);\n         // use a different reference to keep the compiler quiet\n         final Map\u003cString, List\u003cWriteRequest\u003e\u003e upx \u003d unprocessed;\n         res \u003d writeOp.retry(\n             \"batch write\",\n             \"\",\n             true,\n             () -\u003e dynamoDB.batchWriteItemUnprocessed(upx));\n         unprocessed \u003d res.getUnprocessedItems();\n       }\n     }\n     if (itemsToPut !\u003d null) {\n       recordsWritten(itemsToPut.length);\n+      logPut(ancestorState, itemsToPut);\n     }\n     if (keysToDelete !\u003d null) {\n       recordsDeleted(keysToDelete.length);\n+      logDelete(ancestorState, keysToDelete);\n+\n     }\n     return batches;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int processBatchWriteRequest(\n      @Nullable AncestorState ancestorState,\n      PrimaryKey[] keysToDelete,\n      Item[] itemsToPut) throws IOException {\n    final int totalToDelete \u003d (keysToDelete \u003d\u003d null ? 0 : keysToDelete.length);\n    final int totalToPut \u003d (itemsToPut \u003d\u003d null ? 0 : itemsToPut.length);\n    if (totalToPut \u003d\u003d 0 \u0026\u0026 totalToDelete \u003d\u003d 0) {\n      LOG.debug(\"Ignoring empty batch write request\");\n      return 0;\n    }\n    int count \u003d 0;\n    int batches \u003d 0;\n    while (count \u003c totalToDelete + totalToPut) {\n      final TableWriteItems writeItems \u003d new TableWriteItems(tableName);\n      int numToDelete \u003d 0;\n      if (keysToDelete !\u003d null\n          \u0026\u0026 count \u003c totalToDelete) {\n        numToDelete \u003d Math.min(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT,\n            totalToDelete - count);\n        PrimaryKey[] toDelete \u003d Arrays.copyOfRange(keysToDelete,\n            count, count + numToDelete);\n        LOG.debug(\"Deleting {} entries: {}\", toDelete.length, toDelete);\n        writeItems.withPrimaryKeysToDelete(toDelete);\n        count +\u003d numToDelete;\n      }\n\n      if (numToDelete \u003c S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT\n          \u0026\u0026 itemsToPut !\u003d null\n          \u0026\u0026 count \u003c totalToDelete + totalToPut) {\n        final int numToPut \u003d Math.min(\n            S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT - numToDelete,\n            totalToDelete + totalToPut - count);\n        final int index \u003d count - totalToDelete;\n        writeItems.withItemsToPut(\n            Arrays.copyOfRange(itemsToPut, index, index + numToPut));\n        count +\u003d numToPut;\n      }\n\n      // if there\u0027s a retry and another process updates things then it\u0027s not\n      // quite idempotent, but this was the case anyway\n      batches++;\n      BatchWriteItemOutcome res \u003d writeOp.retry(\n          \"batch write\",\n          \"\",\n          true,\n          () -\u003e dynamoDB.batchWriteItem(writeItems));\n      // Check for unprocessed keys in case of exceeding provisioned throughput\n      Map\u003cString, List\u003cWriteRequest\u003e\u003e unprocessed \u003d res.getUnprocessedItems();\n      int retryCount \u003d 0;\n      while (!unprocessed.isEmpty()) {\n        batchWriteCapacityExceededEvents.incrementAndGet();\n        batches++;\n        retryBackoffOnBatchWrite(retryCount++);\n        // use a different reference to keep the compiler quiet\n        final Map\u003cString, List\u003cWriteRequest\u003e\u003e upx \u003d unprocessed;\n        res \u003d writeOp.retry(\n            \"batch write\",\n            \"\",\n            true,\n            () -\u003e dynamoDB.batchWriteItemUnprocessed(upx));\n        unprocessed \u003d res.getUnprocessedItems();\n      }\n    }\n    if (itemsToPut !\u003d null) {\n      recordsWritten(itemsToPut.length);\n      logPut(ancestorState, itemsToPut);\n    }\n    if (keysToDelete !\u003d null) {\n      recordsDeleted(keysToDelete.length);\n      logDelete(ancestorState, keysToDelete);\n\n    }\n    return batches;\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {
            "oldValue": "[keysToDelete-PrimaryKey[], itemsToPut-Item[]]",
            "newValue": "[ancestorState-AncestorState(annotations-@Nullable), keysToDelete-PrimaryKey[], itemsToPut-Item[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-16384: S3A: Avoid inconsistencies between DDB and S3.\n\nContributed by Steve Loughran\n\nContains\n\n- HADOOP-16397. Hadoop S3Guard Prune command to support a -tombstone option.\n- HADOOP-16406. ITestDynamoDBMetadataStore.testProvisionTable times out intermittently\n\nThis patch doesn\u0027t fix the underlying problem but it\n\n* changes some tests to clean up better\n* does a lot more in logging operations in against DDB, if enabled\n* adds an entry point to dump the state of the metastore and s3 tables (precursor to fsck)\n* adds a purge entry point to help clean up after a test run has got a store into a mess\n* s3guard prune command adds -tombstone option to only clear tombstones\n\nThe outcome is that tests should pass consistently and if problems occur we have better diagnostics.\n\nChange-Id: I3eca3f5529d7f6fec398c0ff0472919f08f054eb\n",
          "commitDate": "12/07/19 5:02 AM",
          "commitName": "b15ef7dc3d91c6d50fa515158104fba29f43e6b0",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "08/07/19 10:27 AM",
          "commitNameOld": "de6b7bc67ace7744adb0320ee7de79cf28259d2d",
          "commitAuthorOld": "Sean Mackrory",
          "daysBetweenCommits": 3.77,
          "commitsBetweenForRepo": 38,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,67 +1,74 @@\n-  private int processBatchWriteRequest(PrimaryKey[] keysToDelete,\n+  private int processBatchWriteRequest(\n+      @Nullable AncestorState ancestorState,\n+      PrimaryKey[] keysToDelete,\n       Item[] itemsToPut) throws IOException {\n     final int totalToDelete \u003d (keysToDelete \u003d\u003d null ? 0 : keysToDelete.length);\n     final int totalToPut \u003d (itemsToPut \u003d\u003d null ? 0 : itemsToPut.length);\n     if (totalToPut \u003d\u003d 0 \u0026\u0026 totalToDelete \u003d\u003d 0) {\n       LOG.debug(\"Ignoring empty batch write request\");\n       return 0;\n     }\n     int count \u003d 0;\n     int batches \u003d 0;\n     while (count \u003c totalToDelete + totalToPut) {\n       final TableWriteItems writeItems \u003d new TableWriteItems(tableName);\n       int numToDelete \u003d 0;\n       if (keysToDelete !\u003d null\n           \u0026\u0026 count \u003c totalToDelete) {\n         numToDelete \u003d Math.min(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT,\n             totalToDelete - count);\n-        writeItems.withPrimaryKeysToDelete(\n-            Arrays.copyOfRange(keysToDelete, count, count + numToDelete));\n+        PrimaryKey[] toDelete \u003d Arrays.copyOfRange(keysToDelete,\n+            count, count + numToDelete);\n+        LOG.debug(\"Deleting {} entries: {}\", toDelete.length, toDelete);\n+        writeItems.withPrimaryKeysToDelete(toDelete);\n         count +\u003d numToDelete;\n       }\n \n       if (numToDelete \u003c S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT\n           \u0026\u0026 itemsToPut !\u003d null\n           \u0026\u0026 count \u003c totalToDelete + totalToPut) {\n         final int numToPut \u003d Math.min(\n             S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT - numToDelete,\n             totalToDelete + totalToPut - count);\n         final int index \u003d count - totalToDelete;\n         writeItems.withItemsToPut(\n             Arrays.copyOfRange(itemsToPut, index, index + numToPut));\n         count +\u003d numToPut;\n       }\n \n       // if there\u0027s a retry and another process updates things then it\u0027s not\n       // quite idempotent, but this was the case anyway\n       batches++;\n       BatchWriteItemOutcome res \u003d writeOp.retry(\n           \"batch write\",\n           \"\",\n           true,\n           () -\u003e dynamoDB.batchWriteItem(writeItems));\n       // Check for unprocessed keys in case of exceeding provisioned throughput\n       Map\u003cString, List\u003cWriteRequest\u003e\u003e unprocessed \u003d res.getUnprocessedItems();\n       int retryCount \u003d 0;\n       while (!unprocessed.isEmpty()) {\n         batchWriteCapacityExceededEvents.incrementAndGet();\n         batches++;\n         retryBackoffOnBatchWrite(retryCount++);\n         // use a different reference to keep the compiler quiet\n         final Map\u003cString, List\u003cWriteRequest\u003e\u003e upx \u003d unprocessed;\n         res \u003d writeOp.retry(\n             \"batch write\",\n             \"\",\n             true,\n             () -\u003e dynamoDB.batchWriteItemUnprocessed(upx));\n         unprocessed \u003d res.getUnprocessedItems();\n       }\n     }\n     if (itemsToPut !\u003d null) {\n       recordsWritten(itemsToPut.length);\n+      logPut(ancestorState, itemsToPut);\n     }\n     if (keysToDelete !\u003d null) {\n       recordsDeleted(keysToDelete.length);\n+      logDelete(ancestorState, keysToDelete);\n+\n     }\n     return batches;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int processBatchWriteRequest(\n      @Nullable AncestorState ancestorState,\n      PrimaryKey[] keysToDelete,\n      Item[] itemsToPut) throws IOException {\n    final int totalToDelete \u003d (keysToDelete \u003d\u003d null ? 0 : keysToDelete.length);\n    final int totalToPut \u003d (itemsToPut \u003d\u003d null ? 0 : itemsToPut.length);\n    if (totalToPut \u003d\u003d 0 \u0026\u0026 totalToDelete \u003d\u003d 0) {\n      LOG.debug(\"Ignoring empty batch write request\");\n      return 0;\n    }\n    int count \u003d 0;\n    int batches \u003d 0;\n    while (count \u003c totalToDelete + totalToPut) {\n      final TableWriteItems writeItems \u003d new TableWriteItems(tableName);\n      int numToDelete \u003d 0;\n      if (keysToDelete !\u003d null\n          \u0026\u0026 count \u003c totalToDelete) {\n        numToDelete \u003d Math.min(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT,\n            totalToDelete - count);\n        PrimaryKey[] toDelete \u003d Arrays.copyOfRange(keysToDelete,\n            count, count + numToDelete);\n        LOG.debug(\"Deleting {} entries: {}\", toDelete.length, toDelete);\n        writeItems.withPrimaryKeysToDelete(toDelete);\n        count +\u003d numToDelete;\n      }\n\n      if (numToDelete \u003c S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT\n          \u0026\u0026 itemsToPut !\u003d null\n          \u0026\u0026 count \u003c totalToDelete + totalToPut) {\n        final int numToPut \u003d Math.min(\n            S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT - numToDelete,\n            totalToDelete + totalToPut - count);\n        final int index \u003d count - totalToDelete;\n        writeItems.withItemsToPut(\n            Arrays.copyOfRange(itemsToPut, index, index + numToPut));\n        count +\u003d numToPut;\n      }\n\n      // if there\u0027s a retry and another process updates things then it\u0027s not\n      // quite idempotent, but this was the case anyway\n      batches++;\n      BatchWriteItemOutcome res \u003d writeOp.retry(\n          \"batch write\",\n          \"\",\n          true,\n          () -\u003e dynamoDB.batchWriteItem(writeItems));\n      // Check for unprocessed keys in case of exceeding provisioned throughput\n      Map\u003cString, List\u003cWriteRequest\u003e\u003e unprocessed \u003d res.getUnprocessedItems();\n      int retryCount \u003d 0;\n      while (!unprocessed.isEmpty()) {\n        batchWriteCapacityExceededEvents.incrementAndGet();\n        batches++;\n        retryBackoffOnBatchWrite(retryCount++);\n        // use a different reference to keep the compiler quiet\n        final Map\u003cString, List\u003cWriteRequest\u003e\u003e upx \u003d unprocessed;\n        res \u003d writeOp.retry(\n            \"batch write\",\n            \"\",\n            true,\n            () -\u003e dynamoDB.batchWriteItemUnprocessed(upx));\n        unprocessed \u003d res.getUnprocessedItems();\n      }\n    }\n    if (itemsToPut !\u003d null) {\n      recordsWritten(itemsToPut.length);\n      logPut(ancestorState, itemsToPut);\n    }\n    if (keysToDelete !\u003d null) {\n      recordsDeleted(keysToDelete.length);\n      logDelete(ancestorState, keysToDelete);\n\n    }\n    return batches;\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {}
        }
      ]
    },
    "e02eb24e0a9139418120027b694492e0738df20a": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15183. S3Guard store becomes inconsistent after partial failure of rename.\n\nContributed by Steve Loughran.\n\nChange-Id: I825b0bc36be960475d2d259b1cdab45ae1bb78eb\n",
      "commitDate": "20/06/19 1:56 AM",
      "commitName": "e02eb24e0a9139418120027b694492e0738df20a",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "16/06/19 9:05 AM",
      "commitNameOld": "f9cc9e162175444efe9d5b07ecb9a795f750ca3c",
      "commitAuthorOld": "Gabor Bota",
      "daysBetweenCommits": 3.7,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,67 @@\n   private int processBatchWriteRequest(PrimaryKey[] keysToDelete,\n       Item[] itemsToPut) throws IOException {\n     final int totalToDelete \u003d (keysToDelete \u003d\u003d null ? 0 : keysToDelete.length);\n     final int totalToPut \u003d (itemsToPut \u003d\u003d null ? 0 : itemsToPut.length);\n+    if (totalToPut \u003d\u003d 0 \u0026\u0026 totalToDelete \u003d\u003d 0) {\n+      LOG.debug(\"Ignoring empty batch write request\");\n+      return 0;\n+    }\n     int count \u003d 0;\n     int batches \u003d 0;\n     while (count \u003c totalToDelete + totalToPut) {\n       final TableWriteItems writeItems \u003d new TableWriteItems(tableName);\n       int numToDelete \u003d 0;\n       if (keysToDelete !\u003d null\n           \u0026\u0026 count \u003c totalToDelete) {\n         numToDelete \u003d Math.min(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT,\n             totalToDelete - count);\n         writeItems.withPrimaryKeysToDelete(\n             Arrays.copyOfRange(keysToDelete, count, count + numToDelete));\n         count +\u003d numToDelete;\n       }\n \n       if (numToDelete \u003c S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT\n           \u0026\u0026 itemsToPut !\u003d null\n           \u0026\u0026 count \u003c totalToDelete + totalToPut) {\n         final int numToPut \u003d Math.min(\n             S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT - numToDelete,\n             totalToDelete + totalToPut - count);\n         final int index \u003d count - totalToDelete;\n         writeItems.withItemsToPut(\n             Arrays.copyOfRange(itemsToPut, index, index + numToPut));\n         count +\u003d numToPut;\n       }\n \n       // if there\u0027s a retry and another process updates things then it\u0027s not\n       // quite idempotent, but this was the case anyway\n       batches++;\n       BatchWriteItemOutcome res \u003d writeOp.retry(\n           \"batch write\",\n           \"\",\n           true,\n           () -\u003e dynamoDB.batchWriteItem(writeItems));\n       // Check for unprocessed keys in case of exceeding provisioned throughput\n       Map\u003cString, List\u003cWriteRequest\u003e\u003e unprocessed \u003d res.getUnprocessedItems();\n       int retryCount \u003d 0;\n       while (!unprocessed.isEmpty()) {\n         batchWriteCapacityExceededEvents.incrementAndGet();\n         batches++;\n         retryBackoffOnBatchWrite(retryCount++);\n         // use a different reference to keep the compiler quiet\n         final Map\u003cString, List\u003cWriteRequest\u003e\u003e upx \u003d unprocessed;\n         res \u003d writeOp.retry(\n             \"batch write\",\n             \"\",\n             true,\n             () -\u003e dynamoDB.batchWriteItemUnprocessed(upx));\n         unprocessed \u003d res.getUnprocessedItems();\n       }\n     }\n+    if (itemsToPut !\u003d null) {\n+      recordsWritten(itemsToPut.length);\n+    }\n+    if (keysToDelete !\u003d null) {\n+      recordsDeleted(keysToDelete.length);\n+    }\n     return batches;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private int processBatchWriteRequest(PrimaryKey[] keysToDelete,\n      Item[] itemsToPut) throws IOException {\n    final int totalToDelete \u003d (keysToDelete \u003d\u003d null ? 0 : keysToDelete.length);\n    final int totalToPut \u003d (itemsToPut \u003d\u003d null ? 0 : itemsToPut.length);\n    if (totalToPut \u003d\u003d 0 \u0026\u0026 totalToDelete \u003d\u003d 0) {\n      LOG.debug(\"Ignoring empty batch write request\");\n      return 0;\n    }\n    int count \u003d 0;\n    int batches \u003d 0;\n    while (count \u003c totalToDelete + totalToPut) {\n      final TableWriteItems writeItems \u003d new TableWriteItems(tableName);\n      int numToDelete \u003d 0;\n      if (keysToDelete !\u003d null\n          \u0026\u0026 count \u003c totalToDelete) {\n        numToDelete \u003d Math.min(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT,\n            totalToDelete - count);\n        writeItems.withPrimaryKeysToDelete(\n            Arrays.copyOfRange(keysToDelete, count, count + numToDelete));\n        count +\u003d numToDelete;\n      }\n\n      if (numToDelete \u003c S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT\n          \u0026\u0026 itemsToPut !\u003d null\n          \u0026\u0026 count \u003c totalToDelete + totalToPut) {\n        final int numToPut \u003d Math.min(\n            S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT - numToDelete,\n            totalToDelete + totalToPut - count);\n        final int index \u003d count - totalToDelete;\n        writeItems.withItemsToPut(\n            Arrays.copyOfRange(itemsToPut, index, index + numToPut));\n        count +\u003d numToPut;\n      }\n\n      // if there\u0027s a retry and another process updates things then it\u0027s not\n      // quite idempotent, but this was the case anyway\n      batches++;\n      BatchWriteItemOutcome res \u003d writeOp.retry(\n          \"batch write\",\n          \"\",\n          true,\n          () -\u003e dynamoDB.batchWriteItem(writeItems));\n      // Check for unprocessed keys in case of exceeding provisioned throughput\n      Map\u003cString, List\u003cWriteRequest\u003e\u003e unprocessed \u003d res.getUnprocessedItems();\n      int retryCount \u003d 0;\n      while (!unprocessed.isEmpty()) {\n        batchWriteCapacityExceededEvents.incrementAndGet();\n        batches++;\n        retryBackoffOnBatchWrite(retryCount++);\n        // use a different reference to keep the compiler quiet\n        final Map\u003cString, List\u003cWriteRequest\u003e\u003e upx \u003d unprocessed;\n        res \u003d writeOp.retry(\n            \"batch write\",\n            \"\",\n            true,\n            () -\u003e dynamoDB.batchWriteItemUnprocessed(upx));\n        unprocessed \u003d res.getUnprocessedItems();\n      }\n    }\n    if (itemsToPut !\u003d null) {\n      recordsWritten(itemsToPut.length);\n    }\n    if (keysToDelete !\u003d null) {\n      recordsDeleted(keysToDelete.length);\n    }\n    return batches;\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
      "extendedDetails": {}
    },
    "d7c0a08a1c077752918a8cf1b4f1900ce2721899": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HADOOP-15426 Make S3guard client resilient to DDB throttle events and network failures (Contributed by Steve Loughran)\n",
      "commitDate": "12/09/18 9:04 PM",
      "commitName": "d7c0a08a1c077752918a8cf1b4f1900ce2721899",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HADOOP-15426 Make S3guard client resilient to DDB throttle events and network failures (Contributed by Steve Loughran)\n",
          "commitDate": "12/09/18 9:04 PM",
          "commitName": "d7c0a08a1c077752918a8cf1b4f1900ce2721899",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "12/09/18 4:36 PM",
          "commitNameOld": "d32a8d5d582725eb724b78f27310ad1efd33ed2a",
          "commitAuthorOld": "Aaron Fabbri",
          "daysBetweenCommits": 0.19,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,40 +1,57 @@\n-  private void processBatchWriteRequest(PrimaryKey[] keysToDelete,\n+  private int processBatchWriteRequest(PrimaryKey[] keysToDelete,\n       Item[] itemsToPut) throws IOException {\n     final int totalToDelete \u003d (keysToDelete \u003d\u003d null ? 0 : keysToDelete.length);\n     final int totalToPut \u003d (itemsToPut \u003d\u003d null ? 0 : itemsToPut.length);\n     int count \u003d 0;\n+    int batches \u003d 0;\n     while (count \u003c totalToDelete + totalToPut) {\n       final TableWriteItems writeItems \u003d new TableWriteItems(tableName);\n       int numToDelete \u003d 0;\n       if (keysToDelete !\u003d null\n           \u0026\u0026 count \u003c totalToDelete) {\n         numToDelete \u003d Math.min(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT,\n             totalToDelete - count);\n         writeItems.withPrimaryKeysToDelete(\n             Arrays.copyOfRange(keysToDelete, count, count + numToDelete));\n         count +\u003d numToDelete;\n       }\n \n       if (numToDelete \u003c S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT\n           \u0026\u0026 itemsToPut !\u003d null\n           \u0026\u0026 count \u003c totalToDelete + totalToPut) {\n         final int numToPut \u003d Math.min(\n             S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT - numToDelete,\n             totalToDelete + totalToPut - count);\n         final int index \u003d count - totalToDelete;\n         writeItems.withItemsToPut(\n             Arrays.copyOfRange(itemsToPut, index, index + numToPut));\n         count +\u003d numToPut;\n       }\n \n-      BatchWriteItemOutcome res \u003d dynamoDB.batchWriteItem(writeItems);\n+      // if there\u0027s a retry and another process updates things then it\u0027s not\n+      // quite idempotent, but this was the case anyway\n+      batches++;\n+      BatchWriteItemOutcome res \u003d writeOp.retry(\n+          \"batch write\",\n+          \"\",\n+          true,\n+          () -\u003e dynamoDB.batchWriteItem(writeItems));\n       // Check for unprocessed keys in case of exceeding provisioned throughput\n       Map\u003cString, List\u003cWriteRequest\u003e\u003e unprocessed \u003d res.getUnprocessedItems();\n       int retryCount \u003d 0;\n       while (!unprocessed.isEmpty()) {\n-        retryBackoff(retryCount++);\n-        res \u003d dynamoDB.batchWriteItemUnprocessed(unprocessed);\n+        batchWriteCapacityExceededEvents.incrementAndGet();\n+        batches++;\n+        retryBackoffOnBatchWrite(retryCount++);\n+        // use a different reference to keep the compiler quiet\n+        final Map\u003cString, List\u003cWriteRequest\u003e\u003e upx \u003d unprocessed;\n+        res \u003d writeOp.retry(\n+            \"batch write\",\n+            \"\",\n+            true,\n+            () -\u003e dynamoDB.batchWriteItemUnprocessed(upx));\n         unprocessed \u003d res.getUnprocessedItems();\n       }\n     }\n+    return batches;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int processBatchWriteRequest(PrimaryKey[] keysToDelete,\n      Item[] itemsToPut) throws IOException {\n    final int totalToDelete \u003d (keysToDelete \u003d\u003d null ? 0 : keysToDelete.length);\n    final int totalToPut \u003d (itemsToPut \u003d\u003d null ? 0 : itemsToPut.length);\n    int count \u003d 0;\n    int batches \u003d 0;\n    while (count \u003c totalToDelete + totalToPut) {\n      final TableWriteItems writeItems \u003d new TableWriteItems(tableName);\n      int numToDelete \u003d 0;\n      if (keysToDelete !\u003d null\n          \u0026\u0026 count \u003c totalToDelete) {\n        numToDelete \u003d Math.min(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT,\n            totalToDelete - count);\n        writeItems.withPrimaryKeysToDelete(\n            Arrays.copyOfRange(keysToDelete, count, count + numToDelete));\n        count +\u003d numToDelete;\n      }\n\n      if (numToDelete \u003c S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT\n          \u0026\u0026 itemsToPut !\u003d null\n          \u0026\u0026 count \u003c totalToDelete + totalToPut) {\n        final int numToPut \u003d Math.min(\n            S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT - numToDelete,\n            totalToDelete + totalToPut - count);\n        final int index \u003d count - totalToDelete;\n        writeItems.withItemsToPut(\n            Arrays.copyOfRange(itemsToPut, index, index + numToPut));\n        count +\u003d numToPut;\n      }\n\n      // if there\u0027s a retry and another process updates things then it\u0027s not\n      // quite idempotent, but this was the case anyway\n      batches++;\n      BatchWriteItemOutcome res \u003d writeOp.retry(\n          \"batch write\",\n          \"\",\n          true,\n          () -\u003e dynamoDB.batchWriteItem(writeItems));\n      // Check for unprocessed keys in case of exceeding provisioned throughput\n      Map\u003cString, List\u003cWriteRequest\u003e\u003e unprocessed \u003d res.getUnprocessedItems();\n      int retryCount \u003d 0;\n      while (!unprocessed.isEmpty()) {\n        batchWriteCapacityExceededEvents.incrementAndGet();\n        batches++;\n        retryBackoffOnBatchWrite(retryCount++);\n        // use a different reference to keep the compiler quiet\n        final Map\u003cString, List\u003cWriteRequest\u003e\u003e upx \u003d unprocessed;\n        res \u003d writeOp.retry(\n            \"batch write\",\n            \"\",\n            true,\n            () -\u003e dynamoDB.batchWriteItemUnprocessed(upx));\n        unprocessed \u003d res.getUnprocessedItems();\n      }\n    }\n    return batches;\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "int"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-15426 Make S3guard client resilient to DDB throttle events and network failures (Contributed by Steve Loughran)\n",
          "commitDate": "12/09/18 9:04 PM",
          "commitName": "d7c0a08a1c077752918a8cf1b4f1900ce2721899",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "12/09/18 4:36 PM",
          "commitNameOld": "d32a8d5d582725eb724b78f27310ad1efd33ed2a",
          "commitAuthorOld": "Aaron Fabbri",
          "daysBetweenCommits": 0.19,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,40 +1,57 @@\n-  private void processBatchWriteRequest(PrimaryKey[] keysToDelete,\n+  private int processBatchWriteRequest(PrimaryKey[] keysToDelete,\n       Item[] itemsToPut) throws IOException {\n     final int totalToDelete \u003d (keysToDelete \u003d\u003d null ? 0 : keysToDelete.length);\n     final int totalToPut \u003d (itemsToPut \u003d\u003d null ? 0 : itemsToPut.length);\n     int count \u003d 0;\n+    int batches \u003d 0;\n     while (count \u003c totalToDelete + totalToPut) {\n       final TableWriteItems writeItems \u003d new TableWriteItems(tableName);\n       int numToDelete \u003d 0;\n       if (keysToDelete !\u003d null\n           \u0026\u0026 count \u003c totalToDelete) {\n         numToDelete \u003d Math.min(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT,\n             totalToDelete - count);\n         writeItems.withPrimaryKeysToDelete(\n             Arrays.copyOfRange(keysToDelete, count, count + numToDelete));\n         count +\u003d numToDelete;\n       }\n \n       if (numToDelete \u003c S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT\n           \u0026\u0026 itemsToPut !\u003d null\n           \u0026\u0026 count \u003c totalToDelete + totalToPut) {\n         final int numToPut \u003d Math.min(\n             S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT - numToDelete,\n             totalToDelete + totalToPut - count);\n         final int index \u003d count - totalToDelete;\n         writeItems.withItemsToPut(\n             Arrays.copyOfRange(itemsToPut, index, index + numToPut));\n         count +\u003d numToPut;\n       }\n \n-      BatchWriteItemOutcome res \u003d dynamoDB.batchWriteItem(writeItems);\n+      // if there\u0027s a retry and another process updates things then it\u0027s not\n+      // quite idempotent, but this was the case anyway\n+      batches++;\n+      BatchWriteItemOutcome res \u003d writeOp.retry(\n+          \"batch write\",\n+          \"\",\n+          true,\n+          () -\u003e dynamoDB.batchWriteItem(writeItems));\n       // Check for unprocessed keys in case of exceeding provisioned throughput\n       Map\u003cString, List\u003cWriteRequest\u003e\u003e unprocessed \u003d res.getUnprocessedItems();\n       int retryCount \u003d 0;\n       while (!unprocessed.isEmpty()) {\n-        retryBackoff(retryCount++);\n-        res \u003d dynamoDB.batchWriteItemUnprocessed(unprocessed);\n+        batchWriteCapacityExceededEvents.incrementAndGet();\n+        batches++;\n+        retryBackoffOnBatchWrite(retryCount++);\n+        // use a different reference to keep the compiler quiet\n+        final Map\u003cString, List\u003cWriteRequest\u003e\u003e upx \u003d unprocessed;\n+        res \u003d writeOp.retry(\n+            \"batch write\",\n+            \"\",\n+            true,\n+            () -\u003e dynamoDB.batchWriteItemUnprocessed(upx));\n         unprocessed \u003d res.getUnprocessedItems();\n       }\n     }\n+    return batches;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private int processBatchWriteRequest(PrimaryKey[] keysToDelete,\n      Item[] itemsToPut) throws IOException {\n    final int totalToDelete \u003d (keysToDelete \u003d\u003d null ? 0 : keysToDelete.length);\n    final int totalToPut \u003d (itemsToPut \u003d\u003d null ? 0 : itemsToPut.length);\n    int count \u003d 0;\n    int batches \u003d 0;\n    while (count \u003c totalToDelete + totalToPut) {\n      final TableWriteItems writeItems \u003d new TableWriteItems(tableName);\n      int numToDelete \u003d 0;\n      if (keysToDelete !\u003d null\n          \u0026\u0026 count \u003c totalToDelete) {\n        numToDelete \u003d Math.min(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT,\n            totalToDelete - count);\n        writeItems.withPrimaryKeysToDelete(\n            Arrays.copyOfRange(keysToDelete, count, count + numToDelete));\n        count +\u003d numToDelete;\n      }\n\n      if (numToDelete \u003c S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT\n          \u0026\u0026 itemsToPut !\u003d null\n          \u0026\u0026 count \u003c totalToDelete + totalToPut) {\n        final int numToPut \u003d Math.min(\n            S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT - numToDelete,\n            totalToDelete + totalToPut - count);\n        final int index \u003d count - totalToDelete;\n        writeItems.withItemsToPut(\n            Arrays.copyOfRange(itemsToPut, index, index + numToPut));\n        count +\u003d numToPut;\n      }\n\n      // if there\u0027s a retry and another process updates things then it\u0027s not\n      // quite idempotent, but this was the case anyway\n      batches++;\n      BatchWriteItemOutcome res \u003d writeOp.retry(\n          \"batch write\",\n          \"\",\n          true,\n          () -\u003e dynamoDB.batchWriteItem(writeItems));\n      // Check for unprocessed keys in case of exceeding provisioned throughput\n      Map\u003cString, List\u003cWriteRequest\u003e\u003e unprocessed \u003d res.getUnprocessedItems();\n      int retryCount \u003d 0;\n      while (!unprocessed.isEmpty()) {\n        batchWriteCapacityExceededEvents.incrementAndGet();\n        batches++;\n        retryBackoffOnBatchWrite(retryCount++);\n        // use a different reference to keep the compiler quiet\n        final Map\u003cString, List\u003cWriteRequest\u003e\u003e upx \u003d unprocessed;\n        res \u003d writeOp.retry(\n            \"batch write\",\n            \"\",\n            true,\n            () -\u003e dynamoDB.batchWriteItemUnprocessed(upx));\n        unprocessed \u003d res.getUnprocessedItems();\n      }\n    }\n    return batches;\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
          "extendedDetails": {}
        }
      ]
    },
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13786 Add S3A committer for zero-rename commits to S3 endpoints.\nContributed by Steve Loughran and Ryan Blue.\n",
      "commitDate": "22/11/17 7:28 AM",
      "commitName": "de8b6ca5ef8614de6d6277b7617e27c788b0555c",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "25/09/17 3:59 PM",
      "commitNameOld": "47011d7dd300b0c74bb6cfe25b918c479d718f4f",
      "commitAuthorOld": "Aaron Fabbri",
      "daysBetweenCommits": 57.69,
      "commitsBetweenForRepo": 477,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   private void processBatchWriteRequest(PrimaryKey[] keysToDelete,\n       Item[] itemsToPut) throws IOException {\n     final int totalToDelete \u003d (keysToDelete \u003d\u003d null ? 0 : keysToDelete.length);\n     final int totalToPut \u003d (itemsToPut \u003d\u003d null ? 0 : itemsToPut.length);\n     int count \u003d 0;\n     while (count \u003c totalToDelete + totalToPut) {\n       final TableWriteItems writeItems \u003d new TableWriteItems(tableName);\n       int numToDelete \u003d 0;\n       if (keysToDelete !\u003d null\n           \u0026\u0026 count \u003c totalToDelete) {\n         numToDelete \u003d Math.min(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT,\n             totalToDelete - count);\n         writeItems.withPrimaryKeysToDelete(\n             Arrays.copyOfRange(keysToDelete, count, count + numToDelete));\n         count +\u003d numToDelete;\n       }\n \n       if (numToDelete \u003c S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT\n           \u0026\u0026 itemsToPut !\u003d null\n           \u0026\u0026 count \u003c totalToDelete + totalToPut) {\n         final int numToPut \u003d Math.min(\n             S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT - numToDelete,\n             totalToDelete + totalToPut - count);\n         final int index \u003d count - totalToDelete;\n         writeItems.withItemsToPut(\n             Arrays.copyOfRange(itemsToPut, index, index + numToPut));\n         count +\u003d numToPut;\n       }\n \n       BatchWriteItemOutcome res \u003d dynamoDB.batchWriteItem(writeItems);\n       // Check for unprocessed keys in case of exceeding provisioned throughput\n       Map\u003cString, List\u003cWriteRequest\u003e\u003e unprocessed \u003d res.getUnprocessedItems();\n       int retryCount \u003d 0;\n-      while (unprocessed.size() \u003e 0) {\n+      while (!unprocessed.isEmpty()) {\n         retryBackoff(retryCount++);\n         res \u003d dynamoDB.batchWriteItemUnprocessed(unprocessed);\n         unprocessed \u003d res.getUnprocessedItems();\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processBatchWriteRequest(PrimaryKey[] keysToDelete,\n      Item[] itemsToPut) throws IOException {\n    final int totalToDelete \u003d (keysToDelete \u003d\u003d null ? 0 : keysToDelete.length);\n    final int totalToPut \u003d (itemsToPut \u003d\u003d null ? 0 : itemsToPut.length);\n    int count \u003d 0;\n    while (count \u003c totalToDelete + totalToPut) {\n      final TableWriteItems writeItems \u003d new TableWriteItems(tableName);\n      int numToDelete \u003d 0;\n      if (keysToDelete !\u003d null\n          \u0026\u0026 count \u003c totalToDelete) {\n        numToDelete \u003d Math.min(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT,\n            totalToDelete - count);\n        writeItems.withPrimaryKeysToDelete(\n            Arrays.copyOfRange(keysToDelete, count, count + numToDelete));\n        count +\u003d numToDelete;\n      }\n\n      if (numToDelete \u003c S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT\n          \u0026\u0026 itemsToPut !\u003d null\n          \u0026\u0026 count \u003c totalToDelete + totalToPut) {\n        final int numToPut \u003d Math.min(\n            S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT - numToDelete,\n            totalToDelete + totalToPut - count);\n        final int index \u003d count - totalToDelete;\n        writeItems.withItemsToPut(\n            Arrays.copyOfRange(itemsToPut, index, index + numToPut));\n        count +\u003d numToPut;\n      }\n\n      BatchWriteItemOutcome res \u003d dynamoDB.batchWriteItem(writeItems);\n      // Check for unprocessed keys in case of exceeding provisioned throughput\n      Map\u003cString, List\u003cWriteRequest\u003e\u003e unprocessed \u003d res.getUnprocessedItems();\n      int retryCount \u003d 0;\n      while (!unprocessed.isEmpty()) {\n        retryBackoff(retryCount++);\n        res \u003d dynamoDB.batchWriteItemUnprocessed(unprocessed);\n        unprocessed \u003d res.getUnprocessedItems();\n      }\n    }\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
      "extendedDetails": {}
    },
    "621b43e254afaff708cd6fc4698b29628f6abc33": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-13345 HS3Guard: Improved Consistency for S3A.\nContributed by: Chris Nauroth, Aaron Fabbri, Mingliang Liu, Lei (Eddy) Xu,\nSean Mackrory, Steve Loughran and others.\n",
      "commitDate": "01/09/17 6:13 AM",
      "commitName": "621b43e254afaff708cd6fc4698b29628f6abc33",
      "commitAuthor": "Steve Loughran",
      "diff": "@@ -0,0 +1,40 @@\n+  private void processBatchWriteRequest(PrimaryKey[] keysToDelete,\n+      Item[] itemsToPut) throws IOException {\n+    final int totalToDelete \u003d (keysToDelete \u003d\u003d null ? 0 : keysToDelete.length);\n+    final int totalToPut \u003d (itemsToPut \u003d\u003d null ? 0 : itemsToPut.length);\n+    int count \u003d 0;\n+    while (count \u003c totalToDelete + totalToPut) {\n+      final TableWriteItems writeItems \u003d new TableWriteItems(tableName);\n+      int numToDelete \u003d 0;\n+      if (keysToDelete !\u003d null\n+          \u0026\u0026 count \u003c totalToDelete) {\n+        numToDelete \u003d Math.min(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT,\n+            totalToDelete - count);\n+        writeItems.withPrimaryKeysToDelete(\n+            Arrays.copyOfRange(keysToDelete, count, count + numToDelete));\n+        count +\u003d numToDelete;\n+      }\n+\n+      if (numToDelete \u003c S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT\n+          \u0026\u0026 itemsToPut !\u003d null\n+          \u0026\u0026 count \u003c totalToDelete + totalToPut) {\n+        final int numToPut \u003d Math.min(\n+            S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT - numToDelete,\n+            totalToDelete + totalToPut - count);\n+        final int index \u003d count - totalToDelete;\n+        writeItems.withItemsToPut(\n+            Arrays.copyOfRange(itemsToPut, index, index + numToPut));\n+        count +\u003d numToPut;\n+      }\n+\n+      BatchWriteItemOutcome res \u003d dynamoDB.batchWriteItem(writeItems);\n+      // Check for unprocessed keys in case of exceeding provisioned throughput\n+      Map\u003cString, List\u003cWriteRequest\u003e\u003e unprocessed \u003d res.getUnprocessedItems();\n+      int retryCount \u003d 0;\n+      while (unprocessed.size() \u003e 0) {\n+        retryBackoff(retryCount++);\n+        res \u003d dynamoDB.batchWriteItemUnprocessed(unprocessed);\n+        unprocessed \u003d res.getUnprocessedItems();\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void processBatchWriteRequest(PrimaryKey[] keysToDelete,\n      Item[] itemsToPut) throws IOException {\n    final int totalToDelete \u003d (keysToDelete \u003d\u003d null ? 0 : keysToDelete.length);\n    final int totalToPut \u003d (itemsToPut \u003d\u003d null ? 0 : itemsToPut.length);\n    int count \u003d 0;\n    while (count \u003c totalToDelete + totalToPut) {\n      final TableWriteItems writeItems \u003d new TableWriteItems(tableName);\n      int numToDelete \u003d 0;\n      if (keysToDelete !\u003d null\n          \u0026\u0026 count \u003c totalToDelete) {\n        numToDelete \u003d Math.min(S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT,\n            totalToDelete - count);\n        writeItems.withPrimaryKeysToDelete(\n            Arrays.copyOfRange(keysToDelete, count, count + numToDelete));\n        count +\u003d numToDelete;\n      }\n\n      if (numToDelete \u003c S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT\n          \u0026\u0026 itemsToPut !\u003d null\n          \u0026\u0026 count \u003c totalToDelete + totalToPut) {\n        final int numToPut \u003d Math.min(\n            S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT - numToDelete,\n            totalToDelete + totalToPut - count);\n        final int index \u003d count - totalToDelete;\n        writeItems.withItemsToPut(\n            Arrays.copyOfRange(itemsToPut, index, index + numToPut));\n        count +\u003d numToPut;\n      }\n\n      BatchWriteItemOutcome res \u003d dynamoDB.batchWriteItem(writeItems);\n      // Check for unprocessed keys in case of exceeding provisioned throughput\n      Map\u003cString, List\u003cWriteRequest\u003e\u003e unprocessed \u003d res.getUnprocessedItems();\n      int retryCount \u003d 0;\n      while (unprocessed.size() \u003e 0) {\n        retryBackoff(retryCount++);\n        res \u003d dynamoDB.batchWriteItemUnprocessed(unprocessed);\n        unprocessed \u003d res.getUnprocessedItems();\n      }\n    }\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java"
    }
  }
}