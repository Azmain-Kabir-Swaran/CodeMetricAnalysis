{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSImageFormatPBINode.java",
  "functionName": "loadINodeSectionInParallel",
  "functionId": "loadINodeSectionInParallel___service-ExecutorService__sections-ArrayList__FileSummary.Section____compressionCodec-String__prog-StartupProgress__currentStep-Step",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java",
  "functionStartLine": 377,
  "functionEndLine": 430,
  "numCommitsSeen": 63,
  "timeTaken": 1292,
  "changeHistory": [
    "b67812ea2111fa11bdd76096b923c93e1bdf2923"
  ],
  "changeHistoryShort": {
    "b67812ea2111fa11bdd76096b923c93e1bdf2923": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b67812ea2111fa11bdd76096b923c93e1bdf2923": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-14617. Improve fsimage load time by writing sub-sections to the fsimage index (#1028). Contributed by  Stephen O\u0027Donnell.\n\nReviewed-by: He Xiaoqiao \u003chexiaoqiao@apache.org\u003e",
      "commitDate": "22/08/19 5:09 PM",
      "commitName": "b67812ea2111fa11bdd76096b923c93e1bdf2923",
      "commitAuthor": "Stephen O\u0027Donnell",
      "diff": "@@ -0,0 +1,54 @@\n+    void loadINodeSectionInParallel(ExecutorService service,\n+        ArrayList\u003cFileSummary.Section\u003e sections,\n+        String compressionCodec, StartupProgress prog,\n+        Step currentStep) throws IOException {\n+      LOG.info(\"Loading the INode section in parallel with {} sub-sections\",\n+          sections.size());\n+      long expectedInodes \u003d 0;\n+      CountDownLatch latch \u003d new CountDownLatch(sections.size());\n+      AtomicInteger totalLoaded \u003d new AtomicInteger(0);\n+      final CopyOnWriteArrayList\u003cIOException\u003e exceptions \u003d\n+          new CopyOnWriteArrayList\u003c\u003e();\n+\n+      for (int i\u003d0; i \u003c sections.size(); i++) {\n+        FileSummary.Section s \u003d sections.get(i);\n+        InputStream ins \u003d parent.getInputStreamForSection(s, compressionCodec);\n+        if (i \u003d\u003d 0) {\n+          // The first inode section has a header which must be processed first\n+          expectedInodes \u003d loadINodeSectionHeader(ins, prog, currentStep);\n+        }\n+        service.submit(() -\u003e {\n+          try {\n+            totalLoaded.addAndGet(loadINodesInSection(ins, null));\n+            prog.setCount(Phase.LOADING_FSIMAGE, currentStep,\n+                totalLoaded.get());\n+          } catch (Exception e) {\n+            LOG.error(\"An exception occurred loading INodes in parallel\", e);\n+            exceptions.add(new IOException(e));\n+          } finally {\n+            latch.countDown();\n+            try {\n+              ins.close();\n+            } catch (IOException ioe) {\n+              LOG.warn(\"Failed to close the input stream, ignoring\", ioe);\n+            }\n+          }\n+        });\n+      }\n+      try {\n+        latch.await();\n+      } catch (InterruptedException e) {\n+        LOG.info(\"Interrupted waiting for countdown latch\");\n+      }\n+      if (exceptions.size() !\u003d 0) {\n+        LOG.error(\"{} exceptions occurred loading INodes\", exceptions.size());\n+        throw exceptions.get(0);\n+      }\n+      if (totalLoaded.get() !\u003d expectedInodes) {\n+        throw new IOException(\"Expected to load \"+expectedInodes+\" in \" +\n+            \"parallel, but loaded \"+totalLoaded.get()+\". The image may \" +\n+            \"be corrupt.\");\n+      }\n+      LOG.info(\"Completed loading all INode sections. Loaded {} inodes.\",\n+          totalLoaded.get());\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    void loadINodeSectionInParallel(ExecutorService service,\n        ArrayList\u003cFileSummary.Section\u003e sections,\n        String compressionCodec, StartupProgress prog,\n        Step currentStep) throws IOException {\n      LOG.info(\"Loading the INode section in parallel with {} sub-sections\",\n          sections.size());\n      long expectedInodes \u003d 0;\n      CountDownLatch latch \u003d new CountDownLatch(sections.size());\n      AtomicInteger totalLoaded \u003d new AtomicInteger(0);\n      final CopyOnWriteArrayList\u003cIOException\u003e exceptions \u003d\n          new CopyOnWriteArrayList\u003c\u003e();\n\n      for (int i\u003d0; i \u003c sections.size(); i++) {\n        FileSummary.Section s \u003d sections.get(i);\n        InputStream ins \u003d parent.getInputStreamForSection(s, compressionCodec);\n        if (i \u003d\u003d 0) {\n          // The first inode section has a header which must be processed first\n          expectedInodes \u003d loadINodeSectionHeader(ins, prog, currentStep);\n        }\n        service.submit(() -\u003e {\n          try {\n            totalLoaded.addAndGet(loadINodesInSection(ins, null));\n            prog.setCount(Phase.LOADING_FSIMAGE, currentStep,\n                totalLoaded.get());\n          } catch (Exception e) {\n            LOG.error(\"An exception occurred loading INodes in parallel\", e);\n            exceptions.add(new IOException(e));\n          } finally {\n            latch.countDown();\n            try {\n              ins.close();\n            } catch (IOException ioe) {\n              LOG.warn(\"Failed to close the input stream, ignoring\", ioe);\n            }\n          }\n        });\n      }\n      try {\n        latch.await();\n      } catch (InterruptedException e) {\n        LOG.info(\"Interrupted waiting for countdown latch\");\n      }\n      if (exceptions.size() !\u003d 0) {\n        LOG.error(\"{} exceptions occurred loading INodes\", exceptions.size());\n        throw exceptions.get(0);\n      }\n      if (totalLoaded.get() !\u003d expectedInodes) {\n        throw new IOException(\"Expected to load \"+expectedInodes+\" in \" +\n            \"parallel, but loaded \"+totalLoaded.get()+\". The image may \" +\n            \"be corrupt.\");\n      }\n      LOG.info(\"Completed loading all INode sections. Loaded {} inodes.\",\n          totalLoaded.get());\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java"
    }
  }
}