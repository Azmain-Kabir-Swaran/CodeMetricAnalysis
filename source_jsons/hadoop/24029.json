{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LocalJobRunner.java",
  "functionName": "createReduceExecutor",
  "functionId": "createReduceExecutor",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
  "functionStartLine": 442,
  "functionEndLine": 462,
  "numCommitsSeen": 29,
  "timeTaken": 5523,
  "changeHistory": [
    "2440671a117f165dcda5056404bc898df3c50803",
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
    "8b2f6909ec7df5cffb5ef417f5c9cffdee43e38a",
    "cfb6a9883d2bf02c99f258e9f19ffcd83805d077",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "2440671a117f165dcda5056404bc898df3c50803": "Ybodychange",
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Ybodychange)",
    "8b2f6909ec7df5cffb5ef417f5c9cffdee43e38a": "Ybodychange",
    "cfb6a9883d2bf02c99f258e9f19ffcd83805d077": "Yfilerename",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "2440671a117f165dcda5056404bc898df3c50803": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6634. Log uncaught exceptions/errors in various thread pools in mapreduce. Contributed by Sidharta Seethana.\n",
      "commitDate": "18/02/16 12:48 AM",
      "commitName": "2440671a117f165dcda5056404bc898df3c50803",
      "commitAuthor": "Varun Vasudev",
      "commitDateOld": "28/08/15 12:13 PM",
      "commitNameOld": "cbb249534aa72ff6c290c4f99766415aeea9d6f5",
      "commitAuthorOld": "Zhihai Xu",
      "daysBetweenCommits": 173.57,
      "commitsBetweenForRepo": 1197,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,21 @@\n     protected synchronized ExecutorService createReduceExecutor() {\n \n       // Determine the size of the thread pool to use\n       int maxReduceThreads \u003d job.getInt(LOCAL_MAX_REDUCES, 1);\n       if (maxReduceThreads \u003c 1) {\n         throw new IllegalArgumentException(\n             \"Configured \" + LOCAL_MAX_REDUCES + \" must be \u003e\u003d 1\");\n       }\n       maxReduceThreads \u003d Math.min(maxReduceThreads, this.numReduceTasks);\n       maxReduceThreads \u003d Math.max(maxReduceThreads, 1); // In case of no tasks.\n \n       LOG.debug(\"Starting reduce thread pool executor.\");\n       LOG.debug(\"Max local threads: \" + maxReduceThreads);\n       LOG.debug(\"Reduce tasks to process: \" + this.numReduceTasks);\n \n       // Create a new executor service to drain the work queue.\n-      ExecutorService executor \u003d Executors.newFixedThreadPool(maxReduceThreads);\n+      ExecutorService executor \u003d HadoopExecutors.newFixedThreadPool(\n+          maxReduceThreads);\n \n       return executor;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    protected synchronized ExecutorService createReduceExecutor() {\n\n      // Determine the size of the thread pool to use\n      int maxReduceThreads \u003d job.getInt(LOCAL_MAX_REDUCES, 1);\n      if (maxReduceThreads \u003c 1) {\n        throw new IllegalArgumentException(\n            \"Configured \" + LOCAL_MAX_REDUCES + \" must be \u003e\u003d 1\");\n      }\n      maxReduceThreads \u003d Math.min(maxReduceThreads, this.numReduceTasks);\n      maxReduceThreads \u003d Math.max(maxReduceThreads, 1); // In case of no tasks.\n\n      LOG.debug(\"Starting reduce thread pool executor.\");\n      LOG.debug(\"Max local threads: \" + maxReduceThreads);\n      LOG.debug(\"Reduce tasks to process: \" + this.numReduceTasks);\n\n      // Create a new executor service to drain the work queue.\n      ExecutorService executor \u003d HadoopExecutors.newFixedThreadPool(\n          maxReduceThreads);\n\n      return executor;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {}
    },
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4": {
      "type": "Ymultichange(Yrename,Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-434. LocalJobRunner limited to single reducer (Sandy Ryza and Aaron Kimball via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1510866 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/08/13 11:36 PM",
      "commitName": "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
      "commitAuthor": "Sanford Ryza",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "MAPREDUCE-434. LocalJobRunner limited to single reducer (Sandy Ryza and Aaron Kimball via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1510866 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "05/08/13 11:36 PM",
          "commitName": "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
          "commitAuthor": "Sanford Ryza",
          "commitDateOld": "05/08/13 10:07 AM",
          "commitNameOld": "6ea797d0d0a65275aa6a194e97f8d016ac7803f3",
          "commitAuthorOld": "Sanford Ryza",
          "daysBetweenCommits": 0.56,
          "commitsBetweenForRepo": 13,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,26 +1,20 @@\n-    protected ExecutorService createMapExecutor(int numMapTasks) {\n+    protected synchronized ExecutorService createReduceExecutor() {\n \n       // Determine the size of the thread pool to use\n-      int maxMapThreads \u003d job.getInt(LOCAL_MAX_MAPS, 1);\n-      if (maxMapThreads \u003c 1) {\n+      int maxReduceThreads \u003d job.getInt(LOCAL_MAX_REDUCES, 1);\n+      if (maxReduceThreads \u003c 1) {\n         throw new IllegalArgumentException(\n-            \"Configured \" + LOCAL_MAX_MAPS + \" must be \u003e\u003d 1\");\n+            \"Configured \" + LOCAL_MAX_REDUCES + \" must be \u003e\u003d 1\");\n       }\n-      this.numMapTasks \u003d numMapTasks;\n-      maxMapThreads \u003d Math.min(maxMapThreads, this.numMapTasks);\n-      maxMapThreads \u003d Math.max(maxMapThreads, 1); // In case of no tasks.\n+      maxReduceThreads \u003d Math.min(maxReduceThreads, this.numReduceTasks);\n+      maxReduceThreads \u003d Math.max(maxReduceThreads, 1); // In case of no tasks.\n \n-      initCounters(this.numMapTasks);\n-\n-      LOG.debug(\"Starting thread pool executor.\");\n-      LOG.debug(\"Max local threads: \" + maxMapThreads);\n-      LOG.debug(\"Map tasks to process: \" + this.numMapTasks);\n+      LOG.debug(\"Starting reduce thread pool executor.\");\n+      LOG.debug(\"Max local threads: \" + maxReduceThreads);\n+      LOG.debug(\"Reduce tasks to process: \" + this.numReduceTasks);\n \n       // Create a new executor service to drain the work queue.\n-      ThreadFactory tf \u003d new ThreadFactoryBuilder()\n-        .setNameFormat(\"LocalJobRunner Map Task Executor #%d\")\n-        .build();\n-      ExecutorService executor \u003d Executors.newFixedThreadPool(maxMapThreads, tf);\n+      ExecutorService executor \u003d Executors.newFixedThreadPool(maxReduceThreads);\n \n       return executor;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    protected synchronized ExecutorService createReduceExecutor() {\n\n      // Determine the size of the thread pool to use\n      int maxReduceThreads \u003d job.getInt(LOCAL_MAX_REDUCES, 1);\n      if (maxReduceThreads \u003c 1) {\n        throw new IllegalArgumentException(\n            \"Configured \" + LOCAL_MAX_REDUCES + \" must be \u003e\u003d 1\");\n      }\n      maxReduceThreads \u003d Math.min(maxReduceThreads, this.numReduceTasks);\n      maxReduceThreads \u003d Math.max(maxReduceThreads, 1); // In case of no tasks.\n\n      LOG.debug(\"Starting reduce thread pool executor.\");\n      LOG.debug(\"Max local threads: \" + maxReduceThreads);\n      LOG.debug(\"Reduce tasks to process: \" + this.numReduceTasks);\n\n      // Create a new executor service to drain the work queue.\n      ExecutorService executor \u003d Executors.newFixedThreadPool(maxReduceThreads);\n\n      return executor;\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
          "extendedDetails": {
            "oldValue": "createMapExecutor",
            "newValue": "createReduceExecutor"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-434. LocalJobRunner limited to single reducer (Sandy Ryza and Aaron Kimball via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1510866 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "05/08/13 11:36 PM",
          "commitName": "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
          "commitAuthor": "Sanford Ryza",
          "commitDateOld": "05/08/13 10:07 AM",
          "commitNameOld": "6ea797d0d0a65275aa6a194e97f8d016ac7803f3",
          "commitAuthorOld": "Sanford Ryza",
          "daysBetweenCommits": 0.56,
          "commitsBetweenForRepo": 13,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,26 +1,20 @@\n-    protected ExecutorService createMapExecutor(int numMapTasks) {\n+    protected synchronized ExecutorService createReduceExecutor() {\n \n       // Determine the size of the thread pool to use\n-      int maxMapThreads \u003d job.getInt(LOCAL_MAX_MAPS, 1);\n-      if (maxMapThreads \u003c 1) {\n+      int maxReduceThreads \u003d job.getInt(LOCAL_MAX_REDUCES, 1);\n+      if (maxReduceThreads \u003c 1) {\n         throw new IllegalArgumentException(\n-            \"Configured \" + LOCAL_MAX_MAPS + \" must be \u003e\u003d 1\");\n+            \"Configured \" + LOCAL_MAX_REDUCES + \" must be \u003e\u003d 1\");\n       }\n-      this.numMapTasks \u003d numMapTasks;\n-      maxMapThreads \u003d Math.min(maxMapThreads, this.numMapTasks);\n-      maxMapThreads \u003d Math.max(maxMapThreads, 1); // In case of no tasks.\n+      maxReduceThreads \u003d Math.min(maxReduceThreads, this.numReduceTasks);\n+      maxReduceThreads \u003d Math.max(maxReduceThreads, 1); // In case of no tasks.\n \n-      initCounters(this.numMapTasks);\n-\n-      LOG.debug(\"Starting thread pool executor.\");\n-      LOG.debug(\"Max local threads: \" + maxMapThreads);\n-      LOG.debug(\"Map tasks to process: \" + this.numMapTasks);\n+      LOG.debug(\"Starting reduce thread pool executor.\");\n+      LOG.debug(\"Max local threads: \" + maxReduceThreads);\n+      LOG.debug(\"Reduce tasks to process: \" + this.numReduceTasks);\n \n       // Create a new executor service to drain the work queue.\n-      ThreadFactory tf \u003d new ThreadFactoryBuilder()\n-        .setNameFormat(\"LocalJobRunner Map Task Executor #%d\")\n-        .build();\n-      ExecutorService executor \u003d Executors.newFixedThreadPool(maxMapThreads, tf);\n+      ExecutorService executor \u003d Executors.newFixedThreadPool(maxReduceThreads);\n \n       return executor;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    protected synchronized ExecutorService createReduceExecutor() {\n\n      // Determine the size of the thread pool to use\n      int maxReduceThreads \u003d job.getInt(LOCAL_MAX_REDUCES, 1);\n      if (maxReduceThreads \u003c 1) {\n        throw new IllegalArgumentException(\n            \"Configured \" + LOCAL_MAX_REDUCES + \" must be \u003e\u003d 1\");\n      }\n      maxReduceThreads \u003d Math.min(maxReduceThreads, this.numReduceTasks);\n      maxReduceThreads \u003d Math.max(maxReduceThreads, 1); // In case of no tasks.\n\n      LOG.debug(\"Starting reduce thread pool executor.\");\n      LOG.debug(\"Max local threads: \" + maxReduceThreads);\n      LOG.debug(\"Reduce tasks to process: \" + this.numReduceTasks);\n\n      // Create a new executor service to drain the work queue.\n      ExecutorService executor \u003d Executors.newFixedThreadPool(maxReduceThreads);\n\n      return executor;\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
          "extendedDetails": {
            "oldValue": "[numMapTasks-int]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "MAPREDUCE-434. LocalJobRunner limited to single reducer (Sandy Ryza and Aaron Kimball via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1510866 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "05/08/13 11:36 PM",
          "commitName": "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
          "commitAuthor": "Sanford Ryza",
          "commitDateOld": "05/08/13 10:07 AM",
          "commitNameOld": "6ea797d0d0a65275aa6a194e97f8d016ac7803f3",
          "commitAuthorOld": "Sanford Ryza",
          "daysBetweenCommits": 0.56,
          "commitsBetweenForRepo": 13,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,26 +1,20 @@\n-    protected ExecutorService createMapExecutor(int numMapTasks) {\n+    protected synchronized ExecutorService createReduceExecutor() {\n \n       // Determine the size of the thread pool to use\n-      int maxMapThreads \u003d job.getInt(LOCAL_MAX_MAPS, 1);\n-      if (maxMapThreads \u003c 1) {\n+      int maxReduceThreads \u003d job.getInt(LOCAL_MAX_REDUCES, 1);\n+      if (maxReduceThreads \u003c 1) {\n         throw new IllegalArgumentException(\n-            \"Configured \" + LOCAL_MAX_MAPS + \" must be \u003e\u003d 1\");\n+            \"Configured \" + LOCAL_MAX_REDUCES + \" must be \u003e\u003d 1\");\n       }\n-      this.numMapTasks \u003d numMapTasks;\n-      maxMapThreads \u003d Math.min(maxMapThreads, this.numMapTasks);\n-      maxMapThreads \u003d Math.max(maxMapThreads, 1); // In case of no tasks.\n+      maxReduceThreads \u003d Math.min(maxReduceThreads, this.numReduceTasks);\n+      maxReduceThreads \u003d Math.max(maxReduceThreads, 1); // In case of no tasks.\n \n-      initCounters(this.numMapTasks);\n-\n-      LOG.debug(\"Starting thread pool executor.\");\n-      LOG.debug(\"Max local threads: \" + maxMapThreads);\n-      LOG.debug(\"Map tasks to process: \" + this.numMapTasks);\n+      LOG.debug(\"Starting reduce thread pool executor.\");\n+      LOG.debug(\"Max local threads: \" + maxReduceThreads);\n+      LOG.debug(\"Reduce tasks to process: \" + this.numReduceTasks);\n \n       // Create a new executor service to drain the work queue.\n-      ThreadFactory tf \u003d new ThreadFactoryBuilder()\n-        .setNameFormat(\"LocalJobRunner Map Task Executor #%d\")\n-        .build();\n-      ExecutorService executor \u003d Executors.newFixedThreadPool(maxMapThreads, tf);\n+      ExecutorService executor \u003d Executors.newFixedThreadPool(maxReduceThreads);\n \n       return executor;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    protected synchronized ExecutorService createReduceExecutor() {\n\n      // Determine the size of the thread pool to use\n      int maxReduceThreads \u003d job.getInt(LOCAL_MAX_REDUCES, 1);\n      if (maxReduceThreads \u003c 1) {\n        throw new IllegalArgumentException(\n            \"Configured \" + LOCAL_MAX_REDUCES + \" must be \u003e\u003d 1\");\n      }\n      maxReduceThreads \u003d Math.min(maxReduceThreads, this.numReduceTasks);\n      maxReduceThreads \u003d Math.max(maxReduceThreads, 1); // In case of no tasks.\n\n      LOG.debug(\"Starting reduce thread pool executor.\");\n      LOG.debug(\"Max local threads: \" + maxReduceThreads);\n      LOG.debug(\"Reduce tasks to process: \" + this.numReduceTasks);\n\n      // Create a new executor service to drain the work queue.\n      ExecutorService executor \u003d Executors.newFixedThreadPool(maxReduceThreads);\n\n      return executor;\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
          "extendedDetails": {
            "oldValue": "[protected]",
            "newValue": "[protected, synchronized]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-434. LocalJobRunner limited to single reducer (Sandy Ryza and Aaron Kimball via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1510866 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "05/08/13 11:36 PM",
          "commitName": "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
          "commitAuthor": "Sanford Ryza",
          "commitDateOld": "05/08/13 10:07 AM",
          "commitNameOld": "6ea797d0d0a65275aa6a194e97f8d016ac7803f3",
          "commitAuthorOld": "Sanford Ryza",
          "daysBetweenCommits": 0.56,
          "commitsBetweenForRepo": 13,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,26 +1,20 @@\n-    protected ExecutorService createMapExecutor(int numMapTasks) {\n+    protected synchronized ExecutorService createReduceExecutor() {\n \n       // Determine the size of the thread pool to use\n-      int maxMapThreads \u003d job.getInt(LOCAL_MAX_MAPS, 1);\n-      if (maxMapThreads \u003c 1) {\n+      int maxReduceThreads \u003d job.getInt(LOCAL_MAX_REDUCES, 1);\n+      if (maxReduceThreads \u003c 1) {\n         throw new IllegalArgumentException(\n-            \"Configured \" + LOCAL_MAX_MAPS + \" must be \u003e\u003d 1\");\n+            \"Configured \" + LOCAL_MAX_REDUCES + \" must be \u003e\u003d 1\");\n       }\n-      this.numMapTasks \u003d numMapTasks;\n-      maxMapThreads \u003d Math.min(maxMapThreads, this.numMapTasks);\n-      maxMapThreads \u003d Math.max(maxMapThreads, 1); // In case of no tasks.\n+      maxReduceThreads \u003d Math.min(maxReduceThreads, this.numReduceTasks);\n+      maxReduceThreads \u003d Math.max(maxReduceThreads, 1); // In case of no tasks.\n \n-      initCounters(this.numMapTasks);\n-\n-      LOG.debug(\"Starting thread pool executor.\");\n-      LOG.debug(\"Max local threads: \" + maxMapThreads);\n-      LOG.debug(\"Map tasks to process: \" + this.numMapTasks);\n+      LOG.debug(\"Starting reduce thread pool executor.\");\n+      LOG.debug(\"Max local threads: \" + maxReduceThreads);\n+      LOG.debug(\"Reduce tasks to process: \" + this.numReduceTasks);\n \n       // Create a new executor service to drain the work queue.\n-      ThreadFactory tf \u003d new ThreadFactoryBuilder()\n-        .setNameFormat(\"LocalJobRunner Map Task Executor #%d\")\n-        .build();\n-      ExecutorService executor \u003d Executors.newFixedThreadPool(maxMapThreads, tf);\n+      ExecutorService executor \u003d Executors.newFixedThreadPool(maxReduceThreads);\n \n       return executor;\n     }\n\\ No newline at end of file\n",
          "actualSource": "    protected synchronized ExecutorService createReduceExecutor() {\n\n      // Determine the size of the thread pool to use\n      int maxReduceThreads \u003d job.getInt(LOCAL_MAX_REDUCES, 1);\n      if (maxReduceThreads \u003c 1) {\n        throw new IllegalArgumentException(\n            \"Configured \" + LOCAL_MAX_REDUCES + \" must be \u003e\u003d 1\");\n      }\n      maxReduceThreads \u003d Math.min(maxReduceThreads, this.numReduceTasks);\n      maxReduceThreads \u003d Math.max(maxReduceThreads, 1); // In case of no tasks.\n\n      LOG.debug(\"Starting reduce thread pool executor.\");\n      LOG.debug(\"Max local threads: \" + maxReduceThreads);\n      LOG.debug(\"Reduce tasks to process: \" + this.numReduceTasks);\n\n      // Create a new executor service to drain the work queue.\n      ExecutorService executor \u003d Executors.newFixedThreadPool(maxReduceThreads);\n\n      return executor;\n    }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
          "extendedDetails": {}
        }
      ]
    },
    "8b2f6909ec7df5cffb5ef417f5c9cffdee43e38a": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3684. LocalDistributedCacheManager does not shut down its thread pool.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1232981 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/01/12 10:20 AM",
      "commitName": "8b2f6909ec7df5cffb5ef417f5c9cffdee43e38a",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/12/11 3:07 PM",
      "commitNameOld": "a61a18cc098591eacd998e4a2f61babe27353a31",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 29.8,
      "commitsBetweenForRepo": 107,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,26 @@\n     protected ExecutorService createMapExecutor(int numMapTasks) {\n \n       // Determine the size of the thread pool to use\n       int maxMapThreads \u003d job.getInt(LOCAL_MAX_MAPS, 1);\n       if (maxMapThreads \u003c 1) {\n         throw new IllegalArgumentException(\n             \"Configured \" + LOCAL_MAX_MAPS + \" must be \u003e\u003d 1\");\n       }\n       this.numMapTasks \u003d numMapTasks;\n       maxMapThreads \u003d Math.min(maxMapThreads, this.numMapTasks);\n       maxMapThreads \u003d Math.max(maxMapThreads, 1); // In case of no tasks.\n \n       initCounters(this.numMapTasks);\n \n       LOG.debug(\"Starting thread pool executor.\");\n       LOG.debug(\"Max local threads: \" + maxMapThreads);\n       LOG.debug(\"Map tasks to process: \" + this.numMapTasks);\n \n       // Create a new executor service to drain the work queue.\n-      ExecutorService executor \u003d Executors.newFixedThreadPool(maxMapThreads);\n+      ThreadFactory tf \u003d new ThreadFactoryBuilder()\n+        .setNameFormat(\"LocalJobRunner Map Task Executor #%d\")\n+        .build();\n+      ExecutorService executor \u003d Executors.newFixedThreadPool(maxMapThreads, tf);\n \n       return executor;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    protected ExecutorService createMapExecutor(int numMapTasks) {\n\n      // Determine the size of the thread pool to use\n      int maxMapThreads \u003d job.getInt(LOCAL_MAX_MAPS, 1);\n      if (maxMapThreads \u003c 1) {\n        throw new IllegalArgumentException(\n            \"Configured \" + LOCAL_MAX_MAPS + \" must be \u003e\u003d 1\");\n      }\n      this.numMapTasks \u003d numMapTasks;\n      maxMapThreads \u003d Math.min(maxMapThreads, this.numMapTasks);\n      maxMapThreads \u003d Math.max(maxMapThreads, 1); // In case of no tasks.\n\n      initCounters(this.numMapTasks);\n\n      LOG.debug(\"Starting thread pool executor.\");\n      LOG.debug(\"Max local threads: \" + maxMapThreads);\n      LOG.debug(\"Map tasks to process: \" + this.numMapTasks);\n\n      // Create a new executor service to drain the work queue.\n      ThreadFactory tf \u003d new ThreadFactoryBuilder()\n        .setNameFormat(\"LocalJobRunner Map Task Executor #%d\")\n        .build();\n      ExecutorService executor \u003d Executors.newFixedThreadPool(maxMapThreads, tf);\n\n      return executor;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {}
    },
    "cfb6a9883d2bf02c99f258e9f19ffcd83805d077": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-3237. Move LocalJobRunner to hadoop-mapreduce-client-core. Contributed by Tom White.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195792 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/10/11 8:16 PM",
      "commitName": "cfb6a9883d2bf02c99f258e9f19ffcd83805d077",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "31/10/11 7:09 PM",
      "commitNameOld": "e5badc0c1a817ca8f7e4255ec4dcfdf858abb596",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.05,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    protected ExecutorService createMapExecutor(int numMapTasks) {\n\n      // Determine the size of the thread pool to use\n      int maxMapThreads \u003d job.getInt(LOCAL_MAX_MAPS, 1);\n      if (maxMapThreads \u003c 1) {\n        throw new IllegalArgumentException(\n            \"Configured \" + LOCAL_MAX_MAPS + \" must be \u003e\u003d 1\");\n      }\n      this.numMapTasks \u003d numMapTasks;\n      maxMapThreads \u003d Math.min(maxMapThreads, this.numMapTasks);\n      maxMapThreads \u003d Math.max(maxMapThreads, 1); // In case of no tasks.\n\n      initCounters(this.numMapTasks);\n\n      LOG.debug(\"Starting thread pool executor.\");\n      LOG.debug(\"Max local threads: \" + maxMapThreads);\n      LOG.debug(\"Map tasks to process: \" + this.numMapTasks);\n\n      // Create a new executor service to drain the work queue.\n      ExecutorService executor \u003d Executors.newFixedThreadPool(maxMapThreads);\n\n      return executor;\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    protected ExecutorService createMapExecutor(int numMapTasks) {\n\n      // Determine the size of the thread pool to use\n      int maxMapThreads \u003d job.getInt(LOCAL_MAX_MAPS, 1);\n      if (maxMapThreads \u003c 1) {\n        throw new IllegalArgumentException(\n            \"Configured \" + LOCAL_MAX_MAPS + \" must be \u003e\u003d 1\");\n      }\n      this.numMapTasks \u003d numMapTasks;\n      maxMapThreads \u003d Math.min(maxMapThreads, this.numMapTasks);\n      maxMapThreads \u003d Math.max(maxMapThreads, 1); // In case of no tasks.\n\n      initCounters(this.numMapTasks);\n\n      LOG.debug(\"Starting thread pool executor.\");\n      LOG.debug(\"Max local threads: \" + maxMapThreads);\n      LOG.debug(\"Map tasks to process: \" + this.numMapTasks);\n\n      // Create a new executor service to drain the work queue.\n      ExecutorService executor \u003d Executors.newFixedThreadPool(maxMapThreads);\n\n      return executor;\n    }",
      "path": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
        "newPath": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalJobRunner.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    protected ExecutorService createMapExecutor(int numMapTasks) {\n\n      // Determine the size of the thread pool to use\n      int maxMapThreads \u003d job.getInt(LOCAL_MAX_MAPS, 1);\n      if (maxMapThreads \u003c 1) {\n        throw new IllegalArgumentException(\n            \"Configured \" + LOCAL_MAX_MAPS + \" must be \u003e\u003d 1\");\n      }\n      this.numMapTasks \u003d numMapTasks;\n      maxMapThreads \u003d Math.min(maxMapThreads, this.numMapTasks);\n      maxMapThreads \u003d Math.max(maxMapThreads, 1); // In case of no tasks.\n\n      initCounters(this.numMapTasks);\n\n      LOG.debug(\"Starting thread pool executor.\");\n      LOG.debug(\"Max local threads: \" + maxMapThreads);\n      LOG.debug(\"Map tasks to process: \" + this.numMapTasks);\n\n      // Create a new executor service to drain the work queue.\n      ExecutorService executor \u003d Executors.newFixedThreadPool(maxMapThreads);\n\n      return executor;\n    }",
      "path": "hadoop-mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java",
        "newPath": "hadoop-mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,23 @@\n+    protected ExecutorService createMapExecutor(int numMapTasks) {\n+\n+      // Determine the size of the thread pool to use\n+      int maxMapThreads \u003d job.getInt(LOCAL_MAX_MAPS, 1);\n+      if (maxMapThreads \u003c 1) {\n+        throw new IllegalArgumentException(\n+            \"Configured \" + LOCAL_MAX_MAPS + \" must be \u003e\u003d 1\");\n+      }\n+      this.numMapTasks \u003d numMapTasks;\n+      maxMapThreads \u003d Math.min(maxMapThreads, this.numMapTasks);\n+      maxMapThreads \u003d Math.max(maxMapThreads, 1); // In case of no tasks.\n+\n+      initCounters(this.numMapTasks);\n+\n+      LOG.debug(\"Starting thread pool executor.\");\n+      LOG.debug(\"Max local threads: \" + maxMapThreads);\n+      LOG.debug(\"Map tasks to process: \" + this.numMapTasks);\n+\n+      // Create a new executor service to drain the work queue.\n+      ExecutorService executor \u003d Executors.newFixedThreadPool(maxMapThreads);\n+\n+      return executor;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    protected ExecutorService createMapExecutor(int numMapTasks) {\n\n      // Determine the size of the thread pool to use\n      int maxMapThreads \u003d job.getInt(LOCAL_MAX_MAPS, 1);\n      if (maxMapThreads \u003c 1) {\n        throw new IllegalArgumentException(\n            \"Configured \" + LOCAL_MAX_MAPS + \" must be \u003e\u003d 1\");\n      }\n      this.numMapTasks \u003d numMapTasks;\n      maxMapThreads \u003d Math.min(maxMapThreads, this.numMapTasks);\n      maxMapThreads \u003d Math.max(maxMapThreads, 1); // In case of no tasks.\n\n      initCounters(this.numMapTasks);\n\n      LOG.debug(\"Starting thread pool executor.\");\n      LOG.debug(\"Max local threads: \" + maxMapThreads);\n      LOG.debug(\"Map tasks to process: \" + this.numMapTasks);\n\n      // Create a new executor service to drain the work queue.\n      ExecutorService executor \u003d Executors.newFixedThreadPool(maxMapThreads);\n\n      return executor;\n    }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/LocalJobRunner.java"
    }
  }
}