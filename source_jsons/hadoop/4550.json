{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PBHelper.java",
  "functionName": "convertSlowDiskInfo",
  "functionId": "convertSlowDiskInfo___slowDisks-SlowDiskReports",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
  "functionStartLine": 887,
  "functionEndLine": 916,
  "numCommitsSeen": 195,
  "timeTaken": 1796,
  "changeHistory": [
    "e7c8da614c37e36fb8081234f4c639d6054f6082"
  ],
  "changeHistoryShort": {
    "e7c8da614c37e36fb8081234f4c639d6054f6082": "Yintroduced"
  },
  "changeHistoryDetails": {
    "e7c8da614c37e36fb8081234f4c639d6054f6082": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11545. Propagate DataNode\u0027s slow disks info to the NameNode via Heartbeats. Contributed by Hanisha Koneru.\n",
      "commitDate": "20/03/17 9:54 PM",
      "commitName": "e7c8da614c37e36fb8081234f4c639d6054f6082",
      "commitAuthor": "Arpit Agarwal",
      "diff": "@@ -0,0 +1,30 @@\n+  public static List\u003cSlowDiskReportProto\u003e convertSlowDiskInfo(\n+      SlowDiskReports slowDisks) {\n+    if (slowDisks.getSlowDisks().size() \u003d\u003d 0) {\n+      return Collections.emptyList();\n+    }\n+\n+    List\u003cSlowDiskReportProto\u003e slowDiskInfoProtos \u003d\n+        new ArrayList\u003c\u003e(slowDisks.getSlowDisks().size());\n+    for (Map.Entry\u003cString, Map\u003cSlowDiskReports.DiskOp, Double\u003e\u003e entry :\n+        slowDisks.getSlowDisks().entrySet()) {\n+      SlowDiskReportProto.Builder builder \u003d SlowDiskReportProto.newBuilder();\n+      builder.setBasePath(entry.getKey());\n+      Map\u003cSlowDiskReports.DiskOp, Double\u003e value \u003d entry.getValue();\n+      if (value.get(SlowDiskReports.DiskOp.METADATA) !\u003d null) {\n+        builder.setMeanMetadataOpLatency(value.get(\n+            SlowDiskReports.DiskOp.METADATA));\n+      }\n+      if (value.get(SlowDiskReports.DiskOp.READ) !\u003d null) {\n+        builder.setMeanReadIoLatency(value.get(\n+            SlowDiskReports.DiskOp.READ));\n+      }\n+      if (value.get(SlowDiskReports.DiskOp.WRITE) !\u003d null) {\n+        builder.setMeanWriteIoLatency(value.get(\n+            SlowDiskReports.DiskOp.WRITE));\n+      }\n+      slowDiskInfoProtos.add(builder.build());\n+    }\n+\n+    return slowDiskInfoProtos;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static List\u003cSlowDiskReportProto\u003e convertSlowDiskInfo(\n      SlowDiskReports slowDisks) {\n    if (slowDisks.getSlowDisks().size() \u003d\u003d 0) {\n      return Collections.emptyList();\n    }\n\n    List\u003cSlowDiskReportProto\u003e slowDiskInfoProtos \u003d\n        new ArrayList\u003c\u003e(slowDisks.getSlowDisks().size());\n    for (Map.Entry\u003cString, Map\u003cSlowDiskReports.DiskOp, Double\u003e\u003e entry :\n        slowDisks.getSlowDisks().entrySet()) {\n      SlowDiskReportProto.Builder builder \u003d SlowDiskReportProto.newBuilder();\n      builder.setBasePath(entry.getKey());\n      Map\u003cSlowDiskReports.DiskOp, Double\u003e value \u003d entry.getValue();\n      if (value.get(SlowDiskReports.DiskOp.METADATA) !\u003d null) {\n        builder.setMeanMetadataOpLatency(value.get(\n            SlowDiskReports.DiskOp.METADATA));\n      }\n      if (value.get(SlowDiskReports.DiskOp.READ) !\u003d null) {\n        builder.setMeanReadIoLatency(value.get(\n            SlowDiskReports.DiskOp.READ));\n      }\n      if (value.get(SlowDiskReports.DiskOp.WRITE) !\u003d null) {\n        builder.setMeanWriteIoLatency(value.get(\n            SlowDiskReports.DiskOp.WRITE));\n      }\n      slowDiskInfoProtos.add(builder.build());\n    }\n\n    return slowDiskInfoProtos;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java"
    }
  }
}