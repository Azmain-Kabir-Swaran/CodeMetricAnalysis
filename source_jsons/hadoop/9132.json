{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSNamesystem.java",
  "functionName": "datanodeReport",
  "functionId": "datanodeReport___type-DatanodeReportType(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
  "functionStartLine": 4804,
  "functionEndLine": 4826,
  "numCommitsSeen": 910,
  "timeTaken": 56322,
  "changeHistory": [
    "1824aee9da4056de0fb638906b2172e486bbebe7",
    "9714fc1dd48edb1c40d96d69ae82ed3b0fab7748",
    "9d3e4cccf9cd0ffb60ee0e7c65cea5ae3c8015c2",
    "ff0b99eafeda035ebe0dc82cfe689808047a8893",
    "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d",
    "3bf09c51501a23b7fa28fd0a0c4c0965858d026c",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "1824aee9da4056de0fb638906b2172e486bbebe7": "Ybodychange",
    "9714fc1dd48edb1c40d96d69ae82ed3b0fab7748": "Ybodychange",
    "9d3e4cccf9cd0ffb60ee0e7c65cea5ae3c8015c2": "Ymultichange(Yexceptionschange,Ybodychange)",
    "ff0b99eafeda035ebe0dc82cfe689808047a8893": "Ybodychange",
    "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d": "Ybodychange",
    "3bf09c51501a23b7fa28fd0a0c4c0965858d026c": "Ymultichange(Yexceptionschange,Ybodychange)",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9": "Ymultichange(Ymodifierchange,Ybodychange,Yparametermetachange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "1824aee9da4056de0fb638906b2172e486bbebe7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15217 Add more information to longest write/read lock held log\n\n",
      "commitDate": "18/04/20 1:52 PM",
      "commitName": "1824aee9da4056de0fb638906b2172e486bbebe7",
      "commitAuthor": "Toshihiro Suzuki",
      "commitDateOld": "25/03/20 10:28 AM",
      "commitNameOld": "a700803a18fb957d2799001a2ce1dcb70f75c080",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 24.14,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   DatanodeInfo[] datanodeReport(final DatanodeReportType type)\n       throws IOException {\n     String operationName \u003d \"datanodeReport\";\n     DatanodeInfo[] arr;\n     checkSuperuserPrivilege(operationName);\n     checkOperation(OperationCategory.UNCHECKED);\n     readLock();\n     try {\n       checkOperation(OperationCategory.UNCHECKED);\n       final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n       final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n       arr \u003d new DatanodeInfo[results.size()];\n       for (int i\u003d0; i\u003carr.length; i++) {\n         arr[i] \u003d new DatanodeInfoBuilder().setFrom(results.get(i))\n             .build();\n         arr[i].setNumBlocks(results.get(i).numBlocks());\n       }\n     } finally {\n-      readUnlock(operationName);\n+      readUnlock(operationName, getLockReportInfoSupplier(null));\n     }\n     logAuditEvent(true, operationName, null);\n     return arr;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeInfo[] datanodeReport(final DatanodeReportType type)\n      throws IOException {\n    String operationName \u003d \"datanodeReport\";\n    DatanodeInfo[] arr;\n    checkSuperuserPrivilege(operationName);\n    checkOperation(OperationCategory.UNCHECKED);\n    readLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n      arr \u003d new DatanodeInfo[results.size()];\n      for (int i\u003d0; i\u003carr.length; i++) {\n        arr[i] \u003d new DatanodeInfoBuilder().setFrom(results.get(i))\n            .build();\n        arr[i].setNumBlocks(results.get(i).numBlocks());\n      }\n    } finally {\n      readUnlock(operationName, getLockReportInfoSupplier(null));\n    }\n    logAuditEvent(true, operationName, null);\n    return arr;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "9714fc1dd48edb1c40d96d69ae82ed3b0fab7748": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-336. dfsadmin -report should report number of blocks from datanode. Contributed by Bharat Viswanadham.\n",
      "commitDate": "13/03/18 4:39 PM",
      "commitName": "9714fc1dd48edb1c40d96d69ae82ed3b0fab7748",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "07/03/18 11:27 AM",
      "commitNameOld": "88fba00caa8c8e26f70deb9be5b534e7482620a1",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 6.17,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,23 @@\n   DatanodeInfo[] datanodeReport(final DatanodeReportType type)\n       throws IOException {\n     String operationName \u003d \"datanodeReport\";\n     DatanodeInfo[] arr;\n     checkSuperuserPrivilege(operationName);\n     checkOperation(OperationCategory.UNCHECKED);\n     readLock();\n     try {\n       checkOperation(OperationCategory.UNCHECKED);\n       final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n       final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n       arr \u003d new DatanodeInfo[results.size()];\n       for (int i\u003d0; i\u003carr.length; i++) {\n         arr[i] \u003d new DatanodeInfoBuilder().setFrom(results.get(i))\n             .build();\n+        arr[i].setNumBlocks(results.get(i).numBlocks());\n       }\n     } finally {\n       readUnlock(operationName);\n     }\n     logAuditEvent(true, operationName, null);\n     return arr;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeInfo[] datanodeReport(final DatanodeReportType type)\n      throws IOException {\n    String operationName \u003d \"datanodeReport\";\n    DatanodeInfo[] arr;\n    checkSuperuserPrivilege(operationName);\n    checkOperation(OperationCategory.UNCHECKED);\n    readLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n      arr \u003d new DatanodeInfo[results.size()];\n      for (int i\u003d0; i\u003carr.length; i++) {\n        arr[i] \u003d new DatanodeInfoBuilder().setFrom(results.get(i))\n            .build();\n        arr[i].setNumBlocks(results.get(i).numBlocks());\n      }\n    } finally {\n      readUnlock(operationName);\n    }\n    logAuditEvent(true, operationName, null);\n    return arr;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "9d3e4cccf9cd0ffb60ee0e7c65cea5ae3c8015c2": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-5040.Audit log for admin commands/ logging output of all DFS admin commands. Contributed by Kuhu Shukla.\n",
      "commitDate": "26/09/17 9:29 AM",
      "commitName": "9d3e4cccf9cd0ffb60ee0e7c65cea5ae3c8015c2",
      "commitAuthor": "Brahma Reddy Battula",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-5040.Audit log for admin commands/ logging output of all DFS admin commands. Contributed by Kuhu Shukla.\n",
          "commitDate": "26/09/17 9:29 AM",
          "commitName": "9d3e4cccf9cd0ffb60ee0e7c65cea5ae3c8015c2",
          "commitAuthor": "Brahma Reddy Battula",
          "commitDateOld": "24/09/17 9:03 PM",
          "commitNameOld": "d0b2c5850b523a3888b2fadcfcdf6edbed33f221",
          "commitAuthorOld": "Anu Engineer",
          "daysBetweenCommits": 1.52,
          "commitsBetweenForRepo": 15,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,22 @@\n-  DatanodeInfo[] datanodeReport(final DatanodeReportType type\n-      ) throws AccessControlException, StandbyException {\n-    checkSuperuserPrivilege();\n+  DatanodeInfo[] datanodeReport(final DatanodeReportType type)\n+      throws IOException {\n+    String operationName \u003d \"datanodeReport\";\n+    DatanodeInfo[] arr;\n+    checkSuperuserPrivilege(operationName);\n     checkOperation(OperationCategory.UNCHECKED);\n     readLock();\n     try {\n       checkOperation(OperationCategory.UNCHECKED);\n       final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n       final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n-\n-      DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n+      arr \u003d new DatanodeInfo[results.size()];\n       for (int i\u003d0; i\u003carr.length; i++) {\n         arr[i] \u003d new DatanodeInfoBuilder().setFrom(results.get(i))\n             .build();\n       }\n-      return arr;\n     } finally {\n-      readUnlock(\"datanodeReport\");\n+      readUnlock(operationName);\n     }\n+    logAuditEvent(true, operationName, null);\n+    return arr;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeInfo[] datanodeReport(final DatanodeReportType type)\n      throws IOException {\n    String operationName \u003d \"datanodeReport\";\n    DatanodeInfo[] arr;\n    checkSuperuserPrivilege(operationName);\n    checkOperation(OperationCategory.UNCHECKED);\n    readLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n      arr \u003d new DatanodeInfo[results.size()];\n      for (int i\u003d0; i\u003carr.length; i++) {\n        arr[i] \u003d new DatanodeInfoBuilder().setFrom(results.get(i))\n            .build();\n      }\n    } finally {\n      readUnlock(operationName);\n    }\n    logAuditEvent(true, operationName, null);\n    return arr;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[AccessControlException, StandbyException]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5040.Audit log for admin commands/ logging output of all DFS admin commands. Contributed by Kuhu Shukla.\n",
          "commitDate": "26/09/17 9:29 AM",
          "commitName": "9d3e4cccf9cd0ffb60ee0e7c65cea5ae3c8015c2",
          "commitAuthor": "Brahma Reddy Battula",
          "commitDateOld": "24/09/17 9:03 PM",
          "commitNameOld": "d0b2c5850b523a3888b2fadcfcdf6edbed33f221",
          "commitAuthorOld": "Anu Engineer",
          "daysBetweenCommits": 1.52,
          "commitsBetweenForRepo": 15,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,22 @@\n-  DatanodeInfo[] datanodeReport(final DatanodeReportType type\n-      ) throws AccessControlException, StandbyException {\n-    checkSuperuserPrivilege();\n+  DatanodeInfo[] datanodeReport(final DatanodeReportType type)\n+      throws IOException {\n+    String operationName \u003d \"datanodeReport\";\n+    DatanodeInfo[] arr;\n+    checkSuperuserPrivilege(operationName);\n     checkOperation(OperationCategory.UNCHECKED);\n     readLock();\n     try {\n       checkOperation(OperationCategory.UNCHECKED);\n       final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n       final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n-\n-      DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n+      arr \u003d new DatanodeInfo[results.size()];\n       for (int i\u003d0; i\u003carr.length; i++) {\n         arr[i] \u003d new DatanodeInfoBuilder().setFrom(results.get(i))\n             .build();\n       }\n-      return arr;\n     } finally {\n-      readUnlock(\"datanodeReport\");\n+      readUnlock(operationName);\n     }\n+    logAuditEvent(true, operationName, null);\n+    return arr;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeInfo[] datanodeReport(final DatanodeReportType type)\n      throws IOException {\n    String operationName \u003d \"datanodeReport\";\n    DatanodeInfo[] arr;\n    checkSuperuserPrivilege(operationName);\n    checkOperation(OperationCategory.UNCHECKED);\n    readLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n      arr \u003d new DatanodeInfo[results.size()];\n      for (int i\u003d0; i\u003carr.length; i++) {\n        arr[i] \u003d new DatanodeInfoBuilder().setFrom(results.get(i))\n            .build();\n      }\n    } finally {\n      readUnlock(operationName);\n    }\n    logAuditEvent(true, operationName, null);\n    return arr;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "ff0b99eafeda035ebe0dc82cfe689808047a8893": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10872. Add MutableRate metrics for FSNamesystemLock operations. Contributed by Erik Krogen.\n",
      "commitDate": "14/11/16 11:05 AM",
      "commitName": "ff0b99eafeda035ebe0dc82cfe689808047a8893",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "08/11/16 6:17 PM",
      "commitNameOld": "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 5.7,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,20 @@\n   DatanodeInfo[] datanodeReport(final DatanodeReportType type\n       ) throws AccessControlException, StandbyException {\n     checkSuperuserPrivilege();\n     checkOperation(OperationCategory.UNCHECKED);\n     readLock();\n     try {\n       checkOperation(OperationCategory.UNCHECKED);\n       final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n       final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n \n       DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n       for (int i\u003d0; i\u003carr.length; i++) {\n         arr[i] \u003d new DatanodeInfoBuilder().setFrom(results.get(i))\n             .build();\n       }\n       return arr;\n     } finally {\n-      readUnlock();\n+      readUnlock(\"datanodeReport\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeInfo[] datanodeReport(final DatanodeReportType type\n      ) throws AccessControlException, StandbyException {\n    checkSuperuserPrivilege();\n    checkOperation(OperationCategory.UNCHECKED);\n    readLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n\n      DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n      for (int i\u003d0; i\u003carr.length; i++) {\n        arr[i] \u003d new DatanodeInfoBuilder().setFrom(results.get(i))\n            .build();\n      }\n      return arr;\n    } finally {\n      readUnlock(\"datanodeReport\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9482. Replace DatanodeInfo constructors with a builder pattern. Contributed by Brahma Reddy Battula.\n",
      "commitDate": "08/11/16 6:17 PM",
      "commitName": "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d",
      "commitAuthor": "Brahma Reddy Battula",
      "commitDateOld": "28/10/16 10:53 AM",
      "commitNameOld": "8a9388e5f6d622152798aaaa256064919e4fec37",
      "commitAuthorOld": "Mingliang Liu",
      "daysBetweenCommits": 11.35,
      "commitsBetweenForRepo": 111,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,20 @@\n   DatanodeInfo[] datanodeReport(final DatanodeReportType type\n       ) throws AccessControlException, StandbyException {\n     checkSuperuserPrivilege();\n     checkOperation(OperationCategory.UNCHECKED);\n     readLock();\n     try {\n       checkOperation(OperationCategory.UNCHECKED);\n       final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n       final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n \n       DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n       for (int i\u003d0; i\u003carr.length; i++) {\n-        arr[i] \u003d new DatanodeInfo(results.get(i));\n+        arr[i] \u003d new DatanodeInfoBuilder().setFrom(results.get(i))\n+            .build();\n       }\n       return arr;\n     } finally {\n       readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DatanodeInfo[] datanodeReport(final DatanodeReportType type\n      ) throws AccessControlException, StandbyException {\n    checkSuperuserPrivilege();\n    checkOperation(OperationCategory.UNCHECKED);\n    readLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n\n      DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n      for (int i\u003d0; i\u003carr.length; i++) {\n        arr[i] \u003d new DatanodeInfoBuilder().setFrom(results.get(i))\n            .build();\n      }\n      return arr;\n    } finally {\n      readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "3bf09c51501a23b7fa28fd0a0c4c0965858d026c": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-4591. HA clients can fail to fail over while Standby NN is performing long checkpoint. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1456107 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/03/13 12:51 PM",
      "commitName": "3bf09c51501a23b7fa28fd0a0c4c0965858d026c",
      "commitAuthor": "Aaron Myers",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-4591. HA clients can fail to fail over while Standby NN is performing long checkpoint. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1456107 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/03/13 12:51 PM",
          "commitName": "3bf09c51501a23b7fa28fd0a0c4c0965858d026c",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "12/03/13 7:32 PM",
          "commitNameOld": "86a940f7adc5bd9c9eaea2283df5e014e5079ab6",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 0.72,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,19 @@\n   DatanodeInfo[] datanodeReport(final DatanodeReportType type\n-      ) throws AccessControlException {\n+      ) throws AccessControlException, StandbyException {\n     checkSuperuserPrivilege();\n+    checkOperation(OperationCategory.UNCHECKED);\n     readLock();\n     try {\n+      checkOperation(OperationCategory.UNCHECKED);\n       final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n       final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n \n       DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n       for (int i\u003d0; i\u003carr.length; i++) {\n         arr[i] \u003d new DatanodeInfo(results.get(i));\n       }\n       return arr;\n     } finally {\n       readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeInfo[] datanodeReport(final DatanodeReportType type\n      ) throws AccessControlException, StandbyException {\n    checkSuperuserPrivilege();\n    checkOperation(OperationCategory.UNCHECKED);\n    readLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n\n      DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n      for (int i\u003d0; i\u003carr.length; i++) {\n        arr[i] \u003d new DatanodeInfo(results.get(i));\n      }\n      return arr;\n    } finally {\n      readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[AccessControlException]",
            "newValue": "[AccessControlException, StandbyException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4591. HA clients can fail to fail over while Standby NN is performing long checkpoint. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1456107 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "13/03/13 12:51 PM",
          "commitName": "3bf09c51501a23b7fa28fd0a0c4c0965858d026c",
          "commitAuthor": "Aaron Myers",
          "commitDateOld": "12/03/13 7:32 PM",
          "commitNameOld": "86a940f7adc5bd9c9eaea2283df5e014e5079ab6",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 0.72,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,19 @@\n   DatanodeInfo[] datanodeReport(final DatanodeReportType type\n-      ) throws AccessControlException {\n+      ) throws AccessControlException, StandbyException {\n     checkSuperuserPrivilege();\n+    checkOperation(OperationCategory.UNCHECKED);\n     readLock();\n     try {\n+      checkOperation(OperationCategory.UNCHECKED);\n       final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n       final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n \n       DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n       for (int i\u003d0; i\u003carr.length; i++) {\n         arr[i] \u003d new DatanodeInfo(results.get(i));\n       }\n       return arr;\n     } finally {\n       readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeInfo[] datanodeReport(final DatanodeReportType type\n      ) throws AccessControlException, StandbyException {\n    checkSuperuserPrivilege();\n    checkOperation(OperationCategory.UNCHECKED);\n    readLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n\n      DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n      for (int i\u003d0; i\u003carr.length; i++) {\n        arr[i] \u003d new DatanodeInfo(results.get(i));\n      }\n      return arr;\n    } finally {\n      readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  DatanodeInfo[] datanodeReport(final DatanodeReportType type\n      ) throws AccessControlException {\n    checkSuperuserPrivilege();\n    readLock();\n    try {\n      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n\n      DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n      for (int i\u003d0; i\u003carr.length; i++) {\n        arr[i] \u003d new DatanodeInfo(results.get(i));\n      }\n      return arr;\n    } finally {\n      readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  DatanodeInfo[] datanodeReport(final DatanodeReportType type\n      ) throws AccessControlException {\n    checkSuperuserPrivilege();\n    readLock();\n    try {\n      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n\n      DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n      for (int i\u003d0; i\u003carr.length; i++) {\n        arr[i] \u003d new DatanodeInfo(results.get(i));\n      }\n      return arr;\n    } finally {\n      readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "233a7aa34f37350bf7bcdd9c84b97d613e7344c9": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange,Yparametermetachange)",
      "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/07/11 9:20 PM",
      "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/07/11 9:20 PM",
          "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "20/07/11 4:35 PM",
          "commitNameOld": "08928d067bb9e1d38b5e7db9e23fcf20fe161435",
          "commitAuthorOld": "Matthew Foley",
          "daysBetweenCommits": 1.2,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,17 @@\n-  public DatanodeInfo[] datanodeReport( DatanodeReportType type)\n-      throws AccessControlException {\n+  DatanodeInfo[] datanodeReport(final DatanodeReportType type\n+      ) throws AccessControlException {\n+    checkSuperuserPrivilege();\n     readLock();\n     try {\n-      checkSuperuserPrivilege();\n-  \n-      ArrayList\u003cDatanodeDescriptor\u003e results \u003d getDatanodeListForReport(type);\n+      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n+      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n+\n       DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n       for (int i\u003d0; i\u003carr.length; i++) {\n         arr[i] \u003d new DatanodeInfo(results.get(i));\n       }\n       return arr;\n     } finally {\n       readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeInfo[] datanodeReport(final DatanodeReportType type\n      ) throws AccessControlException {\n    checkSuperuserPrivilege();\n    readLock();\n    try {\n      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n\n      DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n      for (int i\u003d0; i\u003carr.length; i++) {\n        arr[i] \u003d new DatanodeInfo(results.get(i));\n      }\n      return arr;\n    } finally {\n      readUnlock();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/07/11 9:20 PM",
          "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "20/07/11 4:35 PM",
          "commitNameOld": "08928d067bb9e1d38b5e7db9e23fcf20fe161435",
          "commitAuthorOld": "Matthew Foley",
          "daysBetweenCommits": 1.2,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,17 @@\n-  public DatanodeInfo[] datanodeReport( DatanodeReportType type)\n-      throws AccessControlException {\n+  DatanodeInfo[] datanodeReport(final DatanodeReportType type\n+      ) throws AccessControlException {\n+    checkSuperuserPrivilege();\n     readLock();\n     try {\n-      checkSuperuserPrivilege();\n-  \n-      ArrayList\u003cDatanodeDescriptor\u003e results \u003d getDatanodeListForReport(type);\n+      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n+      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n+\n       DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n       for (int i\u003d0; i\u003carr.length; i++) {\n         arr[i] \u003d new DatanodeInfo(results.get(i));\n       }\n       return arr;\n     } finally {\n       readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeInfo[] datanodeReport(final DatanodeReportType type\n      ) throws AccessControlException {\n    checkSuperuserPrivilege();\n    readLock();\n    try {\n      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n\n      DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n      for (int i\u003d0; i\u003carr.length; i++) {\n        arr[i] \u003d new DatanodeInfo(results.get(i));\n      }\n      return arr;\n    } finally {\n      readUnlock();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparametermetachange",
          "commitMessage": "HDFS-2167.  Move dnsToSwitchMapping and hostsReader from FSNamesystem to DatanodeManager.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1149455 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/07/11 9:20 PM",
          "commitName": "233a7aa34f37350bf7bcdd9c84b97d613e7344c9",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "20/07/11 4:35 PM",
          "commitNameOld": "08928d067bb9e1d38b5e7db9e23fcf20fe161435",
          "commitAuthorOld": "Matthew Foley",
          "daysBetweenCommits": 1.2,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,17 @@\n-  public DatanodeInfo[] datanodeReport( DatanodeReportType type)\n-      throws AccessControlException {\n+  DatanodeInfo[] datanodeReport(final DatanodeReportType type\n+      ) throws AccessControlException {\n+    checkSuperuserPrivilege();\n     readLock();\n     try {\n-      checkSuperuserPrivilege();\n-  \n-      ArrayList\u003cDatanodeDescriptor\u003e results \u003d getDatanodeListForReport(type);\n+      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n+      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n+\n       DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n       for (int i\u003d0; i\u003carr.length; i++) {\n         arr[i] \u003d new DatanodeInfo(results.get(i));\n       }\n       return arr;\n     } finally {\n       readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  DatanodeInfo[] datanodeReport(final DatanodeReportType type\n      ) throws AccessControlException {\n    checkSuperuserPrivilege();\n    readLock();\n    try {\n      final DatanodeManager dm \u003d getBlockManager().getDatanodeManager();      \n      final List\u003cDatanodeDescriptor\u003e results \u003d dm.getDatanodeListForReport(type);\n\n      DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n      for (int i\u003d0; i\u003carr.length; i++) {\n        arr[i] \u003d new DatanodeInfo(results.get(i));\n      }\n      return arr;\n    } finally {\n      readUnlock();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[type-DatanodeReportType]",
            "newValue": "[type-DatanodeReportType(modifiers-final)]"
          }
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,16 @@\n+  public DatanodeInfo[] datanodeReport( DatanodeReportType type)\n+      throws AccessControlException {\n+    readLock();\n+    try {\n+      checkSuperuserPrivilege();\n+  \n+      ArrayList\u003cDatanodeDescriptor\u003e results \u003d getDatanodeListForReport(type);\n+      DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n+      for (int i\u003d0; i\u003carr.length; i++) {\n+        arr[i] \u003d new DatanodeInfo(results.get(i));\n+      }\n+      return arr;\n+    } finally {\n+      readUnlock();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeInfo[] datanodeReport( DatanodeReportType type)\n      throws AccessControlException {\n    readLock();\n    try {\n      checkSuperuserPrivilege();\n  \n      ArrayList\u003cDatanodeDescriptor\u003e results \u003d getDatanodeListForReport(type);\n      DatanodeInfo[] arr \u003d new DatanodeInfo[results.size()];\n      for (int i\u003d0; i\u003carr.length; i++) {\n        arr[i] \u003d new DatanodeInfo(results.get(i));\n      }\n      return arr;\n    } finally {\n      readUnlock();\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
    }
  }
}