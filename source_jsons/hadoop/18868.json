{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PeriodicStatsAccumulator.java",
  "functionName": "extend",
  "functionId": "extend___newProgress-double__newValue-int",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/PeriodicStatsAccumulator.java",
  "functionStartLine": 145,
  "functionEndLine": 196,
  "numCommitsSeen": 4,
  "timeTaken": 1326,
  "changeHistory": [
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "989c5e90a58d06320e70d7746a97d9a8376c49e3"
  ],
  "changeHistoryShort": {
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "989c5e90a58d06320e70d7746a97d9a8376c49e3": "Yintroduced"
  },
  "changeHistoryDetails": {
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected void extend(double newProgress, int newValue) {\n    if (state \u003d\u003d null || newProgress \u003c state.oldProgress) {\n      return;\n    }\n\n    // This correctness of this code depends on 100% * count \u003d count.\n    int oldIndex \u003d (int)(state.oldProgress * count);\n    int newIndex \u003d (int)(newProgress * count);\n    int originalOldValue \u003d state.oldValue;\n\n    double fullValueDistance \u003d (double)newValue - state.oldValue;\n    double fullProgressDistance \u003d newProgress - state.oldProgress;\n    double originalOldProgress \u003d state.oldProgress;\n\n    // In this loop we detect each subinterval boundary within the\n    //  range from the old progress to the new one.  Then we\n    //  interpolate the value from the old value to the new one to\n    //  infer what its value might have been at each such boundary.\n    //  Lastly we make the necessary calls to extendInternal to fold\n    //  in the data for each trapazoid where no such trapazoid\n    //  crosses a boundary.\n    for (int closee \u003d oldIndex; closee \u003c newIndex; ++closee) {\n      double interpolationProgress \u003d (double)(closee + 1) / count;\n      // In floats, x * y / y might not equal y.\n      interpolationProgress \u003d Math.min(interpolationProgress, newProgress);\n\n      double progressLength \u003d (interpolationProgress - originalOldProgress);\n      double interpolationProportion \u003d progressLength / fullProgressDistance;\n\n      double interpolationValueDistance\n        \u003d fullValueDistance * interpolationProportion;\n\n      // estimates the value at the next [interpolated] subsegment boundary\n      int interpolationValue\n        \u003d (int)interpolationValueDistance + originalOldValue;\n\n      extendInternal(interpolationProgress, interpolationValue);\n\n      advanceState(interpolationProgress, interpolationValue);\n\n      values[closee] \u003d (int)state.currentAccumulation;\n      initializeInterval();\n\n    }\n\n    extendInternal(newProgress, newValue);\n    advanceState(newProgress, newValue);\n\n    if (newIndex \u003d\u003d count) {\n      state \u003d null;\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/PeriodicStatsAccumulator.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/PeriodicStatsAccumulator.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/PeriodicStatsAccumulator.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected void extend(double newProgress, int newValue) {\n    if (state \u003d\u003d null || newProgress \u003c state.oldProgress) {\n      return;\n    }\n\n    // This correctness of this code depends on 100% * count \u003d count.\n    int oldIndex \u003d (int)(state.oldProgress * count);\n    int newIndex \u003d (int)(newProgress * count);\n    int originalOldValue \u003d state.oldValue;\n\n    double fullValueDistance \u003d (double)newValue - state.oldValue;\n    double fullProgressDistance \u003d newProgress - state.oldProgress;\n    double originalOldProgress \u003d state.oldProgress;\n\n    // In this loop we detect each subinterval boundary within the\n    //  range from the old progress to the new one.  Then we\n    //  interpolate the value from the old value to the new one to\n    //  infer what its value might have been at each such boundary.\n    //  Lastly we make the necessary calls to extendInternal to fold\n    //  in the data for each trapazoid where no such trapazoid\n    //  crosses a boundary.\n    for (int closee \u003d oldIndex; closee \u003c newIndex; ++closee) {\n      double interpolationProgress \u003d (double)(closee + 1) / count;\n      // In floats, x * y / y might not equal y.\n      interpolationProgress \u003d Math.min(interpolationProgress, newProgress);\n\n      double progressLength \u003d (interpolationProgress - originalOldProgress);\n      double interpolationProportion \u003d progressLength / fullProgressDistance;\n\n      double interpolationValueDistance\n        \u003d fullValueDistance * interpolationProportion;\n\n      // estimates the value at the next [interpolated] subsegment boundary\n      int interpolationValue\n        \u003d (int)interpolationValueDistance + originalOldValue;\n\n      extendInternal(interpolationProgress, interpolationValue);\n\n      advanceState(interpolationProgress, interpolationValue);\n\n      values[closee] \u003d (int)state.currentAccumulation;\n      initializeInterval();\n\n    }\n\n    extendInternal(newProgress, newValue);\n    advanceState(newProgress, newValue);\n\n    if (newIndex \u003d\u003d count) {\n      state \u003d null;\n    }\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/PeriodicStatsAccumulator.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/PeriodicStatsAccumulator.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/PeriodicStatsAccumulator.java"
      }
    },
    "989c5e90a58d06320e70d7746a97d9a8376c49e3": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-2037. Capture intermediate progress, CPU and memory usage for tasks. Contributed by Dick King. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1157253 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/08/11 2:05 PM",
      "commitName": "989c5e90a58d06320e70d7746a97d9a8376c49e3",
      "commitAuthor": "Arun Murthy",
      "diff": "@@ -0,0 +1,52 @@\n+  protected void extend(double newProgress, int newValue) {\n+    if (state \u003d\u003d null || newProgress \u003c state.oldProgress) {\n+      return;\n+    }\n+\n+    // This correctness of this code depends on 100% * count \u003d count.\n+    int oldIndex \u003d (int)(state.oldProgress * count);\n+    int newIndex \u003d (int)(newProgress * count);\n+    int originalOldValue \u003d state.oldValue;\n+\n+    double fullValueDistance \u003d (double)newValue - state.oldValue;\n+    double fullProgressDistance \u003d newProgress - state.oldProgress;\n+    double originalOldProgress \u003d state.oldProgress;\n+\n+    // In this loop we detect each subinterval boundary within the\n+    //  range from the old progress to the new one.  Then we\n+    //  interpolate the value from the old value to the new one to\n+    //  infer what its value might have been at each such boundary.\n+    //  Lastly we make the necessary calls to extendInternal to fold\n+    //  in the data for each trapazoid where no such trapazoid\n+    //  crosses a boundary.\n+    for (int closee \u003d oldIndex; closee \u003c newIndex; ++closee) {\n+      double interpolationProgress \u003d (double)(closee + 1) / count;\n+      // In floats, x * y / y might not equal y.\n+      interpolationProgress \u003d Math.min(interpolationProgress, newProgress);\n+\n+      double progressLength \u003d (interpolationProgress - originalOldProgress);\n+      double interpolationProportion \u003d progressLength / fullProgressDistance;\n+\n+      double interpolationValueDistance\n+        \u003d fullValueDistance * interpolationProportion;\n+\n+      // estimates the value at the next [interpolated] subsegment boundary\n+      int interpolationValue\n+        \u003d (int)interpolationValueDistance + originalOldValue;\n+\n+      extendInternal(interpolationProgress, interpolationValue);\n+\n+      advanceState(interpolationProgress, interpolationValue);\n+\n+      values[closee] \u003d (int)state.currentAccumulation;\n+      initializeInterval();\n+\n+    }\n+\n+    extendInternal(newProgress, newValue);\n+    advanceState(newProgress, newValue);\n+\n+    if (newIndex \u003d\u003d count) {\n+      state \u003d null;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected void extend(double newProgress, int newValue) {\n    if (state \u003d\u003d null || newProgress \u003c state.oldProgress) {\n      return;\n    }\n\n    // This correctness of this code depends on 100% * count \u003d count.\n    int oldIndex \u003d (int)(state.oldProgress * count);\n    int newIndex \u003d (int)(newProgress * count);\n    int originalOldValue \u003d state.oldValue;\n\n    double fullValueDistance \u003d (double)newValue - state.oldValue;\n    double fullProgressDistance \u003d newProgress - state.oldProgress;\n    double originalOldProgress \u003d state.oldProgress;\n\n    // In this loop we detect each subinterval boundary within the\n    //  range from the old progress to the new one.  Then we\n    //  interpolate the value from the old value to the new one to\n    //  infer what its value might have been at each such boundary.\n    //  Lastly we make the necessary calls to extendInternal to fold\n    //  in the data for each trapazoid where no such trapazoid\n    //  crosses a boundary.\n    for (int closee \u003d oldIndex; closee \u003c newIndex; ++closee) {\n      double interpolationProgress \u003d (double)(closee + 1) / count;\n      // In floats, x * y / y might not equal y.\n      interpolationProgress \u003d Math.min(interpolationProgress, newProgress);\n\n      double progressLength \u003d (interpolationProgress - originalOldProgress);\n      double interpolationProportion \u003d progressLength / fullProgressDistance;\n\n      double interpolationValueDistance\n        \u003d fullValueDistance * interpolationProportion;\n\n      // estimates the value at the next [interpolated] subsegment boundary\n      int interpolationValue\n        \u003d (int)interpolationValueDistance + originalOldValue;\n\n      extendInternal(interpolationProgress, interpolationValue);\n\n      advanceState(interpolationProgress, interpolationValue);\n\n      values[closee] \u003d (int)state.currentAccumulation;\n      initializeInterval();\n\n    }\n\n    extendInternal(newProgress, newValue);\n    advanceState(newProgress, newValue);\n\n    if (newIndex \u003d\u003d count) {\n      state \u003d null;\n    }\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/PeriodicStatsAccumulator.java"
    }
  }
}