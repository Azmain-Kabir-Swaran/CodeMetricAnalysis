{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetImpl.java",
  "functionName": "computeChecksum",
  "functionId": "computeChecksum___srcReplica-ReplicaInfo__dstMeta-File__smallBufferSize-int__conf-Configuration(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
  "functionStartLine": 1167,
  "functionEndLine": 1223,
  "numCommitsSeen": 530,
  "timeTaken": 12467,
  "changeHistory": [
    "7a3188d054481b9bd563e337901e93476303ce7f",
    "80db744ee57c52a1dc306c576c663ccc76cced4c",
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389",
    "c5573e6a7599da17cad733cd274e7a9b75b22bb0",
    "86c9862bec0248d671e657aa56094a2919b8ac14",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "c992bcf9c136d3df686655a80e636bb7bb0664da",
    "4da8490b512a33a255ed27309860859388d7c168",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
    "463aec11718e47d4aabb86a7a539cb973460aae6"
  ],
  "changeHistoryShort": {
    "7a3188d054481b9bd563e337901e93476303ce7f": "Ybodychange",
    "80db744ee57c52a1dc306c576c663ccc76cced4c": "Ybodychange",
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389": "Ybodychange",
    "c5573e6a7599da17cad733cd274e7a9b75b22bb0": "Ymodifierchange",
    "86c9862bec0248d671e657aa56094a2919b8ac14": "Ymultichange(Yparameterchange,Ybodychange)",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Ybodychange",
    "c992bcf9c136d3df686655a80e636bb7bb0664da": "Ymultichange(Yparameterchange,Ybodychange)",
    "4da8490b512a33a255ed27309860859388d7c168": "Ymultichange(Yparameterchange,Ybodychange)",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": "Ybodychange",
    "463aec11718e47d4aabb86a7a539cb973460aae6": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7a3188d054481b9bd563e337901e93476303ce7f": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16282. Avoid FileStream to improve performance. Contributed by Ayush Saxena.\n",
      "commitDate": "02/05/19 12:58 PM",
      "commitName": "7a3188d054481b9bd563e337901e93476303ce7f",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "29/04/19 2:49 PM",
      "commitNameOld": "4b4200f1f87ad40d9c19ba160f706ffd0470a8d4",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 2.92,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   static void computeChecksum(ReplicaInfo srcReplica, File dstMeta,\n       int smallBufferSize, final Configuration conf)\n       throws IOException {\n     final File srcMeta \u003d new File(srcReplica.getMetadataURI());\n \n     DataChecksum checksum;\n     try (FileInputStream fis \u003d\n              srcReplica.getFileIoProvider().getFileInputStream(\n                  srcReplica.getVolume(), srcMeta)) {\n       checksum \u003d BlockMetadataHeader.readDataChecksum(\n           fis, DFSUtilClient.getIoFileBufferSize(conf), srcMeta);\n     }\n \n     final byte[] data \u003d new byte[1 \u003c\u003c 16];\n     final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n \n     DataOutputStream metaOut \u003d null;\n     try {\n       File parentFile \u003d dstMeta.getParentFile();\n       if (parentFile !\u003d null) {\n         if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n           throw new IOException(\"Destination \u0027\" + parentFile\n               + \"\u0027 directory cannot be created\");\n         }\n       }\n       metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n-          new FileOutputStream(dstMeta), smallBufferSize));\n+          Files.newOutputStream(dstMeta.toPath()), smallBufferSize));\n       BlockMetadataHeader.writeHeader(metaOut, checksum);\n \n       int offset \u003d 0;\n       try (InputStream dataIn \u003d srcReplica.getDataInputStream(0)) {\n \n         for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n           if (n \u003e 0) {\n             n +\u003d offset;\n             offset \u003d n % checksum.getBytesPerChecksum();\n             final int length \u003d n - offset;\n \n             if (length \u003e 0) {\n               checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n               metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n \n               System.arraycopy(data, length, data, 0, offset);\n             }\n           }\n         }\n       }\n \n       // calculate and write the last crc\n       checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n       metaOut.write(crcs, 0, 4);\n       metaOut.close();\n       metaOut \u003d null;\n     } finally {\n       IOUtils.closeStream(metaOut);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void computeChecksum(ReplicaInfo srcReplica, File dstMeta,\n      int smallBufferSize, final Configuration conf)\n      throws IOException {\n    final File srcMeta \u003d new File(srcReplica.getMetadataURI());\n\n    DataChecksum checksum;\n    try (FileInputStream fis \u003d\n             srcReplica.getFileIoProvider().getFileInputStream(\n                 srcReplica.getVolume(), srcMeta)) {\n      checksum \u003d BlockMetadataHeader.readDataChecksum(\n          fis, DFSUtilClient.getIoFileBufferSize(conf), srcMeta);\n    }\n\n    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n\n    DataOutputStream metaOut \u003d null;\n    try {\n      File parentFile \u003d dstMeta.getParentFile();\n      if (parentFile !\u003d null) {\n        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n          throw new IOException(\"Destination \u0027\" + parentFile\n              + \"\u0027 directory cannot be created\");\n        }\n      }\n      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n          Files.newOutputStream(dstMeta.toPath()), smallBufferSize));\n      BlockMetadataHeader.writeHeader(metaOut, checksum);\n\n      int offset \u003d 0;\n      try (InputStream dataIn \u003d srcReplica.getDataInputStream(0)) {\n\n        for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n          if (n \u003e 0) {\n            n +\u003d offset;\n            offset \u003d n % checksum.getBytesPerChecksum();\n            final int length \u003d n - offset;\n\n            if (length \u003e 0) {\n              checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n              metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n\n              System.arraycopy(data, length, data, 0, offset);\n            }\n          }\n        }\n      }\n\n      // calculate and write the last crc\n      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n      metaOut.write(crcs, 0, 4);\n      metaOut.close();\n      metaOut \u003d null;\n    } finally {\n      IOUtils.closeStream(metaOut);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "80db744ee57c52a1dc306c576c663ccc76cced4c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12881. Output streams closed with IOUtils suppressing write errors. Contributed by Ajay Kumar\n",
      "commitDate": "14/12/17 7:45 AM",
      "commitName": "80db744ee57c52a1dc306c576c663ccc76cced4c",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "07/11/17 6:22 PM",
      "commitNameOld": "bb8a6eea52cb1e2c3d0b7f8b49a1bab9e4255acd",
      "commitAuthorOld": "Weiwei Yang",
      "daysBetweenCommits": 36.56,
      "commitsBetweenForRepo": 200,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,57 @@\n   static void computeChecksum(ReplicaInfo srcReplica, File dstMeta,\n       int smallBufferSize, final Configuration conf)\n       throws IOException {\n     final File srcMeta \u003d new File(srcReplica.getMetadataURI());\n \n     DataChecksum checksum;\n     try (FileInputStream fis \u003d\n              srcReplica.getFileIoProvider().getFileInputStream(\n                  srcReplica.getVolume(), srcMeta)) {\n       checksum \u003d BlockMetadataHeader.readDataChecksum(\n           fis, DFSUtilClient.getIoFileBufferSize(conf), srcMeta);\n     }\n \n     final byte[] data \u003d new byte[1 \u003c\u003c 16];\n     final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n \n     DataOutputStream metaOut \u003d null;\n     try {\n       File parentFile \u003d dstMeta.getParentFile();\n       if (parentFile !\u003d null) {\n         if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n           throw new IOException(\"Destination \u0027\" + parentFile\n               + \"\u0027 directory cannot be created\");\n         }\n       }\n       metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n           new FileOutputStream(dstMeta), smallBufferSize));\n       BlockMetadataHeader.writeHeader(metaOut, checksum);\n \n       int offset \u003d 0;\n       try (InputStream dataIn \u003d srcReplica.getDataInputStream(0)) {\n \n         for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n           if (n \u003e 0) {\n             n +\u003d offset;\n             offset \u003d n % checksum.getBytesPerChecksum();\n             final int length \u003d n - offset;\n \n             if (length \u003e 0) {\n               checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n               metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n \n               System.arraycopy(data, length, data, 0, offset);\n             }\n           }\n         }\n       }\n \n       // calculate and write the last crc\n       checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n       metaOut.write(crcs, 0, 4);\n+      metaOut.close();\n+      metaOut \u003d null;\n     } finally {\n-      IOUtils.cleanup(null, metaOut);\n+      IOUtils.closeStream(metaOut);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void computeChecksum(ReplicaInfo srcReplica, File dstMeta,\n      int smallBufferSize, final Configuration conf)\n      throws IOException {\n    final File srcMeta \u003d new File(srcReplica.getMetadataURI());\n\n    DataChecksum checksum;\n    try (FileInputStream fis \u003d\n             srcReplica.getFileIoProvider().getFileInputStream(\n                 srcReplica.getVolume(), srcMeta)) {\n      checksum \u003d BlockMetadataHeader.readDataChecksum(\n          fis, DFSUtilClient.getIoFileBufferSize(conf), srcMeta);\n    }\n\n    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n\n    DataOutputStream metaOut \u003d null;\n    try {\n      File parentFile \u003d dstMeta.getParentFile();\n      if (parentFile !\u003d null) {\n        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n          throw new IOException(\"Destination \u0027\" + parentFile\n              + \"\u0027 directory cannot be created\");\n        }\n      }\n      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n          new FileOutputStream(dstMeta), smallBufferSize));\n      BlockMetadataHeader.writeHeader(metaOut, checksum);\n\n      int offset \u003d 0;\n      try (InputStream dataIn \u003d srcReplica.getDataInputStream(0)) {\n\n        for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n          if (n \u003e 0) {\n            n +\u003d offset;\n            offset \u003d n % checksum.getBytesPerChecksum();\n            final int length \u003d n - offset;\n\n            if (length \u003e 0) {\n              checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n              metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n\n              System.arraycopy(data, length, data, 0, offset);\n            }\n          }\n        }\n      }\n\n      // calculate and write the last crc\n      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n      metaOut.write(crcs, 0, 4);\n      metaOut.close();\n      metaOut \u003d null;\n    } finally {\n      IOUtils.closeStream(metaOut);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10958. Add instrumentation hooks around Datanode disk IO.\n",
      "commitDate": "14/12/16 11:18 AM",
      "commitName": "6ba9587d370fbf39c129c08c00ebbb894ccc1389",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "06/12/16 11:05 AM",
      "commitNameOld": "df983b524ab68ea0c70cee9033bfff2d28052cbf",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 8.01,
      "commitsBetweenForRepo": 51,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,55 @@\n   static void computeChecksum(ReplicaInfo srcReplica, File dstMeta,\n       int smallBufferSize, final Configuration conf)\n       throws IOException {\n-    File srcMeta \u003d new File(srcReplica.getMetadataURI());\n-    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta,\n-        DFSUtilClient.getIoFileBufferSize(conf));\n+    final File srcMeta \u003d new File(srcReplica.getMetadataURI());\n+\n+    DataChecksum checksum;\n+    try (FileInputStream fis \u003d\n+             srcReplica.getFileIoProvider().getFileInputStream(\n+                 srcReplica.getVolume(), srcMeta)) {\n+      checksum \u003d BlockMetadataHeader.readDataChecksum(\n+          fis, DFSUtilClient.getIoFileBufferSize(conf), srcMeta);\n+    }\n+\n     final byte[] data \u003d new byte[1 \u003c\u003c 16];\n     final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n \n     DataOutputStream metaOut \u003d null;\n     try {\n       File parentFile \u003d dstMeta.getParentFile();\n       if (parentFile !\u003d null) {\n         if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n           throw new IOException(\"Destination \u0027\" + parentFile\n               + \"\u0027 directory cannot be created\");\n         }\n       }\n       metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n           new FileOutputStream(dstMeta), smallBufferSize));\n       BlockMetadataHeader.writeHeader(metaOut, checksum);\n \n       int offset \u003d 0;\n       try (InputStream dataIn \u003d srcReplica.getDataInputStream(0)) {\n \n         for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n           if (n \u003e 0) {\n             n +\u003d offset;\n             offset \u003d n % checksum.getBytesPerChecksum();\n             final int length \u003d n - offset;\n \n             if (length \u003e 0) {\n               checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n               metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n \n               System.arraycopy(data, length, data, 0, offset);\n             }\n           }\n         }\n       }\n \n       // calculate and write the last crc\n       checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n       metaOut.write(crcs, 0, 4);\n     } finally {\n       IOUtils.cleanup(null, metaOut);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void computeChecksum(ReplicaInfo srcReplica, File dstMeta,\n      int smallBufferSize, final Configuration conf)\n      throws IOException {\n    final File srcMeta \u003d new File(srcReplica.getMetadataURI());\n\n    DataChecksum checksum;\n    try (FileInputStream fis \u003d\n             srcReplica.getFileIoProvider().getFileInputStream(\n                 srcReplica.getVolume(), srcMeta)) {\n      checksum \u003d BlockMetadataHeader.readDataChecksum(\n          fis, DFSUtilClient.getIoFileBufferSize(conf), srcMeta);\n    }\n\n    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n\n    DataOutputStream metaOut \u003d null;\n    try {\n      File parentFile \u003d dstMeta.getParentFile();\n      if (parentFile !\u003d null) {\n        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n          throw new IOException(\"Destination \u0027\" + parentFile\n              + \"\u0027 directory cannot be created\");\n        }\n      }\n      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n          new FileOutputStream(dstMeta), smallBufferSize));\n      BlockMetadataHeader.writeHeader(metaOut, checksum);\n\n      int offset \u003d 0;\n      try (InputStream dataIn \u003d srcReplica.getDataInputStream(0)) {\n\n        for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n          if (n \u003e 0) {\n            n +\u003d offset;\n            offset \u003d n % checksum.getBytesPerChecksum();\n            final int length \u003d n - offset;\n\n            if (length \u003e 0) {\n              checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n              metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n\n              System.arraycopy(data, length, data, 0, offset);\n            }\n          }\n        }\n      }\n\n      // calculate and write the last crc\n      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n      metaOut.write(crcs, 0, 4);\n    } finally {\n      IOUtils.cleanup(null, metaOut);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "c5573e6a7599da17cad733cd274e7a9b75b22bb0": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-11009. Add a tool to reconstruct block meta file from CLI.\n",
      "commitDate": "18/10/16 10:42 PM",
      "commitName": "c5573e6a7599da17cad733cd274e7a9b75b22bb0",
      "commitAuthor": "Xiao Chen",
      "commitDateOld": "10/10/16 3:30 PM",
      "commitNameOld": "96b12662ea76e3ded4ef13944fc8df206cfb4613",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 8.3,
      "commitsBetweenForRepo": 62,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,48 @@\n-  private static void computeChecksum(ReplicaInfo srcReplica, File dstMeta,\n+  static void computeChecksum(ReplicaInfo srcReplica, File dstMeta,\n       int smallBufferSize, final Configuration conf)\n       throws IOException {\n     File srcMeta \u003d new File(srcReplica.getMetadataURI());\n     final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta,\n         DFSUtilClient.getIoFileBufferSize(conf));\n     final byte[] data \u003d new byte[1 \u003c\u003c 16];\n     final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n \n     DataOutputStream metaOut \u003d null;\n     try {\n       File parentFile \u003d dstMeta.getParentFile();\n       if (parentFile !\u003d null) {\n         if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n           throw new IOException(\"Destination \u0027\" + parentFile\n               + \"\u0027 directory cannot be created\");\n         }\n       }\n       metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n           new FileOutputStream(dstMeta), smallBufferSize));\n       BlockMetadataHeader.writeHeader(metaOut, checksum);\n \n       int offset \u003d 0;\n       try (InputStream dataIn \u003d srcReplica.getDataInputStream(0)) {\n \n         for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n           if (n \u003e 0) {\n             n +\u003d offset;\n             offset \u003d n % checksum.getBytesPerChecksum();\n             final int length \u003d n - offset;\n \n             if (length \u003e 0) {\n               checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n               metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n \n               System.arraycopy(data, length, data, 0, offset);\n             }\n           }\n         }\n       }\n \n       // calculate and write the last crc\n       checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n       metaOut.write(crcs, 0, 4);\n     } finally {\n       IOUtils.cleanup(null, metaOut);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static void computeChecksum(ReplicaInfo srcReplica, File dstMeta,\n      int smallBufferSize, final Configuration conf)\n      throws IOException {\n    File srcMeta \u003d new File(srcReplica.getMetadataURI());\n    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta,\n        DFSUtilClient.getIoFileBufferSize(conf));\n    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n\n    DataOutputStream metaOut \u003d null;\n    try {\n      File parentFile \u003d dstMeta.getParentFile();\n      if (parentFile !\u003d null) {\n        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n          throw new IOException(\"Destination \u0027\" + parentFile\n              + \"\u0027 directory cannot be created\");\n        }\n      }\n      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n          new FileOutputStream(dstMeta), smallBufferSize));\n      BlockMetadataHeader.writeHeader(metaOut, checksum);\n\n      int offset \u003d 0;\n      try (InputStream dataIn \u003d srcReplica.getDataInputStream(0)) {\n\n        for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n          if (n \u003e 0) {\n            n +\u003d offset;\n            offset \u003d n % checksum.getBytesPerChecksum();\n            final int length \u003d n - offset;\n\n            if (length \u003e 0) {\n              checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n              metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n\n              System.arraycopy(data, length, data, 0, offset);\n            }\n          }\n        }\n      }\n\n      // calculate and write the last crc\n      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n      metaOut.write(crcs, 0, 4);\n    } finally {\n      IOUtils.cleanup(null, metaOut);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {
        "oldValue": "[private, static]",
        "newValue": "[static]"
      }
    },
    "86c9862bec0248d671e657aa56094a2919b8ac14": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "13/09/16 12:54 PM",
      "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
      "commitAuthor": "Lei Xu",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
          "commitDate": "13/09/16 12:54 PM",
          "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "10/09/16 6:22 PM",
          "commitNameOld": "a99bf26a0899bcc4307c3a242c8414eaef555aa7",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 2.77,
          "commitsBetweenForRepo": 15,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,48 @@\n-  private static void computeChecksum(File srcMeta, File dstMeta,\n-      File blockFile, int smallBufferSize, final Configuration conf)\n+  private static void computeChecksum(ReplicaInfo srcReplica, File dstMeta,\n+      int smallBufferSize, final Configuration conf)\n       throws IOException {\n+    File srcMeta \u003d new File(srcReplica.getMetadataURI());\n     final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta,\n         DFSUtilClient.getIoFileBufferSize(conf));\n     final byte[] data \u003d new byte[1 \u003c\u003c 16];\n     final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n \n     DataOutputStream metaOut \u003d null;\n     try {\n       File parentFile \u003d dstMeta.getParentFile();\n       if (parentFile !\u003d null) {\n         if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n           throw new IOException(\"Destination \u0027\" + parentFile\n               + \"\u0027 directory cannot be created\");\n         }\n       }\n       metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n           new FileOutputStream(dstMeta), smallBufferSize));\n       BlockMetadataHeader.writeHeader(metaOut, checksum);\n \n       int offset \u003d 0;\n-      try (InputStream dataIn \u003d isNativeIOAvailable ?\n-          NativeIO.getShareDeleteFileInputStream(blockFile) :\n-          new FileInputStream(blockFile)) {\n+      try (InputStream dataIn \u003d srcReplica.getDataInputStream(0)) {\n \n         for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n           if (n \u003e 0) {\n             n +\u003d offset;\n             offset \u003d n % checksum.getBytesPerChecksum();\n             final int length \u003d n - offset;\n \n             if (length \u003e 0) {\n               checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n               metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n \n               System.arraycopy(data, length, data, 0, offset);\n             }\n           }\n         }\n       }\n \n       // calculate and write the last crc\n       checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n       metaOut.write(crcs, 0, 4);\n     } finally {\n-      IOUtils.cleanup(LOG, metaOut);\n+      IOUtils.cleanup(null, metaOut);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void computeChecksum(ReplicaInfo srcReplica, File dstMeta,\n      int smallBufferSize, final Configuration conf)\n      throws IOException {\n    File srcMeta \u003d new File(srcReplica.getMetadataURI());\n    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta,\n        DFSUtilClient.getIoFileBufferSize(conf));\n    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n\n    DataOutputStream metaOut \u003d null;\n    try {\n      File parentFile \u003d dstMeta.getParentFile();\n      if (parentFile !\u003d null) {\n        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n          throw new IOException(\"Destination \u0027\" + parentFile\n              + \"\u0027 directory cannot be created\");\n        }\n      }\n      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n          new FileOutputStream(dstMeta), smallBufferSize));\n      BlockMetadataHeader.writeHeader(metaOut, checksum);\n\n      int offset \u003d 0;\n      try (InputStream dataIn \u003d srcReplica.getDataInputStream(0)) {\n\n        for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n          if (n \u003e 0) {\n            n +\u003d offset;\n            offset \u003d n % checksum.getBytesPerChecksum();\n            final int length \u003d n - offset;\n\n            if (length \u003e 0) {\n              checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n              metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n\n              System.arraycopy(data, length, data, 0, offset);\n            }\n          }\n        }\n      }\n\n      // calculate and write the last crc\n      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n      metaOut.write(crcs, 0, 4);\n    } finally {\n      IOUtils.cleanup(null, metaOut);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[srcMeta-File, dstMeta-File, blockFile-File, smallBufferSize-int, conf-Configuration(modifiers-final)]",
            "newValue": "[srcReplica-ReplicaInfo, dstMeta-File, smallBufferSize-int, conf-Configuration(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
          "commitDate": "13/09/16 12:54 PM",
          "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "10/09/16 6:22 PM",
          "commitNameOld": "a99bf26a0899bcc4307c3a242c8414eaef555aa7",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 2.77,
          "commitsBetweenForRepo": 15,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,49 +1,48 @@\n-  private static void computeChecksum(File srcMeta, File dstMeta,\n-      File blockFile, int smallBufferSize, final Configuration conf)\n+  private static void computeChecksum(ReplicaInfo srcReplica, File dstMeta,\n+      int smallBufferSize, final Configuration conf)\n       throws IOException {\n+    File srcMeta \u003d new File(srcReplica.getMetadataURI());\n     final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta,\n         DFSUtilClient.getIoFileBufferSize(conf));\n     final byte[] data \u003d new byte[1 \u003c\u003c 16];\n     final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n \n     DataOutputStream metaOut \u003d null;\n     try {\n       File parentFile \u003d dstMeta.getParentFile();\n       if (parentFile !\u003d null) {\n         if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n           throw new IOException(\"Destination \u0027\" + parentFile\n               + \"\u0027 directory cannot be created\");\n         }\n       }\n       metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n           new FileOutputStream(dstMeta), smallBufferSize));\n       BlockMetadataHeader.writeHeader(metaOut, checksum);\n \n       int offset \u003d 0;\n-      try (InputStream dataIn \u003d isNativeIOAvailable ?\n-          NativeIO.getShareDeleteFileInputStream(blockFile) :\n-          new FileInputStream(blockFile)) {\n+      try (InputStream dataIn \u003d srcReplica.getDataInputStream(0)) {\n \n         for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n           if (n \u003e 0) {\n             n +\u003d offset;\n             offset \u003d n % checksum.getBytesPerChecksum();\n             final int length \u003d n - offset;\n \n             if (length \u003e 0) {\n               checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n               metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n \n               System.arraycopy(data, length, data, 0, offset);\n             }\n           }\n         }\n       }\n \n       // calculate and write the last crc\n       checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n       metaOut.write(crcs, 0, 4);\n     } finally {\n-      IOUtils.cleanup(LOG, metaOut);\n+      IOUtils.cleanup(null, metaOut);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void computeChecksum(ReplicaInfo srcReplica, File dstMeta,\n      int smallBufferSize, final Configuration conf)\n      throws IOException {\n    File srcMeta \u003d new File(srcReplica.getMetadataURI());\n    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta,\n        DFSUtilClient.getIoFileBufferSize(conf));\n    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n\n    DataOutputStream metaOut \u003d null;\n    try {\n      File parentFile \u003d dstMeta.getParentFile();\n      if (parentFile !\u003d null) {\n        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n          throw new IOException(\"Destination \u0027\" + parentFile\n              + \"\u0027 directory cannot be created\");\n        }\n      }\n      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n          new FileOutputStream(dstMeta), smallBufferSize));\n      BlockMetadataHeader.writeHeader(metaOut, checksum);\n\n      int offset \u003d 0;\n      try (InputStream dataIn \u003d srcReplica.getDataInputStream(0)) {\n\n        for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n          if (n \u003e 0) {\n            n +\u003d offset;\n            offset \u003d n % checksum.getBytesPerChecksum();\n            final int length \u003d n - offset;\n\n            if (length \u003e 0) {\n              checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n              metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n\n              System.arraycopy(data, length, data, 0, offset);\n            }\n          }\n        }\n      }\n\n      // calculate and write the last crc\n      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n      metaOut.write(crcs, 0, 4);\n    } finally {\n      IOUtils.cleanup(null, metaOut);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "18/09/15 4:07 AM",
      "commitNameOld": "92c1af1646b1d91a2ab7821e4f7d450e3b6e10bb",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 8.29,
      "commitsBetweenForRepo": 66,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,49 @@\n   private static void computeChecksum(File srcMeta, File dstMeta,\n       File blockFile, int smallBufferSize, final Configuration conf)\n       throws IOException {\n     final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta,\n-        DFSUtil.getIoFileBufferSize(conf));\n+        DFSUtilClient.getIoFileBufferSize(conf));\n     final byte[] data \u003d new byte[1 \u003c\u003c 16];\n     final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n \n     DataOutputStream metaOut \u003d null;\n     try {\n       File parentFile \u003d dstMeta.getParentFile();\n       if (parentFile !\u003d null) {\n         if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n           throw new IOException(\"Destination \u0027\" + parentFile\n               + \"\u0027 directory cannot be created\");\n         }\n       }\n       metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n           new FileOutputStream(dstMeta), smallBufferSize));\n       BlockMetadataHeader.writeHeader(metaOut, checksum);\n \n       int offset \u003d 0;\n       try (InputStream dataIn \u003d isNativeIOAvailable ?\n           NativeIO.getShareDeleteFileInputStream(blockFile) :\n           new FileInputStream(blockFile)) {\n \n         for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n           if (n \u003e 0) {\n             n +\u003d offset;\n             offset \u003d n % checksum.getBytesPerChecksum();\n             final int length \u003d n - offset;\n \n             if (length \u003e 0) {\n               checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n               metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n \n               System.arraycopy(data, length, data, 0, offset);\n             }\n           }\n         }\n       }\n \n       // calculate and write the last crc\n       checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n       metaOut.write(crcs, 0, 4);\n     } finally {\n       IOUtils.cleanup(LOG, metaOut);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static void computeChecksum(File srcMeta, File dstMeta,\n      File blockFile, int smallBufferSize, final Configuration conf)\n      throws IOException {\n    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta,\n        DFSUtilClient.getIoFileBufferSize(conf));\n    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n\n    DataOutputStream metaOut \u003d null;\n    try {\n      File parentFile \u003d dstMeta.getParentFile();\n      if (parentFile !\u003d null) {\n        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n          throw new IOException(\"Destination \u0027\" + parentFile\n              + \"\u0027 directory cannot be created\");\n        }\n      }\n      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n          new FileOutputStream(dstMeta), smallBufferSize));\n      BlockMetadataHeader.writeHeader(metaOut, checksum);\n\n      int offset \u003d 0;\n      try (InputStream dataIn \u003d isNativeIOAvailable ?\n          NativeIO.getShareDeleteFileInputStream(blockFile) :\n          new FileInputStream(blockFile)) {\n\n        for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n          if (n \u003e 0) {\n            n +\u003d offset;\n            offset \u003d n % checksum.getBytesPerChecksum();\n            final int length \u003d n - offset;\n\n            if (length \u003e 0) {\n              checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n              metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n\n              System.arraycopy(data, length, data, 0, offset);\n            }\n          }\n        }\n      }\n\n      // calculate and write the last crc\n      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n      metaOut.write(crcs, 0, 4);\n    } finally {\n      IOUtils.cleanup(LOG, metaOut);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "c992bcf9c136d3df686655a80e636bb7bb0664da": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8951. Move the shortcircuit package to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/08/15 2:02 PM",
      "commitName": "c992bcf9c136d3df686655a80e636bb7bb0664da",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8951. Move the shortcircuit package to hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "26/08/15 2:02 PM",
          "commitName": "c992bcf9c136d3df686655a80e636bb7bb0664da",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/08/15 5:40 PM",
          "commitNameOld": "eee4d716b48074825e1afcd9c74038a393ddeb69",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 8.85,
          "commitsBetweenForRepo": 45,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,49 @@\n   private static void computeChecksum(File srcMeta, File dstMeta,\n-      File blockFile, int smallBufferSize)\n+      File blockFile, int smallBufferSize, final Configuration conf)\n       throws IOException {\n-    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta);\n+    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta,\n+        DFSUtil.getIoFileBufferSize(conf));\n     final byte[] data \u003d new byte[1 \u003c\u003c 16];\n     final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n \n     DataOutputStream metaOut \u003d null;\n     try {\n       File parentFile \u003d dstMeta.getParentFile();\n       if (parentFile !\u003d null) {\n         if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n           throw new IOException(\"Destination \u0027\" + parentFile\n               + \"\u0027 directory cannot be created\");\n         }\n       }\n       metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n           new FileOutputStream(dstMeta), smallBufferSize));\n       BlockMetadataHeader.writeHeader(metaOut, checksum);\n \n       int offset \u003d 0;\n       try (InputStream dataIn \u003d isNativeIOAvailable ?\n           NativeIO.getShareDeleteFileInputStream(blockFile) :\n           new FileInputStream(blockFile)) {\n \n         for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n           if (n \u003e 0) {\n             n +\u003d offset;\n             offset \u003d n % checksum.getBytesPerChecksum();\n             final int length \u003d n - offset;\n \n             if (length \u003e 0) {\n               checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n               metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n \n               System.arraycopy(data, length, data, 0, offset);\n             }\n           }\n         }\n       }\n \n       // calculate and write the last crc\n       checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n       metaOut.write(crcs, 0, 4);\n     } finally {\n       IOUtils.cleanup(LOG, metaOut);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void computeChecksum(File srcMeta, File dstMeta,\n      File blockFile, int smallBufferSize, final Configuration conf)\n      throws IOException {\n    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta,\n        DFSUtil.getIoFileBufferSize(conf));\n    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n\n    DataOutputStream metaOut \u003d null;\n    try {\n      File parentFile \u003d dstMeta.getParentFile();\n      if (parentFile !\u003d null) {\n        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n          throw new IOException(\"Destination \u0027\" + parentFile\n              + \"\u0027 directory cannot be created\");\n        }\n      }\n      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n          new FileOutputStream(dstMeta), smallBufferSize));\n      BlockMetadataHeader.writeHeader(metaOut, checksum);\n\n      int offset \u003d 0;\n      try (InputStream dataIn \u003d isNativeIOAvailable ?\n          NativeIO.getShareDeleteFileInputStream(blockFile) :\n          new FileInputStream(blockFile)) {\n\n        for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n          if (n \u003e 0) {\n            n +\u003d offset;\n            offset \u003d n % checksum.getBytesPerChecksum();\n            final int length \u003d n - offset;\n\n            if (length \u003e 0) {\n              checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n              metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n\n              System.arraycopy(data, length, data, 0, offset);\n            }\n          }\n        }\n      }\n\n      // calculate and write the last crc\n      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n      metaOut.write(crcs, 0, 4);\n    } finally {\n      IOUtils.cleanup(LOG, metaOut);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[srcMeta-File, dstMeta-File, blockFile-File, smallBufferSize-int]",
            "newValue": "[srcMeta-File, dstMeta-File, blockFile-File, smallBufferSize-int, conf-Configuration(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8951. Move the shortcircuit package to hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "26/08/15 2:02 PM",
          "commitName": "c992bcf9c136d3df686655a80e636bb7bb0664da",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "17/08/15 5:40 PM",
          "commitNameOld": "eee4d716b48074825e1afcd9c74038a393ddeb69",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 8.85,
          "commitsBetweenForRepo": 45,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,48 +1,49 @@\n   private static void computeChecksum(File srcMeta, File dstMeta,\n-      File blockFile, int smallBufferSize)\n+      File blockFile, int smallBufferSize, final Configuration conf)\n       throws IOException {\n-    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta);\n+    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta,\n+        DFSUtil.getIoFileBufferSize(conf));\n     final byte[] data \u003d new byte[1 \u003c\u003c 16];\n     final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n \n     DataOutputStream metaOut \u003d null;\n     try {\n       File parentFile \u003d dstMeta.getParentFile();\n       if (parentFile !\u003d null) {\n         if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n           throw new IOException(\"Destination \u0027\" + parentFile\n               + \"\u0027 directory cannot be created\");\n         }\n       }\n       metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n           new FileOutputStream(dstMeta), smallBufferSize));\n       BlockMetadataHeader.writeHeader(metaOut, checksum);\n \n       int offset \u003d 0;\n       try (InputStream dataIn \u003d isNativeIOAvailable ?\n           NativeIO.getShareDeleteFileInputStream(blockFile) :\n           new FileInputStream(blockFile)) {\n \n         for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n           if (n \u003e 0) {\n             n +\u003d offset;\n             offset \u003d n % checksum.getBytesPerChecksum();\n             final int length \u003d n - offset;\n \n             if (length \u003e 0) {\n               checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n               metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n \n               System.arraycopy(data, length, data, 0, offset);\n             }\n           }\n         }\n       }\n \n       // calculate and write the last crc\n       checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n       metaOut.write(crcs, 0, 4);\n     } finally {\n       IOUtils.cleanup(LOG, metaOut);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void computeChecksum(File srcMeta, File dstMeta,\n      File blockFile, int smallBufferSize, final Configuration conf)\n      throws IOException {\n    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta,\n        DFSUtil.getIoFileBufferSize(conf));\n    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n\n    DataOutputStream metaOut \u003d null;\n    try {\n      File parentFile \u003d dstMeta.getParentFile();\n      if (parentFile !\u003d null) {\n        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n          throw new IOException(\"Destination \u0027\" + parentFile\n              + \"\u0027 directory cannot be created\");\n        }\n      }\n      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n          new FileOutputStream(dstMeta), smallBufferSize));\n      BlockMetadataHeader.writeHeader(metaOut, checksum);\n\n      int offset \u003d 0;\n      try (InputStream dataIn \u003d isNativeIOAvailable ?\n          NativeIO.getShareDeleteFileInputStream(blockFile) :\n          new FileInputStream(blockFile)) {\n\n        for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n          if (n \u003e 0) {\n            n +\u003d offset;\n            offset \u003d n % checksum.getBytesPerChecksum();\n            final int length \u003d n - offset;\n\n            if (length \u003e 0) {\n              checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n              metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n\n              System.arraycopy(data, length, data, 0, offset);\n            }\n          }\n        }\n      }\n\n      // calculate and write the last crc\n      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n      metaOut.write(crcs, 0, 4);\n    } finally {\n      IOUtils.cleanup(LOG, metaOut);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "4da8490b512a33a255ed27309860859388d7c168": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
      "commitDate": "05/05/15 3:41 PM",
      "commitName": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
          "commitDate": "05/05/15 3:41 PM",
          "commitName": "4da8490b512a33a255ed27309860859388d7c168",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "05/05/15 11:08 AM",
          "commitNameOld": "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.19,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,48 @@\n-  private static void computeChecksum(File srcMeta, File dstMeta, File blockFile)\n+  private static void computeChecksum(File srcMeta, File dstMeta,\n+      File blockFile, int smallBufferSize)\n       throws IOException {\n     final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta);\n     final byte[] data \u003d new byte[1 \u003c\u003c 16];\n     final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n \n     DataOutputStream metaOut \u003d null;\n     try {\n       File parentFile \u003d dstMeta.getParentFile();\n       if (parentFile !\u003d null) {\n         if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n           throw new IOException(\"Destination \u0027\" + parentFile\n               + \"\u0027 directory cannot be created\");\n         }\n       }\n       metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n-          new FileOutputStream(dstMeta), HdfsServerConstants.SMALL_BUFFER_SIZE));\n+          new FileOutputStream(dstMeta), smallBufferSize));\n       BlockMetadataHeader.writeHeader(metaOut, checksum);\n \n       int offset \u003d 0;\n       try (InputStream dataIn \u003d isNativeIOAvailable ?\n           NativeIO.getShareDeleteFileInputStream(blockFile) :\n           new FileInputStream(blockFile)) {\n \n         for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n           if (n \u003e 0) {\n             n +\u003d offset;\n             offset \u003d n % checksum.getBytesPerChecksum();\n             final int length \u003d n - offset;\n \n             if (length \u003e 0) {\n               checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n               metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n \n               System.arraycopy(data, length, data, 0, offset);\n             }\n           }\n         }\n       }\n \n       // calculate and write the last crc\n       checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n       metaOut.write(crcs, 0, 4);\n     } finally {\n       IOUtils.cleanup(LOG, metaOut);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void computeChecksum(File srcMeta, File dstMeta,\n      File blockFile, int smallBufferSize)\n      throws IOException {\n    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta);\n    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n\n    DataOutputStream metaOut \u003d null;\n    try {\n      File parentFile \u003d dstMeta.getParentFile();\n      if (parentFile !\u003d null) {\n        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n          throw new IOException(\"Destination \u0027\" + parentFile\n              + \"\u0027 directory cannot be created\");\n        }\n      }\n      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n          new FileOutputStream(dstMeta), smallBufferSize));\n      BlockMetadataHeader.writeHeader(metaOut, checksum);\n\n      int offset \u003d 0;\n      try (InputStream dataIn \u003d isNativeIOAvailable ?\n          NativeIO.getShareDeleteFileInputStream(blockFile) :\n          new FileInputStream(blockFile)) {\n\n        for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n          if (n \u003e 0) {\n            n +\u003d offset;\n            offset \u003d n % checksum.getBytesPerChecksum();\n            final int length \u003d n - offset;\n\n            if (length \u003e 0) {\n              checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n              metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n\n              System.arraycopy(data, length, data, 0, offset);\n            }\n          }\n        }\n      }\n\n      // calculate and write the last crc\n      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n      metaOut.write(crcs, 0, 4);\n    } finally {\n      IOUtils.cleanup(LOG, metaOut);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[srcMeta-File, dstMeta-File, blockFile-File]",
            "newValue": "[srcMeta-File, dstMeta-File, blockFile-File, smallBufferSize-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
          "commitDate": "05/05/15 3:41 PM",
          "commitName": "4da8490b512a33a255ed27309860859388d7c168",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "05/05/15 11:08 AM",
          "commitNameOld": "24d3a2d4fdd836ac9a5bc755a7fb9354f7a582b1",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.19,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,47 +1,48 @@\n-  private static void computeChecksum(File srcMeta, File dstMeta, File blockFile)\n+  private static void computeChecksum(File srcMeta, File dstMeta,\n+      File blockFile, int smallBufferSize)\n       throws IOException {\n     final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta);\n     final byte[] data \u003d new byte[1 \u003c\u003c 16];\n     final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n \n     DataOutputStream metaOut \u003d null;\n     try {\n       File parentFile \u003d dstMeta.getParentFile();\n       if (parentFile !\u003d null) {\n         if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n           throw new IOException(\"Destination \u0027\" + parentFile\n               + \"\u0027 directory cannot be created\");\n         }\n       }\n       metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n-          new FileOutputStream(dstMeta), HdfsServerConstants.SMALL_BUFFER_SIZE));\n+          new FileOutputStream(dstMeta), smallBufferSize));\n       BlockMetadataHeader.writeHeader(metaOut, checksum);\n \n       int offset \u003d 0;\n       try (InputStream dataIn \u003d isNativeIOAvailable ?\n           NativeIO.getShareDeleteFileInputStream(blockFile) :\n           new FileInputStream(blockFile)) {\n \n         for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n           if (n \u003e 0) {\n             n +\u003d offset;\n             offset \u003d n % checksum.getBytesPerChecksum();\n             final int length \u003d n - offset;\n \n             if (length \u003e 0) {\n               checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n               metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n \n               System.arraycopy(data, length, data, 0, offset);\n             }\n           }\n         }\n       }\n \n       // calculate and write the last crc\n       checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n       metaOut.write(crcs, 0, 4);\n     } finally {\n       IOUtils.cleanup(LOG, metaOut);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static void computeChecksum(File srcMeta, File dstMeta,\n      File blockFile, int smallBufferSize)\n      throws IOException {\n    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta);\n    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n\n    DataOutputStream metaOut \u003d null;\n    try {\n      File parentFile \u003d dstMeta.getParentFile();\n      if (parentFile !\u003d null) {\n        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n          throw new IOException(\"Destination \u0027\" + parentFile\n              + \"\u0027 directory cannot be created\");\n        }\n      }\n      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n          new FileOutputStream(dstMeta), smallBufferSize));\n      BlockMetadataHeader.writeHeader(metaOut, checksum);\n\n      int offset \u003d 0;\n      try (InputStream dataIn \u003d isNativeIOAvailable ?\n          NativeIO.getShareDeleteFileInputStream(blockFile) :\n          new FileInputStream(blockFile)) {\n\n        for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n          if (n \u003e 0) {\n            n +\u003d offset;\n            offset \u003d n % checksum.getBytesPerChecksum();\n            final int length \u003d n - offset;\n\n            if (length \u003e 0) {\n              checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n              metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n\n              System.arraycopy(data, length, data, 0, offset);\n            }\n          }\n        }\n      }\n\n      // calculate and write the last crc\n      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n      metaOut.write(crcs, 0, 4);\n    } finally {\n      IOUtils.cleanup(LOG, metaOut);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "27/04/15 4:48 PM",
      "commitNameOld": "feb68cb5470dc3e6c16b6bc1549141613e360601",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 4.72,
      "commitsBetweenForRepo": 40,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,47 @@\n   private static void computeChecksum(File srcMeta, File dstMeta, File blockFile)\n       throws IOException {\n     final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta);\n     final byte[] data \u003d new byte[1 \u003c\u003c 16];\n     final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n \n     DataOutputStream metaOut \u003d null;\n     try {\n       File parentFile \u003d dstMeta.getParentFile();\n       if (parentFile !\u003d null) {\n         if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n           throw new IOException(\"Destination \u0027\" + parentFile\n               + \"\u0027 directory cannot be created\");\n         }\n       }\n       metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n-          new FileOutputStream(dstMeta), HdfsConstants.SMALL_BUFFER_SIZE));\n+          new FileOutputStream(dstMeta), HdfsServerConstants.SMALL_BUFFER_SIZE));\n       BlockMetadataHeader.writeHeader(metaOut, checksum);\n \n       int offset \u003d 0;\n       try (InputStream dataIn \u003d isNativeIOAvailable ?\n           NativeIO.getShareDeleteFileInputStream(blockFile) :\n           new FileInputStream(blockFile)) {\n \n         for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n           if (n \u003e 0) {\n             n +\u003d offset;\n             offset \u003d n % checksum.getBytesPerChecksum();\n             final int length \u003d n - offset;\n \n             if (length \u003e 0) {\n               checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n               metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n \n               System.arraycopy(data, length, data, 0, offset);\n             }\n           }\n         }\n       }\n \n       // calculate and write the last crc\n       checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n       metaOut.write(crcs, 0, 4);\n     } finally {\n       IOUtils.cleanup(LOG, metaOut);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static void computeChecksum(File srcMeta, File dstMeta, File blockFile)\n      throws IOException {\n    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta);\n    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n\n    DataOutputStream metaOut \u003d null;\n    try {\n      File parentFile \u003d dstMeta.getParentFile();\n      if (parentFile !\u003d null) {\n        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n          throw new IOException(\"Destination \u0027\" + parentFile\n              + \"\u0027 directory cannot be created\");\n        }\n      }\n      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n          new FileOutputStream(dstMeta), HdfsServerConstants.SMALL_BUFFER_SIZE));\n      BlockMetadataHeader.writeHeader(metaOut, checksum);\n\n      int offset \u003d 0;\n      try (InputStream dataIn \u003d isNativeIOAvailable ?\n          NativeIO.getShareDeleteFileInputStream(blockFile) :\n          new FileInputStream(blockFile)) {\n\n        for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n          if (n \u003e 0) {\n            n +\u003d offset;\n            offset \u003d n % checksum.getBytesPerChecksum();\n            final int length \u003d n - offset;\n\n            if (length \u003e 0) {\n              checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n              metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n\n              System.arraycopy(data, length, data, 0, offset);\n            }\n          }\n        }\n      }\n\n      // calculate and write the last crc\n      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n      metaOut.write(crcs, 0, 4);\n    } finally {\n      IOUtils.cleanup(LOG, metaOut);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7515. Fix new findbugs warnings in hadoop-hdfs. Contributed by Haohui Mai.\n",
      "commitDate": "11/12/14 12:36 PM",
      "commitName": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/11/14 9:57 AM",
      "commitNameOld": "058af60c56207907f2bedf76df4284e86d923e0c",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 15.11,
      "commitsBetweenForRepo": 103,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,47 @@\n   private static void computeChecksum(File srcMeta, File dstMeta, File blockFile)\n       throws IOException {\n     final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta);\n     final byte[] data \u003d new byte[1 \u003c\u003c 16];\n     final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n \n     DataOutputStream metaOut \u003d null;\n-    InputStream dataIn \u003d null;\n     try {\n       File parentFile \u003d dstMeta.getParentFile();\n       if (parentFile !\u003d null) {\n         if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n           throw new IOException(\"Destination \u0027\" + parentFile\n               + \"\u0027 directory cannot be created\");\n         }\n       }\n       metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n           new FileOutputStream(dstMeta), HdfsConstants.SMALL_BUFFER_SIZE));\n       BlockMetadataHeader.writeHeader(metaOut, checksum);\n \n-      dataIn \u003d isNativeIOAvailable ?\n-          NativeIO.getShareDeleteFileInputStream(blockFile) :\n-          new FileInputStream(blockFile);\n-\n       int offset \u003d 0;\n-      for(int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n-        if (n \u003e 0) {\n-          n +\u003d offset;\n-          offset \u003d n % checksum.getBytesPerChecksum();\n-          final int length \u003d n - offset;\n+      try (InputStream dataIn \u003d isNativeIOAvailable ?\n+          NativeIO.getShareDeleteFileInputStream(blockFile) :\n+          new FileInputStream(blockFile)) {\n \n-          if (length \u003e 0) {\n-            checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n-            metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n+        for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n+          if (n \u003e 0) {\n+            n +\u003d offset;\n+            offset \u003d n % checksum.getBytesPerChecksum();\n+            final int length \u003d n - offset;\n \n-            System.arraycopy(data, length, data, 0, offset);\n+            if (length \u003e 0) {\n+              checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n+              metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n+\n+              System.arraycopy(data, length, data, 0, offset);\n+            }\n           }\n         }\n       }\n \n       // calculate and write the last crc\n       checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n       metaOut.write(crcs, 0, 4);\n     } finally {\n-      IOUtils.cleanup(LOG, dataIn, metaOut);\n+      IOUtils.cleanup(LOG, metaOut);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static void computeChecksum(File srcMeta, File dstMeta, File blockFile)\n      throws IOException {\n    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta);\n    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n\n    DataOutputStream metaOut \u003d null;\n    try {\n      File parentFile \u003d dstMeta.getParentFile();\n      if (parentFile !\u003d null) {\n        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n          throw new IOException(\"Destination \u0027\" + parentFile\n              + \"\u0027 directory cannot be created\");\n        }\n      }\n      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n          new FileOutputStream(dstMeta), HdfsConstants.SMALL_BUFFER_SIZE));\n      BlockMetadataHeader.writeHeader(metaOut, checksum);\n\n      int offset \u003d 0;\n      try (InputStream dataIn \u003d isNativeIOAvailable ?\n          NativeIO.getShareDeleteFileInputStream(blockFile) :\n          new FileInputStream(blockFile)) {\n\n        for (int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n          if (n \u003e 0) {\n            n +\u003d offset;\n            offset \u003d n % checksum.getBytesPerChecksum();\n            final int length \u003d n - offset;\n\n            if (length \u003e 0) {\n              checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n              metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n\n              System.arraycopy(data, length, data, 0, offset);\n            }\n          }\n        }\n      }\n\n      // calculate and write the last crc\n      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n      metaOut.write(crcs, 0, 4);\n    } finally {\n      IOUtils.cleanup(LOG, metaOut);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "463aec11718e47d4aabb86a7a539cb973460aae6": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-6934. Move checksum computation off the hot path when writing to RAM disk. Contributed by Chris Nauroth.\n",
      "commitDate": "27/10/14 9:38 AM",
      "commitName": "463aec11718e47d4aabb86a7a539cb973460aae6",
      "commitAuthor": "cnauroth",
      "diff": "@@ -0,0 +1,47 @@\n+  private static void computeChecksum(File srcMeta, File dstMeta, File blockFile)\n+      throws IOException {\n+    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta);\n+    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n+    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n+\n+    DataOutputStream metaOut \u003d null;\n+    InputStream dataIn \u003d null;\n+    try {\n+      File parentFile \u003d dstMeta.getParentFile();\n+      if (parentFile !\u003d null) {\n+        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n+          throw new IOException(\"Destination \u0027\" + parentFile\n+              + \"\u0027 directory cannot be created\");\n+        }\n+      }\n+      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n+          new FileOutputStream(dstMeta), HdfsConstants.SMALL_BUFFER_SIZE));\n+      BlockMetadataHeader.writeHeader(metaOut, checksum);\n+\n+      dataIn \u003d isNativeIOAvailable ?\n+          NativeIO.getShareDeleteFileInputStream(blockFile) :\n+          new FileInputStream(blockFile);\n+\n+      int offset \u003d 0;\n+      for(int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n+        if (n \u003e 0) {\n+          n +\u003d offset;\n+          offset \u003d n % checksum.getBytesPerChecksum();\n+          final int length \u003d n - offset;\n+\n+          if (length \u003e 0) {\n+            checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n+            metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n+\n+            System.arraycopy(data, length, data, 0, offset);\n+          }\n+        }\n+      }\n+\n+      // calculate and write the last crc\n+      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n+      metaOut.write(crcs, 0, 4);\n+    } finally {\n+      IOUtils.cleanup(LOG, dataIn, metaOut);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private static void computeChecksum(File srcMeta, File dstMeta, File blockFile)\n      throws IOException {\n    final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(srcMeta);\n    final byte[] data \u003d new byte[1 \u003c\u003c 16];\n    final byte[] crcs \u003d new byte[checksum.getChecksumSize(data.length)];\n\n    DataOutputStream metaOut \u003d null;\n    InputStream dataIn \u003d null;\n    try {\n      File parentFile \u003d dstMeta.getParentFile();\n      if (parentFile !\u003d null) {\n        if (!parentFile.mkdirs() \u0026\u0026 !parentFile.isDirectory()) {\n          throw new IOException(\"Destination \u0027\" + parentFile\n              + \"\u0027 directory cannot be created\");\n        }\n      }\n      metaOut \u003d new DataOutputStream(new BufferedOutputStream(\n          new FileOutputStream(dstMeta), HdfsConstants.SMALL_BUFFER_SIZE));\n      BlockMetadataHeader.writeHeader(metaOut, checksum);\n\n      dataIn \u003d isNativeIOAvailable ?\n          NativeIO.getShareDeleteFileInputStream(blockFile) :\n          new FileInputStream(blockFile);\n\n      int offset \u003d 0;\n      for(int n; (n \u003d dataIn.read(data, offset, data.length - offset)) !\u003d -1; ) {\n        if (n \u003e 0) {\n          n +\u003d offset;\n          offset \u003d n % checksum.getBytesPerChecksum();\n          final int length \u003d n - offset;\n\n          if (length \u003e 0) {\n            checksum.calculateChunkedSums(data, 0, length, crcs, 0);\n            metaOut.write(crcs, 0, checksum.getChecksumSize(length));\n\n            System.arraycopy(data, length, data, 0, offset);\n          }\n        }\n      }\n\n      // calculate and write the last crc\n      checksum.calculateChunkedSums(data, 0, offset, crcs, 0);\n      metaOut.write(crcs, 0, 4);\n    } finally {\n      IOUtils.cleanup(LOG, dataIn, metaOut);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java"
    }
  }
}