{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockSender.java",
  "functionName": "manageOsCache",
  "functionId": "manageOsCache",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
  "functionStartLine": 843,
  "functionEndLine": 870,
  "numCommitsSeen": 65,
  "timeTaken": 4019,
  "changeHistory": [
    "df983b524ab68ea0c70cee9033bfff2d28052cbf",
    "dcedb72af468128458e597f08d22f5c34b744ae5",
    "aeecfa24f4fb6af289920cbf8830c394e66bd78e",
    "21d10ccc6e463cf250414264c78acb4a6e7c83e3",
    "efea68dc3538de9aafae206d64903506e41fc9e1",
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
    "638801cce16fc1dc3259c541dc30a599faaddda1",
    "c12e994eda0f7e0c34fb0c0ff208789586c7142c",
    "6b0963c53be360b710614b9f44a29c4171af6b83"
  ],
  "changeHistoryShort": {
    "df983b524ab68ea0c70cee9033bfff2d28052cbf": "Ybodychange",
    "dcedb72af468128458e597f08d22f5c34b744ae5": "Ybodychange",
    "aeecfa24f4fb6af289920cbf8830c394e66bd78e": "Ybodychange",
    "21d10ccc6e463cf250414264c78acb4a6e7c83e3": "Ybodychange",
    "efea68dc3538de9aafae206d64903506e41fc9e1": "Ybodychange",
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24": "Ybodychange",
    "638801cce16fc1dc3259c541dc30a599faaddda1": "Ybodychange",
    "c12e994eda0f7e0c34fb0c0ff208789586c7142c": "Ybodychange",
    "6b0963c53be360b710614b9f44a29c4171af6b83": "Yintroduced"
  },
  "changeHistoryDetails": {
    "df983b524ab68ea0c70cee9033bfff2d28052cbf": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10930. Refactor: Wrap Datanode IO related operations. Contributed by Xiaoyu Yao.\n",
      "commitDate": "06/12/16 11:05 AM",
      "commitName": "df983b524ab68ea0c70cee9033bfff2d28052cbf",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "05/12/16 12:44 PM",
      "commitNameOld": "dcedb72af468128458e597f08d22f5c34b744ae5",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 0.93,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,28 @@\n   private void manageOsCache() throws IOException {\n     // We can\u0027t manage the cache for this block if we don\u0027t have a file\n     // descriptor to work with.\n-    if (blockInFd \u003d\u003d null) return;\n+    if (ris.getDataInFd() \u003d\u003d null) {\n+      return;\n+    }\n \n     // Perform readahead if necessary\n     if ((readaheadLength \u003e 0) \u0026\u0026 (datanode.readaheadPool !\u003d null) \u0026\u0026\n           (alwaysReadahead || isLongRead())) {\n       curReadahead \u003d datanode.readaheadPool.readaheadStream(\n-          clientTraceFmt, blockInFd, offset, readaheadLength, Long.MAX_VALUE,\n-          curReadahead);\n+          clientTraceFmt, ris.getDataInFd(), offset, readaheadLength,\n+          Long.MAX_VALUE, curReadahead);\n     }\n \n     // Drop what we\u0027ve just read from cache, since we aren\u0027t\n     // likely to need it again\n     if (dropCacheBehindAllReads ||\n         (dropCacheBehindLargeReads \u0026\u0026 isLongRead())) {\n       long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n       if (offset \u003e\u003d nextCacheDropOffset) {\n         long dropLength \u003d offset - lastCacheDropOffset;\n-        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n-            block.getBlockName(), blockInFd, lastCacheDropOffset,\n+        ris.dropCacheBehindReads(block.getBlockName(), lastCacheDropOffset,\n             dropLength, POSIX_FADV_DONTNEED);\n         lastCacheDropOffset \u003d offset;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void manageOsCache() throws IOException {\n    // We can\u0027t manage the cache for this block if we don\u0027t have a file\n    // descriptor to work with.\n    if (ris.getDataInFd() \u003d\u003d null) {\n      return;\n    }\n\n    // Perform readahead if necessary\n    if ((readaheadLength \u003e 0) \u0026\u0026 (datanode.readaheadPool !\u003d null) \u0026\u0026\n          (alwaysReadahead || isLongRead())) {\n      curReadahead \u003d datanode.readaheadPool.readaheadStream(\n          clientTraceFmt, ris.getDataInFd(), offset, readaheadLength,\n          Long.MAX_VALUE, curReadahead);\n    }\n\n    // Drop what we\u0027ve just read from cache, since we aren\u0027t\n    // likely to need it again\n    if (dropCacheBehindAllReads ||\n        (dropCacheBehindLargeReads \u0026\u0026 isLongRead())) {\n      long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n      if (offset \u003e\u003d nextCacheDropOffset) {\n        long dropLength \u003d offset - lastCacheDropOffset;\n        ris.dropCacheBehindReads(block.getBlockName(), lastCacheDropOffset,\n            dropLength, POSIX_FADV_DONTNEED);\n        lastCacheDropOffset \u003d offset;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "dcedb72af468128458e597f08d22f5c34b744ae5": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HADOOP-10930. Refactor: Wrap Datanode IO related operations. Contributed by Xiaoyu Yao.\"\n\nThis reverts commit aeecfa24f4fb6af289920cbf8830c394e66bd78e.\n",
      "commitDate": "05/12/16 12:44 PM",
      "commitName": "dcedb72af468128458e597f08d22f5c34b744ae5",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "29/11/16 8:52 PM",
      "commitNameOld": "aeecfa24f4fb6af289920cbf8830c394e66bd78e",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 5.66,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,27 @@\n   private void manageOsCache() throws IOException {\n     // We can\u0027t manage the cache for this block if we don\u0027t have a file\n     // descriptor to work with.\n-    if (ris.getDataInFd() \u003d\u003d null) {\n-      return;\n-    }\n+    if (blockInFd \u003d\u003d null) return;\n \n     // Perform readahead if necessary\n     if ((readaheadLength \u003e 0) \u0026\u0026 (datanode.readaheadPool !\u003d null) \u0026\u0026\n           (alwaysReadahead || isLongRead())) {\n       curReadahead \u003d datanode.readaheadPool.readaheadStream(\n-          clientTraceFmt, ris.getDataInFd(), offset, readaheadLength,\n-          Long.MAX_VALUE, curReadahead);\n+          clientTraceFmt, blockInFd, offset, readaheadLength, Long.MAX_VALUE,\n+          curReadahead);\n     }\n \n     // Drop what we\u0027ve just read from cache, since we aren\u0027t\n     // likely to need it again\n     if (dropCacheBehindAllReads ||\n         (dropCacheBehindLargeReads \u0026\u0026 isLongRead())) {\n       long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n       if (offset \u003e\u003d nextCacheDropOffset) {\n         long dropLength \u003d offset - lastCacheDropOffset;\n-        ris.dropCacheBehindReads(block.getBlockName(), lastCacheDropOffset,\n+        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n+            block.getBlockName(), blockInFd, lastCacheDropOffset,\n             dropLength, POSIX_FADV_DONTNEED);\n         lastCacheDropOffset \u003d offset;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void manageOsCache() throws IOException {\n    // We can\u0027t manage the cache for this block if we don\u0027t have a file\n    // descriptor to work with.\n    if (blockInFd \u003d\u003d null) return;\n\n    // Perform readahead if necessary\n    if ((readaheadLength \u003e 0) \u0026\u0026 (datanode.readaheadPool !\u003d null) \u0026\u0026\n          (alwaysReadahead || isLongRead())) {\n      curReadahead \u003d datanode.readaheadPool.readaheadStream(\n          clientTraceFmt, blockInFd, offset, readaheadLength, Long.MAX_VALUE,\n          curReadahead);\n    }\n\n    // Drop what we\u0027ve just read from cache, since we aren\u0027t\n    // likely to need it again\n    if (dropCacheBehindAllReads ||\n        (dropCacheBehindLargeReads \u0026\u0026 isLongRead())) {\n      long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n      if (offset \u003e\u003d nextCacheDropOffset) {\n        long dropLength \u003d offset - lastCacheDropOffset;\n        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n            block.getBlockName(), blockInFd, lastCacheDropOffset,\n            dropLength, POSIX_FADV_DONTNEED);\n        lastCacheDropOffset \u003d offset;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "aeecfa24f4fb6af289920cbf8830c394e66bd78e": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10930. Refactor: Wrap Datanode IO related operations. Contributed by Xiaoyu Yao.\n",
      "commitDate": "29/11/16 8:52 PM",
      "commitName": "aeecfa24f4fb6af289920cbf8830c394e66bd78e",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "27/10/16 4:14 AM",
      "commitNameOld": "1cf6ec4ad4e1f4ea71f912923b5e8627b61ef482",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 33.74,
      "commitsBetweenForRepo": 277,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,28 @@\n   private void manageOsCache() throws IOException {\n     // We can\u0027t manage the cache for this block if we don\u0027t have a file\n     // descriptor to work with.\n-    if (blockInFd \u003d\u003d null) return;\n+    if (ris.getDataInFd() \u003d\u003d null) {\n+      return;\n+    }\n \n     // Perform readahead if necessary\n     if ((readaheadLength \u003e 0) \u0026\u0026 (datanode.readaheadPool !\u003d null) \u0026\u0026\n           (alwaysReadahead || isLongRead())) {\n       curReadahead \u003d datanode.readaheadPool.readaheadStream(\n-          clientTraceFmt, blockInFd, offset, readaheadLength, Long.MAX_VALUE,\n-          curReadahead);\n+          clientTraceFmt, ris.getDataInFd(), offset, readaheadLength,\n+          Long.MAX_VALUE, curReadahead);\n     }\n \n     // Drop what we\u0027ve just read from cache, since we aren\u0027t\n     // likely to need it again\n     if (dropCacheBehindAllReads ||\n         (dropCacheBehindLargeReads \u0026\u0026 isLongRead())) {\n       long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n       if (offset \u003e\u003d nextCacheDropOffset) {\n         long dropLength \u003d offset - lastCacheDropOffset;\n-        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n-            block.getBlockName(), blockInFd, lastCacheDropOffset,\n+        ris.dropCacheBehindReads(block.getBlockName(), lastCacheDropOffset,\n             dropLength, POSIX_FADV_DONTNEED);\n         lastCacheDropOffset \u003d offset;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void manageOsCache() throws IOException {\n    // We can\u0027t manage the cache for this block if we don\u0027t have a file\n    // descriptor to work with.\n    if (ris.getDataInFd() \u003d\u003d null) {\n      return;\n    }\n\n    // Perform readahead if necessary\n    if ((readaheadLength \u003e 0) \u0026\u0026 (datanode.readaheadPool !\u003d null) \u0026\u0026\n          (alwaysReadahead || isLongRead())) {\n      curReadahead \u003d datanode.readaheadPool.readaheadStream(\n          clientTraceFmt, ris.getDataInFd(), offset, readaheadLength,\n          Long.MAX_VALUE, curReadahead);\n    }\n\n    // Drop what we\u0027ve just read from cache, since we aren\u0027t\n    // likely to need it again\n    if (dropCacheBehindAllReads ||\n        (dropCacheBehindLargeReads \u0026\u0026 isLongRead())) {\n      long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n      if (offset \u003e\u003d nextCacheDropOffset) {\n        long dropLength \u003d offset - lastCacheDropOffset;\n        ris.dropCacheBehindReads(block.getBlockName(), lastCacheDropOffset,\n            dropLength, POSIX_FADV_DONTNEED);\n        lastCacheDropOffset \u003d offset;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "21d10ccc6e463cf250414264c78acb4a6e7c83e3": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-7824. NativeIO.java flags and identifiers must be set correctly for each platform, not hardcoded to their Linux values (Martin Walsh via Colin P. McCabe)\n",
      "commitDate": "31/07/15 3:01 PM",
      "commitName": "21d10ccc6e463cf250414264c78acb4a6e7c83e3",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "05/05/15 3:41 PM",
      "commitNameOld": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 86.97,
      "commitsBetweenForRepo": 681,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,27 @@\n   private void manageOsCache() throws IOException {\n     // We can\u0027t manage the cache for this block if we don\u0027t have a file\n     // descriptor to work with.\n     if (blockInFd \u003d\u003d null) return;\n \n     // Perform readahead if necessary\n     if ((readaheadLength \u003e 0) \u0026\u0026 (datanode.readaheadPool !\u003d null) \u0026\u0026\n           (alwaysReadahead || isLongRead())) {\n       curReadahead \u003d datanode.readaheadPool.readaheadStream(\n           clientTraceFmt, blockInFd, offset, readaheadLength, Long.MAX_VALUE,\n           curReadahead);\n     }\n \n     // Drop what we\u0027ve just read from cache, since we aren\u0027t\n     // likely to need it again\n     if (dropCacheBehindAllReads ||\n         (dropCacheBehindLargeReads \u0026\u0026 isLongRead())) {\n       long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n       if (offset \u003e\u003d nextCacheDropOffset) {\n         long dropLength \u003d offset - lastCacheDropOffset;\n         NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n             block.getBlockName(), blockInFd, lastCacheDropOffset,\n-            dropLength, NativeIO.POSIX.POSIX_FADV_DONTNEED);\n+            dropLength, POSIX_FADV_DONTNEED);\n         lastCacheDropOffset \u003d offset;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void manageOsCache() throws IOException {\n    // We can\u0027t manage the cache for this block if we don\u0027t have a file\n    // descriptor to work with.\n    if (blockInFd \u003d\u003d null) return;\n\n    // Perform readahead if necessary\n    if ((readaheadLength \u003e 0) \u0026\u0026 (datanode.readaheadPool !\u003d null) \u0026\u0026\n          (alwaysReadahead || isLongRead())) {\n      curReadahead \u003d datanode.readaheadPool.readaheadStream(\n          clientTraceFmt, blockInFd, offset, readaheadLength, Long.MAX_VALUE,\n          curReadahead);\n    }\n\n    // Drop what we\u0027ve just read from cache, since we aren\u0027t\n    // likely to need it again\n    if (dropCacheBehindAllReads ||\n        (dropCacheBehindLargeReads \u0026\u0026 isLongRead())) {\n      long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n      if (offset \u003e\u003d nextCacheDropOffset) {\n        long dropLength \u003d offset - lastCacheDropOffset;\n        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n            block.getBlockName(), blockInFd, lastCacheDropOffset,\n            dropLength, POSIX_FADV_DONTNEED);\n        lastCacheDropOffset \u003d offset;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "efea68dc3538de9aafae206d64903506e41fc9e1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5511. improve CacheManipulator interface to allow better unit testing (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1543676 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/11/13 4:48 PM",
      "commitName": "efea68dc3538de9aafae206d64903506e41fc9e1",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "22/07/13 11:15 AM",
      "commitNameOld": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 120.27,
      "commitsBetweenForRepo": 765,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,27 @@\n   private void manageOsCache() throws IOException {\n     // We can\u0027t manage the cache for this block if we don\u0027t have a file\n     // descriptor to work with.\n     if (blockInFd \u003d\u003d null) return;\n \n     // Perform readahead if necessary\n     if ((readaheadLength \u003e 0) \u0026\u0026 (datanode.readaheadPool !\u003d null) \u0026\u0026\n           (alwaysReadahead || isLongRead())) {\n       curReadahead \u003d datanode.readaheadPool.readaheadStream(\n           clientTraceFmt, blockInFd, offset, readaheadLength, Long.MAX_VALUE,\n           curReadahead);\n     }\n \n     // Drop what we\u0027ve just read from cache, since we aren\u0027t\n     // likely to need it again\n     if (dropCacheBehindAllReads ||\n         (dropCacheBehindLargeReads \u0026\u0026 isLongRead())) {\n       long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n       if (offset \u003e\u003d nextCacheDropOffset) {\n         long dropLength \u003d offset - lastCacheDropOffset;\n-        NativeIO.POSIX.posixFadviseIfPossible(block.getBlockName(),\n-            blockInFd, lastCacheDropOffset, dropLength,\n-            NativeIO.POSIX.POSIX_FADV_DONTNEED);\n+        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n+            block.getBlockName(), blockInFd, lastCacheDropOffset,\n+            dropLength, NativeIO.POSIX.POSIX_FADV_DONTNEED);\n         lastCacheDropOffset \u003d offset;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void manageOsCache() throws IOException {\n    // We can\u0027t manage the cache for this block if we don\u0027t have a file\n    // descriptor to work with.\n    if (blockInFd \u003d\u003d null) return;\n\n    // Perform readahead if necessary\n    if ((readaheadLength \u003e 0) \u0026\u0026 (datanode.readaheadPool !\u003d null) \u0026\u0026\n          (alwaysReadahead || isLongRead())) {\n      curReadahead \u003d datanode.readaheadPool.readaheadStream(\n          clientTraceFmt, blockInFd, offset, readaheadLength, Long.MAX_VALUE,\n          curReadahead);\n    }\n\n    // Drop what we\u0027ve just read from cache, since we aren\u0027t\n    // likely to need it again\n    if (dropCacheBehindAllReads ||\n        (dropCacheBehindLargeReads \u0026\u0026 isLongRead())) {\n      long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n      if (offset \u003e\u003d nextCacheDropOffset) {\n        long dropLength \u003d offset - lastCacheDropOffset;\n        NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(\n            block.getBlockName(), blockInFd, lastCacheDropOffset,\n            dropLength, NativeIO.POSIX.POSIX_FADV_DONTNEED);\n        lastCacheDropOffset \u003d offset;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4817.  Make HDFS advisory caching configurable on a per-file basis.  (Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505753 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/13 11:15 AM",
      "commitName": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "06/03/13 11:15 AM",
      "commitNameOld": "638801cce16fc1dc3259c541dc30a599faaddda1",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 137.96,
      "commitsBetweenForRepo": 857,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,27 @@\n   private void manageOsCache() throws IOException {\n-    if (!isLongRead() || blockInFd \u003d\u003d null) {\n-      // don\u0027t manage cache manually for short-reads, like\n-      // HBase random read workloads.\n-      return;\n-    }\n+    // We can\u0027t manage the cache for this block if we don\u0027t have a file\n+    // descriptor to work with.\n+    if (blockInFd \u003d\u003d null) return;\n \n     // Perform readahead if necessary\n-    if (readaheadLength \u003e 0 \u0026\u0026 datanode.readaheadPool !\u003d null) {\n+    if ((readaheadLength \u003e 0) \u0026\u0026 (datanode.readaheadPool !\u003d null) \u0026\u0026\n+          (alwaysReadahead || isLongRead())) {\n       curReadahead \u003d datanode.readaheadPool.readaheadStream(\n-          clientTraceFmt, blockInFd,\n-          offset, readaheadLength, Long.MAX_VALUE,\n+          clientTraceFmt, blockInFd, offset, readaheadLength, Long.MAX_VALUE,\n           curReadahead);\n     }\n \n     // Drop what we\u0027ve just read from cache, since we aren\u0027t\n     // likely to need it again\n-    long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n-    if (shouldDropCacheBehindRead \u0026\u0026\n-        offset \u003e\u003d nextCacheDropOffset) {\n-      long dropLength \u003d offset - lastCacheDropOffset;\n-      if (dropLength \u003e\u003d 1024) {\n-        NativeIO.POSIX.posixFadviseIfPossible(blockInFd,\n-            lastCacheDropOffset, dropLength,\n+    if (dropCacheBehindAllReads ||\n+        (dropCacheBehindLargeReads \u0026\u0026 isLongRead())) {\n+      long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n+      if (offset \u003e\u003d nextCacheDropOffset) {\n+        long dropLength \u003d offset - lastCacheDropOffset;\n+        NativeIO.POSIX.posixFadviseIfPossible(block.getBlockName(),\n+            blockInFd, lastCacheDropOffset, dropLength,\n             NativeIO.POSIX.POSIX_FADV_DONTNEED);\n+        lastCacheDropOffset \u003d offset;\n       }\n-      lastCacheDropOffset +\u003d CACHE_DROP_INTERVAL_BYTES;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void manageOsCache() throws IOException {\n    // We can\u0027t manage the cache for this block if we don\u0027t have a file\n    // descriptor to work with.\n    if (blockInFd \u003d\u003d null) return;\n\n    // Perform readahead if necessary\n    if ((readaheadLength \u003e 0) \u0026\u0026 (datanode.readaheadPool !\u003d null) \u0026\u0026\n          (alwaysReadahead || isLongRead())) {\n      curReadahead \u003d datanode.readaheadPool.readaheadStream(\n          clientTraceFmt, blockInFd, offset, readaheadLength, Long.MAX_VALUE,\n          curReadahead);\n    }\n\n    // Drop what we\u0027ve just read from cache, since we aren\u0027t\n    // likely to need it again\n    if (dropCacheBehindAllReads ||\n        (dropCacheBehindLargeReads \u0026\u0026 isLongRead())) {\n      long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n      if (offset \u003e\u003d nextCacheDropOffset) {\n        long dropLength \u003d offset - lastCacheDropOffset;\n        NativeIO.POSIX.posixFadviseIfPossible(block.getBlockName(),\n            blockInFd, lastCacheDropOffset, dropLength,\n            NativeIO.POSIX.POSIX_FADV_DONTNEED);\n        lastCacheDropOffset \u003d offset;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "638801cce16fc1dc3259c541dc30a599faaddda1": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-8952. Enhancements to support Hadoop on Windows Server and Windows Azure environments. Contributed by Ivan Mitic, Chuan Liu, Ramya Sunil, Bikas Saha, Kanna Karanam, John Gordon, Brandon Li, Chris Nauroth, David Lao, Sumadhur Reddy Bolli, Arpit Agarwal, Ahmed El Baz, Mike Liddell, Jing Zhao, Thejas Nair, Steve Maine, Ganeshan Iyer, Raja Aluri, Giridharan Kesavan, Ramya Bharathi Nimmagadda.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1453486 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/03/13 11:15 AM",
      "commitName": "638801cce16fc1dc3259c541dc30a599faaddda1",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "14/01/13 12:47 PM",
      "commitNameOld": "3052ad1f0069af5caee621374b29d17d7f12ab51",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 50.94,
      "commitsBetweenForRepo": 214,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   private void manageOsCache() throws IOException {\n     if (!isLongRead() || blockInFd \u003d\u003d null) {\n       // don\u0027t manage cache manually for short-reads, like\n       // HBase random read workloads.\n       return;\n     }\n \n     // Perform readahead if necessary\n     if (readaheadLength \u003e 0 \u0026\u0026 datanode.readaheadPool !\u003d null) {\n       curReadahead \u003d datanode.readaheadPool.readaheadStream(\n           clientTraceFmt, blockInFd,\n           offset, readaheadLength, Long.MAX_VALUE,\n           curReadahead);\n     }\n \n     // Drop what we\u0027ve just read from cache, since we aren\u0027t\n     // likely to need it again\n     long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n     if (shouldDropCacheBehindRead \u0026\u0026\n         offset \u003e\u003d nextCacheDropOffset) {\n       long dropLength \u003d offset - lastCacheDropOffset;\n       if (dropLength \u003e\u003d 1024) {\n-        NativeIO.posixFadviseIfPossible(blockInFd,\n+        NativeIO.POSIX.posixFadviseIfPossible(blockInFd,\n             lastCacheDropOffset, dropLength,\n-            NativeIO.POSIX_FADV_DONTNEED);\n+            NativeIO.POSIX.POSIX_FADV_DONTNEED);\n       }\n       lastCacheDropOffset +\u003d CACHE_DROP_INTERVAL_BYTES;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void manageOsCache() throws IOException {\n    if (!isLongRead() || blockInFd \u003d\u003d null) {\n      // don\u0027t manage cache manually for short-reads, like\n      // HBase random read workloads.\n      return;\n    }\n\n    // Perform readahead if necessary\n    if (readaheadLength \u003e 0 \u0026\u0026 datanode.readaheadPool !\u003d null) {\n      curReadahead \u003d datanode.readaheadPool.readaheadStream(\n          clientTraceFmt, blockInFd,\n          offset, readaheadLength, Long.MAX_VALUE,\n          curReadahead);\n    }\n\n    // Drop what we\u0027ve just read from cache, since we aren\u0027t\n    // likely to need it again\n    long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n    if (shouldDropCacheBehindRead \u0026\u0026\n        offset \u003e\u003d nextCacheDropOffset) {\n      long dropLength \u003d offset - lastCacheDropOffset;\n      if (dropLength \u003e\u003d 1024) {\n        NativeIO.POSIX.posixFadviseIfPossible(blockInFd,\n            lastCacheDropOffset, dropLength,\n            NativeIO.POSIX.POSIX_FADV_DONTNEED);\n      }\n      lastCacheDropOffset +\u003d CACHE_DROP_INTERVAL_BYTES;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "c12e994eda0f7e0c34fb0c0ff208789586c7142c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3754. BlockSender doesn\u0027t shutdown ReadaheadPool threads. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370495 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 1:17 PM",
      "commitName": "c12e994eda0f7e0c34fb0c0ff208789586c7142c",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "26/07/12 5:26 PM",
      "commitNameOld": "c1ea9b4490e7d6d030eeaeeff2fad3767d2cfd4a",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 11.83,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   private void manageOsCache() throws IOException {\n     if (!isLongRead() || blockInFd \u003d\u003d null) {\n       // don\u0027t manage cache manually for short-reads, like\n       // HBase random read workloads.\n       return;\n     }\n \n     // Perform readahead if necessary\n-    if (readaheadLength \u003e 0 \u0026\u0026 readaheadPool !\u003d null) {\n-      curReadahead \u003d readaheadPool.readaheadStream(\n+    if (readaheadLength \u003e 0 \u0026\u0026 datanode.readaheadPool !\u003d null) {\n+      curReadahead \u003d datanode.readaheadPool.readaheadStream(\n           clientTraceFmt, blockInFd,\n           offset, readaheadLength, Long.MAX_VALUE,\n           curReadahead);\n     }\n \n     // Drop what we\u0027ve just read from cache, since we aren\u0027t\n     // likely to need it again\n     long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n     if (shouldDropCacheBehindRead \u0026\u0026\n         offset \u003e\u003d nextCacheDropOffset) {\n       long dropLength \u003d offset - lastCacheDropOffset;\n       if (dropLength \u003e\u003d 1024) {\n         NativeIO.posixFadviseIfPossible(blockInFd,\n             lastCacheDropOffset, dropLength,\n             NativeIO.POSIX_FADV_DONTNEED);\n       }\n       lastCacheDropOffset +\u003d CACHE_DROP_INTERVAL_BYTES;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void manageOsCache() throws IOException {\n    if (!isLongRead() || blockInFd \u003d\u003d null) {\n      // don\u0027t manage cache manually for short-reads, like\n      // HBase random read workloads.\n      return;\n    }\n\n    // Perform readahead if necessary\n    if (readaheadLength \u003e 0 \u0026\u0026 datanode.readaheadPool !\u003d null) {\n      curReadahead \u003d datanode.readaheadPool.readaheadStream(\n          clientTraceFmt, blockInFd,\n          offset, readaheadLength, Long.MAX_VALUE,\n          curReadahead);\n    }\n\n    // Drop what we\u0027ve just read from cache, since we aren\u0027t\n    // likely to need it again\n    long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n    if (shouldDropCacheBehindRead \u0026\u0026\n        offset \u003e\u003d nextCacheDropOffset) {\n      long dropLength \u003d offset - lastCacheDropOffset;\n      if (dropLength \u003e\u003d 1024) {\n        NativeIO.posixFadviseIfPossible(blockInFd,\n            lastCacheDropOffset, dropLength,\n            NativeIO.POSIX_FADV_DONTNEED);\n      }\n      lastCacheDropOffset +\u003d CACHE_DROP_INTERVAL_BYTES;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java",
      "extendedDetails": {}
    },
    "6b0963c53be360b710614b9f44a29c4171af6b83": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2465. Add HDFS support for fadvise readahead and drop-behind. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1190626 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/10/11 3:18 PM",
      "commitName": "6b0963c53be360b710614b9f44a29c4171af6b83",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,29 @@\n+  private void manageOsCache() throws IOException {\n+    if (!isLongRead() || blockInFd \u003d\u003d null) {\n+      // don\u0027t manage cache manually for short-reads, like\n+      // HBase random read workloads.\n+      return;\n+    }\n+\n+    // Perform readahead if necessary\n+    if (readaheadLength \u003e 0 \u0026\u0026 readaheadPool !\u003d null) {\n+      curReadahead \u003d readaheadPool.readaheadStream(\n+          clientTraceFmt, blockInFd,\n+          offset, readaheadLength, Long.MAX_VALUE,\n+          curReadahead);\n+    }\n+\n+    // Drop what we\u0027ve just read from cache, since we aren\u0027t\n+    // likely to need it again\n+    long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n+    if (shouldDropCacheBehindRead \u0026\u0026\n+        offset \u003e\u003d nextCacheDropOffset) {\n+      long dropLength \u003d offset - lastCacheDropOffset;\n+      if (dropLength \u003e\u003d 1024) {\n+        NativeIO.posixFadviseIfPossible(blockInFd,\n+            lastCacheDropOffset, dropLength,\n+            NativeIO.POSIX_FADV_DONTNEED);\n+      }\n+      lastCacheDropOffset +\u003d CACHE_DROP_INTERVAL_BYTES;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void manageOsCache() throws IOException {\n    if (!isLongRead() || blockInFd \u003d\u003d null) {\n      // don\u0027t manage cache manually for short-reads, like\n      // HBase random read workloads.\n      return;\n    }\n\n    // Perform readahead if necessary\n    if (readaheadLength \u003e 0 \u0026\u0026 readaheadPool !\u003d null) {\n      curReadahead \u003d readaheadPool.readaheadStream(\n          clientTraceFmt, blockInFd,\n          offset, readaheadLength, Long.MAX_VALUE,\n          curReadahead);\n    }\n\n    // Drop what we\u0027ve just read from cache, since we aren\u0027t\n    // likely to need it again\n    long nextCacheDropOffset \u003d lastCacheDropOffset + CACHE_DROP_INTERVAL_BYTES;\n    if (shouldDropCacheBehindRead \u0026\u0026\n        offset \u003e\u003d nextCacheDropOffset) {\n      long dropLength \u003d offset - lastCacheDropOffset;\n      if (dropLength \u003e\u003d 1024) {\n        NativeIO.posixFadviseIfPossible(blockInFd,\n            lastCacheDropOffset, dropLength,\n            NativeIO.POSIX_FADV_DONTNEED);\n      }\n      lastCacheDropOffset +\u003d CACHE_DROP_INTERVAL_BYTES;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java"
    }
  }
}