{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ClientThrottlingIntercept.java",
  "functionName": "updateMetrics",
  "functionId": "updateMetrics___conn-HttpURLConnection__result-RequestResult",
  "sourceFilePath": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/ClientThrottlingIntercept.java",
  "functionStartLine": 75,
  "functionEndLine": 109,
  "numCommitsSeen": 1,
  "timeTaken": 683,
  "changeHistory": [
    "778d4edd9adbe9519c3d6df65e45ddc8bb0ab2da"
  ],
  "changeHistoryShort": {
    "778d4edd9adbe9519c3d6df65e45ddc8bb0ab2da": "Yintroduced"
  },
  "changeHistoryDetails": {
    "778d4edd9adbe9519c3d6df65e45ddc8bb0ab2da": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-14660 wasb: improve throughput by 34% when account limit exceeded.\nContributed by Thomas Marquardt.\n",
      "commitDate": "01/08/17 1:36 PM",
      "commitName": "778d4edd9adbe9519c3d6df65e45ddc8bb0ab2da",
      "commitAuthor": "Steve Loughran",
      "diff": "@@ -0,0 +1,35 @@\n+  private static void updateMetrics(HttpURLConnection conn,\n+                                    RequestResult result) {\n+    BlobOperationDescriptor.OperationType operationType\n+        \u003d BlobOperationDescriptor.getOperationType(conn);\n+    int status \u003d result.getStatusCode();\n+    long contentLength \u003d 0;\n+    // If the socket is terminated prior to receiving a response, the HTTP\n+    // status may be 0 or -1.  A status less than 200 or greater than or equal\n+    // to 500 is considered an error.\n+    boolean isFailedOperation \u003d (status \u003c HttpURLConnection.HTTP_OK\n+        || status \u003e\u003d java.net.HttpURLConnection.HTTP_INTERNAL_ERROR);\n+\n+    switch (operationType) {\n+      case AppendBlock:\n+      case PutBlock:\n+      case PutPage:\n+        contentLength \u003d BlobOperationDescriptor.getContentLengthIfKnown(conn,\n+            operationType);\n+        if (contentLength \u003e 0) {\n+          singleton.writeThrottler.addBytesTransferred(contentLength,\n+              isFailedOperation);\n+        }\n+        break;\n+      case GetBlob:\n+        contentLength \u003d BlobOperationDescriptor.getContentLengthIfKnown(conn,\n+            operationType);\n+        if (contentLength \u003e 0) {\n+          singleton.readThrottler.addBytesTransferred(contentLength,\n+              isFailedOperation);\n+        }\n+        break;\n+      default:\n+        break;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private static void updateMetrics(HttpURLConnection conn,\n                                    RequestResult result) {\n    BlobOperationDescriptor.OperationType operationType\n        \u003d BlobOperationDescriptor.getOperationType(conn);\n    int status \u003d result.getStatusCode();\n    long contentLength \u003d 0;\n    // If the socket is terminated prior to receiving a response, the HTTP\n    // status may be 0 or -1.  A status less than 200 or greater than or equal\n    // to 500 is considered an error.\n    boolean isFailedOperation \u003d (status \u003c HttpURLConnection.HTTP_OK\n        || status \u003e\u003d java.net.HttpURLConnection.HTTP_INTERNAL_ERROR);\n\n    switch (operationType) {\n      case AppendBlock:\n      case PutBlock:\n      case PutPage:\n        contentLength \u003d BlobOperationDescriptor.getContentLengthIfKnown(conn,\n            operationType);\n        if (contentLength \u003e 0) {\n          singleton.writeThrottler.addBytesTransferred(contentLength,\n              isFailedOperation);\n        }\n        break;\n      case GetBlob:\n        contentLength \u003d BlobOperationDescriptor.getContentLengthIfKnown(conn,\n            operationType);\n        if (contentLength \u003e 0) {\n          singleton.readThrottler.addBytesTransferred(contentLength,\n              isFailedOperation);\n        }\n        break;\n      default:\n        break;\n    }\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/ClientThrottlingIntercept.java"
    }
  }
}