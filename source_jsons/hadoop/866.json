{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSOutputStream.java",
  "functionName": "writeChunk",
  "functionId": "writeChunk___buffer-ByteBuffer__len-int__checksum-byte[]__ckoff-int__cklen-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
  "functionStartLine": 448,
  "functionEndLine": 462,
  "numCommitsSeen": 29,
  "timeTaken": 1161,
  "changeHistory": [
    "b5af9be72c72734d668f817c99d889031922a951"
  ],
  "changeHistoryShort": {
    "b5af9be72c72734d668f817c99d889031922a951": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b5af9be72c72734d668f817c99d889031922a951": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-8668. Erasure Coding: revisit buffer used for encoding and decoding. Contributed by Sammi Chen\n",
      "commitDate": "12/08/16 10:52 PM",
      "commitName": "b5af9be72c72734d668f817c99d889031922a951",
      "commitAuthor": "Kai Zheng",
      "diff": "@@ -0,0 +1,15 @@\n+  protected synchronized void writeChunk(ByteBuffer buffer, int len,\n+      byte[] checksum, int ckoff, int cklen) throws IOException {\n+    writeChunkPrepare(len, ckoff, cklen);\n+\n+    currentPacket.writeChecksum(checksum, ckoff, cklen);\n+    currentPacket.writeData(buffer, len);\n+    currentPacket.incNumChunks();\n+    getStreamer().incBytesCurBlock(len);\n+\n+    // If packet is full, enqueue it for transmission\n+    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n+            getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n+      enqueueCurrentPacketFull();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void writeChunk(ByteBuffer buffer, int len,\n      byte[] checksum, int ckoff, int cklen) throws IOException {\n    writeChunkPrepare(len, ckoff, cklen);\n\n    currentPacket.writeChecksum(checksum, ckoff, cklen);\n    currentPacket.writeData(buffer, len);\n    currentPacket.incNumChunks();\n    getStreamer().incBytesCurBlock(len);\n\n    // If packet is full, enqueue it for transmission\n    if (currentPacket.getNumChunks() \u003d\u003d currentPacket.getMaxChunks() ||\n            getStreamer().getBytesCurBlock() \u003d\u003d blockSize) {\n      enqueueCurrentPacketFull();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}