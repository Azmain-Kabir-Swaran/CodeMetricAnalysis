{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CacheManager.java",
  "functionName": "loadDirectives",
  "functionId": "loadDirectives___in-DataInput",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
  "functionStartLine": 1253,
  "functionEndLine": 1271,
  "numCommitsSeen": 81,
  "timeTaken": 3558,
  "changeHistory": [
    "a2edb11b68ae01a44092cb14ac2717a6aad93305",
    "991c453ca3ac141a3f286f74af8401f83c38b230",
    "9da451cac57f3cd64c2c047675e5b60ca88ecf83",
    "13edb391d06c479720202eb5ac81f1c71fe64748",
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
    "1d96e3601312d771270567482cb0a051be786a21",
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
    "efe545b0c219eeba61ac5259aee4d518beb74316",
    "af1ac9a5e8d8d97a855940d853dd59ab4666f6e2"
  ],
  "changeHistoryShort": {
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": "Ybodychange",
    "991c453ca3ac141a3f286f74af8401f83c38b230": "Ybodychange",
    "9da451cac57f3cd64c2c047675e5b60ca88ecf83": "Ybodychange",
    "13edb391d06c479720202eb5ac81f1c71fe64748": "Ymultichange(Yrename,Ybodychange)",
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde": "Ybodychange",
    "1d96e3601312d771270567482cb0a051be786a21": "Ybodychange",
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a": "Ymultichange(Ymodifierchange,Ybodychange)",
    "efe545b0c219eeba61ac5259aee4d518beb74316": "Ybodychange",
    "af1ac9a5e8d8d97a855940d853dd59ab4666f6e2": "Yintroduced"
  },
  "changeHistoryDetails": {
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5698. Use protobuf to serialize / deserialize FSImage. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/02/14 11:18 AM",
      "commitName": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "29/01/14 3:59 PM",
      "commitNameOld": "c96d0780335de83a694e93cf8b8fc46d2288fe9d",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 10.81,
      "commitsBetweenForRepo": 81,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,19 @@\n     private void loadDirectives(DataInput in) throws IOException {\n       StartupProgress prog \u003d NameNode.getStartupProgress();\n       Step step \u003d new Step(StepType.CACHE_ENTRIES);\n       prog.beginStep(Phase.LOADING_FSIMAGE, step);\n       int numDirectives \u003d in.readInt();\n       prog.setTotal(Phase.LOADING_FSIMAGE, step, numDirectives);\n       Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n       for (int i \u003d 0; i \u003c numDirectives; i++) {\n         CacheDirectiveInfo info \u003d FSImageSerialization.readCacheDirectiveInfo(in);\n         // Get pool reference by looking it up in the map\n         final String poolName \u003d info.getPool();\n-        CachePool pool \u003d cachePools.get(poolName);\n-        if (pool \u003d\u003d null) {\n-          throw new IOException(\"Directive refers to pool \" + poolName +\n-              \", which does not exist.\");\n-        }\n         CacheDirective directive \u003d\n             new CacheDirective(info.getId(), info.getPath().toUri().getPath(),\n                 info.getReplication(), info.getExpiration().getAbsoluteMillis());\n-        boolean addedDirective \u003d pool.getDirectiveList().add(directive);\n-        assert addedDirective;\n-        if (directivesById.put(directive.getId(), directive) !\u003d null) {\n-          throw new IOException(\"A directive with ID \" + directive.getId() +\n-              \" already exists\");\n-        }\n-        List\u003cCacheDirective\u003e directives \u003d\n-            directivesByPath.get(directive.getPath());\n-        if (directives \u003d\u003d null) {\n-          directives \u003d new LinkedList\u003cCacheDirective\u003e();\n-          directivesByPath.put(directive.getPath(), directives);\n-        }\n-        directives.add(directive);\n+        addCacheDirective(poolName, directive);\n         counter.increment();\n       }\n       prog.endStep(Phase.LOADING_FSIMAGE, step);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void loadDirectives(DataInput in) throws IOException {\n      StartupProgress prog \u003d NameNode.getStartupProgress();\n      Step step \u003d new Step(StepType.CACHE_ENTRIES);\n      prog.beginStep(Phase.LOADING_FSIMAGE, step);\n      int numDirectives \u003d in.readInt();\n      prog.setTotal(Phase.LOADING_FSIMAGE, step, numDirectives);\n      Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n      for (int i \u003d 0; i \u003c numDirectives; i++) {\n        CacheDirectiveInfo info \u003d FSImageSerialization.readCacheDirectiveInfo(in);\n        // Get pool reference by looking it up in the map\n        final String poolName \u003d info.getPool();\n        CacheDirective directive \u003d\n            new CacheDirective(info.getId(), info.getPath().toUri().getPath(),\n                info.getReplication(), info.getExpiration().getAbsoluteMillis());\n        addCacheDirective(poolName, directive);\n        counter.increment();\n      }\n      prog.endStep(Phase.LOADING_FSIMAGE, step);\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "991c453ca3ac141a3f286f74af8401f83c38b230": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5431. Support cachepool-based limit management in path-based caching. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551651 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 10:47 AM",
      "commitName": "991c453ca3ac141a3f286f74af8401f83c38b230",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "05/12/13 1:09 PM",
      "commitNameOld": "55e5b0653c34a5f4146ce5a97a5b4a88a976d88a",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 11.9,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,36 @@\n   private void loadDirectives(DataInput in) throws IOException {\n     StartupProgress prog \u003d NameNode.getStartupProgress();\n     Step step \u003d new Step(StepType.CACHE_ENTRIES);\n     prog.beginStep(Phase.LOADING_FSIMAGE, step);\n     int numDirectives \u003d in.readInt();\n     prog.setTotal(Phase.LOADING_FSIMAGE, step, numDirectives);\n     Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n     for (int i \u003d 0; i \u003c numDirectives; i++) {\n-      long directiveId \u003d in.readLong();\n-      String path \u003d Text.readString(in);\n-      short replication \u003d in.readShort();\n-      String poolName \u003d Text.readString(in);\n-      long expiryTime \u003d in.readLong();\n+      CacheDirectiveInfo info \u003d FSImageSerialization.readCacheDirectiveInfo(in);\n       // Get pool reference by looking it up in the map\n+      final String poolName \u003d info.getPool();\n       CachePool pool \u003d cachePools.get(poolName);\n       if (pool \u003d\u003d null) {\n         throw new IOException(\"Directive refers to pool \" + poolName +\n             \", which does not exist.\");\n       }\n       CacheDirective directive \u003d\n-          new CacheDirective(directiveId, path, replication, expiryTime);\n+          new CacheDirective(info.getId(), info.getPath().toUri().getPath(),\n+              info.getReplication(), info.getExpiration().getAbsoluteMillis());\n       boolean addedDirective \u003d pool.getDirectiveList().add(directive);\n       assert addedDirective;\n       if (directivesById.put(directive.getId(), directive) !\u003d null) {\n         throw new IOException(\"A directive with ID \" + directive.getId() +\n             \" already exists\");\n       }\n       List\u003cCacheDirective\u003e directives \u003d\n           directivesByPath.get(directive.getPath());\n       if (directives \u003d\u003d null) {\n         directives \u003d new LinkedList\u003cCacheDirective\u003e();\n         directivesByPath.put(directive.getPath(), directives);\n       }\n       directives.add(directive);\n       counter.increment();\n     }\n     prog.endStep(Phase.LOADING_FSIMAGE, step);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void loadDirectives(DataInput in) throws IOException {\n    StartupProgress prog \u003d NameNode.getStartupProgress();\n    Step step \u003d new Step(StepType.CACHE_ENTRIES);\n    prog.beginStep(Phase.LOADING_FSIMAGE, step);\n    int numDirectives \u003d in.readInt();\n    prog.setTotal(Phase.LOADING_FSIMAGE, step, numDirectives);\n    Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n    for (int i \u003d 0; i \u003c numDirectives; i++) {\n      CacheDirectiveInfo info \u003d FSImageSerialization.readCacheDirectiveInfo(in);\n      // Get pool reference by looking it up in the map\n      final String poolName \u003d info.getPool();\n      CachePool pool \u003d cachePools.get(poolName);\n      if (pool \u003d\u003d null) {\n        throw new IOException(\"Directive refers to pool \" + poolName +\n            \", which does not exist.\");\n      }\n      CacheDirective directive \u003d\n          new CacheDirective(info.getId(), info.getPath().toUri().getPath(),\n              info.getReplication(), info.getExpiration().getAbsoluteMillis());\n      boolean addedDirective \u003d pool.getDirectiveList().add(directive);\n      assert addedDirective;\n      if (directivesById.put(directive.getId(), directive) !\u003d null) {\n        throw new IOException(\"A directive with ID \" + directive.getId() +\n            \" already exists\");\n      }\n      List\u003cCacheDirective\u003e directives \u003d\n          directivesByPath.get(directive.getPath());\n      if (directives \u003d\u003d null) {\n        directives \u003d new LinkedList\u003cCacheDirective\u003e();\n        directivesByPath.put(directive.getPath(), directives);\n      }\n      directives.add(directive);\n      counter.increment();\n    }\n    prog.endStep(Phase.LOADING_FSIMAGE, step);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "9da451cac57f3cd64c2c047675e5b60ca88ecf83": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5430. Support TTL on CacheDirectives. Contributed by Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546301 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/11/13 11:20 PM",
      "commitName": "9da451cac57f3cd64c2c047675e5b60ca88ecf83",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/11/13 9:55 AM",
      "commitNameOld": "13edb391d06c479720202eb5ac81f1c71fe64748",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 0.56,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,38 @@\n   private void loadDirectives(DataInput in) throws IOException {\n     StartupProgress prog \u003d NameNode.getStartupProgress();\n     Step step \u003d new Step(StepType.CACHE_ENTRIES);\n     prog.beginStep(Phase.LOADING_FSIMAGE, step);\n     int numDirectives \u003d in.readInt();\n     prog.setTotal(Phase.LOADING_FSIMAGE, step, numDirectives);\n     Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n     for (int i \u003d 0; i \u003c numDirectives; i++) {\n       long directiveId \u003d in.readLong();\n       String path \u003d Text.readString(in);\n       short replication \u003d in.readShort();\n       String poolName \u003d Text.readString(in);\n+      long expiryTime \u003d in.readLong();\n       // Get pool reference by looking it up in the map\n       CachePool pool \u003d cachePools.get(poolName);\n       if (pool \u003d\u003d null) {\n         throw new IOException(\"Directive refers to pool \" + poolName +\n             \", which does not exist.\");\n       }\n       CacheDirective directive \u003d\n-          new CacheDirective(directiveId, path, replication);\n+          new CacheDirective(directiveId, path, replication, expiryTime);\n       boolean addedDirective \u003d pool.getDirectiveList().add(directive);\n       assert addedDirective;\n       if (directivesById.put(directive.getId(), directive) !\u003d null) {\n         throw new IOException(\"A directive with ID \" + directive.getId() +\n             \" already exists\");\n       }\n       List\u003cCacheDirective\u003e directives \u003d\n           directivesByPath.get(directive.getPath());\n       if (directives \u003d\u003d null) {\n         directives \u003d new LinkedList\u003cCacheDirective\u003e();\n         directivesByPath.put(directive.getPath(), directives);\n       }\n       directives.add(directive);\n       counter.increment();\n     }\n     prog.endStep(Phase.LOADING_FSIMAGE, step);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void loadDirectives(DataInput in) throws IOException {\n    StartupProgress prog \u003d NameNode.getStartupProgress();\n    Step step \u003d new Step(StepType.CACHE_ENTRIES);\n    prog.beginStep(Phase.LOADING_FSIMAGE, step);\n    int numDirectives \u003d in.readInt();\n    prog.setTotal(Phase.LOADING_FSIMAGE, step, numDirectives);\n    Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n    for (int i \u003d 0; i \u003c numDirectives; i++) {\n      long directiveId \u003d in.readLong();\n      String path \u003d Text.readString(in);\n      short replication \u003d in.readShort();\n      String poolName \u003d Text.readString(in);\n      long expiryTime \u003d in.readLong();\n      // Get pool reference by looking it up in the map\n      CachePool pool \u003d cachePools.get(poolName);\n      if (pool \u003d\u003d null) {\n        throw new IOException(\"Directive refers to pool \" + poolName +\n            \", which does not exist.\");\n      }\n      CacheDirective directive \u003d\n          new CacheDirective(directiveId, path, replication, expiryTime);\n      boolean addedDirective \u003d pool.getDirectiveList().add(directive);\n      assert addedDirective;\n      if (directivesById.put(directive.getId(), directive) !\u003d null) {\n        throw new IOException(\"A directive with ID \" + directive.getId() +\n            \" already exists\");\n      }\n      List\u003cCacheDirective\u003e directives \u003d\n          directivesByPath.get(directive.getPath());\n      if (directives \u003d\u003d null) {\n        directives \u003d new LinkedList\u003cCacheDirective\u003e();\n        directivesByPath.put(directive.getPath(), directives);\n      }\n      directives.add(directive);\n      counter.increment();\n    }\n    prog.endStep(Phase.LOADING_FSIMAGE, step);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "13edb391d06c479720202eb5ac81f1c71fe64748": {
      "type": "Ymultichange(Yrename,Ybodychange)",
      "commitMessage": "HDFS-5556. Add some more NameNode cache statistics, cache pool stats (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/11/13 9:55 AM",
      "commitName": "13edb391d06c479720202eb5ac81f1c71fe64748",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-5556. Add some more NameNode cache statistics, cache pool stats (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/11/13 9:55 AM",
          "commitName": "13edb391d06c479720202eb5ac81f1c71fe64748",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "21/11/13 9:12 AM",
          "commitNameOld": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 6.03,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,34 +1,37 @@\n-  private void loadEntries(DataInput in) throws IOException {\n+  private void loadDirectives(DataInput in) throws IOException {\n     StartupProgress prog \u003d NameNode.getStartupProgress();\n     Step step \u003d new Step(StepType.CACHE_ENTRIES);\n     prog.beginStep(Phase.LOADING_FSIMAGE, step);\n-    int numberOfEntries \u003d in.readInt();\n-    prog.setTotal(Phase.LOADING_FSIMAGE, step, numberOfEntries);\n+    int numDirectives \u003d in.readInt();\n+    prog.setTotal(Phase.LOADING_FSIMAGE, step, numDirectives);\n     Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n-    for (int i \u003d 0; i \u003c numberOfEntries; i++) {\n-      long entryId \u003d in.readLong();\n+    for (int i \u003d 0; i \u003c numDirectives; i++) {\n+      long directiveId \u003d in.readLong();\n       String path \u003d Text.readString(in);\n       short replication \u003d in.readShort();\n       String poolName \u003d Text.readString(in);\n       // Get pool reference by looking it up in the map\n       CachePool pool \u003d cachePools.get(poolName);\n       if (pool \u003d\u003d null) {\n-        throw new IOException(\"Entry refers to pool \" + poolName +\n+        throw new IOException(\"Directive refers to pool \" + poolName +\n             \", which does not exist.\");\n       }\n-      CacheDirective entry \u003d\n-          new CacheDirective(entryId, path, replication, pool);\n-      if (entriesById.put(entry.getEntryId(), entry) !\u003d null) {\n-        throw new IOException(\"An entry with ID \" + entry.getEntryId() +\n+      CacheDirective directive \u003d\n+          new CacheDirective(directiveId, path, replication);\n+      boolean addedDirective \u003d pool.getDirectiveList().add(directive);\n+      assert addedDirective;\n+      if (directivesById.put(directive.getId(), directive) !\u003d null) {\n+        throw new IOException(\"A directive with ID \" + directive.getId() +\n             \" already exists\");\n       }\n-      List\u003cCacheDirective\u003e entries \u003d entriesByPath.get(entry.getPath());\n-      if (entries \u003d\u003d null) {\n-        entries \u003d new LinkedList\u003cCacheDirective\u003e();\n-        entriesByPath.put(entry.getPath(), entries);\n+      List\u003cCacheDirective\u003e directives \u003d\n+          directivesByPath.get(directive.getPath());\n+      if (directives \u003d\u003d null) {\n+        directives \u003d new LinkedList\u003cCacheDirective\u003e();\n+        directivesByPath.put(directive.getPath(), directives);\n       }\n-      entries.add(entry);\n+      directives.add(directive);\n       counter.increment();\n     }\n     prog.endStep(Phase.LOADING_FSIMAGE, step);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void loadDirectives(DataInput in) throws IOException {\n    StartupProgress prog \u003d NameNode.getStartupProgress();\n    Step step \u003d new Step(StepType.CACHE_ENTRIES);\n    prog.beginStep(Phase.LOADING_FSIMAGE, step);\n    int numDirectives \u003d in.readInt();\n    prog.setTotal(Phase.LOADING_FSIMAGE, step, numDirectives);\n    Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n    for (int i \u003d 0; i \u003c numDirectives; i++) {\n      long directiveId \u003d in.readLong();\n      String path \u003d Text.readString(in);\n      short replication \u003d in.readShort();\n      String poolName \u003d Text.readString(in);\n      // Get pool reference by looking it up in the map\n      CachePool pool \u003d cachePools.get(poolName);\n      if (pool \u003d\u003d null) {\n        throw new IOException(\"Directive refers to pool \" + poolName +\n            \", which does not exist.\");\n      }\n      CacheDirective directive \u003d\n          new CacheDirective(directiveId, path, replication);\n      boolean addedDirective \u003d pool.getDirectiveList().add(directive);\n      assert addedDirective;\n      if (directivesById.put(directive.getId(), directive) !\u003d null) {\n        throw new IOException(\"A directive with ID \" + directive.getId() +\n            \" already exists\");\n      }\n      List\u003cCacheDirective\u003e directives \u003d\n          directivesByPath.get(directive.getPath());\n      if (directives \u003d\u003d null) {\n        directives \u003d new LinkedList\u003cCacheDirective\u003e();\n        directivesByPath.put(directive.getPath(), directives);\n      }\n      directives.add(directive);\n      counter.increment();\n    }\n    prog.endStep(Phase.LOADING_FSIMAGE, step);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "loadEntries",
            "newValue": "loadDirectives"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5556. Add some more NameNode cache statistics, cache pool stats (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1546143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "27/11/13 9:55 AM",
          "commitName": "13edb391d06c479720202eb5ac81f1c71fe64748",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "21/11/13 9:12 AM",
          "commitNameOld": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 6.03,
          "commitsBetweenForRepo": 22,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,34 +1,37 @@\n-  private void loadEntries(DataInput in) throws IOException {\n+  private void loadDirectives(DataInput in) throws IOException {\n     StartupProgress prog \u003d NameNode.getStartupProgress();\n     Step step \u003d new Step(StepType.CACHE_ENTRIES);\n     prog.beginStep(Phase.LOADING_FSIMAGE, step);\n-    int numberOfEntries \u003d in.readInt();\n-    prog.setTotal(Phase.LOADING_FSIMAGE, step, numberOfEntries);\n+    int numDirectives \u003d in.readInt();\n+    prog.setTotal(Phase.LOADING_FSIMAGE, step, numDirectives);\n     Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n-    for (int i \u003d 0; i \u003c numberOfEntries; i++) {\n-      long entryId \u003d in.readLong();\n+    for (int i \u003d 0; i \u003c numDirectives; i++) {\n+      long directiveId \u003d in.readLong();\n       String path \u003d Text.readString(in);\n       short replication \u003d in.readShort();\n       String poolName \u003d Text.readString(in);\n       // Get pool reference by looking it up in the map\n       CachePool pool \u003d cachePools.get(poolName);\n       if (pool \u003d\u003d null) {\n-        throw new IOException(\"Entry refers to pool \" + poolName +\n+        throw new IOException(\"Directive refers to pool \" + poolName +\n             \", which does not exist.\");\n       }\n-      CacheDirective entry \u003d\n-          new CacheDirective(entryId, path, replication, pool);\n-      if (entriesById.put(entry.getEntryId(), entry) !\u003d null) {\n-        throw new IOException(\"An entry with ID \" + entry.getEntryId() +\n+      CacheDirective directive \u003d\n+          new CacheDirective(directiveId, path, replication);\n+      boolean addedDirective \u003d pool.getDirectiveList().add(directive);\n+      assert addedDirective;\n+      if (directivesById.put(directive.getId(), directive) !\u003d null) {\n+        throw new IOException(\"A directive with ID \" + directive.getId() +\n             \" already exists\");\n       }\n-      List\u003cCacheDirective\u003e entries \u003d entriesByPath.get(entry.getPath());\n-      if (entries \u003d\u003d null) {\n-        entries \u003d new LinkedList\u003cCacheDirective\u003e();\n-        entriesByPath.put(entry.getPath(), entries);\n+      List\u003cCacheDirective\u003e directives \u003d\n+          directivesByPath.get(directive.getPath());\n+      if (directives \u003d\u003d null) {\n+        directives \u003d new LinkedList\u003cCacheDirective\u003e();\n+        directivesByPath.put(directive.getPath(), directives);\n       }\n-      entries.add(entry);\n+      directives.add(directive);\n       counter.increment();\n     }\n     prog.endStep(Phase.LOADING_FSIMAGE, step);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void loadDirectives(DataInput in) throws IOException {\n    StartupProgress prog \u003d NameNode.getStartupProgress();\n    Step step \u003d new Step(StepType.CACHE_ENTRIES);\n    prog.beginStep(Phase.LOADING_FSIMAGE, step);\n    int numDirectives \u003d in.readInt();\n    prog.setTotal(Phase.LOADING_FSIMAGE, step, numDirectives);\n    Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n    for (int i \u003d 0; i \u003c numDirectives; i++) {\n      long directiveId \u003d in.readLong();\n      String path \u003d Text.readString(in);\n      short replication \u003d in.readShort();\n      String poolName \u003d Text.readString(in);\n      // Get pool reference by looking it up in the map\n      CachePool pool \u003d cachePools.get(poolName);\n      if (pool \u003d\u003d null) {\n        throw new IOException(\"Directive refers to pool \" + poolName +\n            \", which does not exist.\");\n      }\n      CacheDirective directive \u003d\n          new CacheDirective(directiveId, path, replication);\n      boolean addedDirective \u003d pool.getDirectiveList().add(directive);\n      assert addedDirective;\n      if (directivesById.put(directive.getId(), directive) !\u003d null) {\n        throw new IOException(\"A directive with ID \" + directive.getId() +\n            \" already exists\");\n      }\n      List\u003cCacheDirective\u003e directives \u003d\n          directivesByPath.get(directive.getPath());\n      if (directives \u003d\u003d null) {\n        directives \u003d new LinkedList\u003cCacheDirective\u003e();\n        directivesByPath.put(directive.getPath(), directives);\n      }\n      directives.add(directive);\n      counter.increment();\n    }\n    prog.endStep(Phase.LOADING_FSIMAGE, step);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "f91a45a96c21db9e5d40097c7d3f5d005ae10dde": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5473. Consistent naming of user-visible caching classes and methods (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1544252 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/11/13 9:12 AM",
      "commitName": "f91a45a96c21db9e5d40097c7d3f5d005ae10dde",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "18/11/13 6:01 PM",
      "commitNameOld": "4f15d0af4f3633bfa35f7cb7c1cc15ef545597d0",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 2.63,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   private void loadEntries(DataInput in) throws IOException {\n     StartupProgress prog \u003d NameNode.getStartupProgress();\n     Step step \u003d new Step(StepType.CACHE_ENTRIES);\n     prog.beginStep(Phase.LOADING_FSIMAGE, step);\n     int numberOfEntries \u003d in.readInt();\n     prog.setTotal(Phase.LOADING_FSIMAGE, step, numberOfEntries);\n     Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n     for (int i \u003d 0; i \u003c numberOfEntries; i++) {\n       long entryId \u003d in.readLong();\n       String path \u003d Text.readString(in);\n       short replication \u003d in.readShort();\n       String poolName \u003d Text.readString(in);\n       // Get pool reference by looking it up in the map\n       CachePool pool \u003d cachePools.get(poolName);\n       if (pool \u003d\u003d null) {\n         throw new IOException(\"Entry refers to pool \" + poolName +\n             \", which does not exist.\");\n       }\n-      PathBasedCacheEntry entry \u003d\n-          new PathBasedCacheEntry(entryId, path, replication, pool);\n+      CacheDirective entry \u003d\n+          new CacheDirective(entryId, path, replication, pool);\n       if (entriesById.put(entry.getEntryId(), entry) !\u003d null) {\n         throw new IOException(\"An entry with ID \" + entry.getEntryId() +\n             \" already exists\");\n       }\n-      List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(entry.getPath());\n+      List\u003cCacheDirective\u003e entries \u003d entriesByPath.get(entry.getPath());\n       if (entries \u003d\u003d null) {\n-        entries \u003d new LinkedList\u003cPathBasedCacheEntry\u003e();\n+        entries \u003d new LinkedList\u003cCacheDirective\u003e();\n         entriesByPath.put(entry.getPath(), entries);\n       }\n       entries.add(entry);\n       counter.increment();\n     }\n     prog.endStep(Phase.LOADING_FSIMAGE, step);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void loadEntries(DataInput in) throws IOException {\n    StartupProgress prog \u003d NameNode.getStartupProgress();\n    Step step \u003d new Step(StepType.CACHE_ENTRIES);\n    prog.beginStep(Phase.LOADING_FSIMAGE, step);\n    int numberOfEntries \u003d in.readInt();\n    prog.setTotal(Phase.LOADING_FSIMAGE, step, numberOfEntries);\n    Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n    for (int i \u003d 0; i \u003c numberOfEntries; i++) {\n      long entryId \u003d in.readLong();\n      String path \u003d Text.readString(in);\n      short replication \u003d in.readShort();\n      String poolName \u003d Text.readString(in);\n      // Get pool reference by looking it up in the map\n      CachePool pool \u003d cachePools.get(poolName);\n      if (pool \u003d\u003d null) {\n        throw new IOException(\"Entry refers to pool \" + poolName +\n            \", which does not exist.\");\n      }\n      CacheDirective entry \u003d\n          new CacheDirective(entryId, path, replication, pool);\n      if (entriesById.put(entry.getEntryId(), entry) !\u003d null) {\n        throw new IOException(\"An entry with ID \" + entry.getEntryId() +\n            \" already exists\");\n      }\n      List\u003cCacheDirective\u003e entries \u003d entriesByPath.get(entry.getPath());\n      if (entries \u003d\u003d null) {\n        entries \u003d new LinkedList\u003cCacheDirective\u003e();\n        entriesByPath.put(entry.getPath(), entries);\n      }\n      entries.add(entry);\n      counter.increment();\n    }\n    prog.endStep(Phase.LOADING_FSIMAGE, step);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "1d96e3601312d771270567482cb0a051be786a21": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5388. Loading fsimage fails to find cache pools during namenode startup (Chris Nauroth via Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1533616 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/10/13 1:20 PM",
      "commitName": "1d96e3601312d771270567482cb0a051be786a21",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "17/10/13 1:31 PM",
      "commitNameOld": "dcb0b853332046f8bf5bb02b8ddbba5b3464fe8f",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.99,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   private void loadEntries(DataInput in) throws IOException {\n     StartupProgress prog \u003d NameNode.getStartupProgress();\n     Step step \u003d new Step(StepType.CACHE_ENTRIES);\n     prog.beginStep(Phase.LOADING_FSIMAGE, step);\n     int numberOfEntries \u003d in.readInt();\n     prog.setTotal(Phase.LOADING_FSIMAGE, step, numberOfEntries);\n     Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n     for (int i \u003d 0; i \u003c numberOfEntries; i++) {\n       long entryId \u003d in.readLong();\n       String path \u003d Text.readString(in);\n       short replication \u003d in.readShort();\n       String poolName \u003d Text.readString(in);\n       // Get pool reference by looking it up in the map\n       CachePool pool \u003d cachePools.get(poolName);\n-      if (pool !\u003d null) {\n+      if (pool \u003d\u003d null) {\n         throw new IOException(\"Entry refers to pool \" + poolName +\n             \", which does not exist.\");\n       }\n       PathBasedCacheEntry entry \u003d\n           new PathBasedCacheEntry(entryId, path, replication, pool);\n       if (entriesById.put(entry.getEntryId(), entry) !\u003d null) {\n         throw new IOException(\"An entry with ID \" + entry.getEntryId() +\n             \" already exists\");\n       }\n       List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(entry.getPath());\n       if (entries \u003d\u003d null) {\n         entries \u003d new LinkedList\u003cPathBasedCacheEntry\u003e();\n         entriesByPath.put(entry.getPath(), entries);\n       }\n       entries.add(entry);\n       counter.increment();\n     }\n     prog.endStep(Phase.LOADING_FSIMAGE, step);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void loadEntries(DataInput in) throws IOException {\n    StartupProgress prog \u003d NameNode.getStartupProgress();\n    Step step \u003d new Step(StepType.CACHE_ENTRIES);\n    prog.beginStep(Phase.LOADING_FSIMAGE, step);\n    int numberOfEntries \u003d in.readInt();\n    prog.setTotal(Phase.LOADING_FSIMAGE, step, numberOfEntries);\n    Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n    for (int i \u003d 0; i \u003c numberOfEntries; i++) {\n      long entryId \u003d in.readLong();\n      String path \u003d Text.readString(in);\n      short replication \u003d in.readShort();\n      String poolName \u003d Text.readString(in);\n      // Get pool reference by looking it up in the map\n      CachePool pool \u003d cachePools.get(poolName);\n      if (pool \u003d\u003d null) {\n        throw new IOException(\"Entry refers to pool \" + poolName +\n            \", which does not exist.\");\n      }\n      PathBasedCacheEntry entry \u003d\n          new PathBasedCacheEntry(entryId, path, replication, pool);\n      if (entriesById.put(entry.getEntryId(), entry) !\u003d null) {\n        throw new IOException(\"An entry with ID \" + entry.getEntryId() +\n            \" already exists\");\n      }\n      List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(entry.getPath());\n      if (entries \u003d\u003d null) {\n        entries \u003d new LinkedList\u003cPathBasedCacheEntry\u003e();\n        entriesByPath.put(entry.getPath(), entries);\n      }\n      entries.add(entry);\n      counter.increment();\n    }\n    prog.endStep(Phase.LOADING_FSIMAGE, step);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/10/13 3:15 PM",
      "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/10/13 3:15 PM",
          "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "14/10/13 3:56 PM",
          "commitNameOld": "efe545b0c219eeba61ac5259aee4d518beb74316",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 1.97,
          "commitsBetweenForRepo": 4,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,34 @@\n-  private synchronized void loadEntries(DataInput in) throws IOException {\n+  private void loadEntries(DataInput in) throws IOException {\n     StartupProgress prog \u003d NameNode.getStartupProgress();\n     Step step \u003d new Step(StepType.CACHE_ENTRIES);\n     prog.beginStep(Phase.LOADING_FSIMAGE, step);\n     int numberOfEntries \u003d in.readInt();\n     prog.setTotal(Phase.LOADING_FSIMAGE, step, numberOfEntries);\n     Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n     for (int i \u003d 0; i \u003c numberOfEntries; i++) {\n       long entryId \u003d in.readLong();\n       String path \u003d Text.readString(in);\n       short replication \u003d in.readShort();\n       String poolName \u003d Text.readString(in);\n       // Get pool reference by looking it up in the map\n       CachePool pool \u003d cachePools.get(poolName);\n+      if (pool !\u003d null) {\n+        throw new IOException(\"Entry refers to pool \" + poolName +\n+            \", which does not exist.\");\n+      }\n       PathBasedCacheEntry entry \u003d\n-        new PathBasedCacheEntry(entryId, path, replication, pool);\n-      unprotectedAddEntry(entry);\n+          new PathBasedCacheEntry(entryId, path, replication, pool);\n+      if (entriesById.put(entry.getEntryId(), entry) !\u003d null) {\n+        throw new IOException(\"An entry with ID \" + entry.getEntryId() +\n+            \" already exists\");\n+      }\n+      List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(entry.getPath());\n+      if (entries \u003d\u003d null) {\n+        entries \u003d new LinkedList\u003cPathBasedCacheEntry\u003e();\n+        entriesByPath.put(entry.getPath(), entries);\n+      }\n+      entries.add(entry);\n       counter.increment();\n     }\n     prog.endStep(Phase.LOADING_FSIMAGE, step);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void loadEntries(DataInput in) throws IOException {\n    StartupProgress prog \u003d NameNode.getStartupProgress();\n    Step step \u003d new Step(StepType.CACHE_ENTRIES);\n    prog.beginStep(Phase.LOADING_FSIMAGE, step);\n    int numberOfEntries \u003d in.readInt();\n    prog.setTotal(Phase.LOADING_FSIMAGE, step, numberOfEntries);\n    Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n    for (int i \u003d 0; i \u003c numberOfEntries; i++) {\n      long entryId \u003d in.readLong();\n      String path \u003d Text.readString(in);\n      short replication \u003d in.readShort();\n      String poolName \u003d Text.readString(in);\n      // Get pool reference by looking it up in the map\n      CachePool pool \u003d cachePools.get(poolName);\n      if (pool !\u003d null) {\n        throw new IOException(\"Entry refers to pool \" + poolName +\n            \", which does not exist.\");\n      }\n      PathBasedCacheEntry entry \u003d\n          new PathBasedCacheEntry(entryId, path, replication, pool);\n      if (entriesById.put(entry.getEntryId(), entry) !\u003d null) {\n        throw new IOException(\"An entry with ID \" + entry.getEntryId() +\n            \" already exists\");\n      }\n      List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(entry.getPath());\n      if (entries \u003d\u003d null) {\n        entries \u003d new LinkedList\u003cPathBasedCacheEntry\u003e();\n        entriesByPath.put(entry.getPath(), entries);\n      }\n      entries.add(entry);\n      counter.increment();\n    }\n    prog.endStep(Phase.LOADING_FSIMAGE, step);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "[private, synchronized]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/10/13 3:15 PM",
          "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "14/10/13 3:56 PM",
          "commitNameOld": "efe545b0c219eeba61ac5259aee4d518beb74316",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 1.97,
          "commitsBetweenForRepo": 4,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,34 @@\n-  private synchronized void loadEntries(DataInput in) throws IOException {\n+  private void loadEntries(DataInput in) throws IOException {\n     StartupProgress prog \u003d NameNode.getStartupProgress();\n     Step step \u003d new Step(StepType.CACHE_ENTRIES);\n     prog.beginStep(Phase.LOADING_FSIMAGE, step);\n     int numberOfEntries \u003d in.readInt();\n     prog.setTotal(Phase.LOADING_FSIMAGE, step, numberOfEntries);\n     Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n     for (int i \u003d 0; i \u003c numberOfEntries; i++) {\n       long entryId \u003d in.readLong();\n       String path \u003d Text.readString(in);\n       short replication \u003d in.readShort();\n       String poolName \u003d Text.readString(in);\n       // Get pool reference by looking it up in the map\n       CachePool pool \u003d cachePools.get(poolName);\n+      if (pool !\u003d null) {\n+        throw new IOException(\"Entry refers to pool \" + poolName +\n+            \", which does not exist.\");\n+      }\n       PathBasedCacheEntry entry \u003d\n-        new PathBasedCacheEntry(entryId, path, replication, pool);\n-      unprotectedAddEntry(entry);\n+          new PathBasedCacheEntry(entryId, path, replication, pool);\n+      if (entriesById.put(entry.getEntryId(), entry) !\u003d null) {\n+        throw new IOException(\"An entry with ID \" + entry.getEntryId() +\n+            \" already exists\");\n+      }\n+      List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(entry.getPath());\n+      if (entries \u003d\u003d null) {\n+        entries \u003d new LinkedList\u003cPathBasedCacheEntry\u003e();\n+        entriesByPath.put(entry.getPath(), entries);\n+      }\n+      entries.add(entry);\n       counter.increment();\n     }\n     prog.endStep(Phase.LOADING_FSIMAGE, step);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void loadEntries(DataInput in) throws IOException {\n    StartupProgress prog \u003d NameNode.getStartupProgress();\n    Step step \u003d new Step(StepType.CACHE_ENTRIES);\n    prog.beginStep(Phase.LOADING_FSIMAGE, step);\n    int numberOfEntries \u003d in.readInt();\n    prog.setTotal(Phase.LOADING_FSIMAGE, step, numberOfEntries);\n    Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n    for (int i \u003d 0; i \u003c numberOfEntries; i++) {\n      long entryId \u003d in.readLong();\n      String path \u003d Text.readString(in);\n      short replication \u003d in.readShort();\n      String poolName \u003d Text.readString(in);\n      // Get pool reference by looking it up in the map\n      CachePool pool \u003d cachePools.get(poolName);\n      if (pool !\u003d null) {\n        throw new IOException(\"Entry refers to pool \" + poolName +\n            \", which does not exist.\");\n      }\n      PathBasedCacheEntry entry \u003d\n          new PathBasedCacheEntry(entryId, path, replication, pool);\n      if (entriesById.put(entry.getEntryId(), entry) !\u003d null) {\n        throw new IOException(\"An entry with ID \" + entry.getEntryId() +\n            \" already exists\");\n      }\n      List\u003cPathBasedCacheEntry\u003e entries \u003d entriesByPath.get(entry.getPath());\n      if (entries \u003d\u003d null) {\n        entries \u003d new LinkedList\u003cPathBasedCacheEntry\u003e();\n        entriesByPath.put(entry.getPath(), entries);\n      }\n      entries.add(entry);\n      counter.increment();\n    }\n    prog.endStep(Phase.LOADING_FSIMAGE, step);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "efe545b0c219eeba61ac5259aee4d518beb74316": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5358. Add replication field to PathBasedCacheDirective. (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532124 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/10/13 3:56 PM",
      "commitName": "efe545b0c219eeba61ac5259aee4d518beb74316",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "11/10/13 12:44 PM",
      "commitNameOld": "8111c3af6b06d6a814e235ad90af5860632d2c25",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 3.13,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,21 @@\n   private synchronized void loadEntries(DataInput in) throws IOException {\n     StartupProgress prog \u003d NameNode.getStartupProgress();\n     Step step \u003d new Step(StepType.CACHE_ENTRIES);\n     prog.beginStep(Phase.LOADING_FSIMAGE, step);\n     int numberOfEntries \u003d in.readInt();\n     prog.setTotal(Phase.LOADING_FSIMAGE, step, numberOfEntries);\n     Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n     for (int i \u003d 0; i \u003c numberOfEntries; i++) {\n       long entryId \u003d in.readLong();\n       String path \u003d Text.readString(in);\n+      short replication \u003d in.readShort();\n       String poolName \u003d Text.readString(in);\n       // Get pool reference by looking it up in the map\n       CachePool pool \u003d cachePools.get(poolName);\n-      PathBasedCacheEntry entry \u003d new PathBasedCacheEntry(entryId, path, pool);\n+      PathBasedCacheEntry entry \u003d\n+        new PathBasedCacheEntry(entryId, path, replication, pool);\n       unprotectedAddEntry(entry);\n       counter.increment();\n     }\n     prog.endStep(Phase.LOADING_FSIMAGE, step);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void loadEntries(DataInput in) throws IOException {\n    StartupProgress prog \u003d NameNode.getStartupProgress();\n    Step step \u003d new Step(StepType.CACHE_ENTRIES);\n    prog.beginStep(Phase.LOADING_FSIMAGE, step);\n    int numberOfEntries \u003d in.readInt();\n    prog.setTotal(Phase.LOADING_FSIMAGE, step, numberOfEntries);\n    Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n    for (int i \u003d 0; i \u003c numberOfEntries; i++) {\n      long entryId \u003d in.readLong();\n      String path \u003d Text.readString(in);\n      short replication \u003d in.readShort();\n      String poolName \u003d Text.readString(in);\n      // Get pool reference by looking it up in the map\n      CachePool pool \u003d cachePools.get(poolName);\n      PathBasedCacheEntry entry \u003d\n        new PathBasedCacheEntry(entryId, path, replication, pool);\n      unprotectedAddEntry(entry);\n      counter.increment();\n    }\n    prog.endStep(Phase.LOADING_FSIMAGE, step);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "af1ac9a5e8d8d97a855940d853dd59ab4666f6e2": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5119. Persist CacheManager state in the edit log. (Contributed by Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1529238 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/10/13 10:46 AM",
      "commitName": "af1ac9a5e8d8d97a855940d853dd59ab4666f6e2",
      "commitAuthor": "Andrew Wang",
      "diff": "@@ -0,0 +1,19 @@\n+  private synchronized void loadEntries(DataInput in) throws IOException {\n+    StartupProgress prog \u003d NameNode.getStartupProgress();\n+    Step step \u003d new Step(StepType.CACHE_ENTRIES);\n+    prog.beginStep(Phase.LOADING_FSIMAGE, step);\n+    int numberOfEntries \u003d in.readInt();\n+    prog.setTotal(Phase.LOADING_FSIMAGE, step, numberOfEntries);\n+    Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n+    for (int i \u003d 0; i \u003c numberOfEntries; i++) {\n+      long entryId \u003d in.readLong();\n+      String path \u003d Text.readString(in);\n+      String poolName \u003d Text.readString(in);\n+      // Get pool reference by looking it up in the map\n+      CachePool pool \u003d cachePools.get(poolName);\n+      PathBasedCacheEntry entry \u003d new PathBasedCacheEntry(entryId, path, pool);\n+      unprotectedAddEntry(entry);\n+      counter.increment();\n+    }\n+    prog.endStep(Phase.LOADING_FSIMAGE, step);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized void loadEntries(DataInput in) throws IOException {\n    StartupProgress prog \u003d NameNode.getStartupProgress();\n    Step step \u003d new Step(StepType.CACHE_ENTRIES);\n    prog.beginStep(Phase.LOADING_FSIMAGE, step);\n    int numberOfEntries \u003d in.readInt();\n    prog.setTotal(Phase.LOADING_FSIMAGE, step, numberOfEntries);\n    Counter counter \u003d prog.getCounter(Phase.LOADING_FSIMAGE, step);\n    for (int i \u003d 0; i \u003c numberOfEntries; i++) {\n      long entryId \u003d in.readLong();\n      String path \u003d Text.readString(in);\n      String poolName \u003d Text.readString(in);\n      // Get pool reference by looking it up in the map\n      CachePool pool \u003d cachePools.get(poolName);\n      PathBasedCacheEntry entry \u003d new PathBasedCacheEntry(entryId, path, pool);\n      unprotectedAddEntry(entry);\n      counter.increment();\n    }\n    prog.endStep(Phase.LOADING_FSIMAGE, step);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java"
    }
  }
}