{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CacheManager.java",
  "functionName": "checkLimit",
  "functionId": "checkLimit___pool-CachePool__path-String__replication-short",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
  "functionStartLine": 427,
  "functionEndLine": 440,
  "numCommitsSeen": 56,
  "timeTaken": 2683,
  "changeHistory": [
    "46f1602e896273b308fbd5df6c75f6c142828227",
    "b9ae3087c0f83bfeeea47ded8e19932b46fd2350",
    "991c453ca3ac141a3f286f74af8401f83c38b230"
  ],
  "changeHistoryShort": {
    "46f1602e896273b308fbd5df6c75f6c142828227": "Ybodychange",
    "b9ae3087c0f83bfeeea47ded8e19932b46fd2350": "Ybodychange",
    "991c453ca3ac141a3f286f74af8401f83c38b230": "Yintroduced"
  },
  "changeHistoryDetails": {
    "46f1602e896273b308fbd5df6c75f6c142828227": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10448. CacheManager#addInternal tracks bytesNeeded incorrectly when dealing with replication factors other than 1 (Yiqun Lin via cmccabe)\n",
      "commitDate": "20/06/16 6:25 PM",
      "commitName": "46f1602e896273b308fbd5df6c75f6c142828227",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "20/06/16 10:42 AM",
      "commitNameOld": "5f6b4157a40e974ccc6a56c39dbd35c54f393fbd",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.32,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,14 @@\n   private void checkLimit(CachePool pool, String path,\n       short replication) throws InvalidRequestException {\n     CacheDirectiveStats stats \u003d computeNeeded(path, replication);\n     if (pool.getLimit() \u003d\u003d CachePoolInfo.LIMIT_UNLIMITED) {\n       return;\n     }\n-    if (pool.getBytesNeeded() + (stats.getBytesNeeded() * replication) \u003e pool\n-        .getLimit()) {\n+    if (pool.getBytesNeeded() + stats.getBytesNeeded() \u003e pool.getLimit()) {\n       throw new InvalidRequestException(\"Caching path \" + path + \" of size \"\n           + stats.getBytesNeeded() / replication + \" bytes at replication \"\n           + replication + \" would exceed pool \" + pool.getPoolName()\n           + \"\u0027s remaining capacity of \"\n           + (pool.getLimit() - pool.getBytesNeeded()) + \" bytes.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void checkLimit(CachePool pool, String path,\n      short replication) throws InvalidRequestException {\n    CacheDirectiveStats stats \u003d computeNeeded(path, replication);\n    if (pool.getLimit() \u003d\u003d CachePoolInfo.LIMIT_UNLIMITED) {\n      return;\n    }\n    if (pool.getBytesNeeded() + stats.getBytesNeeded() \u003e pool.getLimit()) {\n      throw new InvalidRequestException(\"Caching path \" + path + \" of size \"\n          + stats.getBytesNeeded() / replication + \" bytes at replication \"\n          + replication + \" would exceed pool \" + pool.getPoolName()\n          + \"\u0027s remaining capacity of \"\n          + (pool.getLimit() - pool.getBytesNeeded()) + \" bytes.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "b9ae3087c0f83bfeeea47ded8e19932b46fd2350": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5636. Enforce a max TTL per cache pool (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1552841 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/13 3:27 PM",
      "commitName": "b9ae3087c0f83bfeeea47ded8e19932b46fd2350",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "17/12/13 10:47 AM",
      "commitNameOld": "991c453ca3ac141a3f286f74af8401f83c38b230",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 3.19,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,15 @@\n   private void checkLimit(CachePool pool, String path,\n       short replication) throws InvalidRequestException {\n     CacheDirectiveStats stats \u003d computeNeeded(path, replication);\n+    if (pool.getLimit() \u003d\u003d CachePoolInfo.LIMIT_UNLIMITED) {\n+      return;\n+    }\n     if (pool.getBytesNeeded() + (stats.getBytesNeeded() * replication) \u003e pool\n         .getLimit()) {\n       throw new InvalidRequestException(\"Caching path \" + path + \" of size \"\n           + stats.getBytesNeeded() / replication + \" bytes at replication \"\n           + replication + \" would exceed pool \" + pool.getPoolName()\n           + \"\u0027s remaining capacity of \"\n           + (pool.getLimit() - pool.getBytesNeeded()) + \" bytes.\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void checkLimit(CachePool pool, String path,\n      short replication) throws InvalidRequestException {\n    CacheDirectiveStats stats \u003d computeNeeded(path, replication);\n    if (pool.getLimit() \u003d\u003d CachePoolInfo.LIMIT_UNLIMITED) {\n      return;\n    }\n    if (pool.getBytesNeeded() + (stats.getBytesNeeded() * replication) \u003e pool\n        .getLimit()) {\n      throw new InvalidRequestException(\"Caching path \" + path + \" of size \"\n          + stats.getBytesNeeded() / replication + \" bytes at replication \"\n          + replication + \" would exceed pool \" + pool.getPoolName()\n          + \"\u0027s remaining capacity of \"\n          + (pool.getLimit() - pool.getBytesNeeded()) + \" bytes.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "991c453ca3ac141a3f286f74af8401f83c38b230": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5431. Support cachepool-based limit management in path-based caching. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551651 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 10:47 AM",
      "commitName": "991c453ca3ac141a3f286f74af8401f83c38b230",
      "commitAuthor": "Colin McCabe",
      "diff": "@@ -0,0 +1,12 @@\n+  private void checkLimit(CachePool pool, String path,\n+      short replication) throws InvalidRequestException {\n+    CacheDirectiveStats stats \u003d computeNeeded(path, replication);\n+    if (pool.getBytesNeeded() + (stats.getBytesNeeded() * replication) \u003e pool\n+        .getLimit()) {\n+      throw new InvalidRequestException(\"Caching path \" + path + \" of size \"\n+          + stats.getBytesNeeded() / replication + \" bytes at replication \"\n+          + replication + \" would exceed pool \" + pool.getPoolName()\n+          + \"\u0027s remaining capacity of \"\n+          + (pool.getLimit() - pool.getBytesNeeded()) + \" bytes.\");\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void checkLimit(CachePool pool, String path,\n      short replication) throws InvalidRequestException {\n    CacheDirectiveStats stats \u003d computeNeeded(path, replication);\n    if (pool.getBytesNeeded() + (stats.getBytesNeeded() * replication) \u003e pool\n        .getLimit()) {\n      throw new InvalidRequestException(\"Caching path \" + path + \" of size \"\n          + stats.getBytesNeeded() / replication + \" bytes at replication \"\n          + replication + \" would exceed pool \" + pool.getPoolName()\n          + \"\u0027s remaining capacity of \"\n          + (pool.getLimit() - pool.getBytesNeeded()) + \" bytes.\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java"
    }
  }
}