{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStreamer.java",
  "functionName": "createBlockOutputStream",
  "functionId": "createBlockOutputStream___nodes-DatanodeInfo[]__nodeStorageTypes-StorageType[]__nodeStorageIDs-String[]__newGS-long__recoveryFlag-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
  "functionStartLine": 1703,
  "functionEndLine": 1841,
  "numCommitsSeen": 238,
  "timeTaken": 15637,
  "changeHistory": [
    "c2e9783d5f236015f2ad826fcbad061e2118e454",
    "887244de4adebe27693ed4ad3296a6f700cfa8c1",
    "29b7df960fc3d0a7d1416225c3106c7d4222f0ca",
    "a3954ccab148bddc290cb96528e63ff19799bcc9",
    "627da6f7178e18aa41996969c408b6f344e297d1",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
    "8f378733423a5244461df79a92c00239514b8b93",
    "730f9930a48259f34e48404aee51e8d641cc3d36",
    "4da8490b512a33a255ed27309860859388d7c168",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "7947e5b53b9ac9524b535b0384c1c355b74723ff",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498",
    "67ed59348d638d56e6752ba2c71fdcd69567546d",
    "db6606223ca2e17aa7e1b2e2be13c1a19d8e7465",
    "085b1e293ff53f7a86aa21406cfd4bfa0f3bf33b",
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
    "463aec11718e47d4aabb86a7a539cb973460aae6",
    "c2354a7f81ff5a48a5b65d25e1036d3e0ba86420",
    "6824abc19e12ed142d9f32b8706ef73d97edd1cc",
    "552b4fb9f9a76b18605322c0b0e8072613d67773",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54",
    "3b54223c0f32d42a84436c670d80b791a8e9696d",
    "1fbb04e367d7c330e6052207f9f11911f4f5f368",
    "57b28693ee295746c6d168d37dd05eaf7b601b87",
    "90122f25e142ff5ae9e2610b6b8968ac5fee8f79",
    "5829029154b8e8e02bc6aeb45435046ca080bbe9",
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
    "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21",
    "f98d8eb291be364102b5c3011ce72e8f43eab389",
    "9b4a7900c7dfc0590316eedaa97144f938885651",
    "be7dd8333a7e56e732171db0781786987de03195",
    "41737432c07fb1e1d208b5125fd0fd5205c588cd",
    "1c940637b14eee777a65d153d0d712a1aea3866c",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "504b801ca0e7fd3944872d3214539feb2d614f06",
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "c2e9783d5f236015f2ad826fcbad061e2118e454": "Ybodychange",
    "887244de4adebe27693ed4ad3296a6f700cfa8c1": "Ybodychange",
    "29b7df960fc3d0a7d1416225c3106c7d4222f0ca": "Ybodychange",
    "a3954ccab148bddc290cb96528e63ff19799bcc9": "Ymultichange(Yparameterchange,Ybodychange)",
    "627da6f7178e18aa41996969c408b6f344e297d1": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Ymultichange(Yfilerename,Ybodychange)",
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": "Ybodychange",
    "8f378733423a5244461df79a92c00239514b8b93": "Ybodychange",
    "730f9930a48259f34e48404aee51e8d641cc3d36": "Ybodychange",
    "4da8490b512a33a255ed27309860859388d7c168": "Ybodychange",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "7947e5b53b9ac9524b535b0384c1c355b74723ff": "Ybodychange",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ybodychange",
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": "Ymultichange(Ymovefromfile,Ybodychange)",
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": "Ybodychange",
    "67ed59348d638d56e6752ba2c71fdcd69567546d": "Ybodychange",
    "db6606223ca2e17aa7e1b2e2be13c1a19d8e7465": "Ybodychange",
    "085b1e293ff53f7a86aa21406cfd4bfa0f3bf33b": "Ybodychange",
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": "Ybodychange",
    "463aec11718e47d4aabb86a7a539cb973460aae6": "Ybodychange",
    "c2354a7f81ff5a48a5b65d25e1036d3e0ba86420": "Ybodychange",
    "6824abc19e12ed142d9f32b8706ef73d97edd1cc": "Ybodychange",
    "552b4fb9f9a76b18605322c0b0e8072613d67773": "Ymultichange(Yparameterchange,Ybodychange)",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": "Ymultichange(Yparameterchange,Ybodychange)",
    "3b54223c0f32d42a84436c670d80b791a8e9696d": "Ybodychange",
    "1fbb04e367d7c330e6052207f9f11911f4f5f368": "Ybodychange",
    "57b28693ee295746c6d168d37dd05eaf7b601b87": "Ybodychange",
    "90122f25e142ff5ae9e2610b6b8968ac5fee8f79": "Ybodychange",
    "5829029154b8e8e02bc6aeb45435046ca080bbe9": "Ybodychange",
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24": "Ybodychange",
    "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21": "Ybodychange",
    "f98d8eb291be364102b5c3011ce72e8f43eab389": "Ybodychange",
    "9b4a7900c7dfc0590316eedaa97144f938885651": "Ybodychange",
    "be7dd8333a7e56e732171db0781786987de03195": "Ybodychange",
    "41737432c07fb1e1d208b5125fd0fd5205c588cd": "Ybodychange",
    "1c940637b14eee777a65d153d0d712a1aea3866c": "Ybodychange",
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "504b801ca0e7fd3944872d3214539feb2d614f06": "Ybodychange",
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710": "Ybodychange",
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c2e9783d5f236015f2ad826fcbad061e2118e454": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15045. DataStreamer#createBlockOutputStream() should log exception in warn. Contributed by Ravuri Sushma Sree.\n",
      "commitDate": "10/12/19 7:22 PM",
      "commitName": "c2e9783d5f236015f2ad826fcbad061e2118e454",
      "commitAuthor": "Surendra Singh Lilhore",
      "commitDateOld": "02/05/19 2:25 PM",
      "commitNameOld": "d6b7609c9674c3d0175868d7190293f1925d779b",
      "commitAuthorOld": "Shweta",
      "daysBetweenCommits": 222.25,
      "commitsBetweenForRepo": 1544,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,139 +1,139 @@\n   boolean createBlockOutputStream(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, String[] nodeStorageIDs,\n       long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pipeline \u003d \" + Arrays.toString(nodes) + \", \" + this);\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n         long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag ?\n             stage.getRecoveryStage() : stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d block.getCurrentBlock();\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings,\n             nodeStorageIDs[0], nodeStorageIDs);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelperClient.vintPrefixed(blockReplyStream));\n         Status pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             !errorState.isRestartingNode()) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         errorState.resetInternalError();\n         lastException.clear();\n         // remove all restarting nodes from failed nodes list\n         failed.removeAll(restartingNodes);\n         restartingNodes.clear();\n       } catch (IOException ie) {\n         if (!errorState.isRestartingNode()) {\n-          LOG.info(\"Exception in createBlockOutputStream \" + this, ie);\n+          LOG.warn(\"Exception in createBlockOutputStream \" + this, ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026\n             refetchEncryptionKey \u003e 0) {\n           LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorState.setBadNodeIndex(i);\n               break;\n             }\n           }\n         } else {\n           assert !checkRestart;\n           errorState.setBadNodeIndex(0);\n         }\n \n         final int i \u003d errorState.getBadNodeIndex();\n         // Check whether there is a restart worth waiting for.\n         if (checkRestart) {\n           errorState.initRestartingNode(i,\n               \"Datanode \" + i + \" is restarting: \" + nodes[i],\n               shouldWaitForRestart(i));\n         }\n         errorState.setInternalError();\n         lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, String[] nodeStorageIDs,\n      long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.toString(nodes) + \", \" + this);\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n        long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag ?\n            stage.getRecoveryStage() : stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d block.getCurrentBlock();\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings,\n            nodeStorageIDs[0], nodeStorageIDs);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelperClient.vintPrefixed(blockReplyStream));\n        Status pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            !errorState.isRestartingNode()) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        errorState.resetInternalError();\n        lastException.clear();\n        // remove all restarting nodes from failed nodes list\n        failed.removeAll(restartingNodes);\n        restartingNodes.clear();\n      } catch (IOException ie) {\n        if (!errorState.isRestartingNode()) {\n          LOG.warn(\"Exception in createBlockOutputStream \" + this, ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026\n            refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorState.setBadNodeIndex(i);\n              break;\n            }\n          }\n        } else {\n          assert !checkRestart;\n          errorState.setBadNodeIndex(0);\n        }\n\n        final int i \u003d errorState.getBadNodeIndex();\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart) {\n          errorState.initRestartingNode(i,\n              \"Datanode \" + i + \" is restarting: \" + nodes[i],\n              shouldWaitForRestart(i));\n        }\n        errorState.setInternalError();\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "887244de4adebe27693ed4ad3296a6f700cfa8c1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14048. DFSOutputStream close() throws exception on subsequent call after DataNode restart. Contributed by Erik Krogen.\n",
      "commitDate": "06/11/18 11:18 AM",
      "commitName": "887244de4adebe27693ed4ad3296a6f700cfa8c1",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "17/10/17 3:52 PM",
      "commitNameOld": "f27a4ad0324aa0b4080a1c4c6bf4cd560c927e20",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 384.85,
      "commitsBetweenForRepo": 3491,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,138 +1,139 @@\n   boolean createBlockOutputStream(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, String[] nodeStorageIDs,\n       long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pipeline \u003d \" + Arrays.toString(nodes) + \", \" + this);\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n         long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag ?\n             stage.getRecoveryStage() : stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d block.getCurrentBlock();\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings,\n             nodeStorageIDs[0], nodeStorageIDs);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelperClient.vintPrefixed(blockReplyStream));\n         Status pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             !errorState.isRestartingNode()) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         errorState.resetInternalError();\n+        lastException.clear();\n         // remove all restarting nodes from failed nodes list\n         failed.removeAll(restartingNodes);\n         restartingNodes.clear();\n       } catch (IOException ie) {\n         if (!errorState.isRestartingNode()) {\n           LOG.info(\"Exception in createBlockOutputStream \" + this, ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026\n             refetchEncryptionKey \u003e 0) {\n           LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorState.setBadNodeIndex(i);\n               break;\n             }\n           }\n         } else {\n           assert !checkRestart;\n           errorState.setBadNodeIndex(0);\n         }\n \n         final int i \u003d errorState.getBadNodeIndex();\n         // Check whether there is a restart worth waiting for.\n         if (checkRestart) {\n           errorState.initRestartingNode(i,\n               \"Datanode \" + i + \" is restarting: \" + nodes[i],\n               shouldWaitForRestart(i));\n         }\n         errorState.setInternalError();\n         lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, String[] nodeStorageIDs,\n      long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.toString(nodes) + \", \" + this);\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n        long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag ?\n            stage.getRecoveryStage() : stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d block.getCurrentBlock();\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings,\n            nodeStorageIDs[0], nodeStorageIDs);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelperClient.vintPrefixed(blockReplyStream));\n        Status pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            !errorState.isRestartingNode()) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        errorState.resetInternalError();\n        lastException.clear();\n        // remove all restarting nodes from failed nodes list\n        failed.removeAll(restartingNodes);\n        restartingNodes.clear();\n      } catch (IOException ie) {\n        if (!errorState.isRestartingNode()) {\n          LOG.info(\"Exception in createBlockOutputStream \" + this, ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026\n            refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorState.setBadNodeIndex(i);\n              break;\n            }\n          }\n        } else {\n          assert !checkRestart;\n          errorState.setBadNodeIndex(0);\n        }\n\n        final int i \u003d errorState.getBadNodeIndex();\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart) {\n          errorState.initRestartingNode(i,\n              \"Datanode \" + i + \" is restarting: \" + nodes[i],\n              shouldWaitForRestart(i));\n        }\n        errorState.setInternalError();\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "29b7df960fc3d0a7d1416225c3106c7d4222f0ca": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11856. Ability to re-add Upgrading Nodes to pipeline for future pipeline updates. Contributed by Vinayakumar B.\n",
      "commitDate": "25/05/17 11:05 AM",
      "commitName": "29b7df960fc3d0a7d1416225c3106c7d4222f0ca",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "05/05/17 12:01 PM",
      "commitNameOld": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 19.96,
      "commitsBetweenForRepo": 106,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,134 +1,138 @@\n   boolean createBlockOutputStream(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, String[] nodeStorageIDs,\n       long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pipeline \u003d \" + Arrays.toString(nodes) + \", \" + this);\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n         long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag ?\n             stage.getRecoveryStage() : stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d block.getCurrentBlock();\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings,\n             nodeStorageIDs[0], nodeStorageIDs);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelperClient.vintPrefixed(blockReplyStream));\n         Status pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             !errorState.isRestartingNode()) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         errorState.resetInternalError();\n+        // remove all restarting nodes from failed nodes list\n+        failed.removeAll(restartingNodes);\n+        restartingNodes.clear();\n       } catch (IOException ie) {\n         if (!errorState.isRestartingNode()) {\n           LOG.info(\"Exception in createBlockOutputStream \" + this, ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026\n             refetchEncryptionKey \u003e 0) {\n           LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorState.setBadNodeIndex(i);\n               break;\n             }\n           }\n         } else {\n           assert !checkRestart;\n           errorState.setBadNodeIndex(0);\n         }\n \n         final int i \u003d errorState.getBadNodeIndex();\n         // Check whether there is a restart worth waiting for.\n-        if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n-          errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \"\n-              + nodes[i]);\n+        if (checkRestart) {\n+          errorState.initRestartingNode(i,\n+              \"Datanode \" + i + \" is restarting: \" + nodes[i],\n+              shouldWaitForRestart(i));\n         }\n         errorState.setInternalError();\n         lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, String[] nodeStorageIDs,\n      long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.toString(nodes) + \", \" + this);\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n        long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag ?\n            stage.getRecoveryStage() : stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d block.getCurrentBlock();\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings,\n            nodeStorageIDs[0], nodeStorageIDs);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelperClient.vintPrefixed(blockReplyStream));\n        Status pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            !errorState.isRestartingNode()) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        errorState.resetInternalError();\n        // remove all restarting nodes from failed nodes list\n        failed.removeAll(restartingNodes);\n        restartingNodes.clear();\n      } catch (IOException ie) {\n        if (!errorState.isRestartingNode()) {\n          LOG.info(\"Exception in createBlockOutputStream \" + this, ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026\n            refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorState.setBadNodeIndex(i);\n              break;\n            }\n          }\n        } else {\n          assert !checkRestart;\n          errorState.setBadNodeIndex(0);\n        }\n\n        final int i \u003d errorState.getBadNodeIndex();\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart) {\n          errorState.initRestartingNode(i,\n              \"Datanode \" + i + \" is restarting: \" + nodes[i],\n              shouldWaitForRestart(i));\n        }\n        errorState.setInternalError();\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "a3954ccab148bddc290cb96528e63ff19799bcc9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
      "commitDate": "05/05/17 12:01 PM",
      "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthor": "Chris Douglas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
          "commitDate": "05/05/17 12:01 PM",
          "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "15/02/17 10:44 AM",
          "commitNameOld": "627da6f7178e18aa41996969c408b6f344e297d1",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 79.01,
          "commitsBetweenForRepo": 465,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,132 +1,134 @@\n   boolean createBlockOutputStream(DatanodeInfo[] nodes,\n-      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n+      StorageType[] nodeStorageTypes, String[] nodeStorageIDs,\n+      long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pipeline \u003d \" + Arrays.toString(nodes) + \", \" + this);\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n         long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag ?\n             stage.getRecoveryStage() : stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d block.getCurrentBlock();\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n-            (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings);\n+            (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings,\n+            nodeStorageIDs[0], nodeStorageIDs);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelperClient.vintPrefixed(blockReplyStream));\n         Status pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             !errorState.isRestartingNode()) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         errorState.resetInternalError();\n       } catch (IOException ie) {\n         if (!errorState.isRestartingNode()) {\n           LOG.info(\"Exception in createBlockOutputStream \" + this, ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026\n             refetchEncryptionKey \u003e 0) {\n           LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorState.setBadNodeIndex(i);\n               break;\n             }\n           }\n         } else {\n           assert !checkRestart;\n           errorState.setBadNodeIndex(0);\n         }\n \n         final int i \u003d errorState.getBadNodeIndex();\n         // Check whether there is a restart worth waiting for.\n         if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n           errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \"\n               + nodes[i]);\n         }\n         errorState.setInternalError();\n         lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, String[] nodeStorageIDs,\n      long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.toString(nodes) + \", \" + this);\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n        long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag ?\n            stage.getRecoveryStage() : stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d block.getCurrentBlock();\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings,\n            nodeStorageIDs[0], nodeStorageIDs);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelperClient.vintPrefixed(blockReplyStream));\n        Status pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            !errorState.isRestartingNode()) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        errorState.resetInternalError();\n      } catch (IOException ie) {\n        if (!errorState.isRestartingNode()) {\n          LOG.info(\"Exception in createBlockOutputStream \" + this, ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026\n            refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorState.setBadNodeIndex(i);\n              break;\n            }\n          }\n        } else {\n          assert !checkRestart;\n          errorState.setBadNodeIndex(0);\n        }\n\n        final int i \u003d errorState.getBadNodeIndex();\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n          errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \"\n              + nodes[i]);\n        }\n        errorState.setInternalError();\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {
            "oldValue": "[nodes-DatanodeInfo[], nodeStorageTypes-StorageType[], newGS-long, recoveryFlag-boolean]",
            "newValue": "[nodes-DatanodeInfo[], nodeStorageTypes-StorageType[], nodeStorageIDs-String[], newGS-long, recoveryFlag-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
          "commitDate": "05/05/17 12:01 PM",
          "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "15/02/17 10:44 AM",
          "commitNameOld": "627da6f7178e18aa41996969c408b6f344e297d1",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 79.01,
          "commitsBetweenForRepo": 465,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,132 +1,134 @@\n   boolean createBlockOutputStream(DatanodeInfo[] nodes,\n-      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n+      StorageType[] nodeStorageTypes, String[] nodeStorageIDs,\n+      long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pipeline \u003d \" + Arrays.toString(nodes) + \", \" + this);\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n         long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag ?\n             stage.getRecoveryStage() : stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d block.getCurrentBlock();\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n-            (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings);\n+            (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings,\n+            nodeStorageIDs[0], nodeStorageIDs);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelperClient.vintPrefixed(blockReplyStream));\n         Status pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             !errorState.isRestartingNode()) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         errorState.resetInternalError();\n       } catch (IOException ie) {\n         if (!errorState.isRestartingNode()) {\n           LOG.info(\"Exception in createBlockOutputStream \" + this, ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026\n             refetchEncryptionKey \u003e 0) {\n           LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorState.setBadNodeIndex(i);\n               break;\n             }\n           }\n         } else {\n           assert !checkRestart;\n           errorState.setBadNodeIndex(0);\n         }\n \n         final int i \u003d errorState.getBadNodeIndex();\n         // Check whether there is a restart worth waiting for.\n         if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n           errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \"\n               + nodes[i]);\n         }\n         errorState.setInternalError();\n         lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, String[] nodeStorageIDs,\n      long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.toString(nodes) + \", \" + this);\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n        long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag ?\n            stage.getRecoveryStage() : stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d block.getCurrentBlock();\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings,\n            nodeStorageIDs[0], nodeStorageIDs);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelperClient.vintPrefixed(blockReplyStream));\n        Status pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            !errorState.isRestartingNode()) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        errorState.resetInternalError();\n      } catch (IOException ie) {\n        if (!errorState.isRestartingNode()) {\n          LOG.info(\"Exception in createBlockOutputStream \" + this, ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026\n            refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorState.setBadNodeIndex(i);\n              break;\n            }\n          }\n        } else {\n          assert !checkRestart;\n          errorState.setBadNodeIndex(0);\n        }\n\n        final int i \u003d errorState.getBadNodeIndex();\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n          errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \"\n              + nodes[i]);\n        }\n        errorState.setInternalError();\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {}
        }
      ]
    },
    "627da6f7178e18aa41996969c408b6f344e297d1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8498. Blocks can be committed with wrong size. Contributed by Jing Zhao.\n",
      "commitDate": "15/02/17 10:44 AM",
      "commitName": "627da6f7178e18aa41996969c408b6f344e297d1",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "02/02/17 10:08 AM",
      "commitNameOld": "0914fcca312b5e9d20bcf1b6633bc13c9034ba46",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 13.03,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,132 +1,132 @@\n   boolean createBlockOutputStream(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pipeline \u003d \" + Arrays.toString(nodes) + \", \" + this);\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n         long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag ?\n             stage.getRecoveryStage() : stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n-        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n+        ExtendedBlock blockCopy \u003d block.getCurrentBlock();\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelperClient.vintPrefixed(blockReplyStream));\n         Status pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             !errorState.isRestartingNode()) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         errorState.resetInternalError();\n       } catch (IOException ie) {\n         if (!errorState.isRestartingNode()) {\n           LOG.info(\"Exception in createBlockOutputStream \" + this, ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026\n             refetchEncryptionKey \u003e 0) {\n           LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorState.setBadNodeIndex(i);\n               break;\n             }\n           }\n         } else {\n           assert !checkRestart;\n           errorState.setBadNodeIndex(0);\n         }\n \n         final int i \u003d errorState.getBadNodeIndex();\n         // Check whether there is a restart worth waiting for.\n         if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n           errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \"\n               + nodes[i]);\n         }\n         errorState.setInternalError();\n         lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.toString(nodes) + \", \" + this);\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n        long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag ?\n            stage.getRecoveryStage() : stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d block.getCurrentBlock();\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelperClient.vintPrefixed(blockReplyStream));\n        Status pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            !errorState.isRestartingNode()) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        errorState.resetInternalError();\n      } catch (IOException ie) {\n        if (!errorState.isRestartingNode()) {\n          LOG.info(\"Exception in createBlockOutputStream \" + this, ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026\n            refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorState.setBadNodeIndex(i);\n              break;\n            }\n          }\n        } else {\n          assert !checkRestart;\n          errorState.setBadNodeIndex(0);\n        }\n\n        final int i \u003d errorState.getBadNodeIndex();\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n          errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \"\n              + nodes[i]);\n        }\n        errorState.setInternalError();\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,131 +1,132 @@\n   boolean createBlockOutputStream(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n-    Status pipelineStatus \u003d SUCCESS;\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pipeline \u003d \" + Arrays.toString(nodes) + \", \" + this);\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n         long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n-        BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n+        BlockConstructionStage bcs \u003d recoveryFlag ?\n+            stage.getRecoveryStage() : stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n-        boolean[] targetPinnings \u003d getPinnings(nodes, true);\n+        boolean[] targetPinnings \u003d getPinnings(nodes);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n-            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n+            (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelperClient.vintPrefixed(blockReplyStream));\n-        pipelineStatus \u003d resp.getStatus();\n+        Status pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             !errorState.isRestartingNode()) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n-\t\t\n+\n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         errorState.resetInternalError();\n       } catch (IOException ie) {\n         if (!errorState.isRestartingNode()) {\n           LOG.info(\"Exception in createBlockOutputStream \" + this, ie);\n         }\n-        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n+        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026\n+            refetchEncryptionKey \u003e 0) {\n           LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorState.setBadNodeIndex(i);\n               break;\n             }\n           }\n         } else {\n-          assert checkRestart \u003d\u003d false;\n+          assert !checkRestart;\n           errorState.setBadNodeIndex(0);\n         }\n \n         final int i \u003d errorState.getBadNodeIndex();\n         // Check whether there is a restart worth waiting for.\n         if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n-          errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \" + nodes[i]);\n+          errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \"\n+              + nodes[i]);\n         }\n         errorState.setInternalError();\n         lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n-          out \u003d null;\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.toString(nodes) + \", \" + this);\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n        long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag ?\n            stage.getRecoveryStage() : stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings !\u003d null \u0026\u0026 targetPinnings[0]), targetPinnings);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelperClient.vintPrefixed(blockReplyStream));\n        Status pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            !errorState.isRestartingNode()) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        errorState.resetInternalError();\n      } catch (IOException ie) {\n        if (!errorState.isRestartingNode()) {\n          LOG.info(\"Exception in createBlockOutputStream \" + this, ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026\n            refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorState.setBadNodeIndex(i);\n              break;\n            }\n          }\n        } else {\n          assert !checkRestart;\n          errorState.setBadNodeIndex(0);\n        }\n\n        final int i \u003d errorState.getBadNodeIndex();\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n          errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \"\n              + nodes[i]);\n        }\n        errorState.setInternalError();\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Ymultichange(Yfilerename,Ybodychange)",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "26/09/15 11:08 AM",
          "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "26/09/15 9:06 AM",
          "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,131 +1,131 @@\n   private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n     Status pipelineStatus \u003d SUCCESS;\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n         long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n-            DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n+            DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes, true);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelperClient.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             !errorState.isRestartingNode()) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \t\t\n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         errorState.reset();\n       } catch (IOException ie) {\n         if (!errorState.isRestartingNode()) {\n           LOG.info(\"Exception in createBlockOutputStream\", ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorState.setBadNodeIndex(i);\n               break;\n             }\n           }\n         } else {\n           assert checkRestart \u003d\u003d false;\n           errorState.setBadNodeIndex(0);\n         }\n \n         final int i \u003d errorState.getBadNodeIndex();\n         // Check whether there is a restart worth waiting for.\n         if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n           errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \" + nodes[i]);\n         }\n         errorState.setError(true);\n         lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           out \u003d null;\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    Status pipelineStatus \u003d SUCCESS;\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n        long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes, true);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelperClient.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            !errorState.isRestartingNode()) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\t\t\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        errorState.reset();\n      } catch (IOException ie) {\n        if (!errorState.isRestartingNode()) {\n          LOG.info(\"Exception in createBlockOutputStream\", ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorState.setBadNodeIndex(i);\n              break;\n            }\n          }\n        } else {\n          assert checkRestart \u003d\u003d false;\n          errorState.setBadNodeIndex(0);\n        }\n\n        final int i \u003d errorState.getBadNodeIndex();\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n          errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \" + nodes[i]);\n        }\n        errorState.setError(true);\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "26/09/15 11:08 AM",
          "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "26/09/15 9:06 AM",
          "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,131 +1,131 @@\n   private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n     Status pipelineStatus \u003d SUCCESS;\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n         long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n-            DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n+            DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes, true);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelperClient.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             !errorState.isRestartingNode()) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \t\t\n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         errorState.reset();\n       } catch (IOException ie) {\n         if (!errorState.isRestartingNode()) {\n           LOG.info(\"Exception in createBlockOutputStream\", ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorState.setBadNodeIndex(i);\n               break;\n             }\n           }\n         } else {\n           assert checkRestart \u003d\u003d false;\n           errorState.setBadNodeIndex(0);\n         }\n \n         final int i \u003d errorState.getBadNodeIndex();\n         // Check whether there is a restart worth waiting for.\n         if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n           errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \" + nodes[i]);\n         }\n         errorState.setError(true);\n         lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           out \u003d null;\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    Status pipelineStatus \u003d SUCCESS;\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n        long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes, true);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelperClient.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            !errorState.isRestartingNode()) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\t\t\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        errorState.reset();\n      } catch (IOException ie) {\n        if (!errorState.isRestartingNode()) {\n          LOG.info(\"Exception in createBlockOutputStream\", ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorState.setBadNodeIndex(i);\n              break;\n            }\n          }\n        } else {\n          assert checkRestart \u003d\u003d false;\n          errorState.setBadNodeIndex(0);\n        }\n\n        final int i \u003d errorState.getBadNodeIndex();\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n          errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \" + nodes[i]);\n        }\n        errorState.setError(true);\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {}
        }
      ]
    },
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8934. Move ShortCircuitShm to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "22/08/15 1:31 PM",
      "commitName": "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "18/06/15 8:48 AM",
      "commitNameOld": "1c13519e1e7588c3e2974138d37bf3449ca8b3df",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 65.2,
      "commitsBetweenForRepo": 382,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,131 +1,131 @@\n   private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n     Status pipelineStatus \u003d SUCCESS;\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n         long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes, true);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n-            PBHelper.vintPrefixed(blockReplyStream));\n+            PBHelperClient.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             !errorState.isRestartingNode()) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \t\t\n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         errorState.reset();\n       } catch (IOException ie) {\n         if (!errorState.isRestartingNode()) {\n           LOG.info(\"Exception in createBlockOutputStream\", ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorState.setBadNodeIndex(i);\n               break;\n             }\n           }\n         } else {\n           assert checkRestart \u003d\u003d false;\n           errorState.setBadNodeIndex(0);\n         }\n \n         final int i \u003d errorState.getBadNodeIndex();\n         // Check whether there is a restart worth waiting for.\n         if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n           errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \" + nodes[i]);\n         }\n         errorState.setError(true);\n         lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           out \u003d null;\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    Status pipelineStatus \u003d SUCCESS;\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n        long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes, true);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelperClient.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            !errorState.isRestartingNode()) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\t\t\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        errorState.reset();\n      } catch (IOException ie) {\n        if (!errorState.isRestartingNode()) {\n          LOG.info(\"Exception in createBlockOutputStream\", ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorState.setBadNodeIndex(i);\n              break;\n            }\n          }\n        } else {\n          assert checkRestart \u003d\u003d false;\n          errorState.setBadNodeIndex(0);\n        }\n\n        final int i \u003d errorState.getBadNodeIndex();\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n          errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \" + nodes[i]);\n        }\n        errorState.setError(true);\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "8f378733423a5244461df79a92c00239514b8b93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8397. Refactor the error handling code in DataStreamer. Contributed by Tsz Wo Nicholas Sze.\n",
      "commitDate": "15/05/15 4:14 PM",
      "commitName": "8f378733423a5244461df79a92c00239514b8b93",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "08/05/15 12:11 AM",
      "commitNameOld": "730f9930a48259f34e48404aee51e8d641cc3d36",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 7.67,
      "commitsBetweenForRepo": 95,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,135 +1,131 @@\n   private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n     Status pipelineStatus \u003d SUCCESS;\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n         long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes, true);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelper.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n-            restartingNodeIndex.get() \u003d\u003d -1) {\n+            !errorState.isRestartingNode()) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \t\t\n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n-        restartingNodeIndex.set(-1);\n-        hasError \u003d false;\n+        errorState.reset();\n       } catch (IOException ie) {\n-        if (restartingNodeIndex.get() \u003d\u003d -1) {\n+        if (!errorState.isRestartingNode()) {\n           LOG.info(\"Exception in createBlockOutputStream\", ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n-              errorIndex \u003d i;\n+              errorState.setBadNodeIndex(i);\n               break;\n             }\n           }\n         } else {\n           assert checkRestart \u003d\u003d false;\n-          errorIndex \u003d 0;\n+          errorState.setBadNodeIndex(0);\n         }\n+\n+        final int i \u003d errorState.getBadNodeIndex();\n         // Check whether there is a restart worth waiting for.\n-        if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n-          restartDeadline \u003d dfsClient.getConf().getDatanodeRestartTimeout()\n-              + Time.monotonicNow();\n-          restartingNodeIndex.set(errorIndex);\n-          errorIndex \u003d -1;\n-          LOG.info(\"Waiting for the datanode to be restarted: \" +\n-              nodes[restartingNodeIndex.get()]);\n+        if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n+          errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \" + nodes[i]);\n         }\n-        hasError \u003d true;\n+        errorState.setError(true);\n         lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           out \u003d null;\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    Status pipelineStatus \u003d SUCCESS;\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n        long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes, true);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelper.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            !errorState.isRestartingNode()) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\t\t\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        errorState.reset();\n      } catch (IOException ie) {\n        if (!errorState.isRestartingNode()) {\n          LOG.info(\"Exception in createBlockOutputStream\", ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorState.setBadNodeIndex(i);\n              break;\n            }\n          }\n        } else {\n          assert checkRestart \u003d\u003d false;\n          errorState.setBadNodeIndex(0);\n        }\n\n        final int i \u003d errorState.getBadNodeIndex();\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(i)) {\n          errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \" + nodes[i]);\n        }\n        errorState.setError(true);\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "730f9930a48259f34e48404aee51e8d641cc3d36": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8311. DataStreamer.transfer() should timeout the socket InputStream. (Esteban Gutierrez via Yongjun Zhang)\n",
      "commitDate": "08/05/15 12:11 AM",
      "commitName": "730f9930a48259f34e48404aee51e8d641cc3d36",
      "commitAuthor": "Yongjun Zhang",
      "commitDateOld": "08/05/15 4:48 AM",
      "commitNameOld": "c648317a68891e1c900f04b7a9c98ba40c5faddb",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": -0.19,
      "commitsBetweenForRepo": 0,
      "commitsBetweenForFile": 0,
      "diff": "@@ -1,134 +1,135 @@\n   private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n     Status pipelineStatus \u003d SUCCESS;\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n+        long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n-        InputStream unbufIn \u003d NetUtils.getInputStream(s);\n+        InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes, true);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelper.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex.get() \u003d\u003d -1) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \t\t\n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         restartingNodeIndex.set(-1);\n         hasError \u003d false;\n       } catch (IOException ie) {\n         if (restartingNodeIndex.get() \u003d\u003d -1) {\n           LOG.info(\"Exception in createBlockOutputStream\", ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorIndex \u003d i;\n               break;\n             }\n           }\n         } else {\n           assert checkRestart \u003d\u003d false;\n           errorIndex \u003d 0;\n         }\n         // Check whether there is a restart worth waiting for.\n         if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n           restartDeadline \u003d dfsClient.getConf().getDatanodeRestartTimeout()\n               + Time.monotonicNow();\n           restartingNodeIndex.set(errorIndex);\n           errorIndex \u003d -1;\n           LOG.info(\"Waiting for the datanode to be restarted: \" +\n               nodes[restartingNodeIndex.get()]);\n         }\n         hasError \u003d true;\n         lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           out \u003d null;\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    Status pipelineStatus \u003d SUCCESS;\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n        long readTimeout \u003d dfsClient.getDatanodeReadTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s, readTimeout);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes, true);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelper.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex.get() \u003d\u003d -1) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\t\t\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        restartingNodeIndex.set(-1);\n        hasError \u003d false;\n      } catch (IOException ie) {\n        if (restartingNodeIndex.get() \u003d\u003d -1) {\n          LOG.info(\"Exception in createBlockOutputStream\", ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          assert checkRestart \u003d\u003d false;\n          errorIndex \u003d 0;\n        }\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n          restartDeadline \u003d dfsClient.getConf().getDatanodeRestartTimeout()\n              + Time.monotonicNow();\n          restartingNodeIndex.set(errorIndex);\n          errorIndex \u003d -1;\n          LOG.info(\"Waiting for the datanode to be restarted: \" +\n              nodes[restartingNodeIndex.get()]);\n        }\n        hasError \u003d true;\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "4da8490b512a33a255ed27309860859388d7c168": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8314. Move HdfsServerConstants#IO_FILE_BUFFER_SIZE and SMALL_BUFFER_SIZE to the users. Contributed by Li Lu.\n",
      "commitDate": "05/05/15 3:41 PM",
      "commitName": "4da8490b512a33a255ed27309860859388d7c168",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.23,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,134 +1,134 @@\n   private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n     Status pipelineStatus \u003d SUCCESS;\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n-            HdfsServerConstants.SMALL_BUFFER_SIZE));\n+            DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes, true);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelper.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex.get() \u003d\u003d -1) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \t\t\n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         restartingNodeIndex.set(-1);\n         hasError \u003d false;\n       } catch (IOException ie) {\n         if (restartingNodeIndex.get() \u003d\u003d -1) {\n           LOG.info(\"Exception in createBlockOutputStream\", ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorIndex \u003d i;\n               break;\n             }\n           }\n         } else {\n           assert checkRestart \u003d\u003d false;\n           errorIndex \u003d 0;\n         }\n         // Check whether there is a restart worth waiting for.\n         if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n           restartDeadline \u003d dfsClient.getConf().getDatanodeRestartTimeout()\n               + Time.monotonicNow();\n           restartingNodeIndex.set(errorIndex);\n           errorIndex \u003d -1;\n           LOG.info(\"Waiting for the datanode to be restarted: \" +\n               nodes[restartingNodeIndex.get()]);\n         }\n         hasError \u003d true;\n         lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           out \u003d null;\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    Status pipelineStatus \u003d SUCCESS;\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            DFSUtil.getSmallBufferSize(dfsClient.getConfiguration())));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes, true);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelper.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex.get() \u003d\u003d -1) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\t\t\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        restartingNodeIndex.set(-1);\n        hasError \u003d false;\n      } catch (IOException ie) {\n        if (restartingNodeIndex.get() \u003d\u003d -1) {\n          LOG.info(\"Exception in createBlockOutputStream\", ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          assert checkRestart \u003d\u003d false;\n          errorIndex \u003d 0;\n        }\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n          restartDeadline \u003d dfsClient.getConf().getDatanodeRestartTimeout()\n              + Time.monotonicNow();\n          restartingNodeIndex.set(errorIndex);\n          errorIndex \u003d -1;\n          LOG.info(\"Waiting for the datanode to be restarted: \" +\n              nodes[restartingNodeIndex.get()]);\n        }\n        hasError \u003d true;\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/04/15 7:27 PM",
      "commitNameOld": "98a61766286321468bf801a9f17a843d7eae8d9e",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.61,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,134 +1,134 @@\n   private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n     Status pipelineStatus \u003d SUCCESS;\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n-            HdfsConstants.SMALL_BUFFER_SIZE));\n+            HdfsServerConstants.SMALL_BUFFER_SIZE));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes, true);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelper.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex.get() \u003d\u003d -1) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \t\t\n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         restartingNodeIndex.set(-1);\n         hasError \u003d false;\n       } catch (IOException ie) {\n         if (restartingNodeIndex.get() \u003d\u003d -1) {\n           LOG.info(\"Exception in createBlockOutputStream\", ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorIndex \u003d i;\n               break;\n             }\n           }\n         } else {\n           assert checkRestart \u003d\u003d false;\n           errorIndex \u003d 0;\n         }\n         // Check whether there is a restart worth waiting for.\n         if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n           restartDeadline \u003d dfsClient.getConf().getDatanodeRestartTimeout()\n               + Time.monotonicNow();\n           restartingNodeIndex.set(errorIndex);\n           errorIndex \u003d -1;\n           LOG.info(\"Waiting for the datanode to be restarted: \" +\n               nodes[restartingNodeIndex.get()]);\n         }\n         hasError \u003d true;\n         lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           out \u003d null;\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    Status pipelineStatus \u003d SUCCESS;\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsServerConstants.SMALL_BUFFER_SIZE));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes, true);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelper.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex.get() \u003d\u003d -1) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\t\t\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        restartingNodeIndex.set(-1);\n        hasError \u003d false;\n      } catch (IOException ie) {\n        if (restartingNodeIndex.get() \u003d\u003d -1) {\n          LOG.info(\"Exception in createBlockOutputStream\", ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          assert checkRestart \u003d\u003d false;\n          errorIndex \u003d 0;\n        }\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n          restartDeadline \u003d dfsClient.getConf().getDatanodeRestartTimeout()\n              + Time.monotonicNow();\n          restartingNodeIndex.set(errorIndex);\n          errorIndex \u003d -1;\n          LOG.info(\"Waiting for the datanode to be restarted: \" +\n              nodes[restartingNodeIndex.get()]);\n        }\n        hasError \u003d true;\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "7947e5b53b9ac9524b535b0384c1c355b74723ff": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8283. DataStreamer cleanup and some minor improvement. Contributed by Tsz Wo Nicholas Sze.\n",
      "commitDate": "29/04/15 10:41 AM",
      "commitName": "7947e5b53b9ac9524b535b0384c1c355b74723ff",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/04/15 12:21 AM",
      "commitNameOld": "c8d72907ff5a4cb9ce1effca8ad9b69689d11d1d",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 5.43,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,137 +1,134 @@\n   private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n-      DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n-          + block);\n+      LOG.info(\"nodes are empty for write pipeline of \" + block);\n       return false;\n     }\n     Status pipelineStatus \u003d SUCCESS;\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n-    if (DFSClient.LOG.isDebugEnabled()) {\n-      for (int i \u003d 0; i \u003c nodes.length; i++) {\n-        DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n-      }\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.SMALL_BUFFER_SIZE));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes, true);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelper.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex.get() \u003d\u003d -1) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \t\t\n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         restartingNodeIndex.set(-1);\n         hasError \u003d false;\n       } catch (IOException ie) {\n         if (restartingNodeIndex.get() \u003d\u003d -1) {\n-          DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n+          LOG.info(\"Exception in createBlockOutputStream\", ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n-          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n+          LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorIndex \u003d i;\n               break;\n             }\n           }\n         } else {\n           assert checkRestart \u003d\u003d false;\n           errorIndex \u003d 0;\n         }\n         // Check whether there is a restart worth waiting for.\n         if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n           restartDeadline \u003d dfsClient.getConf().getDatanodeRestartTimeout()\n               + Time.monotonicNow();\n           restartingNodeIndex.set(errorIndex);\n           errorIndex \u003d -1;\n-          DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n+          LOG.info(\"Waiting for the datanode to be restarted: \" +\n               nodes[restartingNodeIndex.get()]);\n         }\n         hasError \u003d true;\n-        setLastException(ie);\n+        lastException.set(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           out \u003d null;\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      LOG.info(\"nodes are empty for write pipeline of \" + block);\n      return false;\n    }\n    Status pipelineStatus \u003d SUCCESS;\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pipeline \u003d \" + Arrays.asList(nodes));\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes, true);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelper.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex.get() \u003d\u003d -1) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\t\t\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        restartingNodeIndex.set(-1);\n        hasError \u003d false;\n      } catch (IOException ie) {\n        if (restartingNodeIndex.get() \u003d\u003d -1) {\n          LOG.info(\"Exception in createBlockOutputStream\", ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          assert checkRestart \u003d\u003d false;\n          errorIndex \u003d 0;\n        }\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n          restartDeadline \u003d dfsClient.getConf().getDatanodeRestartTimeout()\n              + Time.monotonicNow();\n          restartingNodeIndex.set(errorIndex);\n          errorIndex \u003d -1;\n          LOG.info(\"Waiting for the datanode to be restarted: \" +\n              nodes[restartingNodeIndex.get()]);\n        }\n        hasError \u003d true;\n        lastException.set(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "07/04/15 1:59 PM",
      "commitNameOld": "571a1ce9d037d99e7c9042bcb77ae7a2c4daf6d3",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 3.03,
      "commitsBetweenForRepo": 48,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,137 +1,137 @@\n   private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n       StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n     if (nodes.length \u003d\u003d 0) {\n       DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n           + block);\n       return false;\n     }\n     Status pipelineStatus \u003d SUCCESS;\n     String firstBadLink \u003d \"\";\n     boolean checkRestart \u003d false;\n     if (DFSClient.LOG.isDebugEnabled()) {\n       for (int i \u003d 0; i \u003c nodes.length; i++) {\n         DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n       }\n     }\n \n     // persist blocks on namenode on next flush\n     persistBlocks.set(true);\n \n     int refetchEncryptionKey \u003d 1;\n     while (true) {\n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n \n         OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n         InputStream unbufIn \u003d NetUtils.getInputStream(s);\n         IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n         unbufOut \u003d saslStreams.out;\n         unbufIn \u003d saslStreams.in;\n         out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n             HdfsConstants.SMALL_BUFFER_SIZE));\n         blockReplyStream \u003d new DataInputStream(unbufIn);\n \n         //\n         // Xmit header info to datanode\n         //\n \n         BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n         // We cannot change the block length in \u0027block\u0027 as it counts the number\n         // of bytes ack\u0027ed.\n         ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n         blockCopy.setNumBytes(stat.getBlockSize());\n \n         boolean[] targetPinnings \u003d getPinnings(nodes, true);\n         // send the request\n         new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n             dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n             nodes.length, block.getNumBytes(), bytesSent, newGS,\n             checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             PBHelper.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n \n         // Got an restart OOB ack.\n         // If a node is already restarting, this status is not likely from\n         // the same node. If it is from a different node, it is not\n         // from the local datanode. Thus it is safe to treat this as a\n         // regular node error.\n         if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex.get() \u003d\u003d -1) {\n           checkRestart \u003d true;\n           throw new IOException(\"A datanode is restarting.\");\n         }\n \t\t\n         String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n         DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n         restartingNodeIndex.set(-1);\n         hasError \u003d false;\n       } catch (IOException ie) {\n         if (restartingNodeIndex.get() \u003d\u003d -1) {\n           DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n         }\n         if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n           DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n               + \"encryption key was invalid when connecting to \"\n               + nodes[0] + \" : \" + ie);\n           // The encryption key used is invalid.\n           refetchEncryptionKey--;\n           dfsClient.clearDataEncryptionKey();\n           // Don\u0027t close the socket/exclude this node just yet. Try again with\n           // a new encryption key.\n           continue;\n         }\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             // NB: Unconditionally using the xfer addr w/o hostname\n             if (firstBadLink.equals(nodes[i].getXferAddr())) {\n               errorIndex \u003d i;\n               break;\n             }\n           }\n         } else {\n           assert checkRestart \u003d\u003d false;\n           errorIndex \u003d 0;\n         }\n         // Check whether there is a restart worth waiting for.\n         if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n-          restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout\n+          restartDeadline \u003d dfsClient.getConf().getDatanodeRestartTimeout()\n               + Time.monotonicNow();\n           restartingNodeIndex.set(errorIndex);\n           errorIndex \u003d -1;\n           DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n               nodes[restartingNodeIndex.get()]);\n         }\n         hasError \u003d true;\n         setLastException(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           out \u003d null;\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n          + block);\n      return false;\n    }\n    Status pipelineStatus \u003d SUCCESS;\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (DFSClient.LOG.isDebugEnabled()) {\n      for (int i \u003d 0; i \u003c nodes.length; i++) {\n        DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n      }\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes, true);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelper.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex.get() \u003d\u003d -1) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\t\t\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        restartingNodeIndex.set(-1);\n        hasError \u003d false;\n      } catch (IOException ie) {\n        if (restartingNodeIndex.get() \u003d\u003d -1) {\n          DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          assert checkRestart \u003d\u003d false;\n          errorIndex \u003d 0;\n        }\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n          restartDeadline \u003d dfsClient.getConf().getDatanodeRestartTimeout()\n              + Time.monotonicNow();\n          restartingNodeIndex.set(errorIndex);\n          errorIndex \u003d -1;\n          DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n              nodes[restartingNodeIndex.get()]);\n        }\n        hasError \u003d true;\n        setLastException(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
      "extendedDetails": {}
    },
    "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
      "commitDate": "24/03/15 11:06 AM",
      "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "24/03/15 11:06 AM",
          "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/03/15 10:49 AM",
          "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,137 +1,137 @@\n-    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n-        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n-      if (nodes.length \u003d\u003d 0) {\n-        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n-            + block);\n-        return false;\n+  private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n+      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n+    if (nodes.length \u003d\u003d 0) {\n+      DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n+          + block);\n+      return false;\n+    }\n+    Status pipelineStatus \u003d SUCCESS;\n+    String firstBadLink \u003d \"\";\n+    boolean checkRestart \u003d false;\n+    if (DFSClient.LOG.isDebugEnabled()) {\n+      for (int i \u003d 0; i \u003c nodes.length; i++) {\n+        DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n       }\n-      Status pipelineStatus \u003d SUCCESS;\n-      String firstBadLink \u003d \"\";\n-      boolean checkRestart \u003d false;\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        for (int i \u003d 0; i \u003c nodes.length; i++) {\n-          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n-        }\n-      }\n+    }\n \n-      // persist blocks on namenode on next flush\n-      persistBlocks.set(true);\n+    // persist blocks on namenode on next flush\n+    persistBlocks.set(true);\n \n-      int refetchEncryptionKey \u003d 1;\n-      while (true) {\n-        boolean result \u003d false;\n-        DataOutputStream out \u003d null;\n-        try {\n-          assert null \u003d\u003d s : \"Previous socket unclosed\";\n-          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n-          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n-          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n-          \n-          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n-          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n-          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n+    int refetchEncryptionKey \u003d 1;\n+    while (true) {\n+      boolean result \u003d false;\n+      DataOutputStream out \u003d null;\n+      try {\n+        assert null \u003d\u003d s : \"Previous socket unclosed\";\n+        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n+        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n+        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n+\n+        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n+        InputStream unbufIn \u003d NetUtils.getInputStream(s);\n+        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n-          unbufOut \u003d saslStreams.out;\n-          unbufIn \u003d saslStreams.in;\n-          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n-              HdfsConstants.SMALL_BUFFER_SIZE));\n-          blockReplyStream \u003d new DataInputStream(unbufIn);\n-  \n-          //\n-          // Xmit header info to datanode\n-          //\n-  \n-          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n+        unbufOut \u003d saslStreams.out;\n+        unbufIn \u003d saslStreams.in;\n+        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n+            HdfsConstants.SMALL_BUFFER_SIZE));\n+        blockReplyStream \u003d new DataInputStream(unbufIn);\n \n-          // We cannot change the block length in \u0027block\u0027 as it counts the number\n-          // of bytes ack\u0027ed.\n-          ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n-          blockCopy.setNumBytes(blockSize);\n+        //\n+        // Xmit header info to datanode\n+        //\n \n-          boolean[] targetPinnings \u003d getPinnings(nodes, true);\n-          // send the request\n-          new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n-              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n-              nodes.length, block.getNumBytes(), bytesSent, newGS,\n-              checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n+        BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n+\n+        // We cannot change the block length in \u0027block\u0027 as it counts the number\n+        // of bytes ack\u0027ed.\n+        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n+        blockCopy.setNumBytes(stat.getBlockSize());\n+\n+        boolean[] targetPinnings \u003d getPinnings(nodes, true);\n+        // send the request\n+        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n+            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n+            nodes.length, block.getNumBytes(), bytesSent, newGS,\n+            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n-  \n-          // receive ack for connect\n-          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n-              PBHelper.vintPrefixed(blockReplyStream));\n-          pipelineStatus \u003d resp.getStatus();\n-          firstBadLink \u003d resp.getFirstBadLink();\n-          \n-          // Got an restart OOB ack.\n-          // If a node is already restarting, this status is not likely from\n-          // the same node. If it is from a different node, it is not\n-          // from the local datanode. Thus it is safe to treat this as a\n-          // regular node error.\n-          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n+\n+        // receive ack for connect\n+        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n+            PBHelper.vintPrefixed(blockReplyStream));\n+        pipelineStatus \u003d resp.getStatus();\n+        firstBadLink \u003d resp.getFirstBadLink();\n+\n+        // Got an restart OOB ack.\n+        // If a node is already restarting, this status is not likely from\n+        // the same node. If it is from a different node, it is not\n+        // from the local datanode. Thus it is safe to treat this as a\n+        // regular node error.\n+        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex.get() \u003d\u003d -1) {\n-            checkRestart \u003d true;\n-            throw new IOException(\"A datanode is restarting.\");\n-          }\n-\n-          String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n-          DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n-\n-          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n-          blockStream \u003d out;\n-          result \u003d  true; // success\n-          restartingNodeIndex.set(-1);\n-          hasError \u003d false;\n-        } catch (IOException ie) {\n-          if (restartingNodeIndex.get() \u003d\u003d -1) {\n-            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n-          }\n-          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n-            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n-                + \"encryption key was invalid when connecting to \"\n-                + nodes[0] + \" : \" + ie);\n-            // The encryption key used is invalid.\n-            refetchEncryptionKey--;\n-            dfsClient.clearDataEncryptionKey();\n-            // Don\u0027t close the socket/exclude this node just yet. Try again with\n-            // a new encryption key.\n-            continue;\n-          }\n-  \n-          // find the datanode that matches\n-          if (firstBadLink.length() !\u003d 0) {\n-            for (int i \u003d 0; i \u003c nodes.length; i++) {\n-              // NB: Unconditionally using the xfer addr w/o hostname\n-              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n-                errorIndex \u003d i;\n-                break;\n-              }\n-            }\n-          } else {\n-            assert checkRestart \u003d\u003d false;\n-            errorIndex \u003d 0;\n-          }\n-          // Check whether there is a restart worth waiting for.\n-          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n-            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n-                Time.monotonicNow();\n-            restartingNodeIndex.set(errorIndex);\n-            errorIndex \u003d -1;\n-            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n-                nodes[restartingNodeIndex.get()]);\n-          }\n-          hasError \u003d true;\n-          setLastException(ie);\n-          result \u003d  false;  // error\n-        } finally {\n-          if (!result) {\n-            IOUtils.closeSocket(s);\n-            s \u003d null;\n-            IOUtils.closeStream(out);\n-            out \u003d null;\n-            IOUtils.closeStream(blockReplyStream);\n-            blockReplyStream \u003d null;\n-          }\n+          checkRestart \u003d true;\n+          throw new IOException(\"A datanode is restarting.\");\n         }\n-        return result;\n+\t\t\n+        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n+        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n+\n+        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n+        blockStream \u003d out;\n+        result \u003d  true; // success\n+        restartingNodeIndex.set(-1);\n+        hasError \u003d false;\n+      } catch (IOException ie) {\n+        if (restartingNodeIndex.get() \u003d\u003d -1) {\n+          DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n+        }\n+        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n+          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n+              + \"encryption key was invalid when connecting to \"\n+              + nodes[0] + \" : \" + ie);\n+          // The encryption key used is invalid.\n+          refetchEncryptionKey--;\n+          dfsClient.clearDataEncryptionKey();\n+          // Don\u0027t close the socket/exclude this node just yet. Try again with\n+          // a new encryption key.\n+          continue;\n+        }\n+\n+        // find the datanode that matches\n+        if (firstBadLink.length() !\u003d 0) {\n+          for (int i \u003d 0; i \u003c nodes.length; i++) {\n+            // NB: Unconditionally using the xfer addr w/o hostname\n+            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n+              errorIndex \u003d i;\n+              break;\n+            }\n+          }\n+        } else {\n+          assert checkRestart \u003d\u003d false;\n+          errorIndex \u003d 0;\n+        }\n+        // Check whether there is a restart worth waiting for.\n+        if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n+          restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout\n+              + Time.monotonicNow();\n+          restartingNodeIndex.set(errorIndex);\n+          errorIndex \u003d -1;\n+          DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n+              nodes[restartingNodeIndex.get()]);\n+        }\n+        hasError \u003d true;\n+        setLastException(ie);\n+        result \u003d  false;  // error\n+      } finally {\n+        if (!result) {\n+          IOUtils.closeSocket(s);\n+          s \u003d null;\n+          IOUtils.closeStream(out);\n+          out \u003d null;\n+          IOUtils.closeStream(blockReplyStream);\n+          blockReplyStream \u003d null;\n+        }\n       }\n-    }\n\\ No newline at end of file\n+      return result;\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n          + block);\n      return false;\n    }\n    Status pipelineStatus \u003d SUCCESS;\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (DFSClient.LOG.isDebugEnabled()) {\n      for (int i \u003d 0; i \u003c nodes.length; i++) {\n        DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n      }\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes, true);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelper.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex.get() \u003d\u003d -1) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\t\t\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        restartingNodeIndex.set(-1);\n        hasError \u003d false;\n      } catch (IOException ie) {\n        if (restartingNodeIndex.get() \u003d\u003d -1) {\n          DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          assert checkRestart \u003d\u003d false;\n          errorIndex \u003d 0;\n        }\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n          restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout\n              + Time.monotonicNow();\n          restartingNodeIndex.set(errorIndex);\n          errorIndex \u003d -1;\n          DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n              nodes[restartingNodeIndex.get()]);\n        }\n        hasError \u003d true;\n        setLastException(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
            "oldMethodName": "createBlockOutputStream",
            "newMethodName": "createBlockOutputStream"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7854. Separate class DataStreamer out of DFSOutputStream. Contributed by Li Bo.\n",
          "commitDate": "24/03/15 11:06 AM",
          "commitName": "a16bfff71bd7f00e06e1f59bfe5445a154bb8c66",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/03/15 10:49 AM",
          "commitNameOld": "570a83ae80faf2076966acf30588733803327844",
          "commitAuthorOld": "Brandon Li",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,137 +1,137 @@\n-    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n-        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n-      if (nodes.length \u003d\u003d 0) {\n-        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n-            + block);\n-        return false;\n+  private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n+      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n+    if (nodes.length \u003d\u003d 0) {\n+      DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n+          + block);\n+      return false;\n+    }\n+    Status pipelineStatus \u003d SUCCESS;\n+    String firstBadLink \u003d \"\";\n+    boolean checkRestart \u003d false;\n+    if (DFSClient.LOG.isDebugEnabled()) {\n+      for (int i \u003d 0; i \u003c nodes.length; i++) {\n+        DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n       }\n-      Status pipelineStatus \u003d SUCCESS;\n-      String firstBadLink \u003d \"\";\n-      boolean checkRestart \u003d false;\n-      if (DFSClient.LOG.isDebugEnabled()) {\n-        for (int i \u003d 0; i \u003c nodes.length; i++) {\n-          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n-        }\n-      }\n+    }\n \n-      // persist blocks on namenode on next flush\n-      persistBlocks.set(true);\n+    // persist blocks on namenode on next flush\n+    persistBlocks.set(true);\n \n-      int refetchEncryptionKey \u003d 1;\n-      while (true) {\n-        boolean result \u003d false;\n-        DataOutputStream out \u003d null;\n-        try {\n-          assert null \u003d\u003d s : \"Previous socket unclosed\";\n-          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n-          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n-          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n-          \n-          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n-          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n-          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n+    int refetchEncryptionKey \u003d 1;\n+    while (true) {\n+      boolean result \u003d false;\n+      DataOutputStream out \u003d null;\n+      try {\n+        assert null \u003d\u003d s : \"Previous socket unclosed\";\n+        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n+        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n+        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n+\n+        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n+        InputStream unbufIn \u003d NetUtils.getInputStream(s);\n+        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n-          unbufOut \u003d saslStreams.out;\n-          unbufIn \u003d saslStreams.in;\n-          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n-              HdfsConstants.SMALL_BUFFER_SIZE));\n-          blockReplyStream \u003d new DataInputStream(unbufIn);\n-  \n-          //\n-          // Xmit header info to datanode\n-          //\n-  \n-          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n+        unbufOut \u003d saslStreams.out;\n+        unbufIn \u003d saslStreams.in;\n+        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n+            HdfsConstants.SMALL_BUFFER_SIZE));\n+        blockReplyStream \u003d new DataInputStream(unbufIn);\n \n-          // We cannot change the block length in \u0027block\u0027 as it counts the number\n-          // of bytes ack\u0027ed.\n-          ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n-          blockCopy.setNumBytes(blockSize);\n+        //\n+        // Xmit header info to datanode\n+        //\n \n-          boolean[] targetPinnings \u003d getPinnings(nodes, true);\n-          // send the request\n-          new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n-              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n-              nodes.length, block.getNumBytes(), bytesSent, newGS,\n-              checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n+        BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n+\n+        // We cannot change the block length in \u0027block\u0027 as it counts the number\n+        // of bytes ack\u0027ed.\n+        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n+        blockCopy.setNumBytes(stat.getBlockSize());\n+\n+        boolean[] targetPinnings \u003d getPinnings(nodes, true);\n+        // send the request\n+        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n+            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n+            nodes.length, block.getNumBytes(), bytesSent, newGS,\n+            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n-  \n-          // receive ack for connect\n-          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n-              PBHelper.vintPrefixed(blockReplyStream));\n-          pipelineStatus \u003d resp.getStatus();\n-          firstBadLink \u003d resp.getFirstBadLink();\n-          \n-          // Got an restart OOB ack.\n-          // If a node is already restarting, this status is not likely from\n-          // the same node. If it is from a different node, it is not\n-          // from the local datanode. Thus it is safe to treat this as a\n-          // regular node error.\n-          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n+\n+        // receive ack for connect\n+        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n+            PBHelper.vintPrefixed(blockReplyStream));\n+        pipelineStatus \u003d resp.getStatus();\n+        firstBadLink \u003d resp.getFirstBadLink();\n+\n+        // Got an restart OOB ack.\n+        // If a node is already restarting, this status is not likely from\n+        // the same node. If it is from a different node, it is not\n+        // from the local datanode. Thus it is safe to treat this as a\n+        // regular node error.\n+        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex.get() \u003d\u003d -1) {\n-            checkRestart \u003d true;\n-            throw new IOException(\"A datanode is restarting.\");\n-          }\n-\n-          String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n-          DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n-\n-          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n-          blockStream \u003d out;\n-          result \u003d  true; // success\n-          restartingNodeIndex.set(-1);\n-          hasError \u003d false;\n-        } catch (IOException ie) {\n-          if (restartingNodeIndex.get() \u003d\u003d -1) {\n-            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n-          }\n-          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n-            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n-                + \"encryption key was invalid when connecting to \"\n-                + nodes[0] + \" : \" + ie);\n-            // The encryption key used is invalid.\n-            refetchEncryptionKey--;\n-            dfsClient.clearDataEncryptionKey();\n-            // Don\u0027t close the socket/exclude this node just yet. Try again with\n-            // a new encryption key.\n-            continue;\n-          }\n-  \n-          // find the datanode that matches\n-          if (firstBadLink.length() !\u003d 0) {\n-            for (int i \u003d 0; i \u003c nodes.length; i++) {\n-              // NB: Unconditionally using the xfer addr w/o hostname\n-              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n-                errorIndex \u003d i;\n-                break;\n-              }\n-            }\n-          } else {\n-            assert checkRestart \u003d\u003d false;\n-            errorIndex \u003d 0;\n-          }\n-          // Check whether there is a restart worth waiting for.\n-          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n-            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n-                Time.monotonicNow();\n-            restartingNodeIndex.set(errorIndex);\n-            errorIndex \u003d -1;\n-            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n-                nodes[restartingNodeIndex.get()]);\n-          }\n-          hasError \u003d true;\n-          setLastException(ie);\n-          result \u003d  false;  // error\n-        } finally {\n-          if (!result) {\n-            IOUtils.closeSocket(s);\n-            s \u003d null;\n-            IOUtils.closeStream(out);\n-            out \u003d null;\n-            IOUtils.closeStream(blockReplyStream);\n-            blockReplyStream \u003d null;\n-          }\n+          checkRestart \u003d true;\n+          throw new IOException(\"A datanode is restarting.\");\n         }\n-        return result;\n+\t\t\n+        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n+        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n+\n+        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n+        blockStream \u003d out;\n+        result \u003d  true; // success\n+        restartingNodeIndex.set(-1);\n+        hasError \u003d false;\n+      } catch (IOException ie) {\n+        if (restartingNodeIndex.get() \u003d\u003d -1) {\n+          DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n+        }\n+        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n+          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n+              + \"encryption key was invalid when connecting to \"\n+              + nodes[0] + \" : \" + ie);\n+          // The encryption key used is invalid.\n+          refetchEncryptionKey--;\n+          dfsClient.clearDataEncryptionKey();\n+          // Don\u0027t close the socket/exclude this node just yet. Try again with\n+          // a new encryption key.\n+          continue;\n+        }\n+\n+        // find the datanode that matches\n+        if (firstBadLink.length() !\u003d 0) {\n+          for (int i \u003d 0; i \u003c nodes.length; i++) {\n+            // NB: Unconditionally using the xfer addr w/o hostname\n+            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n+              errorIndex \u003d i;\n+              break;\n+            }\n+          }\n+        } else {\n+          assert checkRestart \u003d\u003d false;\n+          errorIndex \u003d 0;\n+        }\n+        // Check whether there is a restart worth waiting for.\n+        if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n+          restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout\n+              + Time.monotonicNow();\n+          restartingNodeIndex.set(errorIndex);\n+          errorIndex \u003d -1;\n+          DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n+              nodes[restartingNodeIndex.get()]);\n+        }\n+        hasError \u003d true;\n+        setLastException(ie);\n+        result \u003d  false;  // error\n+      } finally {\n+        if (!result) {\n+          IOUtils.closeSocket(s);\n+          s \u003d null;\n+          IOUtils.closeStream(out);\n+          out \u003d null;\n+          IOUtils.closeStream(blockReplyStream);\n+          blockReplyStream \u003d null;\n+        }\n       }\n-    }\n\\ No newline at end of file\n+      return result;\n+    }\n+  }\n\\ No newline at end of file\n",
          "actualSource": "  private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n      StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n    if (nodes.length \u003d\u003d 0) {\n      DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n          + block);\n      return false;\n    }\n    Status pipelineStatus \u003d SUCCESS;\n    String firstBadLink \u003d \"\";\n    boolean checkRestart \u003d false;\n    if (DFSClient.LOG.isDebugEnabled()) {\n      for (int i \u003d 0; i \u003c nodes.length; i++) {\n        DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n      }\n    }\n\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n\n    int refetchEncryptionKey \u003d 1;\n    while (true) {\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n        InputStream unbufIn \u003d NetUtils.getInputStream(s);\n        IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n        unbufOut \u003d saslStreams.out;\n        unbufIn \u003d saslStreams.in;\n        out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        blockReplyStream \u003d new DataInputStream(unbufIn);\n\n        //\n        // Xmit header info to datanode\n        //\n\n        BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n        // We cannot change the block length in \u0027block\u0027 as it counts the number\n        // of bytes ack\u0027ed.\n        ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n        blockCopy.setNumBytes(stat.getBlockSize());\n\n        boolean[] targetPinnings \u003d getPinnings(nodes, true);\n        // send the request\n        new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n            dfsClient.clientName, nodes, nodeStorageTypes, null, bcs,\n            nodes.length, block.getNumBytes(), bytesSent, newGS,\n            checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            PBHelper.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n\n        // Got an restart OOB ack.\n        // If a node is already restarting, this status is not likely from\n        // the same node. If it is from a different node, it is not\n        // from the local datanode. Thus it is safe to treat this as a\n        // regular node error.\n        if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex.get() \u003d\u003d -1) {\n          checkRestart \u003d true;\n          throw new IOException(\"A datanode is restarting.\");\n        }\n\t\t\n        String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n        DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n        restartingNodeIndex.set(-1);\n        hasError \u003d false;\n      } catch (IOException ie) {\n        if (restartingNodeIndex.get() \u003d\u003d -1) {\n          DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n        }\n        if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n          DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \"\n              + \"encryption key was invalid when connecting to \"\n              + nodes[0] + \" : \" + ie);\n          // The encryption key used is invalid.\n          refetchEncryptionKey--;\n          dfsClient.clearDataEncryptionKey();\n          // Don\u0027t close the socket/exclude this node just yet. Try again with\n          // a new encryption key.\n          continue;\n        }\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            // NB: Unconditionally using the xfer addr w/o hostname\n            if (firstBadLink.equals(nodes[i].getXferAddr())) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          assert checkRestart \u003d\u003d false;\n          errorIndex \u003d 0;\n        }\n        // Check whether there is a restart worth waiting for.\n        if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n          restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout\n              + Time.monotonicNow();\n          restartingNodeIndex.set(errorIndex);\n          errorIndex \u003d -1;\n          DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n              nodes[restartingNodeIndex.get()]);\n        }\n        hasError \u003d true;\n        setLastException(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java",
          "extendedDetails": {}
        }
      ]
    },
    "75ead273bea8a7dad61c4f99c3a16cab2697c498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6841. Use Time.monotonicNow() wherever applicable instead of Time.now(). Contributed by Vinayakumar B\n",
      "commitDate": "20/03/15 12:02 PM",
      "commitName": "75ead273bea8a7dad61c4f99c3a16cab2697c498",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "20/03/15 9:12 AM",
      "commitNameOld": "15612313f578a5115f8d03885e9b0c8c376ed56e",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 0.12,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,137 +1,137 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n         StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n           unbufOut \u003d saslStreams.out;\n           unbufIn \u003d saslStreams.in;\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n           // We cannot change the block length in \u0027block\u0027 as it counts the number\n           // of bytes ack\u0027ed.\n           ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n           blockCopy.setNumBytes(blockSize);\n \n           boolean[] targetPinnings \u003d getPinnings(nodes, true);\n           // send the request\n           new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n               dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n               nodes.length, block.getNumBytes(), bytesSent, newGS,\n               checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           // Got an restart OOB ack.\n           // If a node is already restarting, this status is not likely from\n           // the same node. If it is from a different node, it is not\n           // from the local datanode. Thus it is safe to treat this as a\n           // regular node error.\n           if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex.get() \u003d\u003d -1) {\n             checkRestart \u003d true;\n             throw new IOException(\"A datanode is restarting.\");\n           }\n \n           String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n           DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n \n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n           restartingNodeIndex.set(-1);\n           hasError \u003d false;\n         } catch (IOException ie) {\n           if (restartingNodeIndex.get() \u003d\u003d -1) {\n             DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n           // Check whether there is a restart worth waiting for.\n           if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n             restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n-                Time.now();\n+                Time.monotonicNow();\n             restartingNodeIndex.set(errorIndex);\n             errorIndex \u003d -1;\n             DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                 nodes[restartingNodeIndex.get()]);\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n          unbufOut \u003d saslStreams.out;\n          unbufIn \u003d saslStreams.in;\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n          // We cannot change the block length in \u0027block\u0027 as it counts the number\n          // of bytes ack\u0027ed.\n          ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n          blockCopy.setNumBytes(blockSize);\n\n          boolean[] targetPinnings \u003d getPinnings(nodes, true);\n          // send the request\n          new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n              nodes.length, block.getNumBytes(), bytesSent, newGS,\n              checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex.get() \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n\n          String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n          DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex.set(-1);\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex.get() \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.monotonicNow();\n            restartingNodeIndex.set(errorIndex);\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex.get()]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "67ed59348d638d56e6752ba2c71fdcd69567546d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7439. Add BlockOpResponseProto\u0027s message to the exception messages.  Contributed by Takanobu Asanuma\n",
      "commitDate": "01/03/15 11:03 PM",
      "commitName": "67ed59348d638d56e6752ba2c71fdcd69567546d",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "27/02/15 7:45 AM",
      "commitNameOld": "a979f3b58fafebbd6118ec1f861cf3f62c59c9cb",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 2.64,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,143 +1,137 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n         StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n           unbufOut \u003d saslStreams.out;\n           unbufIn \u003d saslStreams.in;\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n           // We cannot change the block length in \u0027block\u0027 as it counts the number\n           // of bytes ack\u0027ed.\n           ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n           blockCopy.setNumBytes(blockSize);\n \n           boolean[] targetPinnings \u003d getPinnings(nodes, true);\n           // send the request\n           new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n               dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n               nodes.length, block.getNumBytes(), bytesSent, newGS,\n               checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           // Got an restart OOB ack.\n           // If a node is already restarting, this status is not likely from\n           // the same node. If it is from a different node, it is not\n           // from the local datanode. Thus it is safe to treat this as a\n           // regular node error.\n           if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex.get() \u003d\u003d -1) {\n             checkRestart \u003d true;\n             throw new IOException(\"A datanode is restarting.\");\n           }\n-          if (pipelineStatus !\u003d SUCCESS) {\n-            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n-              throw new InvalidBlockTokenException(\n-                  \"Got access token error for connect ack with firstBadLink as \"\n-                      + firstBadLink);\n-            } else {\n-              throw new IOException(\"Bad connect ack with firstBadLink as \"\n-                  + firstBadLink);\n-            }\n-          }\n+\n+          String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n+          DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n+\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n           restartingNodeIndex.set(-1);\n           hasError \u003d false;\n         } catch (IOException ie) {\n           if (restartingNodeIndex.get() \u003d\u003d -1) {\n             DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n           // Check whether there is a restart worth waiting for.\n           if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n             restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                 Time.now();\n             restartingNodeIndex.set(errorIndex);\n             errorIndex \u003d -1;\n             DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                 nodes[restartingNodeIndex.get()]);\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n          unbufOut \u003d saslStreams.out;\n          unbufIn \u003d saslStreams.in;\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n          // We cannot change the block length in \u0027block\u0027 as it counts the number\n          // of bytes ack\u0027ed.\n          ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n          blockCopy.setNumBytes(blockSize);\n\n          boolean[] targetPinnings \u003d getPinnings(nodes, true);\n          // send the request\n          new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n              nodes.length, block.getNumBytes(), bytesSent, newGS,\n              checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex.get() \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n\n          String logInfo \u003d \"ack with firstBadLink as \" + firstBadLink;\n          DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex.set(-1);\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex.get() \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.now();\n            restartingNodeIndex.set(errorIndex);\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex.get()]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "db6606223ca2e17aa7e1b2e2be13c1a19d8e7465": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7795. Show warning if not all favored nodes were chosen by namenode. Contributed by Kihwal Lee.\n",
      "commitDate": "17/02/15 11:05 AM",
      "commitName": "db6606223ca2e17aa7e1b2e2be13c1a19d8e7465",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "11/02/15 11:08 PM",
      "commitNameOld": "89a544928083501625bc69f96b530040228f0a5f",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 5.5,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,143 +1,143 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n         StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n           unbufOut \u003d saslStreams.out;\n           unbufIn \u003d saslStreams.in;\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n           // We cannot change the block length in \u0027block\u0027 as it counts the number\n           // of bytes ack\u0027ed.\n           ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n           blockCopy.setNumBytes(blockSize);\n \n-          boolean[] targetPinnings \u003d getPinnings(nodes);\n+          boolean[] targetPinnings \u003d getPinnings(nodes, true);\n           // send the request\n           new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n               dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n               nodes.length, block.getNumBytes(), bytesSent, newGS,\n               checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n             (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           // Got an restart OOB ack.\n           // If a node is already restarting, this status is not likely from\n           // the same node. If it is from a different node, it is not\n           // from the local datanode. Thus it is safe to treat this as a\n           // regular node error.\n           if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex.get() \u003d\u003d -1) {\n             checkRestart \u003d true;\n             throw new IOException(\"A datanode is restarting.\");\n           }\n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n           restartingNodeIndex.set(-1);\n           hasError \u003d false;\n         } catch (IOException ie) {\n           if (restartingNodeIndex.get() \u003d\u003d -1) {\n             DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n           // Check whether there is a restart worth waiting for.\n           if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n             restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                 Time.now();\n             restartingNodeIndex.set(errorIndex);\n             errorIndex \u003d -1;\n             DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                 nodes[restartingNodeIndex.get()]);\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n          unbufOut \u003d saslStreams.out;\n          unbufIn \u003d saslStreams.in;\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n          // We cannot change the block length in \u0027block\u0027 as it counts the number\n          // of bytes ack\u0027ed.\n          ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n          blockCopy.setNumBytes(blockSize);\n\n          boolean[] targetPinnings \u003d getPinnings(nodes, true);\n          // send the request\n          new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n              nodes.length, block.getNumBytes(), bytesSent, newGS,\n              checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex.get() \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex.set(-1);\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex.get() \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.now();\n            restartingNodeIndex.set(errorIndex);\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex.get()]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "085b1e293ff53f7a86aa21406cfd4bfa0f3bf33b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6133. Add a feature for replica pinning so that a pinned replica will not be moved by Balancer/Mover.  Contributed by zhaoyunjiong\n",
      "commitDate": "11/02/15 3:12 PM",
      "commitName": "085b1e293ff53f7a86aa21406cfd4bfa0f3bf33b",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "05/02/15 10:58 AM",
      "commitNameOld": "c4980a2f343778544ca20ebea1338651793ea0d9",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 6.18,
      "commitsBetweenForRepo": 66,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,141 +1,143 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n         StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n           unbufOut \u003d saslStreams.out;\n           unbufIn \u003d saslStreams.in;\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n           // We cannot change the block length in \u0027block\u0027 as it counts the number\n           // of bytes ack\u0027ed.\n           ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n           blockCopy.setNumBytes(blockSize);\n \n+          boolean[] targetPinnings \u003d getPinnings(nodes);\n           // send the request\n           new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n               dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n               nodes.length, block.getNumBytes(), bytesSent, newGS,\n-              checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile);\n+              checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n+            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           // Got an restart OOB ack.\n           // If a node is already restarting, this status is not likely from\n           // the same node. If it is from a different node, it is not\n           // from the local datanode. Thus it is safe to treat this as a\n           // regular node error.\n           if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex.get() \u003d\u003d -1) {\n             checkRestart \u003d true;\n             throw new IOException(\"A datanode is restarting.\");\n           }\n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n           restartingNodeIndex.set(-1);\n           hasError \u003d false;\n         } catch (IOException ie) {\n           if (restartingNodeIndex.get() \u003d\u003d -1) {\n             DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n           // Check whether there is a restart worth waiting for.\n           if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n             restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                 Time.now();\n             restartingNodeIndex.set(errorIndex);\n             errorIndex \u003d -1;\n             DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                 nodes[restartingNodeIndex.get()]);\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n          unbufOut \u003d saslStreams.out;\n          unbufIn \u003d saslStreams.in;\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n          // We cannot change the block length in \u0027block\u0027 as it counts the number\n          // of bytes ack\u0027ed.\n          ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n          blockCopy.setNumBytes(blockSize);\n\n          boolean[] targetPinnings \u003d getPinnings(nodes);\n          // send the request\n          new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n              nodes.length, block.getNumBytes(), bytesSent, newGS,\n              checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile,\n            (targetPinnings \u003d\u003d null ? false : targetPinnings[0]), targetPinnings);\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex.get() \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex.set(-1);\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex.get() \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.now();\n            restartingNodeIndex.set(errorIndex);\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex.get()]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "b9f6d0c956f0278c8b9b83e05b523a442a730ebb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7515. Fix new findbugs warnings in hadoop-hdfs. Contributed by Haohui Mai.\n",
      "commitDate": "11/12/14 12:36 PM",
      "commitName": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "17/11/14 6:49 AM",
      "commitNameOld": "6783d17fcf5b25165767888f756a6b7802ab1371",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 24.24,
      "commitsBetweenForRepo": 165,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,141 +1,141 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n         StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n           unbufOut \u003d saslStreams.out;\n           unbufIn \u003d saslStreams.in;\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n           // We cannot change the block length in \u0027block\u0027 as it counts the number\n           // of bytes ack\u0027ed.\n           ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n           blockCopy.setNumBytes(blockSize);\n \n           // send the request\n           new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n               dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n               nodes.length, block.getNumBytes(), bytesSent, newGS,\n               checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile);\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           // Got an restart OOB ack.\n           // If a node is already restarting, this status is not likely from\n           // the same node. If it is from a different node, it is not\n           // from the local datanode. Thus it is safe to treat this as a\n           // regular node error.\n           if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n-            restartingNodeIndex \u003d\u003d -1) {\n+            restartingNodeIndex.get() \u003d\u003d -1) {\n             checkRestart \u003d true;\n             throw new IOException(\"A datanode is restarting.\");\n           }\n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n-          restartingNodeIndex \u003d -1;\n+          restartingNodeIndex.set(-1);\n           hasError \u003d false;\n         } catch (IOException ie) {\n-          if (restartingNodeIndex \u003d\u003d -1) {\n+          if (restartingNodeIndex.get() \u003d\u003d -1) {\n             DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n           // Check whether there is a restart worth waiting for.\n           if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n             restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                 Time.now();\n-            restartingNodeIndex \u003d errorIndex;\n+            restartingNodeIndex.set(errorIndex);\n             errorIndex \u003d -1;\n             DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n-                nodes[restartingNodeIndex]);\n+                nodes[restartingNodeIndex.get()]);\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n          unbufOut \u003d saslStreams.out;\n          unbufIn \u003d saslStreams.in;\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n          // We cannot change the block length in \u0027block\u0027 as it counts the number\n          // of bytes ack\u0027ed.\n          ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n          blockCopy.setNumBytes(blockSize);\n\n          // send the request\n          new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n              nodes.length, block.getNumBytes(), bytesSent, newGS,\n              checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile);\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex.get() \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex.set(-1);\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex.get() \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.now();\n            restartingNodeIndex.set(errorIndex);\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex.get()]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "463aec11718e47d4aabb86a7a539cb973460aae6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6934. Move checksum computation off the hot path when writing to RAM disk. Contributed by Chris Nauroth.\n",
      "commitDate": "27/10/14 9:38 AM",
      "commitName": "463aec11718e47d4aabb86a7a539cb973460aae6",
      "commitAuthor": "cnauroth",
      "commitDateOld": "17/10/14 6:30 PM",
      "commitNameOld": "2e140523d3ccb27809cde4a55e95f7e0006c028f",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 9.63,
      "commitsBetweenForRepo": 71,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,141 +1,141 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n         StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n           unbufOut \u003d saslStreams.out;\n           unbufIn \u003d saslStreams.in;\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n           // We cannot change the block length in \u0027block\u0027 as it counts the number\n           // of bytes ack\u0027ed.\n           ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n           blockCopy.setNumBytes(blockSize);\n \n           // send the request\n           new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n               dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n-              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n-              cachingStrategy.get(), isLazyPersistFile);\n+              nodes.length, block.getNumBytes(), bytesSent, newGS,\n+              checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile);\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           // Got an restart OOB ack.\n           // If a node is already restarting, this status is not likely from\n           // the same node. If it is from a different node, it is not\n           // from the local datanode. Thus it is safe to treat this as a\n           // regular node error.\n           if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex \u003d\u003d -1) {\n             checkRestart \u003d true;\n             throw new IOException(\"A datanode is restarting.\");\n           }\n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n           restartingNodeIndex \u003d -1;\n           hasError \u003d false;\n         } catch (IOException ie) {\n           if (restartingNodeIndex \u003d\u003d -1) {\n             DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n           // Check whether there is a restart worth waiting for.\n           if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n             restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                 Time.now();\n             restartingNodeIndex \u003d errorIndex;\n             errorIndex \u003d -1;\n             DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                 nodes[restartingNodeIndex]);\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n          unbufOut \u003d saslStreams.out;\n          unbufIn \u003d saslStreams.in;\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n          // We cannot change the block length in \u0027block\u0027 as it counts the number\n          // of bytes ack\u0027ed.\n          ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n          blockCopy.setNumBytes(blockSize);\n\n          // send the request\n          new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n              nodes.length, block.getNumBytes(), bytesSent, newGS,\n              checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile);\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex \u003d -1;\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.now();\n            restartingNodeIndex \u003d errorIndex;\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "c2354a7f81ff5a48a5b65d25e1036d3e0ba86420": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6923. Propagate LazyPersist flag to DNs via DataTransferProtocol. (Arpit Agarwal)\n",
      "commitDate": "27/08/14 9:47 PM",
      "commitName": "c2354a7f81ff5a48a5b65d25e1036d3e0ba86420",
      "commitAuthor": "arp",
      "commitDateOld": "20/08/14 6:13 PM",
      "commitNameOld": "6824abc19e12ed142d9f32b8706ef73d97edd1cc",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 7.15,
      "commitsBetweenForRepo": 41,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,141 +1,141 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n         StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n           unbufOut \u003d saslStreams.out;\n           unbufIn \u003d saslStreams.in;\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n \n           // We cannot change the block length in \u0027block\u0027 as it counts the number\n           // of bytes ack\u0027ed.\n           ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n           blockCopy.setNumBytes(blockSize);\n \n           // send the request\n           new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n               dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n               nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n-              cachingStrategy.get());\n+              cachingStrategy.get(), isLazyPersistFile);\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           // Got an restart OOB ack.\n           // If a node is already restarting, this status is not likely from\n           // the same node. If it is from a different node, it is not\n           // from the local datanode. Thus it is safe to treat this as a\n           // regular node error.\n           if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex \u003d\u003d -1) {\n             checkRestart \u003d true;\n             throw new IOException(\"A datanode is restarting.\");\n           }\n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n           restartingNodeIndex \u003d -1;\n           hasError \u003d false;\n         } catch (IOException ie) {\n           if (restartingNodeIndex \u003d\u003d -1) {\n             DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n           // Check whether there is a restart worth waiting for.\n           if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n             restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                 Time.now();\n             restartingNodeIndex \u003d errorIndex;\n             errorIndex \u003d -1;\n             DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                 nodes[restartingNodeIndex]);\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n          unbufOut \u003d saslStreams.out;\n          unbufIn \u003d saslStreams.in;\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n          // We cannot change the block length in \u0027block\u0027 as it counts the number\n          // of bytes ack\u0027ed.\n          ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n          blockCopy.setNumBytes(blockSize);\n\n          // send the request\n          new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n              cachingStrategy.get(), isLazyPersistFile);\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex \u003d -1;\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.now();\n            restartingNodeIndex \u003d errorIndex;\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "6824abc19e12ed142d9f32b8706ef73d97edd1cc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6758. Block writer should pass the expected block size to DataXceiverServer (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1619275 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/08/14 6:13 PM",
      "commitName": "6824abc19e12ed142d9f32b8706ef73d97edd1cc",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "27/07/14 6:32 AM",
      "commitNameOld": "1d3e9ec935de0e5bcb6fda0b88fa69d9e9ce6595",
      "commitAuthorOld": "",
      "daysBetweenCommits": 24.49,
      "commitsBetweenForRepo": 211,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,135 +1,141 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n         StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n           unbufOut \u003d saslStreams.out;\n           unbufIn \u003d saslStreams.in;\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n+\n+          // We cannot change the block length in \u0027block\u0027 as it counts the number\n+          // of bytes ack\u0027ed.\n+          ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n+          blockCopy.setNumBytes(blockSize);\n+\n           // send the request\n-          new Sender(out).writeBlock(block, nodeStorageTypes[0], accessToken,\n+          new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n               dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n               nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n               cachingStrategy.get());\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           // Got an restart OOB ack.\n           // If a node is already restarting, this status is not likely from\n           // the same node. If it is from a different node, it is not\n           // from the local datanode. Thus it is safe to treat this as a\n           // regular node error.\n           if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex \u003d\u003d -1) {\n             checkRestart \u003d true;\n             throw new IOException(\"A datanode is restarting.\");\n           }\n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n           restartingNodeIndex \u003d -1;\n           hasError \u003d false;\n         } catch (IOException ie) {\n           if (restartingNodeIndex \u003d\u003d -1) {\n             DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n           // Check whether there is a restart worth waiting for.\n           if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n             restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                 Time.now();\n             restartingNodeIndex \u003d errorIndex;\n             errorIndex \u003d -1;\n             DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                 nodes[restartingNodeIndex]);\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n          unbufOut \u003d saslStreams.out;\n          unbufIn \u003d saslStreams.in;\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n\n          // We cannot change the block length in \u0027block\u0027 as it counts the number\n          // of bytes ack\u0027ed.\n          ExtendedBlock blockCopy \u003d new ExtendedBlock(block);\n          blockCopy.setNumBytes(blockSize);\n\n          // send the request\n          new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken,\n              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n              cachingStrategy.get());\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex \u003d -1;\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.now();\n            restartingNodeIndex \u003d errorIndex;\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "552b4fb9f9a76b18605322c0b0e8072613d67773": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "Merge from trunk to branch\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1612928 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/07/14 12:26 PM",
      "commitName": "552b4fb9f9a76b18605322c0b0e8072613d67773",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "Merge from trunk to branch\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1612928 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/07/14 12:26 PM",
          "commitName": "552b4fb9f9a76b18605322c0b0e8072613d67773",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "15/07/14 2:10 PM",
          "commitNameOld": "56c0bd4d37ab13b6cbcf860eda852da603ab2f62",
          "commitAuthorOld": "",
          "daysBetweenCommits": 7.93,
          "commitsBetweenForRepo": 61,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,134 +1,135 @@\n-    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n-        boolean recoveryFlag) {\n+    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n+        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n           unbufOut \u003d saslStreams.out;\n           unbufIn \u003d saslStreams.in;\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n+          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n           // send the request\n-          new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n-              nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n+          new Sender(out).writeBlock(block, nodeStorageTypes[0], accessToken,\n+              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n               nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n               cachingStrategy.get());\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           // Got an restart OOB ack.\n           // If a node is already restarting, this status is not likely from\n           // the same node. If it is from a different node, it is not\n           // from the local datanode. Thus it is safe to treat this as a\n           // regular node error.\n           if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex \u003d\u003d -1) {\n             checkRestart \u003d true;\n             throw new IOException(\"A datanode is restarting.\");\n           }\n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n           restartingNodeIndex \u003d -1;\n           hasError \u003d false;\n         } catch (IOException ie) {\n           if (restartingNodeIndex \u003d\u003d -1) {\n             DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n           // Check whether there is a restart worth waiting for.\n           if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n             restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                 Time.now();\n             restartingNodeIndex \u003d errorIndex;\n             errorIndex \u003d -1;\n             DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                 nodes[restartingNodeIndex]);\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n          unbufOut \u003d saslStreams.out;\n          unbufIn \u003d saslStreams.in;\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n          // send the request\n          new Sender(out).writeBlock(block, nodeStorageTypes[0], accessToken,\n              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n              cachingStrategy.get());\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex \u003d -1;\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.now();\n            restartingNodeIndex \u003d errorIndex;\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {
            "oldValue": "[nodes-DatanodeInfo[], newGS-long, recoveryFlag-boolean]",
            "newValue": "[nodes-DatanodeInfo[], nodeStorageTypes-StorageType[], newGS-long, recoveryFlag-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "Merge from trunk to branch\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1612928 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "23/07/14 12:26 PM",
          "commitName": "552b4fb9f9a76b18605322c0b0e8072613d67773",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "15/07/14 2:10 PM",
          "commitNameOld": "56c0bd4d37ab13b6cbcf860eda852da603ab2f62",
          "commitAuthorOld": "",
          "daysBetweenCommits": 7.93,
          "commitsBetweenForRepo": 61,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,134 +1,135 @@\n-    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n-        boolean recoveryFlag) {\n+    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n+        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n           unbufOut \u003d saslStreams.out;\n           unbufIn \u003d saslStreams.in;\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n+          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n           // send the request\n-          new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n-              nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n+          new Sender(out).writeBlock(block, nodeStorageTypes[0], accessToken,\n+              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n               nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n               cachingStrategy.get());\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           // Got an restart OOB ack.\n           // If a node is already restarting, this status is not likely from\n           // the same node. If it is from a different node, it is not\n           // from the local datanode. Thus it is safe to treat this as a\n           // regular node error.\n           if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex \u003d\u003d -1) {\n             checkRestart \u003d true;\n             throw new IOException(\"A datanode is restarting.\");\n           }\n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n           restartingNodeIndex \u003d -1;\n           hasError \u003d false;\n         } catch (IOException ie) {\n           if (restartingNodeIndex \u003d\u003d -1) {\n             DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n           // Check whether there is a restart worth waiting for.\n           if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n             restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                 Time.now();\n             restartingNodeIndex \u003d errorIndex;\n             errorIndex \u003d -1;\n             DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                 nodes[restartingNodeIndex]);\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n          unbufOut \u003d saslStreams.out;\n          unbufIn \u003d saslStreams.in;\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n          // send the request\n          new Sender(out).writeBlock(block, nodeStorageTypes[0], accessToken,\n              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n              cachingStrategy.get());\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex \u003d -1;\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.now();\n            restartingNodeIndex \u003d errorIndex;\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/14 12:41 AM",
      "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/07/14 12:41 AM",
          "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "14/07/14 11:10 AM",
          "commitNameOld": "3b54223c0f32d42a84436c670d80b791a8e9696d",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 7.56,
          "commitsBetweenForRepo": 68,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,134 +1,135 @@\n-    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n-        boolean recoveryFlag) {\n+    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n+        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n           unbufOut \u003d saslStreams.out;\n           unbufIn \u003d saslStreams.in;\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n+          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n           // send the request\n-          new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n-              nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n+          new Sender(out).writeBlock(block, nodeStorageTypes[0], accessToken,\n+              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n               nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n               cachingStrategy.get());\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           // Got an restart OOB ack.\n           // If a node is already restarting, this status is not likely from\n           // the same node. If it is from a different node, it is not\n           // from the local datanode. Thus it is safe to treat this as a\n           // regular node error.\n           if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex \u003d\u003d -1) {\n             checkRestart \u003d true;\n             throw new IOException(\"A datanode is restarting.\");\n           }\n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n           restartingNodeIndex \u003d -1;\n           hasError \u003d false;\n         } catch (IOException ie) {\n           if (restartingNodeIndex \u003d\u003d -1) {\n             DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n           // Check whether there is a restart worth waiting for.\n           if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n             restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                 Time.now();\n             restartingNodeIndex \u003d errorIndex;\n             errorIndex \u003d -1;\n             DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                 nodes[restartingNodeIndex]);\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n          unbufOut \u003d saslStreams.out;\n          unbufIn \u003d saslStreams.in;\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n          // send the request\n          new Sender(out).writeBlock(block, nodeStorageTypes[0], accessToken,\n              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n              cachingStrategy.get());\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex \u003d -1;\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.now();\n            restartingNodeIndex \u003d errorIndex;\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {
            "oldValue": "[nodes-DatanodeInfo[], newGS-long, recoveryFlag-boolean]",
            "newValue": "[nodes-DatanodeInfo[], nodeStorageTypes-StorageType[], newGS-long, recoveryFlag-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/07/14 12:41 AM",
          "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "14/07/14 11:10 AM",
          "commitNameOld": "3b54223c0f32d42a84436c670d80b791a8e9696d",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 7.56,
          "commitsBetweenForRepo": 68,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,134 +1,135 @@\n-    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n-        boolean recoveryFlag) {\n+    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n+        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n             unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n           unbufOut \u003d saslStreams.out;\n           unbufIn \u003d saslStreams.in;\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n+          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n           // send the request\n-          new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n-              nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n+          new Sender(out).writeBlock(block, nodeStorageTypes[0], accessToken,\n+              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n               nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n               cachingStrategy.get());\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           // Got an restart OOB ack.\n           // If a node is already restarting, this status is not likely from\n           // the same node. If it is from a different node, it is not\n           // from the local datanode. Thus it is safe to treat this as a\n           // regular node error.\n           if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex \u003d\u003d -1) {\n             checkRestart \u003d true;\n             throw new IOException(\"A datanode is restarting.\");\n           }\n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n           restartingNodeIndex \u003d -1;\n           hasError \u003d false;\n         } catch (IOException ie) {\n           if (restartingNodeIndex \u003d\u003d -1) {\n             DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n           // Check whether there is a restart worth waiting for.\n           if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n             restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                 Time.now();\n             restartingNodeIndex \u003d errorIndex;\n             errorIndex \u003d -1;\n             DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                 nodes[restartingNodeIndex]);\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes,\n        StorageType[] nodeStorageTypes, long newGS, boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n          unbufOut \u003d saslStreams.out;\n          unbufIn \u003d saslStreams.in;\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          BlockConstructionStage bcs \u003d recoveryFlag? stage.getRecoveryStage(): stage;\n          // send the request\n          new Sender(out).writeBlock(block, nodeStorageTypes[0], accessToken,\n              dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n              cachingStrategy.get());\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex \u003d -1;\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.now();\n            restartingNodeIndex \u003d errorIndex;\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "3b54223c0f32d42a84436c670d80b791a8e9696d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2856. Fix block protocol so that Datanodes don\u0027t require root or jsvc. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1610474 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/07/14 11:10 AM",
      "commitName": "3b54223c0f32d42a84436c670d80b791a8e9696d",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "26/05/14 12:38 PM",
      "commitNameOld": "1228f8f6fb16de4f0283dd1c7939e6fc3dfb7aae",
      "commitAuthorOld": "Michael Stack",
      "daysBetweenCommits": 48.94,
      "commitsBetweenForRepo": 301,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,138 +1,134 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n-          if (dfsClient.shouldEncryptData()  \u0026\u0026 \n-              !dfsClient.trustedChannelResolver.isTrusted(s.getInetAddress())) {\n-            IOStreamPair encryptedStreams \u003d\n-                DataTransferEncryptor.getEncryptedStreams(unbufOut,\n-                    unbufIn, dfsClient.getDataEncryptionKey());\n-            unbufOut \u003d encryptedStreams.out;\n-            unbufIn \u003d encryptedStreams.in;\n-          }\n+          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n+            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n+          unbufOut \u003d saslStreams.out;\n+          unbufIn \u003d saslStreams.in;\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           // send the request\n           new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n               nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n               nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n               cachingStrategy.get());\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           // Got an restart OOB ack.\n           // If a node is already restarting, this status is not likely from\n           // the same node. If it is from a different node, it is not\n           // from the local datanode. Thus it is safe to treat this as a\n           // regular node error.\n           if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex \u003d\u003d -1) {\n             checkRestart \u003d true;\n             throw new IOException(\"A datanode is restarting.\");\n           }\n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n           restartingNodeIndex \u003d -1;\n           hasError \u003d false;\n         } catch (IOException ie) {\n           if (restartingNodeIndex \u003d\u003d -1) {\n             DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n           // Check whether there is a restart worth waiting for.\n           if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n             restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                 Time.now();\n             restartingNodeIndex \u003d errorIndex;\n             errorIndex \u003d -1;\n             DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                 nodes[restartingNodeIndex]);\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          IOStreamPair saslStreams \u003d dfsClient.saslClient.socketSend(s,\n            unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n          unbufOut \u003d saslStreams.out;\n          unbufIn \u003d saslStreams.in;\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          // send the request\n          new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n              nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n              cachingStrategy.get());\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex \u003d -1;\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.now();\n            restartingNodeIndex \u003d errorIndex;\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "1fbb04e367d7c330e6052207f9f11911f4f5f368": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5910. Enhance DataTransferProtocol to allow per-connection choice of encryption/plain-text. (Contributed by Benoy Antony)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1581688 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/03/14 9:11 PM",
      "commitName": "1fbb04e367d7c330e6052207f9f11911f4f5f368",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "24/03/14 4:32 PM",
      "commitNameOld": "c2ef7e239eb0e81cf8a3e971378e9e696202de67",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 1.19,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,137 +1,138 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n-          if (dfsClient.shouldEncryptData()) {\n+          if (dfsClient.shouldEncryptData()  \u0026\u0026 \n+              !dfsClient.trustedChannelResolver.isTrusted(s.getInetAddress())) {\n             IOStreamPair encryptedStreams \u003d\n                 DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                     unbufIn, dfsClient.getDataEncryptionKey());\n             unbufOut \u003d encryptedStreams.out;\n             unbufIn \u003d encryptedStreams.in;\n           }\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           // send the request\n           new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n               nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n               nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n               cachingStrategy.get());\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           // Got an restart OOB ack.\n           // If a node is already restarting, this status is not likely from\n           // the same node. If it is from a different node, it is not\n           // from the local datanode. Thus it is safe to treat this as a\n           // regular node error.\n           if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n             restartingNodeIndex \u003d\u003d -1) {\n             checkRestart \u003d true;\n             throw new IOException(\"A datanode is restarting.\");\n           }\n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n           restartingNodeIndex \u003d -1;\n           hasError \u003d false;\n         } catch (IOException ie) {\n           if (restartingNodeIndex \u003d\u003d -1) {\n             DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n           // Check whether there is a restart worth waiting for.\n           if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n             restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                 Time.now();\n             restartingNodeIndex \u003d errorIndex;\n             errorIndex \u003d -1;\n             DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                 nodes[restartingNodeIndex]);\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          if (dfsClient.shouldEncryptData()  \u0026\u0026 \n              !dfsClient.trustedChannelResolver.isTrusted(s.getInetAddress())) {\n            IOStreamPair encryptedStreams \u003d\n                DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                    unbufIn, dfsClient.getDataEncryptionKey());\n            unbufOut \u003d encryptedStreams.out;\n            unbufIn \u003d encryptedStreams.in;\n          }\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          // send the request\n          new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n              nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n              cachingStrategy.get());\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex \u003d -1;\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.now();\n            restartingNodeIndex \u003d errorIndex;\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "57b28693ee295746c6d168d37dd05eaf7b601b87": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5924. Utilize OOB upgrade message processing for writes. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1571792 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/02/14 11:24 AM",
      "commitName": "57b28693ee295746c6d168d37dd05eaf7b601b87",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "24/02/14 6:16 PM",
      "commitNameOld": "440c3cd1050f2a871a73d44406c0013b6ff73f2e",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.71,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,113 +1,137 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n+      boolean checkRestart \u003d false;\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           if (dfsClient.shouldEncryptData()) {\n             IOStreamPair encryptedStreams \u003d\n                 DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                     unbufIn, dfsClient.getDataEncryptionKey());\n             unbufOut \u003d encryptedStreams.out;\n             unbufIn \u003d encryptedStreams.in;\n           }\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           // send the request\n           new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n               nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n               nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n               cachingStrategy.get());\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n+          // Got an restart OOB ack.\n+          // If a node is already restarting, this status is not likely from\n+          // the same node. If it is from a different node, it is not\n+          // from the local datanode. Thus it is safe to treat this as a\n+          // regular node error.\n+          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n+            restartingNodeIndex \u003d\u003d -1) {\n+            checkRestart \u003d true;\n+            throw new IOException(\"A datanode is restarting.\");\n+          }\n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n-  \n+          restartingNodeIndex \u003d -1;\n+          hasError \u003d false;\n         } catch (IOException ie) {\n-          DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n+          if (restartingNodeIndex \u003d\u003d -1) {\n+            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n+          }\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n+            assert checkRestart \u003d\u003d false;\n             errorIndex \u003d 0;\n           }\n+          // Check whether there is a restart worth waiting for.\n+          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n+            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n+                Time.now();\n+            restartingNodeIndex \u003d errorIndex;\n+            errorIndex \u003d -1;\n+            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n+                nodes[restartingNodeIndex]);\n+          }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      boolean checkRestart \u003d false;\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          if (dfsClient.shouldEncryptData()) {\n            IOStreamPair encryptedStreams \u003d\n                DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                    unbufIn, dfsClient.getDataEncryptionKey());\n            unbufOut \u003d encryptedStreams.out;\n            unbufIn \u003d encryptedStreams.in;\n          }\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          // send the request\n          new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n              nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n              cachingStrategy.get());\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          // Got an restart OOB ack.\n          // If a node is already restarting, this status is not likely from\n          // the same node. If it is from a different node, it is not\n          // from the local datanode. Thus it is safe to treat this as a\n          // regular node error.\n          if (PipelineAck.isRestartOOBStatus(pipelineStatus) \u0026\u0026\n            restartingNodeIndex \u003d\u003d -1) {\n            checkRestart \u003d true;\n            throw new IOException(\"A datanode is restarting.\");\n          }\n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n          restartingNodeIndex \u003d -1;\n          hasError \u003d false;\n        } catch (IOException ie) {\n          if (restartingNodeIndex \u003d\u003d -1) {\n            DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          }\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            assert checkRestart \u003d\u003d false;\n            errorIndex \u003d 0;\n          }\n          // Check whether there is a restart worth waiting for.\n          if (checkRestart \u0026\u0026 shouldWaitForRestart(errorIndex)) {\n            restartDeadline \u003d dfsClient.getConf().datanodeRestartTimeout +\n                Time.now();\n            restartingNodeIndex \u003d errorIndex;\n            errorIndex \u003d -1;\n            DFSClient.LOG.info(\"Waiting for the datanode to be restarted: \" +\n                nodes[restartingNodeIndex]);\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "90122f25e142ff5ae9e2610b6b8968ac5fee8f79": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5676. fix inconsistent synchronization of CachingStrategy (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1552162 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/12/13 3:29 PM",
      "commitName": "90122f25e142ff5ae9e2610b6b8968ac5fee8f79",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "03/12/13 9:33 AM",
      "commitNameOld": "674d51e62e8337d2f4712326eab99f9c83bb652a",
      "commitAuthorOld": "",
      "daysBetweenCommits": 15.25,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,113 +1,113 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       if (nodes.length \u003d\u003d 0) {\n         DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n             + block);\n         return false;\n       }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           if (dfsClient.shouldEncryptData()) {\n             IOStreamPair encryptedStreams \u003d\n                 DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                     unbufIn, dfsClient.getDataEncryptionKey());\n             unbufOut \u003d encryptedStreams.out;\n             unbufIn \u003d encryptedStreams.in;\n           }\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           // send the request\n           new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n               nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n               nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n-              cachingStrategy);\n+              cachingStrategy.get());\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n   \n         } catch (IOException ie) {\n           DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             errorIndex \u003d 0;\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          if (dfsClient.shouldEncryptData()) {\n            IOStreamPair encryptedStreams \u003d\n                DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                    unbufIn, dfsClient.getDataEncryptionKey());\n            unbufOut \u003d encryptedStreams.out;\n            unbufIn \u003d encryptedStreams.in;\n          }\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          // send the request\n          new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n              nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n              cachingStrategy.get());\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n  \n        } catch (IOException ie) {\n          DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            errorIndex \u003d 0;\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "5829029154b8e8e02bc6aeb45435046ca080bbe9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5257. addBlock() retry should return LocatedBlock with locations else client will get AIOBE. Contributed by Vinay.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1535811 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/10/13 11:43 AM",
      "commitName": "5829029154b8e8e02bc6aeb45435046ca080bbe9",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "17/10/13 1:47 PM",
      "commitNameOld": "631ccbdd2031a8387d4c2b743a4fc64c990391ce",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 7.91,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,108 +1,113 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n+      if (nodes.length \u003d\u003d 0) {\n+        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n+            + block);\n+        return false;\n+      }\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           if (dfsClient.shouldEncryptData()) {\n             IOStreamPair encryptedStreams \u003d\n                 DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                     unbufIn, dfsClient.getDataEncryptionKey());\n             unbufOut \u003d encryptedStreams.out;\n             unbufIn \u003d encryptedStreams.in;\n           }\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           // send the request\n           new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n               nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n               nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n               cachingStrategy);\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n   \n         } catch (IOException ie) {\n           DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             errorIndex \u003d 0;\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      if (nodes.length \u003d\u003d 0) {\n        DFSClient.LOG.info(\"nodes are empty for write pipeline of block \"\n            + block);\n        return false;\n      }\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          if (dfsClient.shouldEncryptData()) {\n            IOStreamPair encryptedStreams \u003d\n                DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                    unbufIn, dfsClient.getDataEncryptionKey());\n            unbufOut \u003d encryptedStreams.out;\n            unbufIn \u003d encryptedStreams.in;\n          }\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          // send the request\n          new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n              nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n              cachingStrategy);\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n  \n        } catch (IOException ie) {\n          DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            errorIndex \u003d 0;\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4817.  Make HDFS advisory caching configurable on a per-file basis.  (Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1505753 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/13 11:15 AM",
      "commitName": "c1314eb2a382bd9ce045a2fcc4a9e5c1fc368a24",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "17/07/13 5:04 PM",
      "commitNameOld": "68faa67f1b3b681b40ecdc9002d9fb508e529af4",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 4.76,
      "commitsBetweenForRepo": 24,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,107 +1,108 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           if (dfsClient.shouldEncryptData()) {\n             IOStreamPair encryptedStreams \u003d\n                 DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                     unbufIn, dfsClient.getDataEncryptionKey());\n             unbufOut \u003d encryptedStreams.out;\n             unbufIn \u003d encryptedStreams.in;\n           }\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           // send the request\n           new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n               nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n-              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n+              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n+              cachingStrategy);\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n   \n         } catch (IOException ie) {\n           DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             errorIndex \u003d 0;\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          if (dfsClient.shouldEncryptData()) {\n            IOStreamPair encryptedStreams \u003d\n                DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                    unbufIn, dfsClient.getDataEncryptionKey());\n            unbufOut \u003d encryptedStreams.out;\n            unbufIn \u003d encryptedStreams.in;\n          }\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          // send the request\n          new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n              nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum,\n              cachingStrategy);\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n  \n        } catch (IOException ie) {\n          DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            errorIndex \u003d 0;\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4363. Combine PBHelper and HdfsProtoUtil and remove redundant methods. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1431088 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/01/13 1:20 PM",
      "commitName": "3cd17b614e9436d06cd9b4ccc5f9cf59fbe1cf21",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "30/11/12 11:24 AM",
      "commitNameOld": "571da54179f731eb8421ffc681169799588f76bc",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 40.08,
      "commitsBetweenForRepo": 152,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,107 +1,107 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           if (dfsClient.shouldEncryptData()) {\n             IOStreamPair encryptedStreams \u003d\n                 DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                     unbufIn, dfsClient.getDataEncryptionKey());\n             unbufOut \u003d encryptedStreams.out;\n             unbufIn \u003d encryptedStreams.in;\n           }\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           // send the request\n           new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n               nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n               nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n-              HdfsProtoUtil.vintPrefixed(blockReplyStream));\n+              PBHelper.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n   \n         } catch (IOException ie) {\n           DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n                 + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n               // NB: Unconditionally using the xfer addr w/o hostname\n               if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             errorIndex \u003d 0;\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          if (dfsClient.shouldEncryptData()) {\n            IOStreamPair encryptedStreams \u003d\n                DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                    unbufIn, dfsClient.getDataEncryptionKey());\n            unbufOut \u003d encryptedStreams.out;\n            unbufIn \u003d encryptedStreams.in;\n          }\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          // send the request\n          new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n              nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              PBHelper.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n  \n        } catch (IOException ie) {\n          DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            errorIndex \u003d 0;\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "f98d8eb291be364102b5c3011ce72e8f43eab389": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3150. Add option for clients to contact DNs via hostname. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1373094 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/08/12 1:59 PM",
      "commitName": "f98d8eb291be364102b5c3011ce72e8f43eab389",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "09/08/12 2:31 PM",
      "commitNameOld": "9ea7c06468d236452f03c38a31d1a45f7f09dc50",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 4.98,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,106 +1,107 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       int refetchEncryptionKey \u003d 1;\n       while (true) {\n         boolean result \u003d false;\n         DataOutputStream out \u003d null;\n         try {\n           assert null \u003d\u003d s : \"Previous socket unclosed\";\n           assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n           s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n           long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n           \n           OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n           InputStream unbufIn \u003d NetUtils.getInputStream(s);\n           if (dfsClient.shouldEncryptData()) {\n             IOStreamPair encryptedStreams \u003d\n                 DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                     unbufIn, dfsClient.getDataEncryptionKey());\n             unbufOut \u003d encryptedStreams.out;\n             unbufIn \u003d encryptedStreams.in;\n           }\n           out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n               HdfsConstants.SMALL_BUFFER_SIZE));\n           blockReplyStream \u003d new DataInputStream(unbufIn);\n   \n           //\n           // Xmit header info to datanode\n           //\n   \n           // send the request\n           new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n               nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n               nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n   \n           // receive ack for connect\n           BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n               HdfsProtoUtil.vintPrefixed(blockReplyStream));\n           pipelineStatus \u003d resp.getStatus();\n           firstBadLink \u003d resp.getFirstBadLink();\n           \n           if (pipelineStatus !\u003d SUCCESS) {\n             if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n               throw new InvalidBlockTokenException(\n                   \"Got access token error for connect ack with firstBadLink as \"\n                       + firstBadLink);\n             } else {\n               throw new IOException(\"Bad connect ack with firstBadLink as \"\n                   + firstBadLink);\n             }\n           }\n           assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n           blockStream \u003d out;\n           result \u003d  true; // success\n   \n         } catch (IOException ie) {\n           DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n           if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n             DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                 + \"encryption key was invalid when connecting to \"\n-                + nodes[0].getXferAddr() + \" : \" + ie);\n+                + nodes[0] + \" : \" + ie);\n             // The encryption key used is invalid.\n             refetchEncryptionKey--;\n             dfsClient.clearDataEncryptionKey();\n             // Don\u0027t close the socket/exclude this node just yet. Try again with\n             // a new encryption key.\n             continue;\n           }\n   \n           // find the datanode that matches\n           if (firstBadLink.length() !\u003d 0) {\n             for (int i \u003d 0; i \u003c nodes.length; i++) {\n-              if (nodes[i].getXferAddr().equals(firstBadLink)) {\n+              // NB: Unconditionally using the xfer addr w/o hostname\n+              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                 errorIndex \u003d i;\n                 break;\n               }\n             }\n           } else {\n             errorIndex \u003d 0;\n           }\n           hasError \u003d true;\n           setLastException(ie);\n           result \u003d  false;  // error\n         } finally {\n           if (!result) {\n             IOUtils.closeSocket(s);\n             s \u003d null;\n             IOUtils.closeStream(out);\n             out \u003d null;\n             IOUtils.closeStream(blockReplyStream);\n             blockReplyStream \u003d null;\n           }\n         }\n         return result;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          if (dfsClient.shouldEncryptData()) {\n            IOStreamPair encryptedStreams \u003d\n                DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                    unbufIn, dfsClient.getDataEncryptionKey());\n            unbufOut \u003d encryptedStreams.out;\n            unbufIn \u003d encryptedStreams.in;\n          }\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          // send the request\n          new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n              nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              HdfsProtoUtil.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n  \n        } catch (IOException ie) {\n          DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              // NB: Unconditionally using the xfer addr w/o hostname\n              if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            errorIndex \u003d 0;\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "9b4a7900c7dfc0590316eedaa97144f938885651": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3637. Add support for encrypting the DataTransferProtocol. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1370354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/08/12 9:40 AM",
      "commitName": "9b4a7900c7dfc0590316eedaa97144f938885651",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "26/07/12 5:26 PM",
      "commitNameOld": "c1ea9b4490e7d6d030eeaeeff2fad3767d2cfd4a",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 11.68,
      "commitsBetweenForRepo": 45,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,106 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n-      boolean result \u003d false;\n-      DataOutputStream out \u003d null;\n-      try {\n-        assert null \u003d\u003d s : \"Previous socket unclosed\";\n-        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n-        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n-\n-        //\n-        // Xmit header info to datanode\n-        //\n-        out \u003d new DataOutputStream(new BufferedOutputStream(\n-            NetUtils.getOutputStream(s, writeTimeout),\n-            HdfsConstants.SMALL_BUFFER_SIZE));\n-        \n-        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n-        blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n-\n-        // send the request\n-        new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n-            nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n-            nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n-\n-        // receive ack for connect\n-        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n-            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n-        pipelineStatus \u003d resp.getStatus();\n-        firstBadLink \u003d resp.getFirstBadLink();\n-        \n-        if (pipelineStatus !\u003d SUCCESS) {\n-          if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n-            throw new InvalidBlockTokenException(\n-                \"Got access token error for connect ack with firstBadLink as \"\n-                    + firstBadLink);\n-          } else {\n-            throw new IOException(\"Bad connect ack with firstBadLink as \"\n-                + firstBadLink);\n+      int refetchEncryptionKey \u003d 1;\n+      while (true) {\n+        boolean result \u003d false;\n+        DataOutputStream out \u003d null;\n+        try {\n+          assert null \u003d\u003d s : \"Previous socket unclosed\";\n+          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n+          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n+          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n+          \n+          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n+          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n+          if (dfsClient.shouldEncryptData()) {\n+            IOStreamPair encryptedStreams \u003d\n+                DataTransferEncryptor.getEncryptedStreams(unbufOut,\n+                    unbufIn, dfsClient.getDataEncryptionKey());\n+            unbufOut \u003d encryptedStreams.out;\n+            unbufIn \u003d encryptedStreams.in;\n           }\n-        }\n-        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n-        blockStream \u003d out;\n-        result \u003d  true; // success\n-\n-      } catch (IOException ie) {\n-\n-        DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n-\n-        // find the datanode that matches\n-        if (firstBadLink.length() !\u003d 0) {\n-          for (int i \u003d 0; i \u003c nodes.length; i++) {\n-            if (nodes[i].getXferAddr().equals(firstBadLink)) {\n-              errorIndex \u003d i;\n-              break;\n+          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n+              HdfsConstants.SMALL_BUFFER_SIZE));\n+          blockReplyStream \u003d new DataInputStream(unbufIn);\n+  \n+          //\n+          // Xmit header info to datanode\n+          //\n+  \n+          // send the request\n+          new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n+              nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n+              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n+  \n+          // receive ack for connect\n+          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n+              HdfsProtoUtil.vintPrefixed(blockReplyStream));\n+          pipelineStatus \u003d resp.getStatus();\n+          firstBadLink \u003d resp.getFirstBadLink();\n+          \n+          if (pipelineStatus !\u003d SUCCESS) {\n+            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n+              throw new InvalidBlockTokenException(\n+                  \"Got access token error for connect ack with firstBadLink as \"\n+                      + firstBadLink);\n+            } else {\n+              throw new IOException(\"Bad connect ack with firstBadLink as \"\n+                  + firstBadLink);\n             }\n           }\n-        } else {\n-          errorIndex \u003d 0;\n+          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n+          blockStream \u003d out;\n+          result \u003d  true; // success\n+  \n+        } catch (IOException ie) {\n+          DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n+          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n+            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n+                + \"encryption key was invalid when connecting to \"\n+                + nodes[0].getXferAddr() + \" : \" + ie);\n+            // The encryption key used is invalid.\n+            refetchEncryptionKey--;\n+            dfsClient.clearDataEncryptionKey();\n+            // Don\u0027t close the socket/exclude this node just yet. Try again with\n+            // a new encryption key.\n+            continue;\n+          }\n+  \n+          // find the datanode that matches\n+          if (firstBadLink.length() !\u003d 0) {\n+            for (int i \u003d 0; i \u003c nodes.length; i++) {\n+              if (nodes[i].getXferAddr().equals(firstBadLink)) {\n+                errorIndex \u003d i;\n+                break;\n+              }\n+            }\n+          } else {\n+            errorIndex \u003d 0;\n+          }\n+          hasError \u003d true;\n+          setLastException(ie);\n+          result \u003d  false;  // error\n+        } finally {\n+          if (!result) {\n+            IOUtils.closeSocket(s);\n+            s \u003d null;\n+            IOUtils.closeStream(out);\n+            out \u003d null;\n+            IOUtils.closeStream(blockReplyStream);\n+            blockReplyStream \u003d null;\n+          }\n         }\n-        hasError \u003d true;\n-        setLastException(ie);\n-        result \u003d  false;  // error\n-      } finally {\n-        if (!result) {\n-          IOUtils.closeSocket(s);\n-          s \u003d null;\n-          IOUtils.closeStream(out);\n-          out \u003d null;\n-          IOUtils.closeStream(blockReplyStream);\n-          blockReplyStream \u003d null;\n-        }\n+        return result;\n       }\n-      return result;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      int refetchEncryptionKey \u003d 1;\n      while (true) {\n        boolean result \u003d false;\n        DataOutputStream out \u003d null;\n        try {\n          assert null \u003d\u003d s : \"Previous socket unclosed\";\n          assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n          s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n          long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n          \n          OutputStream unbufOut \u003d NetUtils.getOutputStream(s, writeTimeout);\n          InputStream unbufIn \u003d NetUtils.getInputStream(s);\n          if (dfsClient.shouldEncryptData()) {\n            IOStreamPair encryptedStreams \u003d\n                DataTransferEncryptor.getEncryptedStreams(unbufOut,\n                    unbufIn, dfsClient.getDataEncryptionKey());\n            unbufOut \u003d encryptedStreams.out;\n            unbufIn \u003d encryptedStreams.in;\n          }\n          out \u003d new DataOutputStream(new BufferedOutputStream(unbufOut,\n              HdfsConstants.SMALL_BUFFER_SIZE));\n          blockReplyStream \u003d new DataInputStream(unbufIn);\n  \n          //\n          // Xmit header info to datanode\n          //\n  \n          // send the request\n          new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n              nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n              nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n  \n          // receive ack for connect\n          BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n              HdfsProtoUtil.vintPrefixed(blockReplyStream));\n          pipelineStatus \u003d resp.getStatus();\n          firstBadLink \u003d resp.getFirstBadLink();\n          \n          if (pipelineStatus !\u003d SUCCESS) {\n            if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n              throw new InvalidBlockTokenException(\n                  \"Got access token error for connect ack with firstBadLink as \"\n                      + firstBadLink);\n            } else {\n              throw new IOException(\"Bad connect ack with firstBadLink as \"\n                  + firstBadLink);\n            }\n          }\n          assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n          blockStream \u003d out;\n          result \u003d  true; // success\n  \n        } catch (IOException ie) {\n          DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n          if (ie instanceof InvalidEncryptionKeyException \u0026\u0026 refetchEncryptionKey \u003e 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0].getXferAddr() + \" : \" + ie);\n            // The encryption key used is invalid.\n            refetchEncryptionKey--;\n            dfsClient.clearDataEncryptionKey();\n            // Don\u0027t close the socket/exclude this node just yet. Try again with\n            // a new encryption key.\n            continue;\n          }\n  \n          // find the datanode that matches\n          if (firstBadLink.length() !\u003d 0) {\n            for (int i \u003d 0; i \u003c nodes.length; i++) {\n              if (nodes[i].getXferAddr().equals(firstBadLink)) {\n                errorIndex \u003d i;\n                break;\n              }\n            }\n          } else {\n            errorIndex \u003d 0;\n          }\n          hasError \u003d true;\n          setLastException(ie);\n          result \u003d  false;  // error\n        } finally {\n          if (!result) {\n            IOUtils.closeSocket(s);\n            s \u003d null;\n            IOUtils.closeStream(out);\n            out \u003d null;\n            IOUtils.closeStream(blockReplyStream);\n            blockReplyStream \u003d null;\n          }\n        }\n        return result;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "be7dd8333a7e56e732171db0781786987de03195": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3144. Refactor DatanodeID#getName by use. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/12 3:12 PM",
      "commitName": "be7dd8333a7e56e732171db0781786987de03195",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "01/03/12 3:51 PM",
      "commitNameOld": "b2f67b47044a5cbb0c3aaac83299afba541aa771",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 30.93,
      "commitsBetweenForRepo": 182,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,85 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n-          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n+          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n \n         //\n         // Xmit header info to datanode\n         //\n         out \u003d new DataOutputStream(new BufferedOutputStream(\n             NetUtils.getOutputStream(s, writeTimeout),\n             HdfsConstants.SMALL_BUFFER_SIZE));\n         \n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n \n         // send the request\n         new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n             nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n             nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             HdfsProtoUtil.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n         \n         if (pipelineStatus !\u003d SUCCESS) {\n           if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n             throw new InvalidBlockTokenException(\n                 \"Got access token error for connect ack with firstBadLink as \"\n                     + firstBadLink);\n           } else {\n             throw new IOException(\"Bad connect ack with firstBadLink as \"\n                 + firstBadLink);\n           }\n         }\n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n \n       } catch (IOException ie) {\n \n         DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n-            if (nodes[i].getName().equals(firstBadLink)) {\n+            if (nodes[i].getXferAddr().equals(firstBadLink)) {\n               errorIndex \u003d i;\n               break;\n             }\n           }\n         } else {\n           errorIndex \u003d 0;\n         }\n         hasError \u003d true;\n         setLastException(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           out \u003d null;\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i]);\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        //\n        // Xmit header info to datanode\n        //\n        out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(s, writeTimeout),\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        \n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n\n        // send the request\n        new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n            nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n            nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n        \n        if (pipelineStatus !\u003d SUCCESS) {\n          if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n            throw new InvalidBlockTokenException(\n                \"Got access token error for connect ack with firstBadLink as \"\n                    + firstBadLink);\n          } else {\n            throw new IOException(\"Bad connect ack with firstBadLink as \"\n                + firstBadLink);\n          }\n        }\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n\n      } catch (IOException ie) {\n\n        DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            if (nodes[i].getXferAddr().equals(firstBadLink)) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          errorIndex \u003d 0;\n        }\n        hasError \u003d true;\n        setLastException(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "41737432c07fb1e1d208b5125fd0fd5205c588cd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2726. Fix a logging issue under DFSClient\u0027s createBlockOutputStream method (harsh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1225456 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/12/11 11:22 PM",
      "commitName": "41737432c07fb1e1d208b5125fd0fd5205c588cd",
      "commitAuthor": "Harsh J",
      "commitDateOld": "02/11/11 5:35 PM",
      "commitNameOld": "f84552ac35bb5221290be68fece9c779ebeaf4bc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 56.28,
      "commitsBetweenForRepo": 278,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,85 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n \n         //\n         // Xmit header info to datanode\n         //\n         out \u003d new DataOutputStream(new BufferedOutputStream(\n             NetUtils.getOutputStream(s, writeTimeout),\n             HdfsConstants.SMALL_BUFFER_SIZE));\n         \n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n \n         // send the request\n         new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n             nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n             nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             HdfsProtoUtil.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n         \n         if (pipelineStatus !\u003d SUCCESS) {\n           if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n             throw new InvalidBlockTokenException(\n                 \"Got access token error for connect ack with firstBadLink as \"\n                     + firstBadLink);\n           } else {\n             throw new IOException(\"Bad connect ack with firstBadLink as \"\n                 + firstBadLink);\n           }\n         }\n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n \n       } catch (IOException ie) {\n \n-        DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n+        DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             if (nodes[i].getName().equals(firstBadLink)) {\n               errorIndex \u003d i;\n               break;\n             }\n           }\n         } else {\n           errorIndex \u003d 0;\n         }\n         hasError \u003d true;\n         setLastException(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           out \u003d null;\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        //\n        // Xmit header info to datanode\n        //\n        out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(s, writeTimeout),\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        \n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n\n        // send the request\n        new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n            nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n            nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n        \n        if (pipelineStatus !\u003d SUCCESS) {\n          if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n            throw new InvalidBlockTokenException(\n                \"Got access token error for connect ack with firstBadLink as \"\n                    + firstBadLink);\n          } else {\n            throw new IOException(\"Bad connect ack with firstBadLink as \"\n                + firstBadLink);\n          }\n        }\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n\n      } catch (IOException ie) {\n\n        DFSClient.LOG.info(\"Exception in createBlockOutputStream\", ie);\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            if (nodes[i].getName().equals(firstBadLink)) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          errorIndex \u003d 0;\n        }\n        hasError \u003d true;\n        setLastException(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "1c940637b14eee777a65d153d0d712a1aea3866c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2521. Remove custom checksum headers from data transfer protocol. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195829 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/10/11 10:17 PM",
      "commitName": "1c940637b14eee777a65d153d0d712a1aea3866c",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "15/09/11 8:28 AM",
      "commitNameOld": "376a1a251123699806a3114511bdcc3d9f7bc6f4",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 46.58,
      "commitsBetweenForRepo": 397,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,85 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n \n         //\n         // Xmit header info to datanode\n         //\n         out \u003d new DataOutputStream(new BufferedOutputStream(\n             NetUtils.getOutputStream(s, writeTimeout),\n             HdfsConstants.SMALL_BUFFER_SIZE));\n         \n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n \n         // send the request\n         new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n             nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n-            nodes.length, block.getNumBytes(), bytesSent, newGS);\n-        checksum.writeHeader(out);\n-        out.flush();\n+            nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             HdfsProtoUtil.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n         \n         if (pipelineStatus !\u003d SUCCESS) {\n           if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n             throw new InvalidBlockTokenException(\n                 \"Got access token error for connect ack with firstBadLink as \"\n                     + firstBadLink);\n           } else {\n             throw new IOException(\"Bad connect ack with firstBadLink as \"\n                 + firstBadLink);\n           }\n         }\n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n \n       } catch (IOException ie) {\n \n         DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             if (nodes[i].getName().equals(firstBadLink)) {\n               errorIndex \u003d i;\n               break;\n             }\n           }\n         } else {\n           errorIndex \u003d 0;\n         }\n         hasError \u003d true;\n         setLastException(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           out \u003d null;\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        //\n        // Xmit header info to datanode\n        //\n        out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(s, writeTimeout),\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        \n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n\n        // send the request\n        new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n            nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n            nodes.length, block.getNumBytes(), bytesSent, newGS, checksum);\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n        \n        if (pipelineStatus !\u003d SUCCESS) {\n          if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n            throw new InvalidBlockTokenException(\n                \"Got access token error for connect ack with firstBadLink as \"\n                    + firstBadLink);\n          } else {\n            throw new IOException(\"Bad connect ack with firstBadLink as \"\n                + firstBadLink);\n          }\n        }\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n\n      } catch (IOException ie) {\n\n        DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            if (nodes[i].getName().equals(firstBadLink)) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          errorIndex \u003d 0;\n        }\n        hasError \u003d true;\n        setLastException(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "8ae98a9d1ca4725e28783370517cb3a3ecda7324": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1620. Rename HdfsConstants -\u003e HdfsServerConstants, FSConstants -\u003e HdfsConstants. (Harsh J Chouraria via atm)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165096 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/09/11 12:30 PM",
      "commitName": "8ae98a9d1ca4725e28783370517cb3a3ecda7324",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 10.8,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,87 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       boolean result \u003d false;\n       DataOutputStream out \u003d null;\n       try {\n         assert null \u003d\u003d s : \"Previous socket unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n \n         //\n         // Xmit header info to datanode\n         //\n         out \u003d new DataOutputStream(new BufferedOutputStream(\n             NetUtils.getOutputStream(s, writeTimeout),\n-            FSConstants.SMALL_BUFFER_SIZE));\n+            HdfsConstants.SMALL_BUFFER_SIZE));\n         \n         assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n \n         // send the request\n         new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n             nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n             nodes.length, block.getNumBytes(), bytesSent, newGS);\n         checksum.writeHeader(out);\n         out.flush();\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             HdfsProtoUtil.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n         \n         if (pipelineStatus !\u003d SUCCESS) {\n           if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n             throw new InvalidBlockTokenException(\n                 \"Got access token error for connect ack with firstBadLink as \"\n                     + firstBadLink);\n           } else {\n             throw new IOException(\"Bad connect ack with firstBadLink as \"\n                 + firstBadLink);\n           }\n         }\n         assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n \n       } catch (IOException ie) {\n \n         DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             if (nodes[i].getName().equals(firstBadLink)) {\n               errorIndex \u003d i;\n               break;\n             }\n           }\n         } else {\n           errorIndex \u003d 0;\n         }\n         hasError \u003d true;\n         setLastException(ie);\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n           IOUtils.closeStream(out);\n           out \u003d null;\n           IOUtils.closeStream(blockReplyStream);\n           blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        //\n        // Xmit header info to datanode\n        //\n        out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(s, writeTimeout),\n            HdfsConstants.SMALL_BUFFER_SIZE));\n        \n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n\n        // send the request\n        new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n            nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n            nodes.length, block.getNumBytes(), bytesSent, newGS);\n        checksum.writeHeader(out);\n        out.flush();\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n        \n        if (pipelineStatus !\u003d SUCCESS) {\n          if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n            throw new InvalidBlockTokenException(\n                \"Got access token error for connect ack with firstBadLink as \"\n                    + firstBadLink);\n          } else {\n            throw new IOException(\"Bad connect ack with firstBadLink as \"\n                + firstBadLink);\n          }\n        }\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n\n      } catch (IOException ie) {\n\n        DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            if (nodes[i].getName().equals(firstBadLink)) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          errorIndex \u003d 0;\n        }\n        hasError \u003d true;\n        setLastException(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        //\n        // Xmit header info to datanode\n        //\n        out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(s, writeTimeout),\n            FSConstants.SMALL_BUFFER_SIZE));\n        \n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n\n        // send the request\n        new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n            nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n            nodes.length, block.getNumBytes(), bytesSent, newGS);\n        checksum.writeHeader(out);\n        out.flush();\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n        \n        if (pipelineStatus !\u003d SUCCESS) {\n          if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n            throw new InvalidBlockTokenException(\n                \"Got access token error for connect ack with firstBadLink as \"\n                    + firstBadLink);\n          } else {\n            throw new IOException(\"Bad connect ack with firstBadLink as \"\n                + firstBadLink);\n          }\n        }\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n\n      } catch (IOException ie) {\n\n        DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            if (nodes[i].getName().equals(firstBadLink)) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          errorIndex \u003d 0;\n        }\n        hasError \u003d true;\n        setLastException(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        //\n        // Xmit header info to datanode\n        //\n        out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(s, writeTimeout),\n            FSConstants.SMALL_BUFFER_SIZE));\n        \n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n\n        // send the request\n        new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n            nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n            nodes.length, block.getNumBytes(), bytesSent, newGS);\n        checksum.writeHeader(out);\n        out.flush();\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n        \n        if (pipelineStatus !\u003d SUCCESS) {\n          if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n            throw new InvalidBlockTokenException(\n                \"Got access token error for connect ack with firstBadLink as \"\n                    + firstBadLink);\n          } else {\n            throw new IOException(\"Bad connect ack with firstBadLink as \"\n                + firstBadLink);\n          }\n        }\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n\n      } catch (IOException ie) {\n\n        DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            if (nodes[i].getName().equals(firstBadLink)) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          errorIndex \u003d 0;\n        }\n        hasError \u003d true;\n        setLastException(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
      }
    },
    "504b801ca0e7fd3944872d3214539feb2d614f06": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-73. DFSOutputStream does not close all the sockets. Contributed by Uma Maheswara Rao G\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1157232 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/08/11 12:57 PM",
      "commitName": "504b801ca0e7fd3944872d3214539feb2d614f06",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "10/08/11 5:46 PM",
      "commitNameOld": "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 1.8,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,80 +1,87 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       boolean result \u003d false;\n+      DataOutputStream out \u003d null;\n       try {\n+        assert null \u003d\u003d s : \"Previous socket unclosed\";\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n \n         //\n         // Xmit header info to datanode\n         //\n-        DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n+        out \u003d new DataOutputStream(new BufferedOutputStream(\n             NetUtils.getOutputStream(s, writeTimeout),\n             FSConstants.SMALL_BUFFER_SIZE));\n+        \n+        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n         blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n \n         // send the request\n         new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n             nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n             nodes.length, block.getNumBytes(), bytesSent, newGS);\n         checksum.writeHeader(out);\n         out.flush();\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             HdfsProtoUtil.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n         \n         if (pipelineStatus !\u003d SUCCESS) {\n           if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n             throw new InvalidBlockTokenException(\n                 \"Got access token error for connect ack with firstBadLink as \"\n                     + firstBadLink);\n           } else {\n             throw new IOException(\"Bad connect ack with firstBadLink as \"\n                 + firstBadLink);\n           }\n         }\n-\n+        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n         blockStream \u003d out;\n         result \u003d  true; // success\n \n       } catch (IOException ie) {\n \n         DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             if (nodes[i].getName().equals(firstBadLink)) {\n               errorIndex \u003d i;\n               break;\n             }\n           }\n         } else {\n           errorIndex \u003d 0;\n         }\n         hasError \u003d true;\n         setLastException(ie);\n-        blockReplyStream \u003d null;\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n+          IOUtils.closeStream(out);\n+          out \u003d null;\n+          IOUtils.closeStream(blockReplyStream);\n+          blockReplyStream \u003d null;\n         }\n       }\n       return result;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      boolean result \u003d false;\n      DataOutputStream out \u003d null;\n      try {\n        assert null \u003d\u003d s : \"Previous socket unclosed\";\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        //\n        // Xmit header info to datanode\n        //\n        out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(s, writeTimeout),\n            FSConstants.SMALL_BUFFER_SIZE));\n        \n        assert null \u003d\u003d blockReplyStream : \"Previous blockReplyStream unclosed\";\n        blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n\n        // send the request\n        new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n            nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n            nodes.length, block.getNumBytes(), bytesSent, newGS);\n        checksum.writeHeader(out);\n        out.flush();\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n        \n        if (pipelineStatus !\u003d SUCCESS) {\n          if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n            throw new InvalidBlockTokenException(\n                \"Got access token error for connect ack with firstBadLink as \"\n                    + firstBadLink);\n          } else {\n            throw new IOException(\"Bad connect ack with firstBadLink as \"\n                + firstBadLink);\n          }\n        }\n        assert null \u003d\u003d blockStream : \"Previous blockStream unclosed\";\n        blockStream \u003d out;\n        result \u003d  true; // success\n\n      } catch (IOException ie) {\n\n        DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            if (nodes[i].getName().equals(firstBadLink)) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          errorIndex \u003d 0;\n        }\n        hasError \u003d true;\n        setLastException(ie);\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n          IOUtils.closeStream(out);\n          out \u003d null;\n          IOUtils.closeStream(blockReplyStream);\n          blockReplyStream \u003d null;\n        }\n      }\n      return result;\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2241. Remove implementing FSConstants interface to just get the constants from the interface. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1156420 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/08/11 5:46 PM",
      "commitName": "ef223e8e8e1e18733fc18cd84e34dd0bb0f9a710",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "12/07/11 6:11 PM",
      "commitNameOld": "2c5dd549e31aa5d3377ff2619ede8e92b8dc5d0f",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 28.98,
      "commitsBetweenForRepo": 108,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,80 +1,80 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       boolean result \u003d false;\n       try {\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n \n         //\n         // Xmit header info to datanode\n         //\n         DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n             NetUtils.getOutputStream(s, writeTimeout),\n-            DataNode.SMALL_BUFFER_SIZE));\n+            FSConstants.SMALL_BUFFER_SIZE));\n         blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n \n         // send the request\n         new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n             nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n             nodes.length, block.getNumBytes(), bytesSent, newGS);\n         checksum.writeHeader(out);\n         out.flush();\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             HdfsProtoUtil.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n         \n         if (pipelineStatus !\u003d SUCCESS) {\n           if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n             throw new InvalidBlockTokenException(\n                 \"Got access token error for connect ack with firstBadLink as \"\n                     + firstBadLink);\n           } else {\n             throw new IOException(\"Bad connect ack with firstBadLink as \"\n                 + firstBadLink);\n           }\n         }\n \n         blockStream \u003d out;\n         result \u003d  true; // success\n \n       } catch (IOException ie) {\n \n         DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             if (nodes[i].getName().equals(firstBadLink)) {\n               errorIndex \u003d i;\n               break;\n             }\n           }\n         } else {\n           errorIndex \u003d 0;\n         }\n         hasError \u003d true;\n         setLastException(ie);\n         blockReplyStream \u003d null;\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n         }\n       }\n       return result;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      boolean result \u003d false;\n      try {\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        //\n        // Xmit header info to datanode\n        //\n        DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(s, writeTimeout),\n            FSConstants.SMALL_BUFFER_SIZE));\n        blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n\n        // send the request\n        new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n            nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n            nodes.length, block.getNumBytes(), bytesSent, newGS);\n        checksum.writeHeader(out);\n        out.flush();\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n        \n        if (pipelineStatus !\u003d SUCCESS) {\n          if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n            throw new InvalidBlockTokenException(\n                \"Got access token error for connect ack with firstBadLink as \"\n                    + firstBadLink);\n          } else {\n            throw new IOException(\"Bad connect ack with firstBadLink as \"\n                + firstBadLink);\n          }\n        }\n\n        blockStream \u003d out;\n        result \u003d  true; // success\n\n      } catch (IOException ie) {\n\n        DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            if (nodes[i].getName().equals(firstBadLink)) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          errorIndex \u003d 0;\n        }\n        hasError \u003d true;\n        setLastException(ie);\n        blockReplyStream \u003d null;\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n        }\n      }\n      return result;\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "2f48fae72aa52e6ec42264cad24fab36b6a426c2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2087. Declare methods in DataTransferProtocol interface, and change Sender and Receiver to implement the interface.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139124 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/06/11 4:57 PM",
      "commitName": "2f48fae72aa52e6ec42264cad24fab36b6a426c2",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "23/06/11 3:24 PM",
      "commitNameOld": "fd9997989c1f1c6f806c57a806e7225ca599fc0c",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,81 +1,80 @@\n     private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n         boolean recoveryFlag) {\n       Status pipelineStatus \u003d SUCCESS;\n       String firstBadLink \u003d \"\";\n       if (DFSClient.LOG.isDebugEnabled()) {\n         for (int i \u003d 0; i \u003c nodes.length; i++) {\n           DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n         }\n       }\n \n       // persist blocks on namenode on next flush\n       persistBlocks.set(true);\n \n       boolean result \u003d false;\n       try {\n         s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n         long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n \n         //\n         // Xmit header info to datanode\n         //\n         DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n             NetUtils.getOutputStream(s, writeTimeout),\n             DataNode.SMALL_BUFFER_SIZE));\n         blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n \n         // send the request\n-        Sender.opWriteBlock(out, block,\n-            nodes.length, recoveryFlag ? stage.getRecoveryStage() : stage, newGS, \n-            block.getNumBytes(), bytesSent, dfsClient.clientName, null, nodes,\n-            accessToken);\n+        new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n+            nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n+            nodes.length, block.getNumBytes(), bytesSent, newGS);\n         checksum.writeHeader(out);\n         out.flush();\n \n         // receive ack for connect\n         BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n             HdfsProtoUtil.vintPrefixed(blockReplyStream));\n         pipelineStatus \u003d resp.getStatus();\n         firstBadLink \u003d resp.getFirstBadLink();\n         \n         if (pipelineStatus !\u003d SUCCESS) {\n           if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n             throw new InvalidBlockTokenException(\n                 \"Got access token error for connect ack with firstBadLink as \"\n                     + firstBadLink);\n           } else {\n             throw new IOException(\"Bad connect ack with firstBadLink as \"\n                 + firstBadLink);\n           }\n         }\n \n         blockStream \u003d out;\n         result \u003d  true; // success\n \n       } catch (IOException ie) {\n \n         DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n \n         // find the datanode that matches\n         if (firstBadLink.length() !\u003d 0) {\n           for (int i \u003d 0; i \u003c nodes.length; i++) {\n             if (nodes[i].getName().equals(firstBadLink)) {\n               errorIndex \u003d i;\n               break;\n             }\n           }\n         } else {\n           errorIndex \u003d 0;\n         }\n         hasError \u003d true;\n         setLastException(ie);\n         blockReplyStream \u003d null;\n         result \u003d  false;  // error\n       } finally {\n         if (!result) {\n           IOUtils.closeSocket(s);\n           s \u003d null;\n         }\n       }\n       return result;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      boolean result \u003d false;\n      try {\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        //\n        // Xmit header info to datanode\n        //\n        DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(s, writeTimeout),\n            DataNode.SMALL_BUFFER_SIZE));\n        blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n\n        // send the request\n        new Sender(out).writeBlock(block, accessToken, dfsClient.clientName,\n            nodes, null, recoveryFlag? stage.getRecoveryStage() : stage, \n            nodes.length, block.getNumBytes(), bytesSent, newGS);\n        checksum.writeHeader(out);\n        out.flush();\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n        \n        if (pipelineStatus !\u003d SUCCESS) {\n          if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n            throw new InvalidBlockTokenException(\n                \"Got access token error for connect ack with firstBadLink as \"\n                    + firstBadLink);\n          } else {\n            throw new IOException(\"Bad connect ack with firstBadLink as \"\n                + firstBadLink);\n          }\n        }\n\n        blockStream \u003d out;\n        result \u003d  true; // success\n\n      } catch (IOException ie) {\n\n        DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            if (nodes[i].getName().equals(firstBadLink)) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          errorIndex \u003d 0;\n        }\n        hasError \u003d true;\n        setLastException(ie);\n        blockReplyStream \u003d null;\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n        }\n      }\n      return result;\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,81 @@\n+    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n+        boolean recoveryFlag) {\n+      Status pipelineStatus \u003d SUCCESS;\n+      String firstBadLink \u003d \"\";\n+      if (DFSClient.LOG.isDebugEnabled()) {\n+        for (int i \u003d 0; i \u003c nodes.length; i++) {\n+          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n+        }\n+      }\n+\n+      // persist blocks on namenode on next flush\n+      persistBlocks.set(true);\n+\n+      boolean result \u003d false;\n+      try {\n+        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n+        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n+\n+        //\n+        // Xmit header info to datanode\n+        //\n+        DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n+            NetUtils.getOutputStream(s, writeTimeout),\n+            DataNode.SMALL_BUFFER_SIZE));\n+        blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n+\n+        // send the request\n+        Sender.opWriteBlock(out, block,\n+            nodes.length, recoveryFlag ? stage.getRecoveryStage() : stage, newGS, \n+            block.getNumBytes(), bytesSent, dfsClient.clientName, null, nodes,\n+            accessToken);\n+        checksum.writeHeader(out);\n+        out.flush();\n+\n+        // receive ack for connect\n+        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n+            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n+        pipelineStatus \u003d resp.getStatus();\n+        firstBadLink \u003d resp.getFirstBadLink();\n+        \n+        if (pipelineStatus !\u003d SUCCESS) {\n+          if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n+            throw new InvalidBlockTokenException(\n+                \"Got access token error for connect ack with firstBadLink as \"\n+                    + firstBadLink);\n+          } else {\n+            throw new IOException(\"Bad connect ack with firstBadLink as \"\n+                + firstBadLink);\n+          }\n+        }\n+\n+        blockStream \u003d out;\n+        result \u003d  true; // success\n+\n+      } catch (IOException ie) {\n+\n+        DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n+\n+        // find the datanode that matches\n+        if (firstBadLink.length() !\u003d 0) {\n+          for (int i \u003d 0; i \u003c nodes.length; i++) {\n+            if (nodes[i].getName().equals(firstBadLink)) {\n+              errorIndex \u003d i;\n+              break;\n+            }\n+          }\n+        } else {\n+          errorIndex \u003d 0;\n+        }\n+        hasError \u003d true;\n+        setLastException(ie);\n+        blockReplyStream \u003d null;\n+        result \u003d  false;  // error\n+      } finally {\n+        if (!result) {\n+          IOUtils.closeSocket(s);\n+          s \u003d null;\n+        }\n+      }\n+      return result;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private boolean createBlockOutputStream(DatanodeInfo[] nodes, long newGS,\n        boolean recoveryFlag) {\n      Status pipelineStatus \u003d SUCCESS;\n      String firstBadLink \u003d \"\";\n      if (DFSClient.LOG.isDebugEnabled()) {\n        for (int i \u003d 0; i \u003c nodes.length; i++) {\n          DFSClient.LOG.debug(\"pipeline \u003d \" + nodes[i].getName());\n        }\n      }\n\n      // persist blocks on namenode on next flush\n      persistBlocks.set(true);\n\n      boolean result \u003d false;\n      try {\n        s \u003d createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n        long writeTimeout \u003d dfsClient.getDatanodeWriteTimeout(nodes.length);\n\n        //\n        // Xmit header info to datanode\n        //\n        DataOutputStream out \u003d new DataOutputStream(new BufferedOutputStream(\n            NetUtils.getOutputStream(s, writeTimeout),\n            DataNode.SMALL_BUFFER_SIZE));\n        blockReplyStream \u003d new DataInputStream(NetUtils.getInputStream(s));\n\n        // send the request\n        Sender.opWriteBlock(out, block,\n            nodes.length, recoveryFlag ? stage.getRecoveryStage() : stage, newGS, \n            block.getNumBytes(), bytesSent, dfsClient.clientName, null, nodes,\n            accessToken);\n        checksum.writeHeader(out);\n        out.flush();\n\n        // receive ack for connect\n        BlockOpResponseProto resp \u003d BlockOpResponseProto.parseFrom(\n            HdfsProtoUtil.vintPrefixed(blockReplyStream));\n        pipelineStatus \u003d resp.getStatus();\n        firstBadLink \u003d resp.getFirstBadLink();\n        \n        if (pipelineStatus !\u003d SUCCESS) {\n          if (pipelineStatus \u003d\u003d Status.ERROR_ACCESS_TOKEN) {\n            throw new InvalidBlockTokenException(\n                \"Got access token error for connect ack with firstBadLink as \"\n                    + firstBadLink);\n          } else {\n            throw new IOException(\"Bad connect ack with firstBadLink as \"\n                + firstBadLink);\n          }\n        }\n\n        blockStream \u003d out;\n        result \u003d  true; // success\n\n      } catch (IOException ie) {\n\n        DFSClient.LOG.info(\"Exception in createBlockOutputStream \" + ie);\n\n        // find the datanode that matches\n        if (firstBadLink.length() !\u003d 0) {\n          for (int i \u003d 0; i \u003c nodes.length; i++) {\n            if (nodes[i].getName().equals(firstBadLink)) {\n              errorIndex \u003d i;\n              break;\n            }\n          }\n        } else {\n          errorIndex \u003d 0;\n        }\n        hasError \u003d true;\n        setLastException(ie);\n        blockReplyStream \u003d null;\n        result \u003d  false;  // error\n      } finally {\n        if (!result) {\n          IOUtils.closeSocket(s);\n          s \u003d null;\n        }\n      }\n      return result;\n    }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSOutputStream.java"
    }
  }
}