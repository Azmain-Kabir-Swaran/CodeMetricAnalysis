{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LineRecordReader.java",
  "functionName": "nextKeyValue",
  "functionId": "nextKeyValue",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
  "functionStartLine": 185,
  "functionEndLine": 219,
  "numCommitsSeen": 15,
  "timeTaken": 5059,
  "changeHistory": [
    "40ba8c17c1500703c47a154f06708e5924c24e65",
    "4bb4de93d67873b47fd90c61396f52315165c7bf",
    "18d99c12c371cfd7b9604e321d8bd6a7be9c4977",
    "947e97f354edb1e27432cf3f1a2dff098f01071f",
    "6f0c4dca74cab26275d48c05fedd9bc2aa57878f",
    "b55756dd03086f6c081991d949ebcf7926af8af5",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "4796e1adcb912005198c9003305c97cf3a8b523e",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "40ba8c17c1500703c47a154f06708e5924c24e65": "Ybodychange",
    "4bb4de93d67873b47fd90c61396f52315165c7bf": "Ybodychange",
    "18d99c12c371cfd7b9604e321d8bd6a7be9c4977": "Ybodychange",
    "947e97f354edb1e27432cf3f1a2dff098f01071f": "Ybodychange",
    "6f0c4dca74cab26275d48c05fedd9bc2aa57878f": "Ybodychange",
    "b55756dd03086f6c081991d949ebcf7926af8af5": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "4796e1adcb912005198c9003305c97cf3a8b523e": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "40ba8c17c1500703c47a154f06708e5924c24e65": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5777. Support utf-8 text with Byte Order Marker. (Zhihai Xu via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1600977 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/06/14 11:37 AM",
      "commitName": "40ba8c17c1500703c47a154f06708e5924c24e65",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "28/05/14 12:37 PM",
      "commitNameOld": "4bb4de93d67873b47fd90c61396f52315165c7bf",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 8.96,
      "commitsBetweenForRepo": 48,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,35 @@\n   public boolean nextKeyValue() throws IOException {\n     if (key \u003d\u003d null) {\n       key \u003d new LongWritable();\n     }\n     key.set(pos);\n     if (value \u003d\u003d null) {\n       value \u003d new Text();\n     }\n     int newSize \u003d 0;\n     // We always read one extra line, which lies outside the upper\n     // split limit i.e. (end - 1)\n     while (getFilePosition() \u003c\u003d end || in.needAdditionalRecordAfterSplit()) {\n-      newSize \u003d in.readLine(value, maxLineLength, maxBytesToConsume(pos));\n-      pos +\u003d newSize;\n-      if (newSize \u003c maxLineLength) {\n+      if (pos \u003d\u003d 0) {\n+        newSize \u003d skipUtfByteOrderMark();\n+      } else {\n+        newSize \u003d in.readLine(value, maxLineLength, maxBytesToConsume(pos));\n+        pos +\u003d newSize;\n+      }\n+\n+      if ((newSize \u003d\u003d 0) || (newSize \u003c maxLineLength)) {\n         break;\n       }\n \n       // line too long. try again\n       LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n                (pos - newSize));\n     }\n     if (newSize \u003d\u003d 0) {\n       key \u003d null;\n       value \u003d null;\n       return false;\n     } else {\n       return true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean nextKeyValue() throws IOException {\n    if (key \u003d\u003d null) {\n      key \u003d new LongWritable();\n    }\n    key.set(pos);\n    if (value \u003d\u003d null) {\n      value \u003d new Text();\n    }\n    int newSize \u003d 0;\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end || in.needAdditionalRecordAfterSplit()) {\n      if (pos \u003d\u003d 0) {\n        newSize \u003d skipUtfByteOrderMark();\n      } else {\n        newSize \u003d in.readLine(value, maxLineLength, maxBytesToConsume(pos));\n        pos +\u003d newSize;\n      }\n\n      if ((newSize \u003d\u003d 0) || (newSize \u003c maxLineLength)) {\n        break;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n               (pos - newSize));\n    }\n    if (newSize \u003d\u003d 0) {\n      key \u003d null;\n      value \u003d null;\n      return false;\n    } else {\n      return true;\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "4bb4de93d67873b47fd90c61396f52315165c7bf": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5862. Line records longer than 2x split size aren\u0027t handled correctly. Contributed by bc Wong\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1598111 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/05/14 12:37 PM",
      "commitName": "4bb4de93d67873b47fd90c61396f52315165c7bf",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "09/12/13 3:31 PM",
      "commitNameOld": "18d99c12c371cfd7b9604e321d8bd6a7be9c4977",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 169.84,
      "commitsBetweenForRepo": 1155,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,30 @@\n   public boolean nextKeyValue() throws IOException {\n     if (key \u003d\u003d null) {\n       key \u003d new LongWritable();\n     }\n     key.set(pos);\n     if (value \u003d\u003d null) {\n       value \u003d new Text();\n     }\n     int newSize \u003d 0;\n     // We always read one extra line, which lies outside the upper\n     // split limit i.e. (end - 1)\n     while (getFilePosition() \u003c\u003d end || in.needAdditionalRecordAfterSplit()) {\n-      newSize \u003d in.readLine(value, maxLineLength,\n-          Math.max(maxBytesToConsume(pos), maxLineLength));\n+      newSize \u003d in.readLine(value, maxLineLength, maxBytesToConsume(pos));\n       pos +\u003d newSize;\n       if (newSize \u003c maxLineLength) {\n         break;\n       }\n \n       // line too long. try again\n       LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n                (pos - newSize));\n     }\n     if (newSize \u003d\u003d 0) {\n       key \u003d null;\n       value \u003d null;\n       return false;\n     } else {\n       return true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean nextKeyValue() throws IOException {\n    if (key \u003d\u003d null) {\n      key \u003d new LongWritable();\n    }\n    key.set(pos);\n    if (value \u003d\u003d null) {\n      value \u003d new Text();\n    }\n    int newSize \u003d 0;\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end || in.needAdditionalRecordAfterSplit()) {\n      newSize \u003d in.readLine(value, maxLineLength, maxBytesToConsume(pos));\n      pos +\u003d newSize;\n      if (newSize \u003c maxLineLength) {\n        break;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n               (pos - newSize));\n    }\n    if (newSize \u003d\u003d 0) {\n      key \u003d null;\n      value \u003d null;\n      return false;\n    } else {\n      return true;\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "18d99c12c371cfd7b9604e321d8bd6a7be9c4977": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5656. bzip2 codec can drop records when reading data in splits. Contributed by Jason Lowe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1549705 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/12/13 3:31 PM",
      "commitName": "18d99c12c371cfd7b9604e321d8bd6a7be9c4977",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "15/04/13 2:33 PM",
      "commitNameOld": "947e97f354edb1e27432cf3f1a2dff098f01071f",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 238.08,
      "commitsBetweenForRepo": 1478,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   public boolean nextKeyValue() throws IOException {\n     if (key \u003d\u003d null) {\n       key \u003d new LongWritable();\n     }\n     key.set(pos);\n     if (value \u003d\u003d null) {\n       value \u003d new Text();\n     }\n     int newSize \u003d 0;\n     // We always read one extra line, which lies outside the upper\n     // split limit i.e. (end - 1)\n-    while (getFilePosition() \u003c\u003d end) {\n+    while (getFilePosition() \u003c\u003d end || in.needAdditionalRecordAfterSplit()) {\n       newSize \u003d in.readLine(value, maxLineLength,\n           Math.max(maxBytesToConsume(pos), maxLineLength));\n       pos +\u003d newSize;\n       if (newSize \u003c maxLineLength) {\n         break;\n       }\n \n       // line too long. try again\n       LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n                (pos - newSize));\n     }\n     if (newSize \u003d\u003d 0) {\n       key \u003d null;\n       value \u003d null;\n       return false;\n     } else {\n       return true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean nextKeyValue() throws IOException {\n    if (key \u003d\u003d null) {\n      key \u003d new LongWritable();\n    }\n    key.set(pos);\n    if (value \u003d\u003d null) {\n      value \u003d new Text();\n    }\n    int newSize \u003d 0;\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end || in.needAdditionalRecordAfterSplit()) {\n      newSize \u003d in.readLine(value, maxLineLength,\n          Math.max(maxBytesToConsume(pos), maxLineLength));\n      pos +\u003d newSize;\n      if (newSize \u003c maxLineLength) {\n        break;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n               (pos - newSize));\n    }\n    if (newSize \u003d\u003d 0) {\n      key \u003d null;\n      value \u003d null;\n      return false;\n    } else {\n      return true;\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "947e97f354edb1e27432cf3f1a2dff098f01071f": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4974. Optimising the LineRecordReader initialize() method (Gelesh via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1468232 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/04/13 2:33 PM",
      "commitName": "947e97f354edb1e27432cf3f1a2dff098f01071f",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "01/04/13 7:13 PM",
      "commitNameOld": "6f0c4dca74cab26275d48c05fedd9bc2aa57878f",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 13.81,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,31 @@\n   public boolean nextKeyValue() throws IOException {\n     if (key \u003d\u003d null) {\n       key \u003d new LongWritable();\n     }\n     key.set(pos);\n     if (value \u003d\u003d null) {\n       value \u003d new Text();\n     }\n     int newSize \u003d 0;\n     // We always read one extra line, which lies outside the upper\n     // split limit i.e. (end - 1)\n     while (getFilePosition() \u003c\u003d end) {\n       newSize \u003d in.readLine(value, maxLineLength,\n           Math.max(maxBytesToConsume(pos), maxLineLength));\n-      if (newSize \u003d\u003d 0) {\n-        break;\n-      }\n       pos +\u003d newSize;\n       if (newSize \u003c maxLineLength) {\n         break;\n       }\n \n       // line too long. try again\n       LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n                (pos - newSize));\n     }\n     if (newSize \u003d\u003d 0) {\n       key \u003d null;\n       value \u003d null;\n       return false;\n     } else {\n       return true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean nextKeyValue() throws IOException {\n    if (key \u003d\u003d null) {\n      key \u003d new LongWritable();\n    }\n    key.set(pos);\n    if (value \u003d\u003d null) {\n      value \u003d new Text();\n    }\n    int newSize \u003d 0;\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end) {\n      newSize \u003d in.readLine(value, maxLineLength,\n          Math.max(maxBytesToConsume(pos), maxLineLength));\n      pos +\u003d newSize;\n      if (newSize \u003c maxLineLength) {\n        break;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n               (pos - newSize));\n    }\n    if (newSize \u003d\u003d 0) {\n      key \u003d null;\n      value \u003d null;\n      return false;\n    } else {\n      return true;\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "6f0c4dca74cab26275d48c05fedd9bc2aa57878f": {
      "type": "Ybodychange",
      "commitMessage": "Reverted MAPREDUCE-4974 because of test failures.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1463359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/13 7:13 PM",
      "commitName": "6f0c4dca74cab26275d48c05fedd9bc2aa57878f",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "01/04/13 10:45 AM",
      "commitNameOld": "b55756dd03086f6c081991d949ebcf7926af8af5",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 0.35,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,34 @@\n   public boolean nextKeyValue() throws IOException {\n     if (key \u003d\u003d null) {\n       key \u003d new LongWritable();\n     }\n     key.set(pos);\n     if (value \u003d\u003d null) {\n       value \u003d new Text();\n     }\n     int newSize \u003d 0;\n     // We always read one extra line, which lies outside the upper\n     // split limit i.e. (end - 1)\n     while (getFilePosition() \u003c\u003d end) {\n       newSize \u003d in.readLine(value, maxLineLength,\n           Math.max(maxBytesToConsume(pos), maxLineLength));\n+      if (newSize \u003d\u003d 0) {\n+        break;\n+      }\n       pos +\u003d newSize;\n       if (newSize \u003c maxLineLength) {\n         break;\n       }\n \n       // line too long. try again\n       LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n                (pos - newSize));\n     }\n     if (newSize \u003d\u003d 0) {\n       key \u003d null;\n       value \u003d null;\n       return false;\n     } else {\n       return true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean nextKeyValue() throws IOException {\n    if (key \u003d\u003d null) {\n      key \u003d new LongWritable();\n    }\n    key.set(pos);\n    if (value \u003d\u003d null) {\n      value \u003d new Text();\n    }\n    int newSize \u003d 0;\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end) {\n      newSize \u003d in.readLine(value, maxLineLength,\n          Math.max(maxBytesToConsume(pos), maxLineLength));\n      if (newSize \u003d\u003d 0) {\n        break;\n      }\n      pos +\u003d newSize;\n      if (newSize \u003c maxLineLength) {\n        break;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n               (pos - newSize));\n    }\n    if (newSize \u003d\u003d 0) {\n      key \u003d null;\n      value \u003d null;\n      return false;\n    } else {\n      return true;\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "b55756dd03086f6c081991d949ebcf7926af8af5": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4974. Optimising the LineRecordReader initialize() method (Gelesh via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1463221 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/13 10:45 AM",
      "commitName": "b55756dd03086f6c081991d949ebcf7926af8af5",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 585.73,
      "commitsBetweenForRepo": 3537,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,31 @@\n   public boolean nextKeyValue() throws IOException {\n     if (key \u003d\u003d null) {\n       key \u003d new LongWritable();\n     }\n     key.set(pos);\n     if (value \u003d\u003d null) {\n       value \u003d new Text();\n     }\n     int newSize \u003d 0;\n     // We always read one extra line, which lies outside the upper\n     // split limit i.e. (end - 1)\n     while (getFilePosition() \u003c\u003d end) {\n       newSize \u003d in.readLine(value, maxLineLength,\n           Math.max(maxBytesToConsume(pos), maxLineLength));\n-      if (newSize \u003d\u003d 0) {\n-        break;\n-      }\n       pos +\u003d newSize;\n       if (newSize \u003c maxLineLength) {\n         break;\n       }\n \n       // line too long. try again\n       LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n                (pos - newSize));\n     }\n     if (newSize \u003d\u003d 0) {\n       key \u003d null;\n       value \u003d null;\n       return false;\n     } else {\n       return true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean nextKeyValue() throws IOException {\n    if (key \u003d\u003d null) {\n      key \u003d new LongWritable();\n    }\n    key.set(pos);\n    if (value \u003d\u003d null) {\n      value \u003d new Text();\n    }\n    int newSize \u003d 0;\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end) {\n      newSize \u003d in.readLine(value, maxLineLength,\n          Math.max(maxBytesToConsume(pos), maxLineLength));\n      pos +\u003d newSize;\n      if (newSize \u003c maxLineLength) {\n        break;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n               (pos - newSize));\n    }\n    if (newSize \u003d\u003d 0) {\n      key \u003d null;\n      value \u003d null;\n      return false;\n    } else {\n      return true;\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public boolean nextKeyValue() throws IOException {\n    if (key \u003d\u003d null) {\n      key \u003d new LongWritable();\n    }\n    key.set(pos);\n    if (value \u003d\u003d null) {\n      value \u003d new Text();\n    }\n    int newSize \u003d 0;\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end) {\n      newSize \u003d in.readLine(value, maxLineLength,\n          Math.max(maxBytesToConsume(pos), maxLineLength));\n      if (newSize \u003d\u003d 0) {\n        break;\n      }\n      pos +\u003d newSize;\n      if (newSize \u003c maxLineLength) {\n        break;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n               (pos - newSize));\n    }\n    if (newSize \u003d\u003d 0) {\n      key \u003d null;\n      value \u003d null;\n      return false;\n    } else {\n      return true;\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public boolean nextKeyValue() throws IOException {\n    if (key \u003d\u003d null) {\n      key \u003d new LongWritable();\n    }\n    key.set(pos);\n    if (value \u003d\u003d null) {\n      value \u003d new Text();\n    }\n    int newSize \u003d 0;\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end) {\n      newSize \u003d in.readLine(value, maxLineLength,\n          Math.max(maxBytesToConsume(pos), maxLineLength));\n      if (newSize \u003d\u003d 0) {\n        break;\n      }\n      pos +\u003d newSize;\n      if (newSize \u003c maxLineLength) {\n        break;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n               (pos - newSize));\n    }\n    if (newSize \u003d\u003d 0) {\n      key \u003d null;\n      value \u003d null;\n      return false;\n    } else {\n      return true;\n    }\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java"
      }
    },
    "4796e1adcb912005198c9003305c97cf3a8b523e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2365. Add counters to track bytes (read,written) via File(Input,Output)Format. Contributed by Siddharth Seth. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1146515 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/07/11 4:36 PM",
      "commitName": "4796e1adcb912005198c9003305c97cf3a8b523e",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 31.07,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,34 @@\n   public boolean nextKeyValue() throws IOException {\n     if (key \u003d\u003d null) {\n       key \u003d new LongWritable();\n     }\n     key.set(pos);\n     if (value \u003d\u003d null) {\n       value \u003d new Text();\n     }\n     int newSize \u003d 0;\n     // We always read one extra line, which lies outside the upper\n     // split limit i.e. (end - 1)\n     while (getFilePosition() \u003c\u003d end) {\n       newSize \u003d in.readLine(value, maxLineLength,\n           Math.max(maxBytesToConsume(pos), maxLineLength));\n       if (newSize \u003d\u003d 0) {\n         break;\n       }\n       pos +\u003d newSize;\n-      inputByteCounter.increment(newSize);\n       if (newSize \u003c maxLineLength) {\n         break;\n       }\n \n       // line too long. try again\n       LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n                (pos - newSize));\n     }\n     if (newSize \u003d\u003d 0) {\n       key \u003d null;\n       value \u003d null;\n       return false;\n     } else {\n       return true;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean nextKeyValue() throws IOException {\n    if (key \u003d\u003d null) {\n      key \u003d new LongWritable();\n    }\n    key.set(pos);\n    if (value \u003d\u003d null) {\n      value \u003d new Text();\n    }\n    int newSize \u003d 0;\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end) {\n      newSize \u003d in.readLine(value, maxLineLength,\n          Math.max(maxBytesToConsume(pos), maxLineLength));\n      if (newSize \u003d\u003d 0) {\n        break;\n      }\n      pos +\u003d newSize;\n      if (newSize \u003c maxLineLength) {\n        break;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n               (pos - newSize));\n    }\n    if (newSize \u003d\u003d 0) {\n      key \u003d null;\n      value \u003d null;\n      return false;\n    } else {\n      return true;\n    }\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,35 @@\n+  public boolean nextKeyValue() throws IOException {\n+    if (key \u003d\u003d null) {\n+      key \u003d new LongWritable();\n+    }\n+    key.set(pos);\n+    if (value \u003d\u003d null) {\n+      value \u003d new Text();\n+    }\n+    int newSize \u003d 0;\n+    // We always read one extra line, which lies outside the upper\n+    // split limit i.e. (end - 1)\n+    while (getFilePosition() \u003c\u003d end) {\n+      newSize \u003d in.readLine(value, maxLineLength,\n+          Math.max(maxBytesToConsume(pos), maxLineLength));\n+      if (newSize \u003d\u003d 0) {\n+        break;\n+      }\n+      pos +\u003d newSize;\n+      inputByteCounter.increment(newSize);\n+      if (newSize \u003c maxLineLength) {\n+        break;\n+      }\n+\n+      // line too long. try again\n+      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n+               (pos - newSize));\n+    }\n+    if (newSize \u003d\u003d 0) {\n+      key \u003d null;\n+      value \u003d null;\n+      return false;\n+    } else {\n+      return true;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean nextKeyValue() throws IOException {\n    if (key \u003d\u003d null) {\n      key \u003d new LongWritable();\n    }\n    key.set(pos);\n    if (value \u003d\u003d null) {\n      value \u003d new Text();\n    }\n    int newSize \u003d 0;\n    // We always read one extra line, which lies outside the upper\n    // split limit i.e. (end - 1)\n    while (getFilePosition() \u003c\u003d end) {\n      newSize \u003d in.readLine(value, maxLineLength,\n          Math.max(maxBytesToConsume(pos), maxLineLength));\n      if (newSize \u003d\u003d 0) {\n        break;\n      }\n      pos +\u003d newSize;\n      inputByteCounter.increment(newSize);\n      if (newSize \u003c maxLineLength) {\n        break;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n               (pos - newSize));\n    }\n    if (newSize \u003d\u003d 0) {\n      key \u003d null;\n      value \u003d null;\n      return false;\n    } else {\n      return true;\n    }\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java"
    }
  }
}