{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "MergeManagerImpl.java",
  "functionName": "finalMerge",
  "functionId": "finalMerge___job-JobConf__fs-FileSystem__inMemoryMapOutputs-List__InMemoryMapOutput__K,V______onDiskMapOutputs-List__CompressAwarePath__",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
  "functionStartLine": 691,
  "functionEndLine": 832,
  "numCommitsSeen": 34,
  "timeTaken": 12017,
  "changeHistory": [
    "7dc3c1203d1ab14c09d0aaf0869a5bcdfafb0a5a",
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23",
    "df68c56267ca7dfbfee4b241bc84325d1760d12d",
    "14089f1e57e078cf20caed9db6f86de60773d704",
    "0f430e53fde884f24b473043f0a7e2bffa98ebd3",
    "da4cab10990b3a352fc2c699f3b41c994ac55e95",
    "539153a6798a667d39f20972c5ae0936060e2cc1",
    "73fd247c7649919350ecfd16806af57ffe554649",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "7dc3c1203d1ab14c09d0aaf0869a5bcdfafb0a5a": "Ybodychange",
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23": "Ybodychange",
    "df68c56267ca7dfbfee4b241bc84325d1760d12d": "Ybodychange",
    "14089f1e57e078cf20caed9db6f86de60773d704": "Ybodychange",
    "0f430e53fde884f24b473043f0a7e2bffa98ebd3": "Ymultichange(Yparameterchange,Ybodychange)",
    "da4cab10990b3a352fc2c699f3b41c994ac55e95": "Ymultichange(Yparameterchange,Ybodychange)",
    "539153a6798a667d39f20972c5ae0936060e2cc1": "Ymultichange(Yparameterchange,Ybodychange)",
    "73fd247c7649919350ecfd16806af57ffe554649": "Ymultichange(Ymovefromfile,Yparameterchange)",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymovefromfile",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7dc3c1203d1ab14c09d0aaf0869a5bcdfafb0a5a": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5649. Reduce cannot use more than 2G memory for the final merge. Contributed by Gera Shegalov\n",
      "commitDate": "04/05/15 12:02 PM",
      "commitName": "7dc3c1203d1ab14c09d0aaf0869a5bcdfafb0a5a",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "16/03/15 1:58 PM",
      "commitNameOld": "685dbafbe2154e5bf4b638da0668ce32d8c879b0",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 48.92,
      "commitsBetweenForRepo": 441,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,152 +1,142 @@\n   private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                        List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                        List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                        ) throws IOException {\n-    LOG.info(\"finalMerge called with \" + \n-             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n-             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n-    \n-    final float maxRedPer \u003d\n-      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n-    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n-      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n-                            maxRedPer);\n-    }\n-    int maxInMemReduce \u003d (int)Math.min(\n-        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n-    \n-\n+    LOG.info(\"finalMerge called with \" +\n+        inMemoryMapOutputs.size() + \" in-memory map-outputs and \" +\n+        onDiskMapOutputs.size() + \" on-disk map-outputs\");\n+    final long maxInMemReduce \u003d getMaxInMemReduceLimit();\n     // merge config params\n     Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n     Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n     boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n     final Path tmpDir \u003d new Path(reduceId.toString());\n     final RawComparator\u003cK\u003e comparator \u003d\n       (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n \n     // segments required to vacate memory\n     List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemToDiskBytes \u003d 0;\n     boolean mergePhaseFinished \u003d false;\n     if (inMemoryMapOutputs.size() \u003e 0) {\n       TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n       inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                 memDiskSegments,\n                                                 maxInMemReduce);\n       final int numMemDiskSegments \u003d memDiskSegments.size();\n       if (numMemDiskSegments \u003e 0 \u0026\u0026\n             ioSortFactor \u003e onDiskMapOutputs.size()) {\n         \n         // If we reach here, it implies that we have less than io.sort.factor\n         // disk segments and this will be incremented by 1 (result of the \n         // memory segments merge). Since this total would still be \n         // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n         // the merge of all these disk segments would be directly fed to the\n         // reduce method\n         \n         mergePhaseFinished \u003d true;\n         // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n         final Path outputPath \u003d \n           mapOutputFile.getInputFileForWrite(mapId,\n                                              inMemToDiskBytes).suffix(\n                                                  Task.MERGED_OUTPUT_PREFIX);\n         final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n             keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n             tmpDir, comparator, reporter, spilledRecordsCounter, null, \n             mergePhase);\n \n         FSDataOutputStream out \u003d CryptoUtils.wrapIfNecessary(job, fs.create(outputPath));\n         Writer\u003cK, V\u003e writer \u003d new Writer\u003cK, V\u003e(job, out, keyClass, valueClass,\n             codec, null, true);\n         try {\n           Merger.writeFile(rIter, writer, reporter, job);\n           writer.close();\n           onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n               writer.getRawLength(), writer.getCompressedLength()));\n           writer \u003d null;\n           // add to list of final disk outputs.\n         } catch (IOException e) {\n           if (null !\u003d outputPath) {\n             try {\n               fs.delete(outputPath, true);\n             } catch (IOException ie) {\n               // NOTHING\n             }\n           }\n           throw e;\n         } finally {\n           if (null !\u003d writer) {\n             writer.close();\n           }\n         }\n         LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                  \"reduce memory limit\");\n         inMemToDiskBytes \u003d 0;\n         memDiskSegments.clear();\n       } else if (inMemToDiskBytes !\u003d 0) {\n         LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes in memory for \" +\n                  \"intermediate, on-disk merge\");\n       }\n     }\n \n     // segments on disk\n     List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long onDiskBytes \u003d inMemToDiskBytes;\n     long rawBytes \u003d inMemToDiskBytes;\n     CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n         new CompressAwarePath[onDiskMapOutputs.size()]);\n     for (CompressAwarePath file : onDisk) {\n       long fileLength \u003d fs.getFileStatus(file).getLen();\n       onDiskBytes +\u003d fileLength;\n       rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n \n       LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n       diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                          (file.toString().endsWith(\n                                              Task.MERGED_OUTPUT_PREFIX) ?\n                                           null : mergedMapOutputsCounter), file.getRawDataLength()\n                                         ));\n     }\n     LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n              onDiskBytes + \" bytes from disk\");\n     Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n       public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n         if (o1.getLength() \u003d\u003d o2.getLength()) {\n           return 0;\n         }\n         return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n       }\n     });\n \n     // build final list of segments from merged backed by disk + in-mem\n     List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                              finalSegments, 0);\n     LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n              inMemBytes + \" bytes from memory into reduce\");\n     if (0 !\u003d onDiskBytes) {\n       final int numInMemSegments \u003d memDiskSegments.size();\n       diskSegments.addAll(0, memDiskSegments);\n       memDiskSegments.clear();\n       // Pass mergePhase only if there is a going to be intermediate\n       // merges. See comment where mergePhaseFinished is being set\n       Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n       RawKeyValueIterator diskMerge \u003d Merger.merge(\n           job, fs, keyClass, valueClass, codec, diskSegments,\n           ioSortFactor, numInMemSegments, tmpDir, comparator,\n           reporter, false, spilledRecordsCounter, null, thisPhase);\n       diskSegments.clear();\n       if (0 \u003d\u003d finalSegments.size()) {\n         return diskMerge;\n       }\n       finalSegments.add(new Segment\u003cK,V\u003e(\n             new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n     }\n     return Merger.merge(job, fs, keyClass, valueClass,\n                  finalSegments, finalSegments.size(), tmpDir,\n                  comparator, reporter, spilledRecordsCounter, null,\n                  null);\n   \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" +\n        inMemoryMapOutputs.size() + \" in-memory map-outputs and \" +\n        onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    final long maxInMemReduce \u003d getMaxInMemReduceLimit();\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n\n        FSDataOutputStream out \u003d CryptoUtils.wrapIfNecessary(job, fs.create(outputPath));\n        Writer\u003cK, V\u003e writer \u003d new Writer\u003cK, V\u003e(job, out, keyClass, valueClass,\n            codec, null, true);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          writer.close();\n          onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n              writer.getRawLength(), writer.getCompressedLength()));\n          writer \u003d null;\n          // add to list of final disk outputs.\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    long rawBytes \u003d inMemToDiskBytes;\n    CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n        new CompressAwarePath[onDiskMapOutputs.size()]);\n    for (CompressAwarePath file : onDisk) {\n      long fileLength \u003d fs.getFileStatus(file).getLen();\n      onDiskBytes +\u003d fileLength;\n      rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter), file.getRawDataLength()\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, codec, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
      "extendedDetails": {}
    },
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5890. Support for encrypting Intermediate data and spills in local filesystem. (asuresh via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1609597 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/07/14 5:43 PM",
      "commitName": "95986dd2fb4527c43fa4c088c61fb7b4bd794d23",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "06/01/14 10:35 AM",
      "commitNameOld": "76238b9722539b5fd4773129ecc31b11bd8255ef",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 185.26,
      "commitsBetweenForRepo": 1309,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,150 +1,152 @@\n   private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                        List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                        List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                        ) throws IOException {\n     LOG.info(\"finalMerge called with \" + \n              inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n              onDiskMapOutputs.size() + \" on-disk map-outputs\");\n     \n     final float maxRedPer \u003d\n       job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n     if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n       throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                             maxRedPer);\n     }\n     int maxInMemReduce \u003d (int)Math.min(\n         Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n     \n \n     // merge config params\n     Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n     Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n     boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n     final Path tmpDir \u003d new Path(reduceId.toString());\n     final RawComparator\u003cK\u003e comparator \u003d\n       (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n \n     // segments required to vacate memory\n     List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemToDiskBytes \u003d 0;\n     boolean mergePhaseFinished \u003d false;\n     if (inMemoryMapOutputs.size() \u003e 0) {\n       TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n       inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                 memDiskSegments,\n                                                 maxInMemReduce);\n       final int numMemDiskSegments \u003d memDiskSegments.size();\n       if (numMemDiskSegments \u003e 0 \u0026\u0026\n             ioSortFactor \u003e onDiskMapOutputs.size()) {\n         \n         // If we reach here, it implies that we have less than io.sort.factor\n         // disk segments and this will be incremented by 1 (result of the \n         // memory segments merge). Since this total would still be \n         // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n         // the merge of all these disk segments would be directly fed to the\n         // reduce method\n         \n         mergePhaseFinished \u003d true;\n         // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n         final Path outputPath \u003d \n           mapOutputFile.getInputFileForWrite(mapId,\n                                              inMemToDiskBytes).suffix(\n                                                  Task.MERGED_OUTPUT_PREFIX);\n         final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n             keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n             tmpDir, comparator, reporter, spilledRecordsCounter, null, \n             mergePhase);\n-        Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n-            keyClass, valueClass, codec, null);\n+\n+        FSDataOutputStream out \u003d CryptoUtils.wrapIfNecessary(job, fs.create(outputPath));\n+        Writer\u003cK, V\u003e writer \u003d new Writer\u003cK, V\u003e(job, out, keyClass, valueClass,\n+            codec, null, true);\n         try {\n           Merger.writeFile(rIter, writer, reporter, job);\n           writer.close();\n           onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n               writer.getRawLength(), writer.getCompressedLength()));\n           writer \u003d null;\n           // add to list of final disk outputs.\n         } catch (IOException e) {\n           if (null !\u003d outputPath) {\n             try {\n               fs.delete(outputPath, true);\n             } catch (IOException ie) {\n               // NOTHING\n             }\n           }\n           throw e;\n         } finally {\n           if (null !\u003d writer) {\n             writer.close();\n           }\n         }\n         LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                  \"reduce memory limit\");\n         inMemToDiskBytes \u003d 0;\n         memDiskSegments.clear();\n       } else if (inMemToDiskBytes !\u003d 0) {\n         LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes in memory for \" +\n                  \"intermediate, on-disk merge\");\n       }\n     }\n \n     // segments on disk\n     List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long onDiskBytes \u003d inMemToDiskBytes;\n     long rawBytes \u003d inMemToDiskBytes;\n     CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n         new CompressAwarePath[onDiskMapOutputs.size()]);\n     for (CompressAwarePath file : onDisk) {\n       long fileLength \u003d fs.getFileStatus(file).getLen();\n       onDiskBytes +\u003d fileLength;\n       rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n \n       LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n       diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                          (file.toString().endsWith(\n                                              Task.MERGED_OUTPUT_PREFIX) ?\n                                           null : mergedMapOutputsCounter), file.getRawDataLength()\n                                         ));\n     }\n     LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n              onDiskBytes + \" bytes from disk\");\n     Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n       public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n         if (o1.getLength() \u003d\u003d o2.getLength()) {\n           return 0;\n         }\n         return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n       }\n     });\n \n     // build final list of segments from merged backed by disk + in-mem\n     List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                              finalSegments, 0);\n     LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n              inMemBytes + \" bytes from memory into reduce\");\n     if (0 !\u003d onDiskBytes) {\n       final int numInMemSegments \u003d memDiskSegments.size();\n       diskSegments.addAll(0, memDiskSegments);\n       memDiskSegments.clear();\n       // Pass mergePhase only if there is a going to be intermediate\n       // merges. See comment where mergePhaseFinished is being set\n       Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n       RawKeyValueIterator diskMerge \u003d Merger.merge(\n           job, fs, keyClass, valueClass, codec, diskSegments,\n           ioSortFactor, numInMemSegments, tmpDir, comparator,\n           reporter, false, spilledRecordsCounter, null, thisPhase);\n       diskSegments.clear();\n       if (0 \u003d\u003d finalSegments.size()) {\n         return diskMerge;\n       }\n       finalSegments.add(new Segment\u003cK,V\u003e(\n             new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n     }\n     return Merger.merge(job, fs, keyClass, valueClass,\n                  finalSegments, finalSegments.size(), tmpDir,\n                  comparator, reporter, spilledRecordsCounter, null,\n                  null);\n   \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" + \n             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    \n    final float maxRedPer \u003d\n      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                            maxRedPer);\n    }\n    int maxInMemReduce \u003d (int)Math.min(\n        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n    \n\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n\n        FSDataOutputStream out \u003d CryptoUtils.wrapIfNecessary(job, fs.create(outputPath));\n        Writer\u003cK, V\u003e writer \u003d new Writer\u003cK, V\u003e(job, out, keyClass, valueClass,\n            codec, null, true);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          writer.close();\n          onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n              writer.getRawLength(), writer.getCompressedLength()));\n          writer \u003d null;\n          // add to list of final disk outputs.\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    long rawBytes \u003d inMemToDiskBytes;\n    CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n        new CompressAwarePath[onDiskMapOutputs.size()]);\n    for (CompressAwarePath file : onDisk) {\n      long fileLength \u003d fs.getFileStatus(file).getLen();\n      onDiskBytes +\u003d fileLength;\n      rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter), file.getRawDataLength()\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, codec, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
      "extendedDetails": {}
    },
    "df68c56267ca7dfbfee4b241bc84325d1760d12d": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3685. Fix bugs in MergeManager to ensure compression codec is appropriately used and that on-disk segments are correctly sorted on file-size. Contributed by Anty Rao and Ravi Prakash.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1453365 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/03/13 7:02 AM",
      "commitName": "df68c56267ca7dfbfee4b241bc84325d1760d12d",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "27/02/13 2:40 AM",
      "commitNameOld": "14089f1e57e078cf20caed9db6f86de60773d704",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 7.18,
      "commitsBetweenForRepo": 34,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,150 +1,150 @@\n   private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                        List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                        List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                        ) throws IOException {\n     LOG.info(\"finalMerge called with \" + \n              inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n              onDiskMapOutputs.size() + \" on-disk map-outputs\");\n     \n     final float maxRedPer \u003d\n       job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n     if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n       throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                             maxRedPer);\n     }\n     int maxInMemReduce \u003d (int)Math.min(\n         Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n     \n \n     // merge config params\n     Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n     Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n     boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n     final Path tmpDir \u003d new Path(reduceId.toString());\n     final RawComparator\u003cK\u003e comparator \u003d\n       (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n \n     // segments required to vacate memory\n     List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemToDiskBytes \u003d 0;\n     boolean mergePhaseFinished \u003d false;\n     if (inMemoryMapOutputs.size() \u003e 0) {\n       TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n       inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                 memDiskSegments,\n                                                 maxInMemReduce);\n       final int numMemDiskSegments \u003d memDiskSegments.size();\n       if (numMemDiskSegments \u003e 0 \u0026\u0026\n             ioSortFactor \u003e onDiskMapOutputs.size()) {\n         \n         // If we reach here, it implies that we have less than io.sort.factor\n         // disk segments and this will be incremented by 1 (result of the \n         // memory segments merge). Since this total would still be \n         // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n         // the merge of all these disk segments would be directly fed to the\n         // reduce method\n         \n         mergePhaseFinished \u003d true;\n         // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n         final Path outputPath \u003d \n           mapOutputFile.getInputFileForWrite(mapId,\n                                              inMemToDiskBytes).suffix(\n                                                  Task.MERGED_OUTPUT_PREFIX);\n         final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n             keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n             tmpDir, comparator, reporter, spilledRecordsCounter, null, \n             mergePhase);\n         Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n             keyClass, valueClass, codec, null);\n         try {\n           Merger.writeFile(rIter, writer, reporter, job);\n           writer.close();\n           onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n-              writer.getRawLength()));\n+              writer.getRawLength(), writer.getCompressedLength()));\n           writer \u003d null;\n           // add to list of final disk outputs.\n         } catch (IOException e) {\n           if (null !\u003d outputPath) {\n             try {\n               fs.delete(outputPath, true);\n             } catch (IOException ie) {\n               // NOTHING\n             }\n           }\n           throw e;\n         } finally {\n           if (null !\u003d writer) {\n             writer.close();\n           }\n         }\n         LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                  \"reduce memory limit\");\n         inMemToDiskBytes \u003d 0;\n         memDiskSegments.clear();\n       } else if (inMemToDiskBytes !\u003d 0) {\n         LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes in memory for \" +\n                  \"intermediate, on-disk merge\");\n       }\n     }\n \n     // segments on disk\n     List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long onDiskBytes \u003d inMemToDiskBytes;\n     long rawBytes \u003d inMemToDiskBytes;\n     CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n         new CompressAwarePath[onDiskMapOutputs.size()]);\n     for (CompressAwarePath file : onDisk) {\n       long fileLength \u003d fs.getFileStatus(file).getLen();\n       onDiskBytes +\u003d fileLength;\n       rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n \n       LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n       diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                          (file.toString().endsWith(\n                                              Task.MERGED_OUTPUT_PREFIX) ?\n                                           null : mergedMapOutputsCounter), file.getRawDataLength()\n                                         ));\n     }\n     LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n              onDiskBytes + \" bytes from disk\");\n     Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n       public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n         if (o1.getLength() \u003d\u003d o2.getLength()) {\n           return 0;\n         }\n         return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n       }\n     });\n \n     // build final list of segments from merged backed by disk + in-mem\n     List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                              finalSegments, 0);\n     LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n              inMemBytes + \" bytes from memory into reduce\");\n     if (0 !\u003d onDiskBytes) {\n       final int numInMemSegments \u003d memDiskSegments.size();\n       diskSegments.addAll(0, memDiskSegments);\n       memDiskSegments.clear();\n       // Pass mergePhase only if there is a going to be intermediate\n       // merges. See comment where mergePhaseFinished is being set\n       Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n       RawKeyValueIterator diskMerge \u003d Merger.merge(\n-          job, fs, keyClass, valueClass, diskSegments,\n+          job, fs, keyClass, valueClass, codec, diskSegments,\n           ioSortFactor, numInMemSegments, tmpDir, comparator,\n           reporter, false, spilledRecordsCounter, null, thisPhase);\n       diskSegments.clear();\n       if (0 \u003d\u003d finalSegments.size()) {\n         return diskMerge;\n       }\n       finalSegments.add(new Segment\u003cK,V\u003e(\n             new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n     }\n     return Merger.merge(job, fs, keyClass, valueClass,\n                  finalSegments, finalSegments.size(), tmpDir,\n                  comparator, reporter, spilledRecordsCounter, null,\n                  null);\n   \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" + \n             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    \n    final float maxRedPer \u003d\n      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                            maxRedPer);\n    }\n    int maxInMemReduce \u003d (int)Math.min(\n        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n    \n\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n        Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n            keyClass, valueClass, codec, null);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          writer.close();\n          onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n              writer.getRawLength(), writer.getCompressedLength()));\n          writer \u003d null;\n          // add to list of final disk outputs.\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    long rawBytes \u003d inMemToDiskBytes;\n    CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n        new CompressAwarePath[onDiskMapOutputs.size()]);\n    for (CompressAwarePath file : onDisk) {\n      long fileLength \u003d fs.getFileStatus(file).getLen();\n      onDiskBytes +\u003d fileLength;\n      rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter), file.getRawDataLength()\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, codec, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
      "extendedDetails": {}
    },
    "14089f1e57e078cf20caed9db6f86de60773d704": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5008. Merger progress miscounts with respect to EOF_MARKER. Contributed by Sandy Ryza.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1450723 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/02/13 2:40 AM",
      "commitName": "14089f1e57e078cf20caed9db6f86de60773d704",
      "commitAuthor": "Thomas White",
      "commitDateOld": "29/01/13 11:38 AM",
      "commitNameOld": "0f430e53fde884f24b473043f0a7e2bffa98ebd3",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 28.63,
      "commitsBetweenForRepo": 101,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,148 +1,150 @@\n   private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                        List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                        List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                        ) throws IOException {\n     LOG.info(\"finalMerge called with \" + \n              inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n              onDiskMapOutputs.size() + \" on-disk map-outputs\");\n     \n     final float maxRedPer \u003d\n       job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n     if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n       throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                             maxRedPer);\n     }\n     int maxInMemReduce \u003d (int)Math.min(\n         Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n     \n \n     // merge config params\n     Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n     Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n     boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n     final Path tmpDir \u003d new Path(reduceId.toString());\n     final RawComparator\u003cK\u003e comparator \u003d\n       (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n \n     // segments required to vacate memory\n     List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemToDiskBytes \u003d 0;\n     boolean mergePhaseFinished \u003d false;\n     if (inMemoryMapOutputs.size() \u003e 0) {\n       TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n       inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                 memDiskSegments,\n                                                 maxInMemReduce);\n       final int numMemDiskSegments \u003d memDiskSegments.size();\n       if (numMemDiskSegments \u003e 0 \u0026\u0026\n             ioSortFactor \u003e onDiskMapOutputs.size()) {\n         \n         // If we reach here, it implies that we have less than io.sort.factor\n         // disk segments and this will be incremented by 1 (result of the \n         // memory segments merge). Since this total would still be \n         // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n         // the merge of all these disk segments would be directly fed to the\n         // reduce method\n         \n         mergePhaseFinished \u003d true;\n         // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n         final Path outputPath \u003d \n           mapOutputFile.getInputFileForWrite(mapId,\n                                              inMemToDiskBytes).suffix(\n                                                  Task.MERGED_OUTPUT_PREFIX);\n         final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n             keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n             tmpDir, comparator, reporter, spilledRecordsCounter, null, \n             mergePhase);\n-        final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n+        Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n             keyClass, valueClass, codec, null);\n         try {\n           Merger.writeFile(rIter, writer, reporter, job);\n-          // add to list of final disk outputs.\n+          writer.close();\n           onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n               writer.getRawLength()));\n+          writer \u003d null;\n+          // add to list of final disk outputs.\n         } catch (IOException e) {\n           if (null !\u003d outputPath) {\n             try {\n               fs.delete(outputPath, true);\n             } catch (IOException ie) {\n               // NOTHING\n             }\n           }\n           throw e;\n         } finally {\n           if (null !\u003d writer) {\n             writer.close();\n           }\n         }\n         LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                  \"reduce memory limit\");\n         inMemToDiskBytes \u003d 0;\n         memDiskSegments.clear();\n       } else if (inMemToDiskBytes !\u003d 0) {\n         LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes in memory for \" +\n                  \"intermediate, on-disk merge\");\n       }\n     }\n \n     // segments on disk\n     List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long onDiskBytes \u003d inMemToDiskBytes;\n     long rawBytes \u003d inMemToDiskBytes;\n     CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n         new CompressAwarePath[onDiskMapOutputs.size()]);\n     for (CompressAwarePath file : onDisk) {\n       long fileLength \u003d fs.getFileStatus(file).getLen();\n       onDiskBytes +\u003d fileLength;\n       rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n \n       LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n       diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                          (file.toString().endsWith(\n                                              Task.MERGED_OUTPUT_PREFIX) ?\n                                           null : mergedMapOutputsCounter), file.getRawDataLength()\n                                         ));\n     }\n     LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n              onDiskBytes + \" bytes from disk\");\n     Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n       public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n         if (o1.getLength() \u003d\u003d o2.getLength()) {\n           return 0;\n         }\n         return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n       }\n     });\n \n     // build final list of segments from merged backed by disk + in-mem\n     List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                              finalSegments, 0);\n     LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n              inMemBytes + \" bytes from memory into reduce\");\n     if (0 !\u003d onDiskBytes) {\n       final int numInMemSegments \u003d memDiskSegments.size();\n       diskSegments.addAll(0, memDiskSegments);\n       memDiskSegments.clear();\n       // Pass mergePhase only if there is a going to be intermediate\n       // merges. See comment where mergePhaseFinished is being set\n       Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n       RawKeyValueIterator diskMerge \u003d Merger.merge(\n           job, fs, keyClass, valueClass, diskSegments,\n           ioSortFactor, numInMemSegments, tmpDir, comparator,\n           reporter, false, spilledRecordsCounter, null, thisPhase);\n       diskSegments.clear();\n       if (0 \u003d\u003d finalSegments.size()) {\n         return diskMerge;\n       }\n       finalSegments.add(new Segment\u003cK,V\u003e(\n             new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n     }\n     return Merger.merge(job, fs, keyClass, valueClass,\n                  finalSegments, finalSegments.size(), tmpDir,\n                  comparator, reporter, spilledRecordsCounter, null,\n                  null);\n   \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" + \n             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    \n    final float maxRedPer \u003d\n      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                            maxRedPer);\n    }\n    int maxInMemReduce \u003d (int)Math.min(\n        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n    \n\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n        Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n            keyClass, valueClass, codec, null);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          writer.close();\n          onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n              writer.getRawLength()));\n          writer \u003d null;\n          // add to list of final disk outputs.\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    long rawBytes \u003d inMemToDiskBytes;\n    CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n        new CompressAwarePath[onDiskMapOutputs.size()]);\n    for (CompressAwarePath file : onDisk) {\n      long fileLength \u003d fs.getFileStatus(file).getLen();\n      onDiskBytes +\u003d fileLength;\n      rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter), file.getRawDataLength()\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
      "extendedDetails": {}
    },
    "0f430e53fde884f24b473043f0a7e2bffa98ebd3": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-2264. Job status exceeds 100% in some cases. (devaraj.k and sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1440076 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/01/13 11:38 AM",
      "commitName": "0f430e53fde884f24b473043f0a7e2bffa98ebd3",
      "commitAuthor": "Alejandro Abdelnur",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-2264. Job status exceeds 100% in some cases. (devaraj.k and sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1440076 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "29/01/13 11:38 AM",
          "commitName": "0f430e53fde884f24b473043f0a7e2bffa98ebd3",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "28/01/13 10:58 AM",
          "commitNameOld": "da4cab10990b3a352fc2c699f3b41c994ac55e95",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 1.03,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,143 +1,148 @@\n   private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                        List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n-                                       List\u003cPath\u003e onDiskMapOutputs\n+                                       List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                        ) throws IOException {\n     LOG.info(\"finalMerge called with \" + \n              inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n              onDiskMapOutputs.size() + \" on-disk map-outputs\");\n     \n     final float maxRedPer \u003d\n       job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n     if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n       throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                             maxRedPer);\n     }\n     int maxInMemReduce \u003d (int)Math.min(\n         Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n     \n \n     // merge config params\n     Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n     Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n     boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n     final Path tmpDir \u003d new Path(reduceId.toString());\n     final RawComparator\u003cK\u003e comparator \u003d\n       (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n \n     // segments required to vacate memory\n     List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemToDiskBytes \u003d 0;\n     boolean mergePhaseFinished \u003d false;\n     if (inMemoryMapOutputs.size() \u003e 0) {\n       TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n       inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                 memDiskSegments,\n                                                 maxInMemReduce);\n       final int numMemDiskSegments \u003d memDiskSegments.size();\n       if (numMemDiskSegments \u003e 0 \u0026\u0026\n             ioSortFactor \u003e onDiskMapOutputs.size()) {\n         \n         // If we reach here, it implies that we have less than io.sort.factor\n         // disk segments and this will be incremented by 1 (result of the \n         // memory segments merge). Since this total would still be \n         // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n         // the merge of all these disk segments would be directly fed to the\n         // reduce method\n         \n         mergePhaseFinished \u003d true;\n         // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n         final Path outputPath \u003d \n           mapOutputFile.getInputFileForWrite(mapId,\n                                              inMemToDiskBytes).suffix(\n                                                  Task.MERGED_OUTPUT_PREFIX);\n         final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n             keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n             tmpDir, comparator, reporter, spilledRecordsCounter, null, \n             mergePhase);\n         final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n             keyClass, valueClass, codec, null);\n         try {\n           Merger.writeFile(rIter, writer, reporter, job);\n           // add to list of final disk outputs.\n-          onDiskMapOutputs.add(outputPath);\n+          onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n+              writer.getRawLength()));\n         } catch (IOException e) {\n           if (null !\u003d outputPath) {\n             try {\n               fs.delete(outputPath, true);\n             } catch (IOException ie) {\n               // NOTHING\n             }\n           }\n           throw e;\n         } finally {\n           if (null !\u003d writer) {\n             writer.close();\n           }\n         }\n         LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                  \"reduce memory limit\");\n         inMemToDiskBytes \u003d 0;\n         memDiskSegments.clear();\n       } else if (inMemToDiskBytes !\u003d 0) {\n         LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes in memory for \" +\n                  \"intermediate, on-disk merge\");\n       }\n     }\n \n     // segments on disk\n     List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long onDiskBytes \u003d inMemToDiskBytes;\n-    Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n-    for (Path file : onDisk) {\n-      onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n-      LOG.debug(\"Disk file: \" + file + \" Length is \" + \n-          fs.getFileStatus(file).getLen());\n+    long rawBytes \u003d inMemToDiskBytes;\n+    CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n+        new CompressAwarePath[onDiskMapOutputs.size()]);\n+    for (CompressAwarePath file : onDisk) {\n+      long fileLength \u003d fs.getFileStatus(file).getLen();\n+      onDiskBytes +\u003d fileLength;\n+      rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n+\n+      LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n       diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                          (file.toString().endsWith(\n                                              Task.MERGED_OUTPUT_PREFIX) ?\n-                                          null : mergedMapOutputsCounter)\n+                                          null : mergedMapOutputsCounter), file.getRawDataLength()\n                                         ));\n     }\n     LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n              onDiskBytes + \" bytes from disk\");\n     Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n       public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n         if (o1.getLength() \u003d\u003d o2.getLength()) {\n           return 0;\n         }\n         return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n       }\n     });\n \n     // build final list of segments from merged backed by disk + in-mem\n     List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                              finalSegments, 0);\n     LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n              inMemBytes + \" bytes from memory into reduce\");\n     if (0 !\u003d onDiskBytes) {\n       final int numInMemSegments \u003d memDiskSegments.size();\n       diskSegments.addAll(0, memDiskSegments);\n       memDiskSegments.clear();\n       // Pass mergePhase only if there is a going to be intermediate\n       // merges. See comment where mergePhaseFinished is being set\n       Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n       RawKeyValueIterator diskMerge \u003d Merger.merge(\n           job, fs, keyClass, valueClass, diskSegments,\n           ioSortFactor, numInMemSegments, tmpDir, comparator,\n           reporter, false, spilledRecordsCounter, null, thisPhase);\n       diskSegments.clear();\n       if (0 \u003d\u003d finalSegments.size()) {\n         return diskMerge;\n       }\n       finalSegments.add(new Segment\u003cK,V\u003e(\n-            new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n+            new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n     }\n     return Merger.merge(job, fs, keyClass, valueClass,\n                  finalSegments, finalSegments.size(), tmpDir,\n                  comparator, reporter, spilledRecordsCounter, null,\n                  null);\n   \n   }\n\\ No newline at end of file\n",
          "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" + \n             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    \n    final float maxRedPer \u003d\n      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                            maxRedPer);\n    }\n    int maxInMemReduce \u003d (int)Math.min(\n        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n    \n\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n        final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n            keyClass, valueClass, codec, null);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          // add to list of final disk outputs.\n          onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n              writer.getRawLength()));\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    long rawBytes \u003d inMemToDiskBytes;\n    CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n        new CompressAwarePath[onDiskMapOutputs.size()]);\n    for (CompressAwarePath file : onDisk) {\n      long fileLength \u003d fs.getFileStatus(file).getLen();\n      onDiskBytes +\u003d fileLength;\n      rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter), file.getRawDataLength()\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {
            "oldValue": "[job-JobConf, fs-FileSystem, inMemoryMapOutputs-List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e, onDiskMapOutputs-List\u003cPath\u003e]",
            "newValue": "[job-JobConf, fs-FileSystem, inMemoryMapOutputs-List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e, onDiskMapOutputs-List\u003cCompressAwarePath\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-2264. Job status exceeds 100% in some cases. (devaraj.k and sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1440076 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "29/01/13 11:38 AM",
          "commitName": "0f430e53fde884f24b473043f0a7e2bffa98ebd3",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "28/01/13 10:58 AM",
          "commitNameOld": "da4cab10990b3a352fc2c699f3b41c994ac55e95",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 1.03,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,143 +1,148 @@\n   private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                        List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n-                                       List\u003cPath\u003e onDiskMapOutputs\n+                                       List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                        ) throws IOException {\n     LOG.info(\"finalMerge called with \" + \n              inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n              onDiskMapOutputs.size() + \" on-disk map-outputs\");\n     \n     final float maxRedPer \u003d\n       job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n     if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n       throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                             maxRedPer);\n     }\n     int maxInMemReduce \u003d (int)Math.min(\n         Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n     \n \n     // merge config params\n     Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n     Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n     boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n     final Path tmpDir \u003d new Path(reduceId.toString());\n     final RawComparator\u003cK\u003e comparator \u003d\n       (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n \n     // segments required to vacate memory\n     List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemToDiskBytes \u003d 0;\n     boolean mergePhaseFinished \u003d false;\n     if (inMemoryMapOutputs.size() \u003e 0) {\n       TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n       inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                 memDiskSegments,\n                                                 maxInMemReduce);\n       final int numMemDiskSegments \u003d memDiskSegments.size();\n       if (numMemDiskSegments \u003e 0 \u0026\u0026\n             ioSortFactor \u003e onDiskMapOutputs.size()) {\n         \n         // If we reach here, it implies that we have less than io.sort.factor\n         // disk segments and this will be incremented by 1 (result of the \n         // memory segments merge). Since this total would still be \n         // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n         // the merge of all these disk segments would be directly fed to the\n         // reduce method\n         \n         mergePhaseFinished \u003d true;\n         // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n         final Path outputPath \u003d \n           mapOutputFile.getInputFileForWrite(mapId,\n                                              inMemToDiskBytes).suffix(\n                                                  Task.MERGED_OUTPUT_PREFIX);\n         final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n             keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n             tmpDir, comparator, reporter, spilledRecordsCounter, null, \n             mergePhase);\n         final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n             keyClass, valueClass, codec, null);\n         try {\n           Merger.writeFile(rIter, writer, reporter, job);\n           // add to list of final disk outputs.\n-          onDiskMapOutputs.add(outputPath);\n+          onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n+              writer.getRawLength()));\n         } catch (IOException e) {\n           if (null !\u003d outputPath) {\n             try {\n               fs.delete(outputPath, true);\n             } catch (IOException ie) {\n               // NOTHING\n             }\n           }\n           throw e;\n         } finally {\n           if (null !\u003d writer) {\n             writer.close();\n           }\n         }\n         LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                  \"reduce memory limit\");\n         inMemToDiskBytes \u003d 0;\n         memDiskSegments.clear();\n       } else if (inMemToDiskBytes !\u003d 0) {\n         LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes in memory for \" +\n                  \"intermediate, on-disk merge\");\n       }\n     }\n \n     // segments on disk\n     List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long onDiskBytes \u003d inMemToDiskBytes;\n-    Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n-    for (Path file : onDisk) {\n-      onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n-      LOG.debug(\"Disk file: \" + file + \" Length is \" + \n-          fs.getFileStatus(file).getLen());\n+    long rawBytes \u003d inMemToDiskBytes;\n+    CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n+        new CompressAwarePath[onDiskMapOutputs.size()]);\n+    for (CompressAwarePath file : onDisk) {\n+      long fileLength \u003d fs.getFileStatus(file).getLen();\n+      onDiskBytes +\u003d fileLength;\n+      rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n+\n+      LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n       diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                          (file.toString().endsWith(\n                                              Task.MERGED_OUTPUT_PREFIX) ?\n-                                          null : mergedMapOutputsCounter)\n+                                          null : mergedMapOutputsCounter), file.getRawDataLength()\n                                         ));\n     }\n     LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n              onDiskBytes + \" bytes from disk\");\n     Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n       public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n         if (o1.getLength() \u003d\u003d o2.getLength()) {\n           return 0;\n         }\n         return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n       }\n     });\n \n     // build final list of segments from merged backed by disk + in-mem\n     List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                              finalSegments, 0);\n     LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n              inMemBytes + \" bytes from memory into reduce\");\n     if (0 !\u003d onDiskBytes) {\n       final int numInMemSegments \u003d memDiskSegments.size();\n       diskSegments.addAll(0, memDiskSegments);\n       memDiskSegments.clear();\n       // Pass mergePhase only if there is a going to be intermediate\n       // merges. See comment where mergePhaseFinished is being set\n       Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n       RawKeyValueIterator diskMerge \u003d Merger.merge(\n           job, fs, keyClass, valueClass, diskSegments,\n           ioSortFactor, numInMemSegments, tmpDir, comparator,\n           reporter, false, spilledRecordsCounter, null, thisPhase);\n       diskSegments.clear();\n       if (0 \u003d\u003d finalSegments.size()) {\n         return diskMerge;\n       }\n       finalSegments.add(new Segment\u003cK,V\u003e(\n-            new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n+            new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n     }\n     return Merger.merge(job, fs, keyClass, valueClass,\n                  finalSegments, finalSegments.size(), tmpDir,\n                  comparator, reporter, spilledRecordsCounter, null,\n                  null);\n   \n   }\n\\ No newline at end of file\n",
          "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" + \n             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    \n    final float maxRedPer \u003d\n      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                            maxRedPer);\n    }\n    int maxInMemReduce \u003d (int)Math.min(\n        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n    \n\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n        final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n            keyClass, valueClass, codec, null);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          // add to list of final disk outputs.\n          onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n              writer.getRawLength()));\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    long rawBytes \u003d inMemToDiskBytes;\n    CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n        new CompressAwarePath[onDiskMapOutputs.size()]);\n    for (CompressAwarePath file : onDisk) {\n      long fileLength \u003d fs.getFileStatus(file).getLen();\n      onDiskBytes +\u003d fileLength;\n      rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter), file.getRawDataLength()\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "da4cab10990b3a352fc2c699f3b41c994ac55e95": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "Revering MAPREDUCE-2264\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1439561 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/01/13 10:58 AM",
      "commitName": "da4cab10990b3a352fc2c699f3b41c994ac55e95",
      "commitAuthor": "Alejandro Abdelnur",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "Revering MAPREDUCE-2264\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1439561 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/01/13 10:58 AM",
          "commitName": "da4cab10990b3a352fc2c699f3b41c994ac55e95",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "24/01/13 4:25 PM",
          "commitNameOld": "539153a6798a667d39f20972c5ae0936060e2cc1",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 3.77,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,148 +1,143 @@\n   private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                        List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n-                                       List\u003cCompressAwarePath\u003e onDiskMapOutputs\n+                                       List\u003cPath\u003e onDiskMapOutputs\n                                        ) throws IOException {\n     LOG.info(\"finalMerge called with \" + \n              inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n              onDiskMapOutputs.size() + \" on-disk map-outputs\");\n     \n     final float maxRedPer \u003d\n       job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n     if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n       throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                             maxRedPer);\n     }\n     int maxInMemReduce \u003d (int)Math.min(\n         Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n     \n \n     // merge config params\n     Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n     Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n     boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n     final Path tmpDir \u003d new Path(reduceId.toString());\n     final RawComparator\u003cK\u003e comparator \u003d\n       (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n \n     // segments required to vacate memory\n     List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemToDiskBytes \u003d 0;\n     boolean mergePhaseFinished \u003d false;\n     if (inMemoryMapOutputs.size() \u003e 0) {\n       TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n       inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                 memDiskSegments,\n                                                 maxInMemReduce);\n       final int numMemDiskSegments \u003d memDiskSegments.size();\n       if (numMemDiskSegments \u003e 0 \u0026\u0026\n             ioSortFactor \u003e onDiskMapOutputs.size()) {\n         \n         // If we reach here, it implies that we have less than io.sort.factor\n         // disk segments and this will be incremented by 1 (result of the \n         // memory segments merge). Since this total would still be \n         // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n         // the merge of all these disk segments would be directly fed to the\n         // reduce method\n         \n         mergePhaseFinished \u003d true;\n         // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n         final Path outputPath \u003d \n           mapOutputFile.getInputFileForWrite(mapId,\n                                              inMemToDiskBytes).suffix(\n                                                  Task.MERGED_OUTPUT_PREFIX);\n         final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n             keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n             tmpDir, comparator, reporter, spilledRecordsCounter, null, \n             mergePhase);\n         final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n             keyClass, valueClass, codec, null);\n         try {\n           Merger.writeFile(rIter, writer, reporter, job);\n           // add to list of final disk outputs.\n-          onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n-              writer.getRawLength()));\n+          onDiskMapOutputs.add(outputPath);\n         } catch (IOException e) {\n           if (null !\u003d outputPath) {\n             try {\n               fs.delete(outputPath, true);\n             } catch (IOException ie) {\n               // NOTHING\n             }\n           }\n           throw e;\n         } finally {\n           if (null !\u003d writer) {\n             writer.close();\n           }\n         }\n         LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                  \"reduce memory limit\");\n         inMemToDiskBytes \u003d 0;\n         memDiskSegments.clear();\n       } else if (inMemToDiskBytes !\u003d 0) {\n         LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes in memory for \" +\n                  \"intermediate, on-disk merge\");\n       }\n     }\n \n     // segments on disk\n     List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long onDiskBytes \u003d inMemToDiskBytes;\n-    long rawBytes \u003d inMemToDiskBytes;\n-    CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n-        new CompressAwarePath[onDiskMapOutputs.size()]);\n-    for (CompressAwarePath file : onDisk) {\n-      long fileLength \u003d fs.getFileStatus(file.getPath()).getLen();\n-      onDiskBytes +\u003d fileLength;\n-      rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n-\n-      LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n-      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file.getPath(), codec, keepInputs,\n+    Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n+    for (Path file : onDisk) {\n+      onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n+      LOG.debug(\"Disk file: \" + file + \" Length is \" + \n+          fs.getFileStatus(file).getLen());\n+      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                          (file.toString().endsWith(\n                                              Task.MERGED_OUTPUT_PREFIX) ?\n-                                          null : mergedMapOutputsCounter), file.getRawDataLength()\n+                                          null : mergedMapOutputsCounter)\n                                         ));\n     }\n     LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n              onDiskBytes + \" bytes from disk\");\n     Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n       public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n         if (o1.getLength() \u003d\u003d o2.getLength()) {\n           return 0;\n         }\n         return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n       }\n     });\n \n     // build final list of segments from merged backed by disk + in-mem\n     List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                              finalSegments, 0);\n     LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n              inMemBytes + \" bytes from memory into reduce\");\n     if (0 !\u003d onDiskBytes) {\n       final int numInMemSegments \u003d memDiskSegments.size();\n       diskSegments.addAll(0, memDiskSegments);\n       memDiskSegments.clear();\n       // Pass mergePhase only if there is a going to be intermediate\n       // merges. See comment where mergePhaseFinished is being set\n       Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n       RawKeyValueIterator diskMerge \u003d Merger.merge(\n           job, fs, keyClass, valueClass, diskSegments,\n           ioSortFactor, numInMemSegments, tmpDir, comparator,\n           reporter, false, spilledRecordsCounter, null, thisPhase);\n       diskSegments.clear();\n       if (0 \u003d\u003d finalSegments.size()) {\n         return diskMerge;\n       }\n       finalSegments.add(new Segment\u003cK,V\u003e(\n-            new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n+            new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n     }\n     return Merger.merge(job, fs, keyClass, valueClass,\n                  finalSegments, finalSegments.size(), tmpDir,\n                  comparator, reporter, spilledRecordsCounter, null,\n                  null);\n   \n   }\n\\ No newline at end of file\n",
          "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cPath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" + \n             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    \n    final float maxRedPer \u003d\n      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                            maxRedPer);\n    }\n    int maxInMemReduce \u003d (int)Math.min(\n        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n    \n\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n        final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n            keyClass, valueClass, codec, null);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          // add to list of final disk outputs.\n          onDiskMapOutputs.add(outputPath);\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n    for (Path file : onDisk) {\n      onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + \n          fs.getFileStatus(file).getLen());\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter)\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {
            "oldValue": "[job-JobConf, fs-FileSystem, inMemoryMapOutputs-List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e, onDiskMapOutputs-List\u003cCompressAwarePath\u003e]",
            "newValue": "[job-JobConf, fs-FileSystem, inMemoryMapOutputs-List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e, onDiskMapOutputs-List\u003cPath\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "Revering MAPREDUCE-2264\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1439561 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/01/13 10:58 AM",
          "commitName": "da4cab10990b3a352fc2c699f3b41c994ac55e95",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "24/01/13 4:25 PM",
          "commitNameOld": "539153a6798a667d39f20972c5ae0936060e2cc1",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 3.77,
          "commitsBetweenForRepo": 9,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,148 +1,143 @@\n   private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                        List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n-                                       List\u003cCompressAwarePath\u003e onDiskMapOutputs\n+                                       List\u003cPath\u003e onDiskMapOutputs\n                                        ) throws IOException {\n     LOG.info(\"finalMerge called with \" + \n              inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n              onDiskMapOutputs.size() + \" on-disk map-outputs\");\n     \n     final float maxRedPer \u003d\n       job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n     if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n       throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                             maxRedPer);\n     }\n     int maxInMemReduce \u003d (int)Math.min(\n         Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n     \n \n     // merge config params\n     Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n     Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n     boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n     final Path tmpDir \u003d new Path(reduceId.toString());\n     final RawComparator\u003cK\u003e comparator \u003d\n       (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n \n     // segments required to vacate memory\n     List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemToDiskBytes \u003d 0;\n     boolean mergePhaseFinished \u003d false;\n     if (inMemoryMapOutputs.size() \u003e 0) {\n       TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n       inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                 memDiskSegments,\n                                                 maxInMemReduce);\n       final int numMemDiskSegments \u003d memDiskSegments.size();\n       if (numMemDiskSegments \u003e 0 \u0026\u0026\n             ioSortFactor \u003e onDiskMapOutputs.size()) {\n         \n         // If we reach here, it implies that we have less than io.sort.factor\n         // disk segments and this will be incremented by 1 (result of the \n         // memory segments merge). Since this total would still be \n         // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n         // the merge of all these disk segments would be directly fed to the\n         // reduce method\n         \n         mergePhaseFinished \u003d true;\n         // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n         final Path outputPath \u003d \n           mapOutputFile.getInputFileForWrite(mapId,\n                                              inMemToDiskBytes).suffix(\n                                                  Task.MERGED_OUTPUT_PREFIX);\n         final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n             keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n             tmpDir, comparator, reporter, spilledRecordsCounter, null, \n             mergePhase);\n         final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n             keyClass, valueClass, codec, null);\n         try {\n           Merger.writeFile(rIter, writer, reporter, job);\n           // add to list of final disk outputs.\n-          onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n-              writer.getRawLength()));\n+          onDiskMapOutputs.add(outputPath);\n         } catch (IOException e) {\n           if (null !\u003d outputPath) {\n             try {\n               fs.delete(outputPath, true);\n             } catch (IOException ie) {\n               // NOTHING\n             }\n           }\n           throw e;\n         } finally {\n           if (null !\u003d writer) {\n             writer.close();\n           }\n         }\n         LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                  \"reduce memory limit\");\n         inMemToDiskBytes \u003d 0;\n         memDiskSegments.clear();\n       } else if (inMemToDiskBytes !\u003d 0) {\n         LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes in memory for \" +\n                  \"intermediate, on-disk merge\");\n       }\n     }\n \n     // segments on disk\n     List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long onDiskBytes \u003d inMemToDiskBytes;\n-    long rawBytes \u003d inMemToDiskBytes;\n-    CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n-        new CompressAwarePath[onDiskMapOutputs.size()]);\n-    for (CompressAwarePath file : onDisk) {\n-      long fileLength \u003d fs.getFileStatus(file.getPath()).getLen();\n-      onDiskBytes +\u003d fileLength;\n-      rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n-\n-      LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n-      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file.getPath(), codec, keepInputs,\n+    Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n+    for (Path file : onDisk) {\n+      onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n+      LOG.debug(\"Disk file: \" + file + \" Length is \" + \n+          fs.getFileStatus(file).getLen());\n+      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                          (file.toString().endsWith(\n                                              Task.MERGED_OUTPUT_PREFIX) ?\n-                                          null : mergedMapOutputsCounter), file.getRawDataLength()\n+                                          null : mergedMapOutputsCounter)\n                                         ));\n     }\n     LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n              onDiskBytes + \" bytes from disk\");\n     Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n       public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n         if (o1.getLength() \u003d\u003d o2.getLength()) {\n           return 0;\n         }\n         return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n       }\n     });\n \n     // build final list of segments from merged backed by disk + in-mem\n     List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                              finalSegments, 0);\n     LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n              inMemBytes + \" bytes from memory into reduce\");\n     if (0 !\u003d onDiskBytes) {\n       final int numInMemSegments \u003d memDiskSegments.size();\n       diskSegments.addAll(0, memDiskSegments);\n       memDiskSegments.clear();\n       // Pass mergePhase only if there is a going to be intermediate\n       // merges. See comment where mergePhaseFinished is being set\n       Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n       RawKeyValueIterator diskMerge \u003d Merger.merge(\n           job, fs, keyClass, valueClass, diskSegments,\n           ioSortFactor, numInMemSegments, tmpDir, comparator,\n           reporter, false, spilledRecordsCounter, null, thisPhase);\n       diskSegments.clear();\n       if (0 \u003d\u003d finalSegments.size()) {\n         return diskMerge;\n       }\n       finalSegments.add(new Segment\u003cK,V\u003e(\n-            new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n+            new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n     }\n     return Merger.merge(job, fs, keyClass, valueClass,\n                  finalSegments, finalSegments.size(), tmpDir,\n                  comparator, reporter, spilledRecordsCounter, null,\n                  null);\n   \n   }\n\\ No newline at end of file\n",
          "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cPath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" + \n             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    \n    final float maxRedPer \u003d\n      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                            maxRedPer);\n    }\n    int maxInMemReduce \u003d (int)Math.min(\n        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n    \n\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n        final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n            keyClass, valueClass, codec, null);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          // add to list of final disk outputs.\n          onDiskMapOutputs.add(outputPath);\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n    for (Path file : onDisk) {\n      onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + \n          fs.getFileStatus(file).getLen());\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter)\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "539153a6798a667d39f20972c5ae0936060e2cc1": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-2264. Job status exceeds 100% in some cases. (devaraj.k and sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1438277 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/01/13 4:25 PM",
      "commitName": "539153a6798a667d39f20972c5ae0936060e2cc1",
      "commitAuthor": "Alejandro Abdelnur",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-2264. Job status exceeds 100% in some cases. (devaraj.k and sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1438277 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "24/01/13 4:25 PM",
          "commitName": "539153a6798a667d39f20972c5ae0936060e2cc1",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "22/01/13 6:10 AM",
          "commitNameOld": "73fd247c7649919350ecfd16806af57ffe554649",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 2.43,
          "commitsBetweenForRepo": 13,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,143 +1,148 @@\n   private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                        List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n-                                       List\u003cPath\u003e onDiskMapOutputs\n+                                       List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                        ) throws IOException {\n     LOG.info(\"finalMerge called with \" + \n              inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n              onDiskMapOutputs.size() + \" on-disk map-outputs\");\n     \n     final float maxRedPer \u003d\n       job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n     if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n       throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                             maxRedPer);\n     }\n     int maxInMemReduce \u003d (int)Math.min(\n         Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n     \n \n     // merge config params\n     Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n     Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n     boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n     final Path tmpDir \u003d new Path(reduceId.toString());\n     final RawComparator\u003cK\u003e comparator \u003d\n       (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n \n     // segments required to vacate memory\n     List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemToDiskBytes \u003d 0;\n     boolean mergePhaseFinished \u003d false;\n     if (inMemoryMapOutputs.size() \u003e 0) {\n       TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n       inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                 memDiskSegments,\n                                                 maxInMemReduce);\n       final int numMemDiskSegments \u003d memDiskSegments.size();\n       if (numMemDiskSegments \u003e 0 \u0026\u0026\n             ioSortFactor \u003e onDiskMapOutputs.size()) {\n         \n         // If we reach here, it implies that we have less than io.sort.factor\n         // disk segments and this will be incremented by 1 (result of the \n         // memory segments merge). Since this total would still be \n         // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n         // the merge of all these disk segments would be directly fed to the\n         // reduce method\n         \n         mergePhaseFinished \u003d true;\n         // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n         final Path outputPath \u003d \n           mapOutputFile.getInputFileForWrite(mapId,\n                                              inMemToDiskBytes).suffix(\n                                                  Task.MERGED_OUTPUT_PREFIX);\n         final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n             keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n             tmpDir, comparator, reporter, spilledRecordsCounter, null, \n             mergePhase);\n         final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n             keyClass, valueClass, codec, null);\n         try {\n           Merger.writeFile(rIter, writer, reporter, job);\n           // add to list of final disk outputs.\n-          onDiskMapOutputs.add(outputPath);\n+          onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n+              writer.getRawLength()));\n         } catch (IOException e) {\n           if (null !\u003d outputPath) {\n             try {\n               fs.delete(outputPath, true);\n             } catch (IOException ie) {\n               // NOTHING\n             }\n           }\n           throw e;\n         } finally {\n           if (null !\u003d writer) {\n             writer.close();\n           }\n         }\n         LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                  \"reduce memory limit\");\n         inMemToDiskBytes \u003d 0;\n         memDiskSegments.clear();\n       } else if (inMemToDiskBytes !\u003d 0) {\n         LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes in memory for \" +\n                  \"intermediate, on-disk merge\");\n       }\n     }\n \n     // segments on disk\n     List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long onDiskBytes \u003d inMemToDiskBytes;\n-    Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n-    for (Path file : onDisk) {\n-      onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n-      LOG.debug(\"Disk file: \" + file + \" Length is \" + \n-          fs.getFileStatus(file).getLen());\n-      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n+    long rawBytes \u003d inMemToDiskBytes;\n+    CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n+        new CompressAwarePath[onDiskMapOutputs.size()]);\n+    for (CompressAwarePath file : onDisk) {\n+      long fileLength \u003d fs.getFileStatus(file.getPath()).getLen();\n+      onDiskBytes +\u003d fileLength;\n+      rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n+\n+      LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n+      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file.getPath(), codec, keepInputs,\n                                          (file.toString().endsWith(\n                                              Task.MERGED_OUTPUT_PREFIX) ?\n-                                          null : mergedMapOutputsCounter)\n+                                          null : mergedMapOutputsCounter), file.getRawDataLength()\n                                         ));\n     }\n     LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n              onDiskBytes + \" bytes from disk\");\n     Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n       public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n         if (o1.getLength() \u003d\u003d o2.getLength()) {\n           return 0;\n         }\n         return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n       }\n     });\n \n     // build final list of segments from merged backed by disk + in-mem\n     List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                              finalSegments, 0);\n     LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n              inMemBytes + \" bytes from memory into reduce\");\n     if (0 !\u003d onDiskBytes) {\n       final int numInMemSegments \u003d memDiskSegments.size();\n       diskSegments.addAll(0, memDiskSegments);\n       memDiskSegments.clear();\n       // Pass mergePhase only if there is a going to be intermediate\n       // merges. See comment where mergePhaseFinished is being set\n       Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n       RawKeyValueIterator diskMerge \u003d Merger.merge(\n           job, fs, keyClass, valueClass, diskSegments,\n           ioSortFactor, numInMemSegments, tmpDir, comparator,\n           reporter, false, spilledRecordsCounter, null, thisPhase);\n       diskSegments.clear();\n       if (0 \u003d\u003d finalSegments.size()) {\n         return diskMerge;\n       }\n       finalSegments.add(new Segment\u003cK,V\u003e(\n-            new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n+            new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n     }\n     return Merger.merge(job, fs, keyClass, valueClass,\n                  finalSegments, finalSegments.size(), tmpDir,\n                  comparator, reporter, spilledRecordsCounter, null,\n                  null);\n   \n   }\n\\ No newline at end of file\n",
          "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" + \n             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    \n    final float maxRedPer \u003d\n      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                            maxRedPer);\n    }\n    int maxInMemReduce \u003d (int)Math.min(\n        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n    \n\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n        final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n            keyClass, valueClass, codec, null);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          // add to list of final disk outputs.\n          onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n              writer.getRawLength()));\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    long rawBytes \u003d inMemToDiskBytes;\n    CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n        new CompressAwarePath[onDiskMapOutputs.size()]);\n    for (CompressAwarePath file : onDisk) {\n      long fileLength \u003d fs.getFileStatus(file.getPath()).getLen();\n      onDiskBytes +\u003d fileLength;\n      rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file.getPath(), codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter), file.getRawDataLength()\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {
            "oldValue": "[job-JobConf, fs-FileSystem, inMemoryMapOutputs-List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e, onDiskMapOutputs-List\u003cPath\u003e]",
            "newValue": "[job-JobConf, fs-FileSystem, inMemoryMapOutputs-List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e, onDiskMapOutputs-List\u003cCompressAwarePath\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-2264. Job status exceeds 100% in some cases. (devaraj.k and sandyr via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1438277 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "24/01/13 4:25 PM",
          "commitName": "539153a6798a667d39f20972c5ae0936060e2cc1",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "22/01/13 6:10 AM",
          "commitNameOld": "73fd247c7649919350ecfd16806af57ffe554649",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 2.43,
          "commitsBetweenForRepo": 13,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,143 +1,148 @@\n   private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                        List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n-                                       List\u003cPath\u003e onDiskMapOutputs\n+                                       List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                        ) throws IOException {\n     LOG.info(\"finalMerge called with \" + \n              inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n              onDiskMapOutputs.size() + \" on-disk map-outputs\");\n     \n     final float maxRedPer \u003d\n       job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n     if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n       throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                             maxRedPer);\n     }\n     int maxInMemReduce \u003d (int)Math.min(\n         Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n     \n \n     // merge config params\n     Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n     Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n     boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n     final Path tmpDir \u003d new Path(reduceId.toString());\n     final RawComparator\u003cK\u003e comparator \u003d\n       (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n \n     // segments required to vacate memory\n     List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemToDiskBytes \u003d 0;\n     boolean mergePhaseFinished \u003d false;\n     if (inMemoryMapOutputs.size() \u003e 0) {\n       TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n       inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                 memDiskSegments,\n                                                 maxInMemReduce);\n       final int numMemDiskSegments \u003d memDiskSegments.size();\n       if (numMemDiskSegments \u003e 0 \u0026\u0026\n             ioSortFactor \u003e onDiskMapOutputs.size()) {\n         \n         // If we reach here, it implies that we have less than io.sort.factor\n         // disk segments and this will be incremented by 1 (result of the \n         // memory segments merge). Since this total would still be \n         // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n         // the merge of all these disk segments would be directly fed to the\n         // reduce method\n         \n         mergePhaseFinished \u003d true;\n         // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n         final Path outputPath \u003d \n           mapOutputFile.getInputFileForWrite(mapId,\n                                              inMemToDiskBytes).suffix(\n                                                  Task.MERGED_OUTPUT_PREFIX);\n         final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n             keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n             tmpDir, comparator, reporter, spilledRecordsCounter, null, \n             mergePhase);\n         final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n             keyClass, valueClass, codec, null);\n         try {\n           Merger.writeFile(rIter, writer, reporter, job);\n           // add to list of final disk outputs.\n-          onDiskMapOutputs.add(outputPath);\n+          onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n+              writer.getRawLength()));\n         } catch (IOException e) {\n           if (null !\u003d outputPath) {\n             try {\n               fs.delete(outputPath, true);\n             } catch (IOException ie) {\n               // NOTHING\n             }\n           }\n           throw e;\n         } finally {\n           if (null !\u003d writer) {\n             writer.close();\n           }\n         }\n         LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                  \"reduce memory limit\");\n         inMemToDiskBytes \u003d 0;\n         memDiskSegments.clear();\n       } else if (inMemToDiskBytes !\u003d 0) {\n         LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes in memory for \" +\n                  \"intermediate, on-disk merge\");\n       }\n     }\n \n     // segments on disk\n     List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long onDiskBytes \u003d inMemToDiskBytes;\n-    Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n-    for (Path file : onDisk) {\n-      onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n-      LOG.debug(\"Disk file: \" + file + \" Length is \" + \n-          fs.getFileStatus(file).getLen());\n-      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n+    long rawBytes \u003d inMemToDiskBytes;\n+    CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n+        new CompressAwarePath[onDiskMapOutputs.size()]);\n+    for (CompressAwarePath file : onDisk) {\n+      long fileLength \u003d fs.getFileStatus(file.getPath()).getLen();\n+      onDiskBytes +\u003d fileLength;\n+      rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n+\n+      LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n+      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file.getPath(), codec, keepInputs,\n                                          (file.toString().endsWith(\n                                              Task.MERGED_OUTPUT_PREFIX) ?\n-                                          null : mergedMapOutputsCounter)\n+                                          null : mergedMapOutputsCounter), file.getRawDataLength()\n                                         ));\n     }\n     LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n              onDiskBytes + \" bytes from disk\");\n     Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n       public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n         if (o1.getLength() \u003d\u003d o2.getLength()) {\n           return 0;\n         }\n         return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n       }\n     });\n \n     // build final list of segments from merged backed by disk + in-mem\n     List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                              finalSegments, 0);\n     LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n              inMemBytes + \" bytes from memory into reduce\");\n     if (0 !\u003d onDiskBytes) {\n       final int numInMemSegments \u003d memDiskSegments.size();\n       diskSegments.addAll(0, memDiskSegments);\n       memDiskSegments.clear();\n       // Pass mergePhase only if there is a going to be intermediate\n       // merges. See comment where mergePhaseFinished is being set\n       Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n       RawKeyValueIterator diskMerge \u003d Merger.merge(\n           job, fs, keyClass, valueClass, diskSegments,\n           ioSortFactor, numInMemSegments, tmpDir, comparator,\n           reporter, false, spilledRecordsCounter, null, thisPhase);\n       diskSegments.clear();\n       if (0 \u003d\u003d finalSegments.size()) {\n         return diskMerge;\n       }\n       finalSegments.add(new Segment\u003cK,V\u003e(\n-            new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n+            new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n     }\n     return Merger.merge(job, fs, keyClass, valueClass,\n                  finalSegments, finalSegments.size(), tmpDir,\n                  comparator, reporter, spilledRecordsCounter, null,\n                  null);\n   \n   }\n\\ No newline at end of file\n",
          "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cCompressAwarePath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" + \n             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    \n    final float maxRedPer \u003d\n      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                            maxRedPer);\n    }\n    int maxInMemReduce \u003d (int)Math.min(\n        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n    \n\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n        final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n            keyClass, valueClass, codec, null);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          // add to list of final disk outputs.\n          onDiskMapOutputs.add(new CompressAwarePath(outputPath,\n              writer.getRawLength()));\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    long rawBytes \u003d inMemToDiskBytes;\n    CompressAwarePath[] onDisk \u003d onDiskMapOutputs.toArray(\n        new CompressAwarePath[onDiskMapOutputs.size()]);\n    for (CompressAwarePath file : onDisk) {\n      long fileLength \u003d fs.getFileStatus(file.getPath()).getLen();\n      onDiskBytes +\u003d fileLength;\n      rawBytes +\u003d (file.getRawDataLength() \u003e 0) ? file.getRawDataLength() : fileLength;\n\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + fileLength);\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file.getPath(), codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter), file.getRawDataLength()\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true, rawBytes));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "73fd247c7649919350ecfd16806af57ffe554649": {
      "type": "Ymultichange(Ymovefromfile,Yparameterchange)",
      "commitMessage": "MAPREDUCE-4808. Refactor MapOutput and MergeManager to facilitate reuse by Shuffle implementations. (masokan via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1436936 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/01/13 6:10 AM",
      "commitName": "73fd247c7649919350ecfd16806af57ffe554649",
      "commitAuthor": "Alejandro Abdelnur",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "MAPREDUCE-4808. Refactor MapOutput and MergeManager to facilitate reuse by Shuffle implementations. (masokan via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1436936 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/01/13 6:10 AM",
          "commitName": "73fd247c7649919350ecfd16806af57ffe554649",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "21/01/13 6:59 PM",
          "commitNameOld": "cfae13306ac0fb3f3c139d5ac511bf78cede1b77",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.47,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,143 +1,143 @@\n   private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n-                                       List\u003cMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n+                                       List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                        List\u003cPath\u003e onDiskMapOutputs\n                                        ) throws IOException {\n     LOG.info(\"finalMerge called with \" + \n              inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n              onDiskMapOutputs.size() + \" on-disk map-outputs\");\n     \n     final float maxRedPer \u003d\n       job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n     if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n       throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                             maxRedPer);\n     }\n     int maxInMemReduce \u003d (int)Math.min(\n         Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n     \n \n     // merge config params\n     Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n     Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n     boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n     final Path tmpDir \u003d new Path(reduceId.toString());\n     final RawComparator\u003cK\u003e comparator \u003d\n       (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n \n     // segments required to vacate memory\n     List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemToDiskBytes \u003d 0;\n     boolean mergePhaseFinished \u003d false;\n     if (inMemoryMapOutputs.size() \u003e 0) {\n       TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n       inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                 memDiskSegments,\n                                                 maxInMemReduce);\n       final int numMemDiskSegments \u003d memDiskSegments.size();\n       if (numMemDiskSegments \u003e 0 \u0026\u0026\n             ioSortFactor \u003e onDiskMapOutputs.size()) {\n         \n         // If we reach here, it implies that we have less than io.sort.factor\n         // disk segments and this will be incremented by 1 (result of the \n         // memory segments merge). Since this total would still be \n         // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n         // the merge of all these disk segments would be directly fed to the\n         // reduce method\n         \n         mergePhaseFinished \u003d true;\n         // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n         final Path outputPath \u003d \n           mapOutputFile.getInputFileForWrite(mapId,\n                                              inMemToDiskBytes).suffix(\n                                                  Task.MERGED_OUTPUT_PREFIX);\n         final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n             keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n             tmpDir, comparator, reporter, spilledRecordsCounter, null, \n             mergePhase);\n         final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n             keyClass, valueClass, codec, null);\n         try {\n           Merger.writeFile(rIter, writer, reporter, job);\n           // add to list of final disk outputs.\n           onDiskMapOutputs.add(outputPath);\n         } catch (IOException e) {\n           if (null !\u003d outputPath) {\n             try {\n               fs.delete(outputPath, true);\n             } catch (IOException ie) {\n               // NOTHING\n             }\n           }\n           throw e;\n         } finally {\n           if (null !\u003d writer) {\n             writer.close();\n           }\n         }\n         LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                  \"reduce memory limit\");\n         inMemToDiskBytes \u003d 0;\n         memDiskSegments.clear();\n       } else if (inMemToDiskBytes !\u003d 0) {\n         LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes in memory for \" +\n                  \"intermediate, on-disk merge\");\n       }\n     }\n \n     // segments on disk\n     List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long onDiskBytes \u003d inMemToDiskBytes;\n     Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n     for (Path file : onDisk) {\n       onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n       LOG.debug(\"Disk file: \" + file + \" Length is \" + \n           fs.getFileStatus(file).getLen());\n       diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                          (file.toString().endsWith(\n                                              Task.MERGED_OUTPUT_PREFIX) ?\n                                           null : mergedMapOutputsCounter)\n                                         ));\n     }\n     LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n              onDiskBytes + \" bytes from disk\");\n     Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n       public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n         if (o1.getLength() \u003d\u003d o2.getLength()) {\n           return 0;\n         }\n         return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n       }\n     });\n \n     // build final list of segments from merged backed by disk + in-mem\n     List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                              finalSegments, 0);\n     LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n              inMemBytes + \" bytes from memory into reduce\");\n     if (0 !\u003d onDiskBytes) {\n       final int numInMemSegments \u003d memDiskSegments.size();\n       diskSegments.addAll(0, memDiskSegments);\n       memDiskSegments.clear();\n       // Pass mergePhase only if there is a going to be intermediate\n       // merges. See comment where mergePhaseFinished is being set\n       Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n       RawKeyValueIterator diskMerge \u003d Merger.merge(\n           job, fs, keyClass, valueClass, diskSegments,\n           ioSortFactor, numInMemSegments, tmpDir, comparator,\n           reporter, false, spilledRecordsCounter, null, thisPhase);\n       diskSegments.clear();\n       if (0 \u003d\u003d finalSegments.size()) {\n         return diskMerge;\n       }\n       finalSegments.add(new Segment\u003cK,V\u003e(\n             new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n     }\n     return Merger.merge(job, fs, keyClass, valueClass,\n                  finalSegments, finalSegments.size(), tmpDir,\n                  comparator, reporter, spilledRecordsCounter, null,\n                  null);\n   \n   }\n\\ No newline at end of file\n",
          "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cPath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" + \n             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    \n    final float maxRedPer \u003d\n      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                            maxRedPer);\n    }\n    int maxInMemReduce \u003d (int)Math.min(\n        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n    \n\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n        final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n            keyClass, valueClass, codec, null);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          // add to list of final disk outputs.\n          onDiskMapOutputs.add(outputPath);\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n    for (Path file : onDisk) {\n      onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + \n          fs.getFileStatus(file).getLen());\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter)\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {
            "oldPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
            "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
            "oldMethodName": "finalMerge",
            "newMethodName": "finalMerge"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-4808. Refactor MapOutput and MergeManager to facilitate reuse by Shuffle implementations. (masokan via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1436936 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/01/13 6:10 AM",
          "commitName": "73fd247c7649919350ecfd16806af57ffe554649",
          "commitAuthor": "Alejandro Abdelnur",
          "commitDateOld": "21/01/13 6:59 PM",
          "commitNameOld": "cfae13306ac0fb3f3c139d5ac511bf78cede1b77",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.47,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,143 +1,143 @@\n   private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n-                                       List\u003cMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n+                                       List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                        List\u003cPath\u003e onDiskMapOutputs\n                                        ) throws IOException {\n     LOG.info(\"finalMerge called with \" + \n              inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n              onDiskMapOutputs.size() + \" on-disk map-outputs\");\n     \n     final float maxRedPer \u003d\n       job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n     if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n       throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                             maxRedPer);\n     }\n     int maxInMemReduce \u003d (int)Math.min(\n         Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n     \n \n     // merge config params\n     Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n     Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n     boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n     final Path tmpDir \u003d new Path(reduceId.toString());\n     final RawComparator\u003cK\u003e comparator \u003d\n       (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n \n     // segments required to vacate memory\n     List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemToDiskBytes \u003d 0;\n     boolean mergePhaseFinished \u003d false;\n     if (inMemoryMapOutputs.size() \u003e 0) {\n       TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n       inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                 memDiskSegments,\n                                                 maxInMemReduce);\n       final int numMemDiskSegments \u003d memDiskSegments.size();\n       if (numMemDiskSegments \u003e 0 \u0026\u0026\n             ioSortFactor \u003e onDiskMapOutputs.size()) {\n         \n         // If we reach here, it implies that we have less than io.sort.factor\n         // disk segments and this will be incremented by 1 (result of the \n         // memory segments merge). Since this total would still be \n         // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n         // the merge of all these disk segments would be directly fed to the\n         // reduce method\n         \n         mergePhaseFinished \u003d true;\n         // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n         final Path outputPath \u003d \n           mapOutputFile.getInputFileForWrite(mapId,\n                                              inMemToDiskBytes).suffix(\n                                                  Task.MERGED_OUTPUT_PREFIX);\n         final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n             keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n             tmpDir, comparator, reporter, spilledRecordsCounter, null, \n             mergePhase);\n         final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n             keyClass, valueClass, codec, null);\n         try {\n           Merger.writeFile(rIter, writer, reporter, job);\n           // add to list of final disk outputs.\n           onDiskMapOutputs.add(outputPath);\n         } catch (IOException e) {\n           if (null !\u003d outputPath) {\n             try {\n               fs.delete(outputPath, true);\n             } catch (IOException ie) {\n               // NOTHING\n             }\n           }\n           throw e;\n         } finally {\n           if (null !\u003d writer) {\n             writer.close();\n           }\n         }\n         LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                  \"reduce memory limit\");\n         inMemToDiskBytes \u003d 0;\n         memDiskSegments.clear();\n       } else if (inMemToDiskBytes !\u003d 0) {\n         LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                  inMemToDiskBytes + \" bytes in memory for \" +\n                  \"intermediate, on-disk merge\");\n       }\n     }\n \n     // segments on disk\n     List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long onDiskBytes \u003d inMemToDiskBytes;\n     Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n     for (Path file : onDisk) {\n       onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n       LOG.debug(\"Disk file: \" + file + \" Length is \" + \n           fs.getFileStatus(file).getLen());\n       diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                          (file.toString().endsWith(\n                                              Task.MERGED_OUTPUT_PREFIX) ?\n                                           null : mergedMapOutputsCounter)\n                                         ));\n     }\n     LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n              onDiskBytes + \" bytes from disk\");\n     Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n       public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n         if (o1.getLength() \u003d\u003d o2.getLength()) {\n           return 0;\n         }\n         return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n       }\n     });\n \n     // build final list of segments from merged backed by disk + in-mem\n     List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n     long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                              finalSegments, 0);\n     LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n              inMemBytes + \" bytes from memory into reduce\");\n     if (0 !\u003d onDiskBytes) {\n       final int numInMemSegments \u003d memDiskSegments.size();\n       diskSegments.addAll(0, memDiskSegments);\n       memDiskSegments.clear();\n       // Pass mergePhase only if there is a going to be intermediate\n       // merges. See comment where mergePhaseFinished is being set\n       Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n       RawKeyValueIterator diskMerge \u003d Merger.merge(\n           job, fs, keyClass, valueClass, diskSegments,\n           ioSortFactor, numInMemSegments, tmpDir, comparator,\n           reporter, false, spilledRecordsCounter, null, thisPhase);\n       diskSegments.clear();\n       if (0 \u003d\u003d finalSegments.size()) {\n         return diskMerge;\n       }\n       finalSegments.add(new Segment\u003cK,V\u003e(\n             new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n     }\n     return Merger.merge(job, fs, keyClass, valueClass,\n                  finalSegments, finalSegments.size(), tmpDir,\n                  comparator, reporter, spilledRecordsCounter, null,\n                  null);\n   \n   }\n\\ No newline at end of file\n",
          "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cPath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" + \n             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    \n    final float maxRedPer \u003d\n      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                            maxRedPer);\n    }\n    int maxInMemReduce \u003d (int)Math.min(\n        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n    \n\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n        final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n            keyClass, valueClass, codec, null);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          // add to list of final disk outputs.\n          onDiskMapOutputs.add(outputPath);\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n    for (Path file : onDisk) {\n      onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + \n          fs.getFileStatus(file).getLen());\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter)\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManagerImpl.java",
          "extendedDetails": {
            "oldValue": "[job-JobConf, fs-FileSystem, inMemoryMapOutputs-List\u003cMapOutput\u003cK,V\u003e\u003e, onDiskMapOutputs-List\u003cPath\u003e]",
            "newValue": "[job-JobConf, fs-FileSystem, inMemoryMapOutputs-List\u003cInMemoryMapOutput\u003cK,V\u003e\u003e, onDiskMapOutputs-List\u003cPath\u003e]"
          }
        }
      ]
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cPath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" + \n             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    \n    final float maxRedPer \u003d\n      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                            maxRedPer);\n    }\n    int maxInMemReduce \u003d (int)Math.min(\n        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n    \n\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n        final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n            keyClass, valueClass, codec, null);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          // add to list of final disk outputs.\n          onDiskMapOutputs.add(outputPath);\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n    for (Path file : onDisk) {\n      onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + \n          fs.getFileStatus(file).getLen());\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter)\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymovefromfile",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cPath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" + \n             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    \n    final float maxRedPer \u003d\n      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                            maxRedPer);\n    }\n    int maxInMemReduce \u003d (int)Math.min(\n        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n    \n\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n        final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n            keyClass, valueClass, codec, null);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          // add to list of final disk outputs.\n          onDiskMapOutputs.add(outputPath);\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n    for (Path file : onDisk) {\n      onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + \n          fs.getFileStatus(file).getLen());\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter)\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java",
        "oldMethodName": "finalMerge",
        "newMethodName": "finalMerge"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,143 @@\n+  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n+                                       List\u003cMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n+                                       List\u003cPath\u003e onDiskMapOutputs\n+                                       ) throws IOException {\n+    LOG.info(\"finalMerge called with \" + \n+             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n+             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n+    \n+    final float maxRedPer \u003d\n+      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n+    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n+      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n+                            maxRedPer);\n+    }\n+    int maxInMemReduce \u003d (int)Math.min(\n+        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n+    \n+\n+    // merge config params\n+    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n+    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n+    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n+    final Path tmpDir \u003d new Path(reduceId.toString());\n+    final RawComparator\u003cK\u003e comparator \u003d\n+      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n+\n+    // segments required to vacate memory\n+    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n+    long inMemToDiskBytes \u003d 0;\n+    boolean mergePhaseFinished \u003d false;\n+    if (inMemoryMapOutputs.size() \u003e 0) {\n+      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n+      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n+                                                memDiskSegments,\n+                                                maxInMemReduce);\n+      final int numMemDiskSegments \u003d memDiskSegments.size();\n+      if (numMemDiskSegments \u003e 0 \u0026\u0026\n+            ioSortFactor \u003e onDiskMapOutputs.size()) {\n+        \n+        // If we reach here, it implies that we have less than io.sort.factor\n+        // disk segments and this will be incremented by 1 (result of the \n+        // memory segments merge). Since this total would still be \n+        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n+        // the merge of all these disk segments would be directly fed to the\n+        // reduce method\n+        \n+        mergePhaseFinished \u003d true;\n+        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n+        final Path outputPath \u003d \n+          mapOutputFile.getInputFileForWrite(mapId,\n+                                             inMemToDiskBytes).suffix(\n+                                                 Task.MERGED_OUTPUT_PREFIX);\n+        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n+            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n+            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n+            mergePhase);\n+        final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n+            keyClass, valueClass, codec, null);\n+        try {\n+          Merger.writeFile(rIter, writer, reporter, job);\n+          // add to list of final disk outputs.\n+          onDiskMapOutputs.add(outputPath);\n+        } catch (IOException e) {\n+          if (null !\u003d outputPath) {\n+            try {\n+              fs.delete(outputPath, true);\n+            } catch (IOException ie) {\n+              // NOTHING\n+            }\n+          }\n+          throw e;\n+        } finally {\n+          if (null !\u003d writer) {\n+            writer.close();\n+          }\n+        }\n+        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n+                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n+                 \"reduce memory limit\");\n+        inMemToDiskBytes \u003d 0;\n+        memDiskSegments.clear();\n+      } else if (inMemToDiskBytes !\u003d 0) {\n+        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n+                 inMemToDiskBytes + \" bytes in memory for \" +\n+                 \"intermediate, on-disk merge\");\n+      }\n+    }\n+\n+    // segments on disk\n+    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n+    long onDiskBytes \u003d inMemToDiskBytes;\n+    Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n+    for (Path file : onDisk) {\n+      onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n+      LOG.debug(\"Disk file: \" + file + \" Length is \" + \n+          fs.getFileStatus(file).getLen());\n+      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n+                                         (file.toString().endsWith(\n+                                             Task.MERGED_OUTPUT_PREFIX) ?\n+                                          null : mergedMapOutputsCounter)\n+                                        ));\n+    }\n+    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n+             onDiskBytes + \" bytes from disk\");\n+    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n+      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n+        if (o1.getLength() \u003d\u003d o2.getLength()) {\n+          return 0;\n+        }\n+        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n+      }\n+    });\n+\n+    // build final list of segments from merged backed by disk + in-mem\n+    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n+    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n+                                             finalSegments, 0);\n+    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n+             inMemBytes + \" bytes from memory into reduce\");\n+    if (0 !\u003d onDiskBytes) {\n+      final int numInMemSegments \u003d memDiskSegments.size();\n+      diskSegments.addAll(0, memDiskSegments);\n+      memDiskSegments.clear();\n+      // Pass mergePhase only if there is a going to be intermediate\n+      // merges. See comment where mergePhaseFinished is being set\n+      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n+      RawKeyValueIterator diskMerge \u003d Merger.merge(\n+          job, fs, keyClass, valueClass, diskSegments,\n+          ioSortFactor, numInMemSegments, tmpDir, comparator,\n+          reporter, false, spilledRecordsCounter, null, thisPhase);\n+      diskSegments.clear();\n+      if (0 \u003d\u003d finalSegments.size()) {\n+        return diskMerge;\n+      }\n+      finalSegments.add(new Segment\u003cK,V\u003e(\n+            new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n+    }\n+    return Merger.merge(job, fs, keyClass, valueClass,\n+                 finalSegments, finalSegments.size(), tmpDir,\n+                 comparator, reporter, spilledRecordsCounter, null,\n+                 null);\n+  \n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private RawKeyValueIterator finalMerge(JobConf job, FileSystem fs,\n                                       List\u003cMapOutput\u003cK,V\u003e\u003e inMemoryMapOutputs,\n                                       List\u003cPath\u003e onDiskMapOutputs\n                                       ) throws IOException {\n    LOG.info(\"finalMerge called with \" + \n             inMemoryMapOutputs.size() + \" in-memory map-outputs and \" + \n             onDiskMapOutputs.size() + \" on-disk map-outputs\");\n    \n    final float maxRedPer \u003d\n      job.getFloat(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT, 0f);\n    if (maxRedPer \u003e 1.0 || maxRedPer \u003c 0.0) {\n      throw new IOException(MRJobConfig.REDUCE_INPUT_BUFFER_PERCENT +\n                            maxRedPer);\n    }\n    int maxInMemReduce \u003d (int)Math.min(\n        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);\n    \n\n    // merge config params\n    Class\u003cK\u003e keyClass \u003d (Class\u003cK\u003e)job.getMapOutputKeyClass();\n    Class\u003cV\u003e valueClass \u003d (Class\u003cV\u003e)job.getMapOutputValueClass();\n    boolean keepInputs \u003d job.getKeepFailedTaskFiles();\n    final Path tmpDir \u003d new Path(reduceId.toString());\n    final RawComparator\u003cK\u003e comparator \u003d\n      (RawComparator\u003cK\u003e)job.getOutputKeyComparator();\n\n    // segments required to vacate memory\n    List\u003cSegment\u003cK,V\u003e\u003e memDiskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemToDiskBytes \u003d 0;\n    boolean mergePhaseFinished \u003d false;\n    if (inMemoryMapOutputs.size() \u003e 0) {\n      TaskID mapId \u003d inMemoryMapOutputs.get(0).getMapId().getTaskID();\n      inMemToDiskBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                                memDiskSegments,\n                                                maxInMemReduce);\n      final int numMemDiskSegments \u003d memDiskSegments.size();\n      if (numMemDiskSegments \u003e 0 \u0026\u0026\n            ioSortFactor \u003e onDiskMapOutputs.size()) {\n        \n        // If we reach here, it implies that we have less than io.sort.factor\n        // disk segments and this will be incremented by 1 (result of the \n        // memory segments merge). Since this total would still be \n        // \u003c\u003d io.sort.factor, we will not do any more intermediate merges,\n        // the merge of all these disk segments would be directly fed to the\n        // reduce method\n        \n        mergePhaseFinished \u003d true;\n        // must spill to disk, but can\u0027t retain in-mem for intermediate merge\n        final Path outputPath \u003d \n          mapOutputFile.getInputFileForWrite(mapId,\n                                             inMemToDiskBytes).suffix(\n                                                 Task.MERGED_OUTPUT_PREFIX);\n        final RawKeyValueIterator rIter \u003d Merger.merge(job, fs,\n            keyClass, valueClass, memDiskSegments, numMemDiskSegments,\n            tmpDir, comparator, reporter, spilledRecordsCounter, null, \n            mergePhase);\n        final Writer\u003cK,V\u003e writer \u003d new Writer\u003cK,V\u003e(job, fs, outputPath,\n            keyClass, valueClass, codec, null);\n        try {\n          Merger.writeFile(rIter, writer, reporter, job);\n          // add to list of final disk outputs.\n          onDiskMapOutputs.add(outputPath);\n        } catch (IOException e) {\n          if (null !\u003d outputPath) {\n            try {\n              fs.delete(outputPath, true);\n            } catch (IOException ie) {\n              // NOTHING\n            }\n          }\n          throw e;\n        } finally {\n          if (null !\u003d writer) {\n            writer.close();\n          }\n        }\n        LOG.info(\"Merged \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes to disk to satisfy \" +\n                 \"reduce memory limit\");\n        inMemToDiskBytes \u003d 0;\n        memDiskSegments.clear();\n      } else if (inMemToDiskBytes !\u003d 0) {\n        LOG.info(\"Keeping \" + numMemDiskSegments + \" segments, \" +\n                 inMemToDiskBytes + \" bytes in memory for \" +\n                 \"intermediate, on-disk merge\");\n      }\n    }\n\n    // segments on disk\n    List\u003cSegment\u003cK,V\u003e\u003e diskSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long onDiskBytes \u003d inMemToDiskBytes;\n    Path[] onDisk \u003d onDiskMapOutputs.toArray(new Path[onDiskMapOutputs.size()]);\n    for (Path file : onDisk) {\n      onDiskBytes +\u003d fs.getFileStatus(file).getLen();\n      LOG.debug(\"Disk file: \" + file + \" Length is \" + \n          fs.getFileStatus(file).getLen());\n      diskSegments.add(new Segment\u003cK, V\u003e(job, fs, file, codec, keepInputs,\n                                         (file.toString().endsWith(\n                                             Task.MERGED_OUTPUT_PREFIX) ?\n                                          null : mergedMapOutputsCounter)\n                                        ));\n    }\n    LOG.info(\"Merging \" + onDisk.length + \" files, \" +\n             onDiskBytes + \" bytes from disk\");\n    Collections.sort(diskSegments, new Comparator\u003cSegment\u003cK,V\u003e\u003e() {\n      public int compare(Segment\u003cK, V\u003e o1, Segment\u003cK, V\u003e o2) {\n        if (o1.getLength() \u003d\u003d o2.getLength()) {\n          return 0;\n        }\n        return o1.getLength() \u003c o2.getLength() ? -1 : 1;\n      }\n    });\n\n    // build final list of segments from merged backed by disk + in-mem\n    List\u003cSegment\u003cK,V\u003e\u003e finalSegments \u003d new ArrayList\u003cSegment\u003cK,V\u003e\u003e();\n    long inMemBytes \u003d createInMemorySegments(inMemoryMapOutputs, \n                                             finalSegments, 0);\n    LOG.info(\"Merging \" + finalSegments.size() + \" segments, \" +\n             inMemBytes + \" bytes from memory into reduce\");\n    if (0 !\u003d onDiskBytes) {\n      final int numInMemSegments \u003d memDiskSegments.size();\n      diskSegments.addAll(0, memDiskSegments);\n      memDiskSegments.clear();\n      // Pass mergePhase only if there is a going to be intermediate\n      // merges. See comment where mergePhaseFinished is being set\n      Progress thisPhase \u003d (mergePhaseFinished) ? null : mergePhase; \n      RawKeyValueIterator diskMerge \u003d Merger.merge(\n          job, fs, keyClass, valueClass, diskSegments,\n          ioSortFactor, numInMemSegments, tmpDir, comparator,\n          reporter, false, spilledRecordsCounter, null, thisPhase);\n      diskSegments.clear();\n      if (0 \u003d\u003d finalSegments.size()) {\n        return diskMerge;\n      }\n      finalSegments.add(new Segment\u003cK,V\u003e(\n            new RawKVIteratorReader(diskMerge, onDiskBytes), true));\n    }\n    return Merger.merge(job, fs, keyClass, valueClass,\n                 finalSegments, finalSegments.size(), tmpDir,\n                 comparator, reporter, spilledRecordsCounter, null,\n                 null);\n  \n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/task/reduce/MergeManager.java"
    }
  }
}