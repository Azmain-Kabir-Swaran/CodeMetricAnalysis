{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataStorage.java",
  "functionName": "findDuplicateEntries",
  "functionId": "findDuplicateEntries___all-ArrayList__LinkArgs__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java",
  "functionStartLine": 1148,
  "functionEndLine": 1191,
  "numCommitsSeen": 75,
  "timeTaken": 1473,
  "changeHistory": [
    "8fa265a290792ff42635ff9b42416c634f88bdf3"
  ],
  "changeHistoryShort": {
    "8fa265a290792ff42635ff9b42416c634f88bdf3": "Yintroduced"
  },
  "changeHistoryDetails": {
    "8fa265a290792ff42635ff9b42416c634f88bdf3": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7443. Datanode upgrade to BLOCKID_BASED_LAYOUT fails if duplicate block files are present in the same volume (cmccabe)\n",
      "commitDate": "19/12/14 1:18 PM",
      "commitName": "8fa265a290792ff42635ff9b42416c634f88bdf3",
      "commitAuthor": "Colin Patrick Mccabe",
      "diff": "@@ -0,0 +1,44 @@\n+  static ArrayList\u003cLinkArgs\u003e findDuplicateEntries(ArrayList\u003cLinkArgs\u003e all) {\n+    // Find duplicates by sorting the list by the final path component.\n+    Collections.sort(all, new Comparator\u003cLinkArgs\u003e() {\n+      /**\n+       * Compare two LinkArgs objects, such that objects with the same\n+       * terminal source path components are grouped together.\n+       */\n+      @Override\n+      public int compare(LinkArgs a, LinkArgs b) {\n+        return ComparisonChain.start().\n+            compare(a.src.getName(), b.src.getName()).\n+            compare(a.src, b.src).\n+            compare(a.dst, b.dst).\n+            result();\n+      }\n+    });\n+    final ArrayList\u003cLinkArgs\u003e duplicates \u003d Lists.newArrayList();\n+    Long prevBlockId \u003d null;\n+    boolean prevWasMeta \u003d false;\n+    boolean addedPrev \u003d false;\n+    for (int i \u003d 0; i \u003c all.size(); i++) {\n+      LinkArgs args \u003d all.get(i);\n+      long blockId \u003d Block.getBlockId(args.src.getName());\n+      boolean isMeta \u003d Block.isMetaFilename(args.src.getName());\n+      if ((prevBlockId \u003d\u003d null) ||\n+          (prevBlockId.longValue() !\u003d blockId)) {\n+        prevBlockId \u003d blockId;\n+        addedPrev \u003d false;\n+      } else if (isMeta \u003d\u003d prevWasMeta) {\n+        // If we saw another file for the same block ID previously,\n+        // and it had the same meta-ness as this file, we have a\n+        // duplicate.\n+        duplicates.add(args);\n+        if (!addedPrev) {\n+          duplicates.add(all.get(i - 1));\n+        }\n+        addedPrev \u003d true;\n+      } else {\n+        addedPrev \u003d false;\n+      }\n+      prevWasMeta \u003d isMeta;\n+    }\n+    return duplicates;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  static ArrayList\u003cLinkArgs\u003e findDuplicateEntries(ArrayList\u003cLinkArgs\u003e all) {\n    // Find duplicates by sorting the list by the final path component.\n    Collections.sort(all, new Comparator\u003cLinkArgs\u003e() {\n      /**\n       * Compare two LinkArgs objects, such that objects with the same\n       * terminal source path components are grouped together.\n       */\n      @Override\n      public int compare(LinkArgs a, LinkArgs b) {\n        return ComparisonChain.start().\n            compare(a.src.getName(), b.src.getName()).\n            compare(a.src, b.src).\n            compare(a.dst, b.dst).\n            result();\n      }\n    });\n    final ArrayList\u003cLinkArgs\u003e duplicates \u003d Lists.newArrayList();\n    Long prevBlockId \u003d null;\n    boolean prevWasMeta \u003d false;\n    boolean addedPrev \u003d false;\n    for (int i \u003d 0; i \u003c all.size(); i++) {\n      LinkArgs args \u003d all.get(i);\n      long blockId \u003d Block.getBlockId(args.src.getName());\n      boolean isMeta \u003d Block.isMetaFilename(args.src.getName());\n      if ((prevBlockId \u003d\u003d null) ||\n          (prevBlockId.longValue() !\u003d blockId)) {\n        prevBlockId \u003d blockId;\n        addedPrev \u003d false;\n      } else if (isMeta \u003d\u003d prevWasMeta) {\n        // If we saw another file for the same block ID previously,\n        // and it had the same meta-ness as this file, we have a\n        // duplicate.\n        duplicates.add(args);\n        if (!addedPrev) {\n          duplicates.add(all.get(i - 1));\n        }\n        addedPrev \u003d true;\n      } else {\n        addedPrev \u003d false;\n      }\n      prevWasMeta \u003d isMeta;\n    }\n    return duplicates;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java"
    }
  }
}