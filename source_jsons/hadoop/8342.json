{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirErasureCodingOp.java",
  "functionName": "setErasureCodingPolicy",
  "functionId": "setErasureCodingPolicy___fsn-FSNamesystem(modifiers-final)__srcArg-String(modifiers-final)__ecPolicyName-String(modifiers-final)__pc-FSPermissionChecker(modifiers-final)__logRetryCache-boolean(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
  "functionStartLine": 130,
  "functionEndLine": 156,
  "numCommitsSeen": 69,
  "timeTaken": 8987,
  "changeHistory": [
    "92c58901d767f4fea571274544a590608c911cb8",
    "9b90e52f1ec22c18cd535af2a569defcef65b093",
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
    "33a38a534110de454662256545a7f4c075d328c8",
    "3085a604300ed76d06a0011bd5555e419897b6cd",
    "82ef9accafe7318278efb169678e17065e082c8e",
    "e69231658dc4a79da936e6856017b5c4f6124ecb",
    "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
    "869393643de23dcb010cc33091c8eb398de0fd6c",
    "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2",
    "f62237bc2f02afe11ce185e13aa51a60b5960037",
    "1d37a8812160bb030244a1e6b1c753f962d8d2ed",
    "98d340745be682fb251677bb4830aca76119868f"
  ],
  "changeHistoryShort": {
    "92c58901d767f4fea571274544a590608c911cb8": "Ybodychange",
    "9b90e52f1ec22c18cd535af2a569defcef65b093": "Yreturntypechange",
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1": "Ybodychange",
    "33a38a534110de454662256545a7f4c075d328c8": "Ybodychange",
    "3085a604300ed76d06a0011bd5555e419897b6cd": "Ymultichange(Yparameterchange,Yexceptionschange,Ybodychange)",
    "82ef9accafe7318278efb169678e17065e082c8e": "Ymultichange(Yparameterchange,Ybodychange)",
    "e69231658dc4a79da936e6856017b5c4f6124ecb": "Ybodychange",
    "9d175853b0170683ad5f21d9bcdeaac49fe89e04": "Ybodychange",
    "869393643de23dcb010cc33091c8eb398de0fd6c": "Ybodychange",
    "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2": "Ybodychange",
    "f62237bc2f02afe11ce185e13aa51a60b5960037": "Ymultichange(Yrename,Ybodychange)",
    "1d37a8812160bb030244a1e6b1c753f962d8d2ed": "Ymultichange(Yparameterchange,Ybodychange)",
    "98d340745be682fb251677bb4830aca76119868f": "Yintroduced"
  },
  "changeHistoryDetails": {
    "92c58901d767f4fea571274544a590608c911cb8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15117. EC: Add getECTopologyResultForPolicies to DistributedFileSystem. Contributed by Ayush Saxena\n",
      "commitDate": "23/01/20 4:48 AM",
      "commitName": "92c58901d767f4fea571274544a590608c911cb8",
      "commitAuthor": "Ayush Saxena",
      "commitDateOld": "09/09/18 8:40 PM",
      "commitNameOld": "30eceec3420fc6be00d3878ba787bd9518d3ca0e",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 500.38,
      "commitsBetweenForRepo": 3530,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,27 @@\n   static FileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n       final String srcArg, final String ecPolicyName,\n       final FSPermissionChecker pc, final boolean logRetryCache)\n       throws IOException, AccessControlException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n-      ErasureCodingPolicy ecPolicy \u003d getErasureCodingPolicyByName(fsn,\n+      ErasureCodingPolicy ecPolicy \u003d getEnabledErasureCodingPolicyByName(fsn,\n           ecPolicyName);\n       iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n       // Write access is required to set erasure coding policy\n       if (fsd.isPermissionEnabled()) {\n         fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n       }\n       src \u003d iip.getPath();\n       xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static FileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final String ecPolicyName,\n      final FSPermissionChecker pc, final boolean logRetryCache)\n      throws IOException, AccessControlException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      ErasureCodingPolicy ecPolicy \u003d getEnabledErasureCodingPolicyByName(fsn,\n          ecPolicyName);\n      iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n      // Write access is required to set erasure coding policy\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n      src \u003d iip.getPath();\n      xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
      "extendedDetails": {}
    },
    "9b90e52f1ec22c18cd535af2a569defcef65b093": {
      "type": "Yreturntypechange",
      "commitMessage": "HDFS-11641. Reduce cost of audit logging by using FileStatus instead of HdfsFileStatus. Contributed by Daryn Sharp.\n",
      "commitDate": "16/05/17 9:28 AM",
      "commitName": "9b90e52f1ec22c18cd535af2a569defcef65b093",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "27/04/17 10:18 PM",
      "commitNameOld": "cb672a45a0bbd8950b9b5e304c2e03f516945903",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 18.47,
      "commitsBetweenForRepo": 95,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,27 @@\n-  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n+  static FileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n       final String srcArg, final String ecPolicyName,\n       final FSPermissionChecker pc, final boolean logRetryCache)\n       throws IOException, AccessControlException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n       ErasureCodingPolicy ecPolicy \u003d getErasureCodingPolicyByName(fsn,\n           ecPolicyName);\n       iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n       // Write access is required to set erasure coding policy\n       if (fsd.isPermissionEnabled()) {\n         fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n       }\n       src \u003d iip.getPath();\n       xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static FileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final String ecPolicyName,\n      final FSPermissionChecker pc, final boolean logRetryCache)\n      throws IOException, AccessControlException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      ErasureCodingPolicy ecPolicy \u003d getErasureCodingPolicyByName(fsn,\n          ecPolicyName);\n      iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n      // Write access is required to set erasure coding policy\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n      src \u003d iip.getPath();\n      xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
      "extendedDetails": {
        "oldValue": "HdfsFileStatus",
        "newValue": "FileStatus"
      }
    },
    "a7312715a66dec5173c3a0a78dff4e0333e7f0b1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10996. Ability to specify per-file EC policy at create time. Contributed by SammiChen.\n",
      "commitDate": "12/04/17 12:27 PM",
      "commitName": "a7312715a66dec5173c3a0a78dff4e0333e7f0b1",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "07/04/17 4:46 PM",
      "commitNameOld": "e8bdad7385ab63a122c1e8e8a6a73e0f1100e80b",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.82,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,27 @@\n   static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n       final String srcArg, final String ecPolicyName,\n       final FSPermissionChecker pc, final boolean logRetryCache)\n       throws IOException, AccessControlException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n-      ErasureCodingPolicy ecPolicy \u003d fsn.getErasureCodingPolicyManager()\n-          .getEnabledPolicyByName(ecPolicyName);\n-      if (ecPolicy \u003d\u003d null) {\n-        final String sysPolicies \u003d\n-            Arrays.asList(\n-                fsn.getErasureCodingPolicyManager().getEnabledPolicies())\n-                .stream()\n-                .map(ErasureCodingPolicy::getName)\n-                .collect(Collectors.joining(\", \"));\n-        final String message \u003d String.format(\"Policy \u0027%s\u0027 does not match any \" +\n-            \"enabled erasure\" +\n-            \" coding policies: [%s]. The set of enabled erasure coding \" +\n-            \"policies can be configured at \u0027%s\u0027.\",\n-            ecPolicyName,\n-            sysPolicies,\n-            DFSConfigKeys.DFS_NAMENODE_EC_POLICIES_ENABLED_KEY\n-            );\n-        throw new HadoopIllegalArgumentException(message);\n-      }\n+      ErasureCodingPolicy ecPolicy \u003d getErasureCodingPolicyByName(fsn,\n+          ecPolicyName);\n       iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n       // Write access is required to set erasure coding policy\n       if (fsd.isPermissionEnabled()) {\n         fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n       }\n       src \u003d iip.getPath();\n       xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final String ecPolicyName,\n      final FSPermissionChecker pc, final boolean logRetryCache)\n      throws IOException, AccessControlException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      ErasureCodingPolicy ecPolicy \u003d getErasureCodingPolicyByName(fsn,\n          ecPolicyName);\n      iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n      // Write access is required to set erasure coding policy\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n      src \u003d iip.getPath();\n      xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
      "extendedDetails": {}
    },
    "33a38a534110de454662256545a7f4c075d328c8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11314. Enforce set of enabled EC policies on the NameNode.\n",
      "commitDate": "08/03/17 4:41 PM",
      "commitName": "33a38a534110de454662256545a7f4c075d328c8",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "03/03/17 1:00 PM",
      "commitNameOld": "3085a604300ed76d06a0011bd5555e419897b6cd",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 5.15,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,44 @@\n   static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n       final String srcArg, final String ecPolicyName,\n       final FSPermissionChecker pc, final boolean logRetryCache)\n       throws IOException, AccessControlException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n       ErasureCodingPolicy ecPolicy \u003d fsn.getErasureCodingPolicyManager()\n-          .getPolicyByName(ecPolicyName);\n+          .getEnabledPolicyByName(ecPolicyName);\n       if (ecPolicy \u003d\u003d null) {\n-        throw new HadoopIllegalArgumentException(\"Policy \u0027\" +\n-            ecPolicyName + \"\u0027 does not match any supported erasure coding \" +\n-            \"policies.\");\n+        final String sysPolicies \u003d\n+            Arrays.asList(\n+                fsn.getErasureCodingPolicyManager().getEnabledPolicies())\n+                .stream()\n+                .map(ErasureCodingPolicy::getName)\n+                .collect(Collectors.joining(\", \"));\n+        final String message \u003d String.format(\"Policy \u0027%s\u0027 does not match any \" +\n+            \"enabled erasure\" +\n+            \" coding policies: [%s]. The set of enabled erasure coding \" +\n+            \"policies can be configured at \u0027%s\u0027.\",\n+            ecPolicyName,\n+            sysPolicies,\n+            DFSConfigKeys.DFS_NAMENODE_EC_POLICIES_ENABLED_KEY\n+            );\n+        throw new HadoopIllegalArgumentException(message);\n       }\n       iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n       // Write access is required to set erasure coding policy\n       if (fsd.isPermissionEnabled()) {\n         fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n       }\n       src \u003d iip.getPath();\n       xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final String ecPolicyName,\n      final FSPermissionChecker pc, final boolean logRetryCache)\n      throws IOException, AccessControlException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      ErasureCodingPolicy ecPolicy \u003d fsn.getErasureCodingPolicyManager()\n          .getEnabledPolicyByName(ecPolicyName);\n      if (ecPolicy \u003d\u003d null) {\n        final String sysPolicies \u003d\n            Arrays.asList(\n                fsn.getErasureCodingPolicyManager().getEnabledPolicies())\n                .stream()\n                .map(ErasureCodingPolicy::getName)\n                .collect(Collectors.joining(\", \"));\n        final String message \u003d String.format(\"Policy \u0027%s\u0027 does not match any \" +\n            \"enabled erasure\" +\n            \" coding policies: [%s]. The set of enabled erasure coding \" +\n            \"policies can be configured at \u0027%s\u0027.\",\n            ecPolicyName,\n            sysPolicies,\n            DFSConfigKeys.DFS_NAMENODE_EC_POLICIES_ENABLED_KEY\n            );\n        throw new HadoopIllegalArgumentException(message);\n      }\n      iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n      // Write access is required to set erasure coding policy\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n      src \u003d iip.getPath();\n      xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
      "extendedDetails": {}
    },
    "3085a604300ed76d06a0011bd5555e419897b6cd": {
      "type": "Ymultichange(Yparameterchange,Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-8112. Relax permission checking for EC related operations.\n",
      "commitDate": "03/03/17 1:00 PM",
      "commitName": "3085a604300ed76d06a0011bd5555e419897b6cd",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8112. Relax permission checking for EC related operations.\n",
          "commitDate": "03/03/17 1:00 PM",
          "commitName": "3085a604300ed76d06a0011bd5555e419897b6cd",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "02/03/17 7:58 PM",
          "commitNameOld": "3749152b661d0359b3b941ab1d17177230f3b8dc",
          "commitAuthorOld": "Rakesh Radhakrishnan",
          "daysBetweenCommits": 0.71,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,32 @@\n   static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n       final String srcArg, final String ecPolicyName,\n-      final boolean logRetryCache) throws IOException {\n+      final FSPermissionChecker pc, final boolean logRetryCache)\n+      throws IOException, AccessControlException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n-    FSPermissionChecker pc \u003d null;\n-    pc \u003d fsn.getPermissionChecker();\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n       ErasureCodingPolicy ecPolicy \u003d fsn.getErasureCodingPolicyManager()\n           .getPolicyByName(ecPolicyName);\n       if (ecPolicy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\"Policy \u0027\" +\n             ecPolicyName + \"\u0027 does not match any supported erasure coding \" +\n             \"policies.\");\n       }\n       iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n+      // Write access is required to set erasure coding policy\n+      if (fsd.isPermissionEnabled()) {\n+        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n+      }\n       src \u003d iip.getPath();\n       xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final String ecPolicyName,\n      final FSPermissionChecker pc, final boolean logRetryCache)\n      throws IOException, AccessControlException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      ErasureCodingPolicy ecPolicy \u003d fsn.getErasureCodingPolicyManager()\n          .getPolicyByName(ecPolicyName);\n      if (ecPolicy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\"Policy \u0027\" +\n            ecPolicyName + \"\u0027 does not match any supported erasure coding \" +\n            \"policies.\");\n      }\n      iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n      // Write access is required to set erasure coding policy\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n      src \u003d iip.getPath();\n      xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
          "extendedDetails": {
            "oldValue": "[fsn-FSNamesystem(modifiers-final), srcArg-String(modifiers-final), ecPolicyName-String(modifiers-final), logRetryCache-boolean(modifiers-final)]",
            "newValue": "[fsn-FSNamesystem(modifiers-final), srcArg-String(modifiers-final), ecPolicyName-String(modifiers-final), pc-FSPermissionChecker(modifiers-final), logRetryCache-boolean(modifiers-final)]"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-8112. Relax permission checking for EC related operations.\n",
          "commitDate": "03/03/17 1:00 PM",
          "commitName": "3085a604300ed76d06a0011bd5555e419897b6cd",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "02/03/17 7:58 PM",
          "commitNameOld": "3749152b661d0359b3b941ab1d17177230f3b8dc",
          "commitAuthorOld": "Rakesh Radhakrishnan",
          "daysBetweenCommits": 0.71,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,32 @@\n   static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n       final String srcArg, final String ecPolicyName,\n-      final boolean logRetryCache) throws IOException {\n+      final FSPermissionChecker pc, final boolean logRetryCache)\n+      throws IOException, AccessControlException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n-    FSPermissionChecker pc \u003d null;\n-    pc \u003d fsn.getPermissionChecker();\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n       ErasureCodingPolicy ecPolicy \u003d fsn.getErasureCodingPolicyManager()\n           .getPolicyByName(ecPolicyName);\n       if (ecPolicy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\"Policy \u0027\" +\n             ecPolicyName + \"\u0027 does not match any supported erasure coding \" +\n             \"policies.\");\n       }\n       iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n+      // Write access is required to set erasure coding policy\n+      if (fsd.isPermissionEnabled()) {\n+        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n+      }\n       src \u003d iip.getPath();\n       xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final String ecPolicyName,\n      final FSPermissionChecker pc, final boolean logRetryCache)\n      throws IOException, AccessControlException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      ErasureCodingPolicy ecPolicy \u003d fsn.getErasureCodingPolicyManager()\n          .getPolicyByName(ecPolicyName);\n      if (ecPolicy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\"Policy \u0027\" +\n            ecPolicyName + \"\u0027 does not match any supported erasure coding \" +\n            \"policies.\");\n      }\n      iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n      // Write access is required to set erasure coding policy\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n      src \u003d iip.getPath();\n      xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
          "extendedDetails": {
            "oldValue": "[IOException]",
            "newValue": "[IOException, AccessControlException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8112. Relax permission checking for EC related operations.\n",
          "commitDate": "03/03/17 1:00 PM",
          "commitName": "3085a604300ed76d06a0011bd5555e419897b6cd",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "02/03/17 7:58 PM",
          "commitNameOld": "3749152b661d0359b3b941ab1d17177230f3b8dc",
          "commitAuthorOld": "Rakesh Radhakrishnan",
          "daysBetweenCommits": 0.71,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,32 @@\n   static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n       final String srcArg, final String ecPolicyName,\n-      final boolean logRetryCache) throws IOException {\n+      final FSPermissionChecker pc, final boolean logRetryCache)\n+      throws IOException, AccessControlException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n-    FSPermissionChecker pc \u003d null;\n-    pc \u003d fsn.getPermissionChecker();\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n       ErasureCodingPolicy ecPolicy \u003d fsn.getErasureCodingPolicyManager()\n           .getPolicyByName(ecPolicyName);\n       if (ecPolicy \u003d\u003d null) {\n         throw new HadoopIllegalArgumentException(\"Policy \u0027\" +\n             ecPolicyName + \"\u0027 does not match any supported erasure coding \" +\n             \"policies.\");\n       }\n       iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n+      // Write access is required to set erasure coding policy\n+      if (fsd.isPermissionEnabled()) {\n+        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n+      }\n       src \u003d iip.getPath();\n       xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final String ecPolicyName,\n      final FSPermissionChecker pc, final boolean logRetryCache)\n      throws IOException, AccessControlException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      ErasureCodingPolicy ecPolicy \u003d fsn.getErasureCodingPolicyManager()\n          .getPolicyByName(ecPolicyName);\n      if (ecPolicy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\"Policy \u0027\" +\n            ecPolicyName + \"\u0027 does not match any supported erasure coding \" +\n            \"policies.\");\n      }\n      iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n      // Write access is required to set erasure coding policy\n      if (fsd.isPermissionEnabled()) {\n        fsd.checkPathAccess(pc, iip, FsAction.WRITE);\n      }\n      src \u003d iip.getPath();\n      xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "82ef9accafe7318278efb169678e17065e082c8e": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-11428. Change setErasureCodingPolicy to take a required string EC policy name. Contributed by Andrew Wang.\n",
      "commitDate": "01/03/17 2:36 AM",
      "commitName": "82ef9accafe7318278efb169678e17065e082c8e",
      "commitAuthor": "Rakesh Radhakrishnan",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-11428. Change setErasureCodingPolicy to take a required string EC policy name. Contributed by Andrew Wang.\n",
          "commitDate": "01/03/17 2:36 AM",
          "commitName": "82ef9accafe7318278efb169678e17065e082c8e",
          "commitAuthor": "Rakesh Radhakrishnan",
          "commitDateOld": "10/01/17 11:32 AM",
          "commitNameOld": "e69231658dc4a79da936e6856017b5c4f6124ecb",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 49.63,
          "commitsBetweenForRepo": 252,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,29 @@\n   static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n-      final String srcArg, final ErasureCodingPolicy ecPolicy,\n+      final String srcArg, final String ecPolicyName,\n       final boolean logRetryCache) throws IOException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n     FSPermissionChecker pc \u003d null;\n     pc \u003d fsn.getPermissionChecker();\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n+      ErasureCodingPolicy ecPolicy \u003d fsn.getErasureCodingPolicyManager()\n+          .getPolicyByName(ecPolicyName);\n+      if (ecPolicy \u003d\u003d null) {\n+        throw new HadoopIllegalArgumentException(\"Policy \u0027\" +\n+            ecPolicyName + \"\u0027 does not match any supported erasure coding \" +\n+            \"policies.\");\n+      }\n       iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n       src \u003d iip.getPath();\n       xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final String ecPolicyName,\n      final boolean logRetryCache) throws IOException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSPermissionChecker pc \u003d null;\n    pc \u003d fsn.getPermissionChecker();\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      ErasureCodingPolicy ecPolicy \u003d fsn.getErasureCodingPolicyManager()\n          .getPolicyByName(ecPolicyName);\n      if (ecPolicy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\"Policy \u0027\" +\n            ecPolicyName + \"\u0027 does not match any supported erasure coding \" +\n            \"policies.\");\n      }\n      iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n      src \u003d iip.getPath();\n      xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
          "extendedDetails": {
            "oldValue": "[fsn-FSNamesystem(modifiers-final), srcArg-String(modifiers-final), ecPolicy-ErasureCodingPolicy(modifiers-final), logRetryCache-boolean(modifiers-final)]",
            "newValue": "[fsn-FSNamesystem(modifiers-final), srcArg-String(modifiers-final), ecPolicyName-String(modifiers-final), logRetryCache-boolean(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-11428. Change setErasureCodingPolicy to take a required string EC policy name. Contributed by Andrew Wang.\n",
          "commitDate": "01/03/17 2:36 AM",
          "commitName": "82ef9accafe7318278efb169678e17065e082c8e",
          "commitAuthor": "Rakesh Radhakrishnan",
          "commitDateOld": "10/01/17 11:32 AM",
          "commitNameOld": "e69231658dc4a79da936e6856017b5c4f6124ecb",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 49.63,
          "commitsBetweenForRepo": 252,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,29 @@\n   static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n-      final String srcArg, final ErasureCodingPolicy ecPolicy,\n+      final String srcArg, final String ecPolicyName,\n       final boolean logRetryCache) throws IOException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n     FSPermissionChecker pc \u003d null;\n     pc \u003d fsn.getPermissionChecker();\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n+      ErasureCodingPolicy ecPolicy \u003d fsn.getErasureCodingPolicyManager()\n+          .getPolicyByName(ecPolicyName);\n+      if (ecPolicy \u003d\u003d null) {\n+        throw new HadoopIllegalArgumentException(\"Policy \u0027\" +\n+            ecPolicyName + \"\u0027 does not match any supported erasure coding \" +\n+            \"policies.\");\n+      }\n       iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n       src \u003d iip.getPath();\n       xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final String ecPolicyName,\n      final boolean logRetryCache) throws IOException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSPermissionChecker pc \u003d null;\n    pc \u003d fsn.getPermissionChecker();\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      ErasureCodingPolicy ecPolicy \u003d fsn.getErasureCodingPolicyManager()\n          .getPolicyByName(ecPolicyName);\n      if (ecPolicy \u003d\u003d null) {\n        throw new HadoopIllegalArgumentException(\"Policy \u0027\" +\n            ecPolicyName + \"\u0027 does not match any supported erasure coding \" +\n            \"policies.\");\n      }\n      iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n      src \u003d iip.getPath();\n      xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "e69231658dc4a79da936e6856017b5c4f6124ecb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11072. Add ability to unset and change directory EC policy. Contributed by Sammi Chen.\n",
      "commitDate": "10/01/17 11:32 AM",
      "commitName": "e69231658dc4a79da936e6856017b5c4f6124ecb",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "24/10/16 3:14 PM",
      "commitNameOld": "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 77.89,
      "commitsBetweenForRepo": 530,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n       final String srcArg, final ErasureCodingPolicy ecPolicy,\n       final boolean logRetryCache) throws IOException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n     FSPermissionChecker pc \u003d null;\n     pc \u003d fsn.getPermissionChecker();\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n       iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n       src \u003d iip.getPath();\n-      xAttrs \u003d createErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n+      xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final ErasureCodingPolicy ecPolicy,\n      final boolean logRetryCache) throws IOException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSPermissionChecker pc \u003d null;\n    pc \u003d fsn.getPermissionChecker();\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n      src \u003d iip.getPath();\n      xAttrs \u003d setErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
      "extendedDetails": {}
    },
    "9d175853b0170683ad5f21d9bcdeaac49fe89e04": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10997. Reduce number of path resolving methods. Contributed by Daryn Sharp.\n",
      "commitDate": "24/10/16 3:14 PM",
      "commitName": "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "06/10/16 1:11 PM",
      "commitNameOld": "f32e9fc8f7150f0e889c0774b3ad712af26fbd65",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 18.09,
      "commitsBetweenForRepo": 121,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n       final String srcArg, final ErasureCodingPolicy ecPolicy,\n       final boolean logRetryCache) throws IOException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n     FSPermissionChecker pc \u003d null;\n     pc \u003d fsn.getPermissionChecker();\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n-      iip \u003d fsd.resolvePathForWrite(pc, src, false);\n+      iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n       src \u003d iip.getPath();\n       xAttrs \u003d createErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final ErasureCodingPolicy ecPolicy,\n      final boolean logRetryCache) throws IOException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSPermissionChecker pc \u003d null;\n    pc \u003d fsn.getPermissionChecker();\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      iip \u003d fsd.resolvePath(pc, src, DirOp.WRITE_LINK);\n      src \u003d iip.getPath();\n      xAttrs \u003d createErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
      "extendedDetails": {}
    },
    "869393643de23dcb010cc33091c8eb398de0fd6c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10745. Directly resolve paths into INodesInPath. Contributed by Daryn Sharp.\n",
      "commitDate": "17/08/16 1:53 PM",
      "commitName": "869393643de23dcb010cc33091c8eb398de0fd6c",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "15/08/16 2:45 PM",
      "commitNameOld": "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 1.96,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n       final String srcArg, final ErasureCodingPolicy ecPolicy,\n       final boolean logRetryCache) throws IOException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n     FSPermissionChecker pc \u003d null;\n     pc \u003d fsn.getPermissionChecker();\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n-    src \u003d fsd.resolvePath(pc, src);\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n-      iip \u003d fsd.getINodesInPath4Write(src, false);\n+      iip \u003d fsd.resolvePathForWrite(pc, src, false);\n+      src \u003d iip.getPath();\n       xAttrs \u003d createErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final ErasureCodingPolicy ecPolicy,\n      final boolean logRetryCache) throws IOException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSPermissionChecker pc \u003d null;\n    pc \u003d fsn.getPermissionChecker();\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      iip \u003d fsd.resolvePathForWrite(pc, src, false);\n      src \u003d iip.getPath();\n      xAttrs \u003d createErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
      "extendedDetails": {}
    },
    "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10744. Internally optimize path component resolution. Contributed by Daryn Sharp.\n",
      "commitDate": "15/08/16 2:45 PM",
      "commitName": "03dea65e0b17ca2f9460bb6110f6ab3a321b8bf2",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "08/03/16 10:30 PM",
      "commitNameOld": "7600e3c48ff2043654dbe9f415a186a336b5ea6c",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 159.64,
      "commitsBetweenForRepo": 1145,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,22 @@\n   static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n       final String srcArg, final ErasureCodingPolicy ecPolicy,\n       final boolean logRetryCache) throws IOException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n     FSPermissionChecker pc \u003d null;\n-    byte[][] pathComponents \u003d null;\n-    pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     pc \u003d fsn.getPermissionChecker();\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n-    src \u003d fsd.resolvePath(pc, src, pathComponents);\n+    src \u003d fsd.resolvePath(pc, src);\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n       iip \u003d fsd.getINodesInPath4Write(src, false);\n       xAttrs \u003d createErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final ErasureCodingPolicy ecPolicy,\n      final boolean logRetryCache) throws IOException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSPermissionChecker pc \u003d null;\n    pc \u003d fsn.getPermissionChecker();\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    src \u003d fsd.resolvePath(pc, src);\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      iip \u003d fsd.getINodesInPath4Write(src, false);\n      xAttrs \u003d createErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
      "extendedDetails": {}
    },
    "f62237bc2f02afe11ce185e13aa51a60b5960037": {
      "type": "Ymultichange(Yrename,Ybodychange)",
      "commitMessage": "HDFS-8833. Erasure coding: store EC schema and cell size in INodeFile and eliminate notion of EC zones.\n",
      "commitDate": "09/09/15 11:07 PM",
      "commitName": "f62237bc2f02afe11ce185e13aa51a60b5960037",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-8833. Erasure coding: store EC schema and cell size in INodeFile and eliminate notion of EC zones.\n",
          "commitDate": "09/09/15 11:07 PM",
          "commitName": "f62237bc2f02afe11ce185e13aa51a60b5960037",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "13/08/15 10:04 AM",
          "commitNameOld": "1d37a8812160bb030244a1e6b1c753f962d8d2ed",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 27.54,
          "commitsBetweenForRepo": 115,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,24 @@\n-  static HdfsFileStatus createErasureCodingZone(final FSNamesystem fsn,\n+  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n       final String srcArg, final ErasureCodingPolicy ecPolicy,\n       final boolean logRetryCache) throws IOException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n     FSPermissionChecker pc \u003d null;\n     byte[][] pathComponents \u003d null;\n     pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     pc \u003d fsn.getPermissionChecker();\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     src \u003d fsd.resolvePath(pc, src, pathComponents);\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n       iip \u003d fsd.getINodesInPath4Write(src, false);\n-      xAttrs \u003d fsn.getErasureCodingZoneManager().createErasureCodingZone(\n-          iip, ecPolicy);\n+      xAttrs \u003d createErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final ErasureCodingPolicy ecPolicy,\n      final boolean logRetryCache) throws IOException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSPermissionChecker pc \u003d null;\n    byte[][] pathComponents \u003d null;\n    pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    pc \u003d fsn.getPermissionChecker();\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    src \u003d fsd.resolvePath(pc, src, pathComponents);\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      iip \u003d fsd.getINodesInPath4Write(src, false);\n      xAttrs \u003d createErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
          "extendedDetails": {
            "oldValue": "createErasureCodingZone",
            "newValue": "setErasureCodingPolicy"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8833. Erasure coding: store EC schema and cell size in INodeFile and eliminate notion of EC zones.\n",
          "commitDate": "09/09/15 11:07 PM",
          "commitName": "f62237bc2f02afe11ce185e13aa51a60b5960037",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "13/08/15 10:04 AM",
          "commitNameOld": "1d37a8812160bb030244a1e6b1c753f962d8d2ed",
          "commitAuthorOld": "Zhe Zhang",
          "daysBetweenCommits": 27.54,
          "commitsBetweenForRepo": 115,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,24 @@\n-  static HdfsFileStatus createErasureCodingZone(final FSNamesystem fsn,\n+  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n       final String srcArg, final ErasureCodingPolicy ecPolicy,\n       final boolean logRetryCache) throws IOException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n     FSPermissionChecker pc \u003d null;\n     byte[][] pathComponents \u003d null;\n     pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     pc \u003d fsn.getPermissionChecker();\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     src \u003d fsd.resolvePath(pc, src, pathComponents);\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n       iip \u003d fsd.getINodesInPath4Write(src, false);\n-      xAttrs \u003d fsn.getErasureCodingZoneManager().createErasureCodingZone(\n-          iip, ecPolicy);\n+      xAttrs \u003d createErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus setErasureCodingPolicy(final FSNamesystem fsn,\n      final String srcArg, final ErasureCodingPolicy ecPolicy,\n      final boolean logRetryCache) throws IOException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSPermissionChecker pc \u003d null;\n    byte[][] pathComponents \u003d null;\n    pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    pc \u003d fsn.getPermissionChecker();\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    src \u003d fsd.resolvePath(pc, src, pathComponents);\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      iip \u003d fsd.getINodesInPath4Write(src, false);\n      xAttrs \u003d createErasureCodingPolicyXAttr(fsn, iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "1d37a8812160bb030244a1e6b1c753f962d8d2ed": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8854. Erasure coding: add ECPolicy to replace schema+cellSize in hadoop-hdfs. Contributed by Walter Su.\n",
      "commitDate": "13/08/15 10:04 AM",
      "commitName": "1d37a8812160bb030244a1e6b1c753f962d8d2ed",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8854. Erasure coding: add ECPolicy to replace schema+cellSize in hadoop-hdfs. Contributed by Walter Su.\n",
          "commitDate": "13/08/15 10:04 AM",
          "commitName": "1d37a8812160bb030244a1e6b1c753f962d8d2ed",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "10/06/15 10:18 PM",
          "commitNameOld": "98d340745be682fb251677bb4830aca76119868f",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 63.49,
          "commitsBetweenForRepo": 37,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,25 @@\n   static HdfsFileStatus createErasureCodingZone(final FSNamesystem fsn,\n-      final String srcArg, final ECSchema schema, final int cellSize,\n+      final String srcArg, final ErasureCodingPolicy ecPolicy,\n       final boolean logRetryCache) throws IOException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n     FSPermissionChecker pc \u003d null;\n     byte[][] pathComponents \u003d null;\n     pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     pc \u003d fsn.getPermissionChecker();\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     src \u003d fsd.resolvePath(pc, src, pathComponents);\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n       iip \u003d fsd.getINodesInPath4Write(src, false);\n       xAttrs \u003d fsn.getErasureCodingZoneManager().createErasureCodingZone(\n-          iip, schema, cellSize);\n+          iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus createErasureCodingZone(final FSNamesystem fsn,\n      final String srcArg, final ErasureCodingPolicy ecPolicy,\n      final boolean logRetryCache) throws IOException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSPermissionChecker pc \u003d null;\n    byte[][] pathComponents \u003d null;\n    pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    pc \u003d fsn.getPermissionChecker();\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    src \u003d fsd.resolvePath(pc, src, pathComponents);\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      iip \u003d fsd.getINodesInPath4Write(src, false);\n      xAttrs \u003d fsn.getErasureCodingZoneManager().createErasureCodingZone(\n          iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
          "extendedDetails": {
            "oldValue": "[fsn-FSNamesystem(modifiers-final), srcArg-String(modifiers-final), schema-ECSchema(modifiers-final), cellSize-int(modifiers-final), logRetryCache-boolean(modifiers-final)]",
            "newValue": "[fsn-FSNamesystem(modifiers-final), srcArg-String(modifiers-final), ecPolicy-ErasureCodingPolicy(modifiers-final), logRetryCache-boolean(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8854. Erasure coding: add ECPolicy to replace schema+cellSize in hadoop-hdfs. Contributed by Walter Su.\n",
          "commitDate": "13/08/15 10:04 AM",
          "commitName": "1d37a8812160bb030244a1e6b1c753f962d8d2ed",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "10/06/15 10:18 PM",
          "commitNameOld": "98d340745be682fb251677bb4830aca76119868f",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 63.49,
          "commitsBetweenForRepo": 37,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,25 @@\n   static HdfsFileStatus createErasureCodingZone(final FSNamesystem fsn,\n-      final String srcArg, final ECSchema schema, final int cellSize,\n+      final String srcArg, final ErasureCodingPolicy ecPolicy,\n       final boolean logRetryCache) throws IOException {\n     assert fsn.hasWriteLock();\n \n     String src \u003d srcArg;\n     FSPermissionChecker pc \u003d null;\n     byte[][] pathComponents \u003d null;\n     pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n     pc \u003d fsn.getPermissionChecker();\n     FSDirectory fsd \u003d fsn.getFSDirectory();\n     src \u003d fsd.resolvePath(pc, src, pathComponents);\n     final INodesInPath iip;\n     List\u003cXAttr\u003e xAttrs;\n     fsd.writeLock();\n     try {\n       iip \u003d fsd.getINodesInPath4Write(src, false);\n       xAttrs \u003d fsn.getErasureCodingZoneManager().createErasureCodingZone(\n-          iip, schema, cellSize);\n+          iip, ecPolicy);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n     return fsd.getAuditFileInfo(iip);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus createErasureCodingZone(final FSNamesystem fsn,\n      final String srcArg, final ErasureCodingPolicy ecPolicy,\n      final boolean logRetryCache) throws IOException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSPermissionChecker pc \u003d null;\n    byte[][] pathComponents \u003d null;\n    pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    pc \u003d fsn.getPermissionChecker();\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    src \u003d fsd.resolvePath(pc, src, pathComponents);\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      iip \u003d fsd.getINodesInPath4Write(src, false);\n      xAttrs \u003d fsn.getErasureCodingZoneManager().createErasureCodingZone(\n          iip, ecPolicy);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "98d340745be682fb251677bb4830aca76119868f": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-8450. Erasure Coding: Consolidate erasure coding zone related implementation into a single class (Contributed by Rakesh R)\n",
      "commitDate": "10/06/15 10:18 PM",
      "commitName": "98d340745be682fb251677bb4830aca76119868f",
      "commitAuthor": "Vinayakumar B",
      "diff": "@@ -0,0 +1,25 @@\n+  static HdfsFileStatus createErasureCodingZone(final FSNamesystem fsn,\n+      final String srcArg, final ECSchema schema, final int cellSize,\n+      final boolean logRetryCache) throws IOException {\n+    assert fsn.hasWriteLock();\n+\n+    String src \u003d srcArg;\n+    FSPermissionChecker pc \u003d null;\n+    byte[][] pathComponents \u003d null;\n+    pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n+    pc \u003d fsn.getPermissionChecker();\n+    FSDirectory fsd \u003d fsn.getFSDirectory();\n+    src \u003d fsd.resolvePath(pc, src, pathComponents);\n+    final INodesInPath iip;\n+    List\u003cXAttr\u003e xAttrs;\n+    fsd.writeLock();\n+    try {\n+      iip \u003d fsd.getINodesInPath4Write(src, false);\n+      xAttrs \u003d fsn.getErasureCodingZoneManager().createErasureCodingZone(\n+          iip, schema, cellSize);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n+    return fsd.getAuditFileInfo(iip);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus createErasureCodingZone(final FSNamesystem fsn,\n      final String srcArg, final ECSchema schema, final int cellSize,\n      final boolean logRetryCache) throws IOException {\n    assert fsn.hasWriteLock();\n\n    String src \u003d srcArg;\n    FSPermissionChecker pc \u003d null;\n    byte[][] pathComponents \u003d null;\n    pathComponents \u003d FSDirectory.getPathComponentsForReservedPath(src);\n    pc \u003d fsn.getPermissionChecker();\n    FSDirectory fsd \u003d fsn.getFSDirectory();\n    src \u003d fsd.resolvePath(pc, src, pathComponents);\n    final INodesInPath iip;\n    List\u003cXAttr\u003e xAttrs;\n    fsd.writeLock();\n    try {\n      iip \u003d fsd.getINodesInPath4Write(src, false);\n      xAttrs \u003d fsn.getErasureCodingZoneManager().createErasureCodingZone(\n          iip, schema, cellSize);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsn.getEditLog().logSetXAttrs(src, xAttrs, logRetryCache);\n    return fsd.getAuditFileInfo(iip);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirErasureCodingOp.java"
    }
  }
}