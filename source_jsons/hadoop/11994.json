{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetImpl.java",
  "functionName": "createTemporary",
  "functionId": "createTemporary___storageType-StorageType__storageId-String__b-ExtendedBlock__isTransfer-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
  "functionStartLine": 1672,
  "functionEndLine": 1757,
  "numCommitsSeen": 582,
  "timeTaken": 16846,
  "changeHistory": [
    "d3b595157256e198c4340d555e14ad6144f2eaa1",
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8",
    "90d1b47a2a400e07e2b6b812c4bbd9c4f2877786",
    "29b7df960fc3d0a7d1416225c3106c7d4222f0ca",
    "a3954ccab148bddc290cb96528e63ff19799bcc9",
    "86c9862bec0248d671e657aa56094a2919b8ac14",
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c",
    "92c1af1646b1d91a2ab7821e4f7d450e3b6e10bb",
    "28bebc81db8bb6d1bc2574de7564fe4c595cfe09",
    "b7f4a3156c0f5c600816c469637237ba6c9b330c",
    "f02d934fedf00f0ce43d6f3f9b06d89ccc6851a5",
    "d1fa58292e87bc29b4ef1278368c2be938a0afc4",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54",
    "fba994ffe20d387e8ed875e727fc3d93f7097101",
    "f39f8c57344ede533ca4363c98230f3a0c401a76",
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8",
    "bc13dfb1426944ce45293cb8f444239a7406762c",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "d3b595157256e198c4340d555e14ad6144f2eaa1": "Ybodychange",
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8": "Ybodychange",
    "90d1b47a2a400e07e2b6b812c4bbd9c4f2877786": "Ybodychange",
    "29b7df960fc3d0a7d1416225c3106c7d4222f0ca": "Ymultichange(Yparameterchange,Ybodychange)",
    "a3954ccab148bddc290cb96528e63ff19799bcc9": "Ymultichange(Yparameterchange,Ybodychange)",
    "86c9862bec0248d671e657aa56094a2919b8ac14": "Ybodychange",
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c": "Ybodychange",
    "92c1af1646b1d91a2ab7821e4f7d450e3b6e10bb": "Ybodychange",
    "28bebc81db8bb6d1bc2574de7564fe4c595cfe09": "Ymultichange(Ymodifierchange,Ybodychange)",
    "b7f4a3156c0f5c600816c469637237ba6c9b330c": "Ymultichange(Yreturntypechange,Ybodychange)",
    "f02d934fedf00f0ce43d6f3f9b06d89ccc6851a5": "Ybodychange",
    "d1fa58292e87bc29b4ef1278368c2be938a0afc4": "Ybodychange",
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": "Ymultichange(Yparameterchange,Ybodychange)",
    "fba994ffe20d387e8ed875e727fc3d93f7097101": "Ybodychange",
    "f39f8c57344ede533ca4363c98230f3a0c401a76": "Ybodychange",
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8": "Ybodychange",
    "bc13dfb1426944ce45293cb8f444239a7406762c": "Ymultichange(Ymovefromfile,Yreturntypechange,Ybodychange)",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "d3b595157256e198c4340d555e14ad6144f2eaa1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15242. Add metrics for operations hold lock times of FsDatasetImpl. Contributed by Xiaoqiao He.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\nReviewed-by: Inigo Goiri \u003cinigoiri@apache.org\u003e\n",
      "commitDate": "01/04/20 4:36 PM",
      "commitName": "d3b595157256e198c4340d555e14ad6144f2eaa1",
      "commitAuthor": "He Xiaoqiao",
      "commitDateOld": "11/02/20 8:00 AM",
      "commitNameOld": "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8",
      "commitAuthorOld": "Stephen O\u0027Donnell",
      "daysBetweenCommits": 50.32,
      "commitsBetweenForRepo": 174,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,86 @@\n   public ReplicaHandler createTemporary(StorageType storageType,\n       String storageId, ExtendedBlock b, boolean isTransfer)\n       throws IOException {\n     long startTimeMs \u003d Time.monotonicNow();\n     long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n     ReplicaInfo lastFoundReplicaInfo \u003d null;\n     boolean isInPipeline \u003d false;\n     do {\n       try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n         ReplicaInfo currentReplicaInfo \u003d\n             volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n         if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n           break;\n         } else {\n           isInPipeline \u003d currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n               || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW;\n           /*\n            * If the current block is not PROVIDED and old, reject.\n            * else If transfer request, then accept it.\n            * else if state is not RBW/Temporary, then reject\n            * If current block is PROVIDED, ignore the replica.\n            */\n           if (((currentReplicaInfo.getGenerationStamp() \u003e\u003d b\n               .getGenerationStamp()) || (!isTransfer \u0026\u0026 !isInPipeline))\n               \u0026\u0026 !isReplicaProvided(currentReplicaInfo)) {\n             throw new ReplicaAlreadyExistsException(\"Block \" + b\n                 + \" already exists in state \" + currentReplicaInfo.getState()\n                 + \" and thus cannot be created.\");\n           }\n           lastFoundReplicaInfo \u003d currentReplicaInfo;\n         }\n       }\n       if (!isInPipeline) {\n         continue;\n       }\n       // Hang too long, just bail out. This is not supposed to happen.\n       long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n       if (writerStopMs \u003e writerStopTimeoutMs) {\n         LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n             + writerStopMs + \" miniseconds.\");\n         throw new IOException(\"Unable to stop existing writer for block \" + b\n             + \" after \" + writerStopMs + \" miniseconds.\");\n       }\n \n       // if lastFoundReplicaInfo is PROVIDED and FINALIZED,\n       // stopWriter isn\u0027t required.\n       if (isReplicaProvided(lastFoundReplicaInfo) \u0026\u0026\n           lastFoundReplicaInfo.getState() \u003d\u003d ReplicaState.FINALIZED) {\n         continue;\n       }\n       // Stop the previous writer\n       ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n     } while (true);\n+    long holdLockTimeMs \u003d Time.monotonicNow() - startTimeMs;\n     if (lastFoundReplicaInfo !\u003d null\n         \u0026\u0026 !isReplicaProvided(lastFoundReplicaInfo)) {\n       // Old blockfile should be deleted synchronously as it might collide\n       // with the new block if allocated in same volume.\n       // Do the deletion outside of lock as its DISK IO.\n       invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo },\n           false);\n     }\n+    long startHoldLockTimeMs \u003d Time.monotonicNow();\n     try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n       FsVolumeReference ref \u003d volumes.getNextVolume(storageType, storageId, b\n           .getNumBytes());\n       FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n       ReplicaInPipeline newReplicaInfo;\n       try {\n         newReplicaInfo \u003d v.createTemporary(b);\n       } catch (IOException e) {\n         IOUtils.cleanup(null, ref);\n         throw e;\n       }\n \n       volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n       return new ReplicaHandler(newReplicaInfo, ref);\n+    } finally {\n+      if (dataNodeMetrics !\u003d null) {\n+        // Create temporary operation hold write lock twice.\n+        long createTemporaryOpMs \u003d Time.monotonicNow() - startHoldLockTimeMs\n+            + holdLockTimeMs;\n+        dataNodeMetrics.addCreateTemporaryOp(createTemporaryOpMs);\n+      }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public ReplicaHandler createTemporary(StorageType storageType,\n      String storageId, ExtendedBlock b, boolean isTransfer)\n      throws IOException {\n    long startTimeMs \u003d Time.monotonicNow();\n    long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n    ReplicaInfo lastFoundReplicaInfo \u003d null;\n    boolean isInPipeline \u003d false;\n    do {\n      try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n        ReplicaInfo currentReplicaInfo \u003d\n            volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n        if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n          break;\n        } else {\n          isInPipeline \u003d currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n              || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW;\n          /*\n           * If the current block is not PROVIDED and old, reject.\n           * else If transfer request, then accept it.\n           * else if state is not RBW/Temporary, then reject\n           * If current block is PROVIDED, ignore the replica.\n           */\n          if (((currentReplicaInfo.getGenerationStamp() \u003e\u003d b\n              .getGenerationStamp()) || (!isTransfer \u0026\u0026 !isInPipeline))\n              \u0026\u0026 !isReplicaProvided(currentReplicaInfo)) {\n            throw new ReplicaAlreadyExistsException(\"Block \" + b\n                + \" already exists in state \" + currentReplicaInfo.getState()\n                + \" and thus cannot be created.\");\n          }\n          lastFoundReplicaInfo \u003d currentReplicaInfo;\n        }\n      }\n      if (!isInPipeline) {\n        continue;\n      }\n      // Hang too long, just bail out. This is not supposed to happen.\n      long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n      if (writerStopMs \u003e writerStopTimeoutMs) {\n        LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n            + writerStopMs + \" miniseconds.\");\n        throw new IOException(\"Unable to stop existing writer for block \" + b\n            + \" after \" + writerStopMs + \" miniseconds.\");\n      }\n\n      // if lastFoundReplicaInfo is PROVIDED and FINALIZED,\n      // stopWriter isn\u0027t required.\n      if (isReplicaProvided(lastFoundReplicaInfo) \u0026\u0026\n          lastFoundReplicaInfo.getState() \u003d\u003d ReplicaState.FINALIZED) {\n        continue;\n      }\n      // Stop the previous writer\n      ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n    } while (true);\n    long holdLockTimeMs \u003d Time.monotonicNow() - startTimeMs;\n    if (lastFoundReplicaInfo !\u003d null\n        \u0026\u0026 !isReplicaProvided(lastFoundReplicaInfo)) {\n      // Old blockfile should be deleted synchronously as it might collide\n      // with the new block if allocated in same volume.\n      // Do the deletion outside of lock as its DISK IO.\n      invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo },\n          false);\n    }\n    long startHoldLockTimeMs \u003d Time.monotonicNow();\n    try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n      FsVolumeReference ref \u003d volumes.getNextVolume(storageType, storageId, b\n          .getNumBytes());\n      FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n      ReplicaInPipeline newReplicaInfo;\n      try {\n        newReplicaInfo \u003d v.createTemporary(b);\n      } catch (IOException e) {\n        IOUtils.cleanup(null, ref);\n        throw e;\n      }\n\n      volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n      return new ReplicaHandler(newReplicaInfo, ref);\n    } finally {\n      if (dataNodeMetrics !\u003d null) {\n        // Create temporary operation hold write lock twice.\n        long createTemporaryOpMs \u003d Time.monotonicNow() - startHoldLockTimeMs\n            + holdLockTimeMs;\n        dataNodeMetrics.addCreateTemporaryOp(createTemporaryOpMs);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15150. Introduce read write lock to Datanode. Contributed Stephen O\u0027Donnell.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "11/02/20 8:00 AM",
      "commitName": "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8",
      "commitAuthor": "Stephen O\u0027Donnell",
      "commitDateOld": "28/01/20 10:10 AM",
      "commitNameOld": "1839c467f60cbb8592d446694ec3d7710cda5142",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 13.91,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,77 @@\n   public ReplicaHandler createTemporary(StorageType storageType,\n       String storageId, ExtendedBlock b, boolean isTransfer)\n       throws IOException {\n     long startTimeMs \u003d Time.monotonicNow();\n     long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n     ReplicaInfo lastFoundReplicaInfo \u003d null;\n     boolean isInPipeline \u003d false;\n     do {\n-      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n+      try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n         ReplicaInfo currentReplicaInfo \u003d\n             volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n         if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n           break;\n         } else {\n           isInPipeline \u003d currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n               || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW;\n           /*\n            * If the current block is not PROVIDED and old, reject.\n            * else If transfer request, then accept it.\n            * else if state is not RBW/Temporary, then reject\n            * If current block is PROVIDED, ignore the replica.\n            */\n           if (((currentReplicaInfo.getGenerationStamp() \u003e\u003d b\n               .getGenerationStamp()) || (!isTransfer \u0026\u0026 !isInPipeline))\n               \u0026\u0026 !isReplicaProvided(currentReplicaInfo)) {\n             throw new ReplicaAlreadyExistsException(\"Block \" + b\n                 + \" already exists in state \" + currentReplicaInfo.getState()\n                 + \" and thus cannot be created.\");\n           }\n           lastFoundReplicaInfo \u003d currentReplicaInfo;\n         }\n       }\n       if (!isInPipeline) {\n         continue;\n       }\n       // Hang too long, just bail out. This is not supposed to happen.\n       long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n       if (writerStopMs \u003e writerStopTimeoutMs) {\n         LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n             + writerStopMs + \" miniseconds.\");\n         throw new IOException(\"Unable to stop existing writer for block \" + b\n             + \" after \" + writerStopMs + \" miniseconds.\");\n       }\n \n       // if lastFoundReplicaInfo is PROVIDED and FINALIZED,\n       // stopWriter isn\u0027t required.\n       if (isReplicaProvided(lastFoundReplicaInfo) \u0026\u0026\n           lastFoundReplicaInfo.getState() \u003d\u003d ReplicaState.FINALIZED) {\n         continue;\n       }\n       // Stop the previous writer\n       ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n     } while (true);\n     if (lastFoundReplicaInfo !\u003d null\n         \u0026\u0026 !isReplicaProvided(lastFoundReplicaInfo)) {\n       // Old blockfile should be deleted synchronously as it might collide\n       // with the new block if allocated in same volume.\n       // Do the deletion outside of lock as its DISK IO.\n       invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo },\n           false);\n     }\n-    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n+    try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n       FsVolumeReference ref \u003d volumes.getNextVolume(storageType, storageId, b\n           .getNumBytes());\n       FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n       ReplicaInPipeline newReplicaInfo;\n       try {\n         newReplicaInfo \u003d v.createTemporary(b);\n       } catch (IOException e) {\n         IOUtils.cleanup(null, ref);\n         throw e;\n       }\n \n       volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n       return new ReplicaHandler(newReplicaInfo, ref);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public ReplicaHandler createTemporary(StorageType storageType,\n      String storageId, ExtendedBlock b, boolean isTransfer)\n      throws IOException {\n    long startTimeMs \u003d Time.monotonicNow();\n    long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n    ReplicaInfo lastFoundReplicaInfo \u003d null;\n    boolean isInPipeline \u003d false;\n    do {\n      try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n        ReplicaInfo currentReplicaInfo \u003d\n            volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n        if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n          break;\n        } else {\n          isInPipeline \u003d currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n              || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW;\n          /*\n           * If the current block is not PROVIDED and old, reject.\n           * else If transfer request, then accept it.\n           * else if state is not RBW/Temporary, then reject\n           * If current block is PROVIDED, ignore the replica.\n           */\n          if (((currentReplicaInfo.getGenerationStamp() \u003e\u003d b\n              .getGenerationStamp()) || (!isTransfer \u0026\u0026 !isInPipeline))\n              \u0026\u0026 !isReplicaProvided(currentReplicaInfo)) {\n            throw new ReplicaAlreadyExistsException(\"Block \" + b\n                + \" already exists in state \" + currentReplicaInfo.getState()\n                + \" and thus cannot be created.\");\n          }\n          lastFoundReplicaInfo \u003d currentReplicaInfo;\n        }\n      }\n      if (!isInPipeline) {\n        continue;\n      }\n      // Hang too long, just bail out. This is not supposed to happen.\n      long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n      if (writerStopMs \u003e writerStopTimeoutMs) {\n        LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n            + writerStopMs + \" miniseconds.\");\n        throw new IOException(\"Unable to stop existing writer for block \" + b\n            + \" after \" + writerStopMs + \" miniseconds.\");\n      }\n\n      // if lastFoundReplicaInfo is PROVIDED and FINALIZED,\n      // stopWriter isn\u0027t required.\n      if (isReplicaProvided(lastFoundReplicaInfo) \u0026\u0026\n          lastFoundReplicaInfo.getState() \u003d\u003d ReplicaState.FINALIZED) {\n        continue;\n      }\n      // Stop the previous writer\n      ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n    } while (true);\n    if (lastFoundReplicaInfo !\u003d null\n        \u0026\u0026 !isReplicaProvided(lastFoundReplicaInfo)) {\n      // Old blockfile should be deleted synchronously as it might collide\n      // with the new block if allocated in same volume.\n      // Do the deletion outside of lock as its DISK IO.\n      invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo },\n          false);\n    }\n    try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n      FsVolumeReference ref \u003d volumes.getNextVolume(storageType, storageId, b\n          .getNumBytes());\n      FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n      ReplicaInPipeline newReplicaInfo;\n      try {\n        newReplicaInfo \u003d v.createTemporary(b);\n      } catch (IOException e) {\n        IOUtils.cleanup(null, ref);\n        throw e;\n      }\n\n      volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n      return new ReplicaHandler(newReplicaInfo, ref);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "90d1b47a2a400e07e2b6b812c4bbd9c4f2877786": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12776. [READ] Increasing replication for PROVIDED files should create local replicas\n",
      "commitDate": "15/12/17 5:51 PM",
      "commitName": "90d1b47a2a400e07e2b6b812c4bbd9c4f2877786",
      "commitAuthor": "Virajith Jalaparti",
      "commitDateOld": "15/12/17 5:51 PM",
      "commitNameOld": "b668eb91556b8c85c2b4925808ccb1f769031c20",
      "commitAuthorOld": "Virajith Jalaparti",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,77 @@\n   public ReplicaHandler createTemporary(StorageType storageType,\n       String storageId, ExtendedBlock b, boolean isTransfer)\n       throws IOException {\n     long startTimeMs \u003d Time.monotonicNow();\n     long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n     ReplicaInfo lastFoundReplicaInfo \u003d null;\n     boolean isInPipeline \u003d false;\n     do {\n       try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n         ReplicaInfo currentReplicaInfo \u003d\n             volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n         if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n           break;\n         } else {\n           isInPipeline \u003d currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n               || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW;\n           /*\n-           * If the current block is old, reject.\n+           * If the current block is not PROVIDED and old, reject.\n            * else If transfer request, then accept it.\n            * else if state is not RBW/Temporary, then reject\n+           * If current block is PROVIDED, ignore the replica.\n            */\n-          if ((currentReplicaInfo.getGenerationStamp() \u003e\u003d b.getGenerationStamp())\n-              || (!isTransfer \u0026\u0026 !isInPipeline)) {\n+          if (((currentReplicaInfo.getGenerationStamp() \u003e\u003d b\n+              .getGenerationStamp()) || (!isTransfer \u0026\u0026 !isInPipeline))\n+              \u0026\u0026 !isReplicaProvided(currentReplicaInfo)) {\n             throw new ReplicaAlreadyExistsException(\"Block \" + b\n                 + \" already exists in state \" + currentReplicaInfo.getState()\n                 + \" and thus cannot be created.\");\n           }\n           lastFoundReplicaInfo \u003d currentReplicaInfo;\n         }\n       }\n       if (!isInPipeline) {\n         continue;\n       }\n       // Hang too long, just bail out. This is not supposed to happen.\n       long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n       if (writerStopMs \u003e writerStopTimeoutMs) {\n         LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n             + writerStopMs + \" miniseconds.\");\n         throw new IOException(\"Unable to stop existing writer for block \" + b\n             + \" after \" + writerStopMs + \" miniseconds.\");\n       }\n \n+      // if lastFoundReplicaInfo is PROVIDED and FINALIZED,\n+      // stopWriter isn\u0027t required.\n+      if (isReplicaProvided(lastFoundReplicaInfo) \u0026\u0026\n+          lastFoundReplicaInfo.getState() \u003d\u003d ReplicaState.FINALIZED) {\n+        continue;\n+      }\n       // Stop the previous writer\n       ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n     } while (true);\n-\n-    if (lastFoundReplicaInfo !\u003d null) {\n+    if (lastFoundReplicaInfo !\u003d null\n+        \u0026\u0026 !isReplicaProvided(lastFoundReplicaInfo)) {\n       // Old blockfile should be deleted synchronously as it might collide\n       // with the new block if allocated in same volume.\n       // Do the deletion outside of lock as its DISK IO.\n       invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo },\n           false);\n     }\n     try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n       FsVolumeReference ref \u003d volumes.getNextVolume(storageType, storageId, b\n           .getNumBytes());\n       FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n       ReplicaInPipeline newReplicaInfo;\n       try {\n         newReplicaInfo \u003d v.createTemporary(b);\n       } catch (IOException e) {\n         IOUtils.cleanup(null, ref);\n         throw e;\n       }\n \n       volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n       return new ReplicaHandler(newReplicaInfo, ref);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public ReplicaHandler createTemporary(StorageType storageType,\n      String storageId, ExtendedBlock b, boolean isTransfer)\n      throws IOException {\n    long startTimeMs \u003d Time.monotonicNow();\n    long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n    ReplicaInfo lastFoundReplicaInfo \u003d null;\n    boolean isInPipeline \u003d false;\n    do {\n      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n        ReplicaInfo currentReplicaInfo \u003d\n            volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n        if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n          break;\n        } else {\n          isInPipeline \u003d currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n              || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW;\n          /*\n           * If the current block is not PROVIDED and old, reject.\n           * else If transfer request, then accept it.\n           * else if state is not RBW/Temporary, then reject\n           * If current block is PROVIDED, ignore the replica.\n           */\n          if (((currentReplicaInfo.getGenerationStamp() \u003e\u003d b\n              .getGenerationStamp()) || (!isTransfer \u0026\u0026 !isInPipeline))\n              \u0026\u0026 !isReplicaProvided(currentReplicaInfo)) {\n            throw new ReplicaAlreadyExistsException(\"Block \" + b\n                + \" already exists in state \" + currentReplicaInfo.getState()\n                + \" and thus cannot be created.\");\n          }\n          lastFoundReplicaInfo \u003d currentReplicaInfo;\n        }\n      }\n      if (!isInPipeline) {\n        continue;\n      }\n      // Hang too long, just bail out. This is not supposed to happen.\n      long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n      if (writerStopMs \u003e writerStopTimeoutMs) {\n        LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n            + writerStopMs + \" miniseconds.\");\n        throw new IOException(\"Unable to stop existing writer for block \" + b\n            + \" after \" + writerStopMs + \" miniseconds.\");\n      }\n\n      // if lastFoundReplicaInfo is PROVIDED and FINALIZED,\n      // stopWriter isn\u0027t required.\n      if (isReplicaProvided(lastFoundReplicaInfo) \u0026\u0026\n          lastFoundReplicaInfo.getState() \u003d\u003d ReplicaState.FINALIZED) {\n        continue;\n      }\n      // Stop the previous writer\n      ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n    } while (true);\n    if (lastFoundReplicaInfo !\u003d null\n        \u0026\u0026 !isReplicaProvided(lastFoundReplicaInfo)) {\n      // Old blockfile should be deleted synchronously as it might collide\n      // with the new block if allocated in same volume.\n      // Do the deletion outside of lock as its DISK IO.\n      invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo },\n          false);\n    }\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      FsVolumeReference ref \u003d volumes.getNextVolume(storageType, storageId, b\n          .getNumBytes());\n      FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n      ReplicaInPipeline newReplicaInfo;\n      try {\n        newReplicaInfo \u003d v.createTemporary(b);\n      } catch (IOException e) {\n        IOUtils.cleanup(null, ref);\n        throw e;\n      }\n\n      volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n      return new ReplicaHandler(newReplicaInfo, ref);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "29b7df960fc3d0a7d1416225c3106c7d4222f0ca": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-11856. Ability to re-add Upgrading Nodes to pipeline for future pipeline updates. Contributed by Vinayakumar B.\n",
      "commitDate": "25/05/17 11:05 AM",
      "commitName": "29b7df960fc3d0a7d1416225c3106c7d4222f0ca",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-11856. Ability to re-add Upgrading Nodes to pipeline for future pipeline updates. Contributed by Vinayakumar B.\n",
          "commitDate": "25/05/17 11:05 AM",
          "commitName": "29b7df960fc3d0a7d1416225c3106c7d4222f0ca",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "11/05/17 7:08 PM",
          "commitNameOld": "1411612aa4e70c704b941723217ed4efd8a0125b",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 13.66,
          "commitsBetweenForRepo": 65,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,52 +1,69 @@\n-  public ReplicaHandler createTemporary(\n-      StorageType storageType, String storageId, ExtendedBlock b)\n+  public ReplicaHandler createTemporary(StorageType storageType,\n+      String storageId, ExtendedBlock b, boolean isTransfer)\n       throws IOException {\n     long startTimeMs \u003d Time.monotonicNow();\n     long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n     ReplicaInfo lastFoundReplicaInfo \u003d null;\n+    boolean isInPipeline \u003d false;\n     do {\n       try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n         ReplicaInfo currentReplicaInfo \u003d\n             volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n         if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n-          if (lastFoundReplicaInfo !\u003d null) {\n-            invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n-          }\n-          FsVolumeReference ref \u003d\n-              volumes.getNextVolume(storageType, storageId, b.getNumBytes());\n-          FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n-          ReplicaInPipeline newReplicaInfo;\n-          try {\n-            newReplicaInfo \u003d v.createTemporary(b);\n-          } catch (IOException e) {\n-            IOUtils.cleanup(null, ref);\n-            throw e;\n-          }\n-\n-          volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n-          return new ReplicaHandler(newReplicaInfo, ref);\n+          break;\n         } else {\n-          if (!(currentReplicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n-                \u0026\u0026 (currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n-                    || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW))) {\n+          isInPipeline \u003d currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n+              || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW;\n+          /*\n+           * If the current block is old, reject.\n+           * else If transfer request, then accept it.\n+           * else if state is not RBW/Temporary, then reject\n+           */\n+          if ((currentReplicaInfo.getGenerationStamp() \u003e\u003d b.getGenerationStamp())\n+              || (!isTransfer \u0026\u0026 !isInPipeline)) {\n             throw new ReplicaAlreadyExistsException(\"Block \" + b\n                 + \" already exists in state \" + currentReplicaInfo.getState()\n                 + \" and thus cannot be created.\");\n           }\n           lastFoundReplicaInfo \u003d currentReplicaInfo;\n         }\n       }\n-\n+      if (!isInPipeline) {\n+        continue;\n+      }\n       // Hang too long, just bail out. This is not supposed to happen.\n       long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n       if (writerStopMs \u003e writerStopTimeoutMs) {\n         LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n             + writerStopMs + \" miniseconds.\");\n         throw new IOException(\"Unable to stop existing writer for block \" + b\n             + \" after \" + writerStopMs + \" miniseconds.\");\n       }\n \n       // Stop the previous writer\n       ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n     } while (true);\n+\n+    if (lastFoundReplicaInfo !\u003d null) {\n+      // Old blockfile should be deleted synchronously as it might collide\n+      // with the new block if allocated in same volume.\n+      // Do the deletion outside of lock as its DISK IO.\n+      invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo },\n+          false);\n+    }\n+    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n+      FsVolumeReference ref \u003d volumes.getNextVolume(storageType, storageId, b\n+          .getNumBytes());\n+      FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n+      ReplicaInPipeline newReplicaInfo;\n+      try {\n+        newReplicaInfo \u003d v.createTemporary(b);\n+      } catch (IOException e) {\n+        IOUtils.cleanup(null, ref);\n+        throw e;\n+      }\n+\n+      volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n+      return new ReplicaHandler(newReplicaInfo, ref);\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public ReplicaHandler createTemporary(StorageType storageType,\n      String storageId, ExtendedBlock b, boolean isTransfer)\n      throws IOException {\n    long startTimeMs \u003d Time.monotonicNow();\n    long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n    ReplicaInfo lastFoundReplicaInfo \u003d null;\n    boolean isInPipeline \u003d false;\n    do {\n      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n        ReplicaInfo currentReplicaInfo \u003d\n            volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n        if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n          break;\n        } else {\n          isInPipeline \u003d currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n              || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW;\n          /*\n           * If the current block is old, reject.\n           * else If transfer request, then accept it.\n           * else if state is not RBW/Temporary, then reject\n           */\n          if ((currentReplicaInfo.getGenerationStamp() \u003e\u003d b.getGenerationStamp())\n              || (!isTransfer \u0026\u0026 !isInPipeline)) {\n            throw new ReplicaAlreadyExistsException(\"Block \" + b\n                + \" already exists in state \" + currentReplicaInfo.getState()\n                + \" and thus cannot be created.\");\n          }\n          lastFoundReplicaInfo \u003d currentReplicaInfo;\n        }\n      }\n      if (!isInPipeline) {\n        continue;\n      }\n      // Hang too long, just bail out. This is not supposed to happen.\n      long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n      if (writerStopMs \u003e writerStopTimeoutMs) {\n        LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n            + writerStopMs + \" miniseconds.\");\n        throw new IOException(\"Unable to stop existing writer for block \" + b\n            + \" after \" + writerStopMs + \" miniseconds.\");\n      }\n\n      // Stop the previous writer\n      ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n    } while (true);\n\n    if (lastFoundReplicaInfo !\u003d null) {\n      // Old blockfile should be deleted synchronously as it might collide\n      // with the new block if allocated in same volume.\n      // Do the deletion outside of lock as its DISK IO.\n      invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo },\n          false);\n    }\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      FsVolumeReference ref \u003d volumes.getNextVolume(storageType, storageId, b\n          .getNumBytes());\n      FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n      ReplicaInPipeline newReplicaInfo;\n      try {\n        newReplicaInfo \u003d v.createTemporary(b);\n      } catch (IOException e) {\n        IOUtils.cleanup(null, ref);\n        throw e;\n      }\n\n      volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n      return new ReplicaHandler(newReplicaInfo, ref);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[storageType-StorageType, storageId-String, b-ExtendedBlock]",
            "newValue": "[storageType-StorageType, storageId-String, b-ExtendedBlock, isTransfer-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-11856. Ability to re-add Upgrading Nodes to pipeline for future pipeline updates. Contributed by Vinayakumar B.\n",
          "commitDate": "25/05/17 11:05 AM",
          "commitName": "29b7df960fc3d0a7d1416225c3106c7d4222f0ca",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "11/05/17 7:08 PM",
          "commitNameOld": "1411612aa4e70c704b941723217ed4efd8a0125b",
          "commitAuthorOld": "Vinayakumar B",
          "daysBetweenCommits": 13.66,
          "commitsBetweenForRepo": 65,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,52 +1,69 @@\n-  public ReplicaHandler createTemporary(\n-      StorageType storageType, String storageId, ExtendedBlock b)\n+  public ReplicaHandler createTemporary(StorageType storageType,\n+      String storageId, ExtendedBlock b, boolean isTransfer)\n       throws IOException {\n     long startTimeMs \u003d Time.monotonicNow();\n     long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n     ReplicaInfo lastFoundReplicaInfo \u003d null;\n+    boolean isInPipeline \u003d false;\n     do {\n       try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n         ReplicaInfo currentReplicaInfo \u003d\n             volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n         if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n-          if (lastFoundReplicaInfo !\u003d null) {\n-            invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n-          }\n-          FsVolumeReference ref \u003d\n-              volumes.getNextVolume(storageType, storageId, b.getNumBytes());\n-          FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n-          ReplicaInPipeline newReplicaInfo;\n-          try {\n-            newReplicaInfo \u003d v.createTemporary(b);\n-          } catch (IOException e) {\n-            IOUtils.cleanup(null, ref);\n-            throw e;\n-          }\n-\n-          volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n-          return new ReplicaHandler(newReplicaInfo, ref);\n+          break;\n         } else {\n-          if (!(currentReplicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n-                \u0026\u0026 (currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n-                    || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW))) {\n+          isInPipeline \u003d currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n+              || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW;\n+          /*\n+           * If the current block is old, reject.\n+           * else If transfer request, then accept it.\n+           * else if state is not RBW/Temporary, then reject\n+           */\n+          if ((currentReplicaInfo.getGenerationStamp() \u003e\u003d b.getGenerationStamp())\n+              || (!isTransfer \u0026\u0026 !isInPipeline)) {\n             throw new ReplicaAlreadyExistsException(\"Block \" + b\n                 + \" already exists in state \" + currentReplicaInfo.getState()\n                 + \" and thus cannot be created.\");\n           }\n           lastFoundReplicaInfo \u003d currentReplicaInfo;\n         }\n       }\n-\n+      if (!isInPipeline) {\n+        continue;\n+      }\n       // Hang too long, just bail out. This is not supposed to happen.\n       long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n       if (writerStopMs \u003e writerStopTimeoutMs) {\n         LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n             + writerStopMs + \" miniseconds.\");\n         throw new IOException(\"Unable to stop existing writer for block \" + b\n             + \" after \" + writerStopMs + \" miniseconds.\");\n       }\n \n       // Stop the previous writer\n       ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n     } while (true);\n+\n+    if (lastFoundReplicaInfo !\u003d null) {\n+      // Old blockfile should be deleted synchronously as it might collide\n+      // with the new block if allocated in same volume.\n+      // Do the deletion outside of lock as its DISK IO.\n+      invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo },\n+          false);\n+    }\n+    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n+      FsVolumeReference ref \u003d volumes.getNextVolume(storageType, storageId, b\n+          .getNumBytes());\n+      FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n+      ReplicaInPipeline newReplicaInfo;\n+      try {\n+        newReplicaInfo \u003d v.createTemporary(b);\n+      } catch (IOException e) {\n+        IOUtils.cleanup(null, ref);\n+        throw e;\n+      }\n+\n+      volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n+      return new ReplicaHandler(newReplicaInfo, ref);\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public ReplicaHandler createTemporary(StorageType storageType,\n      String storageId, ExtendedBlock b, boolean isTransfer)\n      throws IOException {\n    long startTimeMs \u003d Time.monotonicNow();\n    long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n    ReplicaInfo lastFoundReplicaInfo \u003d null;\n    boolean isInPipeline \u003d false;\n    do {\n      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n        ReplicaInfo currentReplicaInfo \u003d\n            volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n        if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n          break;\n        } else {\n          isInPipeline \u003d currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n              || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW;\n          /*\n           * If the current block is old, reject.\n           * else If transfer request, then accept it.\n           * else if state is not RBW/Temporary, then reject\n           */\n          if ((currentReplicaInfo.getGenerationStamp() \u003e\u003d b.getGenerationStamp())\n              || (!isTransfer \u0026\u0026 !isInPipeline)) {\n            throw new ReplicaAlreadyExistsException(\"Block \" + b\n                + \" already exists in state \" + currentReplicaInfo.getState()\n                + \" and thus cannot be created.\");\n          }\n          lastFoundReplicaInfo \u003d currentReplicaInfo;\n        }\n      }\n      if (!isInPipeline) {\n        continue;\n      }\n      // Hang too long, just bail out. This is not supposed to happen.\n      long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n      if (writerStopMs \u003e writerStopTimeoutMs) {\n        LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n            + writerStopMs + \" miniseconds.\");\n        throw new IOException(\"Unable to stop existing writer for block \" + b\n            + \" after \" + writerStopMs + \" miniseconds.\");\n      }\n\n      // Stop the previous writer\n      ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n    } while (true);\n\n    if (lastFoundReplicaInfo !\u003d null) {\n      // Old blockfile should be deleted synchronously as it might collide\n      // with the new block if allocated in same volume.\n      // Do the deletion outside of lock as its DISK IO.\n      invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo },\n          false);\n    }\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      FsVolumeReference ref \u003d volumes.getNextVolume(storageType, storageId, b\n          .getNumBytes());\n      FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n      ReplicaInPipeline newReplicaInfo;\n      try {\n        newReplicaInfo \u003d v.createTemporary(b);\n      } catch (IOException e) {\n        IOUtils.cleanup(null, ref);\n        throw e;\n      }\n\n      volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n      return new ReplicaHandler(newReplicaInfo, ref);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "a3954ccab148bddc290cb96528e63ff19799bcc9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
      "commitDate": "05/05/17 12:01 PM",
      "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthor": "Chris Douglas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
          "commitDate": "05/05/17 12:01 PM",
          "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "10/03/17 2:37 PM",
          "commitNameOld": "6d356b6b4d8ccb32397cacfb5d0357b21f6035fc",
          "commitAuthorOld": "Lei Xu",
          "daysBetweenCommits": 55.85,
          "commitsBetweenForRepo": 315,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,52 @@\n   public ReplicaHandler createTemporary(\n-      StorageType storageType, ExtendedBlock b) throws IOException {\n+      StorageType storageType, String storageId, ExtendedBlock b)\n+      throws IOException {\n     long startTimeMs \u003d Time.monotonicNow();\n     long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n     ReplicaInfo lastFoundReplicaInfo \u003d null;\n     do {\n       try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n         ReplicaInfo currentReplicaInfo \u003d\n             volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n         if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n           if (lastFoundReplicaInfo !\u003d null) {\n             invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n           }\n           FsVolumeReference ref \u003d\n-              volumes.getNextVolume(storageType, b.getNumBytes());\n+              volumes.getNextVolume(storageType, storageId, b.getNumBytes());\n           FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n           ReplicaInPipeline newReplicaInfo;\n           try {\n             newReplicaInfo \u003d v.createTemporary(b);\n           } catch (IOException e) {\n             IOUtils.cleanup(null, ref);\n             throw e;\n           }\n \n           volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n           return new ReplicaHandler(newReplicaInfo, ref);\n         } else {\n           if (!(currentReplicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n                 \u0026\u0026 (currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n                     || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW))) {\n             throw new ReplicaAlreadyExistsException(\"Block \" + b\n                 + \" already exists in state \" + currentReplicaInfo.getState()\n                 + \" and thus cannot be created.\");\n           }\n           lastFoundReplicaInfo \u003d currentReplicaInfo;\n         }\n       }\n \n       // Hang too long, just bail out. This is not supposed to happen.\n       long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n       if (writerStopMs \u003e writerStopTimeoutMs) {\n         LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n             + writerStopMs + \" miniseconds.\");\n         throw new IOException(\"Unable to stop existing writer for block \" + b\n             + \" after \" + writerStopMs + \" miniseconds.\");\n       }\n \n       // Stop the previous writer\n       ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n     } while (true);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public ReplicaHandler createTemporary(\n      StorageType storageType, String storageId, ExtendedBlock b)\n      throws IOException {\n    long startTimeMs \u003d Time.monotonicNow();\n    long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n    ReplicaInfo lastFoundReplicaInfo \u003d null;\n    do {\n      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n        ReplicaInfo currentReplicaInfo \u003d\n            volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n        if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n          if (lastFoundReplicaInfo !\u003d null) {\n            invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n          }\n          FsVolumeReference ref \u003d\n              volumes.getNextVolume(storageType, storageId, b.getNumBytes());\n          FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n          ReplicaInPipeline newReplicaInfo;\n          try {\n            newReplicaInfo \u003d v.createTemporary(b);\n          } catch (IOException e) {\n            IOUtils.cleanup(null, ref);\n            throw e;\n          }\n\n          volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n          return new ReplicaHandler(newReplicaInfo, ref);\n        } else {\n          if (!(currentReplicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n                \u0026\u0026 (currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n                    || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW))) {\n            throw new ReplicaAlreadyExistsException(\"Block \" + b\n                + \" already exists in state \" + currentReplicaInfo.getState()\n                + \" and thus cannot be created.\");\n          }\n          lastFoundReplicaInfo \u003d currentReplicaInfo;\n        }\n      }\n\n      // Hang too long, just bail out. This is not supposed to happen.\n      long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n      if (writerStopMs \u003e writerStopTimeoutMs) {\n        LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n            + writerStopMs + \" miniseconds.\");\n        throw new IOException(\"Unable to stop existing writer for block \" + b\n            + \" after \" + writerStopMs + \" miniseconds.\");\n      }\n\n      // Stop the previous writer\n      ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n    } while (true);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[storageType-StorageType, b-ExtendedBlock]",
            "newValue": "[storageType-StorageType, storageId-String, b-ExtendedBlock]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9807. Add an optional StorageID to writes. Contributed by Ewan Higgs\n",
          "commitDate": "05/05/17 12:01 PM",
          "commitName": "a3954ccab148bddc290cb96528e63ff19799bcc9",
          "commitAuthor": "Chris Douglas",
          "commitDateOld": "10/03/17 2:37 PM",
          "commitNameOld": "6d356b6b4d8ccb32397cacfb5d0357b21f6035fc",
          "commitAuthorOld": "Lei Xu",
          "daysBetweenCommits": 55.85,
          "commitsBetweenForRepo": 315,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,51 +1,52 @@\n   public ReplicaHandler createTemporary(\n-      StorageType storageType, ExtendedBlock b) throws IOException {\n+      StorageType storageType, String storageId, ExtendedBlock b)\n+      throws IOException {\n     long startTimeMs \u003d Time.monotonicNow();\n     long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n     ReplicaInfo lastFoundReplicaInfo \u003d null;\n     do {\n       try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n         ReplicaInfo currentReplicaInfo \u003d\n             volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n         if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n           if (lastFoundReplicaInfo !\u003d null) {\n             invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n           }\n           FsVolumeReference ref \u003d\n-              volumes.getNextVolume(storageType, b.getNumBytes());\n+              volumes.getNextVolume(storageType, storageId, b.getNumBytes());\n           FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n           ReplicaInPipeline newReplicaInfo;\n           try {\n             newReplicaInfo \u003d v.createTemporary(b);\n           } catch (IOException e) {\n             IOUtils.cleanup(null, ref);\n             throw e;\n           }\n \n           volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n           return new ReplicaHandler(newReplicaInfo, ref);\n         } else {\n           if (!(currentReplicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n                 \u0026\u0026 (currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n                     || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW))) {\n             throw new ReplicaAlreadyExistsException(\"Block \" + b\n                 + \" already exists in state \" + currentReplicaInfo.getState()\n                 + \" and thus cannot be created.\");\n           }\n           lastFoundReplicaInfo \u003d currentReplicaInfo;\n         }\n       }\n \n       // Hang too long, just bail out. This is not supposed to happen.\n       long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n       if (writerStopMs \u003e writerStopTimeoutMs) {\n         LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n             + writerStopMs + \" miniseconds.\");\n         throw new IOException(\"Unable to stop existing writer for block \" + b\n             + \" after \" + writerStopMs + \" miniseconds.\");\n       }\n \n       // Stop the previous writer\n       ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n     } while (true);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public ReplicaHandler createTemporary(\n      StorageType storageType, String storageId, ExtendedBlock b)\n      throws IOException {\n    long startTimeMs \u003d Time.monotonicNow();\n    long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n    ReplicaInfo lastFoundReplicaInfo \u003d null;\n    do {\n      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n        ReplicaInfo currentReplicaInfo \u003d\n            volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n        if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n          if (lastFoundReplicaInfo !\u003d null) {\n            invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n          }\n          FsVolumeReference ref \u003d\n              volumes.getNextVolume(storageType, storageId, b.getNumBytes());\n          FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n          ReplicaInPipeline newReplicaInfo;\n          try {\n            newReplicaInfo \u003d v.createTemporary(b);\n          } catch (IOException e) {\n            IOUtils.cleanup(null, ref);\n            throw e;\n          }\n\n          volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n          return new ReplicaHandler(newReplicaInfo, ref);\n        } else {\n          if (!(currentReplicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n                \u0026\u0026 (currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n                    || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW))) {\n            throw new ReplicaAlreadyExistsException(\"Block \" + b\n                + \" already exists in state \" + currentReplicaInfo.getState()\n                + \" and thus cannot be created.\");\n          }\n          lastFoundReplicaInfo \u003d currentReplicaInfo;\n        }\n      }\n\n      // Hang too long, just bail out. This is not supposed to happen.\n      long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n      if (writerStopMs \u003e writerStopTimeoutMs) {\n        LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n            + writerStopMs + \" miniseconds.\");\n        throw new IOException(\"Unable to stop existing writer for block \" + b\n            + \" after \" + writerStopMs + \" miniseconds.\");\n      }\n\n      // Stop the previous writer\n      ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n    } while (true);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "86c9862bec0248d671e657aa56094a2919b8ac14": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "13/09/16 12:54 PM",
      "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "10/09/16 6:22 PM",
      "commitNameOld": "a99bf26a0899bcc4307c3a242c8414eaef555aa7",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 2.77,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,51 @@\n   public ReplicaHandler createTemporary(\n       StorageType storageType, ExtendedBlock b) throws IOException {\n     long startTimeMs \u003d Time.monotonicNow();\n     long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n     ReplicaInfo lastFoundReplicaInfo \u003d null;\n     do {\n       try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n         ReplicaInfo currentReplicaInfo \u003d\n             volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n         if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n           if (lastFoundReplicaInfo !\u003d null) {\n             invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n           }\n           FsVolumeReference ref \u003d\n               volumes.getNextVolume(storageType, b.getNumBytes());\n           FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n-          // create a temporary file to hold block in the designated volume\n-          File f;\n+          ReplicaInPipeline newReplicaInfo;\n           try {\n-            f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n+            newReplicaInfo \u003d v.createTemporary(b);\n           } catch (IOException e) {\n             IOUtils.cleanup(null, ref);\n             throw e;\n           }\n-          ReplicaInPipeline newReplicaInfo \u003d\n-              new ReplicaInPipeline(b.getBlockId(), b.getGenerationStamp(), v,\n-                  f.getParentFile(), b.getLocalBlock().getNumBytes());\n-          volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n+\n+          volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n           return new ReplicaHandler(newReplicaInfo, ref);\n         } else {\n-          if (!(currentReplicaInfo.getGenerationStamp() \u003c b\n-              .getGenerationStamp() \u0026\u0026 currentReplicaInfo instanceof ReplicaInPipeline)) {\n+          if (!(currentReplicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n+                \u0026\u0026 (currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n+                    || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW))) {\n             throw new ReplicaAlreadyExistsException(\"Block \" + b\n                 + \" already exists in state \" + currentReplicaInfo.getState()\n                 + \" and thus cannot be created.\");\n           }\n           lastFoundReplicaInfo \u003d currentReplicaInfo;\n         }\n       }\n \n       // Hang too long, just bail out. This is not supposed to happen.\n       long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n       if (writerStopMs \u003e writerStopTimeoutMs) {\n         LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n             + writerStopMs + \" miniseconds.\");\n         throw new IOException(\"Unable to stop existing writer for block \" + b\n             + \" after \" + writerStopMs + \" miniseconds.\");\n       }\n \n       // Stop the previous writer\n-      ((ReplicaInPipeline) lastFoundReplicaInfo)\n-          .stopWriter(writerStopTimeoutMs);\n+      ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n     } while (true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public ReplicaHandler createTemporary(\n      StorageType storageType, ExtendedBlock b) throws IOException {\n    long startTimeMs \u003d Time.monotonicNow();\n    long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n    ReplicaInfo lastFoundReplicaInfo \u003d null;\n    do {\n      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n        ReplicaInfo currentReplicaInfo \u003d\n            volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n        if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n          if (lastFoundReplicaInfo !\u003d null) {\n            invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n          }\n          FsVolumeReference ref \u003d\n              volumes.getNextVolume(storageType, b.getNumBytes());\n          FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n          ReplicaInPipeline newReplicaInfo;\n          try {\n            newReplicaInfo \u003d v.createTemporary(b);\n          } catch (IOException e) {\n            IOUtils.cleanup(null, ref);\n            throw e;\n          }\n\n          volumeMap.add(b.getBlockPoolId(), newReplicaInfo.getReplicaInfo());\n          return new ReplicaHandler(newReplicaInfo, ref);\n        } else {\n          if (!(currentReplicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n                \u0026\u0026 (currentReplicaInfo.getState() \u003d\u003d ReplicaState.TEMPORARY\n                    || currentReplicaInfo.getState() \u003d\u003d ReplicaState.RBW))) {\n            throw new ReplicaAlreadyExistsException(\"Block \" + b\n                + \" already exists in state \" + currentReplicaInfo.getState()\n                + \" and thus cannot be created.\");\n          }\n          lastFoundReplicaInfo \u003d currentReplicaInfo;\n        }\n      }\n\n      // Hang too long, just bail out. This is not supposed to happen.\n      long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n      if (writerStopMs \u003e writerStopTimeoutMs) {\n        LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n            + writerStopMs + \" miniseconds.\");\n        throw new IOException(\"Unable to stop existing writer for block \" + b\n            + \" after \" + writerStopMs + \" miniseconds.\");\n      }\n\n      // Stop the previous writer\n      ((ReplicaInPipeline)lastFoundReplicaInfo).stopWriter(writerStopTimeoutMs);\n    } while (true);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10682. Replace FsDatasetImpl object lock with a separate lock object. (Chen Liang)\n",
      "commitDate": "08/08/16 12:02 PM",
      "commitName": "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "08/07/16 7:40 PM",
      "commitNameOld": "da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
      "commitAuthorOld": "Yongjun Zhang",
      "daysBetweenCommits": 30.68,
      "commitsBetweenForRepo": 320,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,54 @@\n   public ReplicaHandler createTemporary(\n       StorageType storageType, ExtendedBlock b) throws IOException {\n     long startTimeMs \u003d Time.monotonicNow();\n     long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n     ReplicaInfo lastFoundReplicaInfo \u003d null;\n     do {\n-      synchronized (this) {\n+      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n         ReplicaInfo currentReplicaInfo \u003d\n             volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n         if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n           if (lastFoundReplicaInfo !\u003d null) {\n             invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n           }\n           FsVolumeReference ref \u003d\n               volumes.getNextVolume(storageType, b.getNumBytes());\n           FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n           // create a temporary file to hold block in the designated volume\n           File f;\n           try {\n             f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n           } catch (IOException e) {\n             IOUtils.cleanup(null, ref);\n             throw e;\n           }\n           ReplicaInPipeline newReplicaInfo \u003d\n               new ReplicaInPipeline(b.getBlockId(), b.getGenerationStamp(), v,\n                   f.getParentFile(), b.getLocalBlock().getNumBytes());\n           volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n           return new ReplicaHandler(newReplicaInfo, ref);\n         } else {\n           if (!(currentReplicaInfo.getGenerationStamp() \u003c b\n               .getGenerationStamp() \u0026\u0026 currentReplicaInfo instanceof ReplicaInPipeline)) {\n             throw new ReplicaAlreadyExistsException(\"Block \" + b\n                 + \" already exists in state \" + currentReplicaInfo.getState()\n                 + \" and thus cannot be created.\");\n           }\n           lastFoundReplicaInfo \u003d currentReplicaInfo;\n         }\n       }\n \n       // Hang too long, just bail out. This is not supposed to happen.\n       long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n       if (writerStopMs \u003e writerStopTimeoutMs) {\n         LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n             + writerStopMs + \" miniseconds.\");\n         throw new IOException(\"Unable to stop existing writer for block \" + b\n             + \" after \" + writerStopMs + \" miniseconds.\");\n       }\n \n       // Stop the previous writer\n       ((ReplicaInPipeline) lastFoundReplicaInfo)\n           .stopWriter(writerStopTimeoutMs);\n     } while (true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public ReplicaHandler createTemporary(\n      StorageType storageType, ExtendedBlock b) throws IOException {\n    long startTimeMs \u003d Time.monotonicNow();\n    long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n    ReplicaInfo lastFoundReplicaInfo \u003d null;\n    do {\n      try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n        ReplicaInfo currentReplicaInfo \u003d\n            volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n        if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n          if (lastFoundReplicaInfo !\u003d null) {\n            invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n          }\n          FsVolumeReference ref \u003d\n              volumes.getNextVolume(storageType, b.getNumBytes());\n          FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n          // create a temporary file to hold block in the designated volume\n          File f;\n          try {\n            f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n          } catch (IOException e) {\n            IOUtils.cleanup(null, ref);\n            throw e;\n          }\n          ReplicaInPipeline newReplicaInfo \u003d\n              new ReplicaInPipeline(b.getBlockId(), b.getGenerationStamp(), v,\n                  f.getParentFile(), b.getLocalBlock().getNumBytes());\n          volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n          return new ReplicaHandler(newReplicaInfo, ref);\n        } else {\n          if (!(currentReplicaInfo.getGenerationStamp() \u003c b\n              .getGenerationStamp() \u0026\u0026 currentReplicaInfo instanceof ReplicaInPipeline)) {\n            throw new ReplicaAlreadyExistsException(\"Block \" + b\n                + \" already exists in state \" + currentReplicaInfo.getState()\n                + \" and thus cannot be created.\");\n          }\n          lastFoundReplicaInfo \u003d currentReplicaInfo;\n        }\n      }\n\n      // Hang too long, just bail out. This is not supposed to happen.\n      long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n      if (writerStopMs \u003e writerStopTimeoutMs) {\n        LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n            + writerStopMs + \" miniseconds.\");\n        throw new IOException(\"Unable to stop existing writer for block \" + b\n            + \" after \" + writerStopMs + \" miniseconds.\");\n      }\n\n      // Stop the previous writer\n      ((ReplicaInPipeline) lastFoundReplicaInfo)\n          .stopWriter(writerStopTimeoutMs);\n    } while (true);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "92c1af1646b1d91a2ab7821e4f7d450e3b6e10bb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6955. DN should reserve disk space for a full block when creating tmp files (Contributed by Kanaka Kumar Avvaru)\n",
      "commitDate": "18/09/15 4:07 AM",
      "commitName": "92c1af1646b1d91a2ab7821e4f7d450e3b6e10bb",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "08/09/15 6:37 PM",
      "commitNameOld": "a153b9601ad8628fdd608d8696310ca8c1f58ff0",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 9.4,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,54 @@\n   public ReplicaHandler createTemporary(\n       StorageType storageType, ExtendedBlock b) throws IOException {\n     long startTimeMs \u003d Time.monotonicNow();\n     long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n     ReplicaInfo lastFoundReplicaInfo \u003d null;\n     do {\n       synchronized (this) {\n         ReplicaInfo currentReplicaInfo \u003d\n             volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n         if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n           if (lastFoundReplicaInfo !\u003d null) {\n             invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n           }\n           FsVolumeReference ref \u003d\n               volumes.getNextVolume(storageType, b.getNumBytes());\n           FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n           // create a temporary file to hold block in the designated volume\n           File f;\n           try {\n             f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n           } catch (IOException e) {\n             IOUtils.cleanup(null, ref);\n             throw e;\n           }\n           ReplicaInPipeline newReplicaInfo \u003d\n               new ReplicaInPipeline(b.getBlockId(), b.getGenerationStamp(), v,\n-                  f.getParentFile(), 0);\n+                  f.getParentFile(), b.getLocalBlock().getNumBytes());\n           volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n           return new ReplicaHandler(newReplicaInfo, ref);\n         } else {\n           if (!(currentReplicaInfo.getGenerationStamp() \u003c b\n               .getGenerationStamp() \u0026\u0026 currentReplicaInfo instanceof ReplicaInPipeline)) {\n             throw new ReplicaAlreadyExistsException(\"Block \" + b\n                 + \" already exists in state \" + currentReplicaInfo.getState()\n                 + \" and thus cannot be created.\");\n           }\n           lastFoundReplicaInfo \u003d currentReplicaInfo;\n         }\n       }\n \n       // Hang too long, just bail out. This is not supposed to happen.\n       long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n       if (writerStopMs \u003e writerStopTimeoutMs) {\n         LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n             + writerStopMs + \" miniseconds.\");\n         throw new IOException(\"Unable to stop existing writer for block \" + b\n             + \" after \" + writerStopMs + \" miniseconds.\");\n       }\n \n       // Stop the previous writer\n       ((ReplicaInPipeline) lastFoundReplicaInfo)\n           .stopWriter(writerStopTimeoutMs);\n     } while (true);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public ReplicaHandler createTemporary(\n      StorageType storageType, ExtendedBlock b) throws IOException {\n    long startTimeMs \u003d Time.monotonicNow();\n    long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n    ReplicaInfo lastFoundReplicaInfo \u003d null;\n    do {\n      synchronized (this) {\n        ReplicaInfo currentReplicaInfo \u003d\n            volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n        if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n          if (lastFoundReplicaInfo !\u003d null) {\n            invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n          }\n          FsVolumeReference ref \u003d\n              volumes.getNextVolume(storageType, b.getNumBytes());\n          FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n          // create a temporary file to hold block in the designated volume\n          File f;\n          try {\n            f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n          } catch (IOException e) {\n            IOUtils.cleanup(null, ref);\n            throw e;\n          }\n          ReplicaInPipeline newReplicaInfo \u003d\n              new ReplicaInPipeline(b.getBlockId(), b.getGenerationStamp(), v,\n                  f.getParentFile(), b.getLocalBlock().getNumBytes());\n          volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n          return new ReplicaHandler(newReplicaInfo, ref);\n        } else {\n          if (!(currentReplicaInfo.getGenerationStamp() \u003c b\n              .getGenerationStamp() \u0026\u0026 currentReplicaInfo instanceof ReplicaInPipeline)) {\n            throw new ReplicaAlreadyExistsException(\"Block \" + b\n                + \" already exists in state \" + currentReplicaInfo.getState()\n                + \" and thus cannot be created.\");\n          }\n          lastFoundReplicaInfo \u003d currentReplicaInfo;\n        }\n      }\n\n      // Hang too long, just bail out. This is not supposed to happen.\n      long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n      if (writerStopMs \u003e writerStopTimeoutMs) {\n        LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n            + writerStopMs + \" miniseconds.\");\n        throw new IOException(\"Unable to stop existing writer for block \" + b\n            + \" after \" + writerStopMs + \" miniseconds.\");\n      }\n\n      // Stop the previous writer\n      ((ReplicaInPipeline) lastFoundReplicaInfo)\n          .stopWriter(writerStopTimeoutMs);\n    } while (true);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "28bebc81db8bb6d1bc2574de7564fe4c595cfe09": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-7999. FsDatasetImpl#createTemporary sometimes holds the FSDatasetImpl lock for a very long time (sinago via cmccabe)\n",
      "commitDate": "06/04/15 8:56 AM",
      "commitName": "28bebc81db8bb6d1bc2574de7564fe4c595cfe09",
      "commitAuthor": "Colin Patrick Mccabe",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7999. FsDatasetImpl#createTemporary sometimes holds the FSDatasetImpl lock for a very long time (sinago via cmccabe)\n",
          "commitDate": "06/04/15 8:56 AM",
          "commitName": "28bebc81db8bb6d1bc2574de7564fe4c595cfe09",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "30/03/15 3:25 PM",
          "commitNameOld": "1a495fbb489c9e9a23b341a52696d10e9e272b04",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 6.73,
          "commitsBetweenForRepo": 50,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,54 @@\n-  public synchronized ReplicaHandler createTemporary(\n+  public ReplicaHandler createTemporary(\n       StorageType storageType, ExtendedBlock b) throws IOException {\n-    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n-    if (replicaInfo !\u003d null) {\n-      if (replicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n-          \u0026\u0026 replicaInfo instanceof ReplicaInPipeline) {\n-        // Stop the previous writer\n-        ((ReplicaInPipeline)replicaInfo)\n-                      .stopWriter(datanode.getDnConf().getXceiverStopTimeout());\n-        invalidate(b.getBlockPoolId(), new Block[]{replicaInfo});\n-      } else {\n-        throw new ReplicaAlreadyExistsException(\"Block \" + b +\n-            \" already exists in state \" + replicaInfo.getState() +\n-            \" and thus cannot be created.\");\n+    long startTimeMs \u003d Time.monotonicNow();\n+    long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n+    ReplicaInfo lastFoundReplicaInfo \u003d null;\n+    do {\n+      synchronized (this) {\n+        ReplicaInfo currentReplicaInfo \u003d\n+            volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n+        if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n+          if (lastFoundReplicaInfo !\u003d null) {\n+            invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n+          }\n+          FsVolumeReference ref \u003d\n+              volumes.getNextVolume(storageType, b.getNumBytes());\n+          FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n+          // create a temporary file to hold block in the designated volume\n+          File f;\n+          try {\n+            f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n+          } catch (IOException e) {\n+            IOUtils.cleanup(null, ref);\n+            throw e;\n+          }\n+          ReplicaInPipeline newReplicaInfo \u003d\n+              new ReplicaInPipeline(b.getBlockId(), b.getGenerationStamp(), v,\n+                  f.getParentFile(), 0);\n+          volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n+          return new ReplicaHandler(newReplicaInfo, ref);\n+        } else {\n+          if (!(currentReplicaInfo.getGenerationStamp() \u003c b\n+              .getGenerationStamp() \u0026\u0026 currentReplicaInfo instanceof ReplicaInPipeline)) {\n+            throw new ReplicaAlreadyExistsException(\"Block \" + b\n+                + \" already exists in state \" + currentReplicaInfo.getState()\n+                + \" and thus cannot be created.\");\n+          }\n+          lastFoundReplicaInfo \u003d currentReplicaInfo;\n+        }\n       }\n-    }\n \n-    FsVolumeReference ref \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n-    FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n-    // create a temporary file to hold block in the designated volume\n-    File f;\n-    try {\n-      f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n-    } catch (IOException e) {\n-      IOUtils.cleanup(null, ref);\n-      throw e;\n-    }\n+      // Hang too long, just bail out. This is not supposed to happen.\n+      long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n+      if (writerStopMs \u003e writerStopTimeoutMs) {\n+        LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n+            + writerStopMs + \" miniseconds.\");\n+        throw new IOException(\"Unable to stop existing writer for block \" + b\n+            + \" after \" + writerStopMs + \" miniseconds.\");\n+      }\n \n-    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n-        b.getGenerationStamp(), v, f.getParentFile(), 0);\n-    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    return new ReplicaHandler(newReplicaInfo, ref);\n+      // Stop the previous writer\n+      ((ReplicaInPipeline) lastFoundReplicaInfo)\n+          .stopWriter(writerStopTimeoutMs);\n+    } while (true);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public ReplicaHandler createTemporary(\n      StorageType storageType, ExtendedBlock b) throws IOException {\n    long startTimeMs \u003d Time.monotonicNow();\n    long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n    ReplicaInfo lastFoundReplicaInfo \u003d null;\n    do {\n      synchronized (this) {\n        ReplicaInfo currentReplicaInfo \u003d\n            volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n        if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n          if (lastFoundReplicaInfo !\u003d null) {\n            invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n          }\n          FsVolumeReference ref \u003d\n              volumes.getNextVolume(storageType, b.getNumBytes());\n          FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n          // create a temporary file to hold block in the designated volume\n          File f;\n          try {\n            f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n          } catch (IOException e) {\n            IOUtils.cleanup(null, ref);\n            throw e;\n          }\n          ReplicaInPipeline newReplicaInfo \u003d\n              new ReplicaInPipeline(b.getBlockId(), b.getGenerationStamp(), v,\n                  f.getParentFile(), 0);\n          volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n          return new ReplicaHandler(newReplicaInfo, ref);\n        } else {\n          if (!(currentReplicaInfo.getGenerationStamp() \u003c b\n              .getGenerationStamp() \u0026\u0026 currentReplicaInfo instanceof ReplicaInPipeline)) {\n            throw new ReplicaAlreadyExistsException(\"Block \" + b\n                + \" already exists in state \" + currentReplicaInfo.getState()\n                + \" and thus cannot be created.\");\n          }\n          lastFoundReplicaInfo \u003d currentReplicaInfo;\n        }\n      }\n\n      // Hang too long, just bail out. This is not supposed to happen.\n      long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n      if (writerStopMs \u003e writerStopTimeoutMs) {\n        LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n            + writerStopMs + \" miniseconds.\");\n        throw new IOException(\"Unable to stop existing writer for block \" + b\n            + \" after \" + writerStopMs + \" miniseconds.\");\n      }\n\n      // Stop the previous writer\n      ((ReplicaInPipeline) lastFoundReplicaInfo)\n          .stopWriter(writerStopTimeoutMs);\n    } while (true);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7999. FsDatasetImpl#createTemporary sometimes holds the FSDatasetImpl lock for a very long time (sinago via cmccabe)\n",
          "commitDate": "06/04/15 8:56 AM",
          "commitName": "28bebc81db8bb6d1bc2574de7564fe4c595cfe09",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "30/03/15 3:25 PM",
          "commitNameOld": "1a495fbb489c9e9a23b341a52696d10e9e272b04",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 6.73,
          "commitsBetweenForRepo": 50,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,54 @@\n-  public synchronized ReplicaHandler createTemporary(\n+  public ReplicaHandler createTemporary(\n       StorageType storageType, ExtendedBlock b) throws IOException {\n-    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n-    if (replicaInfo !\u003d null) {\n-      if (replicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n-          \u0026\u0026 replicaInfo instanceof ReplicaInPipeline) {\n-        // Stop the previous writer\n-        ((ReplicaInPipeline)replicaInfo)\n-                      .stopWriter(datanode.getDnConf().getXceiverStopTimeout());\n-        invalidate(b.getBlockPoolId(), new Block[]{replicaInfo});\n-      } else {\n-        throw new ReplicaAlreadyExistsException(\"Block \" + b +\n-            \" already exists in state \" + replicaInfo.getState() +\n-            \" and thus cannot be created.\");\n+    long startTimeMs \u003d Time.monotonicNow();\n+    long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n+    ReplicaInfo lastFoundReplicaInfo \u003d null;\n+    do {\n+      synchronized (this) {\n+        ReplicaInfo currentReplicaInfo \u003d\n+            volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n+        if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n+          if (lastFoundReplicaInfo !\u003d null) {\n+            invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n+          }\n+          FsVolumeReference ref \u003d\n+              volumes.getNextVolume(storageType, b.getNumBytes());\n+          FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n+          // create a temporary file to hold block in the designated volume\n+          File f;\n+          try {\n+            f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n+          } catch (IOException e) {\n+            IOUtils.cleanup(null, ref);\n+            throw e;\n+          }\n+          ReplicaInPipeline newReplicaInfo \u003d\n+              new ReplicaInPipeline(b.getBlockId(), b.getGenerationStamp(), v,\n+                  f.getParentFile(), 0);\n+          volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n+          return new ReplicaHandler(newReplicaInfo, ref);\n+        } else {\n+          if (!(currentReplicaInfo.getGenerationStamp() \u003c b\n+              .getGenerationStamp() \u0026\u0026 currentReplicaInfo instanceof ReplicaInPipeline)) {\n+            throw new ReplicaAlreadyExistsException(\"Block \" + b\n+                + \" already exists in state \" + currentReplicaInfo.getState()\n+                + \" and thus cannot be created.\");\n+          }\n+          lastFoundReplicaInfo \u003d currentReplicaInfo;\n+        }\n       }\n-    }\n \n-    FsVolumeReference ref \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n-    FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n-    // create a temporary file to hold block in the designated volume\n-    File f;\n-    try {\n-      f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n-    } catch (IOException e) {\n-      IOUtils.cleanup(null, ref);\n-      throw e;\n-    }\n+      // Hang too long, just bail out. This is not supposed to happen.\n+      long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n+      if (writerStopMs \u003e writerStopTimeoutMs) {\n+        LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n+            + writerStopMs + \" miniseconds.\");\n+        throw new IOException(\"Unable to stop existing writer for block \" + b\n+            + \" after \" + writerStopMs + \" miniseconds.\");\n+      }\n \n-    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n-        b.getGenerationStamp(), v, f.getParentFile(), 0);\n-    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    return new ReplicaHandler(newReplicaInfo, ref);\n+      // Stop the previous writer\n+      ((ReplicaInPipeline) lastFoundReplicaInfo)\n+          .stopWriter(writerStopTimeoutMs);\n+    } while (true);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public ReplicaHandler createTemporary(\n      StorageType storageType, ExtendedBlock b) throws IOException {\n    long startTimeMs \u003d Time.monotonicNow();\n    long writerStopTimeoutMs \u003d datanode.getDnConf().getXceiverStopTimeout();\n    ReplicaInfo lastFoundReplicaInfo \u003d null;\n    do {\n      synchronized (this) {\n        ReplicaInfo currentReplicaInfo \u003d\n            volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n        if (currentReplicaInfo \u003d\u003d lastFoundReplicaInfo) {\n          if (lastFoundReplicaInfo !\u003d null) {\n            invalidate(b.getBlockPoolId(), new Block[] { lastFoundReplicaInfo });\n          }\n          FsVolumeReference ref \u003d\n              volumes.getNextVolume(storageType, b.getNumBytes());\n          FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n          // create a temporary file to hold block in the designated volume\n          File f;\n          try {\n            f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n          } catch (IOException e) {\n            IOUtils.cleanup(null, ref);\n            throw e;\n          }\n          ReplicaInPipeline newReplicaInfo \u003d\n              new ReplicaInPipeline(b.getBlockId(), b.getGenerationStamp(), v,\n                  f.getParentFile(), 0);\n          volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n          return new ReplicaHandler(newReplicaInfo, ref);\n        } else {\n          if (!(currentReplicaInfo.getGenerationStamp() \u003c b\n              .getGenerationStamp() \u0026\u0026 currentReplicaInfo instanceof ReplicaInPipeline)) {\n            throw new ReplicaAlreadyExistsException(\"Block \" + b\n                + \" already exists in state \" + currentReplicaInfo.getState()\n                + \" and thus cannot be created.\");\n          }\n          lastFoundReplicaInfo \u003d currentReplicaInfo;\n        }\n      }\n\n      // Hang too long, just bail out. This is not supposed to happen.\n      long writerStopMs \u003d Time.monotonicNow() - startTimeMs;\n      if (writerStopMs \u003e writerStopTimeoutMs) {\n        LOG.warn(\"Unable to stop existing writer for block \" + b + \" after \" \n            + writerStopMs + \" miniseconds.\");\n        throw new IOException(\"Unable to stop existing writer for block \" + b\n            + \" after \" + writerStopMs + \" miniseconds.\");\n      }\n\n      // Stop the previous writer\n      ((ReplicaInPipeline) lastFoundReplicaInfo)\n          .stopWriter(writerStopTimeoutMs);\n    } while (true);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "b7f4a3156c0f5c600816c469637237ba6c9b330c": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-7496. Fix FsVolume removal race conditions on the DataNode by reference-counting the volume instances (lei via cmccabe)\n",
      "commitDate": "20/01/15 7:05 PM",
      "commitName": "b7f4a3156c0f5c600816c469637237ba6c9b330c",
      "commitAuthor": "Colin Patrick Mccabe",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-7496. Fix FsVolume removal race conditions on the DataNode by reference-counting the volume instances (lei via cmccabe)\n",
          "commitDate": "20/01/15 7:05 PM",
          "commitName": "b7f4a3156c0f5c600816c469637237ba6c9b330c",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "13/01/15 12:24 AM",
          "commitNameOld": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
          "commitAuthorOld": "Konstantin V Shvachko",
          "daysBetweenCommits": 7.78,
          "commitsBetweenForRepo": 49,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,33 @@\n-  public synchronized ReplicaInPipeline createTemporary(StorageType storageType,\n-      ExtendedBlock b) throws IOException {\n+  public synchronized ReplicaHandler createTemporary(\n+      StorageType storageType, ExtendedBlock b) throws IOException {\n     ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n     if (replicaInfo !\u003d null) {\n       if (replicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n           \u0026\u0026 replicaInfo instanceof ReplicaInPipeline) {\n         // Stop the previous writer\n         ((ReplicaInPipeline)replicaInfo)\n                       .stopWriter(datanode.getDnConf().getXceiverStopTimeout());\n         invalidate(b.getBlockPoolId(), new Block[]{replicaInfo});\n       } else {\n         throw new ReplicaAlreadyExistsException(\"Block \" + b +\n             \" already exists in state \" + replicaInfo.getState() +\n             \" and thus cannot be created.\");\n       }\n     }\n-    \n-    FsVolumeImpl v \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n+\n+    FsVolumeReference ref \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n+    FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n     // create a temporary file to hold block in the designated volume\n-    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n+    File f;\n+    try {\n+      f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n+    } catch (IOException e) {\n+      IOUtils.cleanup(null, ref);\n+      throw e;\n+    }\n+\n     ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile(), 0);\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    return newReplicaInfo;\n+    return new ReplicaHandler(newReplicaInfo, ref);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized ReplicaHandler createTemporary(\n      StorageType storageType, ExtendedBlock b) throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      if (replicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n          \u0026\u0026 replicaInfo instanceof ReplicaInPipeline) {\n        // Stop the previous writer\n        ((ReplicaInPipeline)replicaInfo)\n                      .stopWriter(datanode.getDnConf().getXceiverStopTimeout());\n        invalidate(b.getBlockPoolId(), new Block[]{replicaInfo});\n      } else {\n        throw new ReplicaAlreadyExistsException(\"Block \" + b +\n            \" already exists in state \" + replicaInfo.getState() +\n            \" and thus cannot be created.\");\n      }\n    }\n\n    FsVolumeReference ref \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n    FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n    // create a temporary file to hold block in the designated volume\n    File f;\n    try {\n      f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    } catch (IOException e) {\n      IOUtils.cleanup(null, ref);\n      throw e;\n    }\n\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile(), 0);\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    return new ReplicaHandler(newReplicaInfo, ref);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "ReplicaInPipeline",
            "newValue": "ReplicaHandler"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7496. Fix FsVolume removal race conditions on the DataNode by reference-counting the volume instances (lei via cmccabe)\n",
          "commitDate": "20/01/15 7:05 PM",
          "commitName": "b7f4a3156c0f5c600816c469637237ba6c9b330c",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "13/01/15 12:24 AM",
          "commitNameOld": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
          "commitAuthorOld": "Konstantin V Shvachko",
          "daysBetweenCommits": 7.78,
          "commitsBetweenForRepo": 49,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,33 @@\n-  public synchronized ReplicaInPipeline createTemporary(StorageType storageType,\n-      ExtendedBlock b) throws IOException {\n+  public synchronized ReplicaHandler createTemporary(\n+      StorageType storageType, ExtendedBlock b) throws IOException {\n     ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n     if (replicaInfo !\u003d null) {\n       if (replicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n           \u0026\u0026 replicaInfo instanceof ReplicaInPipeline) {\n         // Stop the previous writer\n         ((ReplicaInPipeline)replicaInfo)\n                       .stopWriter(datanode.getDnConf().getXceiverStopTimeout());\n         invalidate(b.getBlockPoolId(), new Block[]{replicaInfo});\n       } else {\n         throw new ReplicaAlreadyExistsException(\"Block \" + b +\n             \" already exists in state \" + replicaInfo.getState() +\n             \" and thus cannot be created.\");\n       }\n     }\n-    \n-    FsVolumeImpl v \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n+\n+    FsVolumeReference ref \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n+    FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n     // create a temporary file to hold block in the designated volume\n-    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n+    File f;\n+    try {\n+      f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n+    } catch (IOException e) {\n+      IOUtils.cleanup(null, ref);\n+      throw e;\n+    }\n+\n     ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile(), 0);\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    return newReplicaInfo;\n+    return new ReplicaHandler(newReplicaInfo, ref);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized ReplicaHandler createTemporary(\n      StorageType storageType, ExtendedBlock b) throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      if (replicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n          \u0026\u0026 replicaInfo instanceof ReplicaInPipeline) {\n        // Stop the previous writer\n        ((ReplicaInPipeline)replicaInfo)\n                      .stopWriter(datanode.getDnConf().getXceiverStopTimeout());\n        invalidate(b.getBlockPoolId(), new Block[]{replicaInfo});\n      } else {\n        throw new ReplicaAlreadyExistsException(\"Block \" + b +\n            \" already exists in state \" + replicaInfo.getState() +\n            \" and thus cannot be created.\");\n      }\n    }\n\n    FsVolumeReference ref \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n    FsVolumeImpl v \u003d (FsVolumeImpl) ref.getVolume();\n    // create a temporary file to hold block in the designated volume\n    File f;\n    try {\n      f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    } catch (IOException e) {\n      IOUtils.cleanup(null, ref);\n      throw e;\n    }\n\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile(), 0);\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    return new ReplicaHandler(newReplicaInfo, ref);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "f02d934fedf00f0ce43d6f3f9b06d89ccc6851a5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6948. DN rejects blocks if it has older UC block. Contributed by\nEric Payne.\n",
      "commitDate": "19/09/14 6:50 AM",
      "commitName": "f02d934fedf00f0ce43d6f3f9b06d89ccc6851a5",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "18/09/14 4:52 PM",
      "commitNameOld": "fe38d2e9b5ac7e13f97cd2d3d2a984ab6bbaaf77",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.58,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,26 @@\n   public synchronized ReplicaInPipeline createTemporary(StorageType storageType,\n       ExtendedBlock b) throws IOException {\n     ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n     if (replicaInfo !\u003d null) {\n-      throw new ReplicaAlreadyExistsException(\"Block \" + b +\n-          \" already exists in state \" + replicaInfo.getState() +\n-          \" and thus cannot be created.\");\n+      if (replicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n+          \u0026\u0026 replicaInfo instanceof ReplicaInPipeline) {\n+        // Stop the previous writer\n+        ((ReplicaInPipeline)replicaInfo)\n+                      .stopWriter(datanode.getDnConf().getXceiverStopTimeout());\n+        invalidate(b.getBlockPoolId(), new Block[]{replicaInfo});\n+      } else {\n+        throw new ReplicaAlreadyExistsException(\"Block \" + b +\n+            \" already exists in state \" + replicaInfo.getState() +\n+            \" and thus cannot be created.\");\n+      }\n     }\n     \n     FsVolumeImpl v \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n     // create a temporary file to hold block in the designated volume\n     File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n     ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile(), 0);\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ReplicaInPipeline createTemporary(StorageType storageType,\n      ExtendedBlock b) throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      if (replicaInfo.getGenerationStamp() \u003c b.getGenerationStamp()\n          \u0026\u0026 replicaInfo instanceof ReplicaInPipeline) {\n        // Stop the previous writer\n        ((ReplicaInPipeline)replicaInfo)\n                      .stopWriter(datanode.getDnConf().getXceiverStopTimeout());\n        invalidate(b.getBlockPoolId(), new Block[]{replicaInfo});\n      } else {\n        throw new ReplicaAlreadyExistsException(\"Block \" + b +\n            \" already exists in state \" + replicaInfo.getState() +\n            \" and thus cannot be created.\");\n      }\n    }\n    \n    FsVolumeImpl v \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n    // create a temporary file to hold block in the designated volume\n    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile(), 0);\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    \n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "d1fa58292e87bc29b4ef1278368c2be938a0afc4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6898. DN must reserve space for a full block when an RBW block is created. (Contributed by Arpit Agarwal)\n",
      "commitDate": "06/09/14 9:04 PM",
      "commitName": "d1fa58292e87bc29b4ef1278368c2be938a0afc4",
      "commitAuthor": "arp",
      "commitDateOld": "29/08/14 1:00 PM",
      "commitNameOld": "7eab2a29a5706ce10912c12fa225ef6b27a82cbe",
      "commitAuthorOld": "Aaron T. Myers",
      "daysBetweenCommits": 8.34,
      "commitsBetweenForRepo": 57,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,18 @@\n   public synchronized ReplicaInPipeline createTemporary(StorageType storageType,\n       ExtendedBlock b) throws IOException {\n     ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n     if (replicaInfo !\u003d null) {\n       throw new ReplicaAlreadyExistsException(\"Block \" + b +\n           \" already exists in state \" + replicaInfo.getState() +\n           \" and thus cannot be created.\");\n     }\n     \n     FsVolumeImpl v \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n     // create a temporary file to hold block in the designated volume\n     File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n     ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n-        b.getGenerationStamp(), v, f.getParentFile());\n+        b.getGenerationStamp(), v, f.getParentFile(), 0);\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ReplicaInPipeline createTemporary(StorageType storageType,\n      ExtendedBlock b) throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      throw new ReplicaAlreadyExistsException(\"Block \" + b +\n          \" already exists in state \" + replicaInfo.getState() +\n          \" and thus cannot be created.\");\n    }\n    \n    FsVolumeImpl v \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n    // create a temporary file to hold block in the designated volume\n    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile(), 0);\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    \n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "25b0e8471ed744578b2d8e3f0debe5477b268e54": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/07/14 12:41 AM",
      "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/07/14 12:41 AM",
          "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "02/06/14 1:27 PM",
          "commitNameOld": "f15ff5e4f4eae0540939b2a5c65d2e91833d6e7a",
          "commitAuthorOld": "Michael Stack",
          "daysBetweenCommits": 49.47,
          "commitsBetweenForRepo": 328,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,18 @@\n-  public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n-      throws IOException {\n+  public synchronized ReplicaInPipeline createTemporary(StorageType storageType,\n+      ExtendedBlock b) throws IOException {\n     ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n     if (replicaInfo !\u003d null) {\n       throw new ReplicaAlreadyExistsException(\"Block \" + b +\n           \" already exists in state \" + replicaInfo.getState() +\n           \" and thus cannot be created.\");\n     }\n     \n-    FsVolumeImpl v \u003d volumes.getNextVolume(b.getNumBytes());\n+    FsVolumeImpl v \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n     // create a temporary file to hold block in the designated volume\n     File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n     ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized ReplicaInPipeline createTemporary(StorageType storageType,\n      ExtendedBlock b) throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      throw new ReplicaAlreadyExistsException(\"Block \" + b +\n          \" already exists in state \" + replicaInfo.getState() +\n          \" and thus cannot be created.\");\n    }\n    \n    FsVolumeImpl v \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n    // create a temporary file to hold block in the designated volume\n    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile());\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    \n    return newReplicaInfo;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[b-ExtendedBlock]",
            "newValue": "[storageType-StorageType, b-ExtendedBlock]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6702. Change DFSClient to pass the StorageType from the namenode to datanodes and change datanode to write block replicas using the specified storage type.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612493 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "22/07/14 12:41 AM",
          "commitName": "25b0e8471ed744578b2d8e3f0debe5477b268e54",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "02/06/14 1:27 PM",
          "commitNameOld": "f15ff5e4f4eae0540939b2a5c65d2e91833d6e7a",
          "commitAuthorOld": "Michael Stack",
          "daysBetweenCommits": 49.47,
          "commitsBetweenForRepo": 328,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,18 @@\n-  public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n-      throws IOException {\n+  public synchronized ReplicaInPipeline createTemporary(StorageType storageType,\n+      ExtendedBlock b) throws IOException {\n     ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n     if (replicaInfo !\u003d null) {\n       throw new ReplicaAlreadyExistsException(\"Block \" + b +\n           \" already exists in state \" + replicaInfo.getState() +\n           \" and thus cannot be created.\");\n     }\n     \n-    FsVolumeImpl v \u003d volumes.getNextVolume(b.getNumBytes());\n+    FsVolumeImpl v \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n     // create a temporary file to hold block in the designated volume\n     File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n     ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized ReplicaInPipeline createTemporary(StorageType storageType,\n      ExtendedBlock b) throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      throw new ReplicaAlreadyExistsException(\"Block \" + b +\n          \" already exists in state \" + replicaInfo.getState() +\n          \" and thus cannot be created.\");\n    }\n    \n    FsVolumeImpl v \u003d volumes.getNextVolume(storageType, b.getNumBytes());\n    // create a temporary file to hold block in the designated volume\n    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile());\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    \n    return newReplicaInfo;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "fba994ffe20d387e8ed875e727fc3d93f7097101": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5648. Get rid of FsDatasetImpl#perVolumeReplicaMap.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1550357 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/12/13 11:01 PM",
      "commitName": "fba994ffe20d387e8ed875e727fc3d93f7097101",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "03/12/13 8:30 AM",
      "commitNameOld": "a1aa1836fb6831c25efe326cdfdc014370cf5957",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 8.6,
      "commitsBetweenForRepo": 57,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,18 @@\n   public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n       throws IOException {\n     ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n     if (replicaInfo !\u003d null) {\n       throw new ReplicaAlreadyExistsException(\"Block \" + b +\n           \" already exists in state \" + replicaInfo.getState() +\n           \" and thus cannot be created.\");\n     }\n     \n     FsVolumeImpl v \u003d volumes.getNextVolume(b.getNumBytes());\n     // create a temporary file to hold block in the designated volume\n     File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n     ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n      throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      throw new ReplicaAlreadyExistsException(\"Block \" + b +\n          \" already exists in state \" + replicaInfo.getState() +\n          \" and thus cannot be created.\");\n    }\n    \n    FsVolumeImpl v \u003d volumes.getNextVolume(b.getNumBytes());\n    // create a temporary file to hold block in the designated volume\n    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile());\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    \n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "f39f8c57344ede533ca4363c98230f3a0c401a76": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5401. Fix NPE in Directory Scanner.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1535158 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/10/13 1:28 PM",
      "commitName": "f39f8c57344ede533ca4363c98230f3a0c401a76",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "22/10/13 6:28 PM",
      "commitNameOld": "01f37e42f050207b7659bf74e2484cf8bdae2d89",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.79,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,19 @@\n   public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n       throws IOException {\n     ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n     if (replicaInfo !\u003d null) {\n       throw new ReplicaAlreadyExistsException(\"Block \" + b +\n           \" already exists in state \" + replicaInfo.getState() +\n           \" and thus cannot be created.\");\n     }\n     \n     FsVolumeImpl v \u003d volumes.getNextVolume(b.getNumBytes());\n     // create a temporary file to hold block in the designated volume\n     File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n     ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n      throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      throw new ReplicaAlreadyExistsException(\"Block \" + b +\n          \" already exists in state \" + replicaInfo.getState() +\n          \" and thus cannot be created.\");\n    }\n    \n    FsVolumeImpl v \u003d volumes.getNextVolume(b.getNumBytes());\n    // create a temporary file to hold block in the designated volume\n    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile());\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), newReplicaInfo);\n    \n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4988. Datanode must support all the volumes as individual storages.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1526969 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/13 9:05 AM",
      "commitName": "46099ce7f1a1d5aab85d9408dc1454fcbe54f7e8",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "10/09/13 11:30 PM",
      "commitNameOld": "b2976af14034c6e2a7e9964535b9f363bfc31150",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 16.4,
      "commitsBetweenForRepo": 92,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,19 @@\n   public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n       throws IOException {\n     ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n     if (replicaInfo !\u003d null) {\n       throw new ReplicaAlreadyExistsException(\"Block \" + b +\n           \" already exists in state \" + replicaInfo.getState() +\n           \" and thus cannot be created.\");\n     }\n     \n     FsVolumeImpl v \u003d volumes.getNextVolume(b.getNumBytes());\n     // create a temporary file to hold block in the designated volume\n     File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n     ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n+    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n      throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      throw new ReplicaAlreadyExistsException(\"Block \" + b +\n          \" already exists in state \" + replicaInfo.getState() +\n          \" and thus cannot be created.\");\n    }\n    \n    FsVolumeImpl v \u003d volumes.getNextVolume(b.getNumBytes());\n    // create a temporary file to hold block in the designated volume\n    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile());\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);\n    \n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "bc13dfb1426944ce45293cb8f444239a7406762c": {
      "type": "Ymultichange(Ymovefromfile,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-3130. Move fsdataset implementation to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308437 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/04/12 10:38 AM",
      "commitName": "bc13dfb1426944ce45293cb8f444239a7406762c",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-3130. Move fsdataset implementation to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308437 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "02/04/12 10:38 AM",
          "commitName": "bc13dfb1426944ce45293cb8f444239a7406762c",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "01/04/12 8:48 PM",
          "commitNameOld": "a4ccb8f504e79802f1b3c69acbcbb00b2343c529",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.58,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,18 @@\n-  public synchronized ReplicaInPipelineInterface createTemporary(ExtendedBlock b)\n+  public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n       throws IOException {\n     ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n     if (replicaInfo !\u003d null) {\n       throw new ReplicaAlreadyExistsException(\"Block \" + b +\n           \" already exists in state \" + replicaInfo.getState() +\n           \" and thus cannot be created.\");\n     }\n     \n-    FSVolume v \u003d volumes.getNextVolume(b.getNumBytes());\n+    FsVolumeImpl v \u003d volumes.getNextVolume(b.getNumBytes());\n     // create a temporary file to hold block in the designated volume\n     File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n     ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n      throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      throw new ReplicaAlreadyExistsException(\"Block \" + b +\n          \" already exists in state \" + replicaInfo.getState() +\n          \" and thus cannot be created.\");\n    }\n    \n    FsVolumeImpl v \u003d volumes.getNextVolume(b.getNumBytes());\n    // create a temporary file to hold block in the designated volume\n    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile());\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    \n    return newReplicaInfo;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
            "oldMethodName": "createTemporary",
            "newMethodName": "createTemporary"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-3130. Move fsdataset implementation to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308437 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "02/04/12 10:38 AM",
          "commitName": "bc13dfb1426944ce45293cb8f444239a7406762c",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "01/04/12 8:48 PM",
          "commitNameOld": "a4ccb8f504e79802f1b3c69acbcbb00b2343c529",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.58,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,18 @@\n-  public synchronized ReplicaInPipelineInterface createTemporary(ExtendedBlock b)\n+  public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n       throws IOException {\n     ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n     if (replicaInfo !\u003d null) {\n       throw new ReplicaAlreadyExistsException(\"Block \" + b +\n           \" already exists in state \" + replicaInfo.getState() +\n           \" and thus cannot be created.\");\n     }\n     \n-    FSVolume v \u003d volumes.getNextVolume(b.getNumBytes());\n+    FsVolumeImpl v \u003d volumes.getNextVolume(b.getNumBytes());\n     // create a temporary file to hold block in the designated volume\n     File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n     ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n      throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      throw new ReplicaAlreadyExistsException(\"Block \" + b +\n          \" already exists in state \" + replicaInfo.getState() +\n          \" and thus cannot be created.\");\n    }\n    \n    FsVolumeImpl v \u003d volumes.getNextVolume(b.getNumBytes());\n    // create a temporary file to hold block in the designated volume\n    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile());\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    \n    return newReplicaInfo;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "ReplicaInPipelineInterface",
            "newValue": "ReplicaInPipeline"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3130. Move fsdataset implementation to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308437 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "02/04/12 10:38 AM",
          "commitName": "bc13dfb1426944ce45293cb8f444239a7406762c",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "01/04/12 8:48 PM",
          "commitNameOld": "a4ccb8f504e79802f1b3c69acbcbb00b2343c529",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.58,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,18 @@\n-  public synchronized ReplicaInPipelineInterface createTemporary(ExtendedBlock b)\n+  public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n       throws IOException {\n     ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n     if (replicaInfo !\u003d null) {\n       throw new ReplicaAlreadyExistsException(\"Block \" + b +\n           \" already exists in state \" + replicaInfo.getState() +\n           \" and thus cannot be created.\");\n     }\n     \n-    FSVolume v \u003d volumes.getNextVolume(b.getNumBytes());\n+    FsVolumeImpl v \u003d volumes.getNextVolume(b.getNumBytes());\n     // create a temporary file to hold block in the designated volume\n     File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n     ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n      throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      throw new ReplicaAlreadyExistsException(\"Block \" + b +\n          \" already exists in state \" + replicaInfo.getState() +\n          \" and thus cannot be created.\");\n    }\n    \n    FsVolumeImpl v \u003d volumes.getNextVolume(b.getNumBytes());\n    // create a temporary file to hold block in the designated volume\n    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile());\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    \n    return newReplicaInfo;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized ReplicaInPipelineInterface createTemporary(ExtendedBlock b)\n      throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      throw new ReplicaAlreadyExistsException(\"Block \" + b +\n          \" already exists in state \" + replicaInfo.getState() +\n          \" and thus cannot be created.\");\n    }\n    \n    FSVolume v \u003d volumes.getNextVolume(b.getNumBytes());\n    // create a temporary file to hold block in the designated volume\n    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile());\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    \n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized ReplicaInPipelineInterface createTemporary(ExtendedBlock b)\n      throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      throw new ReplicaAlreadyExistsException(\"Block \" + b +\n          \" already exists in state \" + replicaInfo.getState() +\n          \" and thus cannot be created.\");\n    }\n    \n    FSVolume v \u003d volumes.getNextVolume(b.getNumBytes());\n    // create a temporary file to hold block in the designated volume\n    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile());\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    \n    return newReplicaInfo;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,18 @@\n+  public synchronized ReplicaInPipelineInterface createTemporary(ExtendedBlock b)\n+      throws IOException {\n+    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n+    if (replicaInfo !\u003d null) {\n+      throw new ReplicaAlreadyExistsException(\"Block \" + b +\n+          \" already exists in state \" + replicaInfo.getState() +\n+          \" and thus cannot be created.\");\n+    }\n+    \n+    FSVolume v \u003d volumes.getNextVolume(b.getNumBytes());\n+    // create a temporary file to hold block in the designated volume\n+    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n+    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n+        b.getGenerationStamp(), v, f.getParentFile());\n+    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n+    \n+    return newReplicaInfo;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ReplicaInPipelineInterface createTemporary(ExtendedBlock b)\n      throws IOException {\n    ReplicaInfo replicaInfo \u003d volumeMap.get(b.getBlockPoolId(), b.getBlockId());\n    if (replicaInfo !\u003d null) {\n      throw new ReplicaAlreadyExistsException(\"Block \" + b +\n          \" already exists in state \" + replicaInfo.getState() +\n          \" and thus cannot be created.\");\n    }\n    \n    FSVolume v \u003d volumes.getNextVolume(b.getNumBytes());\n    // create a temporary file to hold block in the designated volume\n    File f \u003d v.createTmpFile(b.getBlockPoolId(), b.getLocalBlock());\n    ReplicaInPipeline newReplicaInfo \u003d new ReplicaInPipeline(b.getBlockId(), \n        b.getGenerationStamp(), v, f.getParentFile());\n    volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n    \n    return newReplicaInfo;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
    }
  }
}