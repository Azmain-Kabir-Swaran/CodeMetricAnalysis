{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "UncompressedSplitLineReader.java",
  "functionName": "fillBuffer",
  "functionId": "fillBuffer___in-InputStream__buffer-byte[]__inDelimiter-boolean",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/UncompressedSplitLineReader.java",
  "functionStartLine": 52,
  "functionEndLine": 81,
  "numCommitsSeen": 5,
  "timeTaken": 1439,
  "changeHistory": [
    "0b7b8a377611b2a3041a2995504a437c36dfa6e6",
    "c6f2d761d5430eac6b9f07f137a7028de4e0660c",
    "58d1a02b8d66b1d2a6ac2158be32bd35ad2e69bd",
    "077250d8d7b4b757543a39a6ce8bb6e3be356c6f"
  ],
  "changeHistoryShort": {
    "0b7b8a377611b2a3041a2995504a437c36dfa6e6": "Ybodychange",
    "c6f2d761d5430eac6b9f07f137a7028de4e0660c": "Ybodychange",
    "58d1a02b8d66b1d2a6ac2158be32bd35ad2e69bd": "Ybodychange",
    "077250d8d7b4b757543a39a6ce8bb6e3be356c6f": "Yintroduced"
  },
  "changeHistoryDetails": {
    "0b7b8a377611b2a3041a2995504a437c36dfa6e6": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6741. Refactor UncompressedSplitLineReader.fillBuffer(). Contributed by Daniel Templeton.\n",
      "commitDate": "10/06/16 3:15 AM",
      "commitName": "0b7b8a377611b2a3041a2995504a437c36dfa6e6",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "22/02/16 11:35 PM",
      "commitNameOld": "c6f2d761d5430eac6b9f07f137a7028de4e0660c",
      "commitAuthorOld": "Varun Vasudev",
      "daysBetweenCommits": 108.11,
      "commitsBetweenForRepo": 693,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,30 @@\n   protected int fillBuffer(InputStream in, byte[] buffer, boolean inDelimiter)\n       throws IOException {\n     int maxBytesToRead \u003d buffer.length;\n     if (totalBytesRead \u003c splitLength) {\n-      long leftBytesForSplit \u003d splitLength - totalBytesRead;\n-      // check if leftBytesForSplit exceed Integer.MAX_VALUE\n-      if (leftBytesForSplit \u003c\u003d Integer.MAX_VALUE) {\n-        maxBytesToRead \u003d Math.min(maxBytesToRead, (int)leftBytesForSplit);\n+      long bytesLeftInSplit \u003d splitLength - totalBytesRead;\n+\n+      if (bytesLeftInSplit \u003c maxBytesToRead) {\n+        maxBytesToRead \u003d (int)bytesLeftInSplit;\n       }\n     }\n     int bytesRead \u003d in.read(buffer, 0, maxBytesToRead);\n \n     // If the split ended in the middle of a record delimiter then we need\n     // to read one additional record, as the consumer of the next split will\n     // not recognize the partial delimiter as a record.\n     // However if using the default delimiter and the next character is a\n     // linefeed then next split will treat it as a delimiter all by itself\n     // and the additional record read should not be performed.\n     if (totalBytesRead \u003d\u003d splitLength \u0026\u0026 inDelimiter \u0026\u0026 bytesRead \u003e 0) {\n       if (usingCRLF) {\n         needAdditionalRecord \u003d (buffer[0] !\u003d \u0027\\n\u0027);\n       } else {\n         needAdditionalRecord \u003d true;\n       }\n     }\n     if (bytesRead \u003e 0) {\n       totalBytesRead +\u003d bytesRead;\n     }\n     return bytesRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected int fillBuffer(InputStream in, byte[] buffer, boolean inDelimiter)\n      throws IOException {\n    int maxBytesToRead \u003d buffer.length;\n    if (totalBytesRead \u003c splitLength) {\n      long bytesLeftInSplit \u003d splitLength - totalBytesRead;\n\n      if (bytesLeftInSplit \u003c maxBytesToRead) {\n        maxBytesToRead \u003d (int)bytesLeftInSplit;\n      }\n    }\n    int bytesRead \u003d in.read(buffer, 0, maxBytesToRead);\n\n    // If the split ended in the middle of a record delimiter then we need\n    // to read one additional record, as the consumer of the next split will\n    // not recognize the partial delimiter as a record.\n    // However if using the default delimiter and the next character is a\n    // linefeed then next split will treat it as a delimiter all by itself\n    // and the additional record read should not be performed.\n    if (totalBytesRead \u003d\u003d splitLength \u0026\u0026 inDelimiter \u0026\u0026 bytesRead \u003e 0) {\n      if (usingCRLF) {\n        needAdditionalRecord \u003d (buffer[0] !\u003d \u0027\\n\u0027);\n      } else {\n        needAdditionalRecord \u003d true;\n      }\n    }\n    if (bytesRead \u003e 0) {\n      totalBytesRead +\u003d bytesRead;\n    }\n    return bytesRead;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/UncompressedSplitLineReader.java",
      "extendedDetails": {}
    },
    "c6f2d761d5430eac6b9f07f137a7028de4e0660c": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6635. Unsafe long to int conversion in UncompressedSplitLineReader and IndexOutOfBoundsException. Contributed by Junping Du.\n",
      "commitDate": "22/02/16 11:35 PM",
      "commitName": "c6f2d761d5430eac6b9f07f137a7028de4e0660c",
      "commitAuthor": "Varun Vasudev",
      "commitDateOld": "25/11/15 5:03 PM",
      "commitNameOld": "7fd00b3db4b7d73afd41276ba9a06ec06a0e1762",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 89.27,
      "commitsBetweenForRepo": 533,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,30 @@\n   protected int fillBuffer(InputStream in, byte[] buffer, boolean inDelimiter)\n       throws IOException {\n     int maxBytesToRead \u003d buffer.length;\n     if (totalBytesRead \u003c splitLength) {\n-      maxBytesToRead \u003d Math.min(maxBytesToRead,\n-                                (int)(splitLength - totalBytesRead));\n+      long leftBytesForSplit \u003d splitLength - totalBytesRead;\n+      // check if leftBytesForSplit exceed Integer.MAX_VALUE\n+      if (leftBytesForSplit \u003c\u003d Integer.MAX_VALUE) {\n+        maxBytesToRead \u003d Math.min(maxBytesToRead, (int)leftBytesForSplit);\n+      }\n     }\n     int bytesRead \u003d in.read(buffer, 0, maxBytesToRead);\n \n     // If the split ended in the middle of a record delimiter then we need\n     // to read one additional record, as the consumer of the next split will\n     // not recognize the partial delimiter as a record.\n     // However if using the default delimiter and the next character is a\n     // linefeed then next split will treat it as a delimiter all by itself\n     // and the additional record read should not be performed.\n     if (totalBytesRead \u003d\u003d splitLength \u0026\u0026 inDelimiter \u0026\u0026 bytesRead \u003e 0) {\n       if (usingCRLF) {\n         needAdditionalRecord \u003d (buffer[0] !\u003d \u0027\\n\u0027);\n       } else {\n         needAdditionalRecord \u003d true;\n       }\n     }\n     if (bytesRead \u003e 0) {\n       totalBytesRead +\u003d bytesRead;\n     }\n     return bytesRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected int fillBuffer(InputStream in, byte[] buffer, boolean inDelimiter)\n      throws IOException {\n    int maxBytesToRead \u003d buffer.length;\n    if (totalBytesRead \u003c splitLength) {\n      long leftBytesForSplit \u003d splitLength - totalBytesRead;\n      // check if leftBytesForSplit exceed Integer.MAX_VALUE\n      if (leftBytesForSplit \u003c\u003d Integer.MAX_VALUE) {\n        maxBytesToRead \u003d Math.min(maxBytesToRead, (int)leftBytesForSplit);\n      }\n    }\n    int bytesRead \u003d in.read(buffer, 0, maxBytesToRead);\n\n    // If the split ended in the middle of a record delimiter then we need\n    // to read one additional record, as the consumer of the next split will\n    // not recognize the partial delimiter as a record.\n    // However if using the default delimiter and the next character is a\n    // linefeed then next split will treat it as a delimiter all by itself\n    // and the additional record read should not be performed.\n    if (totalBytesRead \u003d\u003d splitLength \u0026\u0026 inDelimiter \u0026\u0026 bytesRead \u003e 0) {\n      if (usingCRLF) {\n        needAdditionalRecord \u003d (buffer[0] !\u003d \u0027\\n\u0027);\n      } else {\n        needAdditionalRecord \u003d true;\n      }\n    }\n    if (bytesRead \u003e 0) {\n      totalBytesRead +\u003d bytesRead;\n    }\n    return bytesRead;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/UncompressedSplitLineReader.java",
      "extendedDetails": {}
    },
    "58d1a02b8d66b1d2a6ac2158be32bd35ad2e69bd": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6481. LineRecordReader may give incomplete record and wrong position/key information for uncompressed input sometimes. Contributed by Zhihai Xu\n",
      "commitDate": "17/09/15 7:30 AM",
      "commitName": "58d1a02b8d66b1d2a6ac2158be32bd35ad2e69bd",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "22/06/15 2:59 PM",
      "commitNameOld": "077250d8d7b4b757543a39a6ce8bb6e3be356c6f",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 86.69,
      "commitsBetweenForRepo": 511,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,27 @@\n   protected int fillBuffer(InputStream in, byte[] buffer, boolean inDelimiter)\n       throws IOException {\n     int maxBytesToRead \u003d buffer.length;\n     if (totalBytesRead \u003c splitLength) {\n       maxBytesToRead \u003d Math.min(maxBytesToRead,\n                                 (int)(splitLength - totalBytesRead));\n     }\n     int bytesRead \u003d in.read(buffer, 0, maxBytesToRead);\n-    lastBytesRead \u003d bytesRead;\n \n     // If the split ended in the middle of a record delimiter then we need\n     // to read one additional record, as the consumer of the next split will\n     // not recognize the partial delimiter as a record.\n     // However if using the default delimiter and the next character is a\n     // linefeed then next split will treat it as a delimiter all by itself\n     // and the additional record read should not be performed.\n     if (totalBytesRead \u003d\u003d splitLength \u0026\u0026 inDelimiter \u0026\u0026 bytesRead \u003e 0) {\n       if (usingCRLF) {\n         needAdditionalRecord \u003d (buffer[0] !\u003d \u0027\\n\u0027);\n       } else {\n         needAdditionalRecord \u003d true;\n       }\n     }\n     if (bytesRead \u003e 0) {\n       totalBytesRead +\u003d bytesRead;\n     }\n     return bytesRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected int fillBuffer(InputStream in, byte[] buffer, boolean inDelimiter)\n      throws IOException {\n    int maxBytesToRead \u003d buffer.length;\n    if (totalBytesRead \u003c splitLength) {\n      maxBytesToRead \u003d Math.min(maxBytesToRead,\n                                (int)(splitLength - totalBytesRead));\n    }\n    int bytesRead \u003d in.read(buffer, 0, maxBytesToRead);\n\n    // If the split ended in the middle of a record delimiter then we need\n    // to read one additional record, as the consumer of the next split will\n    // not recognize the partial delimiter as a record.\n    // However if using the default delimiter and the next character is a\n    // linefeed then next split will treat it as a delimiter all by itself\n    // and the additional record read should not be performed.\n    if (totalBytesRead \u003d\u003d splitLength \u0026\u0026 inDelimiter \u0026\u0026 bytesRead \u003e 0) {\n      if (usingCRLF) {\n        needAdditionalRecord \u003d (buffer[0] !\u003d \u0027\\n\u0027);\n      } else {\n        needAdditionalRecord \u003d true;\n      }\n    }\n    if (bytesRead \u003e 0) {\n      totalBytesRead +\u003d bytesRead;\n    }\n    return bytesRead;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/UncompressedSplitLineReader.java",
      "extendedDetails": {}
    },
    "077250d8d7b4b757543a39a6ce8bb6e3be356c6f": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-5948. org.apache.hadoop.mapred.LineRecordReader does not handle multibyte record delimiters well. Contributed by Vinayakumar B, Rushabh Shah, and Akira AJISAKA\n",
      "commitDate": "22/06/15 2:59 PM",
      "commitName": "077250d8d7b4b757543a39a6ce8bb6e3be356c6f",
      "commitAuthor": "Jason Lowe",
      "diff": "@@ -0,0 +1,28 @@\n+  protected int fillBuffer(InputStream in, byte[] buffer, boolean inDelimiter)\n+      throws IOException {\n+    int maxBytesToRead \u003d buffer.length;\n+    if (totalBytesRead \u003c splitLength) {\n+      maxBytesToRead \u003d Math.min(maxBytesToRead,\n+                                (int)(splitLength - totalBytesRead));\n+    }\n+    int bytesRead \u003d in.read(buffer, 0, maxBytesToRead);\n+    lastBytesRead \u003d bytesRead;\n+\n+    // If the split ended in the middle of a record delimiter then we need\n+    // to read one additional record, as the consumer of the next split will\n+    // not recognize the partial delimiter as a record.\n+    // However if using the default delimiter and the next character is a\n+    // linefeed then next split will treat it as a delimiter all by itself\n+    // and the additional record read should not be performed.\n+    if (totalBytesRead \u003d\u003d splitLength \u0026\u0026 inDelimiter \u0026\u0026 bytesRead \u003e 0) {\n+      if (usingCRLF) {\n+        needAdditionalRecord \u003d (buffer[0] !\u003d \u0027\\n\u0027);\n+      } else {\n+        needAdditionalRecord \u003d true;\n+      }\n+    }\n+    if (bytesRead \u003e 0) {\n+      totalBytesRead +\u003d bytesRead;\n+    }\n+    return bytesRead;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected int fillBuffer(InputStream in, byte[] buffer, boolean inDelimiter)\n      throws IOException {\n    int maxBytesToRead \u003d buffer.length;\n    if (totalBytesRead \u003c splitLength) {\n      maxBytesToRead \u003d Math.min(maxBytesToRead,\n                                (int)(splitLength - totalBytesRead));\n    }\n    int bytesRead \u003d in.read(buffer, 0, maxBytesToRead);\n    lastBytesRead \u003d bytesRead;\n\n    // If the split ended in the middle of a record delimiter then we need\n    // to read one additional record, as the consumer of the next split will\n    // not recognize the partial delimiter as a record.\n    // However if using the default delimiter and the next character is a\n    // linefeed then next split will treat it as a delimiter all by itself\n    // and the additional record read should not be performed.\n    if (totalBytesRead \u003d\u003d splitLength \u0026\u0026 inDelimiter \u0026\u0026 bytesRead \u003e 0) {\n      if (usingCRLF) {\n        needAdditionalRecord \u003d (buffer[0] !\u003d \u0027\\n\u0027);\n      } else {\n        needAdditionalRecord \u003d true;\n      }\n    }\n    if (bytesRead \u003e 0) {\n      totalBytesRead +\u003d bytesRead;\n    }\n    return bytesRead;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/UncompressedSplitLineReader.java"
    }
  }
}