{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockReaderLocal.java",
  "functionName": "drainDataBuf",
  "functionId": "drainDataBuf___buf-ByteBuffer",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java",
  "functionStartLine": 319,
  "functionEndLine": 333,
  "numCommitsSeen": 47,
  "timeTaken": 2348,
  "changeHistory": [
    "f308561f1d885491b88db73ac63003202056d661",
    "e2c9b288b223b9fd82dc12018936e13128413492",
    "f93d99990a9a02ce693cd74466c2e5f127c1f560",
    "037a89abc5cc5ea6b983b21c568a50bc729aa194",
    "124e507674c0d396f8494585e64226957199097b"
  ],
  "changeHistoryShort": {
    "f308561f1d885491b88db73ac63003202056d661": "Yfilerename",
    "e2c9b288b223b9fd82dc12018936e13128413492": "Yfilerename",
    "f93d99990a9a02ce693cd74466c2e5f127c1f560": "Yexceptionschange",
    "037a89abc5cc5ea6b983b21c568a50bc729aa194": "Ybodychange",
    "124e507674c0d396f8494585e64226957199097b": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f308561f1d885491b88db73ac63003202056d661": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8057 Move BlockReader implementation to the client implementation package.  Contributed by Takanobu Asanuma\n",
      "commitDate": "25/04/16 12:01 PM",
      "commitName": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "25/04/16 9:38 AM",
      "commitNameOld": "10f0f7851a3255caab775777e8fb6c2781d97062",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private synchronized int drainDataBuf(ByteBuffer buf) {\n    if (dataBuf \u003d\u003d null) return -1;\n    int oldLimit \u003d dataBuf.limit();\n    int nRead \u003d Math.min(dataBuf.remaining(), buf.remaining());\n    if (nRead \u003d\u003d 0) {\n      return (dataBuf.remaining() \u003d\u003d 0) ? -1 : 0;\n    }\n    try {\n      dataBuf.limit(dataBuf.position() + nRead);\n      buf.put(dataBuf);\n    } finally {\n      dataBuf.limit(oldLimit);\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocal.java"
      }
    },
    "e2c9b288b223b9fd82dc12018936e13128413492": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "28/08/15 2:38 PM",
      "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/08/15 2:21 PM",
      "commitNameOld": "b94b56806d3d6e04984e229b479f7ac15b62bbfa",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private synchronized int drainDataBuf(ByteBuffer buf) {\n    if (dataBuf \u003d\u003d null) return -1;\n    int oldLimit \u003d dataBuf.limit();\n    int nRead \u003d Math.min(dataBuf.remaining(), buf.remaining());\n    if (nRead \u003d\u003d 0) {\n      return (dataBuf.remaining() \u003d\u003d 0) ? -1 : 0;\n    }\n    try {\n      dataBuf.limit(dataBuf.position() + nRead);\n      buf.put(dataBuf);\n    } finally {\n      dataBuf.limit(oldLimit);\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java"
      }
    },
    "f93d99990a9a02ce693cd74466c2e5f127c1f560": {
      "type": "Yexceptionschange",
      "commitMessage": "HDFS-6167. Relocate the non-public API classes in the hdfs.client package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1583878 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/14 10:09 PM",
      "commitName": "f93d99990a9a02ce693cd74466c2e5f127c1f560",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "24/03/14 4:32 PM",
      "commitNameOld": "c2ef7e239eb0e81cf8a3e971378e9e696202de67",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 8.23,
      "commitsBetweenForRepo": 58,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,15 @@\n-  private synchronized int drainDataBuf(ByteBuffer buf)\n-      throws IOException {\n+  private synchronized int drainDataBuf(ByteBuffer buf) {\n     if (dataBuf \u003d\u003d null) return -1;\n     int oldLimit \u003d dataBuf.limit();\n     int nRead \u003d Math.min(dataBuf.remaining(), buf.remaining());\n     if (nRead \u003d\u003d 0) {\n       return (dataBuf.remaining() \u003d\u003d 0) ? -1 : 0;\n     }\n     try {\n       dataBuf.limit(dataBuf.position() + nRead);\n       buf.put(dataBuf);\n     } finally {\n       dataBuf.limit(oldLimit);\n     }\n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized int drainDataBuf(ByteBuffer buf) {\n    if (dataBuf \u003d\u003d null) return -1;\n    int oldLimit \u003d dataBuf.limit();\n    int nRead \u003d Math.min(dataBuf.remaining(), buf.remaining());\n    if (nRead \u003d\u003d 0) {\n      return (dataBuf.remaining() \u003d\u003d 0) ? -1 : 0;\n    }\n    try {\n      dataBuf.limit(dataBuf.position() + nRead);\n      buf.put(dataBuf);\n    } finally {\n      dataBuf.limit(oldLimit);\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {
        "oldValue": "[IOException]",
        "newValue": "[]"
      }
    },
    "037a89abc5cc5ea6b983b21c568a50bc729aa194": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5762. BlockReaderLocal does not return -1 on EOF when doing zero-length reads (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1558526 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/01/14 11:17 AM",
      "commitName": "037a89abc5cc5ea6b983b21c568a50bc729aa194",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "17/12/13 12:57 PM",
      "commitNameOld": "124e507674c0d396f8494585e64226957199097b",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 28.93,
      "commitsBetweenForRepo": 117,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,16 @@\n   private synchronized int drainDataBuf(ByteBuffer buf)\n       throws IOException {\n-    if (dataBuf \u003d\u003d null) return 0;\n+    if (dataBuf \u003d\u003d null) return -1;\n     int oldLimit \u003d dataBuf.limit();\n     int nRead \u003d Math.min(dataBuf.remaining(), buf.remaining());\n-    if (nRead \u003d\u003d 0) return 0;\n+    if (nRead \u003d\u003d 0) {\n+      return (dataBuf.remaining() \u003d\u003d 0) ? -1 : 0;\n+    }\n     try {\n       dataBuf.limit(dataBuf.position() + nRead);\n       buf.put(dataBuf);\n     } finally {\n       dataBuf.limit(oldLimit);\n     }\n     return nRead;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized int drainDataBuf(ByteBuffer buf)\n      throws IOException {\n    if (dataBuf \u003d\u003d null) return -1;\n    int oldLimit \u003d dataBuf.limit();\n    int nRead \u003d Math.min(dataBuf.remaining(), buf.remaining());\n    if (nRead \u003d\u003d 0) {\n      return (dataBuf.remaining() \u003d\u003d 0) ? -1 : 0;\n    }\n    try {\n      dataBuf.limit(dataBuf.position() + nRead);\n      buf.put(dataBuf);\n    } finally {\n      dataBuf.limit(oldLimit);\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java",
      "extendedDetails": {}
    },
    "124e507674c0d396f8494585e64226957199097b": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5634. Allow BlockReaderLocal to switch between checksumming and not (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551701 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 12:57 PM",
      "commitName": "124e507674c0d396f8494585e64226957199097b",
      "commitAuthor": "Colin McCabe",
      "diff": "@@ -0,0 +1,14 @@\n+  private synchronized int drainDataBuf(ByteBuffer buf)\n+      throws IOException {\n+    if (dataBuf \u003d\u003d null) return 0;\n+    int oldLimit \u003d dataBuf.limit();\n+    int nRead \u003d Math.min(dataBuf.remaining(), buf.remaining());\n+    if (nRead \u003d\u003d 0) return 0;\n+    try {\n+      dataBuf.limit(dataBuf.position() + nRead);\n+      buf.put(dataBuf);\n+    } finally {\n+      dataBuf.limit(oldLimit);\n+    }\n+    return nRead;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized int drainDataBuf(ByteBuffer buf)\n      throws IOException {\n    if (dataBuf \u003d\u003d null) return 0;\n    int oldLimit \u003d dataBuf.limit();\n    int nRead \u003d Math.min(dataBuf.remaining(), buf.remaining());\n    if (nRead \u003d\u003d 0) return 0;\n    try {\n      dataBuf.limit(dataBuf.position() + nRead);\n      buf.put(dataBuf);\n    } finally {\n      dataBuf.limit(oldLimit);\n    }\n    return nRead;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java"
    }
  }
}