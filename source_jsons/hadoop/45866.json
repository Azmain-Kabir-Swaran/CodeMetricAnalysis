{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LogToolUtils.java",
  "functionName": "outputContainerLog",
  "functionId": "outputContainerLog___containerId-String__nodeId-String__fileName-String__fileLength-long__outputSize-long__lastModifiedTime-String__fis-InputStream__os-OutputStream__buf-byte[]__logType-ContainerLogAggregationType",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/LogToolUtils.java",
  "functionStartLine": 88,
  "functionEndLine": 131,
  "numCommitsSeen": 12,
  "timeTaken": 2772,
  "changeHistory": [
    "bec0864394fbf30d7979bb7359dc0b5403731c0c",
    "95372657fc25c02399b01793833021ccf88dada2",
    "ce2d5bfa5f84e7e563980796549b56ef1e4bbf1e",
    "327c9980aafce52cc02d2b8885fc4e9f628ab23c"
  ],
  "changeHistoryShort": {
    "bec0864394fbf30d7979bb7359dc0b5403731c0c": "Ybodychange",
    "95372657fc25c02399b01793833021ccf88dada2": "Ybodychange",
    "ce2d5bfa5f84e7e563980796549b56ef1e4bbf1e": "Ymultichange(Yparameterchange,Ybodychange)",
    "327c9980aafce52cc02d2b8885fc4e9f628ab23c": "Yintroduced"
  },
  "changeHistoryDetails": {
    "bec0864394fbf30d7979bb7359dc0b5403731c0c": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9808. Zero length files in container log output haven\u0027t got a header. Contributed by Adam Antal\n",
      "commitDate": "25/09/19 1:28 AM",
      "commitName": "bec0864394fbf30d7979bb7359dc0b5403731c0c",
      "commitAuthor": "Szilard Nemeth",
      "commitDateOld": "25/02/19 11:45 AM",
      "commitNameOld": "95372657fc25c02399b01793833021ccf88dada2",
      "commitAuthorOld": "Giovanni Matteo Fumarola",
      "daysBetweenCommits": 211.53,
      "commitsBetweenForRepo": 1651,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,44 @@\n   public static void outputContainerLog(String containerId, String nodeId,\n       String fileName, long fileLength, long outputSize,\n       String lastModifiedTime, InputStream fis, OutputStream os,\n       byte[] buf, ContainerLogAggregationType logType) throws IOException {\n     long toSkip \u003d 0;\n     long totalBytesToRead \u003d fileLength;\n     long skipAfterRead \u003d 0;\n     if (outputSize \u003c 0) {\n       long absBytes \u003d Math.abs(outputSize);\n       if (absBytes \u003c fileLength) {\n         toSkip \u003d fileLength - absBytes;\n         totalBytesToRead \u003d absBytes;\n       }\n       org.apache.hadoop.io.IOUtils.skipFully(fis, toSkip);\n     } else {\n       if (outputSize \u003c fileLength) {\n         totalBytesToRead \u003d outputSize;\n         skipAfterRead \u003d fileLength - outputSize;\n       }\n     }\n \n     long curRead \u003d 0;\n     long pendingRead \u003d totalBytesToRead - curRead;\n     int toRead \u003d pendingRead \u003e buf.length ? buf.length\n         : (int) pendingRead;\n     int len \u003d fis.read(buf, 0, toRead);\n     boolean keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n-    if (keepGoing) {\n-      StringBuilder sb \u003d new StringBuilder();\n-      String containerStr \u003d String.format(\n-          LogToolUtils.CONTAINER_ON_NODE_PATTERN,\n-          containerId, nodeId);\n-      sb.append(containerStr + \"\\n\")\n-          .append(\"LogAggregationType: \" + logType + \"\\n\")\n-          .append(StringUtils.repeat(\"\u003d\", containerStr.length()) + \"\\n\")\n-          .append(\"LogType:\" + fileName + \"\\n\")\n-          .append(\"LogLastModifiedTime:\" + lastModifiedTime + \"\\n\")\n-          .append(\"LogLength:\" + Long.toString(fileLength) + \"\\n\")\n-          .append(\"LogContents:\\n\");\n-      byte[] b \u003d sb.toString().getBytes(\n-          Charset.forName(\"UTF-8\"));\n-      os.write(b, 0, b.length);\n-    }\n+\n+    byte[] b \u003d formatContainerLogHeader(containerId, nodeId, logType, fileName,\n+        lastModifiedTime, fileLength);\n+    os.write(b, 0, b.length);\n     while (keepGoing) {\n       os.write(buf, 0, len);\n       curRead +\u003d len;\n \n       pendingRead \u003d totalBytesToRead - curRead;\n       toRead \u003d pendingRead \u003e buf.length ? buf.length\n           : (int) pendingRead;\n       len \u003d fis.read(buf, 0, toRead);\n       keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n     }\n     org.apache.hadoop.io.IOUtils.skipFully(fis, skipAfterRead);\n     os.flush();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static void outputContainerLog(String containerId, String nodeId,\n      String fileName, long fileLength, long outputSize,\n      String lastModifiedTime, InputStream fis, OutputStream os,\n      byte[] buf, ContainerLogAggregationType logType) throws IOException {\n    long toSkip \u003d 0;\n    long totalBytesToRead \u003d fileLength;\n    long skipAfterRead \u003d 0;\n    if (outputSize \u003c 0) {\n      long absBytes \u003d Math.abs(outputSize);\n      if (absBytes \u003c fileLength) {\n        toSkip \u003d fileLength - absBytes;\n        totalBytesToRead \u003d absBytes;\n      }\n      org.apache.hadoop.io.IOUtils.skipFully(fis, toSkip);\n    } else {\n      if (outputSize \u003c fileLength) {\n        totalBytesToRead \u003d outputSize;\n        skipAfterRead \u003d fileLength - outputSize;\n      }\n    }\n\n    long curRead \u003d 0;\n    long pendingRead \u003d totalBytesToRead - curRead;\n    int toRead \u003d pendingRead \u003e buf.length ? buf.length\n        : (int) pendingRead;\n    int len \u003d fis.read(buf, 0, toRead);\n    boolean keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n\n    byte[] b \u003d formatContainerLogHeader(containerId, nodeId, logType, fileName,\n        lastModifiedTime, fileLength);\n    os.write(b, 0, b.length);\n    while (keepGoing) {\n      os.write(buf, 0, len);\n      curRead +\u003d len;\n\n      pendingRead \u003d totalBytesToRead - curRead;\n      toRead \u003d pendingRead \u003e buf.length ? buf.length\n          : (int) pendingRead;\n      len \u003d fis.read(buf, 0, toRead);\n      keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n    }\n    org.apache.hadoop.io.IOUtils.skipFully(fis, skipAfterRead);\n    os.flush();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/LogToolUtils.java",
      "extendedDetails": {}
    },
    "95372657fc25c02399b01793833021ccf88dada2": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9287. Consecutive StringBuilder append should be reuse. Contributed by Ayush Saxena.\n",
      "commitDate": "25/02/19 11:45 AM",
      "commitName": "95372657fc25c02399b01793833021ccf88dada2",
      "commitAuthor": "Giovanni Matteo Fumarola",
      "commitDateOld": "12/06/18 8:35 AM",
      "commitNameOld": "652bcbb3e4950758e61ce123ecc1798ae2b60a7f",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 258.17,
      "commitsBetweenForRepo": 1981,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,56 @@\n   public static void outputContainerLog(String containerId, String nodeId,\n       String fileName, long fileLength, long outputSize,\n       String lastModifiedTime, InputStream fis, OutputStream os,\n       byte[] buf, ContainerLogAggregationType logType) throws IOException {\n     long toSkip \u003d 0;\n     long totalBytesToRead \u003d fileLength;\n     long skipAfterRead \u003d 0;\n     if (outputSize \u003c 0) {\n       long absBytes \u003d Math.abs(outputSize);\n       if (absBytes \u003c fileLength) {\n         toSkip \u003d fileLength - absBytes;\n         totalBytesToRead \u003d absBytes;\n       }\n       org.apache.hadoop.io.IOUtils.skipFully(fis, toSkip);\n     } else {\n       if (outputSize \u003c fileLength) {\n         totalBytesToRead \u003d outputSize;\n         skipAfterRead \u003d fileLength - outputSize;\n       }\n     }\n \n     long curRead \u003d 0;\n     long pendingRead \u003d totalBytesToRead - curRead;\n     int toRead \u003d pendingRead \u003e buf.length ? buf.length\n         : (int) pendingRead;\n     int len \u003d fis.read(buf, 0, toRead);\n     boolean keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n     if (keepGoing) {\n       StringBuilder sb \u003d new StringBuilder();\n       String containerStr \u003d String.format(\n           LogToolUtils.CONTAINER_ON_NODE_PATTERN,\n           containerId, nodeId);\n-      sb.append(containerStr + \"\\n\");\n-      sb.append(\"LogAggregationType: \" + logType + \"\\n\");\n-      sb.append(StringUtils.repeat(\"\u003d\", containerStr.length()) + \"\\n\");\n-      sb.append(\"LogType:\" + fileName + \"\\n\");\n-      sb.append(\"LogLastModifiedTime:\" + lastModifiedTime + \"\\n\");\n-      sb.append(\"LogLength:\" + Long.toString(fileLength) + \"\\n\");\n-      sb.append(\"LogContents:\\n\");\n+      sb.append(containerStr + \"\\n\")\n+          .append(\"LogAggregationType: \" + logType + \"\\n\")\n+          .append(StringUtils.repeat(\"\u003d\", containerStr.length()) + \"\\n\")\n+          .append(\"LogType:\" + fileName + \"\\n\")\n+          .append(\"LogLastModifiedTime:\" + lastModifiedTime + \"\\n\")\n+          .append(\"LogLength:\" + Long.toString(fileLength) + \"\\n\")\n+          .append(\"LogContents:\\n\");\n       byte[] b \u003d sb.toString().getBytes(\n           Charset.forName(\"UTF-8\"));\n       os.write(b, 0, b.length);\n     }\n     while (keepGoing) {\n       os.write(buf, 0, len);\n       curRead +\u003d len;\n \n       pendingRead \u003d totalBytesToRead - curRead;\n       toRead \u003d pendingRead \u003e buf.length ? buf.length\n           : (int) pendingRead;\n       len \u003d fis.read(buf, 0, toRead);\n       keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n     }\n     org.apache.hadoop.io.IOUtils.skipFully(fis, skipAfterRead);\n     os.flush();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static void outputContainerLog(String containerId, String nodeId,\n      String fileName, long fileLength, long outputSize,\n      String lastModifiedTime, InputStream fis, OutputStream os,\n      byte[] buf, ContainerLogAggregationType logType) throws IOException {\n    long toSkip \u003d 0;\n    long totalBytesToRead \u003d fileLength;\n    long skipAfterRead \u003d 0;\n    if (outputSize \u003c 0) {\n      long absBytes \u003d Math.abs(outputSize);\n      if (absBytes \u003c fileLength) {\n        toSkip \u003d fileLength - absBytes;\n        totalBytesToRead \u003d absBytes;\n      }\n      org.apache.hadoop.io.IOUtils.skipFully(fis, toSkip);\n    } else {\n      if (outputSize \u003c fileLength) {\n        totalBytesToRead \u003d outputSize;\n        skipAfterRead \u003d fileLength - outputSize;\n      }\n    }\n\n    long curRead \u003d 0;\n    long pendingRead \u003d totalBytesToRead - curRead;\n    int toRead \u003d pendingRead \u003e buf.length ? buf.length\n        : (int) pendingRead;\n    int len \u003d fis.read(buf, 0, toRead);\n    boolean keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n    if (keepGoing) {\n      StringBuilder sb \u003d new StringBuilder();\n      String containerStr \u003d String.format(\n          LogToolUtils.CONTAINER_ON_NODE_PATTERN,\n          containerId, nodeId);\n      sb.append(containerStr + \"\\n\")\n          .append(\"LogAggregationType: \" + logType + \"\\n\")\n          .append(StringUtils.repeat(\"\u003d\", containerStr.length()) + \"\\n\")\n          .append(\"LogType:\" + fileName + \"\\n\")\n          .append(\"LogLastModifiedTime:\" + lastModifiedTime + \"\\n\")\n          .append(\"LogLength:\" + Long.toString(fileLength) + \"\\n\")\n          .append(\"LogContents:\\n\");\n      byte[] b \u003d sb.toString().getBytes(\n          Charset.forName(\"UTF-8\"));\n      os.write(b, 0, b.length);\n    }\n    while (keepGoing) {\n      os.write(buf, 0, len);\n      curRead +\u003d len;\n\n      pendingRead \u003d totalBytesToRead - curRead;\n      toRead \u003d pendingRead \u003e buf.length ? buf.length\n          : (int) pendingRead;\n      len \u003d fis.read(buf, 0, toRead);\n      keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n    }\n    org.apache.hadoop.io.IOUtils.skipFully(fis, skipAfterRead);\n    os.flush();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/LogToolUtils.java",
      "extendedDetails": {}
    },
    "ce2d5bfa5f84e7e563980796549b56ef1e4bbf1e": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-6174. Log files pattern should be same for both running and finished container. Contributed by Xuan Gong.\n",
      "commitDate": "15/02/17 9:05 AM",
      "commitName": "ce2d5bfa5f84e7e563980796549b56ef1e4bbf1e",
      "commitAuthor": "Junping Du",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-6174. Log files pattern should be same for both running and finished container. Contributed by Xuan Gong.\n",
          "commitDate": "15/02/17 9:05 AM",
          "commitName": "ce2d5bfa5f84e7e563980796549b56ef1e4bbf1e",
          "commitAuthor": "Junping Du",
          "commitDateOld": "02/02/17 12:41 AM",
          "commitNameOld": "327c9980aafce52cc02d2b8885fc4e9f628ab23c",
          "commitAuthorOld": "Junping Du",
          "daysBetweenCommits": 13.35,
          "commitsBetweenForRepo": 61,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,56 +1,56 @@\n   public static void outputContainerLog(String containerId, String nodeId,\n       String fileName, long fileLength, long outputSize,\n       String lastModifiedTime, InputStream fis, OutputStream os,\n-      byte[] buf, ContainerLogType logType) throws IOException {\n+      byte[] buf, ContainerLogAggregationType logType) throws IOException {\n     long toSkip \u003d 0;\n     long totalBytesToRead \u003d fileLength;\n     long skipAfterRead \u003d 0;\n     if (outputSize \u003c 0) {\n       long absBytes \u003d Math.abs(outputSize);\n       if (absBytes \u003c fileLength) {\n         toSkip \u003d fileLength - absBytes;\n         totalBytesToRead \u003d absBytes;\n       }\n       org.apache.hadoop.io.IOUtils.skipFully(fis, toSkip);\n     } else {\n       if (outputSize \u003c fileLength) {\n         totalBytesToRead \u003d outputSize;\n         skipAfterRead \u003d fileLength - outputSize;\n       }\n     }\n \n     long curRead \u003d 0;\n     long pendingRead \u003d totalBytesToRead - curRead;\n     int toRead \u003d pendingRead \u003e buf.length ? buf.length\n         : (int) pendingRead;\n     int len \u003d fis.read(buf, 0, toRead);\n     boolean keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n     if (keepGoing) {\n       StringBuilder sb \u003d new StringBuilder();\n       String containerStr \u003d String.format(\n           LogToolUtils.CONTAINER_ON_NODE_PATTERN,\n           containerId, nodeId);\n       sb.append(containerStr + \"\\n\");\n-      sb.append(\"LogType: \" + logType + \"\\n\");\n+      sb.append(\"LogAggregationType: \" + logType + \"\\n\");\n       sb.append(StringUtils.repeat(\"\u003d\", containerStr.length()) + \"\\n\");\n-      sb.append(\"FileName:\" + fileName + \"\\n\");\n+      sb.append(\"LogType:\" + fileName + \"\\n\");\n       sb.append(\"LogLastModifiedTime:\" + lastModifiedTime + \"\\n\");\n       sb.append(\"LogLength:\" + Long.toString(fileLength) + \"\\n\");\n       sb.append(\"LogContents:\\n\");\n       byte[] b \u003d sb.toString().getBytes(\n           Charset.forName(\"UTF-8\"));\n       os.write(b, 0, b.length);\n     }\n     while (keepGoing) {\n       os.write(buf, 0, len);\n       curRead +\u003d len;\n \n       pendingRead \u003d totalBytesToRead - curRead;\n       toRead \u003d pendingRead \u003e buf.length ? buf.length\n           : (int) pendingRead;\n       len \u003d fis.read(buf, 0, toRead);\n       keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n     }\n     org.apache.hadoop.io.IOUtils.skipFully(fis, skipAfterRead);\n     os.flush();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static void outputContainerLog(String containerId, String nodeId,\n      String fileName, long fileLength, long outputSize,\n      String lastModifiedTime, InputStream fis, OutputStream os,\n      byte[] buf, ContainerLogAggregationType logType) throws IOException {\n    long toSkip \u003d 0;\n    long totalBytesToRead \u003d fileLength;\n    long skipAfterRead \u003d 0;\n    if (outputSize \u003c 0) {\n      long absBytes \u003d Math.abs(outputSize);\n      if (absBytes \u003c fileLength) {\n        toSkip \u003d fileLength - absBytes;\n        totalBytesToRead \u003d absBytes;\n      }\n      org.apache.hadoop.io.IOUtils.skipFully(fis, toSkip);\n    } else {\n      if (outputSize \u003c fileLength) {\n        totalBytesToRead \u003d outputSize;\n        skipAfterRead \u003d fileLength - outputSize;\n      }\n    }\n\n    long curRead \u003d 0;\n    long pendingRead \u003d totalBytesToRead - curRead;\n    int toRead \u003d pendingRead \u003e buf.length ? buf.length\n        : (int) pendingRead;\n    int len \u003d fis.read(buf, 0, toRead);\n    boolean keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n    if (keepGoing) {\n      StringBuilder sb \u003d new StringBuilder();\n      String containerStr \u003d String.format(\n          LogToolUtils.CONTAINER_ON_NODE_PATTERN,\n          containerId, nodeId);\n      sb.append(containerStr + \"\\n\");\n      sb.append(\"LogAggregationType: \" + logType + \"\\n\");\n      sb.append(StringUtils.repeat(\"\u003d\", containerStr.length()) + \"\\n\");\n      sb.append(\"LogType:\" + fileName + \"\\n\");\n      sb.append(\"LogLastModifiedTime:\" + lastModifiedTime + \"\\n\");\n      sb.append(\"LogLength:\" + Long.toString(fileLength) + \"\\n\");\n      sb.append(\"LogContents:\\n\");\n      byte[] b \u003d sb.toString().getBytes(\n          Charset.forName(\"UTF-8\"));\n      os.write(b, 0, b.length);\n    }\n    while (keepGoing) {\n      os.write(buf, 0, len);\n      curRead +\u003d len;\n\n      pendingRead \u003d totalBytesToRead - curRead;\n      toRead \u003d pendingRead \u003e buf.length ? buf.length\n          : (int) pendingRead;\n      len \u003d fis.read(buf, 0, toRead);\n      keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n    }\n    org.apache.hadoop.io.IOUtils.skipFully(fis, skipAfterRead);\n    os.flush();\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/LogToolUtils.java",
          "extendedDetails": {
            "oldValue": "[containerId-String, nodeId-String, fileName-String, fileLength-long, outputSize-long, lastModifiedTime-String, fis-InputStream, os-OutputStream, buf-byte[], logType-ContainerLogType]",
            "newValue": "[containerId-String, nodeId-String, fileName-String, fileLength-long, outputSize-long, lastModifiedTime-String, fis-InputStream, os-OutputStream, buf-byte[], logType-ContainerLogAggregationType]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-6174. Log files pattern should be same for both running and finished container. Contributed by Xuan Gong.\n",
          "commitDate": "15/02/17 9:05 AM",
          "commitName": "ce2d5bfa5f84e7e563980796549b56ef1e4bbf1e",
          "commitAuthor": "Junping Du",
          "commitDateOld": "02/02/17 12:41 AM",
          "commitNameOld": "327c9980aafce52cc02d2b8885fc4e9f628ab23c",
          "commitAuthorOld": "Junping Du",
          "daysBetweenCommits": 13.35,
          "commitsBetweenForRepo": 61,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,56 +1,56 @@\n   public static void outputContainerLog(String containerId, String nodeId,\n       String fileName, long fileLength, long outputSize,\n       String lastModifiedTime, InputStream fis, OutputStream os,\n-      byte[] buf, ContainerLogType logType) throws IOException {\n+      byte[] buf, ContainerLogAggregationType logType) throws IOException {\n     long toSkip \u003d 0;\n     long totalBytesToRead \u003d fileLength;\n     long skipAfterRead \u003d 0;\n     if (outputSize \u003c 0) {\n       long absBytes \u003d Math.abs(outputSize);\n       if (absBytes \u003c fileLength) {\n         toSkip \u003d fileLength - absBytes;\n         totalBytesToRead \u003d absBytes;\n       }\n       org.apache.hadoop.io.IOUtils.skipFully(fis, toSkip);\n     } else {\n       if (outputSize \u003c fileLength) {\n         totalBytesToRead \u003d outputSize;\n         skipAfterRead \u003d fileLength - outputSize;\n       }\n     }\n \n     long curRead \u003d 0;\n     long pendingRead \u003d totalBytesToRead - curRead;\n     int toRead \u003d pendingRead \u003e buf.length ? buf.length\n         : (int) pendingRead;\n     int len \u003d fis.read(buf, 0, toRead);\n     boolean keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n     if (keepGoing) {\n       StringBuilder sb \u003d new StringBuilder();\n       String containerStr \u003d String.format(\n           LogToolUtils.CONTAINER_ON_NODE_PATTERN,\n           containerId, nodeId);\n       sb.append(containerStr + \"\\n\");\n-      sb.append(\"LogType: \" + logType + \"\\n\");\n+      sb.append(\"LogAggregationType: \" + logType + \"\\n\");\n       sb.append(StringUtils.repeat(\"\u003d\", containerStr.length()) + \"\\n\");\n-      sb.append(\"FileName:\" + fileName + \"\\n\");\n+      sb.append(\"LogType:\" + fileName + \"\\n\");\n       sb.append(\"LogLastModifiedTime:\" + lastModifiedTime + \"\\n\");\n       sb.append(\"LogLength:\" + Long.toString(fileLength) + \"\\n\");\n       sb.append(\"LogContents:\\n\");\n       byte[] b \u003d sb.toString().getBytes(\n           Charset.forName(\"UTF-8\"));\n       os.write(b, 0, b.length);\n     }\n     while (keepGoing) {\n       os.write(buf, 0, len);\n       curRead +\u003d len;\n \n       pendingRead \u003d totalBytesToRead - curRead;\n       toRead \u003d pendingRead \u003e buf.length ? buf.length\n           : (int) pendingRead;\n       len \u003d fis.read(buf, 0, toRead);\n       keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n     }\n     org.apache.hadoop.io.IOUtils.skipFully(fis, skipAfterRead);\n     os.flush();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static void outputContainerLog(String containerId, String nodeId,\n      String fileName, long fileLength, long outputSize,\n      String lastModifiedTime, InputStream fis, OutputStream os,\n      byte[] buf, ContainerLogAggregationType logType) throws IOException {\n    long toSkip \u003d 0;\n    long totalBytesToRead \u003d fileLength;\n    long skipAfterRead \u003d 0;\n    if (outputSize \u003c 0) {\n      long absBytes \u003d Math.abs(outputSize);\n      if (absBytes \u003c fileLength) {\n        toSkip \u003d fileLength - absBytes;\n        totalBytesToRead \u003d absBytes;\n      }\n      org.apache.hadoop.io.IOUtils.skipFully(fis, toSkip);\n    } else {\n      if (outputSize \u003c fileLength) {\n        totalBytesToRead \u003d outputSize;\n        skipAfterRead \u003d fileLength - outputSize;\n      }\n    }\n\n    long curRead \u003d 0;\n    long pendingRead \u003d totalBytesToRead - curRead;\n    int toRead \u003d pendingRead \u003e buf.length ? buf.length\n        : (int) pendingRead;\n    int len \u003d fis.read(buf, 0, toRead);\n    boolean keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n    if (keepGoing) {\n      StringBuilder sb \u003d new StringBuilder();\n      String containerStr \u003d String.format(\n          LogToolUtils.CONTAINER_ON_NODE_PATTERN,\n          containerId, nodeId);\n      sb.append(containerStr + \"\\n\");\n      sb.append(\"LogAggregationType: \" + logType + \"\\n\");\n      sb.append(StringUtils.repeat(\"\u003d\", containerStr.length()) + \"\\n\");\n      sb.append(\"LogType:\" + fileName + \"\\n\");\n      sb.append(\"LogLastModifiedTime:\" + lastModifiedTime + \"\\n\");\n      sb.append(\"LogLength:\" + Long.toString(fileLength) + \"\\n\");\n      sb.append(\"LogContents:\\n\");\n      byte[] b \u003d sb.toString().getBytes(\n          Charset.forName(\"UTF-8\"));\n      os.write(b, 0, b.length);\n    }\n    while (keepGoing) {\n      os.write(buf, 0, len);\n      curRead +\u003d len;\n\n      pendingRead \u003d totalBytesToRead - curRead;\n      toRead \u003d pendingRead \u003e buf.length ? buf.length\n          : (int) pendingRead;\n      len \u003d fis.read(buf, 0, toRead);\n      keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n    }\n    org.apache.hadoop.io.IOUtils.skipFully(fis, skipAfterRead);\n    os.flush();\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/LogToolUtils.java",
          "extendedDetails": {}
        }
      ]
    },
    "327c9980aafce52cc02d2b8885fc4e9f628ab23c": {
      "type": "Yintroduced",
      "commitMessage": "YARN-6100. Improve YARN webservice to output aggregated container logs. Contributed by Xuan Gong.\n",
      "commitDate": "02/02/17 12:41 AM",
      "commitName": "327c9980aafce52cc02d2b8885fc4e9f628ab23c",
      "commitAuthor": "Junping Du",
      "diff": "@@ -0,0 +1,56 @@\n+  public static void outputContainerLog(String containerId, String nodeId,\n+      String fileName, long fileLength, long outputSize,\n+      String lastModifiedTime, InputStream fis, OutputStream os,\n+      byte[] buf, ContainerLogType logType) throws IOException {\n+    long toSkip \u003d 0;\n+    long totalBytesToRead \u003d fileLength;\n+    long skipAfterRead \u003d 0;\n+    if (outputSize \u003c 0) {\n+      long absBytes \u003d Math.abs(outputSize);\n+      if (absBytes \u003c fileLength) {\n+        toSkip \u003d fileLength - absBytes;\n+        totalBytesToRead \u003d absBytes;\n+      }\n+      org.apache.hadoop.io.IOUtils.skipFully(fis, toSkip);\n+    } else {\n+      if (outputSize \u003c fileLength) {\n+        totalBytesToRead \u003d outputSize;\n+        skipAfterRead \u003d fileLength - outputSize;\n+      }\n+    }\n+\n+    long curRead \u003d 0;\n+    long pendingRead \u003d totalBytesToRead - curRead;\n+    int toRead \u003d pendingRead \u003e buf.length ? buf.length\n+        : (int) pendingRead;\n+    int len \u003d fis.read(buf, 0, toRead);\n+    boolean keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n+    if (keepGoing) {\n+      StringBuilder sb \u003d new StringBuilder();\n+      String containerStr \u003d String.format(\n+          LogToolUtils.CONTAINER_ON_NODE_PATTERN,\n+          containerId, nodeId);\n+      sb.append(containerStr + \"\\n\");\n+      sb.append(\"LogType: \" + logType + \"\\n\");\n+      sb.append(StringUtils.repeat(\"\u003d\", containerStr.length()) + \"\\n\");\n+      sb.append(\"FileName:\" + fileName + \"\\n\");\n+      sb.append(\"LogLastModifiedTime:\" + lastModifiedTime + \"\\n\");\n+      sb.append(\"LogLength:\" + Long.toString(fileLength) + \"\\n\");\n+      sb.append(\"LogContents:\\n\");\n+      byte[] b \u003d sb.toString().getBytes(\n+          Charset.forName(\"UTF-8\"));\n+      os.write(b, 0, b.length);\n+    }\n+    while (keepGoing) {\n+      os.write(buf, 0, len);\n+      curRead +\u003d len;\n+\n+      pendingRead \u003d totalBytesToRead - curRead;\n+      toRead \u003d pendingRead \u003e buf.length ? buf.length\n+          : (int) pendingRead;\n+      len \u003d fis.read(buf, 0, toRead);\n+      keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n+    }\n+    org.apache.hadoop.io.IOUtils.skipFully(fis, skipAfterRead);\n+    os.flush();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static void outputContainerLog(String containerId, String nodeId,\n      String fileName, long fileLength, long outputSize,\n      String lastModifiedTime, InputStream fis, OutputStream os,\n      byte[] buf, ContainerLogType logType) throws IOException {\n    long toSkip \u003d 0;\n    long totalBytesToRead \u003d fileLength;\n    long skipAfterRead \u003d 0;\n    if (outputSize \u003c 0) {\n      long absBytes \u003d Math.abs(outputSize);\n      if (absBytes \u003c fileLength) {\n        toSkip \u003d fileLength - absBytes;\n        totalBytesToRead \u003d absBytes;\n      }\n      org.apache.hadoop.io.IOUtils.skipFully(fis, toSkip);\n    } else {\n      if (outputSize \u003c fileLength) {\n        totalBytesToRead \u003d outputSize;\n        skipAfterRead \u003d fileLength - outputSize;\n      }\n    }\n\n    long curRead \u003d 0;\n    long pendingRead \u003d totalBytesToRead - curRead;\n    int toRead \u003d pendingRead \u003e buf.length ? buf.length\n        : (int) pendingRead;\n    int len \u003d fis.read(buf, 0, toRead);\n    boolean keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n    if (keepGoing) {\n      StringBuilder sb \u003d new StringBuilder();\n      String containerStr \u003d String.format(\n          LogToolUtils.CONTAINER_ON_NODE_PATTERN,\n          containerId, nodeId);\n      sb.append(containerStr + \"\\n\");\n      sb.append(\"LogType: \" + logType + \"\\n\");\n      sb.append(StringUtils.repeat(\"\u003d\", containerStr.length()) + \"\\n\");\n      sb.append(\"FileName:\" + fileName + \"\\n\");\n      sb.append(\"LogLastModifiedTime:\" + lastModifiedTime + \"\\n\");\n      sb.append(\"LogLength:\" + Long.toString(fileLength) + \"\\n\");\n      sb.append(\"LogContents:\\n\");\n      byte[] b \u003d sb.toString().getBytes(\n          Charset.forName(\"UTF-8\"));\n      os.write(b, 0, b.length);\n    }\n    while (keepGoing) {\n      os.write(buf, 0, len);\n      curRead +\u003d len;\n\n      pendingRead \u003d totalBytesToRead - curRead;\n      toRead \u003d pendingRead \u003e buf.length ? buf.length\n          : (int) pendingRead;\n      len \u003d fis.read(buf, 0, toRead);\n      keepGoing \u003d (len !\u003d -1 \u0026\u0026 curRead \u003c totalBytesToRead);\n    }\n    org.apache.hadoop.io.IOUtils.skipFully(fis, skipAfterRead);\n    os.flush();\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/LogToolUtils.java"
    }
  }
}