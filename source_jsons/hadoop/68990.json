{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DistCh.java",
  "functionName": "setup",
  "functionId": "setup___ops-List__FileOperation____log-Path",
  "sourceFilePath": "hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/tools/DistCh.java",
  "functionStartLine": 420,
  "functionEndLine": 484,
  "numCommitsSeen": 6,
  "timeTaken": 4455,
  "changeHistory": [
    "02b21b7131d9f1dba6d93fb3f87154e86b9825d6",
    "10325d97329c214bb3899c8535df5a366bc86d2f",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "02b21b7131d9f1dba6d93fb3f87154e86b9825d6": "Ybodychange",
    "10325d97329c214bb3899c8535df5a366bc86d2f": "Yfilerename",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "02b21b7131d9f1dba6d93fb3f87154e86b9825d6": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11429. Findbugs warnings in hadoop extras. Contributed by Varun Saxena.\n",
      "commitDate": "21/12/14 2:57 PM",
      "commitName": "02b21b7131d9f1dba6d93fb3f87154e86b9825d6",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "16/07/13 7:22 PM",
      "commitNameOld": "1b6324265d6d27adc932b53d6ce5cce7698c241d",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 522.86,
      "commitsBetweenForRepo": 3917,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,65 @@\n   private boolean setup(List\u003cFileOperation\u003e ops, Path log) \n   throws IOException {\n     final String randomId \u003d getRandomId();\n     JobClient jClient \u003d new JobClient(jobconf);\n     Path stagingArea;\n     try {\n       stagingArea \u003d JobSubmissionFiles.getStagingDir(\n                        jClient.getClusterHandle(), jobconf);\n     } catch (InterruptedException ie){\n       throw new IOException(ie);\n     }\n     Path jobdir \u003d new Path(stagingArea + NAME + \"_\" + randomId);\n     FsPermission mapredSysPerms \u003d \n       new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     FileSystem.mkdirs(jClient.getFs(), jobdir, mapredSysPerms);\n     LOG.info(JOB_DIR_LABEL + \"\u003d\" + jobdir);\n \n     if (log \u003d\u003d null) {\n       log \u003d new Path(jobdir, \"_logs\");\n     }\n     FileOutputFormat.setOutputPath(jobconf, log);\n     LOG.info(\"log\u003d\" + log);\n \n     //create operation list\n     FileSystem fs \u003d jobdir.getFileSystem(jobconf);\n     Path opList \u003d new Path(jobdir, \"_\" + OP_LIST_LABEL);\n     jobconf.set(OP_LIST_LABEL, opList.toString());\n     int opCount \u003d 0, synCount \u003d 0;\n-    SequenceFile.Writer opWriter \u003d null;\n-    try {\n-      opWriter \u003d SequenceFile.createWriter(fs, jobconf, opList, Text.class,\n-          FileOperation.class, SequenceFile.CompressionType.NONE);\n+    try (SequenceFile.Writer opWriter \u003d SequenceFile.createWriter(fs, jobconf, opList, Text.class,\n+            FileOperation.class, SequenceFile.CompressionType.NONE)) {\n       for(FileOperation op : ops) {\n         FileStatus srcstat \u003d fs.getFileStatus(op.src); \n         if (srcstat.isDirectory() \u0026\u0026 op.isDifferent(srcstat)) {\n           ++opCount;\n           opWriter.append(new Text(op.src.toString()), op);\n         }\n \n         Stack\u003cPath\u003e pathstack \u003d new Stack\u003cPath\u003e();\n         for(pathstack.push(op.src); !pathstack.empty(); ) {\n           for(FileStatus stat : fs.listStatus(pathstack.pop())) {\n             if (stat.isDirectory()) {\n               pathstack.push(stat.getPath());\n             }\n \n             if (op.isDifferent(stat)) {              \n               ++opCount;\n               if (++synCount \u003e SYNC_FILE_MAX) {\n                 opWriter.sync();\n                 synCount \u003d 0;\n               }\n               Path f \u003d stat.getPath();\n               opWriter.append(new Text(f.toString()), new FileOperation(f, op));\n             }\n           }\n         }\n       }\n-    } finally {\n-      opWriter.close();\n     }\n \n     checkDuplication(fs, opList, new Path(jobdir, \"_sorted\"), jobconf);\n     jobconf.setInt(OP_COUNT_LABEL, opCount);\n     LOG.info(OP_COUNT_LABEL + \"\u003d\" + opCount);\n     jobconf.setNumMapTasks(getMapCount(opCount,\n         new JobClient(jobconf).getClusterStatus().getTaskTrackers()));\n     return opCount !\u003d 0;    \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean setup(List\u003cFileOperation\u003e ops, Path log) \n  throws IOException {\n    final String randomId \u003d getRandomId();\n    JobClient jClient \u003d new JobClient(jobconf);\n    Path stagingArea;\n    try {\n      stagingArea \u003d JobSubmissionFiles.getStagingDir(\n                       jClient.getClusterHandle(), jobconf);\n    } catch (InterruptedException ie){\n      throw new IOException(ie);\n    }\n    Path jobdir \u003d new Path(stagingArea + NAME + \"_\" + randomId);\n    FsPermission mapredSysPerms \u003d \n      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jClient.getFs(), jobdir, mapredSysPerms);\n    LOG.info(JOB_DIR_LABEL + \"\u003d\" + jobdir);\n\n    if (log \u003d\u003d null) {\n      log \u003d new Path(jobdir, \"_logs\");\n    }\n    FileOutputFormat.setOutputPath(jobconf, log);\n    LOG.info(\"log\u003d\" + log);\n\n    //create operation list\n    FileSystem fs \u003d jobdir.getFileSystem(jobconf);\n    Path opList \u003d new Path(jobdir, \"_\" + OP_LIST_LABEL);\n    jobconf.set(OP_LIST_LABEL, opList.toString());\n    int opCount \u003d 0, synCount \u003d 0;\n    try (SequenceFile.Writer opWriter \u003d SequenceFile.createWriter(fs, jobconf, opList, Text.class,\n            FileOperation.class, SequenceFile.CompressionType.NONE)) {\n      for(FileOperation op : ops) {\n        FileStatus srcstat \u003d fs.getFileStatus(op.src); \n        if (srcstat.isDirectory() \u0026\u0026 op.isDifferent(srcstat)) {\n          ++opCount;\n          opWriter.append(new Text(op.src.toString()), op);\n        }\n\n        Stack\u003cPath\u003e pathstack \u003d new Stack\u003cPath\u003e();\n        for(pathstack.push(op.src); !pathstack.empty(); ) {\n          for(FileStatus stat : fs.listStatus(pathstack.pop())) {\n            if (stat.isDirectory()) {\n              pathstack.push(stat.getPath());\n            }\n\n            if (op.isDifferent(stat)) {              \n              ++opCount;\n              if (++synCount \u003e SYNC_FILE_MAX) {\n                opWriter.sync();\n                synCount \u003d 0;\n              }\n              Path f \u003d stat.getPath();\n              opWriter.append(new Text(f.toString()), new FileOperation(f, op));\n            }\n          }\n        }\n      }\n    }\n\n    checkDuplication(fs, opList, new Path(jobdir, \"_sorted\"), jobconf);\n    jobconf.setInt(OP_COUNT_LABEL, opCount);\n    LOG.info(OP_COUNT_LABEL + \"\u003d\" + opCount);\n    jobconf.setNumMapTasks(getMapCount(opCount,\n        new JobClient(jobconf).getClusterStatus().getTaskTrackers()));\n    return opCount !\u003d 0;    \n  }",
      "path": "hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/tools/DistCh.java",
      "extendedDetails": {}
    },
    "10325d97329c214bb3899c8535df5a366bc86d2f": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-3582. Move successfully passing MR1 tests to MR2 maven tree.(ahmed via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1233090 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/01/12 2:10 PM",
      "commitName": "10325d97329c214bb3899c8535df5a366bc86d2f",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "18/01/12 10:20 AM",
      "commitNameOld": "8b2f6909ec7df5cffb5ef417f5c9cffdee43e38a",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.16,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private boolean setup(List\u003cFileOperation\u003e ops, Path log) \n  throws IOException {\n    final String randomId \u003d getRandomId();\n    JobClient jClient \u003d new JobClient(jobconf);\n    Path stagingArea;\n    try {\n      stagingArea \u003d JobSubmissionFiles.getStagingDir(\n                       jClient.getClusterHandle(), jobconf);\n    } catch (InterruptedException ie){\n      throw new IOException(ie);\n    }\n    Path jobdir \u003d new Path(stagingArea + NAME + \"_\" + randomId);\n    FsPermission mapredSysPerms \u003d \n      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jClient.getFs(), jobdir, mapredSysPerms);\n    LOG.info(JOB_DIR_LABEL + \"\u003d\" + jobdir);\n\n    if (log \u003d\u003d null) {\n      log \u003d new Path(jobdir, \"_logs\");\n    }\n    FileOutputFormat.setOutputPath(jobconf, log);\n    LOG.info(\"log\u003d\" + log);\n\n    //create operation list\n    FileSystem fs \u003d jobdir.getFileSystem(jobconf);\n    Path opList \u003d new Path(jobdir, \"_\" + OP_LIST_LABEL);\n    jobconf.set(OP_LIST_LABEL, opList.toString());\n    int opCount \u003d 0, synCount \u003d 0;\n    SequenceFile.Writer opWriter \u003d null;\n    try {\n      opWriter \u003d SequenceFile.createWriter(fs, jobconf, opList, Text.class,\n          FileOperation.class, SequenceFile.CompressionType.NONE);\n      for(FileOperation op : ops) {\n        FileStatus srcstat \u003d fs.getFileStatus(op.src); \n        if (srcstat.isDirectory() \u0026\u0026 op.isDifferent(srcstat)) {\n          ++opCount;\n          opWriter.append(new Text(op.src.toString()), op);\n        }\n\n        Stack\u003cPath\u003e pathstack \u003d new Stack\u003cPath\u003e();\n        for(pathstack.push(op.src); !pathstack.empty(); ) {\n          for(FileStatus stat : fs.listStatus(pathstack.pop())) {\n            if (stat.isDirectory()) {\n              pathstack.push(stat.getPath());\n            }\n\n            if (op.isDifferent(stat)) {              \n              ++opCount;\n              if (++synCount \u003e SYNC_FILE_MAX) {\n                opWriter.sync();\n                synCount \u003d 0;\n              }\n              Path f \u003d stat.getPath();\n              opWriter.append(new Text(f.toString()), new FileOperation(f, op));\n            }\n          }\n        }\n      }\n    } finally {\n      opWriter.close();\n    }\n\n    checkDuplication(fs, opList, new Path(jobdir, \"_sorted\"), jobconf);\n    jobconf.setInt(OP_COUNT_LABEL, opCount);\n    LOG.info(OP_COUNT_LABEL + \"\u003d\" + opCount);\n    jobconf.setNumMapTasks(getMapCount(opCount,\n        new JobClient(jobconf).getClusterStatus().getTaskTrackers()));\n    return opCount !\u003d 0;    \n  }",
      "path": "hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/tools/DistCh.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/DistCh.java",
        "newPath": "hadoop-tools/hadoop-extras/src/main/java/org/apache/hadoop/tools/DistCh.java"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private boolean setup(List\u003cFileOperation\u003e ops, Path log) \n  throws IOException {\n    final String randomId \u003d getRandomId();\n    JobClient jClient \u003d new JobClient(jobconf);\n    Path stagingArea;\n    try {\n      stagingArea \u003d JobSubmissionFiles.getStagingDir(\n                       jClient.getClusterHandle(), jobconf);\n    } catch (InterruptedException ie){\n      throw new IOException(ie);\n    }\n    Path jobdir \u003d new Path(stagingArea + NAME + \"_\" + randomId);\n    FsPermission mapredSysPerms \u003d \n      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jClient.getFs(), jobdir, mapredSysPerms);\n    LOG.info(JOB_DIR_LABEL + \"\u003d\" + jobdir);\n\n    if (log \u003d\u003d null) {\n      log \u003d new Path(jobdir, \"_logs\");\n    }\n    FileOutputFormat.setOutputPath(jobconf, log);\n    LOG.info(\"log\u003d\" + log);\n\n    //create operation list\n    FileSystem fs \u003d jobdir.getFileSystem(jobconf);\n    Path opList \u003d new Path(jobdir, \"_\" + OP_LIST_LABEL);\n    jobconf.set(OP_LIST_LABEL, opList.toString());\n    int opCount \u003d 0, synCount \u003d 0;\n    SequenceFile.Writer opWriter \u003d null;\n    try {\n      opWriter \u003d SequenceFile.createWriter(fs, jobconf, opList, Text.class,\n          FileOperation.class, SequenceFile.CompressionType.NONE);\n      for(FileOperation op : ops) {\n        FileStatus srcstat \u003d fs.getFileStatus(op.src); \n        if (srcstat.isDirectory() \u0026\u0026 op.isDifferent(srcstat)) {\n          ++opCount;\n          opWriter.append(new Text(op.src.toString()), op);\n        }\n\n        Stack\u003cPath\u003e pathstack \u003d new Stack\u003cPath\u003e();\n        for(pathstack.push(op.src); !pathstack.empty(); ) {\n          for(FileStatus stat : fs.listStatus(pathstack.pop())) {\n            if (stat.isDirectory()) {\n              pathstack.push(stat.getPath());\n            }\n\n            if (op.isDifferent(stat)) {              \n              ++opCount;\n              if (++synCount \u003e SYNC_FILE_MAX) {\n                opWriter.sync();\n                synCount \u003d 0;\n              }\n              Path f \u003d stat.getPath();\n              opWriter.append(new Text(f.toString()), new FileOperation(f, op));\n            }\n          }\n        }\n      }\n    } finally {\n      opWriter.close();\n    }\n\n    checkDuplication(fs, opList, new Path(jobdir, \"_sorted\"), jobconf);\n    jobconf.setInt(OP_COUNT_LABEL, opCount);\n    LOG.info(OP_COUNT_LABEL + \"\u003d\" + opCount);\n    jobconf.setNumMapTasks(getMapCount(opCount,\n        new JobClient(jobconf).getClusterStatus().getTaskTrackers()));\n    return opCount !\u003d 0;    \n  }",
      "path": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/DistCh.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/DistCh.java",
        "newPath": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/DistCh.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private boolean setup(List\u003cFileOperation\u003e ops, Path log) \n  throws IOException {\n    final String randomId \u003d getRandomId();\n    JobClient jClient \u003d new JobClient(jobconf);\n    Path stagingArea;\n    try {\n      stagingArea \u003d JobSubmissionFiles.getStagingDir(\n                       jClient.getClusterHandle(), jobconf);\n    } catch (InterruptedException ie){\n      throw new IOException(ie);\n    }\n    Path jobdir \u003d new Path(stagingArea + NAME + \"_\" + randomId);\n    FsPermission mapredSysPerms \u003d \n      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jClient.getFs(), jobdir, mapredSysPerms);\n    LOG.info(JOB_DIR_LABEL + \"\u003d\" + jobdir);\n\n    if (log \u003d\u003d null) {\n      log \u003d new Path(jobdir, \"_logs\");\n    }\n    FileOutputFormat.setOutputPath(jobconf, log);\n    LOG.info(\"log\u003d\" + log);\n\n    //create operation list\n    FileSystem fs \u003d jobdir.getFileSystem(jobconf);\n    Path opList \u003d new Path(jobdir, \"_\" + OP_LIST_LABEL);\n    jobconf.set(OP_LIST_LABEL, opList.toString());\n    int opCount \u003d 0, synCount \u003d 0;\n    SequenceFile.Writer opWriter \u003d null;\n    try {\n      opWriter \u003d SequenceFile.createWriter(fs, jobconf, opList, Text.class,\n          FileOperation.class, SequenceFile.CompressionType.NONE);\n      for(FileOperation op : ops) {\n        FileStatus srcstat \u003d fs.getFileStatus(op.src); \n        if (srcstat.isDirectory() \u0026\u0026 op.isDifferent(srcstat)) {\n          ++opCount;\n          opWriter.append(new Text(op.src.toString()), op);\n        }\n\n        Stack\u003cPath\u003e pathstack \u003d new Stack\u003cPath\u003e();\n        for(pathstack.push(op.src); !pathstack.empty(); ) {\n          for(FileStatus stat : fs.listStatus(pathstack.pop())) {\n            if (stat.isDirectory()) {\n              pathstack.push(stat.getPath());\n            }\n\n            if (op.isDifferent(stat)) {              \n              ++opCount;\n              if (++synCount \u003e SYNC_FILE_MAX) {\n                opWriter.sync();\n                synCount \u003d 0;\n              }\n              Path f \u003d stat.getPath();\n              opWriter.append(new Text(f.toString()), new FileOperation(f, op));\n            }\n          }\n        }\n      }\n    } finally {\n      opWriter.close();\n    }\n\n    checkDuplication(fs, opList, new Path(jobdir, \"_sorted\"), jobconf);\n    jobconf.setInt(OP_COUNT_LABEL, opCount);\n    LOG.info(OP_COUNT_LABEL + \"\u003d\" + opCount);\n    jobconf.setNumMapTasks(getMapCount(opCount,\n        new JobClient(jobconf).getClusterStatus().getTaskTrackers()));\n    return opCount !\u003d 0;    \n  }",
      "path": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/DistCh.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/tools/org/apache/hadoop/tools/DistCh.java",
        "newPath": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/DistCh.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,69 @@\n+  private boolean setup(List\u003cFileOperation\u003e ops, Path log) \n+  throws IOException {\n+    final String randomId \u003d getRandomId();\n+    JobClient jClient \u003d new JobClient(jobconf);\n+    Path stagingArea;\n+    try {\n+      stagingArea \u003d JobSubmissionFiles.getStagingDir(\n+                       jClient.getClusterHandle(), jobconf);\n+    } catch (InterruptedException ie){\n+      throw new IOException(ie);\n+    }\n+    Path jobdir \u003d new Path(stagingArea + NAME + \"_\" + randomId);\n+    FsPermission mapredSysPerms \u003d \n+      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n+    FileSystem.mkdirs(jClient.getFs(), jobdir, mapredSysPerms);\n+    LOG.info(JOB_DIR_LABEL + \"\u003d\" + jobdir);\n+\n+    if (log \u003d\u003d null) {\n+      log \u003d new Path(jobdir, \"_logs\");\n+    }\n+    FileOutputFormat.setOutputPath(jobconf, log);\n+    LOG.info(\"log\u003d\" + log);\n+\n+    //create operation list\n+    FileSystem fs \u003d jobdir.getFileSystem(jobconf);\n+    Path opList \u003d new Path(jobdir, \"_\" + OP_LIST_LABEL);\n+    jobconf.set(OP_LIST_LABEL, opList.toString());\n+    int opCount \u003d 0, synCount \u003d 0;\n+    SequenceFile.Writer opWriter \u003d null;\n+    try {\n+      opWriter \u003d SequenceFile.createWriter(fs, jobconf, opList, Text.class,\n+          FileOperation.class, SequenceFile.CompressionType.NONE);\n+      for(FileOperation op : ops) {\n+        FileStatus srcstat \u003d fs.getFileStatus(op.src); \n+        if (srcstat.isDirectory() \u0026\u0026 op.isDifferent(srcstat)) {\n+          ++opCount;\n+          opWriter.append(new Text(op.src.toString()), op);\n+        }\n+\n+        Stack\u003cPath\u003e pathstack \u003d new Stack\u003cPath\u003e();\n+        for(pathstack.push(op.src); !pathstack.empty(); ) {\n+          for(FileStatus stat : fs.listStatus(pathstack.pop())) {\n+            if (stat.isDirectory()) {\n+              pathstack.push(stat.getPath());\n+            }\n+\n+            if (op.isDifferent(stat)) {              \n+              ++opCount;\n+              if (++synCount \u003e SYNC_FILE_MAX) {\n+                opWriter.sync();\n+                synCount \u003d 0;\n+              }\n+              Path f \u003d stat.getPath();\n+              opWriter.append(new Text(f.toString()), new FileOperation(f, op));\n+            }\n+          }\n+        }\n+      }\n+    } finally {\n+      opWriter.close();\n+    }\n+\n+    checkDuplication(fs, opList, new Path(jobdir, \"_sorted\"), jobconf);\n+    jobconf.setInt(OP_COUNT_LABEL, opCount);\n+    LOG.info(OP_COUNT_LABEL + \"\u003d\" + opCount);\n+    jobconf.setNumMapTasks(getMapCount(opCount,\n+        new JobClient(jobconf).getClusterStatus().getTaskTrackers()));\n+    return opCount !\u003d 0;    \n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean setup(List\u003cFileOperation\u003e ops, Path log) \n  throws IOException {\n    final String randomId \u003d getRandomId();\n    JobClient jClient \u003d new JobClient(jobconf);\n    Path stagingArea;\n    try {\n      stagingArea \u003d JobSubmissionFiles.getStagingDir(\n                       jClient.getClusterHandle(), jobconf);\n    } catch (InterruptedException ie){\n      throw new IOException(ie);\n    }\n    Path jobdir \u003d new Path(stagingArea + NAME + \"_\" + randomId);\n    FsPermission mapredSysPerms \u003d \n      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jClient.getFs(), jobdir, mapredSysPerms);\n    LOG.info(JOB_DIR_LABEL + \"\u003d\" + jobdir);\n\n    if (log \u003d\u003d null) {\n      log \u003d new Path(jobdir, \"_logs\");\n    }\n    FileOutputFormat.setOutputPath(jobconf, log);\n    LOG.info(\"log\u003d\" + log);\n\n    //create operation list\n    FileSystem fs \u003d jobdir.getFileSystem(jobconf);\n    Path opList \u003d new Path(jobdir, \"_\" + OP_LIST_LABEL);\n    jobconf.set(OP_LIST_LABEL, opList.toString());\n    int opCount \u003d 0, synCount \u003d 0;\n    SequenceFile.Writer opWriter \u003d null;\n    try {\n      opWriter \u003d SequenceFile.createWriter(fs, jobconf, opList, Text.class,\n          FileOperation.class, SequenceFile.CompressionType.NONE);\n      for(FileOperation op : ops) {\n        FileStatus srcstat \u003d fs.getFileStatus(op.src); \n        if (srcstat.isDirectory() \u0026\u0026 op.isDifferent(srcstat)) {\n          ++opCount;\n          opWriter.append(new Text(op.src.toString()), op);\n        }\n\n        Stack\u003cPath\u003e pathstack \u003d new Stack\u003cPath\u003e();\n        for(pathstack.push(op.src); !pathstack.empty(); ) {\n          for(FileStatus stat : fs.listStatus(pathstack.pop())) {\n            if (stat.isDirectory()) {\n              pathstack.push(stat.getPath());\n            }\n\n            if (op.isDifferent(stat)) {              \n              ++opCount;\n              if (++synCount \u003e SYNC_FILE_MAX) {\n                opWriter.sync();\n                synCount \u003d 0;\n              }\n              Path f \u003d stat.getPath();\n              opWriter.append(new Text(f.toString()), new FileOperation(f, op));\n            }\n          }\n        }\n      }\n    } finally {\n      opWriter.close();\n    }\n\n    checkDuplication(fs, opList, new Path(jobdir, \"_sorted\"), jobconf);\n    jobconf.setInt(OP_COUNT_LABEL, opCount);\n    LOG.info(OP_COUNT_LABEL + \"\u003d\" + opCount);\n    jobconf.setNumMapTasks(getMapCount(opCount,\n        new JobClient(jobconf).getClusterStatus().getTaskTrackers()));\n    return opCount !\u003d 0;    \n  }",
      "path": "mapreduce/src/tools/org/apache/hadoop/tools/DistCh.java"
    }
  }
}