{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RouterClientProtocol.java",
  "functionName": "getDatanodeStorageReport",
  "functionId": "getDatanodeStorageReport___type-HdfsConstants.DatanodeReportType",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
  "functionStartLine": 992,
  "functionEndLine": 1020,
  "numCommitsSeen": 40,
  "timeTaken": 2195,
  "changeHistory": [
    "7fe924b1c03a2fa45188027bdc0a36cb6c8b4ba4",
    "6425ed27ea638da75f656204d6df4adad1d91fe1"
  ],
  "changeHistoryShort": {
    "7fe924b1c03a2fa45188027bdc0a36cb6c8b4ba4": "Ybodychange",
    "6425ed27ea638da75f656204d6df4adad1d91fe1": "Yintroduced"
  },
  "changeHistoryDetails": {
    "7fe924b1c03a2fa45188027bdc0a36cb6c8b4ba4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15016. RBF: getDatanodeReport() should return the latest update. Contributed by Inigo Goiri.\n",
      "commitDate": "13/12/19 10:51 AM",
      "commitName": "7fe924b1c03a2fa45188027bdc0a36cb6c8b4ba4",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "07/11/19 7:18 PM",
      "commitNameOld": "42fc8884ab9763e8778670f301896bf473ecf1d2",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 35.65,
      "commitsBetweenForRepo": 147,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,29 @@\n   public DatanodeStorageReport[] getDatanodeStorageReport(\n       HdfsConstants.DatanodeReportType type) throws IOException {\n     rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED);\n \n     Map\u003cString, DatanodeStorageReport[]\u003e dnSubcluster \u003d\n         rpcServer.getDatanodeStorageReportMap(type);\n \n     // Avoid repeating machines in multiple subclusters\n     Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new LinkedHashMap\u003c\u003e();\n     for (DatanodeStorageReport[] dns : dnSubcluster.values()) {\n       for (DatanodeStorageReport dn : dns) {\n         DatanodeInfo dnInfo \u003d dn.getDatanodeInfo();\n         String nodeId \u003d dnInfo.getXferAddr();\n-        if (!datanodesMap.containsKey(nodeId)) {\n+        DatanodeStorageReport oldDn \u003d datanodesMap.get(nodeId);\n+        if (oldDn \u003d\u003d null ||\n+            dnInfo.getLastUpdate() \u003e oldDn.getDatanodeInfo().getLastUpdate()) {\n           datanodesMap.put(nodeId, dn);\n+        } else {\n+          LOG.debug(\"{} is in multiple subclusters\", nodeId);\n         }\n-        // TODO merge somehow, right now it just takes the first one\n       }\n     }\n \n     Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n     DatanodeStorageReport[] combinedData \u003d\n         new DatanodeStorageReport[datanodes.size()];\n     combinedData \u003d datanodes.toArray(combinedData);\n     return combinedData;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeStorageReport[] getDatanodeStorageReport(\n      HdfsConstants.DatanodeReportType type) throws IOException {\n    rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED);\n\n    Map\u003cString, DatanodeStorageReport[]\u003e dnSubcluster \u003d\n        rpcServer.getDatanodeStorageReportMap(type);\n\n    // Avoid repeating machines in multiple subclusters\n    Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new LinkedHashMap\u003c\u003e();\n    for (DatanodeStorageReport[] dns : dnSubcluster.values()) {\n      for (DatanodeStorageReport dn : dns) {\n        DatanodeInfo dnInfo \u003d dn.getDatanodeInfo();\n        String nodeId \u003d dnInfo.getXferAddr();\n        DatanodeStorageReport oldDn \u003d datanodesMap.get(nodeId);\n        if (oldDn \u003d\u003d null ||\n            dnInfo.getLastUpdate() \u003e oldDn.getDatanodeInfo().getLastUpdate()) {\n          datanodesMap.put(nodeId, dn);\n        } else {\n          LOG.debug(\"{} is in multiple subclusters\", nodeId);\n        }\n      }\n    }\n\n    Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n    DatanodeStorageReport[] combinedData \u003d\n        new DatanodeStorageReport[datanodes.size()];\n    combinedData \u003d datanodes.toArray(combinedData);\n    return combinedData;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
      "extendedDetails": {}
    },
    "6425ed27ea638da75f656204d6df4adad1d91fe1": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13790. RBF: Move ClientProtocol APIs to its own module. Contributed by Chao Sun.\n",
      "commitDate": "19/08/18 11:50 PM",
      "commitName": "6425ed27ea638da75f656204d6df4adad1d91fe1",
      "commitAuthor": "Brahma Reddy Battula",
      "diff": "@@ -0,0 +1,26 @@\n+  public DatanodeStorageReport[] getDatanodeStorageReport(\n+      HdfsConstants.DatanodeReportType type) throws IOException {\n+    rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED);\n+\n+    Map\u003cString, DatanodeStorageReport[]\u003e dnSubcluster \u003d\n+        rpcServer.getDatanodeStorageReportMap(type);\n+\n+    // Avoid repeating machines in multiple subclusters\n+    Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new LinkedHashMap\u003c\u003e();\n+    for (DatanodeStorageReport[] dns : dnSubcluster.values()) {\n+      for (DatanodeStorageReport dn : dns) {\n+        DatanodeInfo dnInfo \u003d dn.getDatanodeInfo();\n+        String nodeId \u003d dnInfo.getXferAddr();\n+        if (!datanodesMap.containsKey(nodeId)) {\n+          datanodesMap.put(nodeId, dn);\n+        }\n+        // TODO merge somehow, right now it just takes the first one\n+      }\n+    }\n+\n+    Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n+    DatanodeStorageReport[] combinedData \u003d\n+        new DatanodeStorageReport[datanodes.size()];\n+    combinedData \u003d datanodes.toArray(combinedData);\n+    return combinedData;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeStorageReport[] getDatanodeStorageReport(\n      HdfsConstants.DatanodeReportType type) throws IOException {\n    rpcServer.checkOperation(NameNode.OperationCategory.UNCHECKED);\n\n    Map\u003cString, DatanodeStorageReport[]\u003e dnSubcluster \u003d\n        rpcServer.getDatanodeStorageReportMap(type);\n\n    // Avoid repeating machines in multiple subclusters\n    Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new LinkedHashMap\u003c\u003e();\n    for (DatanodeStorageReport[] dns : dnSubcluster.values()) {\n      for (DatanodeStorageReport dn : dns) {\n        DatanodeInfo dnInfo \u003d dn.getDatanodeInfo();\n        String nodeId \u003d dnInfo.getXferAddr();\n        if (!datanodesMap.containsKey(nodeId)) {\n          datanodesMap.put(nodeId, dn);\n        }\n        // TODO merge somehow, right now it just takes the first one\n      }\n    }\n\n    Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n    DatanodeStorageReport[] combinedData \u003d\n        new DatanodeStorageReport[datanodes.size()];\n    combinedData \u003d datanodes.toArray(combinedData);\n    return combinedData;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java"
    }
  }
}