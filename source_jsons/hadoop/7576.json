{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "NameNode.java",
  "functionName": "createNameNode",
  "functionId": "createNameNode___argv-String[]__conf-Configuration",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
  "functionStartLine": 1694,
  "functionEndLine": 1758,
  "numCommitsSeen": 231,
  "timeTaken": 10219,
  "changeHistory": [
    "3b22fcd377eecedacceb6e37368463b48e0133c8",
    "8acb376c9c5f7f52a097be221ed18877a403bece",
    "1dd79ffaca4b0c2cb0ab817dff3697686f3367e3",
    "5f9e52f7459d3dc4ac3a5febd1dc6e00829d30ed",
    "280bdb9a16a898118421aee16db11f52eed9bdae",
    "c780454413caffbc37a02c4252eb5ec7abe57f97",
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de",
    "54e612bfb9f877e58f7f153c43cb4147876826d3",
    "28e87740c5797e87a9038d7f151c8b21a5b04592",
    "231a52a7dfa73f7f302cc3bf671d433312373873",
    "f026d8bb1ba6f9b059db5526f1fb1261f818ffd0",
    "c93185df660aa4fbb7885794550177286f9f3029",
    "cdae6953e80e81693bb4c9eb38b62eaba3ac8cf9",
    "3ffdb9152e780f4fd80ac7ea7b1a45ec583edc36",
    "706394d03992b394e9f907aff2155df493e4ea4e",
    "ba688e11c195327d3832610789fdd0cf81a3d0a1",
    "59eb544744f87aaa8966e30568dff9e8e183f342",
    "1a75ec82885e45baf4d5cd56d6c738d8e68d8bc7",
    "41e56dfecee0db1975c9859017c0de1226afb4b5",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "3b22fcd377eecedacceb6e37368463b48e0133c8": "Ybodychange",
    "8acb376c9c5f7f52a097be221ed18877a403bece": "Ybodychange",
    "1dd79ffaca4b0c2cb0ab817dff3697686f3367e3": "Ybodychange",
    "5f9e52f7459d3dc4ac3a5febd1dc6e00829d30ed": "Ybodychange",
    "280bdb9a16a898118421aee16db11f52eed9bdae": "Ybodychange",
    "c780454413caffbc37a02c4252eb5ec7abe57f97": "Ybodychange",
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de": "Ybodychange",
    "54e612bfb9f877e58f7f153c43cb4147876826d3": "Ybodychange",
    "28e87740c5797e87a9038d7f151c8b21a5b04592": "Ybodychange",
    "231a52a7dfa73f7f302cc3bf671d433312373873": "Ybodychange",
    "f026d8bb1ba6f9b059db5526f1fb1261f818ffd0": "Ybodychange",
    "c93185df660aa4fbb7885794550177286f9f3029": "Ybodychange",
    "cdae6953e80e81693bb4c9eb38b62eaba3ac8cf9": "Ybodychange",
    "3ffdb9152e780f4fd80ac7ea7b1a45ec583edc36": "Ybodychange",
    "706394d03992b394e9f907aff2155df493e4ea4e": "Ybodychange",
    "ba688e11c195327d3832610789fdd0cf81a3d0a1": "Ybodychange",
    "59eb544744f87aaa8966e30568dff9e8e183f342": "Ybodychange",
    "1a75ec82885e45baf4d5cd56d6c738d8e68d8bc7": "Ybodychange",
    "41e56dfecee0db1975c9859017c0de1226afb4b5": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "3b22fcd377eecedacceb6e37368463b48e0133c8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14256. Review Logging of NameNode Class. Contributed by David Mollitor.\n",
      "commitDate": "29/08/19 2:21 PM",
      "commitName": "3b22fcd377eecedacceb6e37368463b48e0133c8",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "16/08/19 2:53 PM",
      "commitNameOld": "a38b9e137e67571d2df83a7a9505b66cffefa7c8",
      "commitAuthorOld": "hunshenshi",
      "daysBetweenCommits": 12.98,
      "commitsBetweenForRepo": 124,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,65 +1,65 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     LOG.info(\"createNameNode \" + Arrays.asList(argv));\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     // Parse out some generic args into Configuration.\n     GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n     argv \u003d hParser.getRemainingArgs();\n     // Parse the rest, NN specific args.\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage(System.err);\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n \n     boolean aborted \u003d false;\n     switch (startOpt) {\n     case FORMAT:\n       aborted \u003d format(conf, startOpt.getForceFormat(),\n           startOpt.getInteractiveFormat());\n       terminate(aborted ? 1 : 0);\n       return null; // avoid javac warning\n     case GENCLUSTERID:\n-      System.err.println(\"Generating new cluster id:\");\n-      System.out.println(NNStorage.newClusterID());\n+      String clusterID \u003d NNStorage.newClusterID();\n+      LOG.info(\"Generated new cluster id: {}\", clusterID);\n       terminate(0);\n       return null;\n     case ROLLBACK:\n       aborted \u003d doRollback(conf, true);\n       terminate(aborted ? 1 : 0);\n       return null; // avoid warning\n     case BOOTSTRAPSTANDBY:\n       String[] toolArgs \u003d Arrays.copyOfRange(argv, 1, argv.length);\n       int rc \u003d BootstrapStandby.run(toolArgs, conf);\n       terminate(rc);\n       return null; // avoid warning\n     case INITIALIZESHAREDEDITS:\n       aborted \u003d initializeSharedEdits(conf,\n           startOpt.getForceFormat(),\n           startOpt.getInteractiveFormat());\n       terminate(aborted ? 1 : 0);\n       return null; // avoid warning\n     case BACKUP:\n     case CHECKPOINT:\n       NamenodeRole role \u003d startOpt.toNodeRole();\n       DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n       return new BackupNode(conf, role);\n     case RECOVER:\n       NameNode.doRecovery(startOpt, conf);\n       return null;\n     case METADATAVERSION:\n       printMetadataVersion(conf);\n       terminate(0);\n       return null; // avoid javac warning\n     case UPGRADEONLY:\n       DefaultMetricsSystem.initialize(\"NameNode\");\n       new NameNode(conf);\n       terminate(0);\n       return null;\n     default:\n       DefaultMetricsSystem.initialize(\"NameNode\");\n       return new NameNode(conf);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    LOG.info(\"createNameNode \" + Arrays.asList(argv));\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    // Parse out some generic args into Configuration.\n    GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n    argv \u003d hParser.getRemainingArgs();\n    // Parse the rest, NN specific args.\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n\n    boolean aborted \u003d false;\n    switch (startOpt) {\n    case FORMAT:\n      aborted \u003d format(conf, startOpt.getForceFormat(),\n          startOpt.getInteractiveFormat());\n      terminate(aborted ? 1 : 0);\n      return null; // avoid javac warning\n    case GENCLUSTERID:\n      String clusterID \u003d NNStorage.newClusterID();\n      LOG.info(\"Generated new cluster id: {}\", clusterID);\n      terminate(0);\n      return null;\n    case ROLLBACK:\n      aborted \u003d doRollback(conf, true);\n      terminate(aborted ? 1 : 0);\n      return null; // avoid warning\n    case BOOTSTRAPSTANDBY:\n      String[] toolArgs \u003d Arrays.copyOfRange(argv, 1, argv.length);\n      int rc \u003d BootstrapStandby.run(toolArgs, conf);\n      terminate(rc);\n      return null; // avoid warning\n    case INITIALIZESHAREDEDITS:\n      aborted \u003d initializeSharedEdits(conf,\n          startOpt.getForceFormat(),\n          startOpt.getInteractiveFormat());\n      terminate(aborted ? 1 : 0);\n      return null; // avoid warning\n    case BACKUP:\n    case CHECKPOINT:\n      NamenodeRole role \u003d startOpt.toNodeRole();\n      DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n      return new BackupNode(conf, role);\n    case RECOVER:\n      NameNode.doRecovery(startOpt, conf);\n      return null;\n    case METADATAVERSION:\n      printMetadataVersion(conf);\n      terminate(0);\n      return null; // avoid javac warning\n    case UPGRADEONLY:\n      DefaultMetricsSystem.initialize(\"NameNode\");\n      new NameNode(conf);\n      terminate(0);\n      return null;\n    default:\n      DefaultMetricsSystem.initialize(\"NameNode\");\n      return new NameNode(conf);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "8acb376c9c5f7f52a097be221ed18877a403bece": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11238. Fix checkstyle warnings in NameNode#createNameNode. Contributed by Ethan Li.\n",
      "commitDate": "14/02/17 11:53 PM",
      "commitName": "8acb376c9c5f7f52a097be221ed18877a403bece",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "24/01/17 4:58 PM",
      "commitNameOld": "b57368b6f893cb27d77fc9425e116f1312f4790f",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 21.29,
      "commitsBetweenForRepo": 92,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,65 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     LOG.info(\"createNameNode \" + Arrays.asList(argv));\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     // Parse out some generic args into Configuration.\n     GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n     argv \u003d hParser.getRemainingArgs();\n     // Parse the rest, NN specific args.\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage(System.err);\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n \n+    boolean aborted \u003d false;\n     switch (startOpt) {\n-      case FORMAT: {\n-        boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n-            startOpt.getInteractiveFormat());\n-        terminate(aborted ? 1 : 0);\n-        return null; // avoid javac warning\n-      }\n-      case GENCLUSTERID: {\n-        System.err.println(\"Generating new cluster id:\");\n-        System.out.println(NNStorage.newClusterID());\n-        terminate(0);\n-        return null;\n-      }\n-      case ROLLBACK: {\n-        boolean aborted \u003d doRollback(conf, true);\n-        terminate(aborted ? 1 : 0);\n-        return null; // avoid warning\n-      }\n-      case BOOTSTRAPSTANDBY: {\n-        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n-        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n-        terminate(rc);\n-        return null; // avoid warning\n-      }\n-      case INITIALIZESHAREDEDITS: {\n-        boolean aborted \u003d initializeSharedEdits(conf,\n-            startOpt.getForceFormat(),\n-            startOpt.getInteractiveFormat());\n-        terminate(aborted ? 1 : 0);\n-        return null; // avoid warning\n-      }\n-      case BACKUP:\n-      case CHECKPOINT: {\n-        NamenodeRole role \u003d startOpt.toNodeRole();\n-        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n-        return new BackupNode(conf, role);\n-      }\n-      case RECOVER: {\n-        NameNode.doRecovery(startOpt, conf);\n-        return null;\n-      }\n-      case METADATAVERSION: {\n-        printMetadataVersion(conf);\n-        terminate(0);\n-        return null; // avoid javac warning\n-      }\n-      case UPGRADEONLY: {\n-        DefaultMetricsSystem.initialize(\"NameNode\");\n-        new NameNode(conf);\n-        terminate(0);\n-        return null;\n-      }\n-      default: {\n-        DefaultMetricsSystem.initialize(\"NameNode\");\n-        return new NameNode(conf);\n-      }\n+    case FORMAT:\n+      aborted \u003d format(conf, startOpt.getForceFormat(),\n+          startOpt.getInteractiveFormat());\n+      terminate(aborted ? 1 : 0);\n+      return null; // avoid javac warning\n+    case GENCLUSTERID:\n+      System.err.println(\"Generating new cluster id:\");\n+      System.out.println(NNStorage.newClusterID());\n+      terminate(0);\n+      return null;\n+    case ROLLBACK:\n+      aborted \u003d doRollback(conf, true);\n+      terminate(aborted ? 1 : 0);\n+      return null; // avoid warning\n+    case BOOTSTRAPSTANDBY:\n+      String[] toolArgs \u003d Arrays.copyOfRange(argv, 1, argv.length);\n+      int rc \u003d BootstrapStandby.run(toolArgs, conf);\n+      terminate(rc);\n+      return null; // avoid warning\n+    case INITIALIZESHAREDEDITS:\n+      aborted \u003d initializeSharedEdits(conf,\n+          startOpt.getForceFormat(),\n+          startOpt.getInteractiveFormat());\n+      terminate(aborted ? 1 : 0);\n+      return null; // avoid warning\n+    case BACKUP:\n+    case CHECKPOINT:\n+      NamenodeRole role \u003d startOpt.toNodeRole();\n+      DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n+      return new BackupNode(conf, role);\n+    case RECOVER:\n+      NameNode.doRecovery(startOpt, conf);\n+      return null;\n+    case METADATAVERSION:\n+      printMetadataVersion(conf);\n+      terminate(0);\n+      return null; // avoid javac warning\n+    case UPGRADEONLY:\n+      DefaultMetricsSystem.initialize(\"NameNode\");\n+      new NameNode(conf);\n+      terminate(0);\n+      return null;\n+    default:\n+      DefaultMetricsSystem.initialize(\"NameNode\");\n+      return new NameNode(conf);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    LOG.info(\"createNameNode \" + Arrays.asList(argv));\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    // Parse out some generic args into Configuration.\n    GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n    argv \u003d hParser.getRemainingArgs();\n    // Parse the rest, NN specific args.\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n\n    boolean aborted \u003d false;\n    switch (startOpt) {\n    case FORMAT:\n      aborted \u003d format(conf, startOpt.getForceFormat(),\n          startOpt.getInteractiveFormat());\n      terminate(aborted ? 1 : 0);\n      return null; // avoid javac warning\n    case GENCLUSTERID:\n      System.err.println(\"Generating new cluster id:\");\n      System.out.println(NNStorage.newClusterID());\n      terminate(0);\n      return null;\n    case ROLLBACK:\n      aborted \u003d doRollback(conf, true);\n      terminate(aborted ? 1 : 0);\n      return null; // avoid warning\n    case BOOTSTRAPSTANDBY:\n      String[] toolArgs \u003d Arrays.copyOfRange(argv, 1, argv.length);\n      int rc \u003d BootstrapStandby.run(toolArgs, conf);\n      terminate(rc);\n      return null; // avoid warning\n    case INITIALIZESHAREDEDITS:\n      aborted \u003d initializeSharedEdits(conf,\n          startOpt.getForceFormat(),\n          startOpt.getInteractiveFormat());\n      terminate(aborted ? 1 : 0);\n      return null; // avoid warning\n    case BACKUP:\n    case CHECKPOINT:\n      NamenodeRole role \u003d startOpt.toNodeRole();\n      DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n      return new BackupNode(conf, role);\n    case RECOVER:\n      NameNode.doRecovery(startOpt, conf);\n      return null;\n    case METADATAVERSION:\n      printMetadataVersion(conf);\n      terminate(0);\n      return null; // avoid javac warning\n    case UPGRADEONLY:\n      DefaultMetricsSystem.initialize(\"NameNode\");\n      new NameNode(conf);\n      terminate(0);\n      return null;\n    default:\n      DefaultMetricsSystem.initialize(\"NameNode\");\n      return new NameNode(conf);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "1dd79ffaca4b0c2cb0ab817dff3697686f3367e3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8241. Remove unused NameNode startup option -finalize. Contributed by Brahma Reddy Battula.\n",
      "commitDate": "11/05/15 8:18 AM",
      "commitName": "1dd79ffaca4b0c2cb0ab817dff3697686f3367e3",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 8.93,
      "commitsBetweenForRepo": 139,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,81 +1,74 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     LOG.info(\"createNameNode \" + Arrays.asList(argv));\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     // Parse out some generic args into Configuration.\n     GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n     argv \u003d hParser.getRemainingArgs();\n     // Parse the rest, NN specific args.\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage(System.err);\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n \n     switch (startOpt) {\n       case FORMAT: {\n         boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case GENCLUSTERID: {\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         terminate(0);\n         return null;\n       }\n-      case FINALIZE: {\n-        System.err.println(\"Use of the argument \u0027\" + StartupOption.FINALIZE +\n-            \"\u0027 is no longer supported. To finalize an upgrade, start the NN \" +\n-            \" and then run `hdfs dfsadmin -finalizeUpgrade\u0027\");\n-        terminate(1);\n-        return null; // avoid javac warning\n-      }\n       case ROLLBACK: {\n         boolean aborted \u003d doRollback(conf, true);\n         terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BOOTSTRAPSTANDBY: {\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n         terminate(rc);\n         return null; // avoid warning\n       }\n       case INITIALIZESHAREDEDITS: {\n         boolean aborted \u003d initializeSharedEdits(conf,\n             startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BACKUP:\n       case CHECKPOINT: {\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       }\n       case RECOVER: {\n         NameNode.doRecovery(startOpt, conf);\n         return null;\n       }\n       case METADATAVERSION: {\n         printMetadataVersion(conf);\n         terminate(0);\n         return null; // avoid javac warning\n       }\n       case UPGRADEONLY: {\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         new NameNode(conf);\n         terminate(0);\n         return null;\n       }\n       default: {\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    LOG.info(\"createNameNode \" + Arrays.asList(argv));\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    // Parse out some generic args into Configuration.\n    GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n    argv \u003d hParser.getRemainingArgs();\n    // Parse the rest, NN specific args.\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case ROLLBACK: {\n        boolean aborted \u003d doRollback(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted \u003d initializeSharedEdits(conf,\n            startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      case METADATAVERSION: {\n        printMetadataVersion(conf);\n        terminate(0);\n        return null; // avoid javac warning\n      }\n      case UPGRADEONLY: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        new NameNode(conf);\n        terminate(0);\n        return null;\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "5f9e52f7459d3dc4ac3a5febd1dc6e00829d30ed": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6597. Add a new option to NN upgrade to terminate the process after upgrade on NN is completed. Contributed by Danilo Vunjak.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1611723 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/07/14 10:14 AM",
      "commitName": "5f9e52f7459d3dc4ac3a5febd1dc6e00829d30ed",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "26/06/14 4:12 PM",
      "commitNameOld": "280bdb9a16a898118421aee16db11f52eed9bdae",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 21.75,
      "commitsBetweenForRepo": 143,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,81 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     LOG.info(\"createNameNode \" + Arrays.asList(argv));\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     // Parse out some generic args into Configuration.\n     GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n     argv \u003d hParser.getRemainingArgs();\n     // Parse the rest, NN specific args.\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage(System.err);\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n \n     switch (startOpt) {\n       case FORMAT: {\n         boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case GENCLUSTERID: {\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         terminate(0);\n         return null;\n       }\n       case FINALIZE: {\n         System.err.println(\"Use of the argument \u0027\" + StartupOption.FINALIZE +\n             \"\u0027 is no longer supported. To finalize an upgrade, start the NN \" +\n             \" and then run `hdfs dfsadmin -finalizeUpgrade\u0027\");\n         terminate(1);\n         return null; // avoid javac warning\n       }\n       case ROLLBACK: {\n         boolean aborted \u003d doRollback(conf, true);\n         terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BOOTSTRAPSTANDBY: {\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n         terminate(rc);\n         return null; // avoid warning\n       }\n       case INITIALIZESHAREDEDITS: {\n         boolean aborted \u003d initializeSharedEdits(conf,\n             startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BACKUP:\n       case CHECKPOINT: {\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       }\n       case RECOVER: {\n         NameNode.doRecovery(startOpt, conf);\n         return null;\n       }\n       case METADATAVERSION: {\n         printMetadataVersion(conf);\n         terminate(0);\n         return null; // avoid javac warning\n       }\n+      case UPGRADEONLY: {\n+        DefaultMetricsSystem.initialize(\"NameNode\");\n+        new NameNode(conf);\n+        terminate(0);\n+        return null;\n+      }\n       default: {\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    LOG.info(\"createNameNode \" + Arrays.asList(argv));\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    // Parse out some generic args into Configuration.\n    GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n    argv \u003d hParser.getRemainingArgs();\n    // Parse the rest, NN specific args.\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case FINALIZE: {\n        System.err.println(\"Use of the argument \u0027\" + StartupOption.FINALIZE +\n            \"\u0027 is no longer supported. To finalize an upgrade, start the NN \" +\n            \" and then run `hdfs dfsadmin -finalizeUpgrade\u0027\");\n        terminate(1);\n        return null; // avoid javac warning\n      }\n      case ROLLBACK: {\n        boolean aborted \u003d doRollback(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted \u003d initializeSharedEdits(conf,\n            startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      case METADATAVERSION: {\n        printMetadataVersion(conf);\n        terminate(0);\n        return null; // avoid javac warning\n      }\n      case UPGRADEONLY: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        new NameNode(conf);\n        terminate(0);\n        return null;\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "280bdb9a16a898118421aee16db11f52eed9bdae": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6572. Add an option to the NameNode that prints the software and on-disk image versions. Contributed by Charles Lamb.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1605928 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/06/14 4:12 PM",
      "commitName": "280bdb9a16a898118421aee16db11f52eed9bdae",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "20/06/14 11:54 AM",
      "commitNameOld": "9ca79e8d327e95845ef9794396afd43a52bc3d40",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 6.18,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,75 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     LOG.info(\"createNameNode \" + Arrays.asList(argv));\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     // Parse out some generic args into Configuration.\n     GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n     argv \u003d hParser.getRemainingArgs();\n     // Parse the rest, NN specific args.\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage(System.err);\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n \n     switch (startOpt) {\n       case FORMAT: {\n         boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case GENCLUSTERID: {\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         terminate(0);\n         return null;\n       }\n       case FINALIZE: {\n         System.err.println(\"Use of the argument \u0027\" + StartupOption.FINALIZE +\n             \"\u0027 is no longer supported. To finalize an upgrade, start the NN \" +\n             \" and then run `hdfs dfsadmin -finalizeUpgrade\u0027\");\n         terminate(1);\n         return null; // avoid javac warning\n       }\n       case ROLLBACK: {\n         boolean aborted \u003d doRollback(conf, true);\n         terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BOOTSTRAPSTANDBY: {\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n         terminate(rc);\n         return null; // avoid warning\n       }\n       case INITIALIZESHAREDEDITS: {\n         boolean aborted \u003d initializeSharedEdits(conf,\n             startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BACKUP:\n       case CHECKPOINT: {\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       }\n       case RECOVER: {\n         NameNode.doRecovery(startOpt, conf);\n         return null;\n       }\n+      case METADATAVERSION: {\n+        printMetadataVersion(conf);\n+        terminate(0);\n+        return null; // avoid javac warning\n+      }\n       default: {\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    LOG.info(\"createNameNode \" + Arrays.asList(argv));\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    // Parse out some generic args into Configuration.\n    GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n    argv \u003d hParser.getRemainingArgs();\n    // Parse the rest, NN specific args.\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case FINALIZE: {\n        System.err.println(\"Use of the argument \u0027\" + StartupOption.FINALIZE +\n            \"\u0027 is no longer supported. To finalize an upgrade, start the NN \" +\n            \" and then run `hdfs dfsadmin -finalizeUpgrade\u0027\");\n        terminate(1);\n        return null; // avoid javac warning\n      }\n      case ROLLBACK: {\n        boolean aborted \u003d doRollback(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted \u003d initializeSharedEdits(conf,\n            startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      case METADATAVERSION: {\n        printMetadataVersion(conf);\n        terminate(0);\n        return null; // avoid javac warning\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "c780454413caffbc37a02c4252eb5ec7abe57f97": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5869. When starting rolling upgrade or NN restarts, NN should create a checkpoint right before the upgrade marker.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1565516 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/02/14 6:08 PM",
      "commitName": "c780454413caffbc37a02c4252eb5ec7abe57f97",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "26/01/14 8:34 AM",
      "commitNameOld": "a9110e178837bdcd236e528875daa3651e13dacc",
      "commitAuthorOld": "",
      "daysBetweenCommits": 11.4,
      "commitsBetweenForRepo": 65,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,70 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n+    LOG.info(\"createNameNode \" + Arrays.asList(argv));\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     // Parse out some generic args into Configuration.\n     GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n     argv \u003d hParser.getRemainingArgs();\n     // Parse the rest, NN specific args.\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage(System.err);\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n \n     switch (startOpt) {\n       case FORMAT: {\n         boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case GENCLUSTERID: {\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         terminate(0);\n         return null;\n       }\n       case FINALIZE: {\n         System.err.println(\"Use of the argument \u0027\" + StartupOption.FINALIZE +\n             \"\u0027 is no longer supported. To finalize an upgrade, start the NN \" +\n             \" and then run `hdfs dfsadmin -finalizeUpgrade\u0027\");\n         terminate(1);\n         return null; // avoid javac warning\n       }\n       case ROLLBACK: {\n         boolean aborted \u003d doRollback(conf, true);\n         terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BOOTSTRAPSTANDBY: {\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n         terminate(rc);\n         return null; // avoid warning\n       }\n       case INITIALIZESHAREDEDITS: {\n         boolean aborted \u003d initializeSharedEdits(conf,\n             startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BACKUP:\n       case CHECKPOINT: {\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       }\n       case RECOVER: {\n         NameNode.doRecovery(startOpt, conf);\n         return null;\n       }\n       default: {\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    LOG.info(\"createNameNode \" + Arrays.asList(argv));\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    // Parse out some generic args into Configuration.\n    GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n    argv \u003d hParser.getRemainingArgs();\n    // Parse the rest, NN specific args.\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case FINALIZE: {\n        System.err.println(\"Use of the argument \u0027\" + StartupOption.FINALIZE +\n            \"\u0027 is no longer supported. To finalize an upgrade, start the NN \" +\n            \" and then run `hdfs dfsadmin -finalizeUpgrade\u0027\");\n        terminate(1);\n        return null; // avoid javac warning\n      }\n      case ROLLBACK: {\n        boolean aborted \u003d doRollback(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted \u003d initializeSharedEdits(conf,\n            startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "edb6dc5f303093c2604cd07b0c0dacf12dbce5de": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5138. Support HDFS upgrade in HA. Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1561381 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/01/14 12:01 PM",
      "commitName": "edb6dc5f303093c2604cd07b0c0dacf12dbce5de",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "10/01/14 4:05 PM",
      "commitNameOld": "9d382a41743831fbcfecd302ead02095f36b7f59",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 14.83,
      "commitsBetweenForRepo": 72,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,69 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     // Parse out some generic args into Configuration.\n     GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n     argv \u003d hParser.getRemainingArgs();\n     // Parse the rest, NN specific args.\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage(System.err);\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n-    \n-    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n-        (startOpt \u003d\u003d StartupOption.UPGRADE ||\n-         startOpt \u003d\u003d StartupOption.ROLLBACK ||\n-         startOpt \u003d\u003d StartupOption.FINALIZE)) {\n-      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n-          \"Cannot perform DFS upgrade with HA enabled.\");\n-    }\n \n     switch (startOpt) {\n       case FORMAT: {\n         boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case GENCLUSTERID: {\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         terminate(0);\n         return null;\n       }\n       case FINALIZE: {\n-        boolean aborted \u003d finalize(conf, true);\n-        terminate(aborted ? 1 : 0);\n+        System.err.println(\"Use of the argument \u0027\" + StartupOption.FINALIZE +\n+            \"\u0027 is no longer supported. To finalize an upgrade, start the NN \" +\n+            \" and then run `hdfs dfsadmin -finalizeUpgrade\u0027\");\n+        terminate(1);\n         return null; // avoid javac warning\n       }\n+      case ROLLBACK: {\n+        boolean aborted \u003d doRollback(conf, true);\n+        terminate(aborted ? 1 : 0);\n+        return null; // avoid warning\n+      }\n       case BOOTSTRAPSTANDBY: {\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n         terminate(rc);\n         return null; // avoid warning\n       }\n       case INITIALIZESHAREDEDITS: {\n         boolean aborted \u003d initializeSharedEdits(conf,\n             startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BACKUP:\n       case CHECKPOINT: {\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       }\n       case RECOVER: {\n         NameNode.doRecovery(startOpt, conf);\n         return null;\n       }\n       default: {\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    // Parse out some generic args into Configuration.\n    GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n    argv \u003d hParser.getRemainingArgs();\n    // Parse the rest, NN specific args.\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case FINALIZE: {\n        System.err.println(\"Use of the argument \u0027\" + StartupOption.FINALIZE +\n            \"\u0027 is no longer supported. To finalize an upgrade, start the NN \" +\n            \" and then run `hdfs dfsadmin -finalizeUpgrade\u0027\");\n        terminate(1);\n        return null; // avoid javac warning\n      }\n      case ROLLBACK: {\n        boolean aborted \u003d doRollback(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted \u003d initializeSharedEdits(conf,\n            startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "54e612bfb9f877e58f7f153c43cb4147876826d3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2580. NameNode#main(...) can make use of GenericOptionsParser. Contributed by harsh. (harsh)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1379828 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/09/12 12:22 PM",
      "commitName": "54e612bfb9f877e58f7f153c43cb4147876826d3",
      "commitAuthor": "Harsh J",
      "commitDateOld": "27/08/12 6:41 PM",
      "commitNameOld": "b29cb2d99756c3ae56ab12fac38d95668b8eb2f1",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 4.74,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,70 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n+    // Parse out some generic args into Configuration.\n+    GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n+    argv \u003d hParser.getRemainingArgs();\n+    // Parse the rest, NN specific args.\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage(System.err);\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n     \n     if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n         (startOpt \u003d\u003d StartupOption.UPGRADE ||\n          startOpt \u003d\u003d StartupOption.ROLLBACK ||\n          startOpt \u003d\u003d StartupOption.FINALIZE)) {\n       throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n           \"Cannot perform DFS upgrade with HA enabled.\");\n     }\n \n     switch (startOpt) {\n       case FORMAT: {\n         boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case GENCLUSTERID: {\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         terminate(0);\n         return null;\n       }\n       case FINALIZE: {\n         boolean aborted \u003d finalize(conf, true);\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case BOOTSTRAPSTANDBY: {\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n         terminate(rc);\n         return null; // avoid warning\n       }\n       case INITIALIZESHAREDEDITS: {\n         boolean aborted \u003d initializeSharedEdits(conf,\n             startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BACKUP:\n       case CHECKPOINT: {\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       }\n       case RECOVER: {\n         NameNode.doRecovery(startOpt, conf);\n         return null;\n       }\n       default: {\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    // Parse out some generic args into Configuration.\n    GenericOptionsParser hParser \u003d new GenericOptionsParser(conf, argv);\n    argv \u003d hParser.getRemainingArgs();\n    // Parse the rest, NN specific args.\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n        (startOpt \u003d\u003d StartupOption.UPGRADE ||\n         startOpt \u003d\u003d StartupOption.ROLLBACK ||\n         startOpt \u003d\u003d StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case FINALIZE: {\n        boolean aborted \u003d finalize(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted \u003d initializeSharedEdits(conf,\n            startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "28e87740c5797e87a9038d7f151c8b21a5b04592": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3723. Add support -h, -help to all the commands. Contributed by Jing Zhao\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1373173 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/08/12 5:28 PM",
      "commitName": "28e87740c5797e87a9038d7f151c8b21a5b04592",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "14/08/12 5:25 PM",
      "commitNameOld": "231a52a7dfa73f7f302cc3bf671d433312373873",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,66 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n-      printUsage();\n+      printUsage(System.err);\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n     \n     if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n         (startOpt \u003d\u003d StartupOption.UPGRADE ||\n          startOpt \u003d\u003d StartupOption.ROLLBACK ||\n          startOpt \u003d\u003d StartupOption.FINALIZE)) {\n       throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n           \"Cannot perform DFS upgrade with HA enabled.\");\n     }\n \n     switch (startOpt) {\n       case FORMAT: {\n         boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case GENCLUSTERID: {\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         terminate(0);\n         return null;\n       }\n       case FINALIZE: {\n         boolean aborted \u003d finalize(conf, true);\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case BOOTSTRAPSTANDBY: {\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n         terminate(rc);\n         return null; // avoid warning\n       }\n       case INITIALIZESHAREDEDITS: {\n         boolean aborted \u003d initializeSharedEdits(conf,\n             startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BACKUP:\n       case CHECKPOINT: {\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       }\n       case RECOVER: {\n         NameNode.doRecovery(startOpt, conf);\n         return null;\n       }\n       default: {\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n        (startOpt \u003d\u003d StartupOption.UPGRADE ||\n         startOpt \u003d\u003d StartupOption.ROLLBACK ||\n         startOpt \u003d\u003d StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case FINALIZE: {\n        boolean aborted \u003d finalize(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted \u003d initializeSharedEdits(conf,\n            startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "231a52a7dfa73f7f302cc3bf671d433312373873": {
      "type": "Ybodychange",
      "commitMessage": "Reverting previous incomplete change r1373170 for HDFS-3723\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1373172 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/08/12 5:25 PM",
      "commitName": "231a52a7dfa73f7f302cc3bf671d433312373873",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "14/08/12 5:03 PM",
      "commitNameOld": "f026d8bb1ba6f9b059db5526f1fb1261f818ffd0",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.02,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,66 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n-      printUsage(System.err);\n+      printUsage();\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n     \n     if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n         (startOpt \u003d\u003d StartupOption.UPGRADE ||\n          startOpt \u003d\u003d StartupOption.ROLLBACK ||\n          startOpt \u003d\u003d StartupOption.FINALIZE)) {\n       throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n           \"Cannot perform DFS upgrade with HA enabled.\");\n     }\n \n     switch (startOpt) {\n       case FORMAT: {\n         boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case GENCLUSTERID: {\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         terminate(0);\n         return null;\n       }\n       case FINALIZE: {\n         boolean aborted \u003d finalize(conf, true);\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case BOOTSTRAPSTANDBY: {\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n         terminate(rc);\n         return null; // avoid warning\n       }\n       case INITIALIZESHAREDEDITS: {\n         boolean aborted \u003d initializeSharedEdits(conf,\n             startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BACKUP:\n       case CHECKPOINT: {\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       }\n       case RECOVER: {\n         NameNode.doRecovery(startOpt, conf);\n         return null;\n       }\n       default: {\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage();\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n        (startOpt \u003d\u003d StartupOption.UPGRADE ||\n         startOpt \u003d\u003d StartupOption.ROLLBACK ||\n         startOpt \u003d\u003d StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case FINALIZE: {\n        boolean aborted \u003d finalize(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted \u003d initializeSharedEdits(conf,\n            startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "f026d8bb1ba6f9b059db5526f1fb1261f818ffd0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3723. Add support -h, -help to all the commands. Contributed by Jing Zhao.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1373170 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/08/12 5:03 PM",
      "commitName": "f026d8bb1ba6f9b059db5526f1fb1261f818ffd0",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "14/08/12 1:11 PM",
      "commitNameOld": "b38bd555e837569672dfd48ea1b60e60efc71648",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.16,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,66 +1,66 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n-      printUsage();\n+      printUsage(System.err);\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n     \n     if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n         (startOpt \u003d\u003d StartupOption.UPGRADE ||\n          startOpt \u003d\u003d StartupOption.ROLLBACK ||\n          startOpt \u003d\u003d StartupOption.FINALIZE)) {\n       throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n           \"Cannot perform DFS upgrade with HA enabled.\");\n     }\n \n     switch (startOpt) {\n       case FORMAT: {\n         boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case GENCLUSTERID: {\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         terminate(0);\n         return null;\n       }\n       case FINALIZE: {\n         boolean aborted \u003d finalize(conf, true);\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case BOOTSTRAPSTANDBY: {\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n         terminate(rc);\n         return null; // avoid warning\n       }\n       case INITIALIZESHAREDEDITS: {\n         boolean aborted \u003d initializeSharedEdits(conf,\n             startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BACKUP:\n       case CHECKPOINT: {\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       }\n       case RECOVER: {\n         NameNode.doRecovery(startOpt, conf);\n         return null;\n       }\n       default: {\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage(System.err);\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n        (startOpt \u003d\u003d StartupOption.UPGRADE ||\n         startOpt \u003d\u003d StartupOption.ROLLBACK ||\n         startOpt \u003d\u003d StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case FINALIZE: {\n        boolean aborted \u003d finalize(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted \u003d initializeSharedEdits(conf,\n            startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "c93185df660aa4fbb7885794550177286f9f3029": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3276. initializeSharedEdits should have a -nonInteractive flag. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1372628 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/08/12 2:26 PM",
      "commitName": "c93185df660aa4fbb7885794550177286f9f3029",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "09/08/12 3:13 PM",
      "commitNameOld": "9d0f8792a9a1d3d2b24adfc2c213247a099e7ad1",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 3.97,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,66 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage();\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n     \n     if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n         (startOpt \u003d\u003d StartupOption.UPGRADE ||\n          startOpt \u003d\u003d StartupOption.ROLLBACK ||\n          startOpt \u003d\u003d StartupOption.FINALIZE)) {\n       throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n           \"Cannot perform DFS upgrade with HA enabled.\");\n     }\n \n     switch (startOpt) {\n       case FORMAT: {\n         boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case GENCLUSTERID: {\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         terminate(0);\n         return null;\n       }\n       case FINALIZE: {\n         boolean aborted \u003d finalize(conf, true);\n         terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case BOOTSTRAPSTANDBY: {\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n         terminate(rc);\n         return null; // avoid warning\n       }\n       case INITIALIZESHAREDEDITS: {\n-        boolean aborted \u003d initializeSharedEdits(conf, false, true);\n+        boolean aborted \u003d initializeSharedEdits(conf,\n+            startOpt.getForceFormat(),\n+            startOpt.getInteractiveFormat());\n         terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BACKUP:\n       case CHECKPOINT: {\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       }\n       case RECOVER: {\n         NameNode.doRecovery(startOpt, conf);\n         return null;\n       }\n       default: {\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage();\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n        (startOpt \u003d\u003d StartupOption.UPGRADE ||\n         startOpt \u003d\u003d StartupOption.ROLLBACK ||\n         startOpt \u003d\u003d StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case FINALIZE: {\n        boolean aborted \u003d finalize(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted \u003d initializeSharedEdits(conf,\n            startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "cdae6953e80e81693bb4c9eb38b62eaba3ac8cf9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3582. Hook daemon process exit for testing. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1360329 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/07/12 10:58 AM",
      "commitName": "cdae6953e80e81693bb4c9eb38b62eaba3ac8cf9",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "02/07/12 11:21 AM",
      "commitNameOld": "7accbabdee0b7619ff83514c173e815d290b33bf",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 8.98,
      "commitsBetweenForRepo": 61,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,64 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage();\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n     \n     if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n         (startOpt \u003d\u003d StartupOption.UPGRADE ||\n          startOpt \u003d\u003d StartupOption.ROLLBACK ||\n          startOpt \u003d\u003d StartupOption.FINALIZE)) {\n       throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n           \"Cannot perform DFS upgrade with HA enabled.\");\n     }\n \n     switch (startOpt) {\n       case FORMAT: {\n         boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n             startOpt.getInteractiveFormat());\n-        System.exit(aborted ? 1 : 0);\n+        terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case GENCLUSTERID: {\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n-        System.exit(0);\n+        terminate(0);\n         return null;\n       }\n       case FINALIZE: {\n         boolean aborted \u003d finalize(conf, true);\n-        System.exit(aborted ? 1 : 0);\n+        terminate(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case BOOTSTRAPSTANDBY: {\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n-        System.exit(rc);\n+        terminate(rc);\n         return null; // avoid warning\n       }\n       case INITIALIZESHAREDEDITS: {\n         boolean aborted \u003d initializeSharedEdits(conf, false, true);\n-        System.exit(aborted ? 1 : 0);\n+        terminate(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BACKUP:\n       case CHECKPOINT: {\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       }\n       case RECOVER: {\n         NameNode.doRecovery(startOpt, conf);\n         return null;\n       }\n-      default:\n+      default: {\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n+      }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage();\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n        (startOpt \u003d\u003d StartupOption.UPGRADE ||\n         startOpt \u003d\u003d StartupOption.ROLLBACK ||\n         startOpt \u003d\u003d StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        terminate(0);\n        return null;\n      }\n      case FINALIZE: {\n        boolean aborted \u003d finalize(conf, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        terminate(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted \u003d initializeSharedEdits(conf, false, true);\n        terminate(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default: {\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "3ffdb9152e780f4fd80ac7ea7b1a45ec583edc36": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3094. add -nonInteractive and -force option to namenode -format command. Contributed by Arpit Gupta.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1312025 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/04/12 3:22 PM",
      "commitName": "3ffdb9152e780f4fd80ac7ea7b1a45ec583edc36",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "09/04/12 7:17 PM",
      "commitNameOld": "9597c81f35c17f6ee8bd2cef85f76af306478e6b",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 0.84,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,63 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage();\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n     \n     if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n         (startOpt \u003d\u003d StartupOption.UPGRADE ||\n          startOpt \u003d\u003d StartupOption.ROLLBACK ||\n          startOpt \u003d\u003d StartupOption.FINALIZE)) {\n       throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n           \"Cannot perform DFS upgrade with HA enabled.\");\n     }\n \n     switch (startOpt) {\n       case FORMAT: {\n-        boolean aborted \u003d format(conf, false);\n+        boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n+            startOpt.getInteractiveFormat());\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case GENCLUSTERID: {\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         System.exit(0);\n         return null;\n       }\n       case FINALIZE: {\n         boolean aborted \u003d finalize(conf, true);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case BOOTSTRAPSTANDBY: {\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n         System.exit(rc);\n         return null; // avoid warning\n       }\n       case INITIALIZESHAREDEDITS: {\n         boolean aborted \u003d initializeSharedEdits(conf, false, true);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BACKUP:\n       case CHECKPOINT: {\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       }\n       case RECOVER: {\n         NameNode.doRecovery(startOpt, conf);\n         return null;\n       }\n       default:\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage();\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n        (startOpt \u003d\u003d StartupOption.UPGRADE ||\n         startOpt \u003d\u003d StartupOption.ROLLBACK ||\n         startOpt \u003d\u003d StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted \u003d format(conf, startOpt.getForceFormat(),\n            startOpt.getInteractiveFormat());\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        System.exit(0);\n        return null;\n      }\n      case FINALIZE: {\n        boolean aborted \u003d finalize(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        System.exit(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted \u003d initializeSharedEdits(conf, false, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default:\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "706394d03992b394e9f907aff2155df493e4ea4e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3004. Implement Recovery Mode. Contributed by Colin Patrick McCabe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1311394 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/04/12 12:39 PM",
      "commitName": "706394d03992b394e9f907aff2155df493e4ea4e",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "05/04/12 9:56 PM",
      "commitNameOld": "d483b6f3fc1128cd98f00b1801f22deaaae7eec0",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 3.61,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,62 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage();\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n     \n     if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n         (startOpt \u003d\u003d StartupOption.UPGRADE ||\n          startOpt \u003d\u003d StartupOption.ROLLBACK ||\n          startOpt \u003d\u003d StartupOption.FINALIZE)) {\n       throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n           \"Cannot perform DFS upgrade with HA enabled.\");\n     }\n \n     switch (startOpt) {\n       case FORMAT: {\n         boolean aborted \u003d format(conf, false);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case GENCLUSTERID: {\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         System.exit(0);\n         return null;\n       }\n       case FINALIZE: {\n         boolean aborted \u003d finalize(conf, true);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       }\n       case BOOTSTRAPSTANDBY: {\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n         System.exit(rc);\n         return null; // avoid warning\n       }\n       case INITIALIZESHAREDEDITS: {\n         boolean aborted \u003d initializeSharedEdits(conf, false, true);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid warning\n       }\n       case BACKUP:\n       case CHECKPOINT: {\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       }\n+      case RECOVER: {\n+        NameNode.doRecovery(startOpt, conf);\n+        return null;\n+      }\n       default:\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage();\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n        (startOpt \u003d\u003d StartupOption.UPGRADE ||\n         startOpt \u003d\u003d StartupOption.ROLLBACK ||\n         startOpt \u003d\u003d StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted \u003d format(conf, false);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        System.exit(0);\n        return null;\n      }\n      case FINALIZE: {\n        boolean aborted \u003d finalize(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        System.exit(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted \u003d initializeSharedEdits(conf, false, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      case RECOVER: {\n        NameNode.doRecovery(startOpt, conf);\n        return null;\n      }\n      default:\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "ba688e11c195327d3832610789fdd0cf81a3d0a1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3102. Add CLI tool to initialize the shared-edits dir. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1309580 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/04/12 1:15 PM",
      "commitName": "ba688e11c195327d3832610789fdd0cf81a3d0a1",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "03/04/12 7:51 PM",
      "commitNameOld": "8c0366bf103ca638b5ef9e962671f7728db4fd10",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.72,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,58 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage();\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n     \n     if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n         (startOpt \u003d\u003d StartupOption.UPGRADE ||\n          startOpt \u003d\u003d StartupOption.ROLLBACK ||\n          startOpt \u003d\u003d StartupOption.FINALIZE)) {\n       throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n           \"Cannot perform DFS upgrade with HA enabled.\");\n     }\n \n     switch (startOpt) {\n-      case FORMAT:\n+      case FORMAT: {\n         boolean aborted \u003d format(conf, false);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid javac warning\n-      case GENCLUSTERID:\n+      }\n+      case GENCLUSTERID: {\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         System.exit(0);\n         return null;\n-      case FINALIZE:\n-        aborted \u003d finalize(conf, true);\n+      }\n+      case FINALIZE: {\n+        boolean aborted \u003d finalize(conf, true);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid javac warning\n-      case BOOTSTRAPSTANDBY:\n+      }\n+      case BOOTSTRAPSTANDBY: {\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n         System.exit(rc);\n         return null; // avoid warning\n+      }\n+      case INITIALIZESHAREDEDITS: {\n+        boolean aborted \u003d initializeSharedEdits(conf, false, true);\n+        System.exit(aborted ? 1 : 0);\n+        return null; // avoid warning\n+      }\n       case BACKUP:\n-      case CHECKPOINT:\n+      case CHECKPOINT: {\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n+      }\n       default:\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage();\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n        (startOpt \u003d\u003d StartupOption.UPGRADE ||\n         startOpt \u003d\u003d StartupOption.ROLLBACK ||\n         startOpt \u003d\u003d StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT: {\n        boolean aborted \u003d format(conf, false);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case GENCLUSTERID: {\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        System.exit(0);\n        return null;\n      }\n      case FINALIZE: {\n        boolean aborted \u003d finalize(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      }\n      case BOOTSTRAPSTANDBY: {\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        System.exit(rc);\n        return null; // avoid warning\n      }\n      case INITIALIZESHAREDEDITS: {\n        boolean aborted \u003d initializeSharedEdits(conf, false, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid warning\n      }\n      case BACKUP:\n      case CHECKPOINT: {\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      }\n      default:\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "59eb544744f87aaa8966e30568dff9e8e183f342": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3093. Fix bug where namenode -format interpreted the -force flag in reverse. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1300814 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/03/12 10:36 PM",
      "commitName": "59eb544744f87aaa8966e30568dff9e8e183f342",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "12/03/12 12:41 PM",
      "commitNameOld": "1a75ec82885e45baf4d5cd56d6c738d8e68d8bc7",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 2.41,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,48 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage();\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n     \n     if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n         (startOpt \u003d\u003d StartupOption.UPGRADE ||\n          startOpt \u003d\u003d StartupOption.ROLLBACK ||\n          startOpt \u003d\u003d StartupOption.FINALIZE)) {\n       throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n           \"Cannot perform DFS upgrade with HA enabled.\");\n     }\n \n     switch (startOpt) {\n       case FORMAT:\n-        boolean aborted \u003d format(conf, true);\n+        boolean aborted \u003d format(conf, false);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       case GENCLUSTERID:\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         System.exit(0);\n         return null;\n       case FINALIZE:\n         aborted \u003d finalize(conf, true);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       case BOOTSTRAPSTANDBY:\n         String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n         int rc \u003d BootstrapStandby.run(toolArgs, conf);\n         System.exit(rc);\n         return null; // avoid warning\n       case BACKUP:\n       case CHECKPOINT:\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       default:\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage();\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n        (startOpt \u003d\u003d StartupOption.UPGRADE ||\n         startOpt \u003d\u003d StartupOption.ROLLBACK ||\n         startOpt \u003d\u003d StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT:\n        boolean aborted \u003d format(conf, false);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      case GENCLUSTERID:\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        System.exit(0);\n        return null;\n      case FINALIZE:\n        aborted \u003d finalize(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      case BOOTSTRAPSTANDBY:\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        System.exit(rc);\n        return null; // avoid warning\n      case BACKUP:\n      case CHECKPOINT:\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      default:\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "1a75ec82885e45baf4d5cd56d6c738d8e68d8bc7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2731. Add command to bootstrap the Standby Node\u0027s name directories from the Active NameNode. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1299807 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/03/12 12:41 PM",
      "commitName": "1a75ec82885e45baf4d5cd56d6c738d8e68d8bc7",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "29/02/12 11:03 PM",
      "commitNameOld": "9318ff425019b9f88e154a80f3aeb23e6c69cb69",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 11.53,
      "commitsBetweenForRepo": 65,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,48 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage();\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n     \n     if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n         (startOpt \u003d\u003d StartupOption.UPGRADE ||\n          startOpt \u003d\u003d StartupOption.ROLLBACK ||\n          startOpt \u003d\u003d StartupOption.FINALIZE)) {\n       throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n           \"Cannot perform DFS upgrade with HA enabled.\");\n     }\n \n     switch (startOpt) {\n       case FORMAT:\n         boolean aborted \u003d format(conf, true);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       case GENCLUSTERID:\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         System.exit(0);\n         return null;\n       case FINALIZE:\n         aborted \u003d finalize(conf, true);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid javac warning\n+      case BOOTSTRAPSTANDBY:\n+        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n+        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n+        System.exit(rc);\n+        return null; // avoid warning\n       case BACKUP:\n       case CHECKPOINT:\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       default:\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage();\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n        (startOpt \u003d\u003d StartupOption.UPGRADE ||\n         startOpt \u003d\u003d StartupOption.ROLLBACK ||\n         startOpt \u003d\u003d StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT:\n        boolean aborted \u003d format(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      case GENCLUSTERID:\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        System.exit(0);\n        return null;\n      case FINALIZE:\n        aborted \u003d finalize(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      case BOOTSTRAPSTANDBY:\n        String toolArgs[] \u003d Arrays.copyOfRange(argv, 1, argv.length);\n        int rc \u003d BootstrapStandby.run(toolArgs, conf);\n        System.exit(rc);\n        return null; // avoid warning\n      case BACKUP:\n      case CHECKPOINT:\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      default:\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "41e56dfecee0db1975c9859017c0de1226afb4b5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2952. NN should not start with upgrade option or with a pending an unfinalized upgrade. Contributed by Aaron T. Myers.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1245875 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/02/12 11:12 PM",
      "commitName": "41e56dfecee0db1975c9859017c0de1226afb4b5",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "15/02/12 4:58 PM",
      "commitNameOld": "0663b51ed44db97740096a1002d2b63f3e17a3eb",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 2.26,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,43 @@\n   public static NameNode createNameNode(String argv[], Configuration conf)\n       throws IOException {\n     if (conf \u003d\u003d null)\n       conf \u003d new HdfsConfiguration();\n     StartupOption startOpt \u003d parseArguments(argv);\n     if (startOpt \u003d\u003d null) {\n       printUsage();\n       return null;\n     }\n     setStartupOption(conf, startOpt);\n+    \n+    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n+        (startOpt \u003d\u003d StartupOption.UPGRADE ||\n+         startOpt \u003d\u003d StartupOption.ROLLBACK ||\n+         startOpt \u003d\u003d StartupOption.FINALIZE)) {\n+      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n+          \"Cannot perform DFS upgrade with HA enabled.\");\n+    }\n \n     switch (startOpt) {\n       case FORMAT:\n         boolean aborted \u003d format(conf, true);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       case GENCLUSTERID:\n         System.err.println(\"Generating new cluster id:\");\n         System.out.println(NNStorage.newClusterID());\n         System.exit(0);\n         return null;\n       case FINALIZE:\n         aborted \u003d finalize(conf, true);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       case BACKUP:\n       case CHECKPOINT:\n         NamenodeRole role \u003d startOpt.toNodeRole();\n         DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n         return new BackupNode(conf, role);\n       default:\n         DefaultMetricsSystem.initialize(\"NameNode\");\n         return new NameNode(conf);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage();\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n    \n    if (HAUtil.isHAEnabled(conf, DFSUtil.getNamenodeNameServiceId(conf)) \u0026\u0026\n        (startOpt \u003d\u003d StartupOption.UPGRADE ||\n         startOpt \u003d\u003d StartupOption.ROLLBACK ||\n         startOpt \u003d\u003d StartupOption.FINALIZE)) {\n      throw new HadoopIllegalArgumentException(\"Invalid startup option. \" +\n          \"Cannot perform DFS upgrade with HA enabled.\");\n    }\n\n    switch (startOpt) {\n      case FORMAT:\n        boolean aborted \u003d format(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      case GENCLUSTERID:\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        System.exit(0);\n        return null;\n      case FINALIZE:\n        aborted \u003d finalize(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      case BACKUP:\n      case CHECKPOINT:\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      default:\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage();\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n\n    switch (startOpt) {\n      case FORMAT:\n        boolean aborted \u003d format(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      case GENCLUSTERID:\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        System.exit(0);\n        return null;\n      case FINALIZE:\n        aborted \u003d finalize(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      case BACKUP:\n      case CHECKPOINT:\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      default:\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage();\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n\n    switch (startOpt) {\n      case FORMAT:\n        boolean aborted \u003d format(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      case GENCLUSTERID:\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        System.exit(0);\n        return null;\n      case FINALIZE:\n        aborted \u003d finalize(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      case BACKUP:\n      case CHECKPOINT:\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      default:\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,35 @@\n+  public static NameNode createNameNode(String argv[], Configuration conf)\n+      throws IOException {\n+    if (conf \u003d\u003d null)\n+      conf \u003d new HdfsConfiguration();\n+    StartupOption startOpt \u003d parseArguments(argv);\n+    if (startOpt \u003d\u003d null) {\n+      printUsage();\n+      return null;\n+    }\n+    setStartupOption(conf, startOpt);\n+\n+    switch (startOpt) {\n+      case FORMAT:\n+        boolean aborted \u003d format(conf, true);\n+        System.exit(aborted ? 1 : 0);\n+        return null; // avoid javac warning\n+      case GENCLUSTERID:\n+        System.err.println(\"Generating new cluster id:\");\n+        System.out.println(NNStorage.newClusterID());\n+        System.exit(0);\n+        return null;\n+      case FINALIZE:\n+        aborted \u003d finalize(conf, true);\n+        System.exit(aborted ? 1 : 0);\n+        return null; // avoid javac warning\n+      case BACKUP:\n+      case CHECKPOINT:\n+        NamenodeRole role \u003d startOpt.toNodeRole();\n+        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n+        return new BackupNode(conf, role);\n+      default:\n+        DefaultMetricsSystem.initialize(\"NameNode\");\n+        return new NameNode(conf);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static NameNode createNameNode(String argv[], Configuration conf)\n      throws IOException {\n    if (conf \u003d\u003d null)\n      conf \u003d new HdfsConfiguration();\n    StartupOption startOpt \u003d parseArguments(argv);\n    if (startOpt \u003d\u003d null) {\n      printUsage();\n      return null;\n    }\n    setStartupOption(conf, startOpt);\n\n    switch (startOpt) {\n      case FORMAT:\n        boolean aborted \u003d format(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      case GENCLUSTERID:\n        System.err.println(\"Generating new cluster id:\");\n        System.out.println(NNStorage.newClusterID());\n        System.exit(0);\n        return null;\n      case FINALIZE:\n        aborted \u003d finalize(conf, true);\n        System.exit(aborted ? 1 : 0);\n        return null; // avoid javac warning\n      case BACKUP:\n      case CHECKPOINT:\n        NamenodeRole role \u003d startOpt.toNodeRole();\n        DefaultMetricsSystem.initialize(role.toString().replace(\" \", \"\"));\n        return new BackupNode(conf, role);\n      default:\n        DefaultMetricsSystem.initialize(\"NameNode\");\n        return new NameNode(conf);\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java"
    }
  }
}