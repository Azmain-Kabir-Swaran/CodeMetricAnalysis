{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "S3AInputStream.java",
  "functionName": "closeStream",
  "functionId": "closeStream___reason-String__length-long__forceAbort-boolean",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java",
  "functionStartLine": 552,
  "functionEndLine": 601,
  "numCommitsSeen": 42,
  "timeTaken": 4169,
  "changeHistory": [
    "e553eda9cd492ddc2b3aebe913913005e7b387c9",
    "2382f63fc0bb4108f3f3c542b4be7c04fbedd7c4",
    "72993b33b704991f2a0bf743f31b164e58a2dabc",
    "c6a39232456fa0c98b2b9b6dbeaec762294ca01e",
    "4ee3543625c77c06d566fe81644d21c607d6d74d",
    "27c4e90efce04e1b1302f668b5eb22412e00d033",
    "b9e3eff62a7415d8666656a75db69ff3e43f8e7e"
  ],
  "changeHistoryShort": {
    "e553eda9cd492ddc2b3aebe913913005e7b387c9": "Ybodychange",
    "2382f63fc0bb4108f3f3c542b4be7c04fbedd7c4": "Ybodychange",
    "72993b33b704991f2a0bf743f31b164e58a2dabc": "Ybodychange",
    "c6a39232456fa0c98b2b9b6dbeaec762294ca01e": "Ymultichange(Yparameterchange,Ybodychange)",
    "4ee3543625c77c06d566fe81644d21c607d6d74d": "Ybodychange",
    "27c4e90efce04e1b1302f668b5eb22412e00d033": "Ymultichange(Yparameterchange,Yexceptionschange,Ybodychange)",
    "b9e3eff62a7415d8666656a75db69ff3e43f8e7e": "Yintroduced"
  },
  "changeHistoryDetails": {
    "e553eda9cd492ddc2b3aebe913913005e7b387c9": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16767 Handle non-IO exceptions in reopen()\n\nContributed by Sergei Poganshev.\r\n\r\nCatches Exception instead of IOException in closeStream() \r\nand so handle exceptions such as SdkClientException by \r\naborting the wrapped stream. This will increase resilience\r\nto failures, as any which occuring during stream closure\r\nwill be caught. Furthermore, because the\r\nunderlying HTTP connection is aborted, rather than closed,\r\nit will not be recycled to cause problems on subsequent\r\noperations.\r\n",
      "commitDate": "02/03/20 9:17 AM",
      "commitName": "e553eda9cd492ddc2b3aebe913913005e7b387c9",
      "commitAuthor": "spoganshev",
      "commitDateOld": "14/10/19 8:56 AM",
      "commitNameOld": "dee9e97075e67f53d033df522372064ca19d6b51",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 140.06,
      "commitsBetweenForRepo": 488,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,50 @@\n   private void closeStream(String reason, long length, boolean forceAbort) {\n     if (isObjectStreamOpen()) {\n \n       // if the amount of data remaining in the current request is greater\n       // than the readahead value: abort.\n       long remaining \u003d remainingInCurrentRequest();\n       LOG.debug(\"Closing stream {}: {}\", reason,\n           forceAbort ? \"abort\" : \"soft\");\n       boolean shouldAbort \u003d forceAbort || remaining \u003e readahead;\n       if (!shouldAbort) {\n         try {\n           // clean close. This will read to the end of the stream,\n           // so, while cleaner, can be pathological on a multi-GB object\n \n           // explicitly drain the stream\n           long drained \u003d 0;\n           while (wrappedStream.read() \u003e\u003d 0) {\n             drained++;\n           }\n           LOG.debug(\"Drained stream of {} bytes\", drained);\n \n           // now close it\n           wrappedStream.close();\n           // this MUST come after the close, so that if the IO operations fail\n           // and an abort is triggered, the initial attempt\u0027s statistics\n           // aren\u0027t collected.\n           streamStatistics.streamClose(false, drained);\n-        } catch (IOException e) {\n+        } catch (Exception e) {\n           // exception escalates to an abort\n           LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n           shouldAbort \u003d true;\n         }\n       }\n       if (shouldAbort) {\n         // Abort, rather than just close, the underlying stream.  Otherwise, the\n         // remaining object payload is read from S3 while closing the stream.\n         LOG.debug(\"Aborting stream\");\n         wrappedStream.abort();\n         streamStatistics.streamClose(true, remaining);\n       }\n       LOG.debug(\"Stream {} {}: {}; remaining\u003d{} streamPos\u003d{},\"\n               + \" nextReadPos\u003d{},\" +\n           \" request range {}-{} length\u003d{}\",\n           uri, (shouldAbort ? \"aborted\" : \"closed\"), reason,\n           remaining, pos, nextReadPos,\n           contentRangeStart, contentRangeFinish,\n           length);\n       wrappedStream \u003d null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void closeStream(String reason, long length, boolean forceAbort) {\n    if (isObjectStreamOpen()) {\n\n      // if the amount of data remaining in the current request is greater\n      // than the readahead value: abort.\n      long remaining \u003d remainingInCurrentRequest();\n      LOG.debug(\"Closing stream {}: {}\", reason,\n          forceAbort ? \"abort\" : \"soft\");\n      boolean shouldAbort \u003d forceAbort || remaining \u003e readahead;\n      if (!shouldAbort) {\n        try {\n          // clean close. This will read to the end of the stream,\n          // so, while cleaner, can be pathological on a multi-GB object\n\n          // explicitly drain the stream\n          long drained \u003d 0;\n          while (wrappedStream.read() \u003e\u003d 0) {\n            drained++;\n          }\n          LOG.debug(\"Drained stream of {} bytes\", drained);\n\n          // now close it\n          wrappedStream.close();\n          // this MUST come after the close, so that if the IO operations fail\n          // and an abort is triggered, the initial attempt\u0027s statistics\n          // aren\u0027t collected.\n          streamStatistics.streamClose(false, drained);\n        } catch (Exception e) {\n          // exception escalates to an abort\n          LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n          shouldAbort \u003d true;\n        }\n      }\n      if (shouldAbort) {\n        // Abort, rather than just close, the underlying stream.  Otherwise, the\n        // remaining object payload is read from S3 while closing the stream.\n        LOG.debug(\"Aborting stream\");\n        wrappedStream.abort();\n        streamStatistics.streamClose(true, remaining);\n      }\n      LOG.debug(\"Stream {} {}: {}; remaining\u003d{} streamPos\u003d{},\"\n              + \" nextReadPos\u003d{},\" +\n          \" request range {}-{} length\u003d{}\",\n          uri, (shouldAbort ? \"aborted\" : \"closed\"), reason,\n          remaining, pos, nextReadPos,\n          contentRangeStart, contentRangeFinish,\n          length);\n      wrappedStream \u003d null;\n    }\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java",
      "extendedDetails": {}
    },
    "2382f63fc0bb4108f3f3c542b4be7c04fbedd7c4": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14747. S3AInputStream to implement CanUnbuffer.\n\nAuthor:    Sahil Takiar \u003cstakiar@cloudera.com\u003e\n",
      "commitDate": "12/04/19 6:12 PM",
      "commitName": "2382f63fc0bb4108f3f3c542b4be7c04fbedd7c4",
      "commitAuthor": "Sahil Takiar",
      "commitDateOld": "13/03/19 1:37 PM",
      "commitNameOld": "6fa229891e06eea62cb9634efde755f40247e816",
      "commitAuthorOld": "Ben Roling",
      "daysBetweenCommits": 30.19,
      "commitsBetweenForRepo": 225,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,50 @@\n   private void closeStream(String reason, long length, boolean forceAbort) {\n-    if (wrappedStream !\u003d null) {\n+    if (isObjectStreamOpen()) {\n \n       // if the amount of data remaining in the current request is greater\n       // than the readahead value: abort.\n       long remaining \u003d remainingInCurrentRequest();\n       LOG.debug(\"Closing stream {}: {}\", reason,\n           forceAbort ? \"abort\" : \"soft\");\n       boolean shouldAbort \u003d forceAbort || remaining \u003e readahead;\n       if (!shouldAbort) {\n         try {\n           // clean close. This will read to the end of the stream,\n           // so, while cleaner, can be pathological on a multi-GB object\n \n           // explicitly drain the stream\n           long drained \u003d 0;\n           while (wrappedStream.read() \u003e\u003d 0) {\n             drained++;\n           }\n           LOG.debug(\"Drained stream of {} bytes\", drained);\n \n           // now close it\n           wrappedStream.close();\n           // this MUST come after the close, so that if the IO operations fail\n           // and an abort is triggered, the initial attempt\u0027s statistics\n           // aren\u0027t collected.\n           streamStatistics.streamClose(false, drained);\n         } catch (IOException e) {\n           // exception escalates to an abort\n           LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n           shouldAbort \u003d true;\n         }\n       }\n       if (shouldAbort) {\n         // Abort, rather than just close, the underlying stream.  Otherwise, the\n         // remaining object payload is read from S3 while closing the stream.\n         LOG.debug(\"Aborting stream\");\n         wrappedStream.abort();\n         streamStatistics.streamClose(true, remaining);\n       }\n       LOG.debug(\"Stream {} {}: {}; remaining\u003d{} streamPos\u003d{},\"\n               + \" nextReadPos\u003d{},\" +\n           \" request range {}-{} length\u003d{}\",\n           uri, (shouldAbort ? \"aborted\" : \"closed\"), reason,\n           remaining, pos, nextReadPos,\n           contentRangeStart, contentRangeFinish,\n           length);\n       wrappedStream \u003d null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void closeStream(String reason, long length, boolean forceAbort) {\n    if (isObjectStreamOpen()) {\n\n      // if the amount of data remaining in the current request is greater\n      // than the readahead value: abort.\n      long remaining \u003d remainingInCurrentRequest();\n      LOG.debug(\"Closing stream {}: {}\", reason,\n          forceAbort ? \"abort\" : \"soft\");\n      boolean shouldAbort \u003d forceAbort || remaining \u003e readahead;\n      if (!shouldAbort) {\n        try {\n          // clean close. This will read to the end of the stream,\n          // so, while cleaner, can be pathological on a multi-GB object\n\n          // explicitly drain the stream\n          long drained \u003d 0;\n          while (wrappedStream.read() \u003e\u003d 0) {\n            drained++;\n          }\n          LOG.debug(\"Drained stream of {} bytes\", drained);\n\n          // now close it\n          wrappedStream.close();\n          // this MUST come after the close, so that if the IO operations fail\n          // and an abort is triggered, the initial attempt\u0027s statistics\n          // aren\u0027t collected.\n          streamStatistics.streamClose(false, drained);\n        } catch (IOException e) {\n          // exception escalates to an abort\n          LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n          shouldAbort \u003d true;\n        }\n      }\n      if (shouldAbort) {\n        // Abort, rather than just close, the underlying stream.  Otherwise, the\n        // remaining object payload is read from S3 while closing the stream.\n        LOG.debug(\"Aborting stream\");\n        wrappedStream.abort();\n        streamStatistics.streamClose(true, remaining);\n      }\n      LOG.debug(\"Stream {} {}: {}; remaining\u003d{} streamPos\u003d{},\"\n              + \" nextReadPos\u003d{},\" +\n          \" request range {}-{} length\u003d{}\",\n          uri, (shouldAbort ? \"aborted\" : \"closed\"), reason,\n          remaining, pos, nextReadPos,\n          contentRangeStart, contentRangeFinish,\n          length);\n      wrappedStream \u003d null;\n    }\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java",
      "extendedDetails": {}
    },
    "72993b33b704991f2a0bf743f31b164e58a2dabc": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14596. AWS SDK 1.11+ aborts() on close() if \u003e 0 bytes in stream; logs error. Contributed by Steve Loughran\n\nChange-Id: I49173bf6163796903d64594a8ca8a4bd26ad2bfc\n",
      "commitDate": "29/06/17 5:07 PM",
      "commitName": "72993b33b704991f2a0bf743f31b164e58a2dabc",
      "commitAuthor": "Mingliang Liu",
      "commitDateOld": "11/02/17 1:59 PM",
      "commitNameOld": "839b690ed5edc2ac4984640d58c005bb63cd8a07",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 138.09,
      "commitsBetweenForRepo": 771,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,50 @@\n   private void closeStream(String reason, long length, boolean forceAbort) {\n     if (wrappedStream !\u003d null) {\n \n       // if the amount of data remaining in the current request is greater\n       // than the readahead value: abort.\n       long remaining \u003d remainingInCurrentRequest();\n+      LOG.debug(\"Closing stream {}: {}\", reason,\n+          forceAbort ? \"abort\" : \"soft\");\n       boolean shouldAbort \u003d forceAbort || remaining \u003e readahead;\n       if (!shouldAbort) {\n         try {\n           // clean close. This will read to the end of the stream,\n           // so, while cleaner, can be pathological on a multi-GB object\n+\n+          // explicitly drain the stream\n+          long drained \u003d 0;\n+          while (wrappedStream.read() \u003e\u003d 0) {\n+            drained++;\n+          }\n+          LOG.debug(\"Drained stream of {} bytes\", drained);\n+\n+          // now close it\n           wrappedStream.close();\n-          streamStatistics.streamClose(false, remaining);\n+          // this MUST come after the close, so that if the IO operations fail\n+          // and an abort is triggered, the initial attempt\u0027s statistics\n+          // aren\u0027t collected.\n+          streamStatistics.streamClose(false, drained);\n         } catch (IOException e) {\n           // exception escalates to an abort\n           LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n           shouldAbort \u003d true;\n         }\n       }\n       if (shouldAbort) {\n         // Abort, rather than just close, the underlying stream.  Otherwise, the\n         // remaining object payload is read from S3 while closing the stream.\n+        LOG.debug(\"Aborting stream\");\n         wrappedStream.abort();\n         streamStatistics.streamClose(true, remaining);\n       }\n-      LOG.debug(\"Stream {} {}: {}; streamPos\u003d{}, nextReadPos\u003d{},\" +\n+      LOG.debug(\"Stream {} {}: {}; remaining\u003d{} streamPos\u003d{},\"\n+              + \" nextReadPos\u003d{},\" +\n           \" request range {}-{} length\u003d{}\",\n           uri, (shouldAbort ? \"aborted\" : \"closed\"), reason,\n-          pos, nextReadPos,\n+          remaining, pos, nextReadPos,\n           contentRangeStart, contentRangeFinish,\n           length);\n       wrappedStream \u003d null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void closeStream(String reason, long length, boolean forceAbort) {\n    if (wrappedStream !\u003d null) {\n\n      // if the amount of data remaining in the current request is greater\n      // than the readahead value: abort.\n      long remaining \u003d remainingInCurrentRequest();\n      LOG.debug(\"Closing stream {}: {}\", reason,\n          forceAbort ? \"abort\" : \"soft\");\n      boolean shouldAbort \u003d forceAbort || remaining \u003e readahead;\n      if (!shouldAbort) {\n        try {\n          // clean close. This will read to the end of the stream,\n          // so, while cleaner, can be pathological on a multi-GB object\n\n          // explicitly drain the stream\n          long drained \u003d 0;\n          while (wrappedStream.read() \u003e\u003d 0) {\n            drained++;\n          }\n          LOG.debug(\"Drained stream of {} bytes\", drained);\n\n          // now close it\n          wrappedStream.close();\n          // this MUST come after the close, so that if the IO operations fail\n          // and an abort is triggered, the initial attempt\u0027s statistics\n          // aren\u0027t collected.\n          streamStatistics.streamClose(false, drained);\n        } catch (IOException e) {\n          // exception escalates to an abort\n          LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n          shouldAbort \u003d true;\n        }\n      }\n      if (shouldAbort) {\n        // Abort, rather than just close, the underlying stream.  Otherwise, the\n        // remaining object payload is read from S3 while closing the stream.\n        LOG.debug(\"Aborting stream\");\n        wrappedStream.abort();\n        streamStatistics.streamClose(true, remaining);\n      }\n      LOG.debug(\"Stream {} {}: {}; remaining\u003d{} streamPos\u003d{},\"\n              + \" nextReadPos\u003d{},\" +\n          \" request range {}-{} length\u003d{}\",\n          uri, (shouldAbort ? \"aborted\" : \"closed\"), reason,\n          remaining, pos, nextReadPos,\n          contentRangeStart, contentRangeFinish,\n          length);\n      wrappedStream \u003d null;\n    }\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java",
      "extendedDetails": {}
    },
    "c6a39232456fa0c98b2b9b6dbeaec762294ca01e": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-13871. ITestS3AInputStreamPerformance.testTimeToOpenAndReadWholeFileBlocks performance awful. Contributed by Steve Loughran\n",
      "commitDate": "12/12/16 2:55 PM",
      "commitName": "c6a39232456fa0c98b2b9b6dbeaec762294ca01e",
      "commitAuthor": "Mingliang Liu",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-13871. ITestS3AInputStreamPerformance.testTimeToOpenAndReadWholeFileBlocks performance awful. Contributed by Steve Loughran\n",
          "commitDate": "12/12/16 2:55 PM",
          "commitName": "c6a39232456fa0c98b2b9b6dbeaec762294ca01e",
          "commitAuthor": "Mingliang Liu",
          "commitDateOld": "06/09/16 9:36 AM",
          "commitNameOld": "d152557cf7f4d2288524c222fcbaf152bdc038b0",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 97.26,
          "commitsBetweenForRepo": 728,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,34 +1,34 @@\n-  private void closeStream(String reason, long length) {\n+  private void closeStream(String reason, long length, boolean forceAbort) {\n     if (wrappedStream !\u003d null) {\n \n       // if the amount of data remaining in the current request is greater\n       // than the readahead value: abort.\n       long remaining \u003d remainingInCurrentRequest();\n-      boolean shouldAbort \u003d remaining \u003e readahead;\n+      boolean shouldAbort \u003d forceAbort || remaining \u003e readahead;\n       if (!shouldAbort) {\n         try {\n           // clean close. This will read to the end of the stream,\n           // so, while cleaner, can be pathological on a multi-GB object\n           wrappedStream.close();\n           streamStatistics.streamClose(false, remaining);\n         } catch (IOException e) {\n           // exception escalates to an abort\n           LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n           shouldAbort \u003d true;\n         }\n       }\n       if (shouldAbort) {\n         // Abort, rather than just close, the underlying stream.  Otherwise, the\n         // remaining object payload is read from S3 while closing the stream.\n         wrappedStream.abort();\n         streamStatistics.streamClose(true, remaining);\n       }\n       LOG.debug(\"Stream {} {}: {}; streamPos\u003d{}, nextReadPos\u003d{},\" +\n           \" request range {}-{} length\u003d{}\",\n           uri, (shouldAbort ? \"aborted\" : \"closed\"), reason,\n           pos, nextReadPos,\n           contentRangeStart, contentRangeFinish,\n           length);\n       wrappedStream \u003d null;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void closeStream(String reason, long length, boolean forceAbort) {\n    if (wrappedStream !\u003d null) {\n\n      // if the amount of data remaining in the current request is greater\n      // than the readahead value: abort.\n      long remaining \u003d remainingInCurrentRequest();\n      boolean shouldAbort \u003d forceAbort || remaining \u003e readahead;\n      if (!shouldAbort) {\n        try {\n          // clean close. This will read to the end of the stream,\n          // so, while cleaner, can be pathological on a multi-GB object\n          wrappedStream.close();\n          streamStatistics.streamClose(false, remaining);\n        } catch (IOException e) {\n          // exception escalates to an abort\n          LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n          shouldAbort \u003d true;\n        }\n      }\n      if (shouldAbort) {\n        // Abort, rather than just close, the underlying stream.  Otherwise, the\n        // remaining object payload is read from S3 while closing the stream.\n        wrappedStream.abort();\n        streamStatistics.streamClose(true, remaining);\n      }\n      LOG.debug(\"Stream {} {}: {}; streamPos\u003d{}, nextReadPos\u003d{},\" +\n          \" request range {}-{} length\u003d{}\",\n          uri, (shouldAbort ? \"aborted\" : \"closed\"), reason,\n          pos, nextReadPos,\n          contentRangeStart, contentRangeFinish,\n          length);\n      wrappedStream \u003d null;\n    }\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java",
          "extendedDetails": {
            "oldValue": "[reason-String, length-long]",
            "newValue": "[reason-String, length-long, forceAbort-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-13871. ITestS3AInputStreamPerformance.testTimeToOpenAndReadWholeFileBlocks performance awful. Contributed by Steve Loughran\n",
          "commitDate": "12/12/16 2:55 PM",
          "commitName": "c6a39232456fa0c98b2b9b6dbeaec762294ca01e",
          "commitAuthor": "Mingliang Liu",
          "commitDateOld": "06/09/16 9:36 AM",
          "commitNameOld": "d152557cf7f4d2288524c222fcbaf152bdc038b0",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 97.26,
          "commitsBetweenForRepo": 728,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,34 +1,34 @@\n-  private void closeStream(String reason, long length) {\n+  private void closeStream(String reason, long length, boolean forceAbort) {\n     if (wrappedStream !\u003d null) {\n \n       // if the amount of data remaining in the current request is greater\n       // than the readahead value: abort.\n       long remaining \u003d remainingInCurrentRequest();\n-      boolean shouldAbort \u003d remaining \u003e readahead;\n+      boolean shouldAbort \u003d forceAbort || remaining \u003e readahead;\n       if (!shouldAbort) {\n         try {\n           // clean close. This will read to the end of the stream,\n           // so, while cleaner, can be pathological on a multi-GB object\n           wrappedStream.close();\n           streamStatistics.streamClose(false, remaining);\n         } catch (IOException e) {\n           // exception escalates to an abort\n           LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n           shouldAbort \u003d true;\n         }\n       }\n       if (shouldAbort) {\n         // Abort, rather than just close, the underlying stream.  Otherwise, the\n         // remaining object payload is read from S3 while closing the stream.\n         wrappedStream.abort();\n         streamStatistics.streamClose(true, remaining);\n       }\n       LOG.debug(\"Stream {} {}: {}; streamPos\u003d{}, nextReadPos\u003d{},\" +\n           \" request range {}-{} length\u003d{}\",\n           uri, (shouldAbort ? \"aborted\" : \"closed\"), reason,\n           pos, nextReadPos,\n           contentRangeStart, contentRangeFinish,\n           length);\n       wrappedStream \u003d null;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void closeStream(String reason, long length, boolean forceAbort) {\n    if (wrappedStream !\u003d null) {\n\n      // if the amount of data remaining in the current request is greater\n      // than the readahead value: abort.\n      long remaining \u003d remainingInCurrentRequest();\n      boolean shouldAbort \u003d forceAbort || remaining \u003e readahead;\n      if (!shouldAbort) {\n        try {\n          // clean close. This will read to the end of the stream,\n          // so, while cleaner, can be pathological on a multi-GB object\n          wrappedStream.close();\n          streamStatistics.streamClose(false, remaining);\n        } catch (IOException e) {\n          // exception escalates to an abort\n          LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n          shouldAbort \u003d true;\n        }\n      }\n      if (shouldAbort) {\n        // Abort, rather than just close, the underlying stream.  Otherwise, the\n        // remaining object payload is read from S3 while closing the stream.\n        wrappedStream.abort();\n        streamStatistics.streamClose(true, remaining);\n      }\n      LOG.debug(\"Stream {} {}: {}; streamPos\u003d{}, nextReadPos\u003d{},\" +\n          \" request range {}-{} length\u003d{}\",\n          uri, (shouldAbort ? \"aborted\" : \"closed\"), reason,\n          pos, nextReadPos,\n          contentRangeStart, contentRangeFinish,\n          length);\n      wrappedStream \u003d null;\n    }\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "4ee3543625c77c06d566fe81644d21c607d6d74d": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13203 S3A: Support fadvise \"random\" mode for high performance readPositioned() reads. Contributed by Rajesh Balamohan and stevel.\n",
      "commitDate": "22/06/16 7:45 AM",
      "commitName": "4ee3543625c77c06d566fe81644d21c607d6d74d",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "21/05/16 8:39 AM",
      "commitNameOld": "39ec1515a205952eda7e171408a8b83eceb4abde",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 31.96,
      "commitsBetweenForRepo": 198,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,34 @@\n   private void closeStream(String reason, long length) {\n     if (wrappedStream !\u003d null) {\n-      boolean shouldAbort \u003d length - pos \u003e CLOSE_THRESHOLD;\n+\n+      // if the amount of data remaining in the current request is greater\n+      // than the readahead value: abort.\n+      long remaining \u003d remainingInCurrentRequest();\n+      boolean shouldAbort \u003d remaining \u003e readahead;\n       if (!shouldAbort) {\n         try {\n           // clean close. This will read to the end of the stream,\n           // so, while cleaner, can be pathological on a multi-GB object\n           wrappedStream.close();\n-          streamStatistics.streamClose(false);\n+          streamStatistics.streamClose(false, remaining);\n         } catch (IOException e) {\n           // exception escalates to an abort\n           LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n           shouldAbort \u003d true;\n         }\n       }\n       if (shouldAbort) {\n         // Abort, rather than just close, the underlying stream.  Otherwise, the\n         // remaining object payload is read from S3 while closing the stream.\n         wrappedStream.abort();\n-        streamStatistics.streamClose(true);\n+        streamStatistics.streamClose(true, remaining);\n       }\n       LOG.debug(\"Stream {} {}: {}; streamPos\u003d{}, nextReadPos\u003d{},\" +\n-          \" length\u003d{}\",\n-          uri, (shouldAbort ? \"aborted\":\"closed\"), reason, pos, nextReadPos,\n+          \" request range {}-{} length\u003d{}\",\n+          uri, (shouldAbort ? \"aborted\" : \"closed\"), reason,\n+          pos, nextReadPos,\n+          contentRangeStart, contentRangeFinish,\n           length);\n       wrappedStream \u003d null;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void closeStream(String reason, long length) {\n    if (wrappedStream !\u003d null) {\n\n      // if the amount of data remaining in the current request is greater\n      // than the readahead value: abort.\n      long remaining \u003d remainingInCurrentRequest();\n      boolean shouldAbort \u003d remaining \u003e readahead;\n      if (!shouldAbort) {\n        try {\n          // clean close. This will read to the end of the stream,\n          // so, while cleaner, can be pathological on a multi-GB object\n          wrappedStream.close();\n          streamStatistics.streamClose(false, remaining);\n        } catch (IOException e) {\n          // exception escalates to an abort\n          LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n          shouldAbort \u003d true;\n        }\n      }\n      if (shouldAbort) {\n        // Abort, rather than just close, the underlying stream.  Otherwise, the\n        // remaining object payload is read from S3 while closing the stream.\n        wrappedStream.abort();\n        streamStatistics.streamClose(true, remaining);\n      }\n      LOG.debug(\"Stream {} {}: {}; streamPos\u003d{}, nextReadPos\u003d{},\" +\n          \" request range {}-{} length\u003d{}\",\n          uri, (shouldAbort ? \"aborted\" : \"closed\"), reason,\n          pos, nextReadPos,\n          contentRangeStart, contentRangeFinish,\n          length);\n      wrappedStream \u003d null;\n    }\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java",
      "extendedDetails": {}
    },
    "27c4e90efce04e1b1302f668b5eb22412e00d033": {
      "type": "Ymultichange(Yparameterchange,Yexceptionschange,Ybodychange)",
      "commitMessage": "HADOOP-13028 add low level counter metrics for S3A; use in read performance tests. contributed by: stevel\npatch includes\nHADOOP-12844 Recover when S3A fails on IOException in read()\nHADOOP-13058 S3A FS fails during init against a read-only FS if multipart purge\nHADOOP-13047 S3a Forward seek in stream length to be configurable\n",
      "commitDate": "12/05/16 11:24 AM",
      "commitName": "27c4e90efce04e1b1302f668b5eb22412e00d033",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-13028 add low level counter metrics for S3A; use in read performance tests. contributed by: stevel\npatch includes\nHADOOP-12844 Recover when S3A fails on IOException in read()\nHADOOP-13058 S3A FS fails during init against a read-only FS if multipart purge\nHADOOP-13047 S3a Forward seek in stream length to be configurable\n",
          "commitDate": "12/05/16 11:24 AM",
          "commitName": "27c4e90efce04e1b1302f668b5eb22412e00d033",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "09/04/16 3:25 AM",
          "commitNameOld": "b9e3eff62a7415d8666656a75db69ff3e43f8e7e",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 33.33,
          "commitsBetweenForRepo": 203,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,28 +1,28 @@\n-  private void closeStream(long length) throws IOException {\n+  private void closeStream(String reason, long length) {\n     if (wrappedStream !\u003d null) {\n-      String reason \u003d null;\n       boolean shouldAbort \u003d length - pos \u003e CLOSE_THRESHOLD;\n       if (!shouldAbort) {\n         try {\n-          reason \u003d \"Closed stream\";\n+          // clean close. This will read to the end of the stream,\n+          // so, while cleaner, can be pathological on a multi-GB object\n           wrappedStream.close();\n+          streamStatistics.streamClose(false);\n         } catch (IOException e) {\n           // exception escalates to an abort\n-          LOG.debug(\"When closing stream\", e);\n+          LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n           shouldAbort \u003d true;\n         }\n       }\n       if (shouldAbort) {\n         // Abort, rather than just close, the underlying stream.  Otherwise, the\n         // remaining object payload is read from S3 while closing the stream.\n         wrappedStream.abort();\n-        reason \u003d \"Closed stream with abort\";\n+        streamStatistics.streamClose(true);\n       }\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(reason + \"; streamPos\u003d\" + pos\n-            + \", nextReadPos\u003d\" + nextReadPos\n-            + \", contentLength\u003d\" + length);\n-      }\n+      LOG.debug(\"Stream {} {}: {}; streamPos\u003d{}, nextReadPos\u003d{},\" +\n+          \" length\u003d{}\",\n+          uri, (shouldAbort ? \"aborted\":\"closed\"), reason, pos, nextReadPos,\n+          length);\n       wrappedStream \u003d null;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void closeStream(String reason, long length) {\n    if (wrappedStream !\u003d null) {\n      boolean shouldAbort \u003d length - pos \u003e CLOSE_THRESHOLD;\n      if (!shouldAbort) {\n        try {\n          // clean close. This will read to the end of the stream,\n          // so, while cleaner, can be pathological on a multi-GB object\n          wrappedStream.close();\n          streamStatistics.streamClose(false);\n        } catch (IOException e) {\n          // exception escalates to an abort\n          LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n          shouldAbort \u003d true;\n        }\n      }\n      if (shouldAbort) {\n        // Abort, rather than just close, the underlying stream.  Otherwise, the\n        // remaining object payload is read from S3 while closing the stream.\n        wrappedStream.abort();\n        streamStatistics.streamClose(true);\n      }\n      LOG.debug(\"Stream {} {}: {}; streamPos\u003d{}, nextReadPos\u003d{},\" +\n          \" length\u003d{}\",\n          uri, (shouldAbort ? \"aborted\":\"closed\"), reason, pos, nextReadPos,\n          length);\n      wrappedStream \u003d null;\n    }\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java",
          "extendedDetails": {
            "oldValue": "[length-long]",
            "newValue": "[reason-String, length-long]"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "HADOOP-13028 add low level counter metrics for S3A; use in read performance tests. contributed by: stevel\npatch includes\nHADOOP-12844 Recover when S3A fails on IOException in read()\nHADOOP-13058 S3A FS fails during init against a read-only FS if multipart purge\nHADOOP-13047 S3a Forward seek in stream length to be configurable\n",
          "commitDate": "12/05/16 11:24 AM",
          "commitName": "27c4e90efce04e1b1302f668b5eb22412e00d033",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "09/04/16 3:25 AM",
          "commitNameOld": "b9e3eff62a7415d8666656a75db69ff3e43f8e7e",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 33.33,
          "commitsBetweenForRepo": 203,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,28 +1,28 @@\n-  private void closeStream(long length) throws IOException {\n+  private void closeStream(String reason, long length) {\n     if (wrappedStream !\u003d null) {\n-      String reason \u003d null;\n       boolean shouldAbort \u003d length - pos \u003e CLOSE_THRESHOLD;\n       if (!shouldAbort) {\n         try {\n-          reason \u003d \"Closed stream\";\n+          // clean close. This will read to the end of the stream,\n+          // so, while cleaner, can be pathological on a multi-GB object\n           wrappedStream.close();\n+          streamStatistics.streamClose(false);\n         } catch (IOException e) {\n           // exception escalates to an abort\n-          LOG.debug(\"When closing stream\", e);\n+          LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n           shouldAbort \u003d true;\n         }\n       }\n       if (shouldAbort) {\n         // Abort, rather than just close, the underlying stream.  Otherwise, the\n         // remaining object payload is read from S3 while closing the stream.\n         wrappedStream.abort();\n-        reason \u003d \"Closed stream with abort\";\n+        streamStatistics.streamClose(true);\n       }\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(reason + \"; streamPos\u003d\" + pos\n-            + \", nextReadPos\u003d\" + nextReadPos\n-            + \", contentLength\u003d\" + length);\n-      }\n+      LOG.debug(\"Stream {} {}: {}; streamPos\u003d{}, nextReadPos\u003d{},\" +\n+          \" length\u003d{}\",\n+          uri, (shouldAbort ? \"aborted\":\"closed\"), reason, pos, nextReadPos,\n+          length);\n       wrappedStream \u003d null;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void closeStream(String reason, long length) {\n    if (wrappedStream !\u003d null) {\n      boolean shouldAbort \u003d length - pos \u003e CLOSE_THRESHOLD;\n      if (!shouldAbort) {\n        try {\n          // clean close. This will read to the end of the stream,\n          // so, while cleaner, can be pathological on a multi-GB object\n          wrappedStream.close();\n          streamStatistics.streamClose(false);\n        } catch (IOException e) {\n          // exception escalates to an abort\n          LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n          shouldAbort \u003d true;\n        }\n      }\n      if (shouldAbort) {\n        // Abort, rather than just close, the underlying stream.  Otherwise, the\n        // remaining object payload is read from S3 while closing the stream.\n        wrappedStream.abort();\n        streamStatistics.streamClose(true);\n      }\n      LOG.debug(\"Stream {} {}: {}; streamPos\u003d{}, nextReadPos\u003d{},\" +\n          \" length\u003d{}\",\n          uri, (shouldAbort ? \"aborted\":\"closed\"), reason, pos, nextReadPos,\n          length);\n      wrappedStream \u003d null;\n    }\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java",
          "extendedDetails": {
            "oldValue": "[IOException]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-13028 add low level counter metrics for S3A; use in read performance tests. contributed by: stevel\npatch includes\nHADOOP-12844 Recover when S3A fails on IOException in read()\nHADOOP-13058 S3A FS fails during init against a read-only FS if multipart purge\nHADOOP-13047 S3a Forward seek in stream length to be configurable\n",
          "commitDate": "12/05/16 11:24 AM",
          "commitName": "27c4e90efce04e1b1302f668b5eb22412e00d033",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "09/04/16 3:25 AM",
          "commitNameOld": "b9e3eff62a7415d8666656a75db69ff3e43f8e7e",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 33.33,
          "commitsBetweenForRepo": 203,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,28 +1,28 @@\n-  private void closeStream(long length) throws IOException {\n+  private void closeStream(String reason, long length) {\n     if (wrappedStream !\u003d null) {\n-      String reason \u003d null;\n       boolean shouldAbort \u003d length - pos \u003e CLOSE_THRESHOLD;\n       if (!shouldAbort) {\n         try {\n-          reason \u003d \"Closed stream\";\n+          // clean close. This will read to the end of the stream,\n+          // so, while cleaner, can be pathological on a multi-GB object\n           wrappedStream.close();\n+          streamStatistics.streamClose(false);\n         } catch (IOException e) {\n           // exception escalates to an abort\n-          LOG.debug(\"When closing stream\", e);\n+          LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n           shouldAbort \u003d true;\n         }\n       }\n       if (shouldAbort) {\n         // Abort, rather than just close, the underlying stream.  Otherwise, the\n         // remaining object payload is read from S3 while closing the stream.\n         wrappedStream.abort();\n-        reason \u003d \"Closed stream with abort\";\n+        streamStatistics.streamClose(true);\n       }\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(reason + \"; streamPos\u003d\" + pos\n-            + \", nextReadPos\u003d\" + nextReadPos\n-            + \", contentLength\u003d\" + length);\n-      }\n+      LOG.debug(\"Stream {} {}: {}; streamPos\u003d{}, nextReadPos\u003d{},\" +\n+          \" length\u003d{}\",\n+          uri, (shouldAbort ? \"aborted\":\"closed\"), reason, pos, nextReadPos,\n+          length);\n       wrappedStream \u003d null;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void closeStream(String reason, long length) {\n    if (wrappedStream !\u003d null) {\n      boolean shouldAbort \u003d length - pos \u003e CLOSE_THRESHOLD;\n      if (!shouldAbort) {\n        try {\n          // clean close. This will read to the end of the stream,\n          // so, while cleaner, can be pathological on a multi-GB object\n          wrappedStream.close();\n          streamStatistics.streamClose(false);\n        } catch (IOException e) {\n          // exception escalates to an abort\n          LOG.debug(\"When closing {} stream for {}\", uri, reason, e);\n          shouldAbort \u003d true;\n        }\n      }\n      if (shouldAbort) {\n        // Abort, rather than just close, the underlying stream.  Otherwise, the\n        // remaining object payload is read from S3 while closing the stream.\n        wrappedStream.abort();\n        streamStatistics.streamClose(true);\n      }\n      LOG.debug(\"Stream {} {}: {}; streamPos\u003d{}, nextReadPos\u003d{},\" +\n          \" length\u003d{}\",\n          uri, (shouldAbort ? \"aborted\":\"closed\"), reason, pos, nextReadPos,\n          length);\n      wrappedStream \u003d null;\n    }\n  }",
          "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "b9e3eff62a7415d8666656a75db69ff3e43f8e7e": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-12444 Support lazy seek in S3AInputStream. Rajesh Balamohan via stevel\n",
      "commitDate": "09/04/16 3:25 AM",
      "commitName": "b9e3eff62a7415d8666656a75db69ff3e43f8e7e",
      "commitAuthor": "Steve Loughran",
      "diff": "@@ -0,0 +1,28 @@\n+  private void closeStream(long length) throws IOException {\n+    if (wrappedStream !\u003d null) {\n+      String reason \u003d null;\n+      boolean shouldAbort \u003d length - pos \u003e CLOSE_THRESHOLD;\n+      if (!shouldAbort) {\n+        try {\n+          reason \u003d \"Closed stream\";\n+          wrappedStream.close();\n+        } catch (IOException e) {\n+          // exception escalates to an abort\n+          LOG.debug(\"When closing stream\", e);\n+          shouldAbort \u003d true;\n+        }\n+      }\n+      if (shouldAbort) {\n+        // Abort, rather than just close, the underlying stream.  Otherwise, the\n+        // remaining object payload is read from S3 while closing the stream.\n+        wrappedStream.abort();\n+        reason \u003d \"Closed stream with abort\";\n+      }\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(reason + \"; streamPos\u003d\" + pos\n+            + \", nextReadPos\u003d\" + nextReadPos\n+            + \", contentLength\u003d\" + length);\n+      }\n+      wrappedStream \u003d null;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void closeStream(long length) throws IOException {\n    if (wrappedStream !\u003d null) {\n      String reason \u003d null;\n      boolean shouldAbort \u003d length - pos \u003e CLOSE_THRESHOLD;\n      if (!shouldAbort) {\n        try {\n          reason \u003d \"Closed stream\";\n          wrappedStream.close();\n        } catch (IOException e) {\n          // exception escalates to an abort\n          LOG.debug(\"When closing stream\", e);\n          shouldAbort \u003d true;\n        }\n      }\n      if (shouldAbort) {\n        // Abort, rather than just close, the underlying stream.  Otherwise, the\n        // remaining object payload is read from S3 while closing the stream.\n        wrappedStream.abort();\n        reason \u003d \"Closed stream with abort\";\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(reason + \"; streamPos\u003d\" + pos\n            + \", nextReadPos\u003d\" + nextReadPos\n            + \", contentLength\u003d\" + length);\n      }\n      wrappedStream \u003d null;\n    }\n  }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AInputStream.java"
    }
  }
}