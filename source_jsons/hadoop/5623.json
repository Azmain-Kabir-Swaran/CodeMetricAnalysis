{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CacheManager.java",
  "functionName": "saveState",
  "functionId": "saveState",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
  "functionStartLine": 1057,
  "functionEndLine": 1117,
  "numCommitsSeen": 56,
  "timeTaken": 2932,
  "changeHistory": [
    "3481895f8a9ea9f6e217a0ba158c48da89b3faf2",
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305"
  ],
  "changeHistoryShort": {
    "3481895f8a9ea9f6e217a0ba158c48da89b3faf2": "Ybodychange",
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ybodychange",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": "Yintroduced"
  },
  "changeHistoryDetails": {
    "3481895f8a9ea9f6e217a0ba158c48da89b3faf2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15283. Cache pool MAXTTL is not persisted and restored on cluster restart. Contributed by Stephen O\u0027Donnell.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "16/04/20 8:18 PM",
      "commitName": "3481895f8a9ea9f6e217a0ba158c48da89b3faf2",
      "commitAuthor": "Stephen O\u0027Donnell",
      "commitDateOld": "13/12/18 9:36 PM",
      "commitNameOld": "ca379e1c43fd733a34f3ece6172c96d74c890422",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 489.9,
      "commitsBetweenForRepo": 2956,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,61 @@\n   public PersistState saveState() throws IOException {\n     ArrayList\u003cCachePoolInfoProto\u003e pools \u003d Lists\n         .newArrayListWithCapacity(cachePools.size());\n     ArrayList\u003cCacheDirectiveInfoProto\u003e directives \u003d Lists\n         .newArrayListWithCapacity(directivesById.size());\n \n     for (CachePool pool : cachePools.values()) {\n       CachePoolInfo p \u003d pool.getInfo(true);\n       CachePoolInfoProto.Builder b \u003d CachePoolInfoProto.newBuilder()\n           .setPoolName(p.getPoolName());\n \n       if (p.getOwnerName() !\u003d null)\n         b.setOwnerName(p.getOwnerName());\n \n       if (p.getGroupName() !\u003d null)\n         b.setGroupName(p.getGroupName());\n \n       if (p.getMode() !\u003d null)\n         b.setMode(p.getMode().toShort());\n \n       if (p.getLimit() !\u003d null)\n         b.setLimit(p.getLimit());\n \n+      if (p.getMaxRelativeExpiryMs() !\u003d null) {\n+        b.setMaxRelativeExpiry(p.getMaxRelativeExpiryMs());\n+      }\n+\n       pools.add(b.build());\n     }\n \n     for (CacheDirective directive : directivesById.values()) {\n       CacheDirectiveInfo info \u003d directive.toInfo();\n       CacheDirectiveInfoProto.Builder b \u003d CacheDirectiveInfoProto.newBuilder()\n           .setId(info.getId());\n \n       if (info.getPath() !\u003d null) {\n         b.setPath(info.getPath().toUri().getPath());\n       }\n \n       if (info.getReplication() !\u003d null) {\n         b.setReplication(info.getReplication());\n       }\n \n       if (info.getPool() !\u003d null) {\n         b.setPool(info.getPool());\n       }\n \n       Expiration expiry \u003d info.getExpiration();\n       if (expiry !\u003d null) {\n         assert (!expiry.isRelative());\n         b.setExpiration(PBHelperClient.convert(expiry));\n       }\n \n       directives.add(b.build());\n     }\n     CacheManagerSection s \u003d CacheManagerSection.newBuilder()\n         .setNextDirectiveId(nextDirectiveId).setNumPools(pools.size())\n         .setNumDirectives(directives.size()).build();\n \n     return new PersistState(s, pools, directives);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public PersistState saveState() throws IOException {\n    ArrayList\u003cCachePoolInfoProto\u003e pools \u003d Lists\n        .newArrayListWithCapacity(cachePools.size());\n    ArrayList\u003cCacheDirectiveInfoProto\u003e directives \u003d Lists\n        .newArrayListWithCapacity(directivesById.size());\n\n    for (CachePool pool : cachePools.values()) {\n      CachePoolInfo p \u003d pool.getInfo(true);\n      CachePoolInfoProto.Builder b \u003d CachePoolInfoProto.newBuilder()\n          .setPoolName(p.getPoolName());\n\n      if (p.getOwnerName() !\u003d null)\n        b.setOwnerName(p.getOwnerName());\n\n      if (p.getGroupName() !\u003d null)\n        b.setGroupName(p.getGroupName());\n\n      if (p.getMode() !\u003d null)\n        b.setMode(p.getMode().toShort());\n\n      if (p.getLimit() !\u003d null)\n        b.setLimit(p.getLimit());\n\n      if (p.getMaxRelativeExpiryMs() !\u003d null) {\n        b.setMaxRelativeExpiry(p.getMaxRelativeExpiryMs());\n      }\n\n      pools.add(b.build());\n    }\n\n    for (CacheDirective directive : directivesById.values()) {\n      CacheDirectiveInfo info \u003d directive.toInfo();\n      CacheDirectiveInfoProto.Builder b \u003d CacheDirectiveInfoProto.newBuilder()\n          .setId(info.getId());\n\n      if (info.getPath() !\u003d null) {\n        b.setPath(info.getPath().toUri().getPath());\n      }\n\n      if (info.getReplication() !\u003d null) {\n        b.setReplication(info.getReplication());\n      }\n\n      if (info.getPool() !\u003d null) {\n        b.setPool(info.getPool());\n      }\n\n      Expiration expiry \u003d info.getExpiration();\n      if (expiry !\u003d null) {\n        assert (!expiry.isRelative());\n        b.setExpiration(PBHelperClient.convert(expiry));\n      }\n\n      directives.add(b.build());\n    }\n    CacheManagerSection s \u003d CacheManagerSection.newBuilder()\n        .setNextDirectiveId(nextDirectiveId).setNumPools(pools.size())\n        .setNumDirectives(directives.size()).build();\n\n    return new PersistState(s, pools, directives);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "24/06/15 2:42 PM",
      "commitNameOld": "afe9ea3c12e1f5a71922400eadb642960bc87ca1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 89.17,
      "commitsBetweenForRepo": 521,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,57 @@\n   public PersistState saveState() throws IOException {\n     ArrayList\u003cCachePoolInfoProto\u003e pools \u003d Lists\n         .newArrayListWithCapacity(cachePools.size());\n     ArrayList\u003cCacheDirectiveInfoProto\u003e directives \u003d Lists\n         .newArrayListWithCapacity(directivesById.size());\n \n     for (CachePool pool : cachePools.values()) {\n       CachePoolInfo p \u003d pool.getInfo(true);\n       CachePoolInfoProto.Builder b \u003d CachePoolInfoProto.newBuilder()\n           .setPoolName(p.getPoolName());\n \n       if (p.getOwnerName() !\u003d null)\n         b.setOwnerName(p.getOwnerName());\n \n       if (p.getGroupName() !\u003d null)\n         b.setGroupName(p.getGroupName());\n \n       if (p.getMode() !\u003d null)\n         b.setMode(p.getMode().toShort());\n \n       if (p.getLimit() !\u003d null)\n         b.setLimit(p.getLimit());\n \n       pools.add(b.build());\n     }\n \n     for (CacheDirective directive : directivesById.values()) {\n       CacheDirectiveInfo info \u003d directive.toInfo();\n       CacheDirectiveInfoProto.Builder b \u003d CacheDirectiveInfoProto.newBuilder()\n           .setId(info.getId());\n \n       if (info.getPath() !\u003d null) {\n         b.setPath(info.getPath().toUri().getPath());\n       }\n \n       if (info.getReplication() !\u003d null) {\n         b.setReplication(info.getReplication());\n       }\n \n       if (info.getPool() !\u003d null) {\n         b.setPool(info.getPool());\n       }\n \n       Expiration expiry \u003d info.getExpiration();\n       if (expiry !\u003d null) {\n         assert (!expiry.isRelative());\n-        b.setExpiration(PBHelper.convert(expiry));\n+        b.setExpiration(PBHelperClient.convert(expiry));\n       }\n \n       directives.add(b.build());\n     }\n     CacheManagerSection s \u003d CacheManagerSection.newBuilder()\n         .setNextDirectiveId(nextDirectiveId).setNumPools(pools.size())\n         .setNumDirectives(directives.size()).build();\n \n     return new PersistState(s, pools, directives);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public PersistState saveState() throws IOException {\n    ArrayList\u003cCachePoolInfoProto\u003e pools \u003d Lists\n        .newArrayListWithCapacity(cachePools.size());\n    ArrayList\u003cCacheDirectiveInfoProto\u003e directives \u003d Lists\n        .newArrayListWithCapacity(directivesById.size());\n\n    for (CachePool pool : cachePools.values()) {\n      CachePoolInfo p \u003d pool.getInfo(true);\n      CachePoolInfoProto.Builder b \u003d CachePoolInfoProto.newBuilder()\n          .setPoolName(p.getPoolName());\n\n      if (p.getOwnerName() !\u003d null)\n        b.setOwnerName(p.getOwnerName());\n\n      if (p.getGroupName() !\u003d null)\n        b.setGroupName(p.getGroupName());\n\n      if (p.getMode() !\u003d null)\n        b.setMode(p.getMode().toShort());\n\n      if (p.getLimit() !\u003d null)\n        b.setLimit(p.getLimit());\n\n      pools.add(b.build());\n    }\n\n    for (CacheDirective directive : directivesById.values()) {\n      CacheDirectiveInfo info \u003d directive.toInfo();\n      CacheDirectiveInfoProto.Builder b \u003d CacheDirectiveInfoProto.newBuilder()\n          .setId(info.getId());\n\n      if (info.getPath() !\u003d null) {\n        b.setPath(info.getPath().toUri().getPath());\n      }\n\n      if (info.getReplication() !\u003d null) {\n        b.setReplication(info.getReplication());\n      }\n\n      if (info.getPool() !\u003d null) {\n        b.setPool(info.getPool());\n      }\n\n      Expiration expiry \u003d info.getExpiration();\n      if (expiry !\u003d null) {\n        assert (!expiry.isRelative());\n        b.setExpiration(PBHelperClient.convert(expiry));\n      }\n\n      directives.add(b.build());\n    }\n    CacheManagerSection s \u003d CacheManagerSection.newBuilder()\n        .setNextDirectiveId(nextDirectiveId).setNumPools(pools.size())\n        .setNumDirectives(directives.size()).build();\n\n    return new PersistState(s, pools, directives);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5698. Use protobuf to serialize / deserialize FSImage. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/02/14 11:18 AM",
      "commitName": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,57 @@\n+  public PersistState saveState() throws IOException {\n+    ArrayList\u003cCachePoolInfoProto\u003e pools \u003d Lists\n+        .newArrayListWithCapacity(cachePools.size());\n+    ArrayList\u003cCacheDirectiveInfoProto\u003e directives \u003d Lists\n+        .newArrayListWithCapacity(directivesById.size());\n+\n+    for (CachePool pool : cachePools.values()) {\n+      CachePoolInfo p \u003d pool.getInfo(true);\n+      CachePoolInfoProto.Builder b \u003d CachePoolInfoProto.newBuilder()\n+          .setPoolName(p.getPoolName());\n+\n+      if (p.getOwnerName() !\u003d null)\n+        b.setOwnerName(p.getOwnerName());\n+\n+      if (p.getGroupName() !\u003d null)\n+        b.setGroupName(p.getGroupName());\n+\n+      if (p.getMode() !\u003d null)\n+        b.setMode(p.getMode().toShort());\n+\n+      if (p.getLimit() !\u003d null)\n+        b.setLimit(p.getLimit());\n+\n+      pools.add(b.build());\n+    }\n+\n+    for (CacheDirective directive : directivesById.values()) {\n+      CacheDirectiveInfo info \u003d directive.toInfo();\n+      CacheDirectiveInfoProto.Builder b \u003d CacheDirectiveInfoProto.newBuilder()\n+          .setId(info.getId());\n+\n+      if (info.getPath() !\u003d null) {\n+        b.setPath(info.getPath().toUri().getPath());\n+      }\n+\n+      if (info.getReplication() !\u003d null) {\n+        b.setReplication(info.getReplication());\n+      }\n+\n+      if (info.getPool() !\u003d null) {\n+        b.setPool(info.getPool());\n+      }\n+\n+      Expiration expiry \u003d info.getExpiration();\n+      if (expiry !\u003d null) {\n+        assert (!expiry.isRelative());\n+        b.setExpiration(PBHelper.convert(expiry));\n+      }\n+\n+      directives.add(b.build());\n+    }\n+    CacheManagerSection s \u003d CacheManagerSection.newBuilder()\n+        .setNextDirectiveId(nextDirectiveId).setNumPools(pools.size())\n+        .setNumDirectives(directives.size()).build();\n+\n+    return new PersistState(s, pools, directives);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public PersistState saveState() throws IOException {\n    ArrayList\u003cCachePoolInfoProto\u003e pools \u003d Lists\n        .newArrayListWithCapacity(cachePools.size());\n    ArrayList\u003cCacheDirectiveInfoProto\u003e directives \u003d Lists\n        .newArrayListWithCapacity(directivesById.size());\n\n    for (CachePool pool : cachePools.values()) {\n      CachePoolInfo p \u003d pool.getInfo(true);\n      CachePoolInfoProto.Builder b \u003d CachePoolInfoProto.newBuilder()\n          .setPoolName(p.getPoolName());\n\n      if (p.getOwnerName() !\u003d null)\n        b.setOwnerName(p.getOwnerName());\n\n      if (p.getGroupName() !\u003d null)\n        b.setGroupName(p.getGroupName());\n\n      if (p.getMode() !\u003d null)\n        b.setMode(p.getMode().toShort());\n\n      if (p.getLimit() !\u003d null)\n        b.setLimit(p.getLimit());\n\n      pools.add(b.build());\n    }\n\n    for (CacheDirective directive : directivesById.values()) {\n      CacheDirectiveInfo info \u003d directive.toInfo();\n      CacheDirectiveInfoProto.Builder b \u003d CacheDirectiveInfoProto.newBuilder()\n          .setId(info.getId());\n\n      if (info.getPath() !\u003d null) {\n        b.setPath(info.getPath().toUri().getPath());\n      }\n\n      if (info.getReplication() !\u003d null) {\n        b.setReplication(info.getReplication());\n      }\n\n      if (info.getPool() !\u003d null) {\n        b.setPool(info.getPool());\n      }\n\n      Expiration expiry \u003d info.getExpiration();\n      if (expiry !\u003d null) {\n        assert (!expiry.isRelative());\n        b.setExpiration(PBHelper.convert(expiry));\n      }\n\n      directives.add(b.build());\n    }\n    CacheManagerSection s \u003d CacheManagerSection.newBuilder()\n        .setNextDirectiveId(nextDirectiveId).setNumPools(pools.size())\n        .setNumDirectives(directives.size()).build();\n\n    return new PersistState(s, pools, directives);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java"
    }
  }
}