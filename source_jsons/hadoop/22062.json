{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LocalFetcher.java",
  "functionName": "copyMapOutput",
  "functionId": "copyMapOutput___mapTaskId-TaskAttemptID",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/LocalFetcher.java",
  "functionStartLine": 118,
  "functionEndLine": 165,
  "numCommitsSeen": 6,
  "timeTaken": 2098,
  "changeHistory": [
    "178751ed8c9d47038acf8616c226f1f52e884feb",
    "72d08a0e41efda635baa985d55d67cb059a7c07c",
    "6b710a42e00acca405e085724c89cda016cf7442",
    "b9edad64034a9c8a121ec2b37792c190ba561e26",
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23",
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4"
  ],
  "changeHistoryShort": {
    "178751ed8c9d47038acf8616c226f1f52e884feb": "Ybodychange",
    "72d08a0e41efda635baa985d55d67cb059a7c07c": "Ybodychange",
    "6b710a42e00acca405e085724c89cda016cf7442": "Ybodychange",
    "b9edad64034a9c8a121ec2b37792c190ba561e26": "Ybodychange",
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23": "Ybodychange",
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4": "Yintroduced"
  },
  "changeHistoryDetails": {
    "178751ed8c9d47038acf8616c226f1f52e884feb": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6983. Moving logging APIs over to slf4j in hadoop-mapreduce-client-core. Contributed by Jinjiang Ling.\n",
      "commitDate": "02/11/17 1:43 AM",
      "commitName": "178751ed8c9d47038acf8616c226f1f52e884feb",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "24/06/15 8:29 AM",
      "commitNameOld": "72d08a0e41efda635baa985d55d67cb059a7c07c",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 861.72,
      "commitsBetweenForRepo": 5764,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,48 @@\n   private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {\n     // Figure out where the map task stored its output.\n     Path mapOutputFileName \u003d localMapFiles.get(mapTaskId).getOutputFile();\n     Path indexFileName \u003d mapOutputFileName.suffix(\".index\");\n \n     // Read its index to determine the location of our split\n     // and its size.\n     SpillRecord sr \u003d new SpillRecord(indexFileName, job);\n     IndexRecord ir \u003d sr.getIndex(reduce);\n \n     long compressedLength \u003d ir.partLength;\n     long decompressedLength \u003d ir.rawLength;\n \n     compressedLength -\u003d CryptoUtils.cryptoPadding(job);\n     decompressedLength -\u003d CryptoUtils.cryptoPadding(job);\n \n     // Get the location for the map output - either in-memory or on-disk\n     MapOutput\u003cK, V\u003e mapOutput \u003d merger.reserve(mapTaskId, decompressedLength,\n         id);\n \n     // Check if we can shuffle *now* ...\n     if (mapOutput \u003d\u003d null) {\n       LOG.info(\"fetcher#\" + id + \" - MergeManager returned Status.WAIT ...\");\n       return false;\n     }\n \n     // Go!\n     LOG.info(\"localfetcher#\" + id + \" about to shuffle output of map \" + \n              mapOutput.getMapId() + \" decomp: \" +\n              decompressedLength + \" len: \" + compressedLength + \" to \" +\n              mapOutput.getDescription());\n \n     // now read the file, seek to the appropriate section, and send it.\n     FileSystem localFs \u003d FileSystem.getLocal(job).getRaw();\n     FSDataInputStream inStream \u003d localFs.open(mapOutputFileName);\n     try {\n       inStream \u003d CryptoUtils.wrapIfNecessary(job, inStream);\n       inStream.seek(ir.startOffset + CryptoUtils.cryptoPadding(job));\n       mapOutput.shuffle(LOCALHOST, inStream, compressedLength,\n           decompressedLength, metrics, reporter);\n     } finally {\n-      IOUtils.cleanup(LOG, inStream);\n+      IOUtils.cleanupWithLogger(LOG, inStream);\n     }\n \n     scheduler.copySucceeded(mapTaskId, LOCALHOST, compressedLength, 0, 0,\n         mapOutput);\n     return true; // successful fetch.\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {\n    // Figure out where the map task stored its output.\n    Path mapOutputFileName \u003d localMapFiles.get(mapTaskId).getOutputFile();\n    Path indexFileName \u003d mapOutputFileName.suffix(\".index\");\n\n    // Read its index to determine the location of our split\n    // and its size.\n    SpillRecord sr \u003d new SpillRecord(indexFileName, job);\n    IndexRecord ir \u003d sr.getIndex(reduce);\n\n    long compressedLength \u003d ir.partLength;\n    long decompressedLength \u003d ir.rawLength;\n\n    compressedLength -\u003d CryptoUtils.cryptoPadding(job);\n    decompressedLength -\u003d CryptoUtils.cryptoPadding(job);\n\n    // Get the location for the map output - either in-memory or on-disk\n    MapOutput\u003cK, V\u003e mapOutput \u003d merger.reserve(mapTaskId, decompressedLength,\n        id);\n\n    // Check if we can shuffle *now* ...\n    if (mapOutput \u003d\u003d null) {\n      LOG.info(\"fetcher#\" + id + \" - MergeManager returned Status.WAIT ...\");\n      return false;\n    }\n\n    // Go!\n    LOG.info(\"localfetcher#\" + id + \" about to shuffle output of map \" + \n             mapOutput.getMapId() + \" decomp: \" +\n             decompressedLength + \" len: \" + compressedLength + \" to \" +\n             mapOutput.getDescription());\n\n    // now read the file, seek to the appropriate section, and send it.\n    FileSystem localFs \u003d FileSystem.getLocal(job).getRaw();\n    FSDataInputStream inStream \u003d localFs.open(mapOutputFileName);\n    try {\n      inStream \u003d CryptoUtils.wrapIfNecessary(job, inStream);\n      inStream.seek(ir.startOffset + CryptoUtils.cryptoPadding(job));\n      mapOutput.shuffle(LOCALHOST, inStream, compressedLength,\n          decompressedLength, metrics, reporter);\n    } finally {\n      IOUtils.cleanupWithLogger(LOG, inStream);\n    }\n\n    scheduler.copySucceeded(mapTaskId, LOCALHOST, compressedLength, 0, 0,\n        mapOutput);\n    return true; // successful fetch.\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/LocalFetcher.java",
      "extendedDetails": {}
    },
    "72d08a0e41efda635baa985d55d67cb059a7c07c": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6400. Multiple shuffle transfer fails because input is closed too early. Contributed by Brahma Reddy Battula, Akira AJISAKA, and Gera Shegalov.\n",
      "commitDate": "24/06/15 8:29 AM",
      "commitName": "72d08a0e41efda635baa985d55d67cb059a7c07c",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "14/05/15 4:07 PM",
      "commitNameOld": "6b710a42e00acca405e085724c89cda016cf7442",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 40.68,
      "commitsBetweenForRepo": 281,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,48 @@\n   private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {\n     // Figure out where the map task stored its output.\n     Path mapOutputFileName \u003d localMapFiles.get(mapTaskId).getOutputFile();\n     Path indexFileName \u003d mapOutputFileName.suffix(\".index\");\n \n     // Read its index to determine the location of our split\n     // and its size.\n     SpillRecord sr \u003d new SpillRecord(indexFileName, job);\n     IndexRecord ir \u003d sr.getIndex(reduce);\n \n     long compressedLength \u003d ir.partLength;\n     long decompressedLength \u003d ir.rawLength;\n \n     compressedLength -\u003d CryptoUtils.cryptoPadding(job);\n     decompressedLength -\u003d CryptoUtils.cryptoPadding(job);\n \n     // Get the location for the map output - either in-memory or on-disk\n     MapOutput\u003cK, V\u003e mapOutput \u003d merger.reserve(mapTaskId, decompressedLength,\n         id);\n \n     // Check if we can shuffle *now* ...\n     if (mapOutput \u003d\u003d null) {\n       LOG.info(\"fetcher#\" + id + \" - MergeManager returned Status.WAIT ...\");\n       return false;\n     }\n \n     // Go!\n     LOG.info(\"localfetcher#\" + id + \" about to shuffle output of map \" + \n              mapOutput.getMapId() + \" decomp: \" +\n              decompressedLength + \" len: \" + compressedLength + \" to \" +\n              mapOutput.getDescription());\n \n     // now read the file, seek to the appropriate section, and send it.\n     FileSystem localFs \u003d FileSystem.getLocal(job).getRaw();\n     FSDataInputStream inStream \u003d localFs.open(mapOutputFileName);\n-\n-    inStream \u003d CryptoUtils.wrapIfNecessary(job, inStream);\n-\n     try {\n+      inStream \u003d CryptoUtils.wrapIfNecessary(job, inStream);\n       inStream.seek(ir.startOffset + CryptoUtils.cryptoPadding(job));\n-      mapOutput.shuffle(LOCALHOST, inStream, compressedLength, decompressedLength, metrics, reporter);\n+      mapOutput.shuffle(LOCALHOST, inStream, compressedLength,\n+          decompressedLength, metrics, reporter);\n     } finally {\n-      try {\n-        inStream.close();\n-      } catch (IOException ioe) {\n-        LOG.warn(\"IOException closing inputstream from map output: \"\n-            + ioe.toString());\n-      }\n+      IOUtils.cleanup(LOG, inStream);\n     }\n \n     scheduler.copySucceeded(mapTaskId, LOCALHOST, compressedLength, 0, 0,\n         mapOutput);\n     return true; // successful fetch.\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {\n    // Figure out where the map task stored its output.\n    Path mapOutputFileName \u003d localMapFiles.get(mapTaskId).getOutputFile();\n    Path indexFileName \u003d mapOutputFileName.suffix(\".index\");\n\n    // Read its index to determine the location of our split\n    // and its size.\n    SpillRecord sr \u003d new SpillRecord(indexFileName, job);\n    IndexRecord ir \u003d sr.getIndex(reduce);\n\n    long compressedLength \u003d ir.partLength;\n    long decompressedLength \u003d ir.rawLength;\n\n    compressedLength -\u003d CryptoUtils.cryptoPadding(job);\n    decompressedLength -\u003d CryptoUtils.cryptoPadding(job);\n\n    // Get the location for the map output - either in-memory or on-disk\n    MapOutput\u003cK, V\u003e mapOutput \u003d merger.reserve(mapTaskId, decompressedLength,\n        id);\n\n    // Check if we can shuffle *now* ...\n    if (mapOutput \u003d\u003d null) {\n      LOG.info(\"fetcher#\" + id + \" - MergeManager returned Status.WAIT ...\");\n      return false;\n    }\n\n    // Go!\n    LOG.info(\"localfetcher#\" + id + \" about to shuffle output of map \" + \n             mapOutput.getMapId() + \" decomp: \" +\n             decompressedLength + \" len: \" + compressedLength + \" to \" +\n             mapOutput.getDescription());\n\n    // now read the file, seek to the appropriate section, and send it.\n    FileSystem localFs \u003d FileSystem.getLocal(job).getRaw();\n    FSDataInputStream inStream \u003d localFs.open(mapOutputFileName);\n    try {\n      inStream \u003d CryptoUtils.wrapIfNecessary(job, inStream);\n      inStream.seek(ir.startOffset + CryptoUtils.cryptoPadding(job));\n      mapOutput.shuffle(LOCALHOST, inStream, compressedLength,\n          decompressedLength, metrics, reporter);\n    } finally {\n      IOUtils.cleanup(LOG, inStream);\n    }\n\n    scheduler.copySucceeded(mapTaskId, LOCALHOST, compressedLength, 0, 0,\n        mapOutput);\n    return true; // successful fetch.\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/LocalFetcher.java",
      "extendedDetails": {}
    },
    "6b710a42e00acca405e085724c89cda016cf7442": {
      "type": "Ybodychange",
      "commitMessage": "Fixing MR intermediate spills. Contributed by Arun Suresh.\n",
      "commitDate": "14/05/15 4:07 PM",
      "commitName": "6b710a42e00acca405e085724c89cda016cf7442",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "15/10/14 8:52 AM",
      "commitNameOld": "b9edad64034a9c8a121ec2b37792c190ba561e26",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 211.3,
      "commitsBetweenForRepo": 1815,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,54 @@\n   private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {\n     // Figure out where the map task stored its output.\n     Path mapOutputFileName \u003d localMapFiles.get(mapTaskId).getOutputFile();\n     Path indexFileName \u003d mapOutputFileName.suffix(\".index\");\n \n     // Read its index to determine the location of our split\n     // and its size.\n     SpillRecord sr \u003d new SpillRecord(indexFileName, job);\n     IndexRecord ir \u003d sr.getIndex(reduce);\n \n     long compressedLength \u003d ir.partLength;\n     long decompressedLength \u003d ir.rawLength;\n \n+    compressedLength -\u003d CryptoUtils.cryptoPadding(job);\n+    decompressedLength -\u003d CryptoUtils.cryptoPadding(job);\n+\n     // Get the location for the map output - either in-memory or on-disk\n     MapOutput\u003cK, V\u003e mapOutput \u003d merger.reserve(mapTaskId, decompressedLength,\n         id);\n \n     // Check if we can shuffle *now* ...\n     if (mapOutput \u003d\u003d null) {\n       LOG.info(\"fetcher#\" + id + \" - MergeManager returned Status.WAIT ...\");\n       return false;\n     }\n \n     // Go!\n     LOG.info(\"localfetcher#\" + id + \" about to shuffle output of map \" + \n              mapOutput.getMapId() + \" decomp: \" +\n              decompressedLength + \" len: \" + compressedLength + \" to \" +\n              mapOutput.getDescription());\n \n     // now read the file, seek to the appropriate section, and send it.\n     FileSystem localFs \u003d FileSystem.getLocal(job).getRaw();\n     FSDataInputStream inStream \u003d localFs.open(mapOutputFileName);\n \n     inStream \u003d CryptoUtils.wrapIfNecessary(job, inStream);\n \n     try {\n-      inStream.seek(ir.startOffset);\n-\n+      inStream.seek(ir.startOffset + CryptoUtils.cryptoPadding(job));\n       mapOutput.shuffle(LOCALHOST, inStream, compressedLength, decompressedLength, metrics, reporter);\n     } finally {\n       try {\n         inStream.close();\n       } catch (IOException ioe) {\n         LOG.warn(\"IOException closing inputstream from map output: \"\n             + ioe.toString());\n       }\n     }\n \n     scheduler.copySucceeded(mapTaskId, LOCALHOST, compressedLength, 0, 0,\n         mapOutput);\n     return true; // successful fetch.\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {\n    // Figure out where the map task stored its output.\n    Path mapOutputFileName \u003d localMapFiles.get(mapTaskId).getOutputFile();\n    Path indexFileName \u003d mapOutputFileName.suffix(\".index\");\n\n    // Read its index to determine the location of our split\n    // and its size.\n    SpillRecord sr \u003d new SpillRecord(indexFileName, job);\n    IndexRecord ir \u003d sr.getIndex(reduce);\n\n    long compressedLength \u003d ir.partLength;\n    long decompressedLength \u003d ir.rawLength;\n\n    compressedLength -\u003d CryptoUtils.cryptoPadding(job);\n    decompressedLength -\u003d CryptoUtils.cryptoPadding(job);\n\n    // Get the location for the map output - either in-memory or on-disk\n    MapOutput\u003cK, V\u003e mapOutput \u003d merger.reserve(mapTaskId, decompressedLength,\n        id);\n\n    // Check if we can shuffle *now* ...\n    if (mapOutput \u003d\u003d null) {\n      LOG.info(\"fetcher#\" + id + \" - MergeManager returned Status.WAIT ...\");\n      return false;\n    }\n\n    // Go!\n    LOG.info(\"localfetcher#\" + id + \" about to shuffle output of map \" + \n             mapOutput.getMapId() + \" decomp: \" +\n             decompressedLength + \" len: \" + compressedLength + \" to \" +\n             mapOutput.getDescription());\n\n    // now read the file, seek to the appropriate section, and send it.\n    FileSystem localFs \u003d FileSystem.getLocal(job).getRaw();\n    FSDataInputStream inStream \u003d localFs.open(mapOutputFileName);\n\n    inStream \u003d CryptoUtils.wrapIfNecessary(job, inStream);\n\n    try {\n      inStream.seek(ir.startOffset + CryptoUtils.cryptoPadding(job));\n      mapOutput.shuffle(LOCALHOST, inStream, compressedLength, decompressedLength, metrics, reporter);\n    } finally {\n      try {\n        inStream.close();\n      } catch (IOException ioe) {\n        LOG.warn(\"IOException closing inputstream from map output: \"\n            + ioe.toString());\n      }\n    }\n\n    scheduler.copySucceeded(mapTaskId, LOCALHOST, compressedLength, 0, 0,\n        mapOutput);\n    return true; // successful fetch.\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/LocalFetcher.java",
      "extendedDetails": {}
    },
    "b9edad64034a9c8a121ec2b37792c190ba561e26": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5873. Shuffle bandwidth computation includes time spent waiting for maps. Contributed by Siqi Li\n",
      "commitDate": "15/10/14 8:52 AM",
      "commitName": "b9edad64034a9c8a121ec2b37792c190ba561e26",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "10/07/14 5:43 PM",
      "commitNameOld": "95986dd2fb4527c43fa4c088c61fb7b4bd794d23",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 96.63,
      "commitsBetweenForRepo": 943,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,52 @@\n   private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {\n     // Figure out where the map task stored its output.\n     Path mapOutputFileName \u003d localMapFiles.get(mapTaskId).getOutputFile();\n     Path indexFileName \u003d mapOutputFileName.suffix(\".index\");\n \n     // Read its index to determine the location of our split\n     // and its size.\n     SpillRecord sr \u003d new SpillRecord(indexFileName, job);\n     IndexRecord ir \u003d sr.getIndex(reduce);\n \n     long compressedLength \u003d ir.partLength;\n     long decompressedLength \u003d ir.rawLength;\n \n     // Get the location for the map output - either in-memory or on-disk\n     MapOutput\u003cK, V\u003e mapOutput \u003d merger.reserve(mapTaskId, decompressedLength,\n         id);\n \n     // Check if we can shuffle *now* ...\n     if (mapOutput \u003d\u003d null) {\n       LOG.info(\"fetcher#\" + id + \" - MergeManager returned Status.WAIT ...\");\n       return false;\n     }\n \n     // Go!\n     LOG.info(\"localfetcher#\" + id + \" about to shuffle output of map \" + \n              mapOutput.getMapId() + \" decomp: \" +\n              decompressedLength + \" len: \" + compressedLength + \" to \" +\n              mapOutput.getDescription());\n \n     // now read the file, seek to the appropriate section, and send it.\n     FileSystem localFs \u003d FileSystem.getLocal(job).getRaw();\n     FSDataInputStream inStream \u003d localFs.open(mapOutputFileName);\n \n     inStream \u003d CryptoUtils.wrapIfNecessary(job, inStream);\n \n     try {\n       inStream.seek(ir.startOffset);\n \n       mapOutput.shuffle(LOCALHOST, inStream, compressedLength, decompressedLength, metrics, reporter);\n     } finally {\n       try {\n         inStream.close();\n       } catch (IOException ioe) {\n         LOG.warn(\"IOException closing inputstream from map output: \"\n             + ioe.toString());\n       }\n     }\n \n-    scheduler.copySucceeded(mapTaskId, LOCALHOST, compressedLength, 0,\n+    scheduler.copySucceeded(mapTaskId, LOCALHOST, compressedLength, 0, 0,\n         mapOutput);\n     return true; // successful fetch.\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {\n    // Figure out where the map task stored its output.\n    Path mapOutputFileName \u003d localMapFiles.get(mapTaskId).getOutputFile();\n    Path indexFileName \u003d mapOutputFileName.suffix(\".index\");\n\n    // Read its index to determine the location of our split\n    // and its size.\n    SpillRecord sr \u003d new SpillRecord(indexFileName, job);\n    IndexRecord ir \u003d sr.getIndex(reduce);\n\n    long compressedLength \u003d ir.partLength;\n    long decompressedLength \u003d ir.rawLength;\n\n    // Get the location for the map output - either in-memory or on-disk\n    MapOutput\u003cK, V\u003e mapOutput \u003d merger.reserve(mapTaskId, decompressedLength,\n        id);\n\n    // Check if we can shuffle *now* ...\n    if (mapOutput \u003d\u003d null) {\n      LOG.info(\"fetcher#\" + id + \" - MergeManager returned Status.WAIT ...\");\n      return false;\n    }\n\n    // Go!\n    LOG.info(\"localfetcher#\" + id + \" about to shuffle output of map \" + \n             mapOutput.getMapId() + \" decomp: \" +\n             decompressedLength + \" len: \" + compressedLength + \" to \" +\n             mapOutput.getDescription());\n\n    // now read the file, seek to the appropriate section, and send it.\n    FileSystem localFs \u003d FileSystem.getLocal(job).getRaw();\n    FSDataInputStream inStream \u003d localFs.open(mapOutputFileName);\n\n    inStream \u003d CryptoUtils.wrapIfNecessary(job, inStream);\n\n    try {\n      inStream.seek(ir.startOffset);\n\n      mapOutput.shuffle(LOCALHOST, inStream, compressedLength, decompressedLength, metrics, reporter);\n    } finally {\n      try {\n        inStream.close();\n      } catch (IOException ioe) {\n        LOG.warn(\"IOException closing inputstream from map output: \"\n            + ioe.toString());\n      }\n    }\n\n    scheduler.copySucceeded(mapTaskId, LOCALHOST, compressedLength, 0, 0,\n        mapOutput);\n    return true; // successful fetch.\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/LocalFetcher.java",
      "extendedDetails": {}
    },
    "95986dd2fb4527c43fa4c088c61fb7b4bd794d23": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5890. Support for encrypting Intermediate data and spills in local filesystem. (asuresh via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1609597 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/07/14 5:43 PM",
      "commitName": "95986dd2fb4527c43fa4c088c61fb7b4bd794d23",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "05/08/13 11:36 PM",
      "commitNameOld": "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
      "commitAuthorOld": "Sanford Ryza",
      "daysBetweenCommits": 338.75,
      "commitsBetweenForRepo": 2256,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,52 @@\n   private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {\n     // Figure out where the map task stored its output.\n     Path mapOutputFileName \u003d localMapFiles.get(mapTaskId).getOutputFile();\n     Path indexFileName \u003d mapOutputFileName.suffix(\".index\");\n \n     // Read its index to determine the location of our split\n     // and its size.\n     SpillRecord sr \u003d new SpillRecord(indexFileName, job);\n     IndexRecord ir \u003d sr.getIndex(reduce);\n \n     long compressedLength \u003d ir.partLength;\n     long decompressedLength \u003d ir.rawLength;\n \n     // Get the location for the map output - either in-memory or on-disk\n     MapOutput\u003cK, V\u003e mapOutput \u003d merger.reserve(mapTaskId, decompressedLength,\n         id);\n \n     // Check if we can shuffle *now* ...\n     if (mapOutput \u003d\u003d null) {\n       LOG.info(\"fetcher#\" + id + \" - MergeManager returned Status.WAIT ...\");\n       return false;\n     }\n \n     // Go!\n     LOG.info(\"localfetcher#\" + id + \" about to shuffle output of map \" + \n              mapOutput.getMapId() + \" decomp: \" +\n              decompressedLength + \" len: \" + compressedLength + \" to \" +\n              mapOutput.getDescription());\n \n     // now read the file, seek to the appropriate section, and send it.\n     FileSystem localFs \u003d FileSystem.getLocal(job).getRaw();\n     FSDataInputStream inStream \u003d localFs.open(mapOutputFileName);\n+\n+    inStream \u003d CryptoUtils.wrapIfNecessary(job, inStream);\n+\n     try {\n       inStream.seek(ir.startOffset);\n \n       mapOutput.shuffle(LOCALHOST, inStream, compressedLength, decompressedLength, metrics, reporter);\n     } finally {\n       try {\n         inStream.close();\n       } catch (IOException ioe) {\n         LOG.warn(\"IOException closing inputstream from map output: \"\n             + ioe.toString());\n       }\n     }\n \n     scheduler.copySucceeded(mapTaskId, LOCALHOST, compressedLength, 0,\n         mapOutput);\n     return true; // successful fetch.\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {\n    // Figure out where the map task stored its output.\n    Path mapOutputFileName \u003d localMapFiles.get(mapTaskId).getOutputFile();\n    Path indexFileName \u003d mapOutputFileName.suffix(\".index\");\n\n    // Read its index to determine the location of our split\n    // and its size.\n    SpillRecord sr \u003d new SpillRecord(indexFileName, job);\n    IndexRecord ir \u003d sr.getIndex(reduce);\n\n    long compressedLength \u003d ir.partLength;\n    long decompressedLength \u003d ir.rawLength;\n\n    // Get the location for the map output - either in-memory or on-disk\n    MapOutput\u003cK, V\u003e mapOutput \u003d merger.reserve(mapTaskId, decompressedLength,\n        id);\n\n    // Check if we can shuffle *now* ...\n    if (mapOutput \u003d\u003d null) {\n      LOG.info(\"fetcher#\" + id + \" - MergeManager returned Status.WAIT ...\");\n      return false;\n    }\n\n    // Go!\n    LOG.info(\"localfetcher#\" + id + \" about to shuffle output of map \" + \n             mapOutput.getMapId() + \" decomp: \" +\n             decompressedLength + \" len: \" + compressedLength + \" to \" +\n             mapOutput.getDescription());\n\n    // now read the file, seek to the appropriate section, and send it.\n    FileSystem localFs \u003d FileSystem.getLocal(job).getRaw();\n    FSDataInputStream inStream \u003d localFs.open(mapOutputFileName);\n\n    inStream \u003d CryptoUtils.wrapIfNecessary(job, inStream);\n\n    try {\n      inStream.seek(ir.startOffset);\n\n      mapOutput.shuffle(LOCALHOST, inStream, compressedLength, decompressedLength, metrics, reporter);\n    } finally {\n      try {\n        inStream.close();\n      } catch (IOException ioe) {\n        LOG.warn(\"IOException closing inputstream from map output: \"\n            + ioe.toString());\n      }\n    }\n\n    scheduler.copySucceeded(mapTaskId, LOCALHOST, compressedLength, 0,\n        mapOutput);\n    return true; // successful fetch.\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/LocalFetcher.java",
      "extendedDetails": {}
    },
    "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-434. LocalJobRunner limited to single reducer (Sandy Ryza and Aaron Kimball via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1510866 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/08/13 11:36 PM",
      "commitName": "0cb2fdc3b4fbbaa6153b6421a63082dc006f8eb4",
      "commitAuthor": "Sanford Ryza",
      "diff": "@@ -0,0 +1,49 @@\n+  private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {\n+    // Figure out where the map task stored its output.\n+    Path mapOutputFileName \u003d localMapFiles.get(mapTaskId).getOutputFile();\n+    Path indexFileName \u003d mapOutputFileName.suffix(\".index\");\n+\n+    // Read its index to determine the location of our split\n+    // and its size.\n+    SpillRecord sr \u003d new SpillRecord(indexFileName, job);\n+    IndexRecord ir \u003d sr.getIndex(reduce);\n+\n+    long compressedLength \u003d ir.partLength;\n+    long decompressedLength \u003d ir.rawLength;\n+\n+    // Get the location for the map output - either in-memory or on-disk\n+    MapOutput\u003cK, V\u003e mapOutput \u003d merger.reserve(mapTaskId, decompressedLength,\n+        id);\n+\n+    // Check if we can shuffle *now* ...\n+    if (mapOutput \u003d\u003d null) {\n+      LOG.info(\"fetcher#\" + id + \" - MergeManager returned Status.WAIT ...\");\n+      return false;\n+    }\n+\n+    // Go!\n+    LOG.info(\"localfetcher#\" + id + \" about to shuffle output of map \" + \n+             mapOutput.getMapId() + \" decomp: \" +\n+             decompressedLength + \" len: \" + compressedLength + \" to \" +\n+             mapOutput.getDescription());\n+\n+    // now read the file, seek to the appropriate section, and send it.\n+    FileSystem localFs \u003d FileSystem.getLocal(job).getRaw();\n+    FSDataInputStream inStream \u003d localFs.open(mapOutputFileName);\n+    try {\n+      inStream.seek(ir.startOffset);\n+\n+      mapOutput.shuffle(LOCALHOST, inStream, compressedLength, decompressedLength, metrics, reporter);\n+    } finally {\n+      try {\n+        inStream.close();\n+      } catch (IOException ioe) {\n+        LOG.warn(\"IOException closing inputstream from map output: \"\n+            + ioe.toString());\n+      }\n+    }\n+\n+    scheduler.copySucceeded(mapTaskId, LOCALHOST, compressedLength, 0,\n+        mapOutput);\n+    return true; // successful fetch.\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {\n    // Figure out where the map task stored its output.\n    Path mapOutputFileName \u003d localMapFiles.get(mapTaskId).getOutputFile();\n    Path indexFileName \u003d mapOutputFileName.suffix(\".index\");\n\n    // Read its index to determine the location of our split\n    // and its size.\n    SpillRecord sr \u003d new SpillRecord(indexFileName, job);\n    IndexRecord ir \u003d sr.getIndex(reduce);\n\n    long compressedLength \u003d ir.partLength;\n    long decompressedLength \u003d ir.rawLength;\n\n    // Get the location for the map output - either in-memory or on-disk\n    MapOutput\u003cK, V\u003e mapOutput \u003d merger.reserve(mapTaskId, decompressedLength,\n        id);\n\n    // Check if we can shuffle *now* ...\n    if (mapOutput \u003d\u003d null) {\n      LOG.info(\"fetcher#\" + id + \" - MergeManager returned Status.WAIT ...\");\n      return false;\n    }\n\n    // Go!\n    LOG.info(\"localfetcher#\" + id + \" about to shuffle output of map \" + \n             mapOutput.getMapId() + \" decomp: \" +\n             decompressedLength + \" len: \" + compressedLength + \" to \" +\n             mapOutput.getDescription());\n\n    // now read the file, seek to the appropriate section, and send it.\n    FileSystem localFs \u003d FileSystem.getLocal(job).getRaw();\n    FSDataInputStream inStream \u003d localFs.open(mapOutputFileName);\n    try {\n      inStream.seek(ir.startOffset);\n\n      mapOutput.shuffle(LOCALHOST, inStream, compressedLength, decompressedLength, metrics, reporter);\n    } finally {\n      try {\n        inStream.close();\n      } catch (IOException ioe) {\n        LOG.warn(\"IOException closing inputstream from map output: \"\n            + ioe.toString());\n      }\n    }\n\n    scheduler.copySucceeded(mapTaskId, LOCALHOST, compressedLength, 0,\n        mapOutput);\n    return true; // successful fetch.\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/LocalFetcher.java"
    }
  }
}