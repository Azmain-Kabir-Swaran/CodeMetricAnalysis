{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LogAggregationFileController.java",
  "functionName": "cleanOldLogs",
  "functionId": "cleanOldLogs___remoteNodeLogFileForApp-Path__nodeId-NodeId(modifiers-final)__userUgi-UserGroupInformation",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/LogAggregationFileController.java",
  "functionStartLine": 526,
  "functionEndLine": 577,
  "numCommitsSeen": 62,
  "timeTaken": 3481,
  "changeHistory": [
    "c2cb7ea1ef6532020b69031dbd18b0f9b8369f0f",
    "cb81bac0029fce3a9726df3523f0b692cd3375b8"
  ],
  "changeHistoryShort": {
    "c2cb7ea1ef6532020b69031dbd18b0f9b8369f0f": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "cb81bac0029fce3a9726df3523f0b692cd3375b8": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c2cb7ea1ef6532020b69031dbd18b0f9b8369f0f": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "YARN-6876. Create an abstract log writer for extendability. Contributed by Xuan Gong.\n",
      "commitDate": "24/08/17 1:36 PM",
      "commitName": "c2cb7ea1ef6532020b69031dbd18b0f9b8369f0f",
      "commitAuthor": "Junping Du",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "YARN-6876. Create an abstract log writer for extendability. Contributed by Xuan Gong.\n",
          "commitDate": "24/08/17 1:36 PM",
          "commitName": "c2cb7ea1ef6532020b69031dbd18b0f9b8369f0f",
          "commitAuthor": "Junping Du",
          "commitDateOld": "24/08/17 8:17 AM",
          "commitNameOld": "8196a07c3211385ce85ae24f763b62696edc60b9",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.22,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,53 +1,52 @@\n-  private void cleanOldLogs() {\n+  protected void cleanOldLogs(Path remoteNodeLogFileForApp,\n+      final NodeId nodeId, UserGroupInformation userUgi) {\n     try {\n-      final FileSystem remoteFS \u003d\n-          this.remoteNodeLogFileForApp.getFileSystem(conf);\n-      Path appDir \u003d\n-          this.remoteNodeLogFileForApp.getParent().makeQualified(\n-            remoteFS.getUri(), remoteFS.getWorkingDirectory());\n+      final FileSystem remoteFS \u003d remoteNodeLogFileForApp.getFileSystem(conf);\n+      Path appDir \u003d remoteNodeLogFileForApp.getParent().makeQualified(\n+          remoteFS.getUri(), remoteFS.getWorkingDirectory());\n       Set\u003cFileStatus\u003e status \u003d\n           new HashSet\u003cFileStatus\u003e(Arrays.asList(remoteFS.listStatus(appDir)));\n \n       Iterable\u003cFileStatus\u003e mask \u003d\n           Iterables.filter(status, new Predicate\u003cFileStatus\u003e() {\n             @Override\n             public boolean apply(FileStatus next) {\n               return next.getPath().getName()\n                 .contains(LogAggregationUtils.getNodeString(nodeId))\n                 \u0026\u0026 !next.getPath().getName().endsWith(\n                     LogAggregationUtils.TMP_FILE_SUFFIX);\n             }\n           });\n       status \u003d Sets.newHashSet(mask);\n       // Normally, we just need to delete one oldest log\n       // before we upload a new log.\n       // If we can not delete the older logs in this cycle,\n       // we will delete them in next cycle.\n       if (status.size() \u003e\u003d this.retentionSize) {\n         // sort by the lastModificationTime ascending\n         List\u003cFileStatus\u003e statusList \u003d new ArrayList\u003cFileStatus\u003e(status);\n         Collections.sort(statusList, new Comparator\u003cFileStatus\u003e() {\n           public int compare(FileStatus s1, FileStatus s2) {\n             return s1.getModificationTime() \u003c s2.getModificationTime() ? -1\n                 : s1.getModificationTime() \u003e s2.getModificationTime() ? 1 : 0;\n           }\n         });\n-        for (int i \u003d 0 ; i \u003c\u003d statusList.size() - this.retentionSize; i++) {\n+        for (int i \u003d 0; i \u003c\u003d statusList.size() - this.retentionSize; i++) {\n           final FileStatus remove \u003d statusList.get(i);\n           try {\n             userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n               @Override\n               public Object run() throws Exception {\n                 remoteFS.delete(remove.getPath(), false);\n                 return null;\n               }\n             });\n           } catch (Exception e) {\n             LOG.error(\"Failed to delete \" + remove.getPath(), e);\n           }\n         }\n       }\n     } catch (Exception e) {\n       LOG.error(\"Failed to clean old logs\", e);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void cleanOldLogs(Path remoteNodeLogFileForApp,\n      final NodeId nodeId, UserGroupInformation userUgi) {\n    try {\n      final FileSystem remoteFS \u003d remoteNodeLogFileForApp.getFileSystem(conf);\n      Path appDir \u003d remoteNodeLogFileForApp.getParent().makeQualified(\n          remoteFS.getUri(), remoteFS.getWorkingDirectory());\n      Set\u003cFileStatus\u003e status \u003d\n          new HashSet\u003cFileStatus\u003e(Arrays.asList(remoteFS.listStatus(appDir)));\n\n      Iterable\u003cFileStatus\u003e mask \u003d\n          Iterables.filter(status, new Predicate\u003cFileStatus\u003e() {\n            @Override\n            public boolean apply(FileStatus next) {\n              return next.getPath().getName()\n                .contains(LogAggregationUtils.getNodeString(nodeId))\n                \u0026\u0026 !next.getPath().getName().endsWith(\n                    LogAggregationUtils.TMP_FILE_SUFFIX);\n            }\n          });\n      status \u003d Sets.newHashSet(mask);\n      // Normally, we just need to delete one oldest log\n      // before we upload a new log.\n      // If we can not delete the older logs in this cycle,\n      // we will delete them in next cycle.\n      if (status.size() \u003e\u003d this.retentionSize) {\n        // sort by the lastModificationTime ascending\n        List\u003cFileStatus\u003e statusList \u003d new ArrayList\u003cFileStatus\u003e(status);\n        Collections.sort(statusList, new Comparator\u003cFileStatus\u003e() {\n          public int compare(FileStatus s1, FileStatus s2) {\n            return s1.getModificationTime() \u003c s2.getModificationTime() ? -1\n                : s1.getModificationTime() \u003e s2.getModificationTime() ? 1 : 0;\n          }\n        });\n        for (int i \u003d 0; i \u003c\u003d statusList.size() - this.retentionSize; i++) {\n          final FileStatus remove \u003d statusList.get(i);\n          try {\n            userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n              @Override\n              public Object run() throws Exception {\n                remoteFS.delete(remove.getPath(), false);\n                return null;\n              }\n            });\n          } catch (Exception e) {\n            LOG.error(\"Failed to delete \" + remove.getPath(), e);\n          }\n        }\n      }\n    } catch (Exception e) {\n      LOG.error(\"Failed to clean old logs\", e);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/LogAggregationFileController.java",
          "extendedDetails": {
            "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
            "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/LogAggregationFileController.java",
            "oldMethodName": "cleanOldLogs",
            "newMethodName": "cleanOldLogs"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "YARN-6876. Create an abstract log writer for extendability. Contributed by Xuan Gong.\n",
          "commitDate": "24/08/17 1:36 PM",
          "commitName": "c2cb7ea1ef6532020b69031dbd18b0f9b8369f0f",
          "commitAuthor": "Junping Du",
          "commitDateOld": "24/08/17 8:17 AM",
          "commitNameOld": "8196a07c3211385ce85ae24f763b62696edc60b9",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.22,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,53 +1,52 @@\n-  private void cleanOldLogs() {\n+  protected void cleanOldLogs(Path remoteNodeLogFileForApp,\n+      final NodeId nodeId, UserGroupInformation userUgi) {\n     try {\n-      final FileSystem remoteFS \u003d\n-          this.remoteNodeLogFileForApp.getFileSystem(conf);\n-      Path appDir \u003d\n-          this.remoteNodeLogFileForApp.getParent().makeQualified(\n-            remoteFS.getUri(), remoteFS.getWorkingDirectory());\n+      final FileSystem remoteFS \u003d remoteNodeLogFileForApp.getFileSystem(conf);\n+      Path appDir \u003d remoteNodeLogFileForApp.getParent().makeQualified(\n+          remoteFS.getUri(), remoteFS.getWorkingDirectory());\n       Set\u003cFileStatus\u003e status \u003d\n           new HashSet\u003cFileStatus\u003e(Arrays.asList(remoteFS.listStatus(appDir)));\n \n       Iterable\u003cFileStatus\u003e mask \u003d\n           Iterables.filter(status, new Predicate\u003cFileStatus\u003e() {\n             @Override\n             public boolean apply(FileStatus next) {\n               return next.getPath().getName()\n                 .contains(LogAggregationUtils.getNodeString(nodeId))\n                 \u0026\u0026 !next.getPath().getName().endsWith(\n                     LogAggregationUtils.TMP_FILE_SUFFIX);\n             }\n           });\n       status \u003d Sets.newHashSet(mask);\n       // Normally, we just need to delete one oldest log\n       // before we upload a new log.\n       // If we can not delete the older logs in this cycle,\n       // we will delete them in next cycle.\n       if (status.size() \u003e\u003d this.retentionSize) {\n         // sort by the lastModificationTime ascending\n         List\u003cFileStatus\u003e statusList \u003d new ArrayList\u003cFileStatus\u003e(status);\n         Collections.sort(statusList, new Comparator\u003cFileStatus\u003e() {\n           public int compare(FileStatus s1, FileStatus s2) {\n             return s1.getModificationTime() \u003c s2.getModificationTime() ? -1\n                 : s1.getModificationTime() \u003e s2.getModificationTime() ? 1 : 0;\n           }\n         });\n-        for (int i \u003d 0 ; i \u003c\u003d statusList.size() - this.retentionSize; i++) {\n+        for (int i \u003d 0; i \u003c\u003d statusList.size() - this.retentionSize; i++) {\n           final FileStatus remove \u003d statusList.get(i);\n           try {\n             userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n               @Override\n               public Object run() throws Exception {\n                 remoteFS.delete(remove.getPath(), false);\n                 return null;\n               }\n             });\n           } catch (Exception e) {\n             LOG.error(\"Failed to delete \" + remove.getPath(), e);\n           }\n         }\n       }\n     } catch (Exception e) {\n       LOG.error(\"Failed to clean old logs\", e);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void cleanOldLogs(Path remoteNodeLogFileForApp,\n      final NodeId nodeId, UserGroupInformation userUgi) {\n    try {\n      final FileSystem remoteFS \u003d remoteNodeLogFileForApp.getFileSystem(conf);\n      Path appDir \u003d remoteNodeLogFileForApp.getParent().makeQualified(\n          remoteFS.getUri(), remoteFS.getWorkingDirectory());\n      Set\u003cFileStatus\u003e status \u003d\n          new HashSet\u003cFileStatus\u003e(Arrays.asList(remoteFS.listStatus(appDir)));\n\n      Iterable\u003cFileStatus\u003e mask \u003d\n          Iterables.filter(status, new Predicate\u003cFileStatus\u003e() {\n            @Override\n            public boolean apply(FileStatus next) {\n              return next.getPath().getName()\n                .contains(LogAggregationUtils.getNodeString(nodeId))\n                \u0026\u0026 !next.getPath().getName().endsWith(\n                    LogAggregationUtils.TMP_FILE_SUFFIX);\n            }\n          });\n      status \u003d Sets.newHashSet(mask);\n      // Normally, we just need to delete one oldest log\n      // before we upload a new log.\n      // If we can not delete the older logs in this cycle,\n      // we will delete them in next cycle.\n      if (status.size() \u003e\u003d this.retentionSize) {\n        // sort by the lastModificationTime ascending\n        List\u003cFileStatus\u003e statusList \u003d new ArrayList\u003cFileStatus\u003e(status);\n        Collections.sort(statusList, new Comparator\u003cFileStatus\u003e() {\n          public int compare(FileStatus s1, FileStatus s2) {\n            return s1.getModificationTime() \u003c s2.getModificationTime() ? -1\n                : s1.getModificationTime() \u003e s2.getModificationTime() ? 1 : 0;\n          }\n        });\n        for (int i \u003d 0; i \u003c\u003d statusList.size() - this.retentionSize; i++) {\n          final FileStatus remove \u003d statusList.get(i);\n          try {\n            userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n              @Override\n              public Object run() throws Exception {\n                remoteFS.delete(remove.getPath(), false);\n                return null;\n              }\n            });\n          } catch (Exception e) {\n            LOG.error(\"Failed to delete \" + remove.getPath(), e);\n          }\n        }\n      }\n    } catch (Exception e) {\n      LOG.error(\"Failed to clean old logs\", e);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/LogAggregationFileController.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[protected]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-6876. Create an abstract log writer for extendability. Contributed by Xuan Gong.\n",
          "commitDate": "24/08/17 1:36 PM",
          "commitName": "c2cb7ea1ef6532020b69031dbd18b0f9b8369f0f",
          "commitAuthor": "Junping Du",
          "commitDateOld": "24/08/17 8:17 AM",
          "commitNameOld": "8196a07c3211385ce85ae24f763b62696edc60b9",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.22,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,53 +1,52 @@\n-  private void cleanOldLogs() {\n+  protected void cleanOldLogs(Path remoteNodeLogFileForApp,\n+      final NodeId nodeId, UserGroupInformation userUgi) {\n     try {\n-      final FileSystem remoteFS \u003d\n-          this.remoteNodeLogFileForApp.getFileSystem(conf);\n-      Path appDir \u003d\n-          this.remoteNodeLogFileForApp.getParent().makeQualified(\n-            remoteFS.getUri(), remoteFS.getWorkingDirectory());\n+      final FileSystem remoteFS \u003d remoteNodeLogFileForApp.getFileSystem(conf);\n+      Path appDir \u003d remoteNodeLogFileForApp.getParent().makeQualified(\n+          remoteFS.getUri(), remoteFS.getWorkingDirectory());\n       Set\u003cFileStatus\u003e status \u003d\n           new HashSet\u003cFileStatus\u003e(Arrays.asList(remoteFS.listStatus(appDir)));\n \n       Iterable\u003cFileStatus\u003e mask \u003d\n           Iterables.filter(status, new Predicate\u003cFileStatus\u003e() {\n             @Override\n             public boolean apply(FileStatus next) {\n               return next.getPath().getName()\n                 .contains(LogAggregationUtils.getNodeString(nodeId))\n                 \u0026\u0026 !next.getPath().getName().endsWith(\n                     LogAggregationUtils.TMP_FILE_SUFFIX);\n             }\n           });\n       status \u003d Sets.newHashSet(mask);\n       // Normally, we just need to delete one oldest log\n       // before we upload a new log.\n       // If we can not delete the older logs in this cycle,\n       // we will delete them in next cycle.\n       if (status.size() \u003e\u003d this.retentionSize) {\n         // sort by the lastModificationTime ascending\n         List\u003cFileStatus\u003e statusList \u003d new ArrayList\u003cFileStatus\u003e(status);\n         Collections.sort(statusList, new Comparator\u003cFileStatus\u003e() {\n           public int compare(FileStatus s1, FileStatus s2) {\n             return s1.getModificationTime() \u003c s2.getModificationTime() ? -1\n                 : s1.getModificationTime() \u003e s2.getModificationTime() ? 1 : 0;\n           }\n         });\n-        for (int i \u003d 0 ; i \u003c\u003d statusList.size() - this.retentionSize; i++) {\n+        for (int i \u003d 0; i \u003c\u003d statusList.size() - this.retentionSize; i++) {\n           final FileStatus remove \u003d statusList.get(i);\n           try {\n             userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n               @Override\n               public Object run() throws Exception {\n                 remoteFS.delete(remove.getPath(), false);\n                 return null;\n               }\n             });\n           } catch (Exception e) {\n             LOG.error(\"Failed to delete \" + remove.getPath(), e);\n           }\n         }\n       }\n     } catch (Exception e) {\n       LOG.error(\"Failed to clean old logs\", e);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void cleanOldLogs(Path remoteNodeLogFileForApp,\n      final NodeId nodeId, UserGroupInformation userUgi) {\n    try {\n      final FileSystem remoteFS \u003d remoteNodeLogFileForApp.getFileSystem(conf);\n      Path appDir \u003d remoteNodeLogFileForApp.getParent().makeQualified(\n          remoteFS.getUri(), remoteFS.getWorkingDirectory());\n      Set\u003cFileStatus\u003e status \u003d\n          new HashSet\u003cFileStatus\u003e(Arrays.asList(remoteFS.listStatus(appDir)));\n\n      Iterable\u003cFileStatus\u003e mask \u003d\n          Iterables.filter(status, new Predicate\u003cFileStatus\u003e() {\n            @Override\n            public boolean apply(FileStatus next) {\n              return next.getPath().getName()\n                .contains(LogAggregationUtils.getNodeString(nodeId))\n                \u0026\u0026 !next.getPath().getName().endsWith(\n                    LogAggregationUtils.TMP_FILE_SUFFIX);\n            }\n          });\n      status \u003d Sets.newHashSet(mask);\n      // Normally, we just need to delete one oldest log\n      // before we upload a new log.\n      // If we can not delete the older logs in this cycle,\n      // we will delete them in next cycle.\n      if (status.size() \u003e\u003d this.retentionSize) {\n        // sort by the lastModificationTime ascending\n        List\u003cFileStatus\u003e statusList \u003d new ArrayList\u003cFileStatus\u003e(status);\n        Collections.sort(statusList, new Comparator\u003cFileStatus\u003e() {\n          public int compare(FileStatus s1, FileStatus s2) {\n            return s1.getModificationTime() \u003c s2.getModificationTime() ? -1\n                : s1.getModificationTime() \u003e s2.getModificationTime() ? 1 : 0;\n          }\n        });\n        for (int i \u003d 0; i \u003c\u003d statusList.size() - this.retentionSize; i++) {\n          final FileStatus remove \u003d statusList.get(i);\n          try {\n            userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n              @Override\n              public Object run() throws Exception {\n                remoteFS.delete(remove.getPath(), false);\n                return null;\n              }\n            });\n          } catch (Exception e) {\n            LOG.error(\"Failed to delete \" + remove.getPath(), e);\n          }\n        }\n      }\n    } catch (Exception e) {\n      LOG.error(\"Failed to clean old logs\", e);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/LogAggregationFileController.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-6876. Create an abstract log writer for extendability. Contributed by Xuan Gong.\n",
          "commitDate": "24/08/17 1:36 PM",
          "commitName": "c2cb7ea1ef6532020b69031dbd18b0f9b8369f0f",
          "commitAuthor": "Junping Du",
          "commitDateOld": "24/08/17 8:17 AM",
          "commitNameOld": "8196a07c3211385ce85ae24f763b62696edc60b9",
          "commitAuthorOld": "Steve Loughran",
          "daysBetweenCommits": 0.22,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,53 +1,52 @@\n-  private void cleanOldLogs() {\n+  protected void cleanOldLogs(Path remoteNodeLogFileForApp,\n+      final NodeId nodeId, UserGroupInformation userUgi) {\n     try {\n-      final FileSystem remoteFS \u003d\n-          this.remoteNodeLogFileForApp.getFileSystem(conf);\n-      Path appDir \u003d\n-          this.remoteNodeLogFileForApp.getParent().makeQualified(\n-            remoteFS.getUri(), remoteFS.getWorkingDirectory());\n+      final FileSystem remoteFS \u003d remoteNodeLogFileForApp.getFileSystem(conf);\n+      Path appDir \u003d remoteNodeLogFileForApp.getParent().makeQualified(\n+          remoteFS.getUri(), remoteFS.getWorkingDirectory());\n       Set\u003cFileStatus\u003e status \u003d\n           new HashSet\u003cFileStatus\u003e(Arrays.asList(remoteFS.listStatus(appDir)));\n \n       Iterable\u003cFileStatus\u003e mask \u003d\n           Iterables.filter(status, new Predicate\u003cFileStatus\u003e() {\n             @Override\n             public boolean apply(FileStatus next) {\n               return next.getPath().getName()\n                 .contains(LogAggregationUtils.getNodeString(nodeId))\n                 \u0026\u0026 !next.getPath().getName().endsWith(\n                     LogAggregationUtils.TMP_FILE_SUFFIX);\n             }\n           });\n       status \u003d Sets.newHashSet(mask);\n       // Normally, we just need to delete one oldest log\n       // before we upload a new log.\n       // If we can not delete the older logs in this cycle,\n       // we will delete them in next cycle.\n       if (status.size() \u003e\u003d this.retentionSize) {\n         // sort by the lastModificationTime ascending\n         List\u003cFileStatus\u003e statusList \u003d new ArrayList\u003cFileStatus\u003e(status);\n         Collections.sort(statusList, new Comparator\u003cFileStatus\u003e() {\n           public int compare(FileStatus s1, FileStatus s2) {\n             return s1.getModificationTime() \u003c s2.getModificationTime() ? -1\n                 : s1.getModificationTime() \u003e s2.getModificationTime() ? 1 : 0;\n           }\n         });\n-        for (int i \u003d 0 ; i \u003c\u003d statusList.size() - this.retentionSize; i++) {\n+        for (int i \u003d 0; i \u003c\u003d statusList.size() - this.retentionSize; i++) {\n           final FileStatus remove \u003d statusList.get(i);\n           try {\n             userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n               @Override\n               public Object run() throws Exception {\n                 remoteFS.delete(remove.getPath(), false);\n                 return null;\n               }\n             });\n           } catch (Exception e) {\n             LOG.error(\"Failed to delete \" + remove.getPath(), e);\n           }\n         }\n       }\n     } catch (Exception e) {\n       LOG.error(\"Failed to clean old logs\", e);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void cleanOldLogs(Path remoteNodeLogFileForApp,\n      final NodeId nodeId, UserGroupInformation userUgi) {\n    try {\n      final FileSystem remoteFS \u003d remoteNodeLogFileForApp.getFileSystem(conf);\n      Path appDir \u003d remoteNodeLogFileForApp.getParent().makeQualified(\n          remoteFS.getUri(), remoteFS.getWorkingDirectory());\n      Set\u003cFileStatus\u003e status \u003d\n          new HashSet\u003cFileStatus\u003e(Arrays.asList(remoteFS.listStatus(appDir)));\n\n      Iterable\u003cFileStatus\u003e mask \u003d\n          Iterables.filter(status, new Predicate\u003cFileStatus\u003e() {\n            @Override\n            public boolean apply(FileStatus next) {\n              return next.getPath().getName()\n                .contains(LogAggregationUtils.getNodeString(nodeId))\n                \u0026\u0026 !next.getPath().getName().endsWith(\n                    LogAggregationUtils.TMP_FILE_SUFFIX);\n            }\n          });\n      status \u003d Sets.newHashSet(mask);\n      // Normally, we just need to delete one oldest log\n      // before we upload a new log.\n      // If we can not delete the older logs in this cycle,\n      // we will delete them in next cycle.\n      if (status.size() \u003e\u003d this.retentionSize) {\n        // sort by the lastModificationTime ascending\n        List\u003cFileStatus\u003e statusList \u003d new ArrayList\u003cFileStatus\u003e(status);\n        Collections.sort(statusList, new Comparator\u003cFileStatus\u003e() {\n          public int compare(FileStatus s1, FileStatus s2) {\n            return s1.getModificationTime() \u003c s2.getModificationTime() ? -1\n                : s1.getModificationTime() \u003e s2.getModificationTime() ? 1 : 0;\n          }\n        });\n        for (int i \u003d 0; i \u003c\u003d statusList.size() - this.retentionSize; i++) {\n          final FileStatus remove \u003d statusList.get(i);\n          try {\n            userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n              @Override\n              public Object run() throws Exception {\n                remoteFS.delete(remove.getPath(), false);\n                return null;\n              }\n            });\n          } catch (Exception e) {\n            LOG.error(\"Failed to delete \" + remove.getPath(), e);\n          }\n        }\n      }\n    } catch (Exception e) {\n      LOG.error(\"Failed to clean old logs\", e);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/LogAggregationFileController.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[remoteNodeLogFileForApp-Path, nodeId-NodeId(modifiers-final), userUgi-UserGroupInformation]"
          }
        }
      ]
    },
    "cb81bac0029fce3a9726df3523f0b692cd3375b8": {
      "type": "Yintroduced",
      "commitMessage": "YARN-2583. Modified AggregatedLogDeletionService to be able to delete rolling aggregated logs. Contributed by Xuan Gong.\n",
      "commitDate": "10/10/14 12:11 AM",
      "commitName": "cb81bac0029fce3a9726df3523f0b692cd3375b8",
      "commitAuthor": "Zhijie Shen",
      "diff": "@@ -0,0 +1,53 @@\n+  private void cleanOldLogs() {\n+    try {\n+      final FileSystem remoteFS \u003d\n+          this.remoteNodeLogFileForApp.getFileSystem(conf);\n+      Path appDir \u003d\n+          this.remoteNodeLogFileForApp.getParent().makeQualified(\n+            remoteFS.getUri(), remoteFS.getWorkingDirectory());\n+      Set\u003cFileStatus\u003e status \u003d\n+          new HashSet\u003cFileStatus\u003e(Arrays.asList(remoteFS.listStatus(appDir)));\n+\n+      Iterable\u003cFileStatus\u003e mask \u003d\n+          Iterables.filter(status, new Predicate\u003cFileStatus\u003e() {\n+            @Override\n+            public boolean apply(FileStatus next) {\n+              return next.getPath().getName()\n+                .contains(LogAggregationUtils.getNodeString(nodeId))\n+                \u0026\u0026 !next.getPath().getName().endsWith(\n+                    LogAggregationUtils.TMP_FILE_SUFFIX);\n+            }\n+          });\n+      status \u003d Sets.newHashSet(mask);\n+      // Normally, we just need to delete one oldest log\n+      // before we upload a new log.\n+      // If we can not delete the older logs in this cycle,\n+      // we will delete them in next cycle.\n+      if (status.size() \u003e\u003d this.retentionSize) {\n+        // sort by the lastModificationTime ascending\n+        List\u003cFileStatus\u003e statusList \u003d new ArrayList\u003cFileStatus\u003e(status);\n+        Collections.sort(statusList, new Comparator\u003cFileStatus\u003e() {\n+          public int compare(FileStatus s1, FileStatus s2) {\n+            return s1.getModificationTime() \u003c s2.getModificationTime() ? -1\n+                : s1.getModificationTime() \u003e s2.getModificationTime() ? 1 : 0;\n+          }\n+        });\n+        for (int i \u003d 0 ; i \u003c\u003d statusList.size() - this.retentionSize; i++) {\n+          final FileStatus remove \u003d statusList.get(i);\n+          try {\n+            userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n+              @Override\n+              public Object run() throws Exception {\n+                remoteFS.delete(remove.getPath(), false);\n+                return null;\n+              }\n+            });\n+          } catch (Exception e) {\n+            LOG.error(\"Failed to delete \" + remove.getPath(), e);\n+          }\n+        }\n+      }\n+    } catch (Exception e) {\n+      LOG.error(\"Failed to clean old logs\", e);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void cleanOldLogs() {\n    try {\n      final FileSystem remoteFS \u003d\n          this.remoteNodeLogFileForApp.getFileSystem(conf);\n      Path appDir \u003d\n          this.remoteNodeLogFileForApp.getParent().makeQualified(\n            remoteFS.getUri(), remoteFS.getWorkingDirectory());\n      Set\u003cFileStatus\u003e status \u003d\n          new HashSet\u003cFileStatus\u003e(Arrays.asList(remoteFS.listStatus(appDir)));\n\n      Iterable\u003cFileStatus\u003e mask \u003d\n          Iterables.filter(status, new Predicate\u003cFileStatus\u003e() {\n            @Override\n            public boolean apply(FileStatus next) {\n              return next.getPath().getName()\n                .contains(LogAggregationUtils.getNodeString(nodeId))\n                \u0026\u0026 !next.getPath().getName().endsWith(\n                    LogAggregationUtils.TMP_FILE_SUFFIX);\n            }\n          });\n      status \u003d Sets.newHashSet(mask);\n      // Normally, we just need to delete one oldest log\n      // before we upload a new log.\n      // If we can not delete the older logs in this cycle,\n      // we will delete them in next cycle.\n      if (status.size() \u003e\u003d this.retentionSize) {\n        // sort by the lastModificationTime ascending\n        List\u003cFileStatus\u003e statusList \u003d new ArrayList\u003cFileStatus\u003e(status);\n        Collections.sort(statusList, new Comparator\u003cFileStatus\u003e() {\n          public int compare(FileStatus s1, FileStatus s2) {\n            return s1.getModificationTime() \u003c s2.getModificationTime() ? -1\n                : s1.getModificationTime() \u003e s2.getModificationTime() ? 1 : 0;\n          }\n        });\n        for (int i \u003d 0 ; i \u003c\u003d statusList.size() - this.retentionSize; i++) {\n          final FileStatus remove \u003d statusList.get(i);\n          try {\n            userUgi.doAs(new PrivilegedExceptionAction\u003cObject\u003e() {\n              @Override\n              public Object run() throws Exception {\n                remoteFS.delete(remove.getPath(), false);\n                return null;\n              }\n            });\n          } catch (Exception e) {\n            LOG.error(\"Failed to delete \" + remove.getPath(), e);\n          }\n        }\n      }\n    } catch (Exception e) {\n      LOG.error(\"Failed to clean old logs\", e);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java"
    }
  }
}