{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "NameNodeRpcServer.java",
  "functionName": "addBlock",
  "functionId": "addBlock___src-String__clientName-String__previous-ExtendedBlock__excludedNodes-DatanodeInfo[]__fileId-long__favoredNodes-String[]__addBlockFlags-EnumSet__AddBlockFlag__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
  "functionStartLine": 903,
  "functionEndLine": 914,
  "numCommitsSeen": 663,
  "timeTaken": 13087,
  "changeHistory": [
    "8fb5ca3f405550828a17e689b9c60ddf7fb95ec1",
    "024c87291cb4cc67282fe5645fb827427cc581c6",
    "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7",
    "e5afac5896a1a88e152746598527d91f73cbb724",
    "db334bb8625da97c7e518cbcf477530c7ba7001e",
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a",
    "f98c343c7f11c165bcc0f7cdbaa2a3998b12cfd2",
    "5d2ffde68e2c14ee33fa2ba4a34cb42fbd14b5ec",
    "4525c4a25ba90163c9543116e2bd54239e0dd097",
    "36d1c49486587c2dbb193e8538b1d4510c462fa6",
    "b0632df93ae5d00180b21983d960d50a45f8fb7a",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "8fb5ca3f405550828a17e689b9c60ddf7fb95ec1": "Ybodychange",
    "024c87291cb4cc67282fe5645fb827427cc581c6": "Ybodychange",
    "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7": "Ymultichange(Yparameterchange,Ybodychange)",
    "e5afac5896a1a88e152746598527d91f73cbb724": "Ybodychange",
    "db334bb8625da97c7e518cbcf477530c7ba7001e": "Ybodychange",
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a": "Ybodychange",
    "f98c343c7f11c165bcc0f7cdbaa2a3998b12cfd2": "Ybodychange",
    "5d2ffde68e2c14ee33fa2ba4a34cb42fbd14b5ec": "Ymultichange(Yparameterchange,Ybodychange)",
    "4525c4a25ba90163c9543116e2bd54239e0dd097": "Ymultichange(Yparameterchange,Ybodychange)",
    "36d1c49486587c2dbb193e8538b1d4510c462fa6": "Ybodychange",
    "b0632df93ae5d00180b21983d960d50a45f8fb7a": "Ymovefromfile",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "8fb5ca3f405550828a17e689b9c60ddf7fb95ec1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14611. Move handshake secret field from Token to BlockAccessToken. Contributed by Chen Liang.\n",
      "commitDate": "11/07/19 1:23 PM",
      "commitName": "8fb5ca3f405550828a17e689b9c60ddf7fb95ec1",
      "commitAuthor": "Chen Liang",
      "commitDateOld": "17/06/19 4:20 PM",
      "commitNameOld": "6822193ee6d6ac8b08822fa76c89e1dd61c5ddca",
      "commitAuthorOld": "Santosh Marella",
      "daysBetweenCommits": 23.88,
      "commitsBetweenForRepo": 218,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,12 @@\n   public LocatedBlock addBlock(String src, String clientName,\n       ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n       String[] favoredNodes, EnumSet\u003cAddBlockFlag\u003e addBlockFlags)\n       throws IOException {\n     checkNNStartup();\n     LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n         clientName, previous, excludedNodes, favoredNodes, addBlockFlags);\n     if (locatedBlock !\u003d null) {\n       metrics.incrAddBlockOps();\n     }\n-    if (shouldSendQOP) {\n-      wrapEstablishedQOP(locatedBlock, getEstablishedClientQOP());\n-    }\n     return locatedBlock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n      String[] favoredNodes, EnumSet\u003cAddBlockFlag\u003e addBlockFlags)\n      throws IOException {\n    checkNNStartup();\n    LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodes, favoredNodes, addBlockFlags);\n    if (locatedBlock !\u003d null) {\n      metrics.incrAddBlockOps();\n    }\n    return locatedBlock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
      "extendedDetails": {}
    },
    "024c87291cb4cc67282fe5645fb827427cc581c6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13617. Allow wrapping NN QOP into token in encrypted message. Contributed by Chen Liang\n",
      "commitDate": "13/02/19 12:40 PM",
      "commitName": "024c87291cb4cc67282fe5645fb827427cc581c6",
      "commitAuthor": "Chen Liang",
      "commitDateOld": "24/12/18 9:34 AM",
      "commitNameOld": "652b257478f723a9e119e5e9181f3c7450ac92b5",
      "commitAuthorOld": "Chen Liang",
      "daysBetweenCommits": 51.13,
      "commitsBetweenForRepo": 326,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,15 @@\n   public LocatedBlock addBlock(String src, String clientName,\n       ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n       String[] favoredNodes, EnumSet\u003cAddBlockFlag\u003e addBlockFlags)\n       throws IOException {\n     checkNNStartup();\n     LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n         clientName, previous, excludedNodes, favoredNodes, addBlockFlags);\n     if (locatedBlock !\u003d null) {\n       metrics.incrAddBlockOps();\n     }\n+    if (shouldSendQOP) {\n+      wrapEstablishedQOP(locatedBlock, getEstablishedClientQOP());\n+    }\n     return locatedBlock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n      String[] favoredNodes, EnumSet\u003cAddBlockFlag\u003e addBlockFlags)\n      throws IOException {\n    checkNNStartup();\n    LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodes, favoredNodes, addBlockFlags);\n    if (locatedBlock !\u003d null) {\n      metrics.incrAddBlockOps();\n    }\n    if (shouldSendQOP) {\n      wrapEstablishedQOP(locatedBlock, getEstablishedClientQOP());\n    }\n    return locatedBlock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
      "extendedDetails": {}
    },
    "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-3702. Add an option for NOT writing the blocks locally if there is a datanode on the same box as the client. (Contributed by Lei (Eddy) Xu)\n",
      "commitDate": "27/04/16 2:22 PM",
      "commitName": "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7",
      "commitAuthor": "Lei Xu",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-3702. Add an option for NOT writing the blocks locally if there is a datanode on the same box as the client. (Contributed by Lei (Eddy) Xu)\n",
          "commitDate": "27/04/16 2:22 PM",
          "commitName": "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "21/04/16 10:18 AM",
          "commitNameOld": "b4be288c5d6801988f555a566c2eb793c88a15a4",
          "commitAuthorOld": "Xiaoyu Yao",
          "daysBetweenCommits": 6.17,
          "commitsBetweenForRepo": 45,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,12 +1,12 @@\n   public LocatedBlock addBlock(String src, String clientName,\n       ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n-      String[] favoredNodes)\n+      String[] favoredNodes, EnumSet\u003cAddBlockFlag\u003e addBlockFlags)\n       throws IOException {\n     checkNNStartup();\n     LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n-        clientName, previous, excludedNodes, favoredNodes);\n+        clientName, previous, excludedNodes, favoredNodes, addBlockFlags);\n     if (locatedBlock !\u003d null) {\n       metrics.incrAddBlockOps();\n     }\n     return locatedBlock;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n      String[] favoredNodes, EnumSet\u003cAddBlockFlag\u003e addBlockFlags)\n      throws IOException {\n    checkNNStartup();\n    LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodes, favoredNodes, addBlockFlags);\n    if (locatedBlock !\u003d null) {\n      metrics.incrAddBlockOps();\n    }\n    return locatedBlock;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "[src-String, clientName-String, previous-ExtendedBlock, excludedNodes-DatanodeInfo[], fileId-long, favoredNodes-String[]]",
            "newValue": "[src-String, clientName-String, previous-ExtendedBlock, excludedNodes-DatanodeInfo[], fileId-long, favoredNodes-String[], addBlockFlags-EnumSet\u003cAddBlockFlag\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3702. Add an option for NOT writing the blocks locally if there is a datanode on the same box as the client. (Contributed by Lei (Eddy) Xu)\n",
          "commitDate": "27/04/16 2:22 PM",
          "commitName": "0a152103f19a3e8e1b7f33aeb9dd115ba231d7b7",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "21/04/16 10:18 AM",
          "commitNameOld": "b4be288c5d6801988f555a566c2eb793c88a15a4",
          "commitAuthorOld": "Xiaoyu Yao",
          "daysBetweenCommits": 6.17,
          "commitsBetweenForRepo": 45,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,12 +1,12 @@\n   public LocatedBlock addBlock(String src, String clientName,\n       ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n-      String[] favoredNodes)\n+      String[] favoredNodes, EnumSet\u003cAddBlockFlag\u003e addBlockFlags)\n       throws IOException {\n     checkNNStartup();\n     LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n-        clientName, previous, excludedNodes, favoredNodes);\n+        clientName, previous, excludedNodes, favoredNodes, addBlockFlags);\n     if (locatedBlock !\u003d null) {\n       metrics.incrAddBlockOps();\n     }\n     return locatedBlock;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n      String[] favoredNodes, EnumSet\u003cAddBlockFlag\u003e addBlockFlags)\n      throws IOException {\n    checkNNStartup();\n    LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodes, favoredNodes, addBlockFlags);\n    if (locatedBlock !\u003d null) {\n      metrics.incrAddBlockOps();\n    }\n    return locatedBlock;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {}
        }
      ]
    },
    "e5afac5896a1a88e152746598527d91f73cbb724": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8394. Move getAdditionalBlock() and related functionalities into a separate class. Contributed by Haohui Mai.\n",
      "commitDate": "15/05/15 7:09 PM",
      "commitName": "e5afac5896a1a88e152746598527d91f73cbb724",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 13.38,
      "commitsBetweenForRepo": 197,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,12 @@\n   public LocatedBlock addBlock(String src, String clientName,\n       ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n       String[] favoredNodes)\n       throws IOException {\n     checkNNStartup();\n-    if (stateChangeLog.isDebugEnabled()) {\n-      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n-          + \" fileId\u003d\" + fileId + \" for \" + clientName);\n-    }\n-    Set\u003cNode\u003e excludedNodesSet \u003d null;\n-    if (excludedNodes !\u003d null) {\n-      excludedNodesSet \u003d new HashSet\u003cNode\u003e(excludedNodes.length);\n-      for (Node node : excludedNodes) {\n-        excludedNodesSet.add(node);\n-      }\n-    }\n-    List\u003cString\u003e favoredNodesList \u003d (favoredNodes \u003d\u003d null) ? null\n-        : Arrays.asList(favoredNodes);\n     LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n-        clientName, previous, excludedNodesSet, favoredNodesList);\n-    if (locatedBlock !\u003d null)\n+        clientName, previous, excludedNodes, favoredNodes);\n+    if (locatedBlock !\u003d null) {\n       metrics.incrAddBlockOps();\n+    }\n     return locatedBlock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n      String[] favoredNodes)\n      throws IOException {\n    checkNNStartup();\n    LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodes, favoredNodes);\n    if (locatedBlock !\u003d null) {\n      metrics.incrAddBlockOps();\n    }\n    return locatedBlock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
      "extendedDetails": {}
    },
    "db334bb8625da97c7e518cbcf477530c7ba7001e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3443. Fix NPE when namenode transition to active during startup by adding checkNNStartup() in NameNodeRpcServer.  Contributed by Vinayakumar B\n",
      "commitDate": "21/01/15 11:33 AM",
      "commitName": "db334bb8625da97c7e518cbcf477530c7ba7001e",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "12/01/15 10:50 PM",
      "commitNameOld": "7e9358feb326d48b8c4f00249e7af5023cebd2e2",
      "commitAuthorOld": "Plamen Jeliazkov",
      "daysBetweenCommits": 8.53,
      "commitsBetweenForRepo": 59,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,24 @@\n   public LocatedBlock addBlock(String src, String clientName,\n       ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n       String[] favoredNodes)\n       throws IOException {\n+    checkNNStartup();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n           + \" fileId\u003d\" + fileId + \" for \" + clientName);\n     }\n     Set\u003cNode\u003e excludedNodesSet \u003d null;\n     if (excludedNodes !\u003d null) {\n       excludedNodesSet \u003d new HashSet\u003cNode\u003e(excludedNodes.length);\n       for (Node node : excludedNodes) {\n         excludedNodesSet.add(node);\n       }\n     }\n     List\u003cString\u003e favoredNodesList \u003d (favoredNodes \u003d\u003d null) ? null\n         : Arrays.asList(favoredNodes);\n     LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n         clientName, previous, excludedNodesSet, favoredNodesList);\n     if (locatedBlock !\u003d null)\n       metrics.incrAddBlockOps();\n     return locatedBlock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n      String[] favoredNodes)\n      throws IOException {\n    checkNNStartup();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n          + \" fileId\u003d\" + fileId + \" for \" + clientName);\n    }\n    Set\u003cNode\u003e excludedNodesSet \u003d null;\n    if (excludedNodes !\u003d null) {\n      excludedNodesSet \u003d new HashSet\u003cNode\u003e(excludedNodes.length);\n      for (Node node : excludedNodes) {\n        excludedNodesSet.add(node);\n      }\n    }\n    List\u003cString\u003e favoredNodesList \u003d (favoredNodes \u003d\u003d null) ? null\n        : Arrays.asList(favoredNodes);\n    LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodesSet, favoredNodesList);\n    if (locatedBlock !\u003d null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
      "extendedDetails": {}
    },
    "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a": {
      "type": "Ybodychange",
      "commitMessage": "merge trunk to branch HDFS-4949\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532952 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/10/13 7:14 PM",
      "commitName": "34f08944b7c8d58f531a3f3bf3d4ee4cd3fa643a",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "16/10/13 3:15 PM",
      "commitNameOld": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 0.17,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public LocatedBlock addBlock(String src, String clientName,\n       ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n       String[] favoredNodes)\n       throws IOException {\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n           + \" fileId\u003d\" + fileId + \" for \" + clientName);\n     }\n-    HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n+    Set\u003cNode\u003e excludedNodesSet \u003d null;\n     if (excludedNodes !\u003d null) {\n-      excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n+      excludedNodesSet \u003d new HashSet\u003cNode\u003e(excludedNodes.length);\n       for (Node node : excludedNodes) {\n-        excludedNodesSet.put(node, node);\n+        excludedNodesSet.add(node);\n       }\n     }\n     List\u003cString\u003e favoredNodesList \u003d (favoredNodes \u003d\u003d null) ? null\n         : Arrays.asList(favoredNodes);\n     LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n         clientName, previous, excludedNodesSet, favoredNodesList);\n     if (locatedBlock !\u003d null)\n       metrics.incrAddBlockOps();\n     return locatedBlock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n      String[] favoredNodes)\n      throws IOException {\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n          + \" fileId\u003d\" + fileId + \" for \" + clientName);\n    }\n    Set\u003cNode\u003e excludedNodesSet \u003d null;\n    if (excludedNodes !\u003d null) {\n      excludedNodesSet \u003d new HashSet\u003cNode\u003e(excludedNodes.length);\n      for (Node node : excludedNodes) {\n        excludedNodesSet.add(node);\n      }\n    }\n    List\u003cString\u003e favoredNodesList \u003d (favoredNodes \u003d\u003d null) ? null\n        : Arrays.asList(favoredNodes);\n    LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodesSet, favoredNodesList);\n    if (locatedBlock !\u003d null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
      "extendedDetails": {}
    },
    "f98c343c7f11c165bcc0f7cdbaa2a3998b12cfd2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5207. In BlockPlacementPolicy.chooseTarget(..), change the writer and the excludedNodes parameter types respectively to Node and Set.  Contributed by Junping Du\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1523875 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/09/13 7:38 PM",
      "commitName": "f98c343c7f11c165bcc0f7cdbaa2a3998b12cfd2",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "27/08/13 1:53 PM",
      "commitNameOld": "10a62366a57e2c7f7ee4d47e83b60fb5a5b71200",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 20.24,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   public LocatedBlock addBlock(String src, String clientName,\n       ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n       String[] favoredNodes)\n       throws IOException {\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n           + \" fileId\u003d\" + fileId + \" for \" + clientName);\n     }\n-    HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n+    Set\u003cNode\u003e excludedNodesSet \u003d null;\n     if (excludedNodes !\u003d null) {\n-      excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n+      excludedNodesSet \u003d new HashSet\u003cNode\u003e(excludedNodes.length);\n       for (Node node : excludedNodes) {\n-        excludedNodesSet.put(node, node);\n+        excludedNodesSet.add(node);\n       }\n     }\n     List\u003cString\u003e favoredNodesList \u003d (favoredNodes \u003d\u003d null) ? null\n         : Arrays.asList(favoredNodes);\n     LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n         clientName, previous, excludedNodesSet, favoredNodesList);\n     if (locatedBlock !\u003d null)\n       metrics.incrAddBlockOps();\n     return locatedBlock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n      String[] favoredNodes)\n      throws IOException {\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n          + \" fileId\u003d\" + fileId + \" for \" + clientName);\n    }\n    Set\u003cNode\u003e excludedNodesSet \u003d null;\n    if (excludedNodes !\u003d null) {\n      excludedNodesSet \u003d new HashSet\u003cNode\u003e(excludedNodes.length);\n      for (Node node : excludedNodes) {\n        excludedNodesSet.add(node);\n      }\n    }\n    List\u003cString\u003e favoredNodesList \u003d (favoredNodes \u003d\u003d null) ? null\n        : Arrays.asList(favoredNodes);\n    LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodesSet, favoredNodesList);\n    if (locatedBlock !\u003d null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
      "extendedDetails": {}
    },
    "5d2ffde68e2c14ee33fa2ba4a34cb42fbd14b5ec": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-2576. Enhances the DistributedFileSystem\u0027s create API so that clients can specify favored datanodes for a file\u0027s blocks. Contributed by Devaraj Das and Pritam Damania.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1476395 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/04/13 1:39 PM",
      "commitName": "5d2ffde68e2c14ee33fa2ba4a34cb42fbd14b5ec",
      "commitAuthor": "Devaraj Das",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2576. Enhances the DistributedFileSystem\u0027s create API so that clients can specify favored datanodes for a file\u0027s blocks. Contributed by Devaraj Das and Pritam Damania.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1476395 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "26/04/13 1:39 PM",
          "commitName": "5d2ffde68e2c14ee33fa2ba4a34cb42fbd14b5ec",
          "commitAuthor": "Devaraj Das",
          "commitDateOld": "07/04/13 11:01 AM",
          "commitNameOld": "c5bb615317f1aa8d3cba4cf331f732126655b68e",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 19.11,
          "commitsBetweenForRepo": 105,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,23 @@\n   public LocatedBlock addBlock(String src, String clientName,\n-      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId)\n+      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n+      String[] favoredNodes)\n       throws IOException {\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n           + \" fileId\u003d\" + fileId + \" for \" + clientName);\n     }\n     HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n     if (excludedNodes !\u003d null) {\n       excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n       for (Node node : excludedNodes) {\n         excludedNodesSet.put(node, node);\n       }\n     }\n+    List\u003cString\u003e favoredNodesList \u003d (favoredNodes \u003d\u003d null) ? null\n+        : Arrays.asList(favoredNodes);\n     LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n-        clientName, previous, excludedNodesSet);\n+        clientName, previous, excludedNodesSet, favoredNodesList);\n     if (locatedBlock !\u003d null)\n       metrics.incrAddBlockOps();\n     return locatedBlock;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n      String[] favoredNodes)\n      throws IOException {\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n          + \" fileId\u003d\" + fileId + \" for \" + clientName);\n    }\n    HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n    if (excludedNodes !\u003d null) {\n      excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n      for (Node node : excludedNodes) {\n        excludedNodesSet.put(node, node);\n      }\n    }\n    List\u003cString\u003e favoredNodesList \u003d (favoredNodes \u003d\u003d null) ? null\n        : Arrays.asList(favoredNodes);\n    LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodesSet, favoredNodesList);\n    if (locatedBlock !\u003d null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "[src-String, clientName-String, previous-ExtendedBlock, excludedNodes-DatanodeInfo[], fileId-long]",
            "newValue": "[src-String, clientName-String, previous-ExtendedBlock, excludedNodes-DatanodeInfo[], fileId-long, favoredNodes-String[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2576. Enhances the DistributedFileSystem\u0027s create API so that clients can specify favored datanodes for a file\u0027s blocks. Contributed by Devaraj Das and Pritam Damania.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1476395 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "26/04/13 1:39 PM",
          "commitName": "5d2ffde68e2c14ee33fa2ba4a34cb42fbd14b5ec",
          "commitAuthor": "Devaraj Das",
          "commitDateOld": "07/04/13 11:01 AM",
          "commitNameOld": "c5bb615317f1aa8d3cba4cf331f732126655b68e",
          "commitAuthorOld": "Uma Maheswara Rao G",
          "daysBetweenCommits": 19.11,
          "commitsBetweenForRepo": 105,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,23 @@\n   public LocatedBlock addBlock(String src, String clientName,\n-      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId)\n+      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n+      String[] favoredNodes)\n       throws IOException {\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n           + \" fileId\u003d\" + fileId + \" for \" + clientName);\n     }\n     HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n     if (excludedNodes !\u003d null) {\n       excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n       for (Node node : excludedNodes) {\n         excludedNodesSet.put(node, node);\n       }\n     }\n+    List\u003cString\u003e favoredNodesList \u003d (favoredNodes \u003d\u003d null) ? null\n+        : Arrays.asList(favoredNodes);\n     LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n-        clientName, previous, excludedNodesSet);\n+        clientName, previous, excludedNodesSet, favoredNodesList);\n     if (locatedBlock !\u003d null)\n       metrics.incrAddBlockOps();\n     return locatedBlock;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n      String[] favoredNodes)\n      throws IOException {\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n          + \" fileId\u003d\" + fileId + \" for \" + clientName);\n    }\n    HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n    if (excludedNodes !\u003d null) {\n      excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n      for (Node node : excludedNodes) {\n        excludedNodesSet.put(node, node);\n      }\n    }\n    List\u003cString\u003e favoredNodesList \u003d (favoredNodes \u003d\u003d null) ? null\n        : Arrays.asList(favoredNodes);\n    LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodesSet, favoredNodesList);\n    if (locatedBlock !\u003d null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {}
        }
      ]
    },
    "4525c4a25ba90163c9543116e2bd54239e0dd097": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-4340. Update addBlock() to inculde inode id as additional argument. Contributed Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1443169 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/02/13 11:52 AM",
      "commitName": "4525c4a25ba90163c9543116e2bd54239e0dd097",
      "commitAuthor": "Suresh Srinivas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4340. Update addBlock() to inculde inode id as additional argument. Contributed Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1443169 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/02/13 11:52 AM",
          "commitName": "4525c4a25ba90163c9543116e2bd54239e0dd097",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "10/12/12 4:52 PM",
          "commitNameOld": "47ae6831e9d6331df3cb43e68513228d2e7cfd5b",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 57.79,
          "commitsBetweenForRepo": 264,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,20 @@\n-  public LocatedBlock addBlock(String src,\n-                               String clientName,\n-                               ExtendedBlock previous,\n-                               DatanodeInfo[] excludedNodes)\n+  public LocatedBlock addBlock(String src, String clientName,\n+      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId)\n       throws IOException {\n-    if(stateChangeLog.isDebugEnabled()) {\n-      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \"\n-          +src+\" for \"+clientName);\n+    if (stateChangeLog.isDebugEnabled()) {\n+      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n+          + \" fileId\u003d\" + fileId + \" for \" + clientName);\n     }\n     HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n     if (excludedNodes !\u003d null) {\n       excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n-      for (Node node:excludedNodes) {\n+      for (Node node : excludedNodes) {\n         excludedNodesSet.put(node, node);\n       }\n     }\n-    LocatedBlock locatedBlock \u003d \n-      namesystem.getAdditionalBlock(src, clientName, previous, excludedNodesSet);\n+    LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n+        clientName, previous, excludedNodesSet);\n     if (locatedBlock !\u003d null)\n       metrics.incrAddBlockOps();\n     return locatedBlock;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId)\n      throws IOException {\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n          + \" fileId\u003d\" + fileId + \" for \" + clientName);\n    }\n    HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n    if (excludedNodes !\u003d null) {\n      excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n      for (Node node : excludedNodes) {\n        excludedNodesSet.put(node, node);\n      }\n    }\n    LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodesSet);\n    if (locatedBlock !\u003d null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {
            "oldValue": "[src-String, clientName-String, previous-ExtendedBlock, excludedNodes-DatanodeInfo[]]",
            "newValue": "[src-String, clientName-String, previous-ExtendedBlock, excludedNodes-DatanodeInfo[], fileId-long]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4340. Update addBlock() to inculde inode id as additional argument. Contributed Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1443169 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/02/13 11:52 AM",
          "commitName": "4525c4a25ba90163c9543116e2bd54239e0dd097",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "10/12/12 4:52 PM",
          "commitNameOld": "47ae6831e9d6331df3cb43e68513228d2e7cfd5b",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 57.79,
          "commitsBetweenForRepo": 264,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,20 @@\n-  public LocatedBlock addBlock(String src,\n-                               String clientName,\n-                               ExtendedBlock previous,\n-                               DatanodeInfo[] excludedNodes)\n+  public LocatedBlock addBlock(String src, String clientName,\n+      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId)\n       throws IOException {\n-    if(stateChangeLog.isDebugEnabled()) {\n-      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \"\n-          +src+\" for \"+clientName);\n+    if (stateChangeLog.isDebugEnabled()) {\n+      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n+          + \" fileId\u003d\" + fileId + \" for \" + clientName);\n     }\n     HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n     if (excludedNodes !\u003d null) {\n       excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n-      for (Node node:excludedNodes) {\n+      for (Node node : excludedNodes) {\n         excludedNodesSet.put(node, node);\n       }\n     }\n-    LocatedBlock locatedBlock \u003d \n-      namesystem.getAdditionalBlock(src, clientName, previous, excludedNodesSet);\n+    LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n+        clientName, previous, excludedNodesSet);\n     if (locatedBlock !\u003d null)\n       metrics.incrAddBlockOps();\n     return locatedBlock;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId)\n      throws IOException {\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n          + \" fileId\u003d\" + fileId + \" for \" + clientName);\n    }\n    HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n    if (excludedNodes !\u003d null) {\n      excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n      for (Node node : excludedNodes) {\n        excludedNodesSet.put(node, node);\n      }\n    }\n    LocatedBlock locatedBlock \u003d namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodesSet);\n    if (locatedBlock !\u003d null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
          "extendedDetails": {}
        }
      ]
    },
    "36d1c49486587c2dbb193e8538b1d4510c462fa6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2693. Fix synchronization issues around state transition. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1221582 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/11 7:03 PM",
      "commitName": "36d1c49486587c2dbb193e8538b1d4510c462fa6",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "18/12/11 9:24 PM",
      "commitNameOld": "329717264f8380a1f0fd2cdabd1bf0517ff1067b",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 1.9,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,22 @@\n   public LocatedBlock addBlock(String src,\n                                String clientName,\n                                ExtendedBlock previous,\n                                DatanodeInfo[] excludedNodes)\n       throws IOException {\n-    nn.checkOperation(OperationCategory.WRITE);\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \"\n           +src+\" for \"+clientName);\n     }\n     HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n     if (excludedNodes !\u003d null) {\n       excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n       for (Node node:excludedNodes) {\n         excludedNodesSet.put(node, node);\n       }\n     }\n     LocatedBlock locatedBlock \u003d \n       namesystem.getAdditionalBlock(src, clientName, previous, excludedNodesSet);\n     if (locatedBlock !\u003d null)\n       metrics.incrAddBlockOps();\n     return locatedBlock;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public LocatedBlock addBlock(String src,\n                               String clientName,\n                               ExtendedBlock previous,\n                               DatanodeInfo[] excludedNodes)\n      throws IOException {\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \"\n          +src+\" for \"+clientName);\n    }\n    HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n    if (excludedNodes !\u003d null) {\n      excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n      for (Node node:excludedNodes) {\n        excludedNodesSet.put(node, node);\n      }\n    }\n    LocatedBlock locatedBlock \u003d \n      namesystem.getAdditionalBlock(src, clientName, previous, excludedNodesSet);\n    if (locatedBlock !\u003d null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
      "extendedDetails": {}
    },
    "b0632df93ae5d00180b21983d960d50a45f8fb7a": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-2197. Refactor RPC call implementations out of NameNode class. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1165463 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/09/11 5:41 PM",
      "commitName": "b0632df93ae5d00180b21983d960d50a45f8fb7a",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "05/09/11 5:34 PM",
      "commitNameOld": "d1438b501dae9efc7aa84de35a57e1b8e6f5645e",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public LocatedBlock addBlock(String src,\n                               String clientName,\n                               ExtendedBlock previous,\n                               DatanodeInfo[] excludedNodes)\n      throws IOException {\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \"\n          +src+\" for \"+clientName);\n    }\n    HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n    if (excludedNodes !\u003d null) {\n      excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n      for (Node node:excludedNodes) {\n        excludedNodesSet.put(node, node);\n      }\n    }\n    LocatedBlock locatedBlock \u003d \n      namesystem.getAdditionalBlock(src, clientName, previous, excludedNodesSet);\n    if (locatedBlock !\u003d null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
        "oldMethodName": "addBlock",
        "newMethodName": "addBlock"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public LocatedBlock addBlock(String src,\n                               String clientName,\n                               ExtendedBlock previous,\n                               DatanodeInfo[] excludedNodes)\n      throws IOException {\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \"\n          +src+\" for \"+clientName);\n    }\n    HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n    if (excludedNodes !\u003d null) {\n      excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n      for (Node node:excludedNodes) {\n        excludedNodesSet.put(node, node);\n      }\n    }\n    LocatedBlock locatedBlock \u003d \n      namesystem.getAdditionalBlock(src, clientName, previous, excludedNodesSet);\n    if (locatedBlock !\u003d null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public LocatedBlock addBlock(String src,\n                               String clientName,\n                               ExtendedBlock previous,\n                               DatanodeInfo[] excludedNodes)\n      throws IOException {\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \"\n          +src+\" for \"+clientName);\n    }\n    HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n    if (excludedNodes !\u003d null) {\n      excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n      for (Node node:excludedNodes) {\n        excludedNodesSet.put(node, node);\n      }\n    }\n    LocatedBlock locatedBlock \u003d \n      namesystem.getAdditionalBlock(src, clientName, previous, excludedNodesSet);\n    if (locatedBlock !\u003d null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,22 @@\n+  public LocatedBlock addBlock(String src,\n+                               String clientName,\n+                               ExtendedBlock previous,\n+                               DatanodeInfo[] excludedNodes)\n+      throws IOException {\n+    if(stateChangeLog.isDebugEnabled()) {\n+      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \"\n+          +src+\" for \"+clientName);\n+    }\n+    HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n+    if (excludedNodes !\u003d null) {\n+      excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n+      for (Node node:excludedNodes) {\n+        excludedNodesSet.put(node, node);\n+      }\n+    }\n+    LocatedBlock locatedBlock \u003d \n+      namesystem.getAdditionalBlock(src, clientName, previous, excludedNodesSet);\n+    if (locatedBlock !\u003d null)\n+      metrics.incrAddBlockOps();\n+    return locatedBlock;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public LocatedBlock addBlock(String src,\n                               String clientName,\n                               ExtendedBlock previous,\n                               DatanodeInfo[] excludedNodes)\n      throws IOException {\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \"\n          +src+\" for \"+clientName);\n    }\n    HashMap\u003cNode, Node\u003e excludedNodesSet \u003d null;\n    if (excludedNodes !\u003d null) {\n      excludedNodesSet \u003d new HashMap\u003cNode, Node\u003e(excludedNodes.length);\n      for (Node node:excludedNodes) {\n        excludedNodesSet.put(node, node);\n      }\n    }\n    LocatedBlock locatedBlock \u003d \n      namesystem.getAdditionalBlock(src, clientName, previous, excludedNodesSet);\n    if (locatedBlock !\u003d null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java"
    }
  }
}