{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "Journal.java",
  "functionName": "newEpoch",
  "functionId": "newEpoch___nsInfo-NamespaceInfo__epoch-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
  "functionStartLine": 339,
  "functionEndLine": 365,
  "numCommitsSeen": 64,
  "timeTaken": 2431,
  "changeHistory": [
    "6beb25ab7e4f5454dba0315a296081e61753f301",
    "663e7484c04c197eed53f10a7808140f1c955277",
    "60c20e559b8036410e2d9081b9c60d1e04e56253",
    "c95a1674b61ef2a6963dc64604986ef90a8c636d",
    "f765fdb65701e61887daedb2b369af4be12cb432",
    "b17018e4b821ec860144d8bd38bc1fcb0d7eeaa5",
    "74d4573a23db5586c6e47ff2277aa7c35237da34"
  ],
  "changeHistoryShort": {
    "6beb25ab7e4f5454dba0315a296081e61753f301": "Ybodychange",
    "663e7484c04c197eed53f10a7808140f1c955277": "Ybodychange",
    "60c20e559b8036410e2d9081b9c60d1e04e56253": "Ybodychange",
    "c95a1674b61ef2a6963dc64604986ef90a8c636d": "Ybodychange",
    "f765fdb65701e61887daedb2b369af4be12cb432": "Ybodychange",
    "b17018e4b821ec860144d8bd38bc1fcb0d7eeaa5": "Ybodychange",
    "74d4573a23db5586c6e47ff2277aa7c35237da34": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6beb25ab7e4f5454dba0315a296081e61753f301": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13544. Improve logging for JournalNode in federated cluster.\n",
      "commitDate": "14/05/18 10:12 AM",
      "commitName": "6beb25ab7e4f5454dba0315a296081e61753f301",
      "commitAuthor": "Hanisha Koneru",
      "commitDateOld": "13/10/17 2:22 PM",
      "commitNameOld": "8dd1eeb94fef59feaf19182dd8f1fcf1389c7f34",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 212.83,
      "commitsBetweenForRepo": 2051,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,27 @@\n   synchronized NewEpochResponseProto newEpoch(\n       NamespaceInfo nsInfo, long epoch) throws IOException {\n \n     checkFormatted();\n     storage.checkConsistentNamespace(nsInfo);\n \n     // Check that the new epoch being proposed is in fact newer than\n     // any other that we\u0027ve promised. \n     if (epoch \u003c\u003d getLastPromisedEpoch()) {\n       throw new IOException(\"Proposed epoch \" + epoch + \" \u003c\u003d last promise \" +\n-          getLastPromisedEpoch());\n+          getLastPromisedEpoch() + \" ; journal id: \" + journalId);\n     }\n     \n     updateLastPromisedEpoch(epoch);\n     abortCurSegment();\n     \n     NewEpochResponseProto.Builder builder \u003d\n         NewEpochResponseProto.newBuilder();\n \n     EditLogFile latestFile \u003d scanStorageForLatestEdits();\n \n     if (latestFile !\u003d null) {\n       builder.setLastSegmentTxId(latestFile.getFirstTxId());\n     }\n     \n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized NewEpochResponseProto newEpoch(\n      NamespaceInfo nsInfo, long epoch) throws IOException {\n\n    checkFormatted();\n    storage.checkConsistentNamespace(nsInfo);\n\n    // Check that the new epoch being proposed is in fact newer than\n    // any other that we\u0027ve promised. \n    if (epoch \u003c\u003d getLastPromisedEpoch()) {\n      throw new IOException(\"Proposed epoch \" + epoch + \" \u003c\u003d last promise \" +\n          getLastPromisedEpoch() + \" ; journal id: \" + journalId);\n    }\n    \n    updateLastPromisedEpoch(epoch);\n    abortCurSegment();\n    \n    NewEpochResponseProto.Builder builder \u003d\n        NewEpochResponseProto.newBuilder();\n\n    EditLogFile latestFile \u003d scanStorageForLatestEdits();\n\n    if (latestFile !\u003d null) {\n      builder.setLastSegmentTxId(latestFile.getFirstTxId());\n    }\n    \n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "663e7484c04c197eed53f10a7808140f1c955277": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3950. QJM: misc TODO cleanup, improved log messages, etc. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1387704 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/09/12 11:52 AM",
      "commitName": "663e7484c04c197eed53f10a7808140f1c955277",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "17/09/12 2:51 PM",
      "commitNameOld": "83c14fbd24353b5e882f065faec81e58449afed3",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 1.88,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,27 @@\n   synchronized NewEpochResponseProto newEpoch(\n       NamespaceInfo nsInfo, long epoch) throws IOException {\n \n     checkFormatted();\n     storage.checkConsistentNamespace(nsInfo);\n-    \n+\n+    // Check that the new epoch being proposed is in fact newer than\n+    // any other that we\u0027ve promised. \n     if (epoch \u003c\u003d getLastPromisedEpoch()) {\n       throw new IOException(\"Proposed epoch \" + epoch + \" \u003c\u003d last promise \" +\n           getLastPromisedEpoch());\n     }\n     \n-    lastPromisedEpoch.set(epoch);\n+    updateLastPromisedEpoch(epoch);\n     abortCurSegment();\n     \n     NewEpochResponseProto.Builder builder \u003d\n         NewEpochResponseProto.newBuilder();\n \n     EditLogFile latestFile \u003d scanStorageForLatestEdits();\n \n     if (latestFile !\u003d null) {\n       builder.setLastSegmentTxId(latestFile.getFirstTxId());\n     }\n     \n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized NewEpochResponseProto newEpoch(\n      NamespaceInfo nsInfo, long epoch) throws IOException {\n\n    checkFormatted();\n    storage.checkConsistentNamespace(nsInfo);\n\n    // Check that the new epoch being proposed is in fact newer than\n    // any other that we\u0027ve promised. \n    if (epoch \u003c\u003d getLastPromisedEpoch()) {\n      throw new IOException(\"Proposed epoch \" + epoch + \" \u003c\u003d last promise \" +\n          getLastPromisedEpoch());\n    }\n    \n    updateLastPromisedEpoch(epoch);\n    abortCurSegment();\n    \n    NewEpochResponseProto.Builder builder \u003d\n        NewEpochResponseProto.newBuilder();\n\n    EditLogFile latestFile \u003d scanStorageForLatestEdits();\n\n    if (latestFile !\u003d null) {\n      builder.setLastSegmentTxId(latestFile.getFirstTxId());\n    }\n    \n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "60c20e559b8036410e2d9081b9c60d1e04e56253": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3900. QJM: avoid validating log segments on log rolls. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1383041 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/09/12 11:53 AM",
      "commitName": "60c20e559b8036410e2d9081b9c60d1e04e56253",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "10/09/12 11:51 AM",
      "commitNameOld": "ca4582222e89114e4c61d38fbf973a66d2867abf",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,25 @@\n   synchronized NewEpochResponseProto newEpoch(\n       NamespaceInfo nsInfo, long epoch) throws IOException {\n \n     checkFormatted();\n     storage.checkConsistentNamespace(nsInfo);\n     \n     if (epoch \u003c\u003d getLastPromisedEpoch()) {\n       throw new IOException(\"Proposed epoch \" + epoch + \" \u003c\u003d last promise \" +\n           getLastPromisedEpoch());\n     }\n     \n     lastPromisedEpoch.set(epoch);\n-    if (curSegment !\u003d null) {\n-      curSegment.close();\n-      curSegment \u003d null;\n-      curSegmentTxId \u003d HdfsConstants.INVALID_TXID;\n-    }\n+    abortCurSegment();\n     \n     NewEpochResponseProto.Builder builder \u003d\n         NewEpochResponseProto.newBuilder();\n \n-    // TODO: we only need to do this once, not on writer switchover.\n-    scanStorage();\n+    EditLogFile latestFile \u003d scanStorageForLatestEdits();\n \n-    if (curSegmentTxId !\u003d HdfsConstants.INVALID_TXID) {\n-      builder.setLastSegmentTxId(curSegmentTxId);\n+    if (latestFile !\u003d null) {\n+      builder.setLastSegmentTxId(latestFile.getFirstTxId());\n     }\n     \n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized NewEpochResponseProto newEpoch(\n      NamespaceInfo nsInfo, long epoch) throws IOException {\n\n    checkFormatted();\n    storage.checkConsistentNamespace(nsInfo);\n    \n    if (epoch \u003c\u003d getLastPromisedEpoch()) {\n      throw new IOException(\"Proposed epoch \" + epoch + \" \u003c\u003d last promise \" +\n          getLastPromisedEpoch());\n    }\n    \n    lastPromisedEpoch.set(epoch);\n    abortCurSegment();\n    \n    NewEpochResponseProto.Builder builder \u003d\n        NewEpochResponseProto.newBuilder();\n\n    EditLogFile latestFile \u003d scanStorageForLatestEdits();\n\n    if (latestFile !\u003d null) {\n      builder.setLastSegmentTxId(latestFile.getFirstTxId());\n    }\n    \n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "c95a1674b61ef2a6963dc64604986ef90a8c636d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3799. QJM: handle empty log segments during recovery. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1373183 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/08/12 5:57 PM",
      "commitName": "c95a1674b61ef2a6963dc64604986ef90a8c636d",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "14/08/12 5:54 PM",
      "commitNameOld": "4a9b3c693def87579298fb59b7df0b8892a3508e",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,30 @@\n   synchronized NewEpochResponseProto newEpoch(\n       NamespaceInfo nsInfo, long epoch) throws IOException {\n \n     checkFormatted();\n     storage.checkConsistentNamespace(nsInfo);\n     \n     if (epoch \u003c\u003d getLastPromisedEpoch()) {\n       throw new IOException(\"Proposed epoch \" + epoch + \" \u003c\u003d last promise \" +\n           getLastPromisedEpoch());\n     }\n     \n     lastPromisedEpoch.set(epoch);\n     if (curSegment !\u003d null) {\n       curSegment.close();\n       curSegment \u003d null;\n+      curSegmentTxId \u003d HdfsConstants.INVALID_TXID;\n     }\n     \n     NewEpochResponseProto.Builder builder \u003d\n         NewEpochResponseProto.newBuilder();\n \n     // TODO: we only need to do this once, not on writer switchover.\n     scanStorage();\n \n     if (curSegmentTxId !\u003d HdfsConstants.INVALID_TXID) {\n       builder.setLastSegmentTxId(curSegmentTxId);\n     }\n     \n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized NewEpochResponseProto newEpoch(\n      NamespaceInfo nsInfo, long epoch) throws IOException {\n\n    checkFormatted();\n    storage.checkConsistentNamespace(nsInfo);\n    \n    if (epoch \u003c\u003d getLastPromisedEpoch()) {\n      throw new IOException(\"Proposed epoch \" + epoch + \" \u003c\u003d last promise \" +\n          getLastPromisedEpoch());\n    }\n    \n    lastPromisedEpoch.set(epoch);\n    if (curSegment !\u003d null) {\n      curSegment.close();\n      curSegment \u003d null;\n      curSegmentTxId \u003d HdfsConstants.INVALID_TXID;\n    }\n    \n    NewEpochResponseProto.Builder builder \u003d\n        NewEpochResponseProto.newBuilder();\n\n    // TODO: we only need to do this once, not on writer switchover.\n    scanStorage();\n\n    if (curSegmentTxId !\u003d HdfsConstants.INVALID_TXID) {\n      builder.setLastSegmentTxId(curSegmentTxId);\n    }\n    \n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "f765fdb65701e61887daedb2b369af4be12cb432": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3793. Implement genericized format() in QJM. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1373177 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/08/12 5:48 PM",
      "commitName": "f765fdb65701e61887daedb2b369af4be12cb432",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "25/07/12 2:47 PM",
      "commitNameOld": "b17018e4b821ec860144d8bd38bc1fcb0d7eeaa5",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 20.13,
      "commitsBetweenForRepo": 82,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,29 @@\n   synchronized NewEpochResponseProto newEpoch(\n       NamespaceInfo nsInfo, long epoch) throws IOException {\n \n-    // If the storage is unformatted, format it with this NS.\n-    // Otherwise, check that the NN\u0027s nsinfo matches the storage.\n-    storage.formatIfNecessary(nsInfo);\n+    checkFormatted();\n+    storage.checkConsistentNamespace(nsInfo);\n     \n     if (epoch \u003c\u003d getLastPromisedEpoch()) {\n       throw new IOException(\"Proposed epoch \" + epoch + \" \u003c\u003d last promise \" +\n           getLastPromisedEpoch());\n     }\n     \n     lastPromisedEpoch.set(epoch);\n     if (curSegment !\u003d null) {\n       curSegment.close();\n       curSegment \u003d null;\n     }\n     \n     NewEpochResponseProto.Builder builder \u003d\n         NewEpochResponseProto.newBuilder();\n \n     // TODO: we only need to do this once, not on writer switchover.\n     scanStorage();\n \n     if (curSegmentTxId !\u003d HdfsConstants.INVALID_TXID) {\n       builder.setLastSegmentTxId(curSegmentTxId);\n     }\n     \n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized NewEpochResponseProto newEpoch(\n      NamespaceInfo nsInfo, long epoch) throws IOException {\n\n    checkFormatted();\n    storage.checkConsistentNamespace(nsInfo);\n    \n    if (epoch \u003c\u003d getLastPromisedEpoch()) {\n      throw new IOException(\"Proposed epoch \" + epoch + \" \u003c\u003d last promise \" +\n          getLastPromisedEpoch());\n    }\n    \n    lastPromisedEpoch.set(epoch);\n    if (curSegment !\u003d null) {\n      curSegment.close();\n      curSegment \u003d null;\n    }\n    \n    NewEpochResponseProto.Builder builder \u003d\n        NewEpochResponseProto.newBuilder();\n\n    // TODO: we only need to do this once, not on writer switchover.\n    scanStorage();\n\n    if (curSegmentTxId !\u003d HdfsConstants.INVALID_TXID) {\n      builder.setLastSegmentTxId(curSegmentTxId);\n    }\n    \n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "b17018e4b821ec860144d8bd38bc1fcb0d7eeaa5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3693. JNStorage should read its storage info even before a writer becomes active. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1365794 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/07/12 2:47 PM",
      "commitName": "b17018e4b821ec860144d8bd38bc1fcb0d7eeaa5",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "25/07/12 2:44 PM",
      "commitNameOld": "d2d0736de40c2b2c7872d2438bf1695e23ded5af",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,30 @@\n   synchronized NewEpochResponseProto newEpoch(\n       NamespaceInfo nsInfo, long epoch) throws IOException {\n \n     // If the storage is unformatted, format it with this NS.\n     // Otherwise, check that the NN\u0027s nsinfo matches the storage.\n-    storage.analyzeStorage(nsInfo);\n+    storage.formatIfNecessary(nsInfo);\n     \n     if (epoch \u003c\u003d getLastPromisedEpoch()) {\n       throw new IOException(\"Proposed epoch \" + epoch + \" \u003c\u003d last promise \" +\n           getLastPromisedEpoch());\n     }\n     \n     lastPromisedEpoch.set(epoch);\n     if (curSegment !\u003d null) {\n       curSegment.close();\n       curSegment \u003d null;\n     }\n     \n     NewEpochResponseProto.Builder builder \u003d\n         NewEpochResponseProto.newBuilder();\n \n     // TODO: we only need to do this once, not on writer switchover.\n     scanStorage();\n \n     if (curSegmentTxId !\u003d HdfsConstants.INVALID_TXID) {\n       builder.setLastSegmentTxId(curSegmentTxId);\n     }\n     \n     return builder.build();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized NewEpochResponseProto newEpoch(\n      NamespaceInfo nsInfo, long epoch) throws IOException {\n\n    // If the storage is unformatted, format it with this NS.\n    // Otherwise, check that the NN\u0027s nsinfo matches the storage.\n    storage.formatIfNecessary(nsInfo);\n    \n    if (epoch \u003c\u003d getLastPromisedEpoch()) {\n      throw new IOException(\"Proposed epoch \" + epoch + \" \u003c\u003d last promise \" +\n          getLastPromisedEpoch());\n    }\n    \n    lastPromisedEpoch.set(epoch);\n    if (curSegment !\u003d null) {\n      curSegment.close();\n      curSegment \u003d null;\n    }\n    \n    NewEpochResponseProto.Builder builder \u003d\n        NewEpochResponseProto.newBuilder();\n\n    // TODO: we only need to do this once, not on writer switchover.\n    scanStorage();\n\n    if (curSegmentTxId !\u003d HdfsConstants.INVALID_TXID) {\n      builder.setLastSegmentTxId(curSegmentTxId);\n    }\n    \n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
      "extendedDetails": {}
    },
    "74d4573a23db5586c6e47ff2277aa7c35237da34": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3077. Quorum-based protocol for reading and writing edit logs. Contributed by Todd Lipcon based on initial work from Brandon Li and Hari Mankude.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1363596 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/07/12 5:25 PM",
      "commitName": "74d4573a23db5586c6e47ff2277aa7c35237da34",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,30 @@\n+  synchronized NewEpochResponseProto newEpoch(\n+      NamespaceInfo nsInfo, long epoch) throws IOException {\n+\n+    // If the storage is unformatted, format it with this NS.\n+    // Otherwise, check that the NN\u0027s nsinfo matches the storage.\n+    storage.analyzeStorage(nsInfo);\n+    \n+    if (epoch \u003c\u003d getLastPromisedEpoch()) {\n+      throw new IOException(\"Proposed epoch \" + epoch + \" \u003c\u003d last promise \" +\n+          getLastPromisedEpoch());\n+    }\n+    \n+    lastPromisedEpoch.set(epoch);\n+    if (curSegment !\u003d null) {\n+      curSegment.close();\n+      curSegment \u003d null;\n+    }\n+    \n+    NewEpochResponseProto.Builder builder \u003d\n+        NewEpochResponseProto.newBuilder();\n+\n+    // TODO: we only need to do this once, not on writer switchover.\n+    scanStorage();\n+\n+    if (curSegmentTxId !\u003d HdfsConstants.INVALID_TXID) {\n+      builder.setLastSegmentTxId(curSegmentTxId);\n+    }\n+    \n+    return builder.build();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  synchronized NewEpochResponseProto newEpoch(\n      NamespaceInfo nsInfo, long epoch) throws IOException {\n\n    // If the storage is unformatted, format it with this NS.\n    // Otherwise, check that the NN\u0027s nsinfo matches the storage.\n    storage.analyzeStorage(nsInfo);\n    \n    if (epoch \u003c\u003d getLastPromisedEpoch()) {\n      throw new IOException(\"Proposed epoch \" + epoch + \" \u003c\u003d last promise \" +\n          getLastPromisedEpoch());\n    }\n    \n    lastPromisedEpoch.set(epoch);\n    if (curSegment !\u003d null) {\n      curSegment.close();\n      curSegment \u003d null;\n    }\n    \n    NewEpochResponseProto.Builder builder \u003d\n        NewEpochResponseProto.newBuilder();\n\n    // TODO: we only need to do this once, not on writer switchover.\n    scanStorage();\n\n    if (curSegmentTxId !\u003d HdfsConstants.INVALID_TXID) {\n      builder.setLastSegmentTxId(curSegmentTxId);\n    }\n    \n    return builder.build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java"
    }
  }
}