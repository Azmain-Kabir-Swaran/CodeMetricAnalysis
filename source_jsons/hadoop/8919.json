{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirStatAndListingOp.java",
  "functionName": "getListing",
  "functionId": "getListing___fsd-FSDirectory__iip-INodesInPath__startAfter-byte[]__needLocation-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
  "functionStartLine": 213,
  "functionEndLine": 280,
  "numCommitsSeen": 369,
  "timeTaken": 12779,
  "changeHistory": [
    "ee699dc26c7b660a5222a30782f3bf5cb1e55085",
    "693169ef34f856a27dc09d90a45fb4ec5b66ed2c",
    "0e560f3b8d194c10dce06443979df4074e14b0db",
    "675e9a8f57570771a0219d95940681b067d36b94",
    "b85603e3f85e85da406241b991f3a9974384c3aa",
    "a0730aa5ced7666a8c92f9fb830b615f5f9f477a",
    "ec252ce0fc0998ce13f31af3440c08a236328e5a",
    "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88",
    "22fc46d7659972ff016ccf1c6f781f0c160be26f",
    "9f4bf3bdf9e74800643477cfb18361e01cf6859c",
    "3dadf369d550c2ae393b751cb5a184dbfe2814df",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "c55d609053fe24b3a50fbe17dc1b47717b453ed6",
    "5c97db07fb306842f49d73a67a90cecec19a7833",
    "53a28afe293e5bf185c8d4f2c7aea212e66015c2",
    "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7",
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
    "5776a41da08af653206bb94d7c76c9c4dcce059a",
    "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
    "073bbd805c6680f47bbfcc6e8efd708ad729bca4",
    "1737950d0fc83c68f386881b843c41b0b1e342de"
  ],
  "changeHistoryShort": {
    "ee699dc26c7b660a5222a30782f3bf5cb1e55085": "Ymultichange(Yparameterchange,Ybodychange)",
    "693169ef34f856a27dc09d90a45fb4ec5b66ed2c": "Ybodychange",
    "0e560f3b8d194c10dce06443979df4074e14b0db": "Ybodychange",
    "675e9a8f57570771a0219d95940681b067d36b94": "Ybodychange",
    "b85603e3f85e85da406241b991f3a9974384c3aa": "Ybodychange",
    "a0730aa5ced7666a8c92f9fb830b615f5f9f477a": "Ymultichange(Yparameterchange,Ybodychange)",
    "ec252ce0fc0998ce13f31af3440c08a236328e5a": "Ybodychange",
    "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88": "Ybodychange",
    "22fc46d7659972ff016ccf1c6f781f0c160be26f": "Ybodychange",
    "9f4bf3bdf9e74800643477cfb18361e01cf6859c": "Ybodychange",
    "3dadf369d550c2ae393b751cb5a184dbfe2814df": "Ybodychange",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "c55d609053fe24b3a50fbe17dc1b47717b453ed6": "Ybodychange",
    "5c97db07fb306842f49d73a67a90cecec19a7833": "Ybodychange",
    "53a28afe293e5bf185c8d4f2c7aea212e66015c2": "Ybodychange",
    "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7": "Ybodychange",
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19": "Ymultichange(Yparameterchange,Ybodychange)",
    "5776a41da08af653206bb94d7c76c9c4dcce059a": "Ybodychange",
    "0af44ea8462437f8e7a8271b15a19677fd7f05a1": "Ymultichange(Ymovefromfile,Ymodifierchange,Yexceptionschange,Ybodychange,Yparameterchange)",
    "073bbd805c6680f47bbfcc6e8efd708ad729bca4": "Ybodychange",
    "1737950d0fc83c68f386881b843c41b0b1e342de": "Ybodychange"
  },
  "changeHistoryDetails": {
    "ee699dc26c7b660a5222a30782f3bf5cb1e55085": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-14921. Remove SuperUser Check in Setting Storage Policy in FileStatus During Listing. Contributed by Ayush Saxena.\n",
      "commitDate": "23/10/19 11:44 PM",
      "commitName": "ee699dc26c7b660a5222a30782f3bf5cb1e55085",
      "commitAuthor": "Vinayakumar B",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-14921. Remove SuperUser Check in Setting Storage Policy in FileStatus During Listing. Contributed by Ayush Saxena.\n",
          "commitDate": "23/10/19 11:44 PM",
          "commitName": "ee699dc26c7b660a5222a30782f3bf5cb1e55085",
          "commitAuthor": "Vinayakumar B",
          "commitDateOld": "21/10/19 5:31 PM",
          "commitNameOld": "72003b19bf4c652b53625984d109542abd0cf20e",
          "commitAuthorOld": "Wei-Chiu Chuang",
          "daysBetweenCommits": 2.26,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,69 +1,68 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n-      byte[] startAfter, boolean needLocation, boolean includeStoragePolicy)\n+      byte[] startAfter, boolean needLocation)\n       throws IOException {\n     if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n       return getReservedListing(fsd);\n     }\n \n     fsd.readLock();\n     try {\n       if (iip.isDotSnapshotDir()) {\n         return getSnapshotsListing(fsd, iip, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null) {\n         return null;\n       }\n \n-      byte parentStoragePolicy \u003d includeStoragePolicy\n-          ? targetNode.getStoragePolicyID()\n-          : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n+      byte parentStoragePolicy \u003d targetNode.getStoragePolicyID();\n \n       if (!targetNode.isDirectory()) {\n         // return the file\u0027s status. note that the iip already includes the\n         // target INode\n         return new DirectoryListing(\n             new HdfsFileStatus[]{ createFileStatus(\n                 fsd, iip, null, parentStoragePolicy, needLocation, false)\n             }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n         INode child \u003d contents.get(startChild+i);\n-        byte childStoragePolicy \u003d (includeStoragePolicy \u0026\u0026 !child.isSymlink())\n-            ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n-                                 parentStoragePolicy)\n+        byte childStoragePolicy \u003d\n+            !child.isSymlink()\n+                ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n+                    parentStoragePolicy)\n             : parentStoragePolicy;\n         listing[i] \u003d createFileStatus(fsd, iip, child, childStoragePolicy,\n             needLocation, false);\n         listingCnt++;\n         if (listing[i] instanceof HdfsLocatedFileStatus) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getLocatedBlocks();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      byte[] startAfter, boolean needLocation)\n      throws IOException {\n    if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n      return getReservedListing(fsd);\n    }\n\n    fsd.readLock();\n    try {\n      if (iip.isDotSnapshotDir()) {\n        return getSnapshotsListing(fsd, iip, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null) {\n        return null;\n      }\n\n      byte parentStoragePolicy \u003d targetNode.getStoragePolicyID();\n\n      if (!targetNode.isDirectory()) {\n        // return the file\u0027s status. note that the iip already includes the\n        // target INode\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, iip, null, parentStoragePolicy, needLocation, false)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n        INode child \u003d contents.get(startChild+i);\n        byte childStoragePolicy \u003d\n            !child.isSymlink()\n                ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n                    parentStoragePolicy)\n            : parentStoragePolicy;\n        listing[i] \u003d createFileStatus(fsd, iip, child, childStoragePolicy,\n            needLocation, false);\n        listingCnt++;\n        if (listing[i] instanceof HdfsLocatedFileStatus) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getLocatedBlocks();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, iip-INodesInPath, startAfter-byte[], needLocation-boolean, includeStoragePolicy-boolean]",
            "newValue": "[fsd-FSDirectory, iip-INodesInPath, startAfter-byte[], needLocation-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-14921. Remove SuperUser Check in Setting Storage Policy in FileStatus During Listing. Contributed by Ayush Saxena.\n",
          "commitDate": "23/10/19 11:44 PM",
          "commitName": "ee699dc26c7b660a5222a30782f3bf5cb1e55085",
          "commitAuthor": "Vinayakumar B",
          "commitDateOld": "21/10/19 5:31 PM",
          "commitNameOld": "72003b19bf4c652b53625984d109542abd0cf20e",
          "commitAuthorOld": "Wei-Chiu Chuang",
          "daysBetweenCommits": 2.26,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,69 +1,68 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n-      byte[] startAfter, boolean needLocation, boolean includeStoragePolicy)\n+      byte[] startAfter, boolean needLocation)\n       throws IOException {\n     if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n       return getReservedListing(fsd);\n     }\n \n     fsd.readLock();\n     try {\n       if (iip.isDotSnapshotDir()) {\n         return getSnapshotsListing(fsd, iip, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null) {\n         return null;\n       }\n \n-      byte parentStoragePolicy \u003d includeStoragePolicy\n-          ? targetNode.getStoragePolicyID()\n-          : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n+      byte parentStoragePolicy \u003d targetNode.getStoragePolicyID();\n \n       if (!targetNode.isDirectory()) {\n         // return the file\u0027s status. note that the iip already includes the\n         // target INode\n         return new DirectoryListing(\n             new HdfsFileStatus[]{ createFileStatus(\n                 fsd, iip, null, parentStoragePolicy, needLocation, false)\n             }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n         INode child \u003d contents.get(startChild+i);\n-        byte childStoragePolicy \u003d (includeStoragePolicy \u0026\u0026 !child.isSymlink())\n-            ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n-                                 parentStoragePolicy)\n+        byte childStoragePolicy \u003d\n+            !child.isSymlink()\n+                ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n+                    parentStoragePolicy)\n             : parentStoragePolicy;\n         listing[i] \u003d createFileStatus(fsd, iip, child, childStoragePolicy,\n             needLocation, false);\n         listingCnt++;\n         if (listing[i] instanceof HdfsLocatedFileStatus) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getLocatedBlocks();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      byte[] startAfter, boolean needLocation)\n      throws IOException {\n    if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n      return getReservedListing(fsd);\n    }\n\n    fsd.readLock();\n    try {\n      if (iip.isDotSnapshotDir()) {\n        return getSnapshotsListing(fsd, iip, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null) {\n        return null;\n      }\n\n      byte parentStoragePolicy \u003d targetNode.getStoragePolicyID();\n\n      if (!targetNode.isDirectory()) {\n        // return the file\u0027s status. note that the iip already includes the\n        // target INode\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, iip, null, parentStoragePolicy, needLocation, false)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n        INode child \u003d contents.get(startChild+i);\n        byte childStoragePolicy \u003d\n            !child.isSymlink()\n                ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n                    parentStoragePolicy)\n            : parentStoragePolicy;\n        listing[i] \u003d createFileStatus(fsd, iip, child, childStoragePolicy,\n            needLocation, false);\n        listingCnt++;\n        if (listing[i] instanceof HdfsLocatedFileStatus) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getLocatedBlocks();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "693169ef34f856a27dc09d90a45fb4ec5b66ed2c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12882. Support full open(PathHandle) contract in HDFS\n",
      "commitDate": "11/12/17 8:14 PM",
      "commitName": "693169ef34f856a27dc09d90a45fb4ec5b66ed2c",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "29/11/17 8:28 PM",
      "commitNameOld": "0e560f3b8d194c10dce06443979df4074e14b0db",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 11.99,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,69 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n       byte[] startAfter, boolean needLocation, boolean includeStoragePolicy)\n       throws IOException {\n     if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n       return getReservedListing(fsd);\n     }\n \n     fsd.readLock();\n     try {\n       if (iip.isDotSnapshotDir()) {\n         return getSnapshotsListing(fsd, iip, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null) {\n         return null;\n       }\n \n       byte parentStoragePolicy \u003d includeStoragePolicy\n           ? targetNode.getStoragePolicyID()\n           : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         // return the file\u0027s status. note that the iip already includes the\n         // target INode\n         return new DirectoryListing(\n             new HdfsFileStatus[]{ createFileStatus(\n-                fsd, iip, null, parentStoragePolicy, needLocation)\n+                fsd, iip, null, parentStoragePolicy, needLocation, false)\n             }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n         INode child \u003d contents.get(startChild+i);\n         byte childStoragePolicy \u003d (includeStoragePolicy \u0026\u0026 !child.isSymlink())\n             ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n                                  parentStoragePolicy)\n             : parentStoragePolicy;\n-        listing[i] \u003d\n-            createFileStatus(fsd, iip, child, childStoragePolicy, needLocation);\n+        listing[i] \u003d createFileStatus(fsd, iip, child, childStoragePolicy,\n+            needLocation, false);\n         listingCnt++;\n         if (listing[i] instanceof HdfsLocatedFileStatus) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getLocatedBlocks();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      byte[] startAfter, boolean needLocation, boolean includeStoragePolicy)\n      throws IOException {\n    if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n      return getReservedListing(fsd);\n    }\n\n    fsd.readLock();\n    try {\n      if (iip.isDotSnapshotDir()) {\n        return getSnapshotsListing(fsd, iip, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null) {\n        return null;\n      }\n\n      byte parentStoragePolicy \u003d includeStoragePolicy\n          ? targetNode.getStoragePolicyID()\n          : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        // return the file\u0027s status. note that the iip already includes the\n        // target INode\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, iip, null, parentStoragePolicy, needLocation, false)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n        INode child \u003d contents.get(startChild+i);\n        byte childStoragePolicy \u003d (includeStoragePolicy \u0026\u0026 !child.isSymlink())\n            ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n                                 parentStoragePolicy)\n            : parentStoragePolicy;\n        listing[i] \u003d createFileStatus(fsd, iip, child, childStoragePolicy,\n            needLocation, false);\n        listingCnt++;\n        if (listing[i] instanceof HdfsLocatedFileStatus) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getLocatedBlocks();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "0e560f3b8d194c10dce06443979df4074e14b0db": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12681. Make HdfsLocatedFileStatus a subtype of LocatedFileStatus\n",
      "commitDate": "29/11/17 8:28 PM",
      "commitName": "0e560f3b8d194c10dce06443979df4074e14b0db",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "15/11/17 7:20 PM",
      "commitNameOld": "675e9a8f57570771a0219d95940681b067d36b94",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 14.05,
      "commitsBetweenForRepo": 52,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,69 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n       byte[] startAfter, boolean needLocation, boolean includeStoragePolicy)\n       throws IOException {\n     if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n       return getReservedListing(fsd);\n     }\n \n     fsd.readLock();\n     try {\n       if (iip.isDotSnapshotDir()) {\n         return getSnapshotsListing(fsd, iip, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null) {\n         return null;\n       }\n \n       byte parentStoragePolicy \u003d includeStoragePolicy\n           ? targetNode.getStoragePolicyID()\n           : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         // return the file\u0027s status. note that the iip already includes the\n         // target INode\n         return new DirectoryListing(\n             new HdfsFileStatus[]{ createFileStatus(\n                 fsd, iip, null, parentStoragePolicy, needLocation)\n             }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n         INode child \u003d contents.get(startChild+i);\n         byte childStoragePolicy \u003d (includeStoragePolicy \u0026\u0026 !child.isSymlink())\n             ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n                                  parentStoragePolicy)\n             : parentStoragePolicy;\n         listing[i] \u003d\n             createFileStatus(fsd, iip, child, childStoragePolicy, needLocation);\n         listingCnt++;\n         if (listing[i] instanceof HdfsLocatedFileStatus) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n-                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n+                ((HdfsLocatedFileStatus)listing[i]).getLocatedBlocks();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      byte[] startAfter, boolean needLocation, boolean includeStoragePolicy)\n      throws IOException {\n    if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n      return getReservedListing(fsd);\n    }\n\n    fsd.readLock();\n    try {\n      if (iip.isDotSnapshotDir()) {\n        return getSnapshotsListing(fsd, iip, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null) {\n        return null;\n      }\n\n      byte parentStoragePolicy \u003d includeStoragePolicy\n          ? targetNode.getStoragePolicyID()\n          : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        // return the file\u0027s status. note that the iip already includes the\n        // target INode\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, iip, null, parentStoragePolicy, needLocation)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n        INode child \u003d contents.get(startChild+i);\n        byte childStoragePolicy \u003d (includeStoragePolicy \u0026\u0026 !child.isSymlink())\n            ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n                                 parentStoragePolicy)\n            : parentStoragePolicy;\n        listing[i] \u003d\n            createFileStatus(fsd, iip, child, childStoragePolicy, needLocation);\n        listingCnt++;\n        if (listing[i] instanceof HdfsLocatedFileStatus) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getLocatedBlocks();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "675e9a8f57570771a0219d95940681b067d36b94": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-12681. Fold HdfsLocatedFileStatus into HdfsFileStatus.\"\n\nThis reverts commit b85603e3f85e85da406241b991f3a9974384c3aa.\n",
      "commitDate": "15/11/17 7:20 PM",
      "commitName": "675e9a8f57570771a0219d95940681b067d36b94",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "03/11/17 2:30 PM",
      "commitNameOld": "b85603e3f85e85da406241b991f3a9974384c3aa",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 12.24,
      "commitsBetweenForRepo": 169,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,68 +1,69 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n       byte[] startAfter, boolean needLocation, boolean includeStoragePolicy)\n       throws IOException {\n     if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n       return getReservedListing(fsd);\n     }\n \n     fsd.readLock();\n     try {\n       if (iip.isDotSnapshotDir()) {\n         return getSnapshotsListing(fsd, iip, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null) {\n         return null;\n       }\n \n       byte parentStoragePolicy \u003d includeStoragePolicy\n           ? targetNode.getStoragePolicyID()\n           : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         // return the file\u0027s status. note that the iip already includes the\n         // target INode\n         return new DirectoryListing(\n             new HdfsFileStatus[]{ createFileStatus(\n                 fsd, iip, null, parentStoragePolicy, needLocation)\n             }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n         INode child \u003d contents.get(startChild+i);\n         byte childStoragePolicy \u003d (includeStoragePolicy \u0026\u0026 !child.isSymlink())\n             ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n                                  parentStoragePolicy)\n             : parentStoragePolicy;\n         listing[i] \u003d\n             createFileStatus(fsd, iip, child, childStoragePolicy, needLocation);\n         listingCnt++;\n-        LocatedBlocks blks \u003d listing[i].getLocatedBlocks();\n-        if (blks !\u003d null) {\n-          // Once we  hit lsLimit locations, stop.\n-          // This helps to prevent excessively large response payloads.\n-          // Approximate #locations with locatedBlockCount() * repl_factor\n-          locationBudget -\u003d\n-              blks.locatedBlockCount() * listing[i].getReplication();\n+        if (listing[i] instanceof HdfsLocatedFileStatus) {\n+            // Once we  hit lsLimit locations, stop.\n+            // This helps to prevent excessively large response payloads.\n+            // Approximate #locations with locatedBlockCount() * repl_factor\n+            LocatedBlocks blks \u003d\n+                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n+            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n+               blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      byte[] startAfter, boolean needLocation, boolean includeStoragePolicy)\n      throws IOException {\n    if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n      return getReservedListing(fsd);\n    }\n\n    fsd.readLock();\n    try {\n      if (iip.isDotSnapshotDir()) {\n        return getSnapshotsListing(fsd, iip, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null) {\n        return null;\n      }\n\n      byte parentStoragePolicy \u003d includeStoragePolicy\n          ? targetNode.getStoragePolicyID()\n          : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        // return the file\u0027s status. note that the iip already includes the\n        // target INode\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, iip, null, parentStoragePolicy, needLocation)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n        INode child \u003d contents.get(startChild+i);\n        byte childStoragePolicy \u003d (includeStoragePolicy \u0026\u0026 !child.isSymlink())\n            ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n                                 parentStoragePolicy)\n            : parentStoragePolicy;\n        listing[i] \u003d\n            createFileStatus(fsd, iip, child, childStoragePolicy, needLocation);\n        listingCnt++;\n        if (listing[i] instanceof HdfsLocatedFileStatus) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "b85603e3f85e85da406241b991f3a9974384c3aa": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12681. Fold HdfsLocatedFileStatus into HdfsFileStatus.\n",
      "commitDate": "03/11/17 2:30 PM",
      "commitName": "b85603e3f85e85da406241b991f3a9974384c3aa",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "27/10/17 3:36 PM",
      "commitNameOld": "d55a84951abe87a31c17bd4b84cd309ed202e540",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 6.95,
      "commitsBetweenForRepo": 50,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,69 +1,68 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n       byte[] startAfter, boolean needLocation, boolean includeStoragePolicy)\n       throws IOException {\n     if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n       return getReservedListing(fsd);\n     }\n \n     fsd.readLock();\n     try {\n       if (iip.isDotSnapshotDir()) {\n         return getSnapshotsListing(fsd, iip, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null) {\n         return null;\n       }\n \n       byte parentStoragePolicy \u003d includeStoragePolicy\n           ? targetNode.getStoragePolicyID()\n           : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         // return the file\u0027s status. note that the iip already includes the\n         // target INode\n         return new DirectoryListing(\n             new HdfsFileStatus[]{ createFileStatus(\n                 fsd, iip, null, parentStoragePolicy, needLocation)\n             }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n         INode child \u003d contents.get(startChild+i);\n         byte childStoragePolicy \u003d (includeStoragePolicy \u0026\u0026 !child.isSymlink())\n             ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n                                  parentStoragePolicy)\n             : parentStoragePolicy;\n         listing[i] \u003d\n             createFileStatus(fsd, iip, child, childStoragePolicy, needLocation);\n         listingCnt++;\n-        if (listing[i] instanceof HdfsLocatedFileStatus) {\n-            // Once we  hit lsLimit locations, stop.\n-            // This helps to prevent excessively large response payloads.\n-            // Approximate #locations with locatedBlockCount() * repl_factor\n-            LocatedBlocks blks \u003d\n-                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n-            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n-               blks.locatedBlockCount() * listing[i].getReplication();\n+        LocatedBlocks blks \u003d listing[i].getLocatedBlocks();\n+        if (blks !\u003d null) {\n+          // Once we  hit lsLimit locations, stop.\n+          // This helps to prevent excessively large response payloads.\n+          // Approximate #locations with locatedBlockCount() * repl_factor\n+          locationBudget -\u003d\n+              blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      byte[] startAfter, boolean needLocation, boolean includeStoragePolicy)\n      throws IOException {\n    if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n      return getReservedListing(fsd);\n    }\n\n    fsd.readLock();\n    try {\n      if (iip.isDotSnapshotDir()) {\n        return getSnapshotsListing(fsd, iip, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null) {\n        return null;\n      }\n\n      byte parentStoragePolicy \u003d includeStoragePolicy\n          ? targetNode.getStoragePolicyID()\n          : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        // return the file\u0027s status. note that the iip already includes the\n        // target INode\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, iip, null, parentStoragePolicy, needLocation)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n        INode child \u003d contents.get(startChild+i);\n        byte childStoragePolicy \u003d (includeStoragePolicy \u0026\u0026 !child.isSymlink())\n            ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n                                 parentStoragePolicy)\n            : parentStoragePolicy;\n        listing[i] \u003d\n            createFileStatus(fsd, iip, child, childStoragePolicy, needLocation);\n        listingCnt++;\n        LocatedBlocks blks \u003d listing[i].getLocatedBlocks();\n        if (blks !\u003d null) {\n          // Once we  hit lsLimit locations, stop.\n          // This helps to prevent excessively large response payloads.\n          // Approximate #locations with locatedBlockCount() * repl_factor\n          locationBudget -\u003d\n              blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "a0730aa5ced7666a8c92f9fb830b615f5f9f477a": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10851. FSDirStatAndListingOp: stop passing path as string. Contributed by Daryn Sharp.\n",
      "commitDate": "30/09/16 11:03 AM",
      "commitName": "a0730aa5ced7666a8c92f9fb830b615f5f9f477a",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10851. FSDirStatAndListingOp: stop passing path as string. Contributed by Daryn Sharp.\n",
          "commitDate": "30/09/16 11:03 AM",
          "commitName": "a0730aa5ced7666a8c92f9fb830b615f5f9f477a",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "24/08/16 1:21 PM",
          "commitNameOld": "a1f3293762dddb0ca953d1145f5b53d9086b25b8",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 36.9,
          "commitsBetweenForRepo": 208,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,77 +1,69 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n-      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n+      byte[] startAfter, boolean needLocation, boolean includeStoragePolicy)\n       throws IOException {\n-    String srcs \u003d FSDirectory.normalizePath(src);\n-    if (FSDirectory.isExactReservedName(srcs)) {\n+    if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n       return getReservedListing(fsd);\n     }\n \n     fsd.readLock();\n     try {\n-      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n-        return getSnapshotsListing(fsd, srcs, startAfter);\n+      if (iip.isDotSnapshotDir()) {\n+        return getSnapshotsListing(fsd, iip, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n-      if (targetNode \u003d\u003d null)\n+      if (targetNode \u003d\u003d null) {\n         return null;\n-      byte parentStoragePolicy \u003d isSuperUser ?\n-          targetNode.getStoragePolicyID() : HdfsConstants\n-          .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n+      }\n+\n+      byte parentStoragePolicy \u003d includeStoragePolicy\n+          ? targetNode.getStoragePolicyID()\n+          : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         // return the file\u0027s status. note that the iip already includes the\n         // target INode\n-        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n-            fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n-            snapshot);\n         return new DirectoryListing(\n             new HdfsFileStatus[]{ createFileStatus(\n-                fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n-                needLocation, parentStoragePolicy, iip)\n+                fsd, iip, null, parentStoragePolicy, needLocation)\n             }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n-        INode cur \u003d contents.get(startChild+i);\n-        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n-            cur.getLocalStoragePolicyID():\n-            HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n-        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n-            fsd, src, cur.getLocalNameBytes(), cur,\n-            snapshot);\n-        final INodesInPath iipWithChild \u003d INodesInPath.append(iip, cur,\n-            cur.getLocalNameBytes());\n-        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), nodeAttrs,\n-            needLocation, getStoragePolicyID(curPolicy, parentStoragePolicy),\n-            iipWithChild);\n+        INode child \u003d contents.get(startChild+i);\n+        byte childStoragePolicy \u003d (includeStoragePolicy \u0026\u0026 !child.isSymlink())\n+            ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n+                                 parentStoragePolicy)\n+            : parentStoragePolicy;\n+        listing[i] \u003d\n+            createFileStatus(fsd, iip, child, childStoragePolicy, needLocation);\n         listingCnt++;\n-        if (needLocation) {\n+        if (listing[i] instanceof HdfsLocatedFileStatus) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      byte[] startAfter, boolean needLocation, boolean includeStoragePolicy)\n      throws IOException {\n    if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n      return getReservedListing(fsd);\n    }\n\n    fsd.readLock();\n    try {\n      if (iip.isDotSnapshotDir()) {\n        return getSnapshotsListing(fsd, iip, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null) {\n        return null;\n      }\n\n      byte parentStoragePolicy \u003d includeStoragePolicy\n          ? targetNode.getStoragePolicyID()\n          : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        // return the file\u0027s status. note that the iip already includes the\n        // target INode\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, iip, null, parentStoragePolicy, needLocation)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n        INode child \u003d contents.get(startChild+i);\n        byte childStoragePolicy \u003d (includeStoragePolicy \u0026\u0026 !child.isSymlink())\n            ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n                                 parentStoragePolicy)\n            : parentStoragePolicy;\n        listing[i] \u003d\n            createFileStatus(fsd, iip, child, childStoragePolicy, needLocation);\n        listingCnt++;\n        if (listing[i] instanceof HdfsLocatedFileStatus) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, iip-INodesInPath, src-String, startAfter-byte[], needLocation-boolean, isSuperUser-boolean]",
            "newValue": "[fsd-FSDirectory, iip-INodesInPath, startAfter-byte[], needLocation-boolean, includeStoragePolicy-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10851. FSDirStatAndListingOp: stop passing path as string. Contributed by Daryn Sharp.\n",
          "commitDate": "30/09/16 11:03 AM",
          "commitName": "a0730aa5ced7666a8c92f9fb830b615f5f9f477a",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "24/08/16 1:21 PM",
          "commitNameOld": "a1f3293762dddb0ca953d1145f5b53d9086b25b8",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 36.9,
          "commitsBetweenForRepo": 208,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,77 +1,69 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n-      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n+      byte[] startAfter, boolean needLocation, boolean includeStoragePolicy)\n       throws IOException {\n-    String srcs \u003d FSDirectory.normalizePath(src);\n-    if (FSDirectory.isExactReservedName(srcs)) {\n+    if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n       return getReservedListing(fsd);\n     }\n \n     fsd.readLock();\n     try {\n-      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n-        return getSnapshotsListing(fsd, srcs, startAfter);\n+      if (iip.isDotSnapshotDir()) {\n+        return getSnapshotsListing(fsd, iip, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n-      if (targetNode \u003d\u003d null)\n+      if (targetNode \u003d\u003d null) {\n         return null;\n-      byte parentStoragePolicy \u003d isSuperUser ?\n-          targetNode.getStoragePolicyID() : HdfsConstants\n-          .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n+      }\n+\n+      byte parentStoragePolicy \u003d includeStoragePolicy\n+          ? targetNode.getStoragePolicyID()\n+          : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         // return the file\u0027s status. note that the iip already includes the\n         // target INode\n-        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n-            fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n-            snapshot);\n         return new DirectoryListing(\n             new HdfsFileStatus[]{ createFileStatus(\n-                fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n-                needLocation, parentStoragePolicy, iip)\n+                fsd, iip, null, parentStoragePolicy, needLocation)\n             }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n-        INode cur \u003d contents.get(startChild+i);\n-        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n-            cur.getLocalStoragePolicyID():\n-            HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n-        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n-            fsd, src, cur.getLocalNameBytes(), cur,\n-            snapshot);\n-        final INodesInPath iipWithChild \u003d INodesInPath.append(iip, cur,\n-            cur.getLocalNameBytes());\n-        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), nodeAttrs,\n-            needLocation, getStoragePolicyID(curPolicy, parentStoragePolicy),\n-            iipWithChild);\n+        INode child \u003d contents.get(startChild+i);\n+        byte childStoragePolicy \u003d (includeStoragePolicy \u0026\u0026 !child.isSymlink())\n+            ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n+                                 parentStoragePolicy)\n+            : parentStoragePolicy;\n+        listing[i] \u003d\n+            createFileStatus(fsd, iip, child, childStoragePolicy, needLocation);\n         listingCnt++;\n-        if (needLocation) {\n+        if (listing[i] instanceof HdfsLocatedFileStatus) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      byte[] startAfter, boolean needLocation, boolean includeStoragePolicy)\n      throws IOException {\n    if (FSDirectory.isExactReservedName(iip.getPathComponents())) {\n      return getReservedListing(fsd);\n    }\n\n    fsd.readLock();\n    try {\n      if (iip.isDotSnapshotDir()) {\n        return getSnapshotsListing(fsd, iip, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null) {\n        return null;\n      }\n\n      byte parentStoragePolicy \u003d includeStoragePolicy\n          ? targetNode.getStoragePolicyID()\n          : HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        // return the file\u0027s status. note that the iip already includes the\n        // target INode\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, iip, null, parentStoragePolicy, needLocation)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n        INode child \u003d contents.get(startChild+i);\n        byte childStoragePolicy \u003d (includeStoragePolicy \u0026\u0026 !child.isSymlink())\n            ? getStoragePolicyID(child.getLocalStoragePolicyID(),\n                                 parentStoragePolicy)\n            : parentStoragePolicy;\n        listing[i] \u003d\n            createFileStatus(fsd, iip, child, childStoragePolicy, needLocation);\n        listingCnt++;\n        if (listing[i] instanceof HdfsLocatedFileStatus) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "ec252ce0fc0998ce13f31af3440c08a236328e5a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10762. Pass IIP for file status related methods\n",
      "commitDate": "24/08/16 6:46 AM",
      "commitName": "ec252ce0fc0998ce13f31af3440c08a236328e5a",
      "commitAuthor": "Daryn Sharp",
      "commitDateOld": "22/08/16 2:57 PM",
      "commitNameOld": "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 1.66,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,77 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n       String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n       throws IOException {\n     String srcs \u003d FSDirectory.normalizePath(src);\n-    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n     if (FSDirectory.isExactReservedName(srcs)) {\n       return getReservedListing(fsd);\n     }\n \n     fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n           targetNode.getStoragePolicyID() : HdfsConstants\n           .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         // return the file\u0027s status. note that the iip already includes the\n         // target INode\n         INodeAttributes nodeAttrs \u003d getINodeAttributes(\n             fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n             snapshot);\n         return new DirectoryListing(\n             new HdfsFileStatus[]{ createFileStatus(\n                 fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n-                needLocation, parentStoragePolicy, snapshot, isRawPath, iip)\n+                needLocation, parentStoragePolicy, iip)\n             }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n         INodeAttributes nodeAttrs \u003d getINodeAttributes(\n             fsd, src, cur.getLocalNameBytes(), cur,\n             snapshot);\n         final INodesInPath iipWithChild \u003d INodesInPath.append(iip, cur,\n             cur.getLocalNameBytes());\n         listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), nodeAttrs,\n             needLocation, getStoragePolicyID(curPolicy, parentStoragePolicy),\n-            snapshot, isRawPath, iipWithChild);\n+            iipWithChild);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    if (FSDirectory.isExactReservedName(srcs)) {\n      return getReservedListing(fsd);\n    }\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : HdfsConstants\n          .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        // return the file\u0027s status. note that the iip already includes the\n        // target INode\n        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n            fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n            snapshot);\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n                needLocation, parentStoragePolicy, iip)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n            fsd, src, cur.getLocalNameBytes(), cur,\n            snapshot);\n        final INodesInPath iipWithChild \u003d INodesInPath.append(iip, cur,\n            cur.getLocalNameBytes());\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), nodeAttrs,\n            needLocation, getStoragePolicyID(curPolicy, parentStoragePolicy),\n            iipWithChild);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-10762. Pass IIP for file status related methods. Contributed by Daryn Sharp.\"\n\nThis reverts commit 22fc46d7659972ff016ccf1c6f781f0c160be26f.\n",
      "commitDate": "22/08/16 2:57 PM",
      "commitName": "3ca4d6ddfd199c95677721ff3bcb95d1da45bd88",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "22/08/16 1:37 PM",
      "commitNameOld": "22fc46d7659972ff016ccf1c6f781f0c160be26f",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,78 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n       String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n       throws IOException {\n     String srcs \u003d FSDirectory.normalizePath(src);\n+    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n     if (FSDirectory.isExactReservedName(srcs)) {\n       return getReservedListing(fsd);\n     }\n \n     fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n           targetNode.getStoragePolicyID() : HdfsConstants\n           .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         // return the file\u0027s status. note that the iip already includes the\n         // target INode\n         INodeAttributes nodeAttrs \u003d getINodeAttributes(\n             fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n             snapshot);\n         return new DirectoryListing(\n             new HdfsFileStatus[]{ createFileStatus(\n                 fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n-                needLocation, parentStoragePolicy, iip)\n+                needLocation, parentStoragePolicy, snapshot, isRawPath, iip)\n             }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n         INodeAttributes nodeAttrs \u003d getINodeAttributes(\n             fsd, src, cur.getLocalNameBytes(), cur,\n             snapshot);\n         final INodesInPath iipWithChild \u003d INodesInPath.append(iip, cur,\n             cur.getLocalNameBytes());\n         listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), nodeAttrs,\n             needLocation, getStoragePolicyID(curPolicy, parentStoragePolicy),\n-            iipWithChild);\n+            snapshot, isRawPath, iipWithChild);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n    if (FSDirectory.isExactReservedName(srcs)) {\n      return getReservedListing(fsd);\n    }\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : HdfsConstants\n          .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        // return the file\u0027s status. note that the iip already includes the\n        // target INode\n        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n            fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n            snapshot);\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n                needLocation, parentStoragePolicy, snapshot, isRawPath, iip)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n            fsd, src, cur.getLocalNameBytes(), cur,\n            snapshot);\n        final INodesInPath iipWithChild \u003d INodesInPath.append(iip, cur,\n            cur.getLocalNameBytes());\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), nodeAttrs,\n            needLocation, getStoragePolicyID(curPolicy, parentStoragePolicy),\n            snapshot, isRawPath, iipWithChild);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "22fc46d7659972ff016ccf1c6f781f0c160be26f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10762. Pass IIP for file status related methods. Contributed by Daryn Sharp.\n",
      "commitDate": "22/08/16 1:37 PM",
      "commitName": "22fc46d7659972ff016ccf1c6f781f0c160be26f",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "17/08/16 1:53 PM",
      "commitNameOld": "869393643de23dcb010cc33091c8eb398de0fd6c",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 4.99,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,77 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n       String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n       throws IOException {\n     String srcs \u003d FSDirectory.normalizePath(src);\n-    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n     if (FSDirectory.isExactReservedName(srcs)) {\n       return getReservedListing(fsd);\n     }\n \n     fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n           targetNode.getStoragePolicyID() : HdfsConstants\n           .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         // return the file\u0027s status. note that the iip already includes the\n         // target INode\n         INodeAttributes nodeAttrs \u003d getINodeAttributes(\n             fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n             snapshot);\n         return new DirectoryListing(\n             new HdfsFileStatus[]{ createFileStatus(\n                 fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n-                needLocation, parentStoragePolicy, snapshot, isRawPath, iip)\n+                needLocation, parentStoragePolicy, iip)\n             }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n         INodeAttributes nodeAttrs \u003d getINodeAttributes(\n             fsd, src, cur.getLocalNameBytes(), cur,\n             snapshot);\n         final INodesInPath iipWithChild \u003d INodesInPath.append(iip, cur,\n             cur.getLocalNameBytes());\n         listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), nodeAttrs,\n             needLocation, getStoragePolicyID(curPolicy, parentStoragePolicy),\n-            snapshot, isRawPath, iipWithChild);\n+            iipWithChild);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    if (FSDirectory.isExactReservedName(srcs)) {\n      return getReservedListing(fsd);\n    }\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : HdfsConstants\n          .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        // return the file\u0027s status. note that the iip already includes the\n        // target INode\n        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n            fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n            snapshot);\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n                needLocation, parentStoragePolicy, iip)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n            fsd, src, cur.getLocalNameBytes(), cur,\n            snapshot);\n        final INodesInPath iipWithChild \u003d INodesInPath.append(iip, cur,\n            cur.getLocalNameBytes());\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), nodeAttrs,\n            needLocation, getStoragePolicyID(curPolicy, parentStoragePolicy),\n            iipWithChild);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "9f4bf3bdf9e74800643477cfb18361e01cf6859c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9621. getListing wrongly associates Erasure Coding policy to pre-existing replicated files under an EC directory. Contributed by Jing Zhao.\n",
      "commitDate": "11/01/16 11:31 AM",
      "commitName": "9f4bf3bdf9e74800643477cfb18361e01cf6859c",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/11/15 1:14 PM",
      "commitNameOld": "977e0b3c4ce76746a3d8590d2d790fdc96c86ca5",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 47.93,
      "commitsBetweenForRepo": 246,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,78 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n       String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n       throws IOException {\n     String srcs \u003d FSDirectory.normalizePath(src);\n     final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n     if (FSDirectory.isExactReservedName(srcs)) {\n       return getReservedListing(fsd);\n     }\n \n     fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n           targetNode.getStoragePolicyID() : HdfsConstants\n           .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n+        // return the file\u0027s status. note that the iip already includes the\n+        // target INode\n         INodeAttributes nodeAttrs \u003d getINodeAttributes(\n             fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n             snapshot);\n         return new DirectoryListing(\n             new HdfsFileStatus[]{ createFileStatus(\n-                fsd, HdfsFileStatus.EMPTY_NAME, targetNode, nodeAttrs,\n+                fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n                 needLocation, parentStoragePolicy, snapshot, isRawPath, iip)\n             }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n-      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n+      for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n         INodeAttributes nodeAttrs \u003d getINodeAttributes(\n             fsd, src, cur.getLocalNameBytes(), cur,\n             snapshot);\n-        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(),\n-            cur, nodeAttrs, needLocation, getStoragePolicyID(curPolicy,\n-                parentStoragePolicy), snapshot, isRawPath, iip);\n+        final INodesInPath iipWithChild \u003d INodesInPath.append(iip, cur,\n+            cur.getLocalNameBytes());\n+        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), nodeAttrs,\n+            needLocation, getStoragePolicyID(curPolicy, parentStoragePolicy),\n+            snapshot, isRawPath, iipWithChild);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n    if (FSDirectory.isExactReservedName(srcs)) {\n      return getReservedListing(fsd);\n    }\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : HdfsConstants\n          .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        // return the file\u0027s status. note that the iip already includes the\n        // target INode\n        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n            fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n            snapshot);\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, HdfsFileStatus.EMPTY_NAME, nodeAttrs,\n                needLocation, parentStoragePolicy, snapshot, isRawPath, iip)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i \u003d 0; i \u003c numOfListing \u0026\u0026 locationBudget \u003e 0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n            fsd, src, cur.getLocalNameBytes(), cur,\n            snapshot);\n        final INodesInPath iipWithChild \u003d INodesInPath.append(iip, cur,\n            cur.getLocalNameBytes());\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), nodeAttrs,\n            needLocation, getStoragePolicyID(curPolicy, parentStoragePolicy),\n            snapshot, isRawPath, iipWithChild);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "3dadf369d550c2ae393b751cb5a184dbfe2814df": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7087. Ability to list /.reserved. Contributed by Xiao Chen.\n",
      "commitDate": "21/10/15 4:58 PM",
      "commitName": "3dadf369d550c2ae393b751cb5a184dbfe2814df",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "29/09/15 1:39 AM",
      "commitNameOld": "8fd55202468b28422b0df888641c9b08906fe4a7",
      "commitAuthorOld": "",
      "daysBetweenCommits": 22.64,
      "commitsBetweenForRepo": 171,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,74 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n       String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n       throws IOException {\n     String srcs \u003d FSDirectory.normalizePath(src);\n     final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n+    if (FSDirectory.isExactReservedName(srcs)) {\n+      return getReservedListing(fsd);\n+    }\n \n     fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n           targetNode.getStoragePolicyID() : HdfsConstants\n           .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         INodeAttributes nodeAttrs \u003d getINodeAttributes(\n             fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n             snapshot);\n         return new DirectoryListing(\n             new HdfsFileStatus[]{ createFileStatus(\n                 fsd, HdfsFileStatus.EMPTY_NAME, targetNode, nodeAttrs,\n                 needLocation, parentStoragePolicy, snapshot, isRawPath, iip)\n             }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n         INodeAttributes nodeAttrs \u003d getINodeAttributes(\n             fsd, src, cur.getLocalNameBytes(), cur,\n             snapshot);\n         listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(),\n             cur, nodeAttrs, needLocation, getStoragePolicyID(curPolicy,\n                 parentStoragePolicy), snapshot, isRawPath, iip);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n    if (FSDirectory.isExactReservedName(srcs)) {\n      return getReservedListing(fsd);\n    }\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : HdfsConstants\n          .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n            fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n            snapshot);\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, HdfsFileStatus.EMPTY_NAME, targetNode, nodeAttrs,\n                needLocation, parentStoragePolicy, snapshot, isRawPath, iip)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n            fsd, src, cur.getLocalNameBytes(), cur,\n            snapshot);\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(),\n            cur, nodeAttrs, needLocation, getStoragePolicyID(curPolicy,\n                parentStoragePolicy), snapshot, isRawPath, iip);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/04/15 1:41 PM",
      "commitNameOld": "c55d609053fe24b3a50fbe17dc1b47717b453ed6",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.85,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,71 +1,71 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n       String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n       throws IOException {\n     String srcs \u003d FSDirectory.normalizePath(src);\n     final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n \n     fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n-          targetNode.getStoragePolicyID() : HdfsConstantsClient\n+          targetNode.getStoragePolicyID() : HdfsConstants\n           .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         INodeAttributes nodeAttrs \u003d getINodeAttributes(\n             fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n             snapshot);\n         return new DirectoryListing(\n             new HdfsFileStatus[]{ createFileStatus(\n                 fsd, HdfsFileStatus.EMPTY_NAME, targetNode, nodeAttrs,\n                 needLocation, parentStoragePolicy, snapshot, isRawPath, iip)\n             }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n-            HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n+            HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n         INodeAttributes nodeAttrs \u003d getINodeAttributes(\n             fsd, src, cur.getLocalNameBytes(), cur,\n             snapshot);\n         listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(),\n             cur, nodeAttrs, needLocation, getStoragePolicyID(curPolicy,\n                 parentStoragePolicy), snapshot, isRawPath, iip);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : HdfsConstants\n          .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n            fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n            snapshot);\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, HdfsFileStatus.EMPTY_NAME, targetNode, nodeAttrs,\n                needLocation, parentStoragePolicy, snapshot, isRawPath, iip)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            HdfsConstants.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n            fsd, src, cur.getLocalNameBytes(), cur,\n            snapshot);\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(),\n            cur, nodeAttrs, needLocation, getStoragePolicyID(curPolicy,\n                parentStoragePolicy), snapshot, isRawPath, iip);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "c55d609053fe24b3a50fbe17dc1b47717b453ed6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8200. Refactor FSDirStatAndListingOp. Contributed by Haohui Mai.\n",
      "commitDate": "30/04/15 1:41 PM",
      "commitName": "c55d609053fe24b3a50fbe17dc1b47717b453ed6",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "20/04/15 12:36 AM",
      "commitNameOld": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 10.55,
      "commitsBetweenForRepo": 106,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,71 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n       String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n       throws IOException {\n     String srcs \u003d FSDirectory.normalizePath(src);\n     final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n \n     fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n           targetNode.getStoragePolicyID() : HdfsConstantsClient\n           .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n+        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n+            fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n+            snapshot);\n         return new DirectoryListing(\n-            new HdfsFileStatus[]{createFileStatus(fsd, src,\n-                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n-                parentStoragePolicy, snapshot, isRawPath, iip)}, 0);\n+            new HdfsFileStatus[]{ createFileStatus(\n+                fsd, HdfsFileStatus.EMPTY_NAME, targetNode, nodeAttrs,\n+                needLocation, parentStoragePolicy, snapshot, isRawPath, iip)\n+            }, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n-        listing[i] \u003d createFileStatus(fsd, src, cur.getLocalNameBytes(), cur,\n-            needLocation, getStoragePolicyID(curPolicy,\n+        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n+            fsd, src, cur.getLocalNameBytes(), cur,\n+            snapshot);\n+        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(),\n+            cur, nodeAttrs, needLocation, getStoragePolicyID(curPolicy,\n                 parentStoragePolicy), snapshot, isRawPath, iip);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : HdfsConstantsClient\n          .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n            fsd, src, HdfsFileStatus.EMPTY_NAME, targetNode,\n            snapshot);\n        return new DirectoryListing(\n            new HdfsFileStatus[]{ createFileStatus(\n                fsd, HdfsFileStatus.EMPTY_NAME, targetNode, nodeAttrs,\n                needLocation, parentStoragePolicy, snapshot, isRawPath, iip)\n            }, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n        INodeAttributes nodeAttrs \u003d getINodeAttributes(\n            fsd, src, cur.getLocalNameBytes(), cur,\n            snapshot);\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(),\n            cur, nodeAttrs, needLocation, getStoragePolicyID(curPolicy,\n                parentStoragePolicy), snapshot, isRawPath, iip);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "5c97db07fb306842f49d73a67a90cecec19a7833": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8169. Move LocatedBlocks and related classes to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "20/04/15 12:36 AM",
      "commitName": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "08/04/15 1:38 PM",
      "commitNameOld": "285b31e75e51ec8e3a796c2cb0208739368ca9b8",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 11.46,
      "commitsBetweenForRepo": 85,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,64 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n       String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n       throws IOException {\n     String srcs \u003d FSDirectory.normalizePath(src);\n     final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n \n     fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n-          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n-          .ID_UNSPECIFIED;\n+          targetNode.getStoragePolicyID() : HdfsConstantsClient\n+          .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         return new DirectoryListing(\n             new HdfsFileStatus[]{createFileStatus(fsd, src,\n                 HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                 parentStoragePolicy, snapshot, isRawPath, iip)}, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n-            BlockStoragePolicySuite.ID_UNSPECIFIED;\n+            HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n         listing[i] \u003d createFileStatus(fsd, src, cur.getLocalNameBytes(), cur,\n             needLocation, getStoragePolicyID(curPolicy,\n                 parentStoragePolicy), snapshot, isRawPath, iip);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : HdfsConstantsClient\n          .BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        return new DirectoryListing(\n            new HdfsFileStatus[]{createFileStatus(fsd, src,\n                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                parentStoragePolicy, snapshot, isRawPath, iip)}, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;\n        listing[i] \u003d createFileStatus(fsd, src, cur.getLocalNameBytes(), cur,\n            needLocation, getStoragePolicyID(curPolicy,\n                parentStoragePolicy), snapshot, isRawPath, iip);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "53a28afe293e5bf185c8d4f2c7aea212e66015c2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6826. Plugin interface to enable delegation of HDFS authorization assertions. Contributed by Arun Suresh.\n",
      "commitDate": "24/03/15 4:02 PM",
      "commitName": "53a28afe293e5bf185c8d4f2c7aea212e66015c2",
      "commitAuthor": "Jitendra Pandey",
      "commitDateOld": "13/01/15 12:24 AM",
      "commitNameOld": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 70.61,
      "commitsBetweenForRepo": 649,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,64 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n       String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n       throws IOException {\n     String srcs \u003d FSDirectory.normalizePath(src);\n     final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n \n     fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n           targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n           .ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         return new DirectoryListing(\n-            new HdfsFileStatus[]{createFileStatus(fsd,\n+            new HdfsFileStatus[]{createFileStatus(fsd, src,\n                 HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                 parentStoragePolicy, snapshot, isRawPath, iip)}, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             BlockStoragePolicySuite.ID_UNSPECIFIED;\n-        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n+        listing[i] \u003d createFileStatus(fsd, src, cur.getLocalNameBytes(), cur,\n             needLocation, getStoragePolicyID(curPolicy,\n                 parentStoragePolicy), snapshot, isRawPath, iip);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n          .ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        return new DirectoryListing(\n            new HdfsFileStatus[]{createFileStatus(fsd, src,\n                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                parentStoragePolicy, snapshot, isRawPath, iip)}, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            BlockStoragePolicySuite.ID_UNSPECIFIED;\n        listing[i] \u003d createFileStatus(fsd, src, cur.getLocalNameBytes(), cur,\n            needLocation, getStoragePolicyID(curPolicy,\n                parentStoragePolicy), snapshot, isRawPath, iip);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7506. Consolidate implementation of setting inode attributes into a single class. Contributed by Haohui Mai.\n",
      "commitDate": "15/12/14 10:40 AM",
      "commitName": "832ebd8cb63d91b4aa4bfed412b9799b3b9be4a7",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "12/12/14 3:13 PM",
      "commitNameOld": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.81,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,64 @@\n   private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n       String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n       throws IOException {\n     String srcs \u003d FSDirectory.normalizePath(src);\n     final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n \n     fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n       final int snapshot \u003d iip.getPathSnapshotId();\n       final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n           targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n           .ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         return new DirectoryListing(\n             new HdfsFileStatus[]{createFileStatus(fsd,\n                 HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                 parentStoragePolicy, snapshot, isRawPath, iip)}, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             BlockStoragePolicySuite.ID_UNSPECIFIED;\n         listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n-            needLocation, fsd.getStoragePolicyID(curPolicy,\n+            needLocation, getStoragePolicyID(curPolicy,\n                 parentStoragePolicy), snapshot, isRawPath, iip);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n          .ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        return new DirectoryListing(\n            new HdfsFileStatus[]{createFileStatus(fsd,\n                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                parentStoragePolicy, snapshot, isRawPath, iip)}, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            BlockStoragePolicySuite.ID_UNSPECIFIED;\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n            needLocation, getStoragePolicyID(curPolicy,\n                parentStoragePolicy), snapshot, isRawPath, iip);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "c78e3a7cdd10c40454e9acb06986ba6d8573cb19": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7059. Avoid resolving path multiple times. Contributed by Jing Zhao.\n",
      "commitDate": "12/12/14 3:13 PM",
      "commitName": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7059. Avoid resolving path multiple times. Contributed by Jing Zhao.\n",
          "commitDate": "12/12/14 3:13 PM",
          "commitName": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "11/12/14 12:36 PM",
          "commitNameOld": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 1.11,
          "commitsBetweenForRepo": 10,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,65 +1,64 @@\n-  private static DirectoryListing getListing(\n-      FSDirectory fsd, String src, byte[] startAfter, boolean needLocation,\n-      boolean isSuperUser)\n+  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n+      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n       throws IOException {\n     String srcs \u003d FSDirectory.normalizePath(src);\n     final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n \n     fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n-      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, true);\n-      final int snapshot \u003d inodesInPath.getPathSnapshotId();\n-      final INode targetNode \u003d inodesInPath.getLastINode();\n+      final int snapshot \u003d iip.getPathSnapshotId();\n+      final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n           targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n           .ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         return new DirectoryListing(\n             new HdfsFileStatus[]{createFileStatus(fsd,\n                 HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n-                parentStoragePolicy, snapshot, isRawPath, inodesInPath)}, 0);\n+                parentStoragePolicy, snapshot, isRawPath, iip)}, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             BlockStoragePolicySuite.ID_UNSPECIFIED;\n         listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n-            needLocation, fsd.getStoragePolicyID(curPolicy, parentStoragePolicy), snapshot, isRawPath, inodesInPath);\n+            needLocation, fsd.getStoragePolicyID(curPolicy,\n+                parentStoragePolicy), snapshot, isRawPath, iip);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n          .ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        return new DirectoryListing(\n            new HdfsFileStatus[]{createFileStatus(fsd,\n                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                parentStoragePolicy, snapshot, isRawPath, iip)}, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            BlockStoragePolicySuite.ID_UNSPECIFIED;\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n            needLocation, fsd.getStoragePolicyID(curPolicy,\n                parentStoragePolicy), snapshot, isRawPath, iip);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, src-String, startAfter-byte[], needLocation-boolean, isSuperUser-boolean]",
            "newValue": "[fsd-FSDirectory, iip-INodesInPath, src-String, startAfter-byte[], needLocation-boolean, isSuperUser-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7059. Avoid resolving path multiple times. Contributed by Jing Zhao.\n",
          "commitDate": "12/12/14 3:13 PM",
          "commitName": "c78e3a7cdd10c40454e9acb06986ba6d8573cb19",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "11/12/14 12:36 PM",
          "commitNameOld": "b9f6d0c956f0278c8b9b83e05b523a442a730ebb",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 1.11,
          "commitsBetweenForRepo": 10,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,65 +1,64 @@\n-  private static DirectoryListing getListing(\n-      FSDirectory fsd, String src, byte[] startAfter, boolean needLocation,\n-      boolean isSuperUser)\n+  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n+      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n       throws IOException {\n     String srcs \u003d FSDirectory.normalizePath(src);\n     final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n \n     fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n-      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, true);\n-      final int snapshot \u003d inodesInPath.getPathSnapshotId();\n-      final INode targetNode \u003d inodesInPath.getLastINode();\n+      final int snapshot \u003d iip.getPathSnapshotId();\n+      final INode targetNode \u003d iip.getLastINode();\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n           targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n           .ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         return new DirectoryListing(\n             new HdfsFileStatus[]{createFileStatus(fsd,\n                 HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n-                parentStoragePolicy, snapshot, isRawPath, inodesInPath)}, 0);\n+                parentStoragePolicy, snapshot, isRawPath, iip)}, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             BlockStoragePolicySuite.ID_UNSPECIFIED;\n         listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n-            needLocation, fsd.getStoragePolicyID(curPolicy, parentStoragePolicy), snapshot, isRawPath, inodesInPath);\n+            needLocation, fsd.getStoragePolicyID(curPolicy,\n+                parentStoragePolicy), snapshot, isRawPath, iip);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getListing(FSDirectory fsd, INodesInPath iip,\n      String src, byte[] startAfter, boolean needLocation, boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final int snapshot \u003d iip.getPathSnapshotId();\n      final INode targetNode \u003d iip.getLastINode();\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n          .ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        return new DirectoryListing(\n            new HdfsFileStatus[]{createFileStatus(fsd,\n                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                parentStoragePolicy, snapshot, isRawPath, iip)}, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            BlockStoragePolicySuite.ID_UNSPECIFIED;\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n            needLocation, fsd.getStoragePolicyID(curPolicy,\n                parentStoragePolicy), snapshot, isRawPath, iip);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "5776a41da08af653206bb94d7c76c9c4dcce059a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7498. Simplify the logic in INodesInPath. Contributed by Jing Zhao.\n",
      "commitDate": "09/12/14 11:37 AM",
      "commitName": "5776a41da08af653206bb94d7c76c9c4dcce059a",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "05/12/14 2:17 PM",
      "commitNameOld": "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 3.89,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,66 @@\n   private static DirectoryListing getListing(\n       FSDirectory fsd, String src, byte[] startAfter, boolean needLocation,\n       boolean isSuperUser)\n       throws IOException {\n     String srcs \u003d FSDirectory.normalizePath(src);\n     final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n \n     fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n       final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, true);\n-      final INode[] inodes \u003d inodesInPath.getINodes();\n       final int snapshot \u003d inodesInPath.getPathSnapshotId();\n-      final INode targetNode \u003d inodes[inodes.length - 1];\n+      final INode targetNode \u003d inodesInPath.getLastINode();\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n           targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n           .ID_UNSPECIFIED;\n \n       if (!targetNode.isDirectory()) {\n         return new DirectoryListing(\n             new HdfsFileStatus[]{createFileStatus(fsd,\n                 HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                 parentStoragePolicy, snapshot, isRawPath, inodesInPath)}, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren - startChild,\n           fsd.getLsLimit());\n       int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             BlockStoragePolicySuite.ID_UNSPECIFIED;\n         listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n             needLocation, fsd.getStoragePolicyID(curPolicy,\n                 parentStoragePolicy), snapshot, isRawPath, inodesInPath);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private static DirectoryListing getListing(\n      FSDirectory fsd, String src, byte[] startAfter, boolean needLocation,\n      boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, true);\n      final int snapshot \u003d inodesInPath.getPathSnapshotId();\n      final INode targetNode \u003d inodesInPath.getLastINode();\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n          .ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        return new DirectoryListing(\n            new HdfsFileStatus[]{createFileStatus(fsd,\n                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                parentStoragePolicy, snapshot, isRawPath, inodesInPath)}, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            BlockStoragePolicySuite.ID_UNSPECIFIED;\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n            needLocation, fsd.getStoragePolicyID(curPolicy,\n                parentStoragePolicy), snapshot, isRawPath, inodesInPath);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
      "extendedDetails": {}
    },
    "0af44ea8462437f8e7a8271b15a19677fd7f05a1": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Yexceptionschange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
      "commitDate": "01/12/14 9:36 PM",
      "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "01/12/14 9:36 PM",
          "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "01/12/14 9:21 PM",
          "commitNameOld": "9fa29902575ac3774bf3728e7bcde7f3eefb1d4c",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,64 +1,67 @@\n-  DirectoryListing getListing(String src, byte[] startAfter,\n-      boolean needLocation, boolean isSuperUser)\n-      throws UnresolvedLinkException, IOException {\n-    String srcs \u003d normalizePath(src);\n-    final boolean isRawPath \u003d isReservedRawName(src);\n+  private static DirectoryListing getListing(\n+      FSDirectory fsd, String src, byte[] startAfter, boolean needLocation,\n+      boolean isSuperUser)\n+      throws IOException {\n+    String srcs \u003d FSDirectory.normalizePath(src);\n+    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n \n-    readLock();\n+    fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n-        return getSnapshotsListing(srcs, startAfter);\n+        return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n-      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, true);\n+      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, true);\n       final INode[] inodes \u003d inodesInPath.getINodes();\n       final int snapshot \u003d inodesInPath.getPathSnapshotId();\n       final INode targetNode \u003d inodes[inodes.length - 1];\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n-          targetNode.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n-      \n+          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n+          .ID_UNSPECIFIED;\n+\n       if (!targetNode.isDirectory()) {\n         return new DirectoryListing(\n-            new HdfsFileStatus[]{createFileStatus(HdfsFileStatus.EMPTY_NAME,\n-                targetNode, needLocation, parentStoragePolicy, snapshot,\n-                isRawPath, inodesInPath)}, 0);\n+            new HdfsFileStatus[]{createFileStatus(fsd,\n+                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n+                parentStoragePolicy, snapshot, isRawPath, inodesInPath)}, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n-      int numOfListing \u003d Math.min(totalNumChildren-startChild, this.lsLimit);\n-      int locationBudget \u003d this.lsLimit;\n+      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n+          fsd.getLsLimit());\n+      int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             BlockStoragePolicySuite.ID_UNSPECIFIED;\n-        listing[i] \u003d createFileStatus(cur.getLocalNameBytes(), cur, needLocation,\n-            getStoragePolicyID(curPolicy, parentStoragePolicy), snapshot,\n-            isRawPath, inodesInPath);\n+        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n+            needLocation, fsd.getStoragePolicyID(curPolicy,\n+                parentStoragePolicy), snapshot, isRawPath, inodesInPath);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n-            LocatedBlocks blks \u003d \n+            LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n-      readUnlock();\n+      fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getListing(\n      FSDirectory fsd, String src, byte[] startAfter, boolean needLocation,\n      boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, true);\n      final INode[] inodes \u003d inodesInPath.getINodes();\n      final int snapshot \u003d inodesInPath.getPathSnapshotId();\n      final INode targetNode \u003d inodes[inodes.length - 1];\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n          .ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        return new DirectoryListing(\n            new HdfsFileStatus[]{createFileStatus(fsd,\n                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                parentStoragePolicy, snapshot, isRawPath, inodesInPath)}, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            BlockStoragePolicySuite.ID_UNSPECIFIED;\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n            needLocation, fsd.getStoragePolicyID(curPolicy,\n                parentStoragePolicy), snapshot, isRawPath, inodesInPath);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
            "oldMethodName": "getListing",
            "newMethodName": "getListing"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "01/12/14 9:36 PM",
          "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "01/12/14 9:21 PM",
          "commitNameOld": "9fa29902575ac3774bf3728e7bcde7f3eefb1d4c",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,64 +1,67 @@\n-  DirectoryListing getListing(String src, byte[] startAfter,\n-      boolean needLocation, boolean isSuperUser)\n-      throws UnresolvedLinkException, IOException {\n-    String srcs \u003d normalizePath(src);\n-    final boolean isRawPath \u003d isReservedRawName(src);\n+  private static DirectoryListing getListing(\n+      FSDirectory fsd, String src, byte[] startAfter, boolean needLocation,\n+      boolean isSuperUser)\n+      throws IOException {\n+    String srcs \u003d FSDirectory.normalizePath(src);\n+    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n \n-    readLock();\n+    fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n-        return getSnapshotsListing(srcs, startAfter);\n+        return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n-      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, true);\n+      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, true);\n       final INode[] inodes \u003d inodesInPath.getINodes();\n       final int snapshot \u003d inodesInPath.getPathSnapshotId();\n       final INode targetNode \u003d inodes[inodes.length - 1];\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n-          targetNode.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n-      \n+          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n+          .ID_UNSPECIFIED;\n+\n       if (!targetNode.isDirectory()) {\n         return new DirectoryListing(\n-            new HdfsFileStatus[]{createFileStatus(HdfsFileStatus.EMPTY_NAME,\n-                targetNode, needLocation, parentStoragePolicy, snapshot,\n-                isRawPath, inodesInPath)}, 0);\n+            new HdfsFileStatus[]{createFileStatus(fsd,\n+                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n+                parentStoragePolicy, snapshot, isRawPath, inodesInPath)}, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n-      int numOfListing \u003d Math.min(totalNumChildren-startChild, this.lsLimit);\n-      int locationBudget \u003d this.lsLimit;\n+      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n+          fsd.getLsLimit());\n+      int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             BlockStoragePolicySuite.ID_UNSPECIFIED;\n-        listing[i] \u003d createFileStatus(cur.getLocalNameBytes(), cur, needLocation,\n-            getStoragePolicyID(curPolicy, parentStoragePolicy), snapshot,\n-            isRawPath, inodesInPath);\n+        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n+            needLocation, fsd.getStoragePolicyID(curPolicy,\n+                parentStoragePolicy), snapshot, isRawPath, inodesInPath);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n-            LocatedBlocks blks \u003d \n+            LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n-      readUnlock();\n+      fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getListing(\n      FSDirectory fsd, String src, byte[] startAfter, boolean needLocation,\n      boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, true);\n      final INode[] inodes \u003d inodesInPath.getINodes();\n      final int snapshot \u003d inodesInPath.getPathSnapshotId();\n      final INode targetNode \u003d inodes[inodes.length - 1];\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n          .ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        return new DirectoryListing(\n            new HdfsFileStatus[]{createFileStatus(fsd,\n                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                parentStoragePolicy, snapshot, isRawPath, inodesInPath)}, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            BlockStoragePolicySuite.ID_UNSPECIFIED;\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n            needLocation, fsd.getStoragePolicyID(curPolicy,\n                parentStoragePolicy), snapshot, isRawPath, inodesInPath);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[private, static]"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "01/12/14 9:36 PM",
          "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "01/12/14 9:21 PM",
          "commitNameOld": "9fa29902575ac3774bf3728e7bcde7f3eefb1d4c",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,64 +1,67 @@\n-  DirectoryListing getListing(String src, byte[] startAfter,\n-      boolean needLocation, boolean isSuperUser)\n-      throws UnresolvedLinkException, IOException {\n-    String srcs \u003d normalizePath(src);\n-    final boolean isRawPath \u003d isReservedRawName(src);\n+  private static DirectoryListing getListing(\n+      FSDirectory fsd, String src, byte[] startAfter, boolean needLocation,\n+      boolean isSuperUser)\n+      throws IOException {\n+    String srcs \u003d FSDirectory.normalizePath(src);\n+    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n \n-    readLock();\n+    fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n-        return getSnapshotsListing(srcs, startAfter);\n+        return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n-      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, true);\n+      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, true);\n       final INode[] inodes \u003d inodesInPath.getINodes();\n       final int snapshot \u003d inodesInPath.getPathSnapshotId();\n       final INode targetNode \u003d inodes[inodes.length - 1];\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n-          targetNode.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n-      \n+          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n+          .ID_UNSPECIFIED;\n+\n       if (!targetNode.isDirectory()) {\n         return new DirectoryListing(\n-            new HdfsFileStatus[]{createFileStatus(HdfsFileStatus.EMPTY_NAME,\n-                targetNode, needLocation, parentStoragePolicy, snapshot,\n-                isRawPath, inodesInPath)}, 0);\n+            new HdfsFileStatus[]{createFileStatus(fsd,\n+                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n+                parentStoragePolicy, snapshot, isRawPath, inodesInPath)}, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n-      int numOfListing \u003d Math.min(totalNumChildren-startChild, this.lsLimit);\n-      int locationBudget \u003d this.lsLimit;\n+      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n+          fsd.getLsLimit());\n+      int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             BlockStoragePolicySuite.ID_UNSPECIFIED;\n-        listing[i] \u003d createFileStatus(cur.getLocalNameBytes(), cur, needLocation,\n-            getStoragePolicyID(curPolicy, parentStoragePolicy), snapshot,\n-            isRawPath, inodesInPath);\n+        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n+            needLocation, fsd.getStoragePolicyID(curPolicy,\n+                parentStoragePolicy), snapshot, isRawPath, inodesInPath);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n-            LocatedBlocks blks \u003d \n+            LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n-      readUnlock();\n+      fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getListing(\n      FSDirectory fsd, String src, byte[] startAfter, boolean needLocation,\n      boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, true);\n      final INode[] inodes \u003d inodesInPath.getINodes();\n      final int snapshot \u003d inodesInPath.getPathSnapshotId();\n      final INode targetNode \u003d inodes[inodes.length - 1];\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n          .ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        return new DirectoryListing(\n            new HdfsFileStatus[]{createFileStatus(fsd,\n                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                parentStoragePolicy, snapshot, isRawPath, inodesInPath)}, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            BlockStoragePolicySuite.ID_UNSPECIFIED;\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n            needLocation, fsd.getStoragePolicyID(curPolicy,\n                parentStoragePolicy), snapshot, isRawPath, inodesInPath);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[UnresolvedLinkException, IOException]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "01/12/14 9:36 PM",
          "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "01/12/14 9:21 PM",
          "commitNameOld": "9fa29902575ac3774bf3728e7bcde7f3eefb1d4c",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,64 +1,67 @@\n-  DirectoryListing getListing(String src, byte[] startAfter,\n-      boolean needLocation, boolean isSuperUser)\n-      throws UnresolvedLinkException, IOException {\n-    String srcs \u003d normalizePath(src);\n-    final boolean isRawPath \u003d isReservedRawName(src);\n+  private static DirectoryListing getListing(\n+      FSDirectory fsd, String src, byte[] startAfter, boolean needLocation,\n+      boolean isSuperUser)\n+      throws IOException {\n+    String srcs \u003d FSDirectory.normalizePath(src);\n+    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n \n-    readLock();\n+    fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n-        return getSnapshotsListing(srcs, startAfter);\n+        return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n-      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, true);\n+      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, true);\n       final INode[] inodes \u003d inodesInPath.getINodes();\n       final int snapshot \u003d inodesInPath.getPathSnapshotId();\n       final INode targetNode \u003d inodes[inodes.length - 1];\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n-          targetNode.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n-      \n+          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n+          .ID_UNSPECIFIED;\n+\n       if (!targetNode.isDirectory()) {\n         return new DirectoryListing(\n-            new HdfsFileStatus[]{createFileStatus(HdfsFileStatus.EMPTY_NAME,\n-                targetNode, needLocation, parentStoragePolicy, snapshot,\n-                isRawPath, inodesInPath)}, 0);\n+            new HdfsFileStatus[]{createFileStatus(fsd,\n+                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n+                parentStoragePolicy, snapshot, isRawPath, inodesInPath)}, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n-      int numOfListing \u003d Math.min(totalNumChildren-startChild, this.lsLimit);\n-      int locationBudget \u003d this.lsLimit;\n+      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n+          fsd.getLsLimit());\n+      int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             BlockStoragePolicySuite.ID_UNSPECIFIED;\n-        listing[i] \u003d createFileStatus(cur.getLocalNameBytes(), cur, needLocation,\n-            getStoragePolicyID(curPolicy, parentStoragePolicy), snapshot,\n-            isRawPath, inodesInPath);\n+        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n+            needLocation, fsd.getStoragePolicyID(curPolicy,\n+                parentStoragePolicy), snapshot, isRawPath, inodesInPath);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n-            LocatedBlocks blks \u003d \n+            LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n-      readUnlock();\n+      fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getListing(\n      FSDirectory fsd, String src, byte[] startAfter, boolean needLocation,\n      boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, true);\n      final INode[] inodes \u003d inodesInPath.getINodes();\n      final int snapshot \u003d inodesInPath.getPathSnapshotId();\n      final INode targetNode \u003d inodes[inodes.length - 1];\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n          .ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        return new DirectoryListing(\n            new HdfsFileStatus[]{createFileStatus(fsd,\n                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                parentStoragePolicy, snapshot, isRawPath, inodesInPath)}, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            BlockStoragePolicySuite.ID_UNSPECIFIED;\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n            needLocation, fsd.getStoragePolicyID(curPolicy,\n                parentStoragePolicy), snapshot, isRawPath, inodesInPath);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7450. Consolidate the implementation of GetFileInfo(), GetListings() and GetContentSummary() into a single class. Contributed by Haohui Mai.\n",
          "commitDate": "01/12/14 9:36 PM",
          "commitName": "0af44ea8462437f8e7a8271b15a19677fd7f05a1",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "01/12/14 9:21 PM",
          "commitNameOld": "9fa29902575ac3774bf3728e7bcde7f3eefb1d4c",
          "commitAuthorOld": "Andrew Wang",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,64 +1,67 @@\n-  DirectoryListing getListing(String src, byte[] startAfter,\n-      boolean needLocation, boolean isSuperUser)\n-      throws UnresolvedLinkException, IOException {\n-    String srcs \u003d normalizePath(src);\n-    final boolean isRawPath \u003d isReservedRawName(src);\n+  private static DirectoryListing getListing(\n+      FSDirectory fsd, String src, byte[] startAfter, boolean needLocation,\n+      boolean isSuperUser)\n+      throws IOException {\n+    String srcs \u003d FSDirectory.normalizePath(src);\n+    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n \n-    readLock();\n+    fsd.readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n-        return getSnapshotsListing(srcs, startAfter);\n+        return getSnapshotsListing(fsd, srcs, startAfter);\n       }\n-      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, true);\n+      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, true);\n       final INode[] inodes \u003d inodesInPath.getINodes();\n       final int snapshot \u003d inodesInPath.getPathSnapshotId();\n       final INode targetNode \u003d inodes[inodes.length - 1];\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n-          targetNode.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n-      \n+          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n+          .ID_UNSPECIFIED;\n+\n       if (!targetNode.isDirectory()) {\n         return new DirectoryListing(\n-            new HdfsFileStatus[]{createFileStatus(HdfsFileStatus.EMPTY_NAME,\n-                targetNode, needLocation, parentStoragePolicy, snapshot,\n-                isRawPath, inodesInPath)}, 0);\n+            new HdfsFileStatus[]{createFileStatus(fsd,\n+                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n+                parentStoragePolicy, snapshot, isRawPath, inodesInPath)}, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n-      int numOfListing \u003d Math.min(totalNumChildren-startChild, this.lsLimit);\n-      int locationBudget \u003d this.lsLimit;\n+      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n+          fsd.getLsLimit());\n+      int locationBudget \u003d fsd.getLsLimit();\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID():\n             BlockStoragePolicySuite.ID_UNSPECIFIED;\n-        listing[i] \u003d createFileStatus(cur.getLocalNameBytes(), cur, needLocation,\n-            getStoragePolicyID(curPolicy, parentStoragePolicy), snapshot,\n-            isRawPath, inodesInPath);\n+        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n+            needLocation, fsd.getStoragePolicyID(curPolicy,\n+                parentStoragePolicy), snapshot, isRawPath, inodesInPath);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n-            LocatedBlocks blks \u003d \n+            LocatedBlocks blks \u003d\n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n-      readUnlock();\n+      fsd.readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private static DirectoryListing getListing(\n      FSDirectory fsd, String src, byte[] startAfter, boolean needLocation,\n      boolean isSuperUser)\n      throws IOException {\n    String srcs \u003d FSDirectory.normalizePath(src);\n    final boolean isRawPath \u003d FSDirectory.isReservedRawName(src);\n\n    fsd.readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(fsd, srcs, startAfter);\n      }\n      final INodesInPath inodesInPath \u003d fsd.getINodesInPath(srcs, true);\n      final INode[] inodes \u003d inodesInPath.getINodes();\n      final int snapshot \u003d inodesInPath.getPathSnapshotId();\n      final INode targetNode \u003d inodes[inodes.length - 1];\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : BlockStoragePolicySuite\n          .ID_UNSPECIFIED;\n\n      if (!targetNode.isDirectory()) {\n        return new DirectoryListing(\n            new HdfsFileStatus[]{createFileStatus(fsd,\n                HdfsFileStatus.EMPTY_NAME, targetNode, needLocation,\n                parentStoragePolicy, snapshot, isRawPath, inodesInPath)}, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren - startChild,\n          fsd.getLsLimit());\n      int locationBudget \u003d fsd.getLsLimit();\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            BlockStoragePolicySuite.ID_UNSPECIFIED;\n        listing[i] \u003d createFileStatus(fsd, cur.getLocalNameBytes(), cur,\n            needLocation, fsd.getStoragePolicyID(curPolicy,\n                parentStoragePolicy), snapshot, isRawPath, inodesInPath);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d\n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      fsd.readUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
          "extendedDetails": {
            "oldValue": "[src-String, startAfter-byte[], needLocation-boolean, isSuperUser-boolean]",
            "newValue": "[fsd-FSDirectory, src-String, startAfter-byte[], needLocation-boolean, isSuperUser-boolean]"
          }
        }
      ]
    },
    "073bbd805c6680f47bbfcc6e8efd708ad729bca4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7081. Add new DistributedFileSystem API for getting all the existing storage policies. Contributed by Jing Zhao.\n",
      "commitDate": "24/09/14 10:05 AM",
      "commitName": "073bbd805c6680f47bbfcc6e8efd708ad729bca4",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "21/09/14 9:29 PM",
      "commitNameOld": "1737950d0fc83c68f386881b843c41b0b1e342de",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 2.52,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,64 @@\n   DirectoryListing getListing(String src, byte[] startAfter,\n       boolean needLocation, boolean isSuperUser)\n       throws UnresolvedLinkException, IOException {\n     String srcs \u003d normalizePath(src);\n     final boolean isRawPath \u003d isReservedRawName(src);\n \n     readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(srcs, startAfter);\n       }\n       final INodesInPath inodesInPath \u003d getINodesInPath(srcs, true);\n       final INode[] inodes \u003d inodesInPath.getINodes();\n       final int snapshot \u003d inodesInPath.getPathSnapshotId();\n       final INode targetNode \u003d inodes[inodes.length - 1];\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n-          targetNode.getStoragePolicyID() : BlockStoragePolicy.ID_UNSPECIFIED;\n+          targetNode.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n       \n       if (!targetNode.isDirectory()) {\n         return new DirectoryListing(\n             new HdfsFileStatus[]{createFileStatus(HdfsFileStatus.EMPTY_NAME,\n                 targetNode, needLocation, parentStoragePolicy, snapshot,\n                 isRawPath, inodesInPath)}, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren-startChild, this.lsLimit);\n       int locationBudget \u003d this.lsLimit;\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n-            cur.getLocalStoragePolicyID(): BlockStoragePolicy.ID_UNSPECIFIED;\n+            cur.getLocalStoragePolicyID():\n+            BlockStoragePolicySuite.ID_UNSPECIFIED;\n         listing[i] \u003d createFileStatus(cur.getLocalNameBytes(), cur, needLocation,\n             getStoragePolicyID(curPolicy, parentStoragePolicy), snapshot,\n             isRawPath, inodesInPath);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d \n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DirectoryListing getListing(String src, byte[] startAfter,\n      boolean needLocation, boolean isSuperUser)\n      throws UnresolvedLinkException, IOException {\n    String srcs \u003d normalizePath(src);\n    final boolean isRawPath \u003d isReservedRawName(src);\n\n    readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(srcs, startAfter);\n      }\n      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, true);\n      final INode[] inodes \u003d inodesInPath.getINodes();\n      final int snapshot \u003d inodesInPath.getPathSnapshotId();\n      final INode targetNode \u003d inodes[inodes.length - 1];\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : BlockStoragePolicySuite.ID_UNSPECIFIED;\n      \n      if (!targetNode.isDirectory()) {\n        return new DirectoryListing(\n            new HdfsFileStatus[]{createFileStatus(HdfsFileStatus.EMPTY_NAME,\n                targetNode, needLocation, parentStoragePolicy, snapshot,\n                isRawPath, inodesInPath)}, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren-startChild, this.lsLimit);\n      int locationBudget \u003d this.lsLimit;\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID():\n            BlockStoragePolicySuite.ID_UNSPECIFIED;\n        listing[i] \u003d createFileStatus(cur.getLocalNameBytes(), cur, needLocation,\n            getStoragePolicyID(curPolicy, parentStoragePolicy), snapshot,\n            isRawPath, inodesInPath);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d \n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    },
    "1737950d0fc83c68f386881b843c41b0b1e342de": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6987. Move CipherSuite xattr information up to the encryption zone root. Contributed by Zhe Zhang.\n",
      "commitDate": "21/09/14 9:29 PM",
      "commitName": "1737950d0fc83c68f386881b843c41b0b1e342de",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "17/09/14 10:00 PM",
      "commitNameOld": "2d2b0009e662db75cf22e2ce8d618ed0a8e61c2f",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.98,
      "commitsBetweenForRepo": 42,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,62 +1,63 @@\n   DirectoryListing getListing(String src, byte[] startAfter,\n       boolean needLocation, boolean isSuperUser)\n       throws UnresolvedLinkException, IOException {\n     String srcs \u003d normalizePath(src);\n     final boolean isRawPath \u003d isReservedRawName(src);\n \n     readLock();\n     try {\n       if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n         return getSnapshotsListing(srcs, startAfter);\n       }\n-      final INodesInPath inodesInPath \u003d getLastINodeInPath(srcs);\n+      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, true);\n+      final INode[] inodes \u003d inodesInPath.getINodes();\n       final int snapshot \u003d inodesInPath.getPathSnapshotId();\n-      final INode targetNode \u003d inodesInPath.getLastINode();\n+      final INode targetNode \u003d inodes[inodes.length - 1];\n       if (targetNode \u003d\u003d null)\n         return null;\n       byte parentStoragePolicy \u003d isSuperUser ?\n           targetNode.getStoragePolicyID() : BlockStoragePolicy.ID_UNSPECIFIED;\n       \n       if (!targetNode.isDirectory()) {\n         return new DirectoryListing(\n             new HdfsFileStatus[]{createFileStatus(HdfsFileStatus.EMPTY_NAME,\n                 targetNode, needLocation, parentStoragePolicy, snapshot,\n-                isRawPath)}, 0);\n+                isRawPath, inodesInPath)}, 0);\n       }\n \n       final INodeDirectory dirInode \u003d targetNode.asDirectory();\n       final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n       int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n       int totalNumChildren \u003d contents.size();\n       int numOfListing \u003d Math.min(totalNumChildren-startChild, this.lsLimit);\n       int locationBudget \u003d this.lsLimit;\n       int listingCnt \u003d 0;\n       HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n       for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n         INode cur \u003d contents.get(startChild+i);\n         byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n             cur.getLocalStoragePolicyID(): BlockStoragePolicy.ID_UNSPECIFIED;\n         listing[i] \u003d createFileStatus(cur.getLocalNameBytes(), cur, needLocation,\n             getStoragePolicyID(curPolicy, parentStoragePolicy), snapshot,\n-            isRawPath);\n+            isRawPath, inodesInPath);\n         listingCnt++;\n         if (needLocation) {\n             // Once we  hit lsLimit locations, stop.\n             // This helps to prevent excessively large response payloads.\n             // Approximate #locations with locatedBlockCount() * repl_factor\n             LocatedBlocks blks \u003d \n                 ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n             locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n                blks.locatedBlockCount() * listing[i].getReplication();\n         }\n       }\n       // truncate return array if necessary\n       if (listingCnt \u003c numOfListing) {\n           listing \u003d Arrays.copyOf(listing, listingCnt);\n       }\n       return new DirectoryListing(\n           listing, totalNumChildren-startChild-listingCnt);\n     } finally {\n       readUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  DirectoryListing getListing(String src, byte[] startAfter,\n      boolean needLocation, boolean isSuperUser)\n      throws UnresolvedLinkException, IOException {\n    String srcs \u003d normalizePath(src);\n    final boolean isRawPath \u003d isReservedRawName(src);\n\n    readLock();\n    try {\n      if (srcs.endsWith(HdfsConstants.SEPARATOR_DOT_SNAPSHOT_DIR)) {\n        return getSnapshotsListing(srcs, startAfter);\n      }\n      final INodesInPath inodesInPath \u003d getINodesInPath(srcs, true);\n      final INode[] inodes \u003d inodesInPath.getINodes();\n      final int snapshot \u003d inodesInPath.getPathSnapshotId();\n      final INode targetNode \u003d inodes[inodes.length - 1];\n      if (targetNode \u003d\u003d null)\n        return null;\n      byte parentStoragePolicy \u003d isSuperUser ?\n          targetNode.getStoragePolicyID() : BlockStoragePolicy.ID_UNSPECIFIED;\n      \n      if (!targetNode.isDirectory()) {\n        return new DirectoryListing(\n            new HdfsFileStatus[]{createFileStatus(HdfsFileStatus.EMPTY_NAME,\n                targetNode, needLocation, parentStoragePolicy, snapshot,\n                isRawPath, inodesInPath)}, 0);\n      }\n\n      final INodeDirectory dirInode \u003d targetNode.asDirectory();\n      final ReadOnlyList\u003cINode\u003e contents \u003d dirInode.getChildrenList(snapshot);\n      int startChild \u003d INodeDirectory.nextChild(contents, startAfter);\n      int totalNumChildren \u003d contents.size();\n      int numOfListing \u003d Math.min(totalNumChildren-startChild, this.lsLimit);\n      int locationBudget \u003d this.lsLimit;\n      int listingCnt \u003d 0;\n      HdfsFileStatus listing[] \u003d new HdfsFileStatus[numOfListing];\n      for (int i\u003d0; i\u003cnumOfListing \u0026\u0026 locationBudget\u003e0; i++) {\n        INode cur \u003d contents.get(startChild+i);\n        byte curPolicy \u003d isSuperUser \u0026\u0026 !cur.isSymlink()?\n            cur.getLocalStoragePolicyID(): BlockStoragePolicy.ID_UNSPECIFIED;\n        listing[i] \u003d createFileStatus(cur.getLocalNameBytes(), cur, needLocation,\n            getStoragePolicyID(curPolicy, parentStoragePolicy), snapshot,\n            isRawPath, inodesInPath);\n        listingCnt++;\n        if (needLocation) {\n            // Once we  hit lsLimit locations, stop.\n            // This helps to prevent excessively large response payloads.\n            // Approximate #locations with locatedBlockCount() * repl_factor\n            LocatedBlocks blks \u003d \n                ((HdfsLocatedFileStatus)listing[i]).getBlockLocations();\n            locationBudget -\u003d (blks \u003d\u003d null) ? 0 :\n               blks.locatedBlockCount() * listing[i].getReplication();\n        }\n      }\n      // truncate return array if necessary\n      if (listingCnt \u003c numOfListing) {\n          listing \u003d Arrays.copyOf(listing, listingCnt);\n      }\n      return new DirectoryListing(\n          listing, totalNumChildren-startChild-listingCnt);\n    } finally {\n      readUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
      "extendedDetails": {}
    }
  }
}