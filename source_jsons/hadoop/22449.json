{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LineRecordReader.java",
  "functionName": "initialize",
  "functionId": "initialize___genericSplit-InputSplit__context-TaskAttemptContext",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
  "functionStartLine": 77,
  "functionEndLine": 133,
  "numCommitsSeen": 15,
  "timeTaken": 6096,
  "changeHistory": [
    "f365957c6326f88734bc0a5d01cfb7eac713db20",
    "077250d8d7b4b757543a39a6ce8bb6e3be356c6f",
    "2edcf931d7843cddcf3da5666a73d6ee9a10d00d",
    "18d99c12c371cfd7b9604e321d8bd6a7be9c4977",
    "947e97f354edb1e27432cf3f1a2dff098f01071f",
    "6f0c4dca74cab26275d48c05fedd9bc2aa57878f",
    "b55756dd03086f6c081991d949ebcf7926af8af5",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "4796e1adcb912005198c9003305c97cf3a8b523e",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "f365957c6326f88734bc0a5d01cfb7eac713db20": "Ybodychange",
    "077250d8d7b4b757543a39a6ce8bb6e3be356c6f": "Ybodychange",
    "2edcf931d7843cddcf3da5666a73d6ee9a10d00d": "Ybodychange",
    "18d99c12c371cfd7b9604e321d8bd6a7be9c4977": "Ybodychange",
    "947e97f354edb1e27432cf3f1a2dff098f01071f": "Ybodychange",
    "6f0c4dca74cab26275d48c05fedd9bc2aa57878f": "Ybodychange",
    "b55756dd03086f6c081991d949ebcf7926af8af5": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "4796e1adcb912005198c9003305c97cf3a8b523e": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f365957c6326f88734bc0a5d01cfb7eac713db20": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15229. Add FileSystem builder-based openFile() API to match createFile();\nS3A to implement S3 Select through this API.\n\nThe new openFile() API is asynchronous, and implemented across FileSystem and FileContext.\n\nThe MapReduce V2 inputs are moved to this API, and you can actually set must/may\noptions to pass in.\n\nThis is more useful for setting things like s3a seek policy than for S3 select,\nas the existing input format/record readers can\u0027t handle S3 select output where\nthe stream is shorter than the file length, and splitting plain text is suboptimal.\nFuture work is needed there.\n\nIn the meantime, any/all filesystem connectors are now free to add their own filesystem-specific\nconfiguration parameters which can be set in jobs and used to set filesystem input stream\noptions (seek policy, retry, encryption secrets, etc).\n\nContributed by Steve Loughran\n",
      "commitDate": "05/02/19 3:51 AM",
      "commitName": "f365957c6326f88734bc0a5d01cfb7eac713db20",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "02/11/17 1:43 AM",
      "commitNameOld": "178751ed8c9d47038acf8616c226f1f52e884feb",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 460.13,
      "commitsBetweenForRepo": 3984,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,53 +1,57 @@\n   public void initialize(InputSplit genericSplit,\n                          TaskAttemptContext context) throws IOException {\n     FileSplit split \u003d (FileSplit) genericSplit;\n     Configuration job \u003d context.getConfiguration();\n     this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n     start \u003d split.getStart();\n     end \u003d start + split.getLength();\n     final Path file \u003d split.getPath();\n \n     // open the file and seek to the start of the split\n-    final FileSystem fs \u003d file.getFileSystem(job);\n-    fileIn \u003d fs.open(file);\n+    final FutureDataInputStreamBuilder builder \u003d\n+        file.getFileSystem(job).openFile(file);\n+    FutureIOSupport.propagateOptions(builder, job,\n+        MRJobConfig.INPUT_FILE_OPTION_PREFIX,\n+        MRJobConfig.INPUT_FILE_MANDATORY_PREFIX);\n+    fileIn \u003d FutureIOSupport.awaitFuture(builder.build());\n     \n     CompressionCodec codec \u003d new CompressionCodecFactory(job).getCodec(file);\n     if (null!\u003dcodec) {\n       isCompressedInput \u003d true;\n       decompressor \u003d CodecPool.getDecompressor(codec);\n       if (codec instanceof SplittableCompressionCodec) {\n         final SplitCompressionInputStream cIn \u003d\n           ((SplittableCompressionCodec)codec).createInputStream(\n             fileIn, decompressor, start, end,\n             SplittableCompressionCodec.READ_MODE.BYBLOCK);\n         in \u003d new CompressedSplitLineReader(cIn, job,\n             this.recordDelimiterBytes);\n         start \u003d cIn.getAdjustedStart();\n         end \u003d cIn.getAdjustedEnd();\n         filePosition \u003d cIn;\n       } else {\n         if (start !\u003d 0) {\n           // So we have a split that is only part of a file stored using\n           // a Compression codec that cannot be split.\n           throw new IOException(\"Cannot seek in \" +\n               codec.getClass().getSimpleName() + \" compressed stream\");\n         }\n \n         in \u003d new SplitLineReader(codec.createInputStream(fileIn,\n             decompressor), job, this.recordDelimiterBytes);\n         filePosition \u003d fileIn;\n       }\n     } else {\n       fileIn.seek(start);\n       in \u003d new UncompressedSplitLineReader(\n           fileIn, job, this.recordDelimiterBytes, split.getLength());\n       filePosition \u003d fileIn;\n     }\n     // If this is not the first split, we always throw away first record\n     // because we always (except the last split) read one extra line in\n     // next() method.\n     if (start !\u003d 0) {\n       start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n     }\n     this.pos \u003d start;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(InputSplit genericSplit,\n                         TaskAttemptContext context) throws IOException {\n    FileSplit split \u003d (FileSplit) genericSplit;\n    Configuration job \u003d context.getConfiguration();\n    this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n    start \u003d split.getStart();\n    end \u003d start + split.getLength();\n    final Path file \u003d split.getPath();\n\n    // open the file and seek to the start of the split\n    final FutureDataInputStreamBuilder builder \u003d\n        file.getFileSystem(job).openFile(file);\n    FutureIOSupport.propagateOptions(builder, job,\n        MRJobConfig.INPUT_FILE_OPTION_PREFIX,\n        MRJobConfig.INPUT_FILE_MANDATORY_PREFIX);\n    fileIn \u003d FutureIOSupport.awaitFuture(builder.build());\n    \n    CompressionCodec codec \u003d new CompressionCodecFactory(job).getCodec(file);\n    if (null!\u003dcodec) {\n      isCompressedInput \u003d true;\n      decompressor \u003d CodecPool.getDecompressor(codec);\n      if (codec instanceof SplittableCompressionCodec) {\n        final SplitCompressionInputStream cIn \u003d\n          ((SplittableCompressionCodec)codec).createInputStream(\n            fileIn, decompressor, start, end,\n            SplittableCompressionCodec.READ_MODE.BYBLOCK);\n        in \u003d new CompressedSplitLineReader(cIn, job,\n            this.recordDelimiterBytes);\n        start \u003d cIn.getAdjustedStart();\n        end \u003d cIn.getAdjustedEnd();\n        filePosition \u003d cIn;\n      } else {\n        if (start !\u003d 0) {\n          // So we have a split that is only part of a file stored using\n          // a Compression codec that cannot be split.\n          throw new IOException(\"Cannot seek in \" +\n              codec.getClass().getSimpleName() + \" compressed stream\");\n        }\n\n        in \u003d new SplitLineReader(codec.createInputStream(fileIn,\n            decompressor), job, this.recordDelimiterBytes);\n        filePosition \u003d fileIn;\n      }\n    } else {\n      fileIn.seek(start);\n      in \u003d new UncompressedSplitLineReader(\n          fileIn, job, this.recordDelimiterBytes, split.getLength());\n      filePosition \u003d fileIn;\n    }\n    // If this is not the first split, we always throw away first record\n    // because we always (except the last split) read one extra line in\n    // next() method.\n    if (start !\u003d 0) {\n      start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n    }\n    this.pos \u003d start;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "077250d8d7b4b757543a39a6ce8bb6e3be356c6f": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5948. org.apache.hadoop.mapred.LineRecordReader does not handle multibyte record delimiters well. Contributed by Vinayakumar B, Rushabh Shah, and Akira AJISAKA\n",
      "commitDate": "22/06/15 2:59 PM",
      "commitName": "077250d8d7b4b757543a39a6ce8bb6e3be356c6f",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "08/05/15 2:31 PM",
      "commitNameOld": "2edcf931d7843cddcf3da5666a73d6ee9a10d00d",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 45.02,
      "commitsBetweenForRepo": 351,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,53 @@\n   public void initialize(InputSplit genericSplit,\n                          TaskAttemptContext context) throws IOException {\n     FileSplit split \u003d (FileSplit) genericSplit;\n     Configuration job \u003d context.getConfiguration();\n     this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n     start \u003d split.getStart();\n     end \u003d start + split.getLength();\n     final Path file \u003d split.getPath();\n \n     // open the file and seek to the start of the split\n     final FileSystem fs \u003d file.getFileSystem(job);\n     fileIn \u003d fs.open(file);\n     \n     CompressionCodec codec \u003d new CompressionCodecFactory(job).getCodec(file);\n     if (null!\u003dcodec) {\n       isCompressedInput \u003d true;\n       decompressor \u003d CodecPool.getDecompressor(codec);\n       if (codec instanceof SplittableCompressionCodec) {\n         final SplitCompressionInputStream cIn \u003d\n           ((SplittableCompressionCodec)codec).createInputStream(\n             fileIn, decompressor, start, end,\n             SplittableCompressionCodec.READ_MODE.BYBLOCK);\n         in \u003d new CompressedSplitLineReader(cIn, job,\n             this.recordDelimiterBytes);\n         start \u003d cIn.getAdjustedStart();\n         end \u003d cIn.getAdjustedEnd();\n         filePosition \u003d cIn;\n       } else {\n         if (start !\u003d 0) {\n           // So we have a split that is only part of a file stored using\n           // a Compression codec that cannot be split.\n           throw new IOException(\"Cannot seek in \" +\n               codec.getClass().getSimpleName() + \" compressed stream\");\n         }\n \n         in \u003d new SplitLineReader(codec.createInputStream(fileIn,\n             decompressor), job, this.recordDelimiterBytes);\n         filePosition \u003d fileIn;\n       }\n     } else {\n       fileIn.seek(start);\n-      in \u003d new SplitLineReader(fileIn, job, this.recordDelimiterBytes);\n+      in \u003d new UncompressedSplitLineReader(\n+          fileIn, job, this.recordDelimiterBytes, split.getLength());\n       filePosition \u003d fileIn;\n     }\n     // If this is not the first split, we always throw away first record\n     // because we always (except the last split) read one extra line in\n     // next() method.\n     if (start !\u003d 0) {\n       start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n     }\n     this.pos \u003d start;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(InputSplit genericSplit,\n                         TaskAttemptContext context) throws IOException {\n    FileSplit split \u003d (FileSplit) genericSplit;\n    Configuration job \u003d context.getConfiguration();\n    this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n    start \u003d split.getStart();\n    end \u003d start + split.getLength();\n    final Path file \u003d split.getPath();\n\n    // open the file and seek to the start of the split\n    final FileSystem fs \u003d file.getFileSystem(job);\n    fileIn \u003d fs.open(file);\n    \n    CompressionCodec codec \u003d new CompressionCodecFactory(job).getCodec(file);\n    if (null!\u003dcodec) {\n      isCompressedInput \u003d true;\n      decompressor \u003d CodecPool.getDecompressor(codec);\n      if (codec instanceof SplittableCompressionCodec) {\n        final SplitCompressionInputStream cIn \u003d\n          ((SplittableCompressionCodec)codec).createInputStream(\n            fileIn, decompressor, start, end,\n            SplittableCompressionCodec.READ_MODE.BYBLOCK);\n        in \u003d new CompressedSplitLineReader(cIn, job,\n            this.recordDelimiterBytes);\n        start \u003d cIn.getAdjustedStart();\n        end \u003d cIn.getAdjustedEnd();\n        filePosition \u003d cIn;\n      } else {\n        if (start !\u003d 0) {\n          // So we have a split that is only part of a file stored using\n          // a Compression codec that cannot be split.\n          throw new IOException(\"Cannot seek in \" +\n              codec.getClass().getSimpleName() + \" compressed stream\");\n        }\n\n        in \u003d new SplitLineReader(codec.createInputStream(fileIn,\n            decompressor), job, this.recordDelimiterBytes);\n        filePosition \u003d fileIn;\n      }\n    } else {\n      fileIn.seek(start);\n      in \u003d new UncompressedSplitLineReader(\n          fileIn, job, this.recordDelimiterBytes, split.getLength());\n      filePosition \u003d fileIn;\n    }\n    // If this is not the first split, we always throw away first record\n    // because we always (except the last split) read one extra line in\n    // next() method.\n    if (start !\u003d 0) {\n      start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n    }\n    this.pos \u003d start;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "2edcf931d7843cddcf3da5666a73d6ee9a10d00d": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2094. LineRecordReader should not seek into non-splittable, compressed streams.\n",
      "commitDate": "08/05/15 2:31 PM",
      "commitName": "2edcf931d7843cddcf3da5666a73d6ee9a10d00d",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "14/11/14 3:45 AM",
      "commitNameOld": "1a1dcce827d8747a5629151afa335598fbc94f9e",
      "commitAuthorOld": "Ravi Prakash",
      "daysBetweenCommits": 175.41,
      "commitsBetweenForRepo": 1431,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,52 @@\n   public void initialize(InputSplit genericSplit,\n                          TaskAttemptContext context) throws IOException {\n     FileSplit split \u003d (FileSplit) genericSplit;\n     Configuration job \u003d context.getConfiguration();\n     this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n     start \u003d split.getStart();\n     end \u003d start + split.getLength();\n     final Path file \u003d split.getPath();\n \n     // open the file and seek to the start of the split\n     final FileSystem fs \u003d file.getFileSystem(job);\n     fileIn \u003d fs.open(file);\n     \n     CompressionCodec codec \u003d new CompressionCodecFactory(job).getCodec(file);\n     if (null!\u003dcodec) {\n-      isCompressedInput \u003d true;\t\n+      isCompressedInput \u003d true;\n       decompressor \u003d CodecPool.getDecompressor(codec);\n       if (codec instanceof SplittableCompressionCodec) {\n         final SplitCompressionInputStream cIn \u003d\n           ((SplittableCompressionCodec)codec).createInputStream(\n             fileIn, decompressor, start, end,\n             SplittableCompressionCodec.READ_MODE.BYBLOCK);\n         in \u003d new CompressedSplitLineReader(cIn, job,\n             this.recordDelimiterBytes);\n         start \u003d cIn.getAdjustedStart();\n         end \u003d cIn.getAdjustedEnd();\n         filePosition \u003d cIn;\n       } else {\n+        if (start !\u003d 0) {\n+          // So we have a split that is only part of a file stored using\n+          // a Compression codec that cannot be split.\n+          throw new IOException(\"Cannot seek in \" +\n+              codec.getClass().getSimpleName() + \" compressed stream\");\n+        }\n+\n         in \u003d new SplitLineReader(codec.createInputStream(fileIn,\n             decompressor), job, this.recordDelimiterBytes);\n         filePosition \u003d fileIn;\n       }\n     } else {\n       fileIn.seek(start);\n       in \u003d new SplitLineReader(fileIn, job, this.recordDelimiterBytes);\n       filePosition \u003d fileIn;\n     }\n     // If this is not the first split, we always throw away first record\n     // because we always (except the last split) read one extra line in\n     // next() method.\n     if (start !\u003d 0) {\n       start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n     }\n     this.pos \u003d start;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(InputSplit genericSplit,\n                         TaskAttemptContext context) throws IOException {\n    FileSplit split \u003d (FileSplit) genericSplit;\n    Configuration job \u003d context.getConfiguration();\n    this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n    start \u003d split.getStart();\n    end \u003d start + split.getLength();\n    final Path file \u003d split.getPath();\n\n    // open the file and seek to the start of the split\n    final FileSystem fs \u003d file.getFileSystem(job);\n    fileIn \u003d fs.open(file);\n    \n    CompressionCodec codec \u003d new CompressionCodecFactory(job).getCodec(file);\n    if (null!\u003dcodec) {\n      isCompressedInput \u003d true;\n      decompressor \u003d CodecPool.getDecompressor(codec);\n      if (codec instanceof SplittableCompressionCodec) {\n        final SplitCompressionInputStream cIn \u003d\n          ((SplittableCompressionCodec)codec).createInputStream(\n            fileIn, decompressor, start, end,\n            SplittableCompressionCodec.READ_MODE.BYBLOCK);\n        in \u003d new CompressedSplitLineReader(cIn, job,\n            this.recordDelimiterBytes);\n        start \u003d cIn.getAdjustedStart();\n        end \u003d cIn.getAdjustedEnd();\n        filePosition \u003d cIn;\n      } else {\n        if (start !\u003d 0) {\n          // So we have a split that is only part of a file stored using\n          // a Compression codec that cannot be split.\n          throw new IOException(\"Cannot seek in \" +\n              codec.getClass().getSimpleName() + \" compressed stream\");\n        }\n\n        in \u003d new SplitLineReader(codec.createInputStream(fileIn,\n            decompressor), job, this.recordDelimiterBytes);\n        filePosition \u003d fileIn;\n      }\n    } else {\n      fileIn.seek(start);\n      in \u003d new SplitLineReader(fileIn, job, this.recordDelimiterBytes);\n      filePosition \u003d fileIn;\n    }\n    // If this is not the first split, we always throw away first record\n    // because we always (except the last split) read one extra line in\n    // next() method.\n    if (start !\u003d 0) {\n      start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n    }\n    this.pos \u003d start;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "18d99c12c371cfd7b9604e321d8bd6a7be9c4977": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5656. bzip2 codec can drop records when reading data in splits. Contributed by Jason Lowe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1549705 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/12/13 3:31 PM",
      "commitName": "18d99c12c371cfd7b9604e321d8bd6a7be9c4977",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "15/04/13 2:33 PM",
      "commitNameOld": "947e97f354edb1e27432cf3f1a2dff098f01071f",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 238.08,
      "commitsBetweenForRepo": 1478,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,45 @@\n   public void initialize(InputSplit genericSplit,\n                          TaskAttemptContext context) throws IOException {\n     FileSplit split \u003d (FileSplit) genericSplit;\n     Configuration job \u003d context.getConfiguration();\n     this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n     start \u003d split.getStart();\n     end \u003d start + split.getLength();\n     final Path file \u003d split.getPath();\n \n     // open the file and seek to the start of the split\n     final FileSystem fs \u003d file.getFileSystem(job);\n     fileIn \u003d fs.open(file);\n     \n     CompressionCodec codec \u003d new CompressionCodecFactory(job).getCodec(file);\n     if (null!\u003dcodec) {\n       isCompressedInput \u003d true;\t\n       decompressor \u003d CodecPool.getDecompressor(codec);\n       if (codec instanceof SplittableCompressionCodec) {\n         final SplitCompressionInputStream cIn \u003d\n           ((SplittableCompressionCodec)codec).createInputStream(\n             fileIn, decompressor, start, end,\n             SplittableCompressionCodec.READ_MODE.BYBLOCK);\n-        if (null \u003d\u003d this.recordDelimiterBytes){\n-          in \u003d new LineReader(cIn, job);\n-        } else {\n-          in \u003d new LineReader(cIn, job, this.recordDelimiterBytes);\n-        }\n-\n+        in \u003d new CompressedSplitLineReader(cIn, job,\n+            this.recordDelimiterBytes);\n         start \u003d cIn.getAdjustedStart();\n         end \u003d cIn.getAdjustedEnd();\n         filePosition \u003d cIn;\n       } else {\n-        if (null \u003d\u003d this.recordDelimiterBytes) {\n-          in \u003d new LineReader(codec.createInputStream(fileIn, decompressor),\n-              job);\n-        } else {\n-          in \u003d new LineReader(codec.createInputStream(fileIn,\n-              decompressor), job, this.recordDelimiterBytes);\n-        }\n+        in \u003d new SplitLineReader(codec.createInputStream(fileIn,\n+            decompressor), job, this.recordDelimiterBytes);\n         filePosition \u003d fileIn;\n       }\n     } else {\n       fileIn.seek(start);\n-      if (null \u003d\u003d this.recordDelimiterBytes){\n-        in \u003d new LineReader(fileIn, job);\n-      } else {\n-        in \u003d new LineReader(fileIn, job, this.recordDelimiterBytes);\n-      }\n-\n+      in \u003d new SplitLineReader(fileIn, job, this.recordDelimiterBytes);\n       filePosition \u003d fileIn;\n     }\n     // If this is not the first split, we always throw away first record\n     // because we always (except the last split) read one extra line in\n     // next() method.\n     if (start !\u003d 0) {\n       start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n     }\n     this.pos \u003d start;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(InputSplit genericSplit,\n                         TaskAttemptContext context) throws IOException {\n    FileSplit split \u003d (FileSplit) genericSplit;\n    Configuration job \u003d context.getConfiguration();\n    this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n    start \u003d split.getStart();\n    end \u003d start + split.getLength();\n    final Path file \u003d split.getPath();\n\n    // open the file and seek to the start of the split\n    final FileSystem fs \u003d file.getFileSystem(job);\n    fileIn \u003d fs.open(file);\n    \n    CompressionCodec codec \u003d new CompressionCodecFactory(job).getCodec(file);\n    if (null!\u003dcodec) {\n      isCompressedInput \u003d true;\t\n      decompressor \u003d CodecPool.getDecompressor(codec);\n      if (codec instanceof SplittableCompressionCodec) {\n        final SplitCompressionInputStream cIn \u003d\n          ((SplittableCompressionCodec)codec).createInputStream(\n            fileIn, decompressor, start, end,\n            SplittableCompressionCodec.READ_MODE.BYBLOCK);\n        in \u003d new CompressedSplitLineReader(cIn, job,\n            this.recordDelimiterBytes);\n        start \u003d cIn.getAdjustedStart();\n        end \u003d cIn.getAdjustedEnd();\n        filePosition \u003d cIn;\n      } else {\n        in \u003d new SplitLineReader(codec.createInputStream(fileIn,\n            decompressor), job, this.recordDelimiterBytes);\n        filePosition \u003d fileIn;\n      }\n    } else {\n      fileIn.seek(start);\n      in \u003d new SplitLineReader(fileIn, job, this.recordDelimiterBytes);\n      filePosition \u003d fileIn;\n    }\n    // If this is not the first split, we always throw away first record\n    // because we always (except the last split) read one extra line in\n    // next() method.\n    if (start !\u003d 0) {\n      start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n    }\n    this.pos \u003d start;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "947e97f354edb1e27432cf3f1a2dff098f01071f": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4974. Optimising the LineRecordReader initialize() method (Gelesh via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1468232 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/04/13 2:33 PM",
      "commitName": "947e97f354edb1e27432cf3f1a2dff098f01071f",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "01/04/13 7:13 PM",
      "commitNameOld": "6f0c4dca74cab26275d48c05fedd9bc2aa57878f",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 13.81,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,59 @@\n   public void initialize(InputSplit genericSplit,\n                          TaskAttemptContext context) throws IOException {\n     FileSplit split \u003d (FileSplit) genericSplit;\n     Configuration job \u003d context.getConfiguration();\n     this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n     start \u003d split.getStart();\n     end \u003d start + split.getLength();\n     final Path file \u003d split.getPath();\n-    compressionCodecs \u003d new CompressionCodecFactory(job);\n-    codec \u003d compressionCodecs.getCodec(file);\n \n     // open the file and seek to the start of the split\n     final FileSystem fs \u003d file.getFileSystem(job);\n     fileIn \u003d fs.open(file);\n-    if (isCompressedInput()) {\n+    \n+    CompressionCodec codec \u003d new CompressionCodecFactory(job).getCodec(file);\n+    if (null!\u003dcodec) {\n+      isCompressedInput \u003d true;\t\n       decompressor \u003d CodecPool.getDecompressor(codec);\n       if (codec instanceof SplittableCompressionCodec) {\n         final SplitCompressionInputStream cIn \u003d\n           ((SplittableCompressionCodec)codec).createInputStream(\n             fileIn, decompressor, start, end,\n             SplittableCompressionCodec.READ_MODE.BYBLOCK);\n         if (null \u003d\u003d this.recordDelimiterBytes){\n           in \u003d new LineReader(cIn, job);\n         } else {\n           in \u003d new LineReader(cIn, job, this.recordDelimiterBytes);\n         }\n \n         start \u003d cIn.getAdjustedStart();\n         end \u003d cIn.getAdjustedEnd();\n         filePosition \u003d cIn;\n       } else {\n         if (null \u003d\u003d this.recordDelimiterBytes) {\n           in \u003d new LineReader(codec.createInputStream(fileIn, decompressor),\n               job);\n         } else {\n           in \u003d new LineReader(codec.createInputStream(fileIn,\n               decompressor), job, this.recordDelimiterBytes);\n         }\n         filePosition \u003d fileIn;\n       }\n     } else {\n       fileIn.seek(start);\n       if (null \u003d\u003d this.recordDelimiterBytes){\n         in \u003d new LineReader(fileIn, job);\n       } else {\n         in \u003d new LineReader(fileIn, job, this.recordDelimiterBytes);\n       }\n \n       filePosition \u003d fileIn;\n     }\n     // If this is not the first split, we always throw away first record\n     // because we always (except the last split) read one extra line in\n     // next() method.\n     if (start !\u003d 0) {\n       start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n     }\n     this.pos \u003d start;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(InputSplit genericSplit,\n                         TaskAttemptContext context) throws IOException {\n    FileSplit split \u003d (FileSplit) genericSplit;\n    Configuration job \u003d context.getConfiguration();\n    this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n    start \u003d split.getStart();\n    end \u003d start + split.getLength();\n    final Path file \u003d split.getPath();\n\n    // open the file and seek to the start of the split\n    final FileSystem fs \u003d file.getFileSystem(job);\n    fileIn \u003d fs.open(file);\n    \n    CompressionCodec codec \u003d new CompressionCodecFactory(job).getCodec(file);\n    if (null!\u003dcodec) {\n      isCompressedInput \u003d true;\t\n      decompressor \u003d CodecPool.getDecompressor(codec);\n      if (codec instanceof SplittableCompressionCodec) {\n        final SplitCompressionInputStream cIn \u003d\n          ((SplittableCompressionCodec)codec).createInputStream(\n            fileIn, decompressor, start, end,\n            SplittableCompressionCodec.READ_MODE.BYBLOCK);\n        if (null \u003d\u003d this.recordDelimiterBytes){\n          in \u003d new LineReader(cIn, job);\n        } else {\n          in \u003d new LineReader(cIn, job, this.recordDelimiterBytes);\n        }\n\n        start \u003d cIn.getAdjustedStart();\n        end \u003d cIn.getAdjustedEnd();\n        filePosition \u003d cIn;\n      } else {\n        if (null \u003d\u003d this.recordDelimiterBytes) {\n          in \u003d new LineReader(codec.createInputStream(fileIn, decompressor),\n              job);\n        } else {\n          in \u003d new LineReader(codec.createInputStream(fileIn,\n              decompressor), job, this.recordDelimiterBytes);\n        }\n        filePosition \u003d fileIn;\n      }\n    } else {\n      fileIn.seek(start);\n      if (null \u003d\u003d this.recordDelimiterBytes){\n        in \u003d new LineReader(fileIn, job);\n      } else {\n        in \u003d new LineReader(fileIn, job, this.recordDelimiterBytes);\n      }\n\n      filePosition \u003d fileIn;\n    }\n    // If this is not the first split, we always throw away first record\n    // because we always (except the last split) read one extra line in\n    // next() method.\n    if (start !\u003d 0) {\n      start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n    }\n    this.pos \u003d start;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "6f0c4dca74cab26275d48c05fedd9bc2aa57878f": {
      "type": "Ybodychange",
      "commitMessage": "Reverted MAPREDUCE-4974 because of test failures.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1463359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/13 7:13 PM",
      "commitName": "6f0c4dca74cab26275d48c05fedd9bc2aa57878f",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "01/04/13 10:45 AM",
      "commitNameOld": "b55756dd03086f6c081991d949ebcf7926af8af5",
      "commitAuthorOld": "Robert Joseph Evans",
      "daysBetweenCommits": 0.35,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,58 @@\n   public void initialize(InputSplit genericSplit,\n                          TaskAttemptContext context) throws IOException {\n     FileSplit split \u003d (FileSplit) genericSplit;\n     Configuration job \u003d context.getConfiguration();\n     this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n     start \u003d split.getStart();\n     end \u003d start + split.getLength();\n     final Path file \u003d split.getPath();\n+    compressionCodecs \u003d new CompressionCodecFactory(job);\n+    codec \u003d compressionCodecs.getCodec(file);\n \n     // open the file and seek to the start of the split\n     final FileSystem fs \u003d file.getFileSystem(job);\n     fileIn \u003d fs.open(file);\n     if (isCompressedInput()) {\n-      compressionCodecs \u003d new CompressionCodecFactory(job);\n-      codec \u003d compressionCodecs.getCodec(file);\n       decompressor \u003d CodecPool.getDecompressor(codec);\n       if (codec instanceof SplittableCompressionCodec) {\n         final SplitCompressionInputStream cIn \u003d\n           ((SplittableCompressionCodec)codec).createInputStream(\n             fileIn, decompressor, start, end,\n             SplittableCompressionCodec.READ_MODE.BYBLOCK);\n         if (null \u003d\u003d this.recordDelimiterBytes){\n           in \u003d new LineReader(cIn, job);\n         } else {\n           in \u003d new LineReader(cIn, job, this.recordDelimiterBytes);\n         }\n \n         start \u003d cIn.getAdjustedStart();\n         end \u003d cIn.getAdjustedEnd();\n         filePosition \u003d cIn;\n       } else {\n         if (null \u003d\u003d this.recordDelimiterBytes) {\n           in \u003d new LineReader(codec.createInputStream(fileIn, decompressor),\n               job);\n         } else {\n           in \u003d new LineReader(codec.createInputStream(fileIn,\n               decompressor), job, this.recordDelimiterBytes);\n         }\n         filePosition \u003d fileIn;\n       }\n     } else {\n       fileIn.seek(start);\n       if (null \u003d\u003d this.recordDelimiterBytes){\n         in \u003d new LineReader(fileIn, job);\n       } else {\n         in \u003d new LineReader(fileIn, job, this.recordDelimiterBytes);\n       }\n \n       filePosition \u003d fileIn;\n     }\n     // If this is not the first split, we always throw away first record\n     // because we always (except the last split) read one extra line in\n     // next() method.\n     if (start !\u003d 0) {\n       start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n     }\n     this.pos \u003d start;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(InputSplit genericSplit,\n                         TaskAttemptContext context) throws IOException {\n    FileSplit split \u003d (FileSplit) genericSplit;\n    Configuration job \u003d context.getConfiguration();\n    this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n    start \u003d split.getStart();\n    end \u003d start + split.getLength();\n    final Path file \u003d split.getPath();\n    compressionCodecs \u003d new CompressionCodecFactory(job);\n    codec \u003d compressionCodecs.getCodec(file);\n\n    // open the file and seek to the start of the split\n    final FileSystem fs \u003d file.getFileSystem(job);\n    fileIn \u003d fs.open(file);\n    if (isCompressedInput()) {\n      decompressor \u003d CodecPool.getDecompressor(codec);\n      if (codec instanceof SplittableCompressionCodec) {\n        final SplitCompressionInputStream cIn \u003d\n          ((SplittableCompressionCodec)codec).createInputStream(\n            fileIn, decompressor, start, end,\n            SplittableCompressionCodec.READ_MODE.BYBLOCK);\n        if (null \u003d\u003d this.recordDelimiterBytes){\n          in \u003d new LineReader(cIn, job);\n        } else {\n          in \u003d new LineReader(cIn, job, this.recordDelimiterBytes);\n        }\n\n        start \u003d cIn.getAdjustedStart();\n        end \u003d cIn.getAdjustedEnd();\n        filePosition \u003d cIn;\n      } else {\n        if (null \u003d\u003d this.recordDelimiterBytes) {\n          in \u003d new LineReader(codec.createInputStream(fileIn, decompressor),\n              job);\n        } else {\n          in \u003d new LineReader(codec.createInputStream(fileIn,\n              decompressor), job, this.recordDelimiterBytes);\n        }\n        filePosition \u003d fileIn;\n      }\n    } else {\n      fileIn.seek(start);\n      if (null \u003d\u003d this.recordDelimiterBytes){\n        in \u003d new LineReader(fileIn, job);\n      } else {\n        in \u003d new LineReader(fileIn, job, this.recordDelimiterBytes);\n      }\n\n      filePosition \u003d fileIn;\n    }\n    // If this is not the first split, we always throw away first record\n    // because we always (except the last split) read one extra line in\n    // next() method.\n    if (start !\u003d 0) {\n      start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n    }\n    this.pos \u003d start;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "b55756dd03086f6c081991d949ebcf7926af8af5": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4974. Optimising the LineRecordReader initialize() method (Gelesh via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1463221 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/04/13 10:45 AM",
      "commitName": "b55756dd03086f6c081991d949ebcf7926af8af5",
      "commitAuthor": "Robert Joseph Evans",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 585.73,
      "commitsBetweenForRepo": 3537,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,58 @@\n   public void initialize(InputSplit genericSplit,\n                          TaskAttemptContext context) throws IOException {\n     FileSplit split \u003d (FileSplit) genericSplit;\n     Configuration job \u003d context.getConfiguration();\n     this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n     start \u003d split.getStart();\n     end \u003d start + split.getLength();\n     final Path file \u003d split.getPath();\n-    compressionCodecs \u003d new CompressionCodecFactory(job);\n-    codec \u003d compressionCodecs.getCodec(file);\n \n     // open the file and seek to the start of the split\n     final FileSystem fs \u003d file.getFileSystem(job);\n     fileIn \u003d fs.open(file);\n     if (isCompressedInput()) {\n+      compressionCodecs \u003d new CompressionCodecFactory(job);\n+      codec \u003d compressionCodecs.getCodec(file);\n       decompressor \u003d CodecPool.getDecompressor(codec);\n       if (codec instanceof SplittableCompressionCodec) {\n         final SplitCompressionInputStream cIn \u003d\n           ((SplittableCompressionCodec)codec).createInputStream(\n             fileIn, decompressor, start, end,\n             SplittableCompressionCodec.READ_MODE.BYBLOCK);\n         if (null \u003d\u003d this.recordDelimiterBytes){\n           in \u003d new LineReader(cIn, job);\n         } else {\n           in \u003d new LineReader(cIn, job, this.recordDelimiterBytes);\n         }\n \n         start \u003d cIn.getAdjustedStart();\n         end \u003d cIn.getAdjustedEnd();\n         filePosition \u003d cIn;\n       } else {\n         if (null \u003d\u003d this.recordDelimiterBytes) {\n           in \u003d new LineReader(codec.createInputStream(fileIn, decompressor),\n               job);\n         } else {\n           in \u003d new LineReader(codec.createInputStream(fileIn,\n               decompressor), job, this.recordDelimiterBytes);\n         }\n         filePosition \u003d fileIn;\n       }\n     } else {\n       fileIn.seek(start);\n       if (null \u003d\u003d this.recordDelimiterBytes){\n         in \u003d new LineReader(fileIn, job);\n       } else {\n         in \u003d new LineReader(fileIn, job, this.recordDelimiterBytes);\n       }\n \n       filePosition \u003d fileIn;\n     }\n     // If this is not the first split, we always throw away first record\n     // because we always (except the last split) read one extra line in\n     // next() method.\n     if (start !\u003d 0) {\n       start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n     }\n     this.pos \u003d start;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(InputSplit genericSplit,\n                         TaskAttemptContext context) throws IOException {\n    FileSplit split \u003d (FileSplit) genericSplit;\n    Configuration job \u003d context.getConfiguration();\n    this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n    start \u003d split.getStart();\n    end \u003d start + split.getLength();\n    final Path file \u003d split.getPath();\n\n    // open the file and seek to the start of the split\n    final FileSystem fs \u003d file.getFileSystem(job);\n    fileIn \u003d fs.open(file);\n    if (isCompressedInput()) {\n      compressionCodecs \u003d new CompressionCodecFactory(job);\n      codec \u003d compressionCodecs.getCodec(file);\n      decompressor \u003d CodecPool.getDecompressor(codec);\n      if (codec instanceof SplittableCompressionCodec) {\n        final SplitCompressionInputStream cIn \u003d\n          ((SplittableCompressionCodec)codec).createInputStream(\n            fileIn, decompressor, start, end,\n            SplittableCompressionCodec.READ_MODE.BYBLOCK);\n        if (null \u003d\u003d this.recordDelimiterBytes){\n          in \u003d new LineReader(cIn, job);\n        } else {\n          in \u003d new LineReader(cIn, job, this.recordDelimiterBytes);\n        }\n\n        start \u003d cIn.getAdjustedStart();\n        end \u003d cIn.getAdjustedEnd();\n        filePosition \u003d cIn;\n      } else {\n        if (null \u003d\u003d this.recordDelimiterBytes) {\n          in \u003d new LineReader(codec.createInputStream(fileIn, decompressor),\n              job);\n        } else {\n          in \u003d new LineReader(codec.createInputStream(fileIn,\n              decompressor), job, this.recordDelimiterBytes);\n        }\n        filePosition \u003d fileIn;\n      }\n    } else {\n      fileIn.seek(start);\n      if (null \u003d\u003d this.recordDelimiterBytes){\n        in \u003d new LineReader(fileIn, job);\n      } else {\n        in \u003d new LineReader(fileIn, job, this.recordDelimiterBytes);\n      }\n\n      filePosition \u003d fileIn;\n    }\n    // If this is not the first split, we always throw away first record\n    // because we always (except the last split) read one extra line in\n    // next() method.\n    if (start !\u003d 0) {\n      start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n    }\n    this.pos \u003d start;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void initialize(InputSplit genericSplit,\n                         TaskAttemptContext context) throws IOException {\n    FileSplit split \u003d (FileSplit) genericSplit;\n    Configuration job \u003d context.getConfiguration();\n    this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n    start \u003d split.getStart();\n    end \u003d start + split.getLength();\n    final Path file \u003d split.getPath();\n    compressionCodecs \u003d new CompressionCodecFactory(job);\n    codec \u003d compressionCodecs.getCodec(file);\n\n    // open the file and seek to the start of the split\n    final FileSystem fs \u003d file.getFileSystem(job);\n    fileIn \u003d fs.open(file);\n    if (isCompressedInput()) {\n      decompressor \u003d CodecPool.getDecompressor(codec);\n      if (codec instanceof SplittableCompressionCodec) {\n        final SplitCompressionInputStream cIn \u003d\n          ((SplittableCompressionCodec)codec).createInputStream(\n            fileIn, decompressor, start, end,\n            SplittableCompressionCodec.READ_MODE.BYBLOCK);\n        if (null \u003d\u003d this.recordDelimiterBytes){\n          in \u003d new LineReader(cIn, job);\n        } else {\n          in \u003d new LineReader(cIn, job, this.recordDelimiterBytes);\n        }\n\n        start \u003d cIn.getAdjustedStart();\n        end \u003d cIn.getAdjustedEnd();\n        filePosition \u003d cIn;\n      } else {\n        if (null \u003d\u003d this.recordDelimiterBytes) {\n          in \u003d new LineReader(codec.createInputStream(fileIn, decompressor),\n              job);\n        } else {\n          in \u003d new LineReader(codec.createInputStream(fileIn,\n              decompressor), job, this.recordDelimiterBytes);\n        }\n        filePosition \u003d fileIn;\n      }\n    } else {\n      fileIn.seek(start);\n      if (null \u003d\u003d this.recordDelimiterBytes){\n        in \u003d new LineReader(fileIn, job);\n      } else {\n        in \u003d new LineReader(fileIn, job, this.recordDelimiterBytes);\n      }\n\n      filePosition \u003d fileIn;\n    }\n    // If this is not the first split, we always throw away first record\n    // because we always (except the last split) read one extra line in\n    // next() method.\n    if (start !\u003d 0) {\n      start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n    }\n    this.pos \u003d start;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void initialize(InputSplit genericSplit,\n                         TaskAttemptContext context) throws IOException {\n    FileSplit split \u003d (FileSplit) genericSplit;\n    Configuration job \u003d context.getConfiguration();\n    this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n    start \u003d split.getStart();\n    end \u003d start + split.getLength();\n    final Path file \u003d split.getPath();\n    compressionCodecs \u003d new CompressionCodecFactory(job);\n    codec \u003d compressionCodecs.getCodec(file);\n\n    // open the file and seek to the start of the split\n    final FileSystem fs \u003d file.getFileSystem(job);\n    fileIn \u003d fs.open(file);\n    if (isCompressedInput()) {\n      decompressor \u003d CodecPool.getDecompressor(codec);\n      if (codec instanceof SplittableCompressionCodec) {\n        final SplitCompressionInputStream cIn \u003d\n          ((SplittableCompressionCodec)codec).createInputStream(\n            fileIn, decompressor, start, end,\n            SplittableCompressionCodec.READ_MODE.BYBLOCK);\n        if (null \u003d\u003d this.recordDelimiterBytes){\n          in \u003d new LineReader(cIn, job);\n        } else {\n          in \u003d new LineReader(cIn, job, this.recordDelimiterBytes);\n        }\n\n        start \u003d cIn.getAdjustedStart();\n        end \u003d cIn.getAdjustedEnd();\n        filePosition \u003d cIn;\n      } else {\n        if (null \u003d\u003d this.recordDelimiterBytes) {\n          in \u003d new LineReader(codec.createInputStream(fileIn, decompressor),\n              job);\n        } else {\n          in \u003d new LineReader(codec.createInputStream(fileIn,\n              decompressor), job, this.recordDelimiterBytes);\n        }\n        filePosition \u003d fileIn;\n      }\n    } else {\n      fileIn.seek(start);\n      if (null \u003d\u003d this.recordDelimiterBytes){\n        in \u003d new LineReader(fileIn, job);\n      } else {\n        in \u003d new LineReader(fileIn, job, this.recordDelimiterBytes);\n      }\n\n      filePosition \u003d fileIn;\n    }\n    // If this is not the first split, we always throw away first record\n    // because we always (except the last split) read one extra line in\n    // next() method.\n    if (start !\u003d 0) {\n      start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n    }\n    this.pos \u003d start;\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java"
      }
    },
    "4796e1adcb912005198c9003305c97cf3a8b523e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2365. Add counters to track bytes (read,written) via File(Input,Output)Format. Contributed by Siddharth Seth. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1146515 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/07/11 4:36 PM",
      "commitName": "4796e1adcb912005198c9003305c97cf3a8b523e",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 31.07,
      "commitsBetweenForRepo": 104,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,58 @@\n   public void initialize(InputSplit genericSplit,\n                          TaskAttemptContext context) throws IOException {\n     FileSplit split \u003d (FileSplit) genericSplit;\n-    inputByteCounter \u003d context.getCounter(\n-      FileInputFormat.COUNTER_GROUP, FileInputFormat.BYTES_READ);\n     Configuration job \u003d context.getConfiguration();\n     this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n     start \u003d split.getStart();\n     end \u003d start + split.getLength();\n     final Path file \u003d split.getPath();\n     compressionCodecs \u003d new CompressionCodecFactory(job);\n     codec \u003d compressionCodecs.getCodec(file);\n \n     // open the file and seek to the start of the split\n     final FileSystem fs \u003d file.getFileSystem(job);\n     fileIn \u003d fs.open(file);\n     if (isCompressedInput()) {\n       decompressor \u003d CodecPool.getDecompressor(codec);\n       if (codec instanceof SplittableCompressionCodec) {\n         final SplitCompressionInputStream cIn \u003d\n           ((SplittableCompressionCodec)codec).createInputStream(\n             fileIn, decompressor, start, end,\n             SplittableCompressionCodec.READ_MODE.BYBLOCK);\n         if (null \u003d\u003d this.recordDelimiterBytes){\n           in \u003d new LineReader(cIn, job);\n         } else {\n           in \u003d new LineReader(cIn, job, this.recordDelimiterBytes);\n         }\n \n         start \u003d cIn.getAdjustedStart();\n         end \u003d cIn.getAdjustedEnd();\n         filePosition \u003d cIn;\n       } else {\n         if (null \u003d\u003d this.recordDelimiterBytes) {\n           in \u003d new LineReader(codec.createInputStream(fileIn, decompressor),\n               job);\n         } else {\n           in \u003d new LineReader(codec.createInputStream(fileIn,\n               decompressor), job, this.recordDelimiterBytes);\n         }\n         filePosition \u003d fileIn;\n       }\n     } else {\n       fileIn.seek(start);\n       if (null \u003d\u003d this.recordDelimiterBytes){\n         in \u003d new LineReader(fileIn, job);\n       } else {\n         in \u003d new LineReader(fileIn, job, this.recordDelimiterBytes);\n       }\n \n       filePosition \u003d fileIn;\n     }\n     // If this is not the first split, we always throw away first record\n     // because we always (except the last split) read one extra line in\n     // next() method.\n     if (start !\u003d 0) {\n       start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n     }\n     this.pos \u003d start;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(InputSplit genericSplit,\n                         TaskAttemptContext context) throws IOException {\n    FileSplit split \u003d (FileSplit) genericSplit;\n    Configuration job \u003d context.getConfiguration();\n    this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n    start \u003d split.getStart();\n    end \u003d start + split.getLength();\n    final Path file \u003d split.getPath();\n    compressionCodecs \u003d new CompressionCodecFactory(job);\n    codec \u003d compressionCodecs.getCodec(file);\n\n    // open the file and seek to the start of the split\n    final FileSystem fs \u003d file.getFileSystem(job);\n    fileIn \u003d fs.open(file);\n    if (isCompressedInput()) {\n      decompressor \u003d CodecPool.getDecompressor(codec);\n      if (codec instanceof SplittableCompressionCodec) {\n        final SplitCompressionInputStream cIn \u003d\n          ((SplittableCompressionCodec)codec).createInputStream(\n            fileIn, decompressor, start, end,\n            SplittableCompressionCodec.READ_MODE.BYBLOCK);\n        if (null \u003d\u003d this.recordDelimiterBytes){\n          in \u003d new LineReader(cIn, job);\n        } else {\n          in \u003d new LineReader(cIn, job, this.recordDelimiterBytes);\n        }\n\n        start \u003d cIn.getAdjustedStart();\n        end \u003d cIn.getAdjustedEnd();\n        filePosition \u003d cIn;\n      } else {\n        if (null \u003d\u003d this.recordDelimiterBytes) {\n          in \u003d new LineReader(codec.createInputStream(fileIn, decompressor),\n              job);\n        } else {\n          in \u003d new LineReader(codec.createInputStream(fileIn,\n              decompressor), job, this.recordDelimiterBytes);\n        }\n        filePosition \u003d fileIn;\n      }\n    } else {\n      fileIn.seek(start);\n      if (null \u003d\u003d this.recordDelimiterBytes){\n        in \u003d new LineReader(fileIn, job);\n      } else {\n        in \u003d new LineReader(fileIn, job, this.recordDelimiterBytes);\n      }\n\n      filePosition \u003d fileIn;\n    }\n    // If this is not the first split, we always throw away first record\n    // because we always (except the last split) read one extra line in\n    // next() method.\n    if (start !\u003d 0) {\n      start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n    }\n    this.pos \u003d start;\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,60 @@\n+  public void initialize(InputSplit genericSplit,\n+                         TaskAttemptContext context) throws IOException {\n+    FileSplit split \u003d (FileSplit) genericSplit;\n+    inputByteCounter \u003d context.getCounter(\n+      FileInputFormat.COUNTER_GROUP, FileInputFormat.BYTES_READ);\n+    Configuration job \u003d context.getConfiguration();\n+    this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n+    start \u003d split.getStart();\n+    end \u003d start + split.getLength();\n+    final Path file \u003d split.getPath();\n+    compressionCodecs \u003d new CompressionCodecFactory(job);\n+    codec \u003d compressionCodecs.getCodec(file);\n+\n+    // open the file and seek to the start of the split\n+    final FileSystem fs \u003d file.getFileSystem(job);\n+    fileIn \u003d fs.open(file);\n+    if (isCompressedInput()) {\n+      decompressor \u003d CodecPool.getDecompressor(codec);\n+      if (codec instanceof SplittableCompressionCodec) {\n+        final SplitCompressionInputStream cIn \u003d\n+          ((SplittableCompressionCodec)codec).createInputStream(\n+            fileIn, decompressor, start, end,\n+            SplittableCompressionCodec.READ_MODE.BYBLOCK);\n+        if (null \u003d\u003d this.recordDelimiterBytes){\n+          in \u003d new LineReader(cIn, job);\n+        } else {\n+          in \u003d new LineReader(cIn, job, this.recordDelimiterBytes);\n+        }\n+\n+        start \u003d cIn.getAdjustedStart();\n+        end \u003d cIn.getAdjustedEnd();\n+        filePosition \u003d cIn;\n+      } else {\n+        if (null \u003d\u003d this.recordDelimiterBytes) {\n+          in \u003d new LineReader(codec.createInputStream(fileIn, decompressor),\n+              job);\n+        } else {\n+          in \u003d new LineReader(codec.createInputStream(fileIn,\n+              decompressor), job, this.recordDelimiterBytes);\n+        }\n+        filePosition \u003d fileIn;\n+      }\n+    } else {\n+      fileIn.seek(start);\n+      if (null \u003d\u003d this.recordDelimiterBytes){\n+        in \u003d new LineReader(fileIn, job);\n+      } else {\n+        in \u003d new LineReader(fileIn, job, this.recordDelimiterBytes);\n+      }\n+\n+      filePosition \u003d fileIn;\n+    }\n+    // If this is not the first split, we always throw away first record\n+    // because we always (except the last split) read one extra line in\n+    // next() method.\n+    if (start !\u003d 0) {\n+      start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n+    }\n+    this.pos \u003d start;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(InputSplit genericSplit,\n                         TaskAttemptContext context) throws IOException {\n    FileSplit split \u003d (FileSplit) genericSplit;\n    inputByteCounter \u003d context.getCounter(\n      FileInputFormat.COUNTER_GROUP, FileInputFormat.BYTES_READ);\n    Configuration job \u003d context.getConfiguration();\n    this.maxLineLength \u003d job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n    start \u003d split.getStart();\n    end \u003d start + split.getLength();\n    final Path file \u003d split.getPath();\n    compressionCodecs \u003d new CompressionCodecFactory(job);\n    codec \u003d compressionCodecs.getCodec(file);\n\n    // open the file and seek to the start of the split\n    final FileSystem fs \u003d file.getFileSystem(job);\n    fileIn \u003d fs.open(file);\n    if (isCompressedInput()) {\n      decompressor \u003d CodecPool.getDecompressor(codec);\n      if (codec instanceof SplittableCompressionCodec) {\n        final SplitCompressionInputStream cIn \u003d\n          ((SplittableCompressionCodec)codec).createInputStream(\n            fileIn, decompressor, start, end,\n            SplittableCompressionCodec.READ_MODE.BYBLOCK);\n        if (null \u003d\u003d this.recordDelimiterBytes){\n          in \u003d new LineReader(cIn, job);\n        } else {\n          in \u003d new LineReader(cIn, job, this.recordDelimiterBytes);\n        }\n\n        start \u003d cIn.getAdjustedStart();\n        end \u003d cIn.getAdjustedEnd();\n        filePosition \u003d cIn;\n      } else {\n        if (null \u003d\u003d this.recordDelimiterBytes) {\n          in \u003d new LineReader(codec.createInputStream(fileIn, decompressor),\n              job);\n        } else {\n          in \u003d new LineReader(codec.createInputStream(fileIn,\n              decompressor), job, this.recordDelimiterBytes);\n        }\n        filePosition \u003d fileIn;\n      }\n    } else {\n      fileIn.seek(start);\n      if (null \u003d\u003d this.recordDelimiterBytes){\n        in \u003d new LineReader(fileIn, job);\n      } else {\n        in \u003d new LineReader(fileIn, job, this.recordDelimiterBytes);\n      }\n\n      filePosition \u003d fileIn;\n    }\n    // If this is not the first split, we always throw away first record\n    // because we always (except the last split) read one extra line in\n    // next() method.\n    if (start !\u003d 0) {\n      start +\u003d in.readLine(new Text(), 0, maxBytesToConsume(start));\n    }\n    this.pos \u003d start;\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java"
    }
  }
}