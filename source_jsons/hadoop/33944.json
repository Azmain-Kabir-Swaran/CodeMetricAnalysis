{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DefaultOOMHandler.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/DefaultOOMHandler.java",
  "functionStartLine": 183,
  "functionEndLine": 212,
  "numCommitsSeen": 5,
  "timeTaken": 1458,
  "changeHistory": [
    "d5eca1a6a0e3939eead6711805b7a61c364d254b",
    "d9964799544eefcf424fcc178d987525f5356cdf"
  ],
  "changeHistoryShort": {
    "d5eca1a6a0e3939eead6711805b7a61c364d254b": "Ybodychange",
    "d9964799544eefcf424fcc178d987525f5356cdf": "Yintroduced"
  },
  "changeHistoryDetails": {
    "d5eca1a6a0e3939eead6711805b7a61c364d254b": {
      "type": "Ybodychange",
      "commitMessage": "YARN-6677. Preempt opportunistic containers when root container cgroup goes over memory limit. Contributed by Haibo Chen.\n",
      "commitDate": "07/06/18 4:38 PM",
      "commitName": "d5eca1a6a0e3939eead6711805b7a61c364d254b",
      "commitAuthor": "Miklos Szegedi",
      "commitDateOld": "23/05/18 4:35 PM",
      "commitNameOld": "d9964799544eefcf424fcc178d987525f5356cdf",
      "commitAuthorOld": "Haibo Chen",
      "daysBetweenCommits": 15.0,
      "commitsBetweenForRepo": 124,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,30 @@\n   public void run() {\n     try {\n-      // Reverse order by start time\n-      Comparator\u003cContainer\u003e comparator \u003d (Container o1, Container o2) -\u003e {\n-        long order \u003d o1.getContainerStartTime() - o2.getContainerStartTime();\n-        return order \u003e 0 ? -1 : order \u003c 0 ? 1 : 0;\n-      };\n-\n       // We kill containers until the kernel reports the OOM situation resolved\n       // Note: If the kernel has a delay this may kill more than necessary\n       while (true) {\n         String status \u003d cgroups.getCGroupParam(\n             CGroupsHandler.CGroupController.MEMORY,\n             \"\",\n             CGROUP_PARAM_MEMORY_OOM_CONTROL);\n         if (!status.contains(CGroupsHandler.UNDER_OOM)) {\n           break;\n         }\n \n-        // The first pass kills a recent container\n-        // that uses more than its request\n-        ArrayList\u003cContainer\u003e containers \u003d new ArrayList\u003c\u003e();\n-        containers.addAll(context.getContainers().values());\n-        // Note: Sorting may take a long time with 10K+ containers\n-        // but it is acceptable now with low number of containers per node\n-        containers.sort(comparator);\n+        boolean containerKilled \u003d killContainer();\n \n-        // Kill the latest container that exceeded its request\n-        boolean found \u003d false;\n-        for (Container container : containers) {\n-          if (!virtual) {\n-            if (killContainerIfOOM(container,\n-                CGROUP_PARAM_MEMORY_USAGE_BYTES)) {\n-              found \u003d true;\n-              break;\n-            }\n-          } else {\n-            if (killContainerIfOOM(container,\n-                CGROUP_PARAM_MEMORY_MEMSW_USAGE_BYTES)) {\n-              found \u003d true;\n-              break;\n-            }\n-          }\n+        if (!containerKilled) {\n+          // This can happen, if SIGKILL did not clean up\n+          // non-PGID or containers or containers launched by other users\n+          // or if a process was put to the root YARN cgroup.\n+          throw new YarnRuntimeException(\n+              \"Could not find any containers but CGroups \" +\n+                  \"reserved for containers ran out of memory. \" +\n+                  \"I am giving up\");\n         }\n-        if (found) {\n-          continue;\n-        }\n-\n-        // We have not found any containers that ran out of their limit,\n-        // so we will kill the latest one. This can happen, if all use\n-        // close to their request and one of them requests a big block\n-        // triggering the OOM freeze.\n-        // Currently there is no other way to identify the outstanding one.\n-        if (containers.size() \u003e 0) {\n-          Container container \u003d containers.get(0);\n-          sigKill(container);\n-          String message \u003d String.format(\n-              \"Newest container %s killed by elastic cgroups OOM handler using\",\n-              container.getContainerId());\n-          LOG.warn(message);\n-          continue;\n-        }\n-\n-        // This can happen, if SIGKILL did not clean up\n-        // non-PGID or containers or containers launched by other users\n-        // or if a process was put to the root YARN cgroup.\n-        throw new YarnRuntimeException(\n-            \"Could not find any containers but CGroups \" +\n-                \"reserved for containers ran out of memory. \" +\n-                \"I am giving up\");\n       }\n     } catch (ResourceHandlerException ex) {\n-      LOG.warn(\"Could not fecth OOM status. \" +\n+      LOG.warn(\"Could not fetch OOM status. \" +\n           \"This is expected at shutdown. Exiting.\", ex);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    try {\n      // We kill containers until the kernel reports the OOM situation resolved\n      // Note: If the kernel has a delay this may kill more than necessary\n      while (true) {\n        String status \u003d cgroups.getCGroupParam(\n            CGroupsHandler.CGroupController.MEMORY,\n            \"\",\n            CGROUP_PARAM_MEMORY_OOM_CONTROL);\n        if (!status.contains(CGroupsHandler.UNDER_OOM)) {\n          break;\n        }\n\n        boolean containerKilled \u003d killContainer();\n\n        if (!containerKilled) {\n          // This can happen, if SIGKILL did not clean up\n          // non-PGID or containers or containers launched by other users\n          // or if a process was put to the root YARN cgroup.\n          throw new YarnRuntimeException(\n              \"Could not find any containers but CGroups \" +\n                  \"reserved for containers ran out of memory. \" +\n                  \"I am giving up\");\n        }\n      }\n    } catch (ResourceHandlerException ex) {\n      LOG.warn(\"Could not fetch OOM status. \" +\n          \"This is expected at shutdown. Exiting.\", ex);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/DefaultOOMHandler.java",
      "extendedDetails": {}
    },
    "d9964799544eefcf424fcc178d987525f5356cdf": {
      "type": "Yintroduced",
      "commitMessage": "YARN-4599. Set OOM control for memory cgroups. (Miklos Szegedi via Haibo Chen)\n",
      "commitDate": "23/05/18 4:35 PM",
      "commitName": "d9964799544eefcf424fcc178d987525f5356cdf",
      "commitAuthor": "Haibo Chen",
      "diff": "@@ -0,0 +1,76 @@\n+  public void run() {\n+    try {\n+      // Reverse order by start time\n+      Comparator\u003cContainer\u003e comparator \u003d (Container o1, Container o2) -\u003e {\n+        long order \u003d o1.getContainerStartTime() - o2.getContainerStartTime();\n+        return order \u003e 0 ? -1 : order \u003c 0 ? 1 : 0;\n+      };\n+\n+      // We kill containers until the kernel reports the OOM situation resolved\n+      // Note: If the kernel has a delay this may kill more than necessary\n+      while (true) {\n+        String status \u003d cgroups.getCGroupParam(\n+            CGroupsHandler.CGroupController.MEMORY,\n+            \"\",\n+            CGROUP_PARAM_MEMORY_OOM_CONTROL);\n+        if (!status.contains(CGroupsHandler.UNDER_OOM)) {\n+          break;\n+        }\n+\n+        // The first pass kills a recent container\n+        // that uses more than its request\n+        ArrayList\u003cContainer\u003e containers \u003d new ArrayList\u003c\u003e();\n+        containers.addAll(context.getContainers().values());\n+        // Note: Sorting may take a long time with 10K+ containers\n+        // but it is acceptable now with low number of containers per node\n+        containers.sort(comparator);\n+\n+        // Kill the latest container that exceeded its request\n+        boolean found \u003d false;\n+        for (Container container : containers) {\n+          if (!virtual) {\n+            if (killContainerIfOOM(container,\n+                CGROUP_PARAM_MEMORY_USAGE_BYTES)) {\n+              found \u003d true;\n+              break;\n+            }\n+          } else {\n+            if (killContainerIfOOM(container,\n+                CGROUP_PARAM_MEMORY_MEMSW_USAGE_BYTES)) {\n+              found \u003d true;\n+              break;\n+            }\n+          }\n+        }\n+        if (found) {\n+          continue;\n+        }\n+\n+        // We have not found any containers that ran out of their limit,\n+        // so we will kill the latest one. This can happen, if all use\n+        // close to their request and one of them requests a big block\n+        // triggering the OOM freeze.\n+        // Currently there is no other way to identify the outstanding one.\n+        if (containers.size() \u003e 0) {\n+          Container container \u003d containers.get(0);\n+          sigKill(container);\n+          String message \u003d String.format(\n+              \"Newest container %s killed by elastic cgroups OOM handler using\",\n+              container.getContainerId());\n+          LOG.warn(message);\n+          continue;\n+        }\n+\n+        // This can happen, if SIGKILL did not clean up\n+        // non-PGID or containers or containers launched by other users\n+        // or if a process was put to the root YARN cgroup.\n+        throw new YarnRuntimeException(\n+            \"Could not find any containers but CGroups \" +\n+                \"reserved for containers ran out of memory. \" +\n+                \"I am giving up\");\n+      }\n+    } catch (ResourceHandlerException ex) {\n+      LOG.warn(\"Could not fecth OOM status. \" +\n+          \"This is expected at shutdown. Exiting.\", ex);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    try {\n      // Reverse order by start time\n      Comparator\u003cContainer\u003e comparator \u003d (Container o1, Container o2) -\u003e {\n        long order \u003d o1.getContainerStartTime() - o2.getContainerStartTime();\n        return order \u003e 0 ? -1 : order \u003c 0 ? 1 : 0;\n      };\n\n      // We kill containers until the kernel reports the OOM situation resolved\n      // Note: If the kernel has a delay this may kill more than necessary\n      while (true) {\n        String status \u003d cgroups.getCGroupParam(\n            CGroupsHandler.CGroupController.MEMORY,\n            \"\",\n            CGROUP_PARAM_MEMORY_OOM_CONTROL);\n        if (!status.contains(CGroupsHandler.UNDER_OOM)) {\n          break;\n        }\n\n        // The first pass kills a recent container\n        // that uses more than its request\n        ArrayList\u003cContainer\u003e containers \u003d new ArrayList\u003c\u003e();\n        containers.addAll(context.getContainers().values());\n        // Note: Sorting may take a long time with 10K+ containers\n        // but it is acceptable now with low number of containers per node\n        containers.sort(comparator);\n\n        // Kill the latest container that exceeded its request\n        boolean found \u003d false;\n        for (Container container : containers) {\n          if (!virtual) {\n            if (killContainerIfOOM(container,\n                CGROUP_PARAM_MEMORY_USAGE_BYTES)) {\n              found \u003d true;\n              break;\n            }\n          } else {\n            if (killContainerIfOOM(container,\n                CGROUP_PARAM_MEMORY_MEMSW_USAGE_BYTES)) {\n              found \u003d true;\n              break;\n            }\n          }\n        }\n        if (found) {\n          continue;\n        }\n\n        // We have not found any containers that ran out of their limit,\n        // so we will kill the latest one. This can happen, if all use\n        // close to their request and one of them requests a big block\n        // triggering the OOM freeze.\n        // Currently there is no other way to identify the outstanding one.\n        if (containers.size() \u003e 0) {\n          Container container \u003d containers.get(0);\n          sigKill(container);\n          String message \u003d String.format(\n              \"Newest container %s killed by elastic cgroups OOM handler using\",\n              container.getContainerId());\n          LOG.warn(message);\n          continue;\n        }\n\n        // This can happen, if SIGKILL did not clean up\n        // non-PGID or containers or containers launched by other users\n        // or if a process was put to the root YARN cgroup.\n        throw new YarnRuntimeException(\n            \"Could not find any containers but CGroups \" +\n                \"reserved for containers ran out of memory. \" +\n                \"I am giving up\");\n      }\n    } catch (ResourceHandlerException ex) {\n      LOG.warn(\"Could not fecth OOM status. \" +\n          \"This is expected at shutdown. Exiting.\", ex);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/resources/DefaultOOMHandler.java"
    }
  }
}