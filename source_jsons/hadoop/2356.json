{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PBHelperClient.java",
  "functionName": "convertDatanodeInfo",
  "functionId": "convertDatanodeInfo___di-DatanodeInfo",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
  "functionStartLine": 458,
  "functionEndLine": 461,
  "numCommitsSeen": 230,
  "timeTaken": 7625,
  "changeHistory": [
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
    "b4eb963c3c3e0b123003d7b32cdf7c9202cfb441",
    "8bd825bb6f35fd6fef397e3ccae0898bf7bed201",
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
    "b5229fd19bfecc2e5249db652ad34ec08152334b",
    "3001a172c8868763f8e59e866e36f7f50dee62cc",
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e",
    "48da033901d3471ef176a94104158546152353e9",
    "7a59150bff64fc81f838de586eacd6d062172605",
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9"
  ],
  "changeHistoryShort": {
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": "Ymovefromfile",
    "b4eb963c3c3e0b123003d7b32cdf7c9202cfb441": "Ybodychange",
    "8bd825bb6f35fd6fef397e3ccae0898bf7bed201": "Ybodychange",
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c": "Ybodychange",
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256": "Ybodychange",
    "b5229fd19bfecc2e5249db652ad34ec08152334b": "Ybodychange",
    "3001a172c8868763f8e59e866e36f7f50dee62cc": "Ybodychange",
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e": "Ybodychange",
    "48da033901d3471ef176a94104158546152353e9": "Ymultichange(Yrename,Yparameterchange,Yreturntypechange,Ybodychange)",
    "7a59150bff64fc81f838de586eacd6d062172605": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9": "Yintroduced"
  },
  "changeHistoryDetails": {
    "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-8934. Move ShortCircuitShm to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "22/08/15 1:31 PM",
      "commitName": "490bb5ebd6c6d6f9c08fcad167f976687fc3aa42",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "22/08/15 12:39 AM",
      "commitNameOld": "61bf9cae6f3882c6e9a9222f59457b9be91e3018",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 0.54,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n    if (di \u003d\u003d null) return null;\n    return convert(di);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
        "oldMethodName": "convertDatanodeInfo",
        "newMethodName": "convertDatanodeInfo"
      }
    },
    "b4eb963c3c3e0b123003d7b32cdf7c9202cfb441": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5659. dfsadmin -report doesn\u0027t output cache information properly. Contributed by Andrew Wang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1554893 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/01/14 12:11 PM",
      "commitName": "b4eb963c3c3e0b123003d7b32cdf7c9202cfb441",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "20/12/13 3:27 PM",
      "commitNameOld": "b9ae3087c0f83bfeeea47ded8e19932b46fd2350",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 12.86,
      "commitsBetweenForRepo": 28,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,4 @@\n   static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n     if (di \u003d\u003d null) return null;\n-    DatanodeInfoProto.Builder builder \u003d DatanodeInfoProto.newBuilder();\n-    if (di.getNetworkLocation() !\u003d null) {\n-      builder.setLocation(di.getNetworkLocation());\n-    }\n-        \n-    return builder.\n-     setId(PBHelper.convert((DatanodeID) di)).\n-     setCapacity(di.getCapacity()).\n-     setDfsUsed(di.getDfsUsed()).\n-     setRemaining(di.getRemaining()).\n-     setBlockPoolUsed(di.getBlockPoolUsed()).\n-     setLastUpdate(di.getLastUpdate()).\n-     setXceiverCount(di.getXceiverCount()).\n-     setAdminState(PBHelper.convert(di.getAdminState())).\n-     build();     \n+    return convert(di);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n    if (di \u003d\u003d null) return null;\n    return convert(di);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "8bd825bb6f35fd6fef397e3ccae0898bf7bed201": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3164. Move DatanodeInfo#hostName to DatanodeID. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1307890 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/03/12 12:58 PM",
      "commitName": "8bd825bb6f35fd6fef397e3ccae0898bf7bed201",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "17/02/12 5:27 PM",
      "commitNameOld": "ef5d7156dba5fb5f03b3d46e9ecfec5273eca66f",
      "commitAuthorOld": "",
      "daysBetweenCommits": 42.77,
      "commitsBetweenForRepo": 278,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,18 @@\n   static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n     if (di \u003d\u003d null) return null;\n     DatanodeInfoProto.Builder builder \u003d DatanodeInfoProto.newBuilder();\n-    if (di.getHostName() !\u003d null) {\n-      builder.setHostName(di.getHostName());\n-    }\n     if (di.getNetworkLocation() !\u003d null) {\n       builder.setLocation(di.getNetworkLocation());\n     }\n         \n     return builder.\n      setId(PBHelper.convert((DatanodeID) di)).\n      setCapacity(di.getCapacity()).\n      setDfsUsed(di.getDfsUsed()).\n      setRemaining(di.getRemaining()).\n      setBlockPoolUsed(di.getBlockPoolUsed()).\n      setLastUpdate(di.getLastUpdate()).\n      setXceiverCount(di.getXceiverCount()).\n      setAdminState(PBHelper.convert(di.getAdminState())).\n      build();     \n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n    if (di \u003d\u003d null) return null;\n    DatanodeInfoProto.Builder builder \u003d DatanodeInfoProto.newBuilder();\n    if (di.getNetworkLocation() !\u003d null) {\n      builder.setLocation(di.getNetworkLocation());\n    }\n        \n    return builder.\n     setId(PBHelper.convert((DatanodeID) di)).\n     setCapacity(di.getCapacity()).\n     setDfsUsed(di.getDfsUsed()).\n     setRemaining(di.getRemaining()).\n     setBlockPoolUsed(di.getBlockPoolUsed()).\n     setLastUpdate(di.getLastUpdate()).\n     setXceiverCount(di.getXceiverCount()).\n     setAdminState(PBHelper.convert(di.getAdminState())).\n     build();     \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Optional protobuf parameters are not handled correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213985 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:31 PM",
      "commitName": "3954a2fb1cbc7a8a0d1ad5859e7f5c9415530f4c",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:27 PM",
      "commitNameOld": "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,21 @@\n   static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n     if (di \u003d\u003d null) return null;\n-    return DatanodeInfoProto.newBuilder().\n+    DatanodeInfoProto.Builder builder \u003d DatanodeInfoProto.newBuilder();\n+    if (di.getHostName() !\u003d null) {\n+      builder.setHostName(di.getHostName());\n+    }\n+    if (di.getNetworkLocation() !\u003d null) {\n+      builder.setLocation(di.getNetworkLocation());\n+    }\n+        \n+    return builder.\n      setId(PBHelper.convert((DatanodeID) di)).\n-     setLocation(di.getNetworkLocation()).\n-     setHostName(di.getHostName()).\n      setCapacity(di.getCapacity()).\n      setDfsUsed(di.getDfsUsed()).\n      setRemaining(di.getRemaining()).\n      setBlockPoolUsed(di.getBlockPoolUsed()).\n      setLastUpdate(di.getLastUpdate()).\n      setXceiverCount(di.getXceiverCount()).\n      setAdminState(PBHelper.convert(di.getAdminState())).\n      build();     \n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n    if (di \u003d\u003d null) return null;\n    DatanodeInfoProto.Builder builder \u003d DatanodeInfoProto.newBuilder();\n    if (di.getHostName() !\u003d null) {\n      builder.setHostName(di.getHostName());\n    }\n    if (di.getNetworkLocation() !\u003d null) {\n      builder.setLocation(di.getNetworkLocation());\n    }\n        \n    return builder.\n     setId(PBHelper.convert((DatanodeID) di)).\n     setCapacity(di.getCapacity()).\n     setDfsUsed(di.getDfsUsed()).\n     setRemaining(di.getRemaining()).\n     setBlockPoolUsed(di.getBlockPoolUsed()).\n     setLastUpdate(di.getLastUpdate()).\n     setXceiverCount(di.getXceiverCount()).\n     setAdminState(PBHelper.convert(di.getAdminState())).\n     build();     \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "6a609cb471d413b15e3659cc9d7cd6f5f3357256": {
      "type": "Ybodychange",
      "commitMessage": "Reverting the patch r1213981\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213984 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:27 PM",
      "commitName": "6a609cb471d413b15e3659cc9d7cd6f5f3357256",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:22 PM",
      "commitNameOld": "b5229fd19bfecc2e5249db652ad34ec08152334b",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,15 @@\n   static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n     if (di \u003d\u003d null) return null;\n-    DatanodeInfoProto.Builder builder \u003d DatanodeInfoProto.newBuilder();\n-    if (di.getHostName() !\u003d null) {\n-      builder.setHostName(di.getHostName());\n-    }\n-    if (di.getNetworkLocation() !\u003d null) {\n-      builder.setLocation(di.getNetworkLocation());\n-    }\n-        \n-    return builder.\n+    return DatanodeInfoProto.newBuilder().\n      setId(PBHelper.convert((DatanodeID) di)).\n+     setLocation(di.getNetworkLocation()).\n+     setHostName(di.getHostName()).\n      setCapacity(di.getCapacity()).\n      setDfsUsed(di.getDfsUsed()).\n      setRemaining(di.getRemaining()).\n      setBlockPoolUsed(di.getBlockPoolUsed()).\n      setLastUpdate(di.getLastUpdate()).\n      setXceiverCount(di.getXceiverCount()).\n      setAdminState(PBHelper.convert(di.getAdminState())).\n      build();     \n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n    if (di \u003d\u003d null) return null;\n    return DatanodeInfoProto.newBuilder().\n     setId(PBHelper.convert((DatanodeID) di)).\n     setLocation(di.getNetworkLocation()).\n     setHostName(di.getHostName()).\n     setCapacity(di.getCapacity()).\n     setDfsUsed(di.getDfsUsed()).\n     setRemaining(di.getRemaining()).\n     setBlockPoolUsed(di.getBlockPoolUsed()).\n     setLastUpdate(di.getLastUpdate()).\n     setXceiverCount(di.getXceiverCount()).\n     setAdminState(PBHelper.convert(di.getAdminState())).\n     build();     \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "b5229fd19bfecc2e5249db652ad34ec08152334b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Optional protobuf parameters are not handled correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213981 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:22 PM",
      "commitName": "b5229fd19bfecc2e5249db652ad34ec08152334b",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 3:17 PM",
      "commitNameOld": "3001a172c8868763f8e59e866e36f7f50dee62cc",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,21 @@\n   static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n     if (di \u003d\u003d null) return null;\n-    return DatanodeInfoProto.newBuilder().\n+    DatanodeInfoProto.Builder builder \u003d DatanodeInfoProto.newBuilder();\n+    if (di.getHostName() !\u003d null) {\n+      builder.setHostName(di.getHostName());\n+    }\n+    if (di.getNetworkLocation() !\u003d null) {\n+      builder.setLocation(di.getNetworkLocation());\n+    }\n+        \n+    return builder.\n      setId(PBHelper.convert((DatanodeID) di)).\n-     setLocation(di.getNetworkLocation()).\n-     setHostName(di.getHostName()).\n      setCapacity(di.getCapacity()).\n      setDfsUsed(di.getDfsUsed()).\n      setRemaining(di.getRemaining()).\n      setBlockPoolUsed(di.getBlockPoolUsed()).\n      setLastUpdate(di.getLastUpdate()).\n      setXceiverCount(di.getXceiverCount()).\n      setAdminState(PBHelper.convert(di.getAdminState())).\n      build();     \n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n    if (di \u003d\u003d null) return null;\n    DatanodeInfoProto.Builder builder \u003d DatanodeInfoProto.newBuilder();\n    if (di.getHostName() !\u003d null) {\n      builder.setHostName(di.getHostName());\n    }\n    if (di.getNetworkLocation() !\u003d null) {\n      builder.setLocation(di.getNetworkLocation());\n    }\n        \n    return builder.\n     setId(PBHelper.convert((DatanodeID) di)).\n     setCapacity(di.getCapacity()).\n     setDfsUsed(di.getDfsUsed()).\n     setRemaining(di.getRemaining()).\n     setBlockPoolUsed(di.getBlockPoolUsed()).\n     setLastUpdate(di.getLastUpdate()).\n     setXceiverCount(di.getXceiverCount()).\n     setAdminState(PBHelper.convert(di.getAdminState())).\n     build();     \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "3001a172c8868763f8e59e866e36f7f50dee62cc": {
      "type": "Ybodychange",
      "commitMessage": "Reverting r1213512 because it committed changes that were not part of the patch.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213980 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:17 PM",
      "commitName": "3001a172c8868763f8e59e866e36f7f50dee62cc",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "13/12/11 9:59 AM",
      "commitNameOld": "4ec8424e5d8c3f4d802aaacb05cd39d9633eddf8",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.22,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,15 @@\n   static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n     if (di \u003d\u003d null) return null;\n-    DatanodeInfoProto.Builder builder \u003d DatanodeInfoProto.newBuilder();\n-    if (di.getHostName() !\u003d null) {\n-      builder.setHostName(di.getHostName());\n-    }\n-    if (di.getNetworkLocation() !\u003d null) {\n-      builder.setLocation(di.getNetworkLocation());\n-    }\n-        \n-    return builder.\n+    return DatanodeInfoProto.newBuilder().\n      setId(PBHelper.convert((DatanodeID) di)).\n+     setLocation(di.getNetworkLocation()).\n+     setHostName(di.getHostName()).\n      setCapacity(di.getCapacity()).\n      setDfsUsed(di.getDfsUsed()).\n      setRemaining(di.getRemaining()).\n      setBlockPoolUsed(di.getBlockPoolUsed()).\n      setLastUpdate(di.getLastUpdate()).\n      setXceiverCount(di.getXceiverCount()).\n      setAdminState(PBHelper.convert(di.getAdminState())).\n      build();     \n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n    if (di \u003d\u003d null) return null;\n    return DatanodeInfoProto.newBuilder().\n     setId(PBHelper.convert((DatanodeID) di)).\n     setLocation(di.getNetworkLocation()).\n     setHostName(di.getHostName()).\n     setCapacity(di.getCapacity()).\n     setDfsUsed(di.getDfsUsed()).\n     setRemaining(di.getRemaining()).\n     setBlockPoolUsed(di.getBlockPoolUsed()).\n     setLastUpdate(di.getLastUpdate()).\n     setXceiverCount(di.getXceiverCount()).\n     setAdminState(PBHelper.convert(di.getAdminState())).\n     build();     \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2663. Handle protobuf optional parameters correctly. Contributed by Suresh Srinivas.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213512 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/12/11 4:21 PM",
      "commitName": "13345f3a85b6b66c71a38e7c187c8ebb7cb5c35e",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "11/12/11 9:36 PM",
      "commitNameOld": "48da033901d3471ef176a94104158546152353e9",
      "commitAuthorOld": "Sanjay Radia",
      "daysBetweenCommits": 0.78,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,21 @@\n   static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n     if (di \u003d\u003d null) return null;\n-    return DatanodeInfoProto.newBuilder().\n+    DatanodeInfoProto.Builder builder \u003d DatanodeInfoProto.newBuilder();\n+    if (di.getHostName() !\u003d null) {\n+      builder.setHostName(di.getHostName());\n+    }\n+    if (di.getNetworkLocation() !\u003d null) {\n+      builder.setLocation(di.getNetworkLocation());\n+    }\n+        \n+    return builder.\n      setId(PBHelper.convert((DatanodeID) di)).\n-     setLocation(di.getNetworkLocation()).\n-     setHostName(di.getHostName()).\n      setCapacity(di.getCapacity()).\n      setDfsUsed(di.getDfsUsed()).\n      setRemaining(di.getRemaining()).\n      setBlockPoolUsed(di.getBlockPoolUsed()).\n      setLastUpdate(di.getLastUpdate()).\n      setXceiverCount(di.getXceiverCount()).\n      setAdminState(PBHelper.convert(di.getAdminState())).\n      build();     \n   }\n\\ No newline at end of file\n",
      "actualSource": "  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n    if (di \u003d\u003d null) return null;\n    DatanodeInfoProto.Builder builder \u003d DatanodeInfoProto.newBuilder();\n    if (di.getHostName() !\u003d null) {\n      builder.setHostName(di.getHostName());\n    }\n    if (di.getNetworkLocation() !\u003d null) {\n      builder.setLocation(di.getNetworkLocation());\n    }\n        \n    return builder.\n     setId(PBHelper.convert((DatanodeID) di)).\n     setCapacity(di.getCapacity()).\n     setDfsUsed(di.getDfsUsed()).\n     setRemaining(di.getRemaining()).\n     setBlockPoolUsed(di.getBlockPoolUsed()).\n     setLastUpdate(di.getLastUpdate()).\n     setXceiverCount(di.getXceiverCount()).\n     setAdminState(PBHelper.convert(di.getAdminState())).\n     build();     \n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "48da033901d3471ef176a94104158546152353e9": {
      "type": "Ymultichange(Yrename,Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/12/11 9:36 PM",
      "commitName": "48da033901d3471ef176a94104158546152353e9",
      "commitAuthor": "Sanjay Radia",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,15 @@\n-  public static BlockTokenIdentifierProto convert(\n-      Token\u003cBlockTokenIdentifier\u003e token) {\n-    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n-    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n-    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n-        .setKind(token.getKind().toString()).setPassword(password)\n-        .setService(token.getService().toString()).build();\n+  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n+    if (di \u003d\u003d null) return null;\n+    return DatanodeInfoProto.newBuilder().\n+     setId(PBHelper.convert((DatanodeID) di)).\n+     setLocation(di.getNetworkLocation()).\n+     setHostName(di.getHostName()).\n+     setCapacity(di.getCapacity()).\n+     setDfsUsed(di.getDfsUsed()).\n+     setRemaining(di.getRemaining()).\n+     setBlockPoolUsed(di.getBlockPoolUsed()).\n+     setLastUpdate(di.getLastUpdate()).\n+     setXceiverCount(di.getXceiverCount()).\n+     setAdminState(PBHelper.convert(di.getAdminState())).\n+     build();     \n   }\n\\ No newline at end of file\n",
          "actualSource": "  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n    if (di \u003d\u003d null) return null;\n    return DatanodeInfoProto.newBuilder().\n     setId(PBHelper.convert((DatanodeID) di)).\n     setLocation(di.getNetworkLocation()).\n     setHostName(di.getHostName()).\n     setCapacity(di.getCapacity()).\n     setDfsUsed(di.getDfsUsed()).\n     setRemaining(di.getRemaining()).\n     setBlockPoolUsed(di.getBlockPoolUsed()).\n     setLastUpdate(di.getLastUpdate()).\n     setXceiverCount(di.getXceiverCount()).\n     setAdminState(PBHelper.convert(di.getAdminState())).\n     build();     \n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "convert",
            "newValue": "convertDatanodeInfo"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,15 @@\n-  public static BlockTokenIdentifierProto convert(\n-      Token\u003cBlockTokenIdentifier\u003e token) {\n-    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n-    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n-    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n-        .setKind(token.getKind().toString()).setPassword(password)\n-        .setService(token.getService().toString()).build();\n+  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n+    if (di \u003d\u003d null) return null;\n+    return DatanodeInfoProto.newBuilder().\n+     setId(PBHelper.convert((DatanodeID) di)).\n+     setLocation(di.getNetworkLocation()).\n+     setHostName(di.getHostName()).\n+     setCapacity(di.getCapacity()).\n+     setDfsUsed(di.getDfsUsed()).\n+     setRemaining(di.getRemaining()).\n+     setBlockPoolUsed(di.getBlockPoolUsed()).\n+     setLastUpdate(di.getLastUpdate()).\n+     setXceiverCount(di.getXceiverCount()).\n+     setAdminState(PBHelper.convert(di.getAdminState())).\n+     build();     \n   }\n\\ No newline at end of file\n",
          "actualSource": "  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n    if (di \u003d\u003d null) return null;\n    return DatanodeInfoProto.newBuilder().\n     setId(PBHelper.convert((DatanodeID) di)).\n     setLocation(di.getNetworkLocation()).\n     setHostName(di.getHostName()).\n     setCapacity(di.getCapacity()).\n     setDfsUsed(di.getDfsUsed()).\n     setRemaining(di.getRemaining()).\n     setBlockPoolUsed(di.getBlockPoolUsed()).\n     setLastUpdate(di.getLastUpdate()).\n     setXceiverCount(di.getXceiverCount()).\n     setAdminState(PBHelper.convert(di.getAdminState())).\n     build();     \n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "[token-Token\u003cBlockTokenIdentifier\u003e]",
            "newValue": "[di-DatanodeInfo]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,15 @@\n-  public static BlockTokenIdentifierProto convert(\n-      Token\u003cBlockTokenIdentifier\u003e token) {\n-    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n-    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n-    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n-        .setKind(token.getKind().toString()).setPassword(password)\n-        .setService(token.getService().toString()).build();\n+  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n+    if (di \u003d\u003d null) return null;\n+    return DatanodeInfoProto.newBuilder().\n+     setId(PBHelper.convert((DatanodeID) di)).\n+     setLocation(di.getNetworkLocation()).\n+     setHostName(di.getHostName()).\n+     setCapacity(di.getCapacity()).\n+     setDfsUsed(di.getDfsUsed()).\n+     setRemaining(di.getRemaining()).\n+     setBlockPoolUsed(di.getBlockPoolUsed()).\n+     setLastUpdate(di.getLastUpdate()).\n+     setXceiverCount(di.getXceiverCount()).\n+     setAdminState(PBHelper.convert(di.getAdminState())).\n+     build();     \n   }\n\\ No newline at end of file\n",
          "actualSource": "  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n    if (di \u003d\u003d null) return null;\n    return DatanodeInfoProto.newBuilder().\n     setId(PBHelper.convert((DatanodeID) di)).\n     setLocation(di.getNetworkLocation()).\n     setHostName(di.getHostName()).\n     setCapacity(di.getCapacity()).\n     setDfsUsed(di.getDfsUsed()).\n     setRemaining(di.getRemaining()).\n     setBlockPoolUsed(di.getBlockPoolUsed()).\n     setLastUpdate(di.getLastUpdate()).\n     setXceiverCount(di.getXceiverCount()).\n     setAdminState(PBHelper.convert(di.getAdminState())).\n     build();     \n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "BlockTokenIdentifierProto",
            "newValue": "DatanodeInfoProto"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,15 @@\n-  public static BlockTokenIdentifierProto convert(\n-      Token\u003cBlockTokenIdentifier\u003e token) {\n-    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n-    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n-    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n-        .setKind(token.getKind().toString()).setPassword(password)\n-        .setService(token.getService().toString()).build();\n+  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n+    if (di \u003d\u003d null) return null;\n+    return DatanodeInfoProto.newBuilder().\n+     setId(PBHelper.convert((DatanodeID) di)).\n+     setLocation(di.getNetworkLocation()).\n+     setHostName(di.getHostName()).\n+     setCapacity(di.getCapacity()).\n+     setDfsUsed(di.getDfsUsed()).\n+     setRemaining(di.getRemaining()).\n+     setBlockPoolUsed(di.getBlockPoolUsed()).\n+     setLastUpdate(di.getLastUpdate()).\n+     setXceiverCount(di.getXceiverCount()).\n+     setAdminState(PBHelper.convert(di.getAdminState())).\n+     build();     \n   }\n\\ No newline at end of file\n",
          "actualSource": "  static public DatanodeInfoProto convertDatanodeInfo(DatanodeInfo di) {\n    if (di \u003d\u003d null) return null;\n    return DatanodeInfoProto.newBuilder().\n     setId(PBHelper.convert((DatanodeID) di)).\n     setLocation(di.getNetworkLocation()).\n     setHostName(di.getHostName()).\n     setCapacity(di.getCapacity()).\n     setDfsUsed(di.getDfsUsed()).\n     setRemaining(di.getRemaining()).\n     setBlockPoolUsed(di.getBlockPoolUsed()).\n     setLastUpdate(di.getLastUpdate()).\n     setXceiverCount(di.getXceiverCount()).\n     setAdminState(PBHelper.convert(di.getAdminState())).\n     build();     \n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {}
        }
      ]
    },
    "7a59150bff64fc81f838de586eacd6d062172605": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-2629. Implement protobuf service for InterDatanodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1211206 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/12/11 2:19 PM",
      "commitName": "7a59150bff64fc81f838de586eacd6d062172605",
      "commitAuthor": "Suresh Srinivas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2629. Implement protobuf service for InterDatanodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1211206 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/11 2:19 PM",
          "commitName": "7a59150bff64fc81f838de586eacd6d062172605",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "05/12/11 4:25 PM",
          "commitNameOld": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.91,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,8 @@\n-  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n-    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n-    int i \u003d 0;\n-    for (BlockWithLocationsProto entry : b) {\n-      ret[i++] \u003d convert(entry);\n-    }\n-    return ret;\n+  public static BlockTokenIdentifierProto convert(\n+      Token\u003cBlockTokenIdentifier\u003e token) {\n+    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n+    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n+    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n+        .setKind(token.getKind().toString()).setPassword(password)\n+        .setService(token.getService().toString()).build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockTokenIdentifierProto convert(\n      Token\u003cBlockTokenIdentifier\u003e token) {\n    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n        .setKind(token.getKind().toString()).setPassword(password)\n        .setService(token.getService().toString()).build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "[b-List\u003cBlockWithLocationsProto\u003e]",
            "newValue": "[token-Token\u003cBlockTokenIdentifier\u003e]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-2629. Implement protobuf service for InterDatanodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1211206 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/11 2:19 PM",
          "commitName": "7a59150bff64fc81f838de586eacd6d062172605",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "05/12/11 4:25 PM",
          "commitNameOld": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.91,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,8 @@\n-  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n-    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n-    int i \u003d 0;\n-    for (BlockWithLocationsProto entry : b) {\n-      ret[i++] \u003d convert(entry);\n-    }\n-    return ret;\n+  public static BlockTokenIdentifierProto convert(\n+      Token\u003cBlockTokenIdentifier\u003e token) {\n+    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n+    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n+    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n+        .setKind(token.getKind().toString()).setPassword(password)\n+        .setService(token.getService().toString()).build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockTokenIdentifierProto convert(\n      Token\u003cBlockTokenIdentifier\u003e token) {\n    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n        .setKind(token.getKind().toString()).setPassword(password)\n        .setService(token.getService().toString()).build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "BlockWithLocations[]",
            "newValue": "BlockTokenIdentifierProto"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2629. Implement protobuf service for InterDatanodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1211206 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "06/12/11 2:19 PM",
          "commitName": "7a59150bff64fc81f838de586eacd6d062172605",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "05/12/11 4:25 PM",
          "commitNameOld": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.91,
          "commitsBetweenForRepo": 5,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,8 +1,8 @@\n-  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n-    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n-    int i \u003d 0;\n-    for (BlockWithLocationsProto entry : b) {\n-      ret[i++] \u003d convert(entry);\n-    }\n-    return ret;\n+  public static BlockTokenIdentifierProto convert(\n+      Token\u003cBlockTokenIdentifier\u003e token) {\n+    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n+    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n+    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n+        .setKind(token.getKind().toString()).setPassword(password)\n+        .setService(token.getService().toString()).build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static BlockTokenIdentifierProto convert(\n      Token\u003cBlockTokenIdentifier\u003e token) {\n    ByteString tokenId \u003d ByteString.copyFrom(token.getIdentifier());\n    ByteString password \u003d ByteString.copyFrom(token.getPassword());\n    return BlockTokenIdentifierProto.newBuilder().setIdentifier(tokenId)\n        .setKind(token.getKind().toString()).setPassword(password)\n        .setService(token.getService().toString()).build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {}
        }
      ]
    },
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2618. Implement protobuf service for NamenodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1210719 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/12/11 4:25 PM",
      "commitName": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
      "commitAuthor": "Suresh Srinivas",
      "diff": "@@ -0,0 +1,8 @@\n+  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n+    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n+    int i \u003d 0;\n+    for (BlockWithLocationsProto entry : b) {\n+      ret[i++] \u003d convert(entry);\n+    }\n+    return ret;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static BlockWithLocations[] convert(List\u003cBlockWithLocationsProto\u003e b) {\n    BlockWithLocations[] ret \u003d new BlockWithLocations[b.size()];\n    int i \u003d 0;\n    for (BlockWithLocationsProto entry : b) {\n      ret[i++] \u003d convert(entry);\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java"
    }
  }
}