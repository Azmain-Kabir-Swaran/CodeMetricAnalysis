{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PBHelperClient.java",
  "functionName": "convertCacheFlags",
  "functionId": "convertCacheFlags___flags-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
  "functionStartLine": 2290,
  "functionEndLine": 2296,
  "numCommitsSeen": 308,
  "timeTaken": 8664,
  "changeHistory": [
    "06022b8fdc40e50eaac63758246353058e8cfa6d",
    "991c453ca3ac141a3f286f74af8401f83c38b230",
    "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e",
    "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e",
    "48da033901d3471ef176a94104158546152353e9",
    "38a19bc293dec6221ae96e304fc6ab660d94e706",
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9"
  ],
  "changeHistoryShort": {
    "06022b8fdc40e50eaac63758246353058e8cfa6d": "Ymovefromfile",
    "991c453ca3ac141a3f286f74af8401f83c38b230": "Ymultichange(Yrename,Yparameterchange,Yreturntypechange,Ybodychange)",
    "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e": "Ybodychange",
    "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e": "Ybodychange",
    "48da033901d3471ef176a94104158546152353e9": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
    "38a19bc293dec6221ae96e304fc6ab660d94e706": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yparameterchange)",
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9": "Yintroduced"
  },
  "changeHistoryDetails": {
    "06022b8fdc40e50eaac63758246353058e8cfa6d": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-9111. Move hdfs-client protobuf convert methods from PBHelper to PBHelperClient. Contributed by Mingliang Liu.\n",
      "commitDate": "21/09/15 6:53 PM",
      "commitName": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "21/09/15 5:51 PM",
      "commitNameOld": "8e01b0d97ac3d74b049a801dfa1cc6e77d8f680a",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static EnumSet\u003cCacheFlag\u003e convertCacheFlags(int flags) {\n    EnumSet\u003cCacheFlag\u003e result \u003d EnumSet.noneOf(CacheFlag.class);\n    if ((flags \u0026 CacheFlagProto.FORCE_VALUE) \u003d\u003d CacheFlagProto.FORCE_VALUE) {\n      result.add(CacheFlag.FORCE);\n    }\n    return result;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
        "oldMethodName": "convertCacheFlags",
        "newMethodName": "convertCacheFlags"
      }
    },
    "991c453ca3ac141a3f286f74af8401f83c38b230": {
      "type": "Ymultichange(Yrename,Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-5431. Support cachepool-based limit management in path-based caching. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551651 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 10:47 AM",
      "commitName": "991c453ca3ac141a3f286f74af8401f83c38b230",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-5431. Support cachepool-based limit management in path-based caching. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551651 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/12/13 10:47 AM",
          "commitName": "991c453ca3ac141a3f286f74af8401f83c38b230",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "05/12/13 3:41 PM",
          "commitNameOld": "00718c2ffaa11cbdabac6f5ef4b2de5dcf9d6859",
          "commitAuthorOld": "",
          "daysBetweenCommits": 11.8,
          "commitsBetweenForRepo": 65,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,7 @@\n-  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n-    EnumSet\u003cCreateFlag\u003e result \u003d \n-       EnumSet.noneOf(CreateFlag.class);   \n-    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n-      result.add(CreateFlag.APPEND);\n+  public static EnumSet\u003cCacheFlag\u003e convertCacheFlags(int flags) {\n+    EnumSet\u003cCacheFlag\u003e result \u003d EnumSet.noneOf(CacheFlag.class);\n+    if ((flags \u0026 CacheFlagProto.FORCE_VALUE) \u003d\u003d CacheFlagProto.FORCE_VALUE) {\n+      result.add(CacheFlag.FORCE);\n     }\n-    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n-      result.add(CreateFlag.CREATE);\n-    }\n-    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n-        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n-      result.add(CreateFlag.OVERWRITE);\n-    }\n-    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n+    return result;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static EnumSet\u003cCacheFlag\u003e convertCacheFlags(int flags) {\n    EnumSet\u003cCacheFlag\u003e result \u003d EnumSet.noneOf(CacheFlag.class);\n    if ((flags \u0026 CacheFlagProto.FORCE_VALUE) \u003d\u003d CacheFlagProto.FORCE_VALUE) {\n      result.add(CacheFlag.FORCE);\n    }\n    return result;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "convert",
            "newValue": "convertCacheFlags"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5431. Support cachepool-based limit management in path-based caching. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551651 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/12/13 10:47 AM",
          "commitName": "991c453ca3ac141a3f286f74af8401f83c38b230",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "05/12/13 3:41 PM",
          "commitNameOld": "00718c2ffaa11cbdabac6f5ef4b2de5dcf9d6859",
          "commitAuthorOld": "",
          "daysBetweenCommits": 11.8,
          "commitsBetweenForRepo": 65,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,7 @@\n-  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n-    EnumSet\u003cCreateFlag\u003e result \u003d \n-       EnumSet.noneOf(CreateFlag.class);   \n-    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n-      result.add(CreateFlag.APPEND);\n+  public static EnumSet\u003cCacheFlag\u003e convertCacheFlags(int flags) {\n+    EnumSet\u003cCacheFlag\u003e result \u003d EnumSet.noneOf(CacheFlag.class);\n+    if ((flags \u0026 CacheFlagProto.FORCE_VALUE) \u003d\u003d CacheFlagProto.FORCE_VALUE) {\n+      result.add(CacheFlag.FORCE);\n     }\n-    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n-      result.add(CreateFlag.CREATE);\n-    }\n-    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n-        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n-      result.add(CreateFlag.OVERWRITE);\n-    }\n-    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n+    return result;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static EnumSet\u003cCacheFlag\u003e convertCacheFlags(int flags) {\n    EnumSet\u003cCacheFlag\u003e result \u003d EnumSet.noneOf(CacheFlag.class);\n    if ((flags \u0026 CacheFlagProto.FORCE_VALUE) \u003d\u003d CacheFlagProto.FORCE_VALUE) {\n      result.add(CacheFlag.FORCE);\n    }\n    return result;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "[flag-int]",
            "newValue": "[flags-int]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-5431. Support cachepool-based limit management in path-based caching. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551651 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/12/13 10:47 AM",
          "commitName": "991c453ca3ac141a3f286f74af8401f83c38b230",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "05/12/13 3:41 PM",
          "commitNameOld": "00718c2ffaa11cbdabac6f5ef4b2de5dcf9d6859",
          "commitAuthorOld": "",
          "daysBetweenCommits": 11.8,
          "commitsBetweenForRepo": 65,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,7 @@\n-  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n-    EnumSet\u003cCreateFlag\u003e result \u003d \n-       EnumSet.noneOf(CreateFlag.class);   \n-    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n-      result.add(CreateFlag.APPEND);\n+  public static EnumSet\u003cCacheFlag\u003e convertCacheFlags(int flags) {\n+    EnumSet\u003cCacheFlag\u003e result \u003d EnumSet.noneOf(CacheFlag.class);\n+    if ((flags \u0026 CacheFlagProto.FORCE_VALUE) \u003d\u003d CacheFlagProto.FORCE_VALUE) {\n+      result.add(CacheFlag.FORCE);\n     }\n-    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n-      result.add(CreateFlag.CREATE);\n-    }\n-    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n-        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n-      result.add(CreateFlag.OVERWRITE);\n-    }\n-    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n+    return result;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static EnumSet\u003cCacheFlag\u003e convertCacheFlags(int flags) {\n    EnumSet\u003cCacheFlag\u003e result \u003d EnumSet.noneOf(CacheFlag.class);\n    if ((flags \u0026 CacheFlagProto.FORCE_VALUE) \u003d\u003d CacheFlagProto.FORCE_VALUE) {\n      result.add(CacheFlag.FORCE);\n    }\n    return result;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "EnumSetWritable\u003cCreateFlag\u003e",
            "newValue": "EnumSet\u003cCacheFlag\u003e"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5431. Support cachepool-based limit management in path-based caching. (awang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551651 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/12/13 10:47 AM",
          "commitName": "991c453ca3ac141a3f286f74af8401f83c38b230",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "05/12/13 3:41 PM",
          "commitNameOld": "00718c2ffaa11cbdabac6f5ef4b2de5dcf9d6859",
          "commitAuthorOld": "",
          "daysBetweenCommits": 11.8,
          "commitsBetweenForRepo": 65,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,15 +1,7 @@\n-  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n-    EnumSet\u003cCreateFlag\u003e result \u003d \n-       EnumSet.noneOf(CreateFlag.class);   \n-    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n-      result.add(CreateFlag.APPEND);\n+  public static EnumSet\u003cCacheFlag\u003e convertCacheFlags(int flags) {\n+    EnumSet\u003cCacheFlag\u003e result \u003d EnumSet.noneOf(CacheFlag.class);\n+    if ((flags \u0026 CacheFlagProto.FORCE_VALUE) \u003d\u003d CacheFlagProto.FORCE_VALUE) {\n+      result.add(CacheFlag.FORCE);\n     }\n-    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n-      result.add(CreateFlag.CREATE);\n-    }\n-    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n-        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n-      result.add(CreateFlag.OVERWRITE);\n-    }\n-    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n+    return result;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static EnumSet\u003cCacheFlag\u003e convertCacheFlags(int flags) {\n    EnumSet\u003cCacheFlag\u003e result \u003d EnumSet.noneOf(CacheFlag.class);\n    if ((flags \u0026 CacheFlagProto.FORCE_VALUE) \u003d\u003d CacheFlagProto.FORCE_VALUE) {\n      result.add(CacheFlag.FORCE);\n    }\n    return result;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {}
        }
      ]
    },
    "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e": {
      "type": "Ybodychange",
      "commitMessage": "Merge trunk into HA branch.\n\nSeveral conflicts around introduction of protobuf translator for DatanodeProtocol - mostly trivial resolutions.\n\nNB: this does not successfully pass any tests since the HAStatus field needs\nto be integrated into the HeartbeatResponse Protobuf implementation.\nThat will be a separate commit for clearer history.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1214518 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/12/11 2:47 PM",
      "commitName": "8134b1c8702d7d6b3994c73b34afc7f8ee33ac6e",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "13/12/11 11:02 AM",
      "commitNameOld": "a0fe4f476ae907c9c070af48a250739a4fb33362",
      "commitAuthorOld": "",
      "daysBetweenCommits": 1.16,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,15 @@\n   public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n     EnumSet\u003cCreateFlag\u003e result \u003d \n        EnumSet.noneOf(CreateFlag.class);   \n     if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n       result.add(CreateFlag.APPEND);\n     }\n+    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n+      result.add(CreateFlag.CREATE);\n+    }\n+    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n+        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n+      result.add(CreateFlag.OVERWRITE);\n+    }\n     return new EnumSetWritable\u003cCreateFlag\u003e(result);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d \n       EnumSet.noneOf(CreateFlag.class);   \n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n      result.add(CreateFlag.CREATE);\n    }\n    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n      result.add(CreateFlag.OVERWRITE);\n    }\n    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e": {
      "type": "Ybodychange",
      "commitMessage": "    HDFS-2669 Enable protobuf rpc for ClientNamenodeProtocol\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1214128 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/12/11 1:27 AM",
      "commitName": "d8dfcdcbc2e2df3aa1d7b309f263434739475e7e",
      "commitAuthor": "Sanjay Radia",
      "commitDateOld": "13/12/11 6:15 PM",
      "commitNameOld": "3cffe34177c72ea67194c3b0aaf0ddbf67ff3a0c",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 0.3,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,15 @@\n   public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n     EnumSet\u003cCreateFlag\u003e result \u003d \n        EnumSet.noneOf(CreateFlag.class);   \n     if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n       result.add(CreateFlag.APPEND);\n     }\n+    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n+      result.add(CreateFlag.CREATE);\n+    }\n+    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n+        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n+      result.add(CreateFlag.OVERWRITE);\n+    }\n     return new EnumSetWritable\u003cCreateFlag\u003e(result);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d \n       EnumSet.noneOf(CreateFlag.class);   \n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    if ((flag \u0026 CreateFlagProto.CREATE_VALUE) \u003d\u003d CreateFlagProto.CREATE_VALUE) {\n      result.add(CreateFlag.CREATE);\n    }\n    if ((flag \u0026 CreateFlagProto.OVERWRITE_VALUE) \n        \u003d\u003d CreateFlagProto.OVERWRITE_VALUE) {\n      result.add(CreateFlag.OVERWRITE);\n    }\n    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
      "extendedDetails": {}
    },
    "48da033901d3471ef176a94104158546152353e9": {
      "type": "Ymultichange(Yparameterchange,Yreturntypechange,Ybodychange)",
      "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/12/11 9:36 PM",
      "commitName": "48da033901d3471ef176a94104158546152353e9",
      "commitAuthor": "Sanjay Radia",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,8 @@\n-  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n-    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n-    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n-      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n+    EnumSet\u003cCreateFlag\u003e result \u003d \n+       EnumSet.noneOf(CreateFlag.class);   \n+    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n+      result.add(CreateFlag.APPEND);\n     }\n-    return builder.build();\n+    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d \n       EnumSet.noneOf(CreateFlag.class);   \n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "[datanodeInfos-DatanodeInfo[]]",
            "newValue": "[flag-int]"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,8 @@\n-  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n-    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n-    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n-      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n+    EnumSet\u003cCreateFlag\u003e result \u003d \n+       EnumSet.noneOf(CreateFlag.class);   \n+    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n+      result.add(CreateFlag.APPEND);\n     }\n-    return builder.build();\n+    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d \n       EnumSet.noneOf(CreateFlag.class);   \n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "DatanodeInfosProto",
            "newValue": "EnumSetWritable\u003cCreateFlag\u003e"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "    HDFS-2651 ClientNameNodeProtocol Translators for Protocol Buffers (sanjay)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213143 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "11/12/11 9:36 PM",
          "commitName": "48da033901d3471ef176a94104158546152353e9",
          "commitAuthor": "Sanjay Radia",
          "commitDateOld": "11/12/11 10:53 AM",
          "commitNameOld": "2740112bb64e1cc8132a1dc450d9e461c2e4729e",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,8 @@\n-  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n-    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n-    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n-      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n+    EnumSet\u003cCreateFlag\u003e result \u003d \n+       EnumSet.noneOf(CreateFlag.class);   \n+    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n+      result.add(CreateFlag.APPEND);\n     }\n-    return builder.build();\n+    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static EnumSetWritable\u003cCreateFlag\u003e convert(int flag) {\n    EnumSet\u003cCreateFlag\u003e result \u003d \n       EnumSet.noneOf(CreateFlag.class);   \n    if ((flag \u0026 CreateFlagProto.APPEND_VALUE) \u003d\u003d CreateFlagProto.APPEND_VALUE) {\n      result.add(CreateFlag.APPEND);\n    }\n    return new EnumSetWritable\u003cCreateFlag\u003e(result);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {}
        }
      ]
    },
    "38a19bc293dec6221ae96e304fc6ab660d94e706": {
      "type": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/12/11 12:02 PM",
      "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
      "commitAuthor": "Jitendra Nath Pandey",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/12/11 12:02 PM",
          "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
          "commitAuthor": "Jitendra Nath Pandey",
          "commitDateOld": "09/12/11 11:28 AM",
          "commitNameOld": "66fce20802653ebed492024b01441644e028036a",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,7 @@\n-  private NamespaceInfoProto convert(NamespaceInfo info) {\n-    return NamespaceInfoProto.newBuilder()\n-        .setBlockPoolID(info.getBlockPoolID())\n-        .setBuildVersion(info.getBuildVersion())\n-        .setDistUpgradeVersion(info.getDistributedUpgradeVersion())\n-        .setStorageInfo(PBHelper.convert(info)).build();\n+  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n+    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n+    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n+      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n    }\n    return builder.build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolServerSideTranslatorPB.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
            "oldMethodName": "convert",
            "newMethodName": "convert"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/12/11 12:02 PM",
          "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
          "commitAuthor": "Jitendra Nath Pandey",
          "commitDateOld": "09/12/11 11:28 AM",
          "commitNameOld": "66fce20802653ebed492024b01441644e028036a",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,7 @@\n-  private NamespaceInfoProto convert(NamespaceInfo info) {\n-    return NamespaceInfoProto.newBuilder()\n-        .setBlockPoolID(info.getBlockPoolID())\n-        .setBuildVersion(info.getBuildVersion())\n-        .setDistUpgradeVersion(info.getDistributedUpgradeVersion())\n-        .setStorageInfo(PBHelper.convert(info)).build();\n+  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n+    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n+    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n+      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n    }\n    return builder.build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "NamespaceInfoProto",
            "newValue": "DatanodeInfosProto"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/12/11 12:02 PM",
          "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
          "commitAuthor": "Jitendra Nath Pandey",
          "commitDateOld": "09/12/11 11:28 AM",
          "commitNameOld": "66fce20802653ebed492024b01441644e028036a",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,7 @@\n-  private NamespaceInfoProto convert(NamespaceInfo info) {\n-    return NamespaceInfoProto.newBuilder()\n-        .setBlockPoolID(info.getBlockPoolID())\n-        .setBuildVersion(info.getBuildVersion())\n-        .setDistUpgradeVersion(info.getDistributedUpgradeVersion())\n-        .setStorageInfo(PBHelper.convert(info)).build();\n+  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n+    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n+    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n+      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n    }\n    return builder.build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[public, static]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/12/11 12:02 PM",
          "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
          "commitAuthor": "Jitendra Nath Pandey",
          "commitDateOld": "09/12/11 11:28 AM",
          "commitNameOld": "66fce20802653ebed492024b01441644e028036a",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,7 @@\n-  private NamespaceInfoProto convert(NamespaceInfo info) {\n-    return NamespaceInfoProto.newBuilder()\n-        .setBlockPoolID(info.getBlockPoolID())\n-        .setBuildVersion(info.getBuildVersion())\n-        .setDistUpgradeVersion(info.getDistributedUpgradeVersion())\n-        .setStorageInfo(PBHelper.convert(info)).build();\n+  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n+    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n+    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n+      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n    }\n    return builder.build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-2642. Protobuf translators for DatanodeProtocol.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1212606 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "09/12/11 12:02 PM",
          "commitName": "38a19bc293dec6221ae96e304fc6ab660d94e706",
          "commitAuthor": "Jitendra Nath Pandey",
          "commitDateOld": "09/12/11 11:28 AM",
          "commitNameOld": "66fce20802653ebed492024b01441644e028036a",
          "commitAuthorOld": "Alejandro Abdelnur",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,7 +1,7 @@\n-  private NamespaceInfoProto convert(NamespaceInfo info) {\n-    return NamespaceInfoProto.newBuilder()\n-        .setBlockPoolID(info.getBlockPoolID())\n-        .setBuildVersion(info.getBuildVersion())\n-        .setDistUpgradeVersion(info.getDistributedUpgradeVersion())\n-        .setStorageInfo(PBHelper.convert(info)).build();\n+  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n+    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n+    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n+      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n+    }\n+    return builder.build();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public static DatanodeInfosProto convert(DatanodeInfo[] datanodeInfos) {\n    DatanodeInfosProto.Builder builder \u003d DatanodeInfosProto.newBuilder();\n    for (int i \u003d 0; i \u003c datanodeInfos.length; i++) {\n      builder.addDatanodes(PBHelper.convert(datanodeInfos[i]));\n    }\n    return builder.build();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java",
          "extendedDetails": {
            "oldValue": "[info-NamespaceInfo]",
            "newValue": "[datanodeInfos-DatanodeInfo[]]"
          }
        }
      ]
    },
    "0a713035f2fb1a222291cfdb2cbde906814c2fd9": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2618. Implement protobuf service for NamenodeProtocol. Contributed by Suresh Srinivas.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1210719 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/12/11 4:25 PM",
      "commitName": "0a713035f2fb1a222291cfdb2cbde906814c2fd9",
      "commitAuthor": "Suresh Srinivas",
      "diff": "@@ -0,0 +1,7 @@\n+  private NamespaceInfoProto convert(NamespaceInfo info) {\n+    return NamespaceInfoProto.newBuilder()\n+        .setBlockPoolID(info.getBlockPoolID())\n+        .setBuildVersion(info.getBuildVersion())\n+        .setDistUpgradeVersion(info.getDistributedUpgradeVersion())\n+        .setStorageInfo(PBHelper.convert(info)).build();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private NamespaceInfoProto convert(NamespaceInfo info) {\n    return NamespaceInfoProto.newBuilder()\n        .setBlockPoolID(info.getBlockPoolID())\n        .setBuildVersion(info.getBuildVersion())\n        .setDistUpgradeVersion(info.getDistributedUpgradeVersion())\n        .setStorageInfo(PBHelper.convert(info)).build();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/NamenodeProtocolServerSideTranslatorPB.java"
    }
  }
}