{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RouterRpcServer.java",
  "functionName": "getDatanodeStorageReportMap",
  "functionId": "getDatanodeStorageReportMap___type-DatanodeReportType",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java",
  "functionStartLine": 928,
  "functionEndLine": 946,
  "numCommitsSeen": 75,
  "timeTaken": 3034,
  "changeHistory": [
    "6e2b5fa493ff8e8c2bb28e6f6f4c19347bc9b99d",
    "e71bc00a471422ddb26dd54e706f09f0fe09925c"
  ],
  "changeHistoryShort": {
    "6e2b5fa493ff8e8c2bb28e6f6f4c19347bc9b99d": "Yfilerename",
    "e71bc00a471422ddb26dd54e706f09f0fe09925c": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6e2b5fa493ff8e8c2bb28e6f6f4c19347bc9b99d": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-13215. RBF: Move Router to its own module. Contributed by Wei Yan\n",
      "commitDate": "19/03/18 10:13 PM",
      "commitName": "6e2b5fa493ff8e8c2bb28e6f6f4c19347bc9b99d",
      "commitAuthor": "weiy",
      "commitDateOld": "19/03/18 5:19 PM",
      "commitNameOld": "e65ff1c8be48ef4f04ed96f96ac4caef4974944d",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 0.2,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public Map\u003cString, DatanodeStorageReport[]\u003e getDatanodeStorageReportMap(\n      DatanodeReportType type) throws IOException {\n\n    Map\u003cString, DatanodeStorageReport[]\u003e ret \u003d new LinkedHashMap\u003c\u003e();\n    RemoteMethod method \u003d new RemoteMethod(\"getDatanodeStorageReport\",\n        new Class\u003c?\u003e[] {DatanodeReportType.class}, type);\n    Set\u003cFederationNamespaceInfo\u003e nss \u003d namenodeResolver.getNamespaces();\n    Map\u003cFederationNamespaceInfo, DatanodeStorageReport[]\u003e results \u003d\n        rpcClient.invokeConcurrent(\n            nss, method, true, false, DatanodeStorageReport[].class);\n    for (Entry\u003cFederationNamespaceInfo, DatanodeStorageReport[]\u003e entry :\n        results.entrySet()) {\n      FederationNamespaceInfo ns \u003d entry.getKey();\n      String nsId \u003d ns.getNameserviceId();\n      DatanodeStorageReport[] result \u003d entry.getValue();\n      ret.put(nsId, result);\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java"
      }
    },
    "e71bc00a471422ddb26dd54e706f09f0fe09925c": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13224. RBF: Resolvers to support mount points across multiple subclusters. Contributed by Inigo Goiri.\n",
      "commitDate": "15/03/18 10:32 AM",
      "commitName": "e71bc00a471422ddb26dd54e706f09f0fe09925c",
      "commitAuthor": "Inigo Goiri",
      "diff": "@@ -0,0 +1,19 @@\n+  public Map\u003cString, DatanodeStorageReport[]\u003e getDatanodeStorageReportMap(\n+      DatanodeReportType type) throws IOException {\n+\n+    Map\u003cString, DatanodeStorageReport[]\u003e ret \u003d new LinkedHashMap\u003c\u003e();\n+    RemoteMethod method \u003d new RemoteMethod(\"getDatanodeStorageReport\",\n+        new Class\u003c?\u003e[] {DatanodeReportType.class}, type);\n+    Set\u003cFederationNamespaceInfo\u003e nss \u003d namenodeResolver.getNamespaces();\n+    Map\u003cFederationNamespaceInfo, DatanodeStorageReport[]\u003e results \u003d\n+        rpcClient.invokeConcurrent(\n+            nss, method, true, false, DatanodeStorageReport[].class);\n+    for (Entry\u003cFederationNamespaceInfo, DatanodeStorageReport[]\u003e entry :\n+        results.entrySet()) {\n+      FederationNamespaceInfo ns \u003d entry.getKey();\n+      String nsId \u003d ns.getNameserviceId();\n+      DatanodeStorageReport[] result \u003d entry.getValue();\n+      ret.put(nsId, result);\n+    }\n+    return ret;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cString, DatanodeStorageReport[]\u003e getDatanodeStorageReportMap(\n      DatanodeReportType type) throws IOException {\n\n    Map\u003cString, DatanodeStorageReport[]\u003e ret \u003d new LinkedHashMap\u003c\u003e();\n    RemoteMethod method \u003d new RemoteMethod(\"getDatanodeStorageReport\",\n        new Class\u003c?\u003e[] {DatanodeReportType.class}, type);\n    Set\u003cFederationNamespaceInfo\u003e nss \u003d namenodeResolver.getNamespaces();\n    Map\u003cFederationNamespaceInfo, DatanodeStorageReport[]\u003e results \u003d\n        rpcClient.invokeConcurrent(\n            nss, method, true, false, DatanodeStorageReport[].class);\n    for (Entry\u003cFederationNamespaceInfo, DatanodeStorageReport[]\u003e entry :\n        results.entrySet()) {\n      FederationNamespaceInfo ns \u003d entry.getKey();\n      String nsId \u003d ns.getNameserviceId();\n      DatanodeStorageReport[] result \u003d entry.getValue();\n      ret.put(nsId, result);\n    }\n    return ret;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java"
    }
  }
}