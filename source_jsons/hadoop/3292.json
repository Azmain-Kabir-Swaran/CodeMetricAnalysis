{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "WebHdfsFileSystem.java",
  "functionName": "getRedirectedUrl",
  "functionId": "getRedirectedUrl",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
  "functionStartLine": 2255,
  "functionEndLine": 2282,
  "numCommitsSeen": 73,
  "timeTaken": 1772,
  "changeHistory": [
    "fde95d463c3123b315b3d07cb5b7b7dc19f7cb73"
  ],
  "changeHistoryShort": {
    "fde95d463c3123b315b3d07cb5b7b7dc19f7cb73": "Yintroduced"
  },
  "changeHistoryDetails": {
    "fde95d463c3123b315b3d07cb5b7b7dc19f7cb73": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-12574. Add CryptoInputStream to WebHdfsFileSystem read call. Contributed by Rushabh S Shah\n",
      "commitDate": "29/01/18 3:23 PM",
      "commitName": "fde95d463c3123b315b3d07cb5b7b7dc19f7cb73",
      "commitAuthor": "Kihwal Lee",
      "diff": "@@ -0,0 +1,28 @@\n+    private void getRedirectedUrl() throws IOException {\n+      URLRunner urlRunner \u003d new URLRunner(GetOpParam.Op.OPEN, null, false,\n+          false) {\n+        @Override\n+        protected URL getUrl() throws IOException {\n+          return toUrl(op, path, new BufferSizeParam(bufferSize));\n+        }\n+      };\n+      HttpURLConnection conn \u003d urlRunner.run();\n+      String feInfoStr \u003d conn.getHeaderField(FEFINFO_HEADER);\n+      if (feInfoStr !\u003d null) {\n+        Decoder decoder \u003d Base64.getDecoder();\n+        byte[] decodedBytes \u003d decoder.decode(\n+            feInfoStr.getBytes(StandardCharsets.UTF_8));\n+        feInfo \u003d PBHelperClient\n+            .convert(FileEncryptionInfoProto.parseFrom(decodedBytes));\n+      }\n+      String location \u003d conn.getHeaderField(\"Location\");\n+      if (location !\u003d null) {\n+        // This saves the location for datanode where redirect was issued.\n+        // Need to remove offset because seek can be called after open.\n+        resolvedUrl \u003d removeOffsetParam(new URL(location));\n+      } else {\n+        // This is cached for proxies like httpfsfilesystem.\n+        cachedConnection \u003d conn;\n+      }\n+      originalUrl \u003d super.getUrl();\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void getRedirectedUrl() throws IOException {\n      URLRunner urlRunner \u003d new URLRunner(GetOpParam.Op.OPEN, null, false,\n          false) {\n        @Override\n        protected URL getUrl() throws IOException {\n          return toUrl(op, path, new BufferSizeParam(bufferSize));\n        }\n      };\n      HttpURLConnection conn \u003d urlRunner.run();\n      String feInfoStr \u003d conn.getHeaderField(FEFINFO_HEADER);\n      if (feInfoStr !\u003d null) {\n        Decoder decoder \u003d Base64.getDecoder();\n        byte[] decodedBytes \u003d decoder.decode(\n            feInfoStr.getBytes(StandardCharsets.UTF_8));\n        feInfo \u003d PBHelperClient\n            .convert(FileEncryptionInfoProto.parseFrom(decodedBytes));\n      }\n      String location \u003d conn.getHeaderField(\"Location\");\n      if (location !\u003d null) {\n        // This saves the location for datanode where redirect was issued.\n        // Need to remove offset because seek can be called after open.\n        resolvedUrl \u003d removeOffsetParam(new URL(location));\n      } else {\n        // This is cached for proxies like httpfsfilesystem.\n        cachedConnection \u003d conn;\n      }\n      originalUrl \u003d super.getUrl();\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java"
    }
  }
}