{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "SplitMetaInfoReader.java",
  "functionName": "readSplitMetaInfo",
  "functionId": "readSplitMetaInfo___jobId-JobID__fs-FileSystem__conf-Configuration__jobSubmitDir-Path",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java",
  "functionStartLine": 44,
  "functionEndLine": 82,
  "numCommitsSeen": 5,
  "timeTaken": 4422,
  "changeHistory": [
    "892846dc0495aa4d0247e3a75cefecc6c191fd97",
    "1283ab51a9fd514a51929120fff02c16ef545fea",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "892846dc0495aa4d0247e3a75cefecc6c191fd97": "Ybodychange",
    "1283ab51a9fd514a51929120fff02c16ef545fea": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "892846dc0495aa4d0247e3a75cefecc6c191fd97": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4871. AM uses mapreduce.jobtracker.split.metainfo.maxsize but mapred-default has mapreduce.job.split.metainfo.maxsize (Jason Lowe via jeagles)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1451318 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/02/13 11:55 AM",
      "commitName": "892846dc0495aa4d0247e3a75cefecc6c191fd97",
      "commitAuthor": "Jonathan Turner Eagles",
      "commitDateOld": "04/06/12 8:14 AM",
      "commitNameOld": "1283ab51a9fd514a51929120fff02c16ef545fea",
      "commitAuthorOld": "Thomas Graves",
      "daysBetweenCommits": 269.19,
      "commitsBetweenForRepo": 1370,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,39 +1,39 @@\n   public static JobSplit.TaskSplitMetaInfo[] readSplitMetaInfo(\n       JobID jobId, FileSystem fs, Configuration conf, Path jobSubmitDir) \n   throws IOException {\n-    long maxMetaInfoSize \u003d conf.getLong(JTConfig.JT_MAX_JOB_SPLIT_METAINFO_SIZE, \n-        10000000L);\n+    long maxMetaInfoSize \u003d conf.getLong(MRJobConfig.SPLIT_METAINFO_MAXSIZE,\n+        MRJobConfig.DEFAULT_SPLIT_METAINFO_MAXSIZE);\n     Path metaSplitFile \u003d JobSubmissionFiles.getJobSplitMetaFile(jobSubmitDir);\n     String jobSplitFile \u003d JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString();\n     FileStatus fStatus \u003d fs.getFileStatus(metaSplitFile);\n     if (maxMetaInfoSize \u003e 0 \u0026\u0026 fStatus.getLen() \u003e maxMetaInfoSize) {\n       throw new IOException(\"Split metadata size exceeded \" +\n           maxMetaInfoSize +\". Aborting job \" + jobId);\n     }\n     FSDataInputStream in \u003d fs.open(metaSplitFile);\n     byte[] header \u003d new byte[JobSplit.META_SPLIT_FILE_HEADER.length];\n     in.readFully(header);\n     if (!Arrays.equals(JobSplit.META_SPLIT_FILE_HEADER, header)) {\n       throw new IOException(\"Invalid header on split file\");\n     }\n     int vers \u003d WritableUtils.readVInt(in);\n     if (vers !\u003d JobSplit.META_SPLIT_VERSION) {\n       in.close();\n       throw new IOException(\"Unsupported split version \" + vers);\n     }\n     int numSplits \u003d WritableUtils.readVInt(in); //TODO: check for insane values\n     JobSplit.TaskSplitMetaInfo[] allSplitMetaInfo \u003d \n       new JobSplit.TaskSplitMetaInfo[numSplits];\n     for (int i \u003d 0; i \u003c numSplits; i++) {\n       JobSplit.SplitMetaInfo splitMetaInfo \u003d new JobSplit.SplitMetaInfo();\n       splitMetaInfo.readFields(in);\n       JobSplit.TaskSplitIndex splitIndex \u003d new JobSplit.TaskSplitIndex(\n           jobSplitFile, \n           splitMetaInfo.getStartOffset());\n       allSplitMetaInfo[i] \u003d new JobSplit.TaskSplitMetaInfo(splitIndex, \n           splitMetaInfo.getLocations(), \n           splitMetaInfo.getInputDataLength());\n     }\n     in.close();\n     return allSplitMetaInfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static JobSplit.TaskSplitMetaInfo[] readSplitMetaInfo(\n      JobID jobId, FileSystem fs, Configuration conf, Path jobSubmitDir) \n  throws IOException {\n    long maxMetaInfoSize \u003d conf.getLong(MRJobConfig.SPLIT_METAINFO_MAXSIZE,\n        MRJobConfig.DEFAULT_SPLIT_METAINFO_MAXSIZE);\n    Path metaSplitFile \u003d JobSubmissionFiles.getJobSplitMetaFile(jobSubmitDir);\n    String jobSplitFile \u003d JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString();\n    FileStatus fStatus \u003d fs.getFileStatus(metaSplitFile);\n    if (maxMetaInfoSize \u003e 0 \u0026\u0026 fStatus.getLen() \u003e maxMetaInfoSize) {\n      throw new IOException(\"Split metadata size exceeded \" +\n          maxMetaInfoSize +\". Aborting job \" + jobId);\n    }\n    FSDataInputStream in \u003d fs.open(metaSplitFile);\n    byte[] header \u003d new byte[JobSplit.META_SPLIT_FILE_HEADER.length];\n    in.readFully(header);\n    if (!Arrays.equals(JobSplit.META_SPLIT_FILE_HEADER, header)) {\n      throw new IOException(\"Invalid header on split file\");\n    }\n    int vers \u003d WritableUtils.readVInt(in);\n    if (vers !\u003d JobSplit.META_SPLIT_VERSION) {\n      in.close();\n      throw new IOException(\"Unsupported split version \" + vers);\n    }\n    int numSplits \u003d WritableUtils.readVInt(in); //TODO: check for insane values\n    JobSplit.TaskSplitMetaInfo[] allSplitMetaInfo \u003d \n      new JobSplit.TaskSplitMetaInfo[numSplits];\n    for (int i \u003d 0; i \u003c numSplits; i++) {\n      JobSplit.SplitMetaInfo splitMetaInfo \u003d new JobSplit.SplitMetaInfo();\n      splitMetaInfo.readFields(in);\n      JobSplit.TaskSplitIndex splitIndex \u003d new JobSplit.TaskSplitIndex(\n          jobSplitFile, \n          splitMetaInfo.getStartOffset());\n      allSplitMetaInfo[i] \u003d new JobSplit.TaskSplitMetaInfo(splitIndex, \n          splitMetaInfo.getLocations(), \n          splitMetaInfo.getInputDataLength());\n    }\n    in.close();\n    return allSplitMetaInfo;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java",
      "extendedDetails": {}
    },
    "1283ab51a9fd514a51929120fff02c16ef545fea": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4301. Dedupe some strings in MRAM for memory savings (bobby via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1346002 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/06/12 8:14 AM",
      "commitName": "1283ab51a9fd514a51929120fff02c16ef545fea",
      "commitAuthor": "Thomas Graves",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 284.63,
      "commitsBetweenForRepo": 1997,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,39 @@\n   public static JobSplit.TaskSplitMetaInfo[] readSplitMetaInfo(\n       JobID jobId, FileSystem fs, Configuration conf, Path jobSubmitDir) \n   throws IOException {\n     long maxMetaInfoSize \u003d conf.getLong(JTConfig.JT_MAX_JOB_SPLIT_METAINFO_SIZE, \n         10000000L);\n     Path metaSplitFile \u003d JobSubmissionFiles.getJobSplitMetaFile(jobSubmitDir);\n+    String jobSplitFile \u003d JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString();\n     FileStatus fStatus \u003d fs.getFileStatus(metaSplitFile);\n     if (maxMetaInfoSize \u003e 0 \u0026\u0026 fStatus.getLen() \u003e maxMetaInfoSize) {\n       throw new IOException(\"Split metadata size exceeded \" +\n           maxMetaInfoSize +\". Aborting job \" + jobId);\n     }\n     FSDataInputStream in \u003d fs.open(metaSplitFile);\n     byte[] header \u003d new byte[JobSplit.META_SPLIT_FILE_HEADER.length];\n     in.readFully(header);\n     if (!Arrays.equals(JobSplit.META_SPLIT_FILE_HEADER, header)) {\n       throw new IOException(\"Invalid header on split file\");\n     }\n     int vers \u003d WritableUtils.readVInt(in);\n     if (vers !\u003d JobSplit.META_SPLIT_VERSION) {\n       in.close();\n       throw new IOException(\"Unsupported split version \" + vers);\n     }\n     int numSplits \u003d WritableUtils.readVInt(in); //TODO: check for insane values\n     JobSplit.TaskSplitMetaInfo[] allSplitMetaInfo \u003d \n       new JobSplit.TaskSplitMetaInfo[numSplits];\n     for (int i \u003d 0; i \u003c numSplits; i++) {\n       JobSplit.SplitMetaInfo splitMetaInfo \u003d new JobSplit.SplitMetaInfo();\n       splitMetaInfo.readFields(in);\n       JobSplit.TaskSplitIndex splitIndex \u003d new JobSplit.TaskSplitIndex(\n-          JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString(), \n+          jobSplitFile, \n           splitMetaInfo.getStartOffset());\n       allSplitMetaInfo[i] \u003d new JobSplit.TaskSplitMetaInfo(splitIndex, \n           splitMetaInfo.getLocations(), \n           splitMetaInfo.getInputDataLength());\n     }\n     in.close();\n     return allSplitMetaInfo;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public static JobSplit.TaskSplitMetaInfo[] readSplitMetaInfo(\n      JobID jobId, FileSystem fs, Configuration conf, Path jobSubmitDir) \n  throws IOException {\n    long maxMetaInfoSize \u003d conf.getLong(JTConfig.JT_MAX_JOB_SPLIT_METAINFO_SIZE, \n        10000000L);\n    Path metaSplitFile \u003d JobSubmissionFiles.getJobSplitMetaFile(jobSubmitDir);\n    String jobSplitFile \u003d JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString();\n    FileStatus fStatus \u003d fs.getFileStatus(metaSplitFile);\n    if (maxMetaInfoSize \u003e 0 \u0026\u0026 fStatus.getLen() \u003e maxMetaInfoSize) {\n      throw new IOException(\"Split metadata size exceeded \" +\n          maxMetaInfoSize +\". Aborting job \" + jobId);\n    }\n    FSDataInputStream in \u003d fs.open(metaSplitFile);\n    byte[] header \u003d new byte[JobSplit.META_SPLIT_FILE_HEADER.length];\n    in.readFully(header);\n    if (!Arrays.equals(JobSplit.META_SPLIT_FILE_HEADER, header)) {\n      throw new IOException(\"Invalid header on split file\");\n    }\n    int vers \u003d WritableUtils.readVInt(in);\n    if (vers !\u003d JobSplit.META_SPLIT_VERSION) {\n      in.close();\n      throw new IOException(\"Unsupported split version \" + vers);\n    }\n    int numSplits \u003d WritableUtils.readVInt(in); //TODO: check for insane values\n    JobSplit.TaskSplitMetaInfo[] allSplitMetaInfo \u003d \n      new JobSplit.TaskSplitMetaInfo[numSplits];\n    for (int i \u003d 0; i \u003c numSplits; i++) {\n      JobSplit.SplitMetaInfo splitMetaInfo \u003d new JobSplit.SplitMetaInfo();\n      splitMetaInfo.readFields(in);\n      JobSplit.TaskSplitIndex splitIndex \u003d new JobSplit.TaskSplitIndex(\n          jobSplitFile, \n          splitMetaInfo.getStartOffset());\n      allSplitMetaInfo[i] \u003d new JobSplit.TaskSplitMetaInfo(splitIndex, \n          splitMetaInfo.getLocations(), \n          splitMetaInfo.getInputDataLength());\n    }\n    in.close();\n    return allSplitMetaInfo;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static JobSplit.TaskSplitMetaInfo[] readSplitMetaInfo(\n      JobID jobId, FileSystem fs, Configuration conf, Path jobSubmitDir) \n  throws IOException {\n    long maxMetaInfoSize \u003d conf.getLong(JTConfig.JT_MAX_JOB_SPLIT_METAINFO_SIZE, \n        10000000L);\n    Path metaSplitFile \u003d JobSubmissionFiles.getJobSplitMetaFile(jobSubmitDir);\n    FileStatus fStatus \u003d fs.getFileStatus(metaSplitFile);\n    if (maxMetaInfoSize \u003e 0 \u0026\u0026 fStatus.getLen() \u003e maxMetaInfoSize) {\n      throw new IOException(\"Split metadata size exceeded \" +\n          maxMetaInfoSize +\". Aborting job \" + jobId);\n    }\n    FSDataInputStream in \u003d fs.open(metaSplitFile);\n    byte[] header \u003d new byte[JobSplit.META_SPLIT_FILE_HEADER.length];\n    in.readFully(header);\n    if (!Arrays.equals(JobSplit.META_SPLIT_FILE_HEADER, header)) {\n      throw new IOException(\"Invalid header on split file\");\n    }\n    int vers \u003d WritableUtils.readVInt(in);\n    if (vers !\u003d JobSplit.META_SPLIT_VERSION) {\n      in.close();\n      throw new IOException(\"Unsupported split version \" + vers);\n    }\n    int numSplits \u003d WritableUtils.readVInt(in); //TODO: check for insane values\n    JobSplit.TaskSplitMetaInfo[] allSplitMetaInfo \u003d \n      new JobSplit.TaskSplitMetaInfo[numSplits];\n    for (int i \u003d 0; i \u003c numSplits; i++) {\n      JobSplit.SplitMetaInfo splitMetaInfo \u003d new JobSplit.SplitMetaInfo();\n      splitMetaInfo.readFields(in);\n      JobSplit.TaskSplitIndex splitIndex \u003d new JobSplit.TaskSplitIndex(\n          JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString(), \n          splitMetaInfo.getStartOffset());\n      allSplitMetaInfo[i] \u003d new JobSplit.TaskSplitMetaInfo(splitIndex, \n          splitMetaInfo.getLocations(), \n          splitMetaInfo.getInputDataLength());\n    }\n    in.close();\n    return allSplitMetaInfo;\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public static JobSplit.TaskSplitMetaInfo[] readSplitMetaInfo(\n      JobID jobId, FileSystem fs, Configuration conf, Path jobSubmitDir) \n  throws IOException {\n    long maxMetaInfoSize \u003d conf.getLong(JTConfig.JT_MAX_JOB_SPLIT_METAINFO_SIZE, \n        10000000L);\n    Path metaSplitFile \u003d JobSubmissionFiles.getJobSplitMetaFile(jobSubmitDir);\n    FileStatus fStatus \u003d fs.getFileStatus(metaSplitFile);\n    if (maxMetaInfoSize \u003e 0 \u0026\u0026 fStatus.getLen() \u003e maxMetaInfoSize) {\n      throw new IOException(\"Split metadata size exceeded \" +\n          maxMetaInfoSize +\". Aborting job \" + jobId);\n    }\n    FSDataInputStream in \u003d fs.open(metaSplitFile);\n    byte[] header \u003d new byte[JobSplit.META_SPLIT_FILE_HEADER.length];\n    in.readFully(header);\n    if (!Arrays.equals(JobSplit.META_SPLIT_FILE_HEADER, header)) {\n      throw new IOException(\"Invalid header on split file\");\n    }\n    int vers \u003d WritableUtils.readVInt(in);\n    if (vers !\u003d JobSplit.META_SPLIT_VERSION) {\n      in.close();\n      throw new IOException(\"Unsupported split version \" + vers);\n    }\n    int numSplits \u003d WritableUtils.readVInt(in); //TODO: check for insane values\n    JobSplit.TaskSplitMetaInfo[] allSplitMetaInfo \u003d \n      new JobSplit.TaskSplitMetaInfo[numSplits];\n    for (int i \u003d 0; i \u003c numSplits; i++) {\n      JobSplit.SplitMetaInfo splitMetaInfo \u003d new JobSplit.SplitMetaInfo();\n      splitMetaInfo.readFields(in);\n      JobSplit.TaskSplitIndex splitIndex \u003d new JobSplit.TaskSplitIndex(\n          JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString(), \n          splitMetaInfo.getStartOffset());\n      allSplitMetaInfo[i] \u003d new JobSplit.TaskSplitMetaInfo(splitIndex, \n          splitMetaInfo.getLocations(), \n          splitMetaInfo.getInputDataLength());\n    }\n    in.close();\n    return allSplitMetaInfo;\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,38 @@\n+  public static JobSplit.TaskSplitMetaInfo[] readSplitMetaInfo(\n+      JobID jobId, FileSystem fs, Configuration conf, Path jobSubmitDir) \n+  throws IOException {\n+    long maxMetaInfoSize \u003d conf.getLong(JTConfig.JT_MAX_JOB_SPLIT_METAINFO_SIZE, \n+        10000000L);\n+    Path metaSplitFile \u003d JobSubmissionFiles.getJobSplitMetaFile(jobSubmitDir);\n+    FileStatus fStatus \u003d fs.getFileStatus(metaSplitFile);\n+    if (maxMetaInfoSize \u003e 0 \u0026\u0026 fStatus.getLen() \u003e maxMetaInfoSize) {\n+      throw new IOException(\"Split metadata size exceeded \" +\n+          maxMetaInfoSize +\". Aborting job \" + jobId);\n+    }\n+    FSDataInputStream in \u003d fs.open(metaSplitFile);\n+    byte[] header \u003d new byte[JobSplit.META_SPLIT_FILE_HEADER.length];\n+    in.readFully(header);\n+    if (!Arrays.equals(JobSplit.META_SPLIT_FILE_HEADER, header)) {\n+      throw new IOException(\"Invalid header on split file\");\n+    }\n+    int vers \u003d WritableUtils.readVInt(in);\n+    if (vers !\u003d JobSplit.META_SPLIT_VERSION) {\n+      in.close();\n+      throw new IOException(\"Unsupported split version \" + vers);\n+    }\n+    int numSplits \u003d WritableUtils.readVInt(in); //TODO: check for insane values\n+    JobSplit.TaskSplitMetaInfo[] allSplitMetaInfo \u003d \n+      new JobSplit.TaskSplitMetaInfo[numSplits];\n+    for (int i \u003d 0; i \u003c numSplits; i++) {\n+      JobSplit.SplitMetaInfo splitMetaInfo \u003d new JobSplit.SplitMetaInfo();\n+      splitMetaInfo.readFields(in);\n+      JobSplit.TaskSplitIndex splitIndex \u003d new JobSplit.TaskSplitIndex(\n+          JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString(), \n+          splitMetaInfo.getStartOffset());\n+      allSplitMetaInfo[i] \u003d new JobSplit.TaskSplitMetaInfo(splitIndex, \n+          splitMetaInfo.getLocations(), \n+          splitMetaInfo.getInputDataLength());\n+    }\n+    in.close();\n+    return allSplitMetaInfo;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public static JobSplit.TaskSplitMetaInfo[] readSplitMetaInfo(\n      JobID jobId, FileSystem fs, Configuration conf, Path jobSubmitDir) \n  throws IOException {\n    long maxMetaInfoSize \u003d conf.getLong(JTConfig.JT_MAX_JOB_SPLIT_METAINFO_SIZE, \n        10000000L);\n    Path metaSplitFile \u003d JobSubmissionFiles.getJobSplitMetaFile(jobSubmitDir);\n    FileStatus fStatus \u003d fs.getFileStatus(metaSplitFile);\n    if (maxMetaInfoSize \u003e 0 \u0026\u0026 fStatus.getLen() \u003e maxMetaInfoSize) {\n      throw new IOException(\"Split metadata size exceeded \" +\n          maxMetaInfoSize +\". Aborting job \" + jobId);\n    }\n    FSDataInputStream in \u003d fs.open(metaSplitFile);\n    byte[] header \u003d new byte[JobSplit.META_SPLIT_FILE_HEADER.length];\n    in.readFully(header);\n    if (!Arrays.equals(JobSplit.META_SPLIT_FILE_HEADER, header)) {\n      throw new IOException(\"Invalid header on split file\");\n    }\n    int vers \u003d WritableUtils.readVInt(in);\n    if (vers !\u003d JobSplit.META_SPLIT_VERSION) {\n      in.close();\n      throw new IOException(\"Unsupported split version \" + vers);\n    }\n    int numSplits \u003d WritableUtils.readVInt(in); //TODO: check for insane values\n    JobSplit.TaskSplitMetaInfo[] allSplitMetaInfo \u003d \n      new JobSplit.TaskSplitMetaInfo[numSplits];\n    for (int i \u003d 0; i \u003c numSplits; i++) {\n      JobSplit.SplitMetaInfo splitMetaInfo \u003d new JobSplit.SplitMetaInfo();\n      splitMetaInfo.readFields(in);\n      JobSplit.TaskSplitIndex splitIndex \u003d new JobSplit.TaskSplitIndex(\n          JobSubmissionFiles.getJobSplitFile(jobSubmitDir).toString(), \n          splitMetaInfo.getStartOffset());\n      allSplitMetaInfo[i] \u003d new JobSplit.TaskSplitMetaInfo(splitIndex, \n          splitMetaInfo.getLocations(), \n          splitMetaInfo.getInputDataLength());\n    }\n    in.close();\n    return allSplitMetaInfo;\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/split/SplitMetaInfoReader.java"
    }
  }
}