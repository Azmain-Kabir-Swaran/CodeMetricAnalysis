{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "AdlFileSystem.java",
  "functionName": "initialize",
  "functionId": "initialize___storeUri-URI__originalConf-Configuration",
  "sourceFilePath": "hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlFileSystem.java",
  "functionStartLine": 132,
  "functionEndLine": 224,
  "numCommitsSeen": 32,
  "timeTaken": 5268,
  "changeHistory": [
    "147f98629cfa799044d5a911221f365a03f9380c",
    "1cfe7506f7e9aff808af4ec0e57639130a6d0f35",
    "481d79fedc48942654dab08e23e71e80c8eb2aca",
    "f735ad1b67ed82d9b11b1afd7ae39035a6aed18b",
    "924def78544a64449785f305cb6984c3559aea4d",
    "5c61ad24887f76dfc5a5935b2c5dceb6bfd99417"
  ],
  "changeHistoryShort": {
    "147f98629cfa799044d5a911221f365a03f9380c": "Ybodychange",
    "1cfe7506f7e9aff808af4ec0e57639130a6d0f35": "Ybodychange",
    "481d79fedc48942654dab08e23e71e80c8eb2aca": "Ymultichange(Yparameterchange,Ybodychange)",
    "f735ad1b67ed82d9b11b1afd7ae39035a6aed18b": "Ybodychange",
    "924def78544a64449785f305cb6984c3559aea4d": "Ybodychange",
    "5c61ad24887f76dfc5a5935b2c5dceb6bfd99417": "Yintroduced"
  },
  "changeHistoryDetails": {
    "147f98629cfa799044d5a911221f365a03f9380c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16438. ADLS Gen1 OpenSSL config control.\n\nContributed by Sneha Vijayarajan.\n\nChange-Id: Ib79ea6b4a90ad068033e175f3f59c5185868872d\n",
      "commitDate": "09/09/19 9:09 AM",
      "commitName": "147f98629cfa799044d5a911221f365a03f9380c",
      "commitAuthor": "Sneha Vijayarajan",
      "commitDateOld": "27/06/18 10:37 PM",
      "commitNameOld": "2b2399d623539ab68e71a38fa9fbfc9a405bddb8",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 438.44,
      "commitsBetweenForRepo": 3405,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,89 +1,93 @@\n   public void initialize(URI storeUri, Configuration originalConf)\n       throws IOException {\n     String hostname \u003d storeUri.getHost();\n     String accountName \u003d getAccountNameFromFQDN(hostname);\n     Configuration conf \u003d propagateAccountOptions(originalConf, accountName);\n \n     super.initialize(storeUri, conf);\n     this.setConf(conf);\n     this.uri \u003d URI\n         .create(storeUri.getScheme() + \"://\" + storeUri.getAuthority());\n \n     try {\n       userName \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n     } catch (IOException e) {\n       userName \u003d \"hadoop\";\n       LOG.warn(\"Got exception when getting Hadoop user name.\"\n           + \" Set the user name to \u0027\" + userName + \"\u0027.\", e);\n     }\n \n     this.setWorkingDirectory(getHomeDirectory());\n \n     overrideOwner \u003d getConf().getBoolean(ADL_DEBUG_OVERRIDE_LOCAL_USER_AS_OWNER,\n         ADL_DEBUG_SET_LOCAL_USER_AS_OWNER_DEFAULT);\n \n     aclBitStatus \u003d conf.getBoolean(ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION,\n         ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION_DEFAULT);\n \n     String accountFQDN \u003d null;\n     String mountPoint \u003d null;\n     if (!hostname.contains(\".\") \u0026\u0026 !hostname.equalsIgnoreCase(\n         \"localhost\")) {  // this is a symbolic name. Resolve it.\n       String hostNameProperty \u003d \"dfs.adls.\" + hostname + \".hostname\";\n       String mountPointProperty \u003d \"dfs.adls.\" + hostname + \".mountpoint\";\n       accountFQDN \u003d getNonEmptyVal(conf, hostNameProperty);\n       mountPoint \u003d getNonEmptyVal(conf, mountPointProperty);\n     } else {\n       accountFQDN \u003d hostname;\n     }\n \n     if (storeUri.getPort() \u003e 0) {\n       accountFQDN \u003d accountFQDN + \":\" + storeUri.getPort();\n     }\n \n     adlClient \u003d ADLStoreClient\n         .createClient(accountFQDN, getAccessTokenProvider(conf));\n \n     ADLStoreOptions options \u003d new ADLStoreOptions();\n     options.enableThrowingRemoteExceptions();\n \n     if (getTransportScheme().equalsIgnoreCase(INSECURE_TRANSPORT_SCHEME)) {\n       options.setInsecureTransport();\n     }\n \n     if (mountPoint !\u003d null) {\n       options.setFilePathPrefix(mountPoint);\n     }\n \n     String clusterName \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERNAME, \"UNKNOWN\");\n     String clusterType \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERTYPE, \"UNKNOWN\");\n \n     String clientVersion \u003d ADL_HADOOP_CLIENT_NAME + (StringUtils\n         .isEmpty(VersionInfo.getVersion().trim()) ?\n         ADL_HADOOP_CLIENT_VERSION.trim() :\n         VersionInfo.getVersion().trim());\n     options.setUserAgentSuffix(clientVersion + \"/\" +\n         VersionInfo.getVersion().trim() + \"/\" + clusterName + \"/\"\n         + clusterType);\n \n     int timeout \u003d conf.getInt(ADL_HTTP_TIMEOUT, -1);\n     if (timeout \u003e 0) {\n       // only set timeout if specified in config. Otherwise use SDK default\n       options.setDefaultTimeout(timeout);\n     } else {\n       LOG.info(\"No valid ADL SDK timeout configured: using SDK default.\");\n     }\n \n+    String sslChannelMode \u003d conf.get(ADL_SSL_CHANNEL_MODE,\n+        \"Default\");\n+    options.setSSLChannelMode(sslChannelMode);\n+\n     adlClient.setOptions(options);\n \n     boolean trackLatency \u003d conf\n         .getBoolean(LATENCY_TRACKER_KEY, LATENCY_TRACKER_DEFAULT);\n     if (!trackLatency) {\n       LatencyTracker.disable();\n     }\n \n     boolean enableUPN \u003d conf.getBoolean(ADL_ENABLEUPN_FOR_OWNERGROUP_KEY,\n         ADL_ENABLEUPN_FOR_OWNERGROUP_DEFAULT);\n     oidOrUpn \u003d enableUPN ? UserGroupRepresentation.UPN :\n         UserGroupRepresentation.OID;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI storeUri, Configuration originalConf)\n      throws IOException {\n    String hostname \u003d storeUri.getHost();\n    String accountName \u003d getAccountNameFromFQDN(hostname);\n    Configuration conf \u003d propagateAccountOptions(originalConf, accountName);\n\n    super.initialize(storeUri, conf);\n    this.setConf(conf);\n    this.uri \u003d URI\n        .create(storeUri.getScheme() + \"://\" + storeUri.getAuthority());\n\n    try {\n      userName \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n    } catch (IOException e) {\n      userName \u003d \"hadoop\";\n      LOG.warn(\"Got exception when getting Hadoop user name.\"\n          + \" Set the user name to \u0027\" + userName + \"\u0027.\", e);\n    }\n\n    this.setWorkingDirectory(getHomeDirectory());\n\n    overrideOwner \u003d getConf().getBoolean(ADL_DEBUG_OVERRIDE_LOCAL_USER_AS_OWNER,\n        ADL_DEBUG_SET_LOCAL_USER_AS_OWNER_DEFAULT);\n\n    aclBitStatus \u003d conf.getBoolean(ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION,\n        ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION_DEFAULT);\n\n    String accountFQDN \u003d null;\n    String mountPoint \u003d null;\n    if (!hostname.contains(\".\") \u0026\u0026 !hostname.equalsIgnoreCase(\n        \"localhost\")) {  // this is a symbolic name. Resolve it.\n      String hostNameProperty \u003d \"dfs.adls.\" + hostname + \".hostname\";\n      String mountPointProperty \u003d \"dfs.adls.\" + hostname + \".mountpoint\";\n      accountFQDN \u003d getNonEmptyVal(conf, hostNameProperty);\n      mountPoint \u003d getNonEmptyVal(conf, mountPointProperty);\n    } else {\n      accountFQDN \u003d hostname;\n    }\n\n    if (storeUri.getPort() \u003e 0) {\n      accountFQDN \u003d accountFQDN + \":\" + storeUri.getPort();\n    }\n\n    adlClient \u003d ADLStoreClient\n        .createClient(accountFQDN, getAccessTokenProvider(conf));\n\n    ADLStoreOptions options \u003d new ADLStoreOptions();\n    options.enableThrowingRemoteExceptions();\n\n    if (getTransportScheme().equalsIgnoreCase(INSECURE_TRANSPORT_SCHEME)) {\n      options.setInsecureTransport();\n    }\n\n    if (mountPoint !\u003d null) {\n      options.setFilePathPrefix(mountPoint);\n    }\n\n    String clusterName \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERNAME, \"UNKNOWN\");\n    String clusterType \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERTYPE, \"UNKNOWN\");\n\n    String clientVersion \u003d ADL_HADOOP_CLIENT_NAME + (StringUtils\n        .isEmpty(VersionInfo.getVersion().trim()) ?\n        ADL_HADOOP_CLIENT_VERSION.trim() :\n        VersionInfo.getVersion().trim());\n    options.setUserAgentSuffix(clientVersion + \"/\" +\n        VersionInfo.getVersion().trim() + \"/\" + clusterName + \"/\"\n        + clusterType);\n\n    int timeout \u003d conf.getInt(ADL_HTTP_TIMEOUT, -1);\n    if (timeout \u003e 0) {\n      // only set timeout if specified in config. Otherwise use SDK default\n      options.setDefaultTimeout(timeout);\n    } else {\n      LOG.info(\"No valid ADL SDK timeout configured: using SDK default.\");\n    }\n\n    String sslChannelMode \u003d conf.get(ADL_SSL_CHANNEL_MODE,\n        \"Default\");\n    options.setSSLChannelMode(sslChannelMode);\n\n    adlClient.setOptions(options);\n\n    boolean trackLatency \u003d conf\n        .getBoolean(LATENCY_TRACKER_KEY, LATENCY_TRACKER_DEFAULT);\n    if (!trackLatency) {\n      LatencyTracker.disable();\n    }\n\n    boolean enableUPN \u003d conf.getBoolean(ADL_ENABLEUPN_FOR_OWNERGROUP_KEY,\n        ADL_ENABLEUPN_FOR_OWNERGROUP_DEFAULT);\n    oidOrUpn \u003d enableUPN ? UserGroupRepresentation.UPN :\n        UserGroupRepresentation.OID;\n  }",
      "path": "hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlFileSystem.java",
      "extendedDetails": {}
    },
    "1cfe7506f7e9aff808af4ec0e57639130a6d0f35": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15356. Make HTTP timeout configurable in ADLS connector. Contributed by Atul Sikaria and Sean Mackrory.\n",
      "commitDate": "09/05/18 2:15 PM",
      "commitName": "1cfe7506f7e9aff808af4ec0e57639130a6d0f35",
      "commitAuthor": "Sean Mackrory",
      "commitDateOld": "28/03/18 11:58 AM",
      "commitNameOld": "081c3501885c543bb1f159929d456d1ba2e3650c",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 42.1,
      "commitsBetweenForRepo": 771,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,81 +1,89 @@\n   public void initialize(URI storeUri, Configuration originalConf)\n       throws IOException {\n     String hostname \u003d storeUri.getHost();\n     String accountName \u003d getAccountNameFromFQDN(hostname);\n     Configuration conf \u003d propagateAccountOptions(originalConf, accountName);\n \n     super.initialize(storeUri, conf);\n     this.setConf(conf);\n     this.uri \u003d URI\n         .create(storeUri.getScheme() + \"://\" + storeUri.getAuthority());\n \n     try {\n       userName \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n     } catch (IOException e) {\n       userName \u003d \"hadoop\";\n       LOG.warn(\"Got exception when getting Hadoop user name.\"\n           + \" Set the user name to \u0027\" + userName + \"\u0027.\", e);\n     }\n \n     this.setWorkingDirectory(getHomeDirectory());\n \n     overrideOwner \u003d getConf().getBoolean(ADL_DEBUG_OVERRIDE_LOCAL_USER_AS_OWNER,\n         ADL_DEBUG_SET_LOCAL_USER_AS_OWNER_DEFAULT);\n \n     aclBitStatus \u003d conf.getBoolean(ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION,\n         ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION_DEFAULT);\n \n     String accountFQDN \u003d null;\n     String mountPoint \u003d null;\n     if (!hostname.contains(\".\") \u0026\u0026 !hostname.equalsIgnoreCase(\n         \"localhost\")) {  // this is a symbolic name. Resolve it.\n       String hostNameProperty \u003d \"dfs.adls.\" + hostname + \".hostname\";\n       String mountPointProperty \u003d \"dfs.adls.\" + hostname + \".mountpoint\";\n       accountFQDN \u003d getNonEmptyVal(conf, hostNameProperty);\n       mountPoint \u003d getNonEmptyVal(conf, mountPointProperty);\n     } else {\n       accountFQDN \u003d hostname;\n     }\n \n     if (storeUri.getPort() \u003e 0) {\n       accountFQDN \u003d accountFQDN + \":\" + storeUri.getPort();\n     }\n \n     adlClient \u003d ADLStoreClient\n         .createClient(accountFQDN, getAccessTokenProvider(conf));\n \n     ADLStoreOptions options \u003d new ADLStoreOptions();\n     options.enableThrowingRemoteExceptions();\n \n     if (getTransportScheme().equalsIgnoreCase(INSECURE_TRANSPORT_SCHEME)) {\n       options.setInsecureTransport();\n     }\n \n     if (mountPoint !\u003d null) {\n       options.setFilePathPrefix(mountPoint);\n     }\n \n     String clusterName \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERNAME, \"UNKNOWN\");\n     String clusterType \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERTYPE, \"UNKNOWN\");\n \n     String clientVersion \u003d ADL_HADOOP_CLIENT_NAME + (StringUtils\n         .isEmpty(VersionInfo.getVersion().trim()) ?\n         ADL_HADOOP_CLIENT_VERSION.trim() :\n         VersionInfo.getVersion().trim());\n     options.setUserAgentSuffix(clientVersion + \"/\" +\n         VersionInfo.getVersion().trim() + \"/\" + clusterName + \"/\"\n         + clusterType);\n \n+    int timeout \u003d conf.getInt(ADL_HTTP_TIMEOUT, -1);\n+    if (timeout \u003e 0) {\n+      // only set timeout if specified in config. Otherwise use SDK default\n+      options.setDefaultTimeout(timeout);\n+    } else {\n+      LOG.info(\"No valid ADL SDK timeout configured: using SDK default.\");\n+    }\n+\n     adlClient.setOptions(options);\n \n     boolean trackLatency \u003d conf\n         .getBoolean(LATENCY_TRACKER_KEY, LATENCY_TRACKER_DEFAULT);\n     if (!trackLatency) {\n       LatencyTracker.disable();\n     }\n \n     boolean enableUPN \u003d conf.getBoolean(ADL_ENABLEUPN_FOR_OWNERGROUP_KEY,\n         ADL_ENABLEUPN_FOR_OWNERGROUP_DEFAULT);\n     oidOrUpn \u003d enableUPN ? UserGroupRepresentation.UPN :\n         UserGroupRepresentation.OID;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI storeUri, Configuration originalConf)\n      throws IOException {\n    String hostname \u003d storeUri.getHost();\n    String accountName \u003d getAccountNameFromFQDN(hostname);\n    Configuration conf \u003d propagateAccountOptions(originalConf, accountName);\n\n    super.initialize(storeUri, conf);\n    this.setConf(conf);\n    this.uri \u003d URI\n        .create(storeUri.getScheme() + \"://\" + storeUri.getAuthority());\n\n    try {\n      userName \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n    } catch (IOException e) {\n      userName \u003d \"hadoop\";\n      LOG.warn(\"Got exception when getting Hadoop user name.\"\n          + \" Set the user name to \u0027\" + userName + \"\u0027.\", e);\n    }\n\n    this.setWorkingDirectory(getHomeDirectory());\n\n    overrideOwner \u003d getConf().getBoolean(ADL_DEBUG_OVERRIDE_LOCAL_USER_AS_OWNER,\n        ADL_DEBUG_SET_LOCAL_USER_AS_OWNER_DEFAULT);\n\n    aclBitStatus \u003d conf.getBoolean(ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION,\n        ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION_DEFAULT);\n\n    String accountFQDN \u003d null;\n    String mountPoint \u003d null;\n    if (!hostname.contains(\".\") \u0026\u0026 !hostname.equalsIgnoreCase(\n        \"localhost\")) {  // this is a symbolic name. Resolve it.\n      String hostNameProperty \u003d \"dfs.adls.\" + hostname + \".hostname\";\n      String mountPointProperty \u003d \"dfs.adls.\" + hostname + \".mountpoint\";\n      accountFQDN \u003d getNonEmptyVal(conf, hostNameProperty);\n      mountPoint \u003d getNonEmptyVal(conf, mountPointProperty);\n    } else {\n      accountFQDN \u003d hostname;\n    }\n\n    if (storeUri.getPort() \u003e 0) {\n      accountFQDN \u003d accountFQDN + \":\" + storeUri.getPort();\n    }\n\n    adlClient \u003d ADLStoreClient\n        .createClient(accountFQDN, getAccessTokenProvider(conf));\n\n    ADLStoreOptions options \u003d new ADLStoreOptions();\n    options.enableThrowingRemoteExceptions();\n\n    if (getTransportScheme().equalsIgnoreCase(INSECURE_TRANSPORT_SCHEME)) {\n      options.setInsecureTransport();\n    }\n\n    if (mountPoint !\u003d null) {\n      options.setFilePathPrefix(mountPoint);\n    }\n\n    String clusterName \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERNAME, \"UNKNOWN\");\n    String clusterType \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERTYPE, \"UNKNOWN\");\n\n    String clientVersion \u003d ADL_HADOOP_CLIENT_NAME + (StringUtils\n        .isEmpty(VersionInfo.getVersion().trim()) ?\n        ADL_HADOOP_CLIENT_VERSION.trim() :\n        VersionInfo.getVersion().trim());\n    options.setUserAgentSuffix(clientVersion + \"/\" +\n        VersionInfo.getVersion().trim() + \"/\" + clusterName + \"/\"\n        + clusterType);\n\n    int timeout \u003d conf.getInt(ADL_HTTP_TIMEOUT, -1);\n    if (timeout \u003e 0) {\n      // only set timeout if specified in config. Otherwise use SDK default\n      options.setDefaultTimeout(timeout);\n    } else {\n      LOG.info(\"No valid ADL SDK timeout configured: using SDK default.\");\n    }\n\n    adlClient.setOptions(options);\n\n    boolean trackLatency \u003d conf\n        .getBoolean(LATENCY_TRACKER_KEY, LATENCY_TRACKER_DEFAULT);\n    if (!trackLatency) {\n      LatencyTracker.disable();\n    }\n\n    boolean enableUPN \u003d conf.getBoolean(ADL_ENABLEUPN_FOR_OWNERGROUP_KEY,\n        ADL_ENABLEUPN_FOR_OWNERGROUP_DEFAULT);\n    oidOrUpn \u003d enableUPN ? UserGroupRepresentation.UPN :\n        UserGroupRepresentation.OID;\n  }",
      "path": "hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlFileSystem.java",
      "extendedDetails": {}
    },
    "481d79fedc48942654dab08e23e71e80c8eb2aca": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-13972. ADLS to support per-store configuration.\nContributed by Sharad Sonker.\n\n(cherry picked from commit 050f5287b79324b7f6231b879c0bfc608203b980)\n",
      "commitDate": "15/02/18 8:27 AM",
      "commitName": "481d79fedc48942654dab08e23e71e80c8eb2aca",
      "commitAuthor": "Steve Loughran",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-13972. ADLS to support per-store configuration.\nContributed by Sharad Sonker.\n\n(cherry picked from commit 050f5287b79324b7f6231b879c0bfc608203b980)\n",
          "commitDate": "15/02/18 8:27 AM",
          "commitName": "481d79fedc48942654dab08e23e71e80c8eb2aca",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "08/09/17 11:51 AM",
          "commitNameOld": "a4661850c1e0794baf493a468191e12681d68ab4",
          "commitAuthorOld": "John Zhuge",
          "daysBetweenCommits": 159.9,
          "commitsBetweenForRepo": 1145,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,77 +1,81 @@\n-  public void initialize(URI storeUri, Configuration conf) throws IOException {\n+  public void initialize(URI storeUri, Configuration originalConf)\n+      throws IOException {\n+    String hostname \u003d storeUri.getHost();\n+    String accountName \u003d getAccountNameFromFQDN(hostname);\n+    Configuration conf \u003d propagateAccountOptions(originalConf, accountName);\n+\n     super.initialize(storeUri, conf);\n     this.setConf(conf);\n     this.uri \u003d URI\n         .create(storeUri.getScheme() + \"://\" + storeUri.getAuthority());\n \n     try {\n       userName \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n     } catch (IOException e) {\n       userName \u003d \"hadoop\";\n       LOG.warn(\"Got exception when getting Hadoop user name.\"\n           + \" Set the user name to \u0027\" + userName + \"\u0027.\", e);\n     }\n \n     this.setWorkingDirectory(getHomeDirectory());\n \n     overrideOwner \u003d getConf().getBoolean(ADL_DEBUG_OVERRIDE_LOCAL_USER_AS_OWNER,\n         ADL_DEBUG_SET_LOCAL_USER_AS_OWNER_DEFAULT);\n \n     aclBitStatus \u003d conf.getBoolean(ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION,\n         ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION_DEFAULT);\n \n     String accountFQDN \u003d null;\n     String mountPoint \u003d null;\n-    String hostname \u003d storeUri.getHost();\n     if (!hostname.contains(\".\") \u0026\u0026 !hostname.equalsIgnoreCase(\n         \"localhost\")) {  // this is a symbolic name. Resolve it.\n       String hostNameProperty \u003d \"dfs.adls.\" + hostname + \".hostname\";\n       String mountPointProperty \u003d \"dfs.adls.\" + hostname + \".mountpoint\";\n       accountFQDN \u003d getNonEmptyVal(conf, hostNameProperty);\n       mountPoint \u003d getNonEmptyVal(conf, mountPointProperty);\n     } else {\n       accountFQDN \u003d hostname;\n     }\n \n     if (storeUri.getPort() \u003e 0) {\n       accountFQDN \u003d accountFQDN + \":\" + storeUri.getPort();\n     }\n \n     adlClient \u003d ADLStoreClient\n         .createClient(accountFQDN, getAccessTokenProvider(conf));\n \n     ADLStoreOptions options \u003d new ADLStoreOptions();\n     options.enableThrowingRemoteExceptions();\n \n     if (getTransportScheme().equalsIgnoreCase(INSECURE_TRANSPORT_SCHEME)) {\n       options.setInsecureTransport();\n     }\n \n     if (mountPoint !\u003d null) {\n       options.setFilePathPrefix(mountPoint);\n     }\n \n     String clusterName \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERNAME, \"UNKNOWN\");\n     String clusterType \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERTYPE, \"UNKNOWN\");\n \n     String clientVersion \u003d ADL_HADOOP_CLIENT_NAME + (StringUtils\n         .isEmpty(VersionInfo.getVersion().trim()) ?\n         ADL_HADOOP_CLIENT_VERSION.trim() :\n         VersionInfo.getVersion().trim());\n     options.setUserAgentSuffix(clientVersion + \"/\" +\n         VersionInfo.getVersion().trim() + \"/\" + clusterName + \"/\"\n         + clusterType);\n \n     adlClient.setOptions(options);\n \n     boolean trackLatency \u003d conf\n         .getBoolean(LATENCY_TRACKER_KEY, LATENCY_TRACKER_DEFAULT);\n     if (!trackLatency) {\n       LatencyTracker.disable();\n     }\n \n     boolean enableUPN \u003d conf.getBoolean(ADL_ENABLEUPN_FOR_OWNERGROUP_KEY,\n         ADL_ENABLEUPN_FOR_OWNERGROUP_DEFAULT);\n     oidOrUpn \u003d enableUPN ? UserGroupRepresentation.UPN :\n         UserGroupRepresentation.OID;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void initialize(URI storeUri, Configuration originalConf)\n      throws IOException {\n    String hostname \u003d storeUri.getHost();\n    String accountName \u003d getAccountNameFromFQDN(hostname);\n    Configuration conf \u003d propagateAccountOptions(originalConf, accountName);\n\n    super.initialize(storeUri, conf);\n    this.setConf(conf);\n    this.uri \u003d URI\n        .create(storeUri.getScheme() + \"://\" + storeUri.getAuthority());\n\n    try {\n      userName \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n    } catch (IOException e) {\n      userName \u003d \"hadoop\";\n      LOG.warn(\"Got exception when getting Hadoop user name.\"\n          + \" Set the user name to \u0027\" + userName + \"\u0027.\", e);\n    }\n\n    this.setWorkingDirectory(getHomeDirectory());\n\n    overrideOwner \u003d getConf().getBoolean(ADL_DEBUG_OVERRIDE_LOCAL_USER_AS_OWNER,\n        ADL_DEBUG_SET_LOCAL_USER_AS_OWNER_DEFAULT);\n\n    aclBitStatus \u003d conf.getBoolean(ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION,\n        ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION_DEFAULT);\n\n    String accountFQDN \u003d null;\n    String mountPoint \u003d null;\n    if (!hostname.contains(\".\") \u0026\u0026 !hostname.equalsIgnoreCase(\n        \"localhost\")) {  // this is a symbolic name. Resolve it.\n      String hostNameProperty \u003d \"dfs.adls.\" + hostname + \".hostname\";\n      String mountPointProperty \u003d \"dfs.adls.\" + hostname + \".mountpoint\";\n      accountFQDN \u003d getNonEmptyVal(conf, hostNameProperty);\n      mountPoint \u003d getNonEmptyVal(conf, mountPointProperty);\n    } else {\n      accountFQDN \u003d hostname;\n    }\n\n    if (storeUri.getPort() \u003e 0) {\n      accountFQDN \u003d accountFQDN + \":\" + storeUri.getPort();\n    }\n\n    adlClient \u003d ADLStoreClient\n        .createClient(accountFQDN, getAccessTokenProvider(conf));\n\n    ADLStoreOptions options \u003d new ADLStoreOptions();\n    options.enableThrowingRemoteExceptions();\n\n    if (getTransportScheme().equalsIgnoreCase(INSECURE_TRANSPORT_SCHEME)) {\n      options.setInsecureTransport();\n    }\n\n    if (mountPoint !\u003d null) {\n      options.setFilePathPrefix(mountPoint);\n    }\n\n    String clusterName \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERNAME, \"UNKNOWN\");\n    String clusterType \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERTYPE, \"UNKNOWN\");\n\n    String clientVersion \u003d ADL_HADOOP_CLIENT_NAME + (StringUtils\n        .isEmpty(VersionInfo.getVersion().trim()) ?\n        ADL_HADOOP_CLIENT_VERSION.trim() :\n        VersionInfo.getVersion().trim());\n    options.setUserAgentSuffix(clientVersion + \"/\" +\n        VersionInfo.getVersion().trim() + \"/\" + clusterName + \"/\"\n        + clusterType);\n\n    adlClient.setOptions(options);\n\n    boolean trackLatency \u003d conf\n        .getBoolean(LATENCY_TRACKER_KEY, LATENCY_TRACKER_DEFAULT);\n    if (!trackLatency) {\n      LatencyTracker.disable();\n    }\n\n    boolean enableUPN \u003d conf.getBoolean(ADL_ENABLEUPN_FOR_OWNERGROUP_KEY,\n        ADL_ENABLEUPN_FOR_OWNERGROUP_DEFAULT);\n    oidOrUpn \u003d enableUPN ? UserGroupRepresentation.UPN :\n        UserGroupRepresentation.OID;\n  }",
          "path": "hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlFileSystem.java",
          "extendedDetails": {
            "oldValue": "[storeUri-URI, conf-Configuration]",
            "newValue": "[storeUri-URI, originalConf-Configuration]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-13972. ADLS to support per-store configuration.\nContributed by Sharad Sonker.\n\n(cherry picked from commit 050f5287b79324b7f6231b879c0bfc608203b980)\n",
          "commitDate": "15/02/18 8:27 AM",
          "commitName": "481d79fedc48942654dab08e23e71e80c8eb2aca",
          "commitAuthor": "Steve Loughran",
          "commitDateOld": "08/09/17 11:51 AM",
          "commitNameOld": "a4661850c1e0794baf493a468191e12681d68ab4",
          "commitAuthorOld": "John Zhuge",
          "daysBetweenCommits": 159.9,
          "commitsBetweenForRepo": 1145,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,77 +1,81 @@\n-  public void initialize(URI storeUri, Configuration conf) throws IOException {\n+  public void initialize(URI storeUri, Configuration originalConf)\n+      throws IOException {\n+    String hostname \u003d storeUri.getHost();\n+    String accountName \u003d getAccountNameFromFQDN(hostname);\n+    Configuration conf \u003d propagateAccountOptions(originalConf, accountName);\n+\n     super.initialize(storeUri, conf);\n     this.setConf(conf);\n     this.uri \u003d URI\n         .create(storeUri.getScheme() + \"://\" + storeUri.getAuthority());\n \n     try {\n       userName \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n     } catch (IOException e) {\n       userName \u003d \"hadoop\";\n       LOG.warn(\"Got exception when getting Hadoop user name.\"\n           + \" Set the user name to \u0027\" + userName + \"\u0027.\", e);\n     }\n \n     this.setWorkingDirectory(getHomeDirectory());\n \n     overrideOwner \u003d getConf().getBoolean(ADL_DEBUG_OVERRIDE_LOCAL_USER_AS_OWNER,\n         ADL_DEBUG_SET_LOCAL_USER_AS_OWNER_DEFAULT);\n \n     aclBitStatus \u003d conf.getBoolean(ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION,\n         ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION_DEFAULT);\n \n     String accountFQDN \u003d null;\n     String mountPoint \u003d null;\n-    String hostname \u003d storeUri.getHost();\n     if (!hostname.contains(\".\") \u0026\u0026 !hostname.equalsIgnoreCase(\n         \"localhost\")) {  // this is a symbolic name. Resolve it.\n       String hostNameProperty \u003d \"dfs.adls.\" + hostname + \".hostname\";\n       String mountPointProperty \u003d \"dfs.adls.\" + hostname + \".mountpoint\";\n       accountFQDN \u003d getNonEmptyVal(conf, hostNameProperty);\n       mountPoint \u003d getNonEmptyVal(conf, mountPointProperty);\n     } else {\n       accountFQDN \u003d hostname;\n     }\n \n     if (storeUri.getPort() \u003e 0) {\n       accountFQDN \u003d accountFQDN + \":\" + storeUri.getPort();\n     }\n \n     adlClient \u003d ADLStoreClient\n         .createClient(accountFQDN, getAccessTokenProvider(conf));\n \n     ADLStoreOptions options \u003d new ADLStoreOptions();\n     options.enableThrowingRemoteExceptions();\n \n     if (getTransportScheme().equalsIgnoreCase(INSECURE_TRANSPORT_SCHEME)) {\n       options.setInsecureTransport();\n     }\n \n     if (mountPoint !\u003d null) {\n       options.setFilePathPrefix(mountPoint);\n     }\n \n     String clusterName \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERNAME, \"UNKNOWN\");\n     String clusterType \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERTYPE, \"UNKNOWN\");\n \n     String clientVersion \u003d ADL_HADOOP_CLIENT_NAME + (StringUtils\n         .isEmpty(VersionInfo.getVersion().trim()) ?\n         ADL_HADOOP_CLIENT_VERSION.trim() :\n         VersionInfo.getVersion().trim());\n     options.setUserAgentSuffix(clientVersion + \"/\" +\n         VersionInfo.getVersion().trim() + \"/\" + clusterName + \"/\"\n         + clusterType);\n \n     adlClient.setOptions(options);\n \n     boolean trackLatency \u003d conf\n         .getBoolean(LATENCY_TRACKER_KEY, LATENCY_TRACKER_DEFAULT);\n     if (!trackLatency) {\n       LatencyTracker.disable();\n     }\n \n     boolean enableUPN \u003d conf.getBoolean(ADL_ENABLEUPN_FOR_OWNERGROUP_KEY,\n         ADL_ENABLEUPN_FOR_OWNERGROUP_DEFAULT);\n     oidOrUpn \u003d enableUPN ? UserGroupRepresentation.UPN :\n         UserGroupRepresentation.OID;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void initialize(URI storeUri, Configuration originalConf)\n      throws IOException {\n    String hostname \u003d storeUri.getHost();\n    String accountName \u003d getAccountNameFromFQDN(hostname);\n    Configuration conf \u003d propagateAccountOptions(originalConf, accountName);\n\n    super.initialize(storeUri, conf);\n    this.setConf(conf);\n    this.uri \u003d URI\n        .create(storeUri.getScheme() + \"://\" + storeUri.getAuthority());\n\n    try {\n      userName \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n    } catch (IOException e) {\n      userName \u003d \"hadoop\";\n      LOG.warn(\"Got exception when getting Hadoop user name.\"\n          + \" Set the user name to \u0027\" + userName + \"\u0027.\", e);\n    }\n\n    this.setWorkingDirectory(getHomeDirectory());\n\n    overrideOwner \u003d getConf().getBoolean(ADL_DEBUG_OVERRIDE_LOCAL_USER_AS_OWNER,\n        ADL_DEBUG_SET_LOCAL_USER_AS_OWNER_DEFAULT);\n\n    aclBitStatus \u003d conf.getBoolean(ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION,\n        ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION_DEFAULT);\n\n    String accountFQDN \u003d null;\n    String mountPoint \u003d null;\n    if (!hostname.contains(\".\") \u0026\u0026 !hostname.equalsIgnoreCase(\n        \"localhost\")) {  // this is a symbolic name. Resolve it.\n      String hostNameProperty \u003d \"dfs.adls.\" + hostname + \".hostname\";\n      String mountPointProperty \u003d \"dfs.adls.\" + hostname + \".mountpoint\";\n      accountFQDN \u003d getNonEmptyVal(conf, hostNameProperty);\n      mountPoint \u003d getNonEmptyVal(conf, mountPointProperty);\n    } else {\n      accountFQDN \u003d hostname;\n    }\n\n    if (storeUri.getPort() \u003e 0) {\n      accountFQDN \u003d accountFQDN + \":\" + storeUri.getPort();\n    }\n\n    adlClient \u003d ADLStoreClient\n        .createClient(accountFQDN, getAccessTokenProvider(conf));\n\n    ADLStoreOptions options \u003d new ADLStoreOptions();\n    options.enableThrowingRemoteExceptions();\n\n    if (getTransportScheme().equalsIgnoreCase(INSECURE_TRANSPORT_SCHEME)) {\n      options.setInsecureTransport();\n    }\n\n    if (mountPoint !\u003d null) {\n      options.setFilePathPrefix(mountPoint);\n    }\n\n    String clusterName \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERNAME, \"UNKNOWN\");\n    String clusterType \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERTYPE, \"UNKNOWN\");\n\n    String clientVersion \u003d ADL_HADOOP_CLIENT_NAME + (StringUtils\n        .isEmpty(VersionInfo.getVersion().trim()) ?\n        ADL_HADOOP_CLIENT_VERSION.trim() :\n        VersionInfo.getVersion().trim());\n    options.setUserAgentSuffix(clientVersion + \"/\" +\n        VersionInfo.getVersion().trim() + \"/\" + clusterName + \"/\"\n        + clusterType);\n\n    adlClient.setOptions(options);\n\n    boolean trackLatency \u003d conf\n        .getBoolean(LATENCY_TRACKER_KEY, LATENCY_TRACKER_DEFAULT);\n    if (!trackLatency) {\n      LatencyTracker.disable();\n    }\n\n    boolean enableUPN \u003d conf.getBoolean(ADL_ENABLEUPN_FOR_OWNERGROUP_KEY,\n        ADL_ENABLEUPN_FOR_OWNERGROUP_DEFAULT);\n    oidOrUpn \u003d enableUPN ? UserGroupRepresentation.UPN :\n        UserGroupRepresentation.OID;\n  }",
          "path": "hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlFileSystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "f735ad1b67ed82d9b11b1afd7ae39035a6aed18b": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14678. AdlFilesystem#initialize swallows exception when getting user name. Contributed by John Zhuge.\n",
      "commitDate": "28/07/17 9:46 AM",
      "commitName": "f735ad1b67ed82d9b11b1afd7ae39035a6aed18b",
      "commitAuthor": "John Zhuge",
      "commitDateOld": "07/04/17 12:04 PM",
      "commitNameOld": "56e81f2a204d5e65d29222df1b2ba4d892f9c0d5",
      "commitAuthorOld": "John Zhuge",
      "daysBetweenCommits": 111.9,
      "commitsBetweenForRepo": 556,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,77 @@\n   public void initialize(URI storeUri, Configuration conf) throws IOException {\n     super.initialize(storeUri, conf);\n     this.setConf(conf);\n     this.uri \u003d URI\n         .create(storeUri.getScheme() + \"://\" + storeUri.getAuthority());\n \n     try {\n       userName \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n     } catch (IOException e) {\n       userName \u003d \"hadoop\";\n+      LOG.warn(\"Got exception when getting Hadoop user name.\"\n+          + \" Set the user name to \u0027\" + userName + \"\u0027.\", e);\n     }\n \n     this.setWorkingDirectory(getHomeDirectory());\n \n     overrideOwner \u003d getConf().getBoolean(ADL_DEBUG_OVERRIDE_LOCAL_USER_AS_OWNER,\n         ADL_DEBUG_SET_LOCAL_USER_AS_OWNER_DEFAULT);\n \n     aclBitStatus \u003d conf.getBoolean(ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION,\n         ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION_DEFAULT);\n \n     String accountFQDN \u003d null;\n     String mountPoint \u003d null;\n     String hostname \u003d storeUri.getHost();\n     if (!hostname.contains(\".\") \u0026\u0026 !hostname.equalsIgnoreCase(\n         \"localhost\")) {  // this is a symbolic name. Resolve it.\n       String hostNameProperty \u003d \"dfs.adls.\" + hostname + \".hostname\";\n       String mountPointProperty \u003d \"dfs.adls.\" + hostname + \".mountpoint\";\n       accountFQDN \u003d getNonEmptyVal(conf, hostNameProperty);\n       mountPoint \u003d getNonEmptyVal(conf, mountPointProperty);\n     } else {\n       accountFQDN \u003d hostname;\n     }\n \n     if (storeUri.getPort() \u003e 0) {\n       accountFQDN \u003d accountFQDN + \":\" + storeUri.getPort();\n     }\n \n     adlClient \u003d ADLStoreClient\n         .createClient(accountFQDN, getAccessTokenProvider(conf));\n \n     ADLStoreOptions options \u003d new ADLStoreOptions();\n     options.enableThrowingRemoteExceptions();\n \n     if (getTransportScheme().equalsIgnoreCase(INSECURE_TRANSPORT_SCHEME)) {\n       options.setInsecureTransport();\n     }\n \n     if (mountPoint !\u003d null) {\n       options.setFilePathPrefix(mountPoint);\n     }\n \n     String clusterName \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERNAME, \"UNKNOWN\");\n     String clusterType \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERTYPE, \"UNKNOWN\");\n \n     String clientVersion \u003d ADL_HADOOP_CLIENT_NAME + (StringUtils\n         .isEmpty(VersionInfo.getVersion().trim()) ?\n         ADL_HADOOP_CLIENT_VERSION.trim() :\n         VersionInfo.getVersion().trim());\n     options.setUserAgentSuffix(clientVersion + \"/\" +\n         VersionInfo.getVersion().trim() + \"/\" + clusterName + \"/\"\n         + clusterType);\n \n     adlClient.setOptions(options);\n \n     boolean trackLatency \u003d conf\n         .getBoolean(LATENCY_TRACKER_KEY, LATENCY_TRACKER_DEFAULT);\n     if (!trackLatency) {\n       LatencyTracker.disable();\n     }\n \n     boolean enableUPN \u003d conf.getBoolean(ADL_ENABLEUPN_FOR_OWNERGROUP_KEY,\n         ADL_ENABLEUPN_FOR_OWNERGROUP_DEFAULT);\n     oidOrUpn \u003d enableUPN ? UserGroupRepresentation.UPN :\n         UserGroupRepresentation.OID;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI storeUri, Configuration conf) throws IOException {\n    super.initialize(storeUri, conf);\n    this.setConf(conf);\n    this.uri \u003d URI\n        .create(storeUri.getScheme() + \"://\" + storeUri.getAuthority());\n\n    try {\n      userName \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n    } catch (IOException e) {\n      userName \u003d \"hadoop\";\n      LOG.warn(\"Got exception when getting Hadoop user name.\"\n          + \" Set the user name to \u0027\" + userName + \"\u0027.\", e);\n    }\n\n    this.setWorkingDirectory(getHomeDirectory());\n\n    overrideOwner \u003d getConf().getBoolean(ADL_DEBUG_OVERRIDE_LOCAL_USER_AS_OWNER,\n        ADL_DEBUG_SET_LOCAL_USER_AS_OWNER_DEFAULT);\n\n    aclBitStatus \u003d conf.getBoolean(ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION,\n        ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION_DEFAULT);\n\n    String accountFQDN \u003d null;\n    String mountPoint \u003d null;\n    String hostname \u003d storeUri.getHost();\n    if (!hostname.contains(\".\") \u0026\u0026 !hostname.equalsIgnoreCase(\n        \"localhost\")) {  // this is a symbolic name. Resolve it.\n      String hostNameProperty \u003d \"dfs.adls.\" + hostname + \".hostname\";\n      String mountPointProperty \u003d \"dfs.adls.\" + hostname + \".mountpoint\";\n      accountFQDN \u003d getNonEmptyVal(conf, hostNameProperty);\n      mountPoint \u003d getNonEmptyVal(conf, mountPointProperty);\n    } else {\n      accountFQDN \u003d hostname;\n    }\n\n    if (storeUri.getPort() \u003e 0) {\n      accountFQDN \u003d accountFQDN + \":\" + storeUri.getPort();\n    }\n\n    adlClient \u003d ADLStoreClient\n        .createClient(accountFQDN, getAccessTokenProvider(conf));\n\n    ADLStoreOptions options \u003d new ADLStoreOptions();\n    options.enableThrowingRemoteExceptions();\n\n    if (getTransportScheme().equalsIgnoreCase(INSECURE_TRANSPORT_SCHEME)) {\n      options.setInsecureTransport();\n    }\n\n    if (mountPoint !\u003d null) {\n      options.setFilePathPrefix(mountPoint);\n    }\n\n    String clusterName \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERNAME, \"UNKNOWN\");\n    String clusterType \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERTYPE, \"UNKNOWN\");\n\n    String clientVersion \u003d ADL_HADOOP_CLIENT_NAME + (StringUtils\n        .isEmpty(VersionInfo.getVersion().trim()) ?\n        ADL_HADOOP_CLIENT_VERSION.trim() :\n        VersionInfo.getVersion().trim());\n    options.setUserAgentSuffix(clientVersion + \"/\" +\n        VersionInfo.getVersion().trim() + \"/\" + clusterName + \"/\"\n        + clusterType);\n\n    adlClient.setOptions(options);\n\n    boolean trackLatency \u003d conf\n        .getBoolean(LATENCY_TRACKER_KEY, LATENCY_TRACKER_DEFAULT);\n    if (!trackLatency) {\n      LatencyTracker.disable();\n    }\n\n    boolean enableUPN \u003d conf.getBoolean(ADL_ENABLEUPN_FOR_OWNERGROUP_KEY,\n        ADL_ENABLEUPN_FOR_OWNERGROUP_DEFAULT);\n    oidOrUpn \u003d enableUPN ? UserGroupRepresentation.UPN :\n        UserGroupRepresentation.OID;\n  }",
      "path": "hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlFileSystem.java",
      "extendedDetails": {}
    },
    "924def78544a64449785f305cb6984c3559aea4d": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14017. User friendly name for ADLS user and group. Contributed by Vishwajeet Dusane\n",
      "commitDate": "21/02/17 1:44 PM",
      "commitName": "924def78544a64449785f305cb6984c3559aea4d",
      "commitAuthor": "Mingliang Liu",
      "commitDateOld": "16/02/17 3:14 PM",
      "commitNameOld": "f4329990250bed62efdebe3ce2bc740092cf9573",
      "commitAuthorOld": "Mingliang Liu",
      "daysBetweenCommits": 4.94,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,70 +1,75 @@\n   public void initialize(URI storeUri, Configuration conf) throws IOException {\n     super.initialize(storeUri, conf);\n     this.setConf(conf);\n     this.uri \u003d URI\n         .create(storeUri.getScheme() + \"://\" + storeUri.getAuthority());\n \n     try {\n       userName \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n     } catch (IOException e) {\n       userName \u003d \"hadoop\";\n     }\n \n     this.setWorkingDirectory(getHomeDirectory());\n \n     overrideOwner \u003d getConf().getBoolean(ADL_DEBUG_OVERRIDE_LOCAL_USER_AS_OWNER,\n         ADL_DEBUG_SET_LOCAL_USER_AS_OWNER_DEFAULT);\n \n     aclBitStatus \u003d conf.getBoolean(ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION,\n         ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION_DEFAULT);\n \n     String accountFQDN \u003d null;\n     String mountPoint \u003d null;\n     String hostname \u003d storeUri.getHost();\n     if (!hostname.contains(\".\") \u0026\u0026 !hostname.equalsIgnoreCase(\n         \"localhost\")) {  // this is a symbolic name. Resolve it.\n       String hostNameProperty \u003d \"dfs.adls.\" + hostname + \".hostname\";\n       String mountPointProperty \u003d \"dfs.adls.\" + hostname + \".mountpoint\";\n       accountFQDN \u003d getNonEmptyVal(conf, hostNameProperty);\n       mountPoint \u003d getNonEmptyVal(conf, mountPointProperty);\n     } else {\n       accountFQDN \u003d hostname;\n     }\n \n     if (storeUri.getPort() \u003e 0) {\n       accountFQDN \u003d accountFQDN + \":\" + storeUri.getPort();\n     }\n \n     adlClient \u003d ADLStoreClient\n         .createClient(accountFQDN, getAccessTokenProvider(conf));\n \n     ADLStoreOptions options \u003d new ADLStoreOptions();\n     options.enableThrowingRemoteExceptions();\n \n     if (getTransportScheme().equalsIgnoreCase(INSECURE_TRANSPORT_SCHEME)) {\n       options.setInsecureTransport();\n     }\n \n     if (mountPoint !\u003d null) {\n       options.setFilePathPrefix(mountPoint);\n     }\n \n     String clusterName \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERNAME, \"UNKNOWN\");\n     String clusterType \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERTYPE, \"UNKNOWN\");\n \n     String clientVersion \u003d ADL_HADOOP_CLIENT_NAME + (StringUtils\n         .isEmpty(VersionInfo.getVersion().trim()) ?\n         ADL_HADOOP_CLIENT_VERSION.trim() :\n         VersionInfo.getVersion().trim());\n     options.setUserAgentSuffix(clientVersion + \"/\" +\n         VersionInfo.getVersion().trim() + \"/\" + clusterName + \"/\"\n         + clusterType);\n \n     adlClient.setOptions(options);\n \n     boolean trackLatency \u003d conf\n         .getBoolean(LATENCY_TRACKER_KEY, LATENCY_TRACKER_DEFAULT);\n     if (!trackLatency) {\n       LatencyTracker.disable();\n     }\n+\n+    boolean enableUPN \u003d conf.getBoolean(ADL_ENABLEUPN_FOR_OWNERGROUP_KEY,\n+        ADL_ENABLEUPN_FOR_OWNERGROUP_DEFAULT);\n+    oidOrUpn \u003d enableUPN ? UserGroupRepresentation.UPN :\n+        UserGroupRepresentation.OID;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI storeUri, Configuration conf) throws IOException {\n    super.initialize(storeUri, conf);\n    this.setConf(conf);\n    this.uri \u003d URI\n        .create(storeUri.getScheme() + \"://\" + storeUri.getAuthority());\n\n    try {\n      userName \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n    } catch (IOException e) {\n      userName \u003d \"hadoop\";\n    }\n\n    this.setWorkingDirectory(getHomeDirectory());\n\n    overrideOwner \u003d getConf().getBoolean(ADL_DEBUG_OVERRIDE_LOCAL_USER_AS_OWNER,\n        ADL_DEBUG_SET_LOCAL_USER_AS_OWNER_DEFAULT);\n\n    aclBitStatus \u003d conf.getBoolean(ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION,\n        ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION_DEFAULT);\n\n    String accountFQDN \u003d null;\n    String mountPoint \u003d null;\n    String hostname \u003d storeUri.getHost();\n    if (!hostname.contains(\".\") \u0026\u0026 !hostname.equalsIgnoreCase(\n        \"localhost\")) {  // this is a symbolic name. Resolve it.\n      String hostNameProperty \u003d \"dfs.adls.\" + hostname + \".hostname\";\n      String mountPointProperty \u003d \"dfs.adls.\" + hostname + \".mountpoint\";\n      accountFQDN \u003d getNonEmptyVal(conf, hostNameProperty);\n      mountPoint \u003d getNonEmptyVal(conf, mountPointProperty);\n    } else {\n      accountFQDN \u003d hostname;\n    }\n\n    if (storeUri.getPort() \u003e 0) {\n      accountFQDN \u003d accountFQDN + \":\" + storeUri.getPort();\n    }\n\n    adlClient \u003d ADLStoreClient\n        .createClient(accountFQDN, getAccessTokenProvider(conf));\n\n    ADLStoreOptions options \u003d new ADLStoreOptions();\n    options.enableThrowingRemoteExceptions();\n\n    if (getTransportScheme().equalsIgnoreCase(INSECURE_TRANSPORT_SCHEME)) {\n      options.setInsecureTransport();\n    }\n\n    if (mountPoint !\u003d null) {\n      options.setFilePathPrefix(mountPoint);\n    }\n\n    String clusterName \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERNAME, \"UNKNOWN\");\n    String clusterType \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERTYPE, \"UNKNOWN\");\n\n    String clientVersion \u003d ADL_HADOOP_CLIENT_NAME + (StringUtils\n        .isEmpty(VersionInfo.getVersion().trim()) ?\n        ADL_HADOOP_CLIENT_VERSION.trim() :\n        VersionInfo.getVersion().trim());\n    options.setUserAgentSuffix(clientVersion + \"/\" +\n        VersionInfo.getVersion().trim() + \"/\" + clusterName + \"/\"\n        + clusterType);\n\n    adlClient.setOptions(options);\n\n    boolean trackLatency \u003d conf\n        .getBoolean(LATENCY_TRACKER_KEY, LATENCY_TRACKER_DEFAULT);\n    if (!trackLatency) {\n      LatencyTracker.disable();\n    }\n\n    boolean enableUPN \u003d conf.getBoolean(ADL_ENABLEUPN_FOR_OWNERGROUP_KEY,\n        ADL_ENABLEUPN_FOR_OWNERGROUP_DEFAULT);\n    oidOrUpn \u003d enableUPN ? UserGroupRepresentation.UPN :\n        UserGroupRepresentation.OID;\n  }",
      "path": "hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlFileSystem.java",
      "extendedDetails": {}
    },
    "5c61ad24887f76dfc5a5935b2c5dceb6bfd99417": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-13037. Refactor Azure Data Lake Store as an independent FileSystem. Contributed by Vishwajeet Dusane\n",
      "commitDate": "11/11/16 11:15 AM",
      "commitName": "5c61ad24887f76dfc5a5935b2c5dceb6bfd99417",
      "commitAuthor": "Chris Douglas",
      "diff": "@@ -0,0 +1,70 @@\n+  public void initialize(URI storeUri, Configuration conf) throws IOException {\n+    super.initialize(storeUri, conf);\n+    this.setConf(conf);\n+    this.uri \u003d URI\n+        .create(storeUri.getScheme() + \"://\" + storeUri.getAuthority());\n+\n+    try {\n+      userName \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n+    } catch (IOException e) {\n+      userName \u003d \"hadoop\";\n+    }\n+\n+    this.setWorkingDirectory(getHomeDirectory());\n+\n+    overrideOwner \u003d getConf().getBoolean(ADL_DEBUG_OVERRIDE_LOCAL_USER_AS_OWNER,\n+        ADL_DEBUG_SET_LOCAL_USER_AS_OWNER_DEFAULT);\n+\n+    aclBitStatus \u003d conf.getBoolean(ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION,\n+        ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION_DEFAULT);\n+\n+    String accountFQDN \u003d null;\n+    String mountPoint \u003d null;\n+    String hostname \u003d storeUri.getHost();\n+    if (!hostname.contains(\".\") \u0026\u0026 !hostname.equalsIgnoreCase(\n+        \"localhost\")) {  // this is a symbolic name. Resolve it.\n+      String hostNameProperty \u003d \"dfs.adls.\" + hostname + \".hostname\";\n+      String mountPointProperty \u003d \"dfs.adls.\" + hostname + \".mountpoint\";\n+      accountFQDN \u003d getNonEmptyVal(conf, hostNameProperty);\n+      mountPoint \u003d getNonEmptyVal(conf, mountPointProperty);\n+    } else {\n+      accountFQDN \u003d hostname;\n+    }\n+\n+    if (storeUri.getPort() \u003e 0) {\n+      accountFQDN \u003d accountFQDN + \":\" + storeUri.getPort();\n+    }\n+\n+    adlClient \u003d ADLStoreClient\n+        .createClient(accountFQDN, getAccessTokenProvider(conf));\n+\n+    ADLStoreOptions options \u003d new ADLStoreOptions();\n+    options.enableThrowingRemoteExceptions();\n+\n+    if (getTransportScheme().equalsIgnoreCase(INSECURE_TRANSPORT_SCHEME)) {\n+      options.setInsecureTransport();\n+    }\n+\n+    if (mountPoint !\u003d null) {\n+      options.setFilePathPrefix(mountPoint);\n+    }\n+\n+    String clusterName \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERNAME, \"UNKNOWN\");\n+    String clusterType \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERTYPE, \"UNKNOWN\");\n+\n+    String clientVersion \u003d ADL_HADOOP_CLIENT_NAME + (StringUtils\n+        .isEmpty(VersionInfo.getVersion().trim()) ?\n+        ADL_HADOOP_CLIENT_VERSION.trim() :\n+        VersionInfo.getVersion().trim());\n+    options.setUserAgentSuffix(clientVersion + \"/\" +\n+        VersionInfo.getVersion().trim() + \"/\" + clusterName + \"/\"\n+        + clusterType);\n+\n+    adlClient.setOptions(options);\n+\n+    boolean trackLatency \u003d conf\n+        .getBoolean(LATENCY_TRACKER_KEY, LATENCY_TRACKER_DEFAULT);\n+    if (!trackLatency) {\n+      LatencyTracker.disable();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI storeUri, Configuration conf) throws IOException {\n    super.initialize(storeUri, conf);\n    this.setConf(conf);\n    this.uri \u003d URI\n        .create(storeUri.getScheme() + \"://\" + storeUri.getAuthority());\n\n    try {\n      userName \u003d UserGroupInformation.getCurrentUser().getShortUserName();\n    } catch (IOException e) {\n      userName \u003d \"hadoop\";\n    }\n\n    this.setWorkingDirectory(getHomeDirectory());\n\n    overrideOwner \u003d getConf().getBoolean(ADL_DEBUG_OVERRIDE_LOCAL_USER_AS_OWNER,\n        ADL_DEBUG_SET_LOCAL_USER_AS_OWNER_DEFAULT);\n\n    aclBitStatus \u003d conf.getBoolean(ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION,\n        ADL_SUPPORT_ACL_BIT_IN_FSPERMISSION_DEFAULT);\n\n    String accountFQDN \u003d null;\n    String mountPoint \u003d null;\n    String hostname \u003d storeUri.getHost();\n    if (!hostname.contains(\".\") \u0026\u0026 !hostname.equalsIgnoreCase(\n        \"localhost\")) {  // this is a symbolic name. Resolve it.\n      String hostNameProperty \u003d \"dfs.adls.\" + hostname + \".hostname\";\n      String mountPointProperty \u003d \"dfs.adls.\" + hostname + \".mountpoint\";\n      accountFQDN \u003d getNonEmptyVal(conf, hostNameProperty);\n      mountPoint \u003d getNonEmptyVal(conf, mountPointProperty);\n    } else {\n      accountFQDN \u003d hostname;\n    }\n\n    if (storeUri.getPort() \u003e 0) {\n      accountFQDN \u003d accountFQDN + \":\" + storeUri.getPort();\n    }\n\n    adlClient \u003d ADLStoreClient\n        .createClient(accountFQDN, getAccessTokenProvider(conf));\n\n    ADLStoreOptions options \u003d new ADLStoreOptions();\n    options.enableThrowingRemoteExceptions();\n\n    if (getTransportScheme().equalsIgnoreCase(INSECURE_TRANSPORT_SCHEME)) {\n      options.setInsecureTransport();\n    }\n\n    if (mountPoint !\u003d null) {\n      options.setFilePathPrefix(mountPoint);\n    }\n\n    String clusterName \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERNAME, \"UNKNOWN\");\n    String clusterType \u003d conf.get(ADL_EVENTS_TRACKING_CLUSTERTYPE, \"UNKNOWN\");\n\n    String clientVersion \u003d ADL_HADOOP_CLIENT_NAME + (StringUtils\n        .isEmpty(VersionInfo.getVersion().trim()) ?\n        ADL_HADOOP_CLIENT_VERSION.trim() :\n        VersionInfo.getVersion().trim());\n    options.setUserAgentSuffix(clientVersion + \"/\" +\n        VersionInfo.getVersion().trim() + \"/\" + clusterName + \"/\"\n        + clusterType);\n\n    adlClient.setOptions(options);\n\n    boolean trackLatency \u003d conf\n        .getBoolean(LATENCY_TRACKER_KEY, LATENCY_TRACKER_DEFAULT);\n    if (!trackLatency) {\n      LatencyTracker.disable();\n    }\n  }",
      "path": "hadoop-tools/hadoop-azure-datalake/src/main/java/org/apache/hadoop/fs/adl/AdlFileSystem.java"
    }
  }
}