{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FileOutputCommitter.java",
  "functionName": "commitJobInternal",
  "functionId": "commitJobInternal___context-JobContext",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
  "functionStartLine": 397,
  "functionEndLine": 445,
  "numCommitsSeen": 38,
  "timeTaken": 7451,
  "changeHistory": [
    "c0a0c353e86fc3f3d4eedd91aeb9d5b9c8eb748a",
    "4d8de7ab690ef919b392b12d856482a6a1f2bb3d",
    "6502d59e73cd6f3f3a358fce58d398ca38a61fba",
    "372ad270a0d7ea5c581cd9a42b3c3cb189eca204",
    "aa92b764a7ddb888d097121c4d610089a0053d11",
    "94242c93857a06fb9c56ee571a47d6ca18f00f48",
    "c7fb49b3c5b8aa394f578e644bbc13fc9cbfcaca",
    "f29bfa0e0e88de49f1558138eeaba1396fa5a1d2",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "c0a0c353e86fc3f3d4eedd91aeb9d5b9c8eb748a": "Ybodychange",
    "4d8de7ab690ef919b392b12d856482a6a1f2bb3d": "Ybodychange",
    "6502d59e73cd6f3f3a358fce58d398ca38a61fba": "Ymultichange(Yrename,Ymodifierchange,Ybodychange)",
    "372ad270a0d7ea5c581cd9a42b3c3cb189eca204": "Ybodychange",
    "aa92b764a7ddb888d097121c4d610089a0053d11": "Ybodychange",
    "94242c93857a06fb9c56ee571a47d6ca18f00f48": "Ybodychange",
    "c7fb49b3c5b8aa394f578e644bbc13fc9cbfcaca": "Ybodychange",
    "f29bfa0e0e88de49f1558138eeaba1396fa5a1d2": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c0a0c353e86fc3f3d4eedd91aeb9d5b9c8eb748a": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6973. Fix comments on creating _SUCCESS file.\n\nThis closes #280\n\nSigned-off-by: Akira Ajisaka \u003caajisaka@apache.org\u003e\n",
      "commitDate": "26/07/19 5:21 AM",
      "commitName": "c0a0c353e86fc3f3d4eedd91aeb9d5b9c8eb748a",
      "commitAuthor": "Mehul Garnara",
      "commitDateOld": "28/11/18 12:54 PM",
      "commitNameOld": "4d8de7ab690ef919b392b12d856482a6a1f2bb3d",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 239.64,
      "commitsBetweenForRepo": 1712,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,49 @@\n   protected void commitJobInternal(JobContext context) throws IOException {\n     if (hasOutputPath()) {\n       Path finalOutput \u003d getOutputPath();\n       FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n \n       if (algorithmVersion \u003d\u003d 1) {\n         for (FileStatus stat: getAllCommittedTaskPaths(context)) {\n           mergePaths(fs, stat, finalOutput, context);\n         }\n       }\n \n       if (skipCleanup) {\n         LOG.info(\"Skip cleanup the _temporary folders under job\u0027s output \" +\n             \"directory in commitJob.\");\n       } else {\n-        // delete the _temporary folder and create a _done file in the o/p\n+        // delete the _temporary folder and create a _SUCCESS file in the o/p\n         // folder\n         try {\n           cleanupJob(context);\n         } catch (IOException e) {\n           if (ignoreCleanupFailures) {\n             // swallow exceptions in cleanup as user configure to make sure\n             // commitJob could be success even when cleanup get failure.\n             LOG.error(\"Error in cleanup job, manually cleanup is needed.\", e);\n           } else {\n             // throw back exception to fail commitJob.\n             throw e;\n           }\n         }\n       }\n       // True if the job requires output.dir marked on successful job.\n       // Note that by default it is set to true.\n+      // Create a _SUCCESS file in the o/p folder.\n       if (context.getConfiguration().getBoolean(\n           SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n         Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n         // If job commit is repeatable and previous/another AM could write\n         // mark file already, we need to set overwritten to be true explicitly\n         // in case other FS implementations don\u0027t overwritten by default.\n         if (isCommitJobRepeatable(context)) {\n           fs.create(markerPath, true).close();\n         } else {\n           fs.create(markerPath).close();\n         }\n       }\n     } else {\n       LOG.warn(\"Output Path is null in commitJob()\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void commitJobInternal(JobContext context) throws IOException {\n    if (hasOutputPath()) {\n      Path finalOutput \u003d getOutputPath();\n      FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n\n      if (algorithmVersion \u003d\u003d 1) {\n        for (FileStatus stat: getAllCommittedTaskPaths(context)) {\n          mergePaths(fs, stat, finalOutput, context);\n        }\n      }\n\n      if (skipCleanup) {\n        LOG.info(\"Skip cleanup the _temporary folders under job\u0027s output \" +\n            \"directory in commitJob.\");\n      } else {\n        // delete the _temporary folder and create a _SUCCESS file in the o/p\n        // folder\n        try {\n          cleanupJob(context);\n        } catch (IOException e) {\n          if (ignoreCleanupFailures) {\n            // swallow exceptions in cleanup as user configure to make sure\n            // commitJob could be success even when cleanup get failure.\n            LOG.error(\"Error in cleanup job, manually cleanup is needed.\", e);\n          } else {\n            // throw back exception to fail commitJob.\n            throw e;\n          }\n        }\n      }\n      // True if the job requires output.dir marked on successful job.\n      // Note that by default it is set to true.\n      // Create a _SUCCESS file in the o/p folder.\n      if (context.getConfiguration().getBoolean(\n          SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n        Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n        // If job commit is repeatable and previous/another AM could write\n        // mark file already, we need to set overwritten to be true explicitly\n        // in case other FS implementations don\u0027t overwritten by default.\n        if (isCommitJobRepeatable(context)) {\n          fs.create(markerPath, true).close();\n        } else {\n          fs.create(markerPath).close();\n        }\n      }\n    } else {\n      LOG.warn(\"Output Path is null in commitJob()\");\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
      "extendedDetails": {}
    },
    "4d8de7ab690ef919b392b12d856482a6a1f2bb3d": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-7164. FileOutputCommitter does not report progress while merging paths. Contributed by Kuhu Shukla\n",
      "commitDate": "28/11/18 12:54 PM",
      "commitName": "4d8de7ab690ef919b392b12d856482a6a1f2bb3d",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "17/01/18 6:14 AM",
      "commitNameOld": "6e42d058292d9656e9ebc9a47be13280e3c919ea",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 315.28,
      "commitsBetweenForRepo": 3035,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,48 @@\n   protected void commitJobInternal(JobContext context) throws IOException {\n     if (hasOutputPath()) {\n       Path finalOutput \u003d getOutputPath();\n       FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n \n       if (algorithmVersion \u003d\u003d 1) {\n         for (FileStatus stat: getAllCommittedTaskPaths(context)) {\n-          mergePaths(fs, stat, finalOutput);\n+          mergePaths(fs, stat, finalOutput, context);\n         }\n       }\n \n       if (skipCleanup) {\n         LOG.info(\"Skip cleanup the _temporary folders under job\u0027s output \" +\n             \"directory in commitJob.\");\n       } else {\n         // delete the _temporary folder and create a _done file in the o/p\n         // folder\n         try {\n           cleanupJob(context);\n         } catch (IOException e) {\n           if (ignoreCleanupFailures) {\n             // swallow exceptions in cleanup as user configure to make sure\n             // commitJob could be success even when cleanup get failure.\n             LOG.error(\"Error in cleanup job, manually cleanup is needed.\", e);\n           } else {\n             // throw back exception to fail commitJob.\n             throw e;\n           }\n         }\n       }\n       // True if the job requires output.dir marked on successful job.\n       // Note that by default it is set to true.\n       if (context.getConfiguration().getBoolean(\n           SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n         Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n         // If job commit is repeatable and previous/another AM could write\n         // mark file already, we need to set overwritten to be true explicitly\n         // in case other FS implementations don\u0027t overwritten by default.\n         if (isCommitJobRepeatable(context)) {\n           fs.create(markerPath, true).close();\n         } else {\n           fs.create(markerPath).close();\n         }\n       }\n     } else {\n       LOG.warn(\"Output Path is null in commitJob()\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void commitJobInternal(JobContext context) throws IOException {\n    if (hasOutputPath()) {\n      Path finalOutput \u003d getOutputPath();\n      FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n\n      if (algorithmVersion \u003d\u003d 1) {\n        for (FileStatus stat: getAllCommittedTaskPaths(context)) {\n          mergePaths(fs, stat, finalOutput, context);\n        }\n      }\n\n      if (skipCleanup) {\n        LOG.info(\"Skip cleanup the _temporary folders under job\u0027s output \" +\n            \"directory in commitJob.\");\n      } else {\n        // delete the _temporary folder and create a _done file in the o/p\n        // folder\n        try {\n          cleanupJob(context);\n        } catch (IOException e) {\n          if (ignoreCleanupFailures) {\n            // swallow exceptions in cleanup as user configure to make sure\n            // commitJob could be success even when cleanup get failure.\n            LOG.error(\"Error in cleanup job, manually cleanup is needed.\", e);\n          } else {\n            // throw back exception to fail commitJob.\n            throw e;\n          }\n        }\n      }\n      // True if the job requires output.dir marked on successful job.\n      // Note that by default it is set to true.\n      if (context.getConfiguration().getBoolean(\n          SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n        Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n        // If job commit is repeatable and previous/another AM could write\n        // mark file already, we need to set overwritten to be true explicitly\n        // in case other FS implementations don\u0027t overwritten by default.\n        if (isCommitJobRepeatable(context)) {\n          fs.create(markerPath, true).close();\n        } else {\n          fs.create(markerPath).close();\n        }\n      }\n    } else {\n      LOG.warn(\"Output Path is null in commitJob()\");\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
      "extendedDetails": {}
    },
    "6502d59e73cd6f3f3a358fce58d398ca38a61fba": {
      "type": "Ymultichange(Yrename,Ymodifierchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-5485. Allow repeating job commit by extending OutputCommitter API. Contributed by Junping Du\n",
      "commitDate": "16/11/15 5:06 PM",
      "commitName": "6502d59e73cd6f3f3a358fce58d398ca38a61fba",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "MAPREDUCE-5485. Allow repeating job commit by extending OutputCommitter API. Contributed by Junping Du\n",
          "commitDate": "16/11/15 5:06 PM",
          "commitName": "6502d59e73cd6f3f3a358fce58d398ca38a61fba",
          "commitAuthor": "Jian He",
          "commitDateOld": "18/09/15 10:10 AM",
          "commitNameOld": "372ad270a0d7ea5c581cd9a42b3c3cb189eca204",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 59.33,
          "commitsBetweenForRepo": 452,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,40 +1,48 @@\n-  public void commitJob(JobContext context) throws IOException {\n+  protected void commitJobInternal(JobContext context) throws IOException {\n     if (hasOutputPath()) {\n       Path finalOutput \u003d getOutputPath();\n       FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n \n       if (algorithmVersion \u003d\u003d 1) {\n         for (FileStatus stat: getAllCommittedTaskPaths(context)) {\n           mergePaths(fs, stat, finalOutput);\n         }\n       }\n \n       if (skipCleanup) {\n         LOG.info(\"Skip cleanup the _temporary folders under job\u0027s output \" +\n             \"directory in commitJob.\");\n       } else {\n         // delete the _temporary folder and create a _done file in the o/p\n         // folder\n         try {\n           cleanupJob(context);\n         } catch (IOException e) {\n           if (ignoreCleanupFailures) {\n             // swallow exceptions in cleanup as user configure to make sure\n             // commitJob could be success even when cleanup get failure.\n             LOG.error(\"Error in cleanup job, manually cleanup is needed.\", e);\n           } else {\n             // throw back exception to fail commitJob.\n             throw e;\n           }\n         }\n       }\n       // True if the job requires output.dir marked on successful job.\n       // Note that by default it is set to true.\n-      if (context.getConfiguration().getBoolean(SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n+      if (context.getConfiguration().getBoolean(\n+          SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n         Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n-        fs.create(markerPath).close();\n+        // If job commit is repeatable and previous/another AM could write\n+        // mark file already, we need to set overwritten to be true explicitly\n+        // in case other FS implementations don\u0027t overwritten by default.\n+        if (isCommitJobRepeatable(context)) {\n+          fs.create(markerPath, true).close();\n+        } else {\n+          fs.create(markerPath).close();\n+        }\n       }\n     } else {\n       LOG.warn(\"Output Path is null in commitJob()\");\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void commitJobInternal(JobContext context) throws IOException {\n    if (hasOutputPath()) {\n      Path finalOutput \u003d getOutputPath();\n      FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n\n      if (algorithmVersion \u003d\u003d 1) {\n        for (FileStatus stat: getAllCommittedTaskPaths(context)) {\n          mergePaths(fs, stat, finalOutput);\n        }\n      }\n\n      if (skipCleanup) {\n        LOG.info(\"Skip cleanup the _temporary folders under job\u0027s output \" +\n            \"directory in commitJob.\");\n      } else {\n        // delete the _temporary folder and create a _done file in the o/p\n        // folder\n        try {\n          cleanupJob(context);\n        } catch (IOException e) {\n          if (ignoreCleanupFailures) {\n            // swallow exceptions in cleanup as user configure to make sure\n            // commitJob could be success even when cleanup get failure.\n            LOG.error(\"Error in cleanup job, manually cleanup is needed.\", e);\n          } else {\n            // throw back exception to fail commitJob.\n            throw e;\n          }\n        }\n      }\n      // True if the job requires output.dir marked on successful job.\n      // Note that by default it is set to true.\n      if (context.getConfiguration().getBoolean(\n          SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n        Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n        // If job commit is repeatable and previous/another AM could write\n        // mark file already, we need to set overwritten to be true explicitly\n        // in case other FS implementations don\u0027t overwritten by default.\n        if (isCommitJobRepeatable(context)) {\n          fs.create(markerPath, true).close();\n        } else {\n          fs.create(markerPath).close();\n        }\n      }\n    } else {\n      LOG.warn(\"Output Path is null in commitJob()\");\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
          "extendedDetails": {
            "oldValue": "commitJob",
            "newValue": "commitJobInternal"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "MAPREDUCE-5485. Allow repeating job commit by extending OutputCommitter API. Contributed by Junping Du\n",
          "commitDate": "16/11/15 5:06 PM",
          "commitName": "6502d59e73cd6f3f3a358fce58d398ca38a61fba",
          "commitAuthor": "Jian He",
          "commitDateOld": "18/09/15 10:10 AM",
          "commitNameOld": "372ad270a0d7ea5c581cd9a42b3c3cb189eca204",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 59.33,
          "commitsBetweenForRepo": 452,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,40 +1,48 @@\n-  public void commitJob(JobContext context) throws IOException {\n+  protected void commitJobInternal(JobContext context) throws IOException {\n     if (hasOutputPath()) {\n       Path finalOutput \u003d getOutputPath();\n       FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n \n       if (algorithmVersion \u003d\u003d 1) {\n         for (FileStatus stat: getAllCommittedTaskPaths(context)) {\n           mergePaths(fs, stat, finalOutput);\n         }\n       }\n \n       if (skipCleanup) {\n         LOG.info(\"Skip cleanup the _temporary folders under job\u0027s output \" +\n             \"directory in commitJob.\");\n       } else {\n         // delete the _temporary folder and create a _done file in the o/p\n         // folder\n         try {\n           cleanupJob(context);\n         } catch (IOException e) {\n           if (ignoreCleanupFailures) {\n             // swallow exceptions in cleanup as user configure to make sure\n             // commitJob could be success even when cleanup get failure.\n             LOG.error(\"Error in cleanup job, manually cleanup is needed.\", e);\n           } else {\n             // throw back exception to fail commitJob.\n             throw e;\n           }\n         }\n       }\n       // True if the job requires output.dir marked on successful job.\n       // Note that by default it is set to true.\n-      if (context.getConfiguration().getBoolean(SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n+      if (context.getConfiguration().getBoolean(\n+          SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n         Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n-        fs.create(markerPath).close();\n+        // If job commit is repeatable and previous/another AM could write\n+        // mark file already, we need to set overwritten to be true explicitly\n+        // in case other FS implementations don\u0027t overwritten by default.\n+        if (isCommitJobRepeatable(context)) {\n+          fs.create(markerPath, true).close();\n+        } else {\n+          fs.create(markerPath).close();\n+        }\n       }\n     } else {\n       LOG.warn(\"Output Path is null in commitJob()\");\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void commitJobInternal(JobContext context) throws IOException {\n    if (hasOutputPath()) {\n      Path finalOutput \u003d getOutputPath();\n      FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n\n      if (algorithmVersion \u003d\u003d 1) {\n        for (FileStatus stat: getAllCommittedTaskPaths(context)) {\n          mergePaths(fs, stat, finalOutput);\n        }\n      }\n\n      if (skipCleanup) {\n        LOG.info(\"Skip cleanup the _temporary folders under job\u0027s output \" +\n            \"directory in commitJob.\");\n      } else {\n        // delete the _temporary folder and create a _done file in the o/p\n        // folder\n        try {\n          cleanupJob(context);\n        } catch (IOException e) {\n          if (ignoreCleanupFailures) {\n            // swallow exceptions in cleanup as user configure to make sure\n            // commitJob could be success even when cleanup get failure.\n            LOG.error(\"Error in cleanup job, manually cleanup is needed.\", e);\n          } else {\n            // throw back exception to fail commitJob.\n            throw e;\n          }\n        }\n      }\n      // True if the job requires output.dir marked on successful job.\n      // Note that by default it is set to true.\n      if (context.getConfiguration().getBoolean(\n          SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n        Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n        // If job commit is repeatable and previous/another AM could write\n        // mark file already, we need to set overwritten to be true explicitly\n        // in case other FS implementations don\u0027t overwritten by default.\n        if (isCommitJobRepeatable(context)) {\n          fs.create(markerPath, true).close();\n        } else {\n          fs.create(markerPath).close();\n        }\n      }\n    } else {\n      LOG.warn(\"Output Path is null in commitJob()\");\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[protected]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-5485. Allow repeating job commit by extending OutputCommitter API. Contributed by Junping Du\n",
          "commitDate": "16/11/15 5:06 PM",
          "commitName": "6502d59e73cd6f3f3a358fce58d398ca38a61fba",
          "commitAuthor": "Jian He",
          "commitDateOld": "18/09/15 10:10 AM",
          "commitNameOld": "372ad270a0d7ea5c581cd9a42b3c3cb189eca204",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 59.33,
          "commitsBetweenForRepo": 452,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,40 +1,48 @@\n-  public void commitJob(JobContext context) throws IOException {\n+  protected void commitJobInternal(JobContext context) throws IOException {\n     if (hasOutputPath()) {\n       Path finalOutput \u003d getOutputPath();\n       FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n \n       if (algorithmVersion \u003d\u003d 1) {\n         for (FileStatus stat: getAllCommittedTaskPaths(context)) {\n           mergePaths(fs, stat, finalOutput);\n         }\n       }\n \n       if (skipCleanup) {\n         LOG.info(\"Skip cleanup the _temporary folders under job\u0027s output \" +\n             \"directory in commitJob.\");\n       } else {\n         // delete the _temporary folder and create a _done file in the o/p\n         // folder\n         try {\n           cleanupJob(context);\n         } catch (IOException e) {\n           if (ignoreCleanupFailures) {\n             // swallow exceptions in cleanup as user configure to make sure\n             // commitJob could be success even when cleanup get failure.\n             LOG.error(\"Error in cleanup job, manually cleanup is needed.\", e);\n           } else {\n             // throw back exception to fail commitJob.\n             throw e;\n           }\n         }\n       }\n       // True if the job requires output.dir marked on successful job.\n       // Note that by default it is set to true.\n-      if (context.getConfiguration().getBoolean(SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n+      if (context.getConfiguration().getBoolean(\n+          SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n         Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n-        fs.create(markerPath).close();\n+        // If job commit is repeatable and previous/another AM could write\n+        // mark file already, we need to set overwritten to be true explicitly\n+        // in case other FS implementations don\u0027t overwritten by default.\n+        if (isCommitJobRepeatable(context)) {\n+          fs.create(markerPath, true).close();\n+        } else {\n+          fs.create(markerPath).close();\n+        }\n       }\n     } else {\n       LOG.warn(\"Output Path is null in commitJob()\");\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void commitJobInternal(JobContext context) throws IOException {\n    if (hasOutputPath()) {\n      Path finalOutput \u003d getOutputPath();\n      FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n\n      if (algorithmVersion \u003d\u003d 1) {\n        for (FileStatus stat: getAllCommittedTaskPaths(context)) {\n          mergePaths(fs, stat, finalOutput);\n        }\n      }\n\n      if (skipCleanup) {\n        LOG.info(\"Skip cleanup the _temporary folders under job\u0027s output \" +\n            \"directory in commitJob.\");\n      } else {\n        // delete the _temporary folder and create a _done file in the o/p\n        // folder\n        try {\n          cleanupJob(context);\n        } catch (IOException e) {\n          if (ignoreCleanupFailures) {\n            // swallow exceptions in cleanup as user configure to make sure\n            // commitJob could be success even when cleanup get failure.\n            LOG.error(\"Error in cleanup job, manually cleanup is needed.\", e);\n          } else {\n            // throw back exception to fail commitJob.\n            throw e;\n          }\n        }\n      }\n      // True if the job requires output.dir marked on successful job.\n      // Note that by default it is set to true.\n      if (context.getConfiguration().getBoolean(\n          SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n        Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n        // If job commit is repeatable and previous/another AM could write\n        // mark file already, we need to set overwritten to be true explicitly\n        // in case other FS implementations don\u0027t overwritten by default.\n        if (isCommitJobRepeatable(context)) {\n          fs.create(markerPath, true).close();\n        } else {\n          fs.create(markerPath).close();\n        }\n      }\n    } else {\n      LOG.warn(\"Output Path is null in commitJob()\");\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
          "extendedDetails": {}
        }
      ]
    },
    "372ad270a0d7ea5c581cd9a42b3c3cb189eca204": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6478. Add an option to skip cleanupJob stage or ignore cleanup failure during commitJob. (Junping Du via wangda)\n",
      "commitDate": "18/09/15 10:10 AM",
      "commitName": "372ad270a0d7ea5c581cd9a42b3c3cb189eca204",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "22/06/15 9:53 PM",
      "commitNameOld": "41ae7768ebb8637c09c7f9733bcfe9dc43dbce69",
      "commitAuthorOld": "Devaraj K",
      "daysBetweenCommits": 87.51,
      "commitsBetweenForRepo": 520,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,40 @@\n   public void commitJob(JobContext context) throws IOException {\n     if (hasOutputPath()) {\n       Path finalOutput \u003d getOutputPath();\n       FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n \n       if (algorithmVersion \u003d\u003d 1) {\n         for (FileStatus stat: getAllCommittedTaskPaths(context)) {\n           mergePaths(fs, stat, finalOutput);\n         }\n       }\n \n-      // delete the _temporary folder and create a _done file in the o/p folder\n-      cleanupJob(context);\n+      if (skipCleanup) {\n+        LOG.info(\"Skip cleanup the _temporary folders under job\u0027s output \" +\n+            \"directory in commitJob.\");\n+      } else {\n+        // delete the _temporary folder and create a _done file in the o/p\n+        // folder\n+        try {\n+          cleanupJob(context);\n+        } catch (IOException e) {\n+          if (ignoreCleanupFailures) {\n+            // swallow exceptions in cleanup as user configure to make sure\n+            // commitJob could be success even when cleanup get failure.\n+            LOG.error(\"Error in cleanup job, manually cleanup is needed.\", e);\n+          } else {\n+            // throw back exception to fail commitJob.\n+            throw e;\n+          }\n+        }\n+      }\n       // True if the job requires output.dir marked on successful job.\n       // Note that by default it is set to true.\n       if (context.getConfiguration().getBoolean(SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n         Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n         fs.create(markerPath).close();\n       }\n     } else {\n       LOG.warn(\"Output Path is null in commitJob()\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void commitJob(JobContext context) throws IOException {\n    if (hasOutputPath()) {\n      Path finalOutput \u003d getOutputPath();\n      FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n\n      if (algorithmVersion \u003d\u003d 1) {\n        for (FileStatus stat: getAllCommittedTaskPaths(context)) {\n          mergePaths(fs, stat, finalOutput);\n        }\n      }\n\n      if (skipCleanup) {\n        LOG.info(\"Skip cleanup the _temporary folders under job\u0027s output \" +\n            \"directory in commitJob.\");\n      } else {\n        // delete the _temporary folder and create a _done file in the o/p\n        // folder\n        try {\n          cleanupJob(context);\n        } catch (IOException e) {\n          if (ignoreCleanupFailures) {\n            // swallow exceptions in cleanup as user configure to make sure\n            // commitJob could be success even when cleanup get failure.\n            LOG.error(\"Error in cleanup job, manually cleanup is needed.\", e);\n          } else {\n            // throw back exception to fail commitJob.\n            throw e;\n          }\n        }\n      }\n      // True if the job requires output.dir marked on successful job.\n      // Note that by default it is set to true.\n      if (context.getConfiguration().getBoolean(SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n        Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n        fs.create(markerPath).close();\n      }\n    } else {\n      LOG.warn(\"Output Path is null in commitJob()\");\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
      "extendedDetails": {}
    },
    "aa92b764a7ddb888d097121c4d610089a0053d11": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4815. Speed up FileOutputCommitter#commitJob for many output files. (Siqi Li via gera)\n",
      "commitDate": "10/03/15 11:32 AM",
      "commitName": "aa92b764a7ddb888d097121c4d610089a0053d11",
      "commitAuthor": "Gera Shegalov",
      "commitDateOld": "28/04/14 8:18 AM",
      "commitNameOld": "bb7ce82816574f67aa1898f67e0e0cff54fa67be",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 316.13,
      "commitsBetweenForRepo": 2567,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,23 @@\n   public void commitJob(JobContext context) throws IOException {\n     if (hasOutputPath()) {\n       Path finalOutput \u003d getOutputPath();\n       FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n-      for(FileStatus stat: getAllCommittedTaskPaths(context)) {\n-        mergePaths(fs, stat, finalOutput);\n+\n+      if (algorithmVersion \u003d\u003d 1) {\n+        for (FileStatus stat: getAllCommittedTaskPaths(context)) {\n+          mergePaths(fs, stat, finalOutput);\n+        }\n       }\n \n       // delete the _temporary folder and create a _done file in the o/p folder\n       cleanupJob(context);\n       // True if the job requires output.dir marked on successful job.\n       // Note that by default it is set to true.\n       if (context.getConfiguration().getBoolean(SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n         Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n         fs.create(markerPath).close();\n       }\n     } else {\n       LOG.warn(\"Output Path is null in commitJob()\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void commitJob(JobContext context) throws IOException {\n    if (hasOutputPath()) {\n      Path finalOutput \u003d getOutputPath();\n      FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n\n      if (algorithmVersion \u003d\u003d 1) {\n        for (FileStatus stat: getAllCommittedTaskPaths(context)) {\n          mergePaths(fs, stat, finalOutput);\n        }\n      }\n\n      // delete the _temporary folder and create a _done file in the o/p folder\n      cleanupJob(context);\n      // True if the job requires output.dir marked on successful job.\n      // Note that by default it is set to true.\n      if (context.getConfiguration().getBoolean(SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n        Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n        fs.create(markerPath).close();\n      }\n    } else {\n      LOG.warn(\"Output Path is null in commitJob()\");\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
      "extendedDetails": {}
    },
    "94242c93857a06fb9c56ee571a47d6ca18f00f48": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3711. Fixed MR AM recovery so that only single selected task output is recovered and thus reduce the unnecessarily bloated recovery time. Contributed by Robert Joseph Evans.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1240413 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/02/12 4:04 PM",
      "commitName": "94242c93857a06fb9c56ee571a47d6ca18f00f48",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "19/12/11 3:07 PM",
      "commitNameOld": "a61a18cc098591eacd998e4a2f61babe27353a31",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 46.04,
      "commitsBetweenForRepo": 217,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,20 @@\n   public void commitJob(JobContext context) throws IOException {\n-    if (outputPath !\u003d null) {\n-      //delete the task temp directory from the current jobtempdir\n-      Path tmpDir \u003d new Path(outputPath, getJobAttemptBaseDirName(context) +\n-          Path.SEPARATOR + FileOutputCommitter.TEMP_DIR_NAME);\n-      FileSystem fileSys \u003d tmpDir.getFileSystem(context.getConfiguration());\n-      if (fileSys.exists(tmpDir)) {\n-        fileSys.delete(tmpDir, true);\n-      } else {\n-        LOG.warn(\"Task temp dir could not be deleted \" + tmpDir);\n+    if (hasOutputPath()) {\n+      Path finalOutput \u003d getOutputPath();\n+      FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n+      for(FileStatus stat: getAllCommittedTaskPaths(context)) {\n+        mergePaths(fs, stat, finalOutput);\n       }\n \n-      //move the job output to final place\n-      Path jobOutputPath \u003d \n-          new Path(outputPath, getJobAttemptBaseDirName(context));\n-      moveJobOutputs(outputFileSystem, jobOutputPath, outputPath, jobOutputPath);\n-\n       // delete the _temporary folder and create a _done file in the o/p folder\n       cleanupJob(context);\n-      if (shouldMarkOutputDir(context.getConfiguration())) {\n-        markOutputDirSuccessful(context);\n+      // True if the job requires output.dir marked on successful job.\n+      // Note that by default it is set to true.\n+      if (context.getConfiguration().getBoolean(SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n+        Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n+        fs.create(markerPath).close();\n       }\n+    } else {\n+      LOG.warn(\"Output Path is null in commitJob()\");\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void commitJob(JobContext context) throws IOException {\n    if (hasOutputPath()) {\n      Path finalOutput \u003d getOutputPath();\n      FileSystem fs \u003d finalOutput.getFileSystem(context.getConfiguration());\n      for(FileStatus stat: getAllCommittedTaskPaths(context)) {\n        mergePaths(fs, stat, finalOutput);\n      }\n\n      // delete the _temporary folder and create a _done file in the o/p folder\n      cleanupJob(context);\n      // True if the job requires output.dir marked on successful job.\n      // Note that by default it is set to true.\n      if (context.getConfiguration().getBoolean(SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)) {\n        Path markerPath \u003d new Path(outputPath, SUCCEEDED_FILE_NAME);\n        fs.create(markerPath).close();\n      }\n    } else {\n      LOG.warn(\"Output Path is null in commitJob()\");\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
      "extendedDetails": {}
    },
    "c7fb49b3c5b8aa394f578e644bbc13fc9cbfcaca": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3170. Fixed job output commit for deep hierarchies. Contributed by Hitesh Shah.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1183185 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/10/11 6:16 PM",
      "commitName": "c7fb49b3c5b8aa394f578e644bbc13fc9cbfcaca",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "05/10/11 5:15 AM",
      "commitNameOld": "f29bfa0e0e88de49f1558138eeaba1396fa5a1d2",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 8.54,
      "commitsBetweenForRepo": 72,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,24 @@\n   public void commitJob(JobContext context) throws IOException {\n-    //delete the task temp directory from the current jobtempdir\n-    Path tmpDir \u003d new Path(outputPath, getJobAttemptBaseDirName(context) +\n-        Path.SEPARATOR + FileOutputCommitter.TEMP_DIR_NAME);\n-    FileSystem fileSys \u003d tmpDir.getFileSystem(context.getConfiguration());\n-    if (fileSys.exists(tmpDir)) {\n-      fileSys.delete(tmpDir, true);\n-    } else {\n-      LOG.warn(\"Task temp dir could not be deleted \" + tmpDir);\n-    }\n-    \n-\t  //move the job output to final place\n-    Path jobOutputPath \u003d \n-        new Path(outputPath, getJobAttemptBaseDirName(context));\n-\t  moveJobOutputs(outputFileSystem, outputPath, jobOutputPath);\n-\t  \n-    // delete the _temporary folder and create a _done file in the o/p folder\n-    cleanupJob(context);\n-    if (shouldMarkOutputDir(context.getConfiguration())) {\n-      markOutputDirSuccessful(context);\n+    if (outputPath !\u003d null) {\n+      //delete the task temp directory from the current jobtempdir\n+      Path tmpDir \u003d new Path(outputPath, getJobAttemptBaseDirName(context) +\n+          Path.SEPARATOR + FileOutputCommitter.TEMP_DIR_NAME);\n+      FileSystem fileSys \u003d tmpDir.getFileSystem(context.getConfiguration());\n+      if (fileSys.exists(tmpDir)) {\n+        fileSys.delete(tmpDir, true);\n+      } else {\n+        LOG.warn(\"Task temp dir could not be deleted \" + tmpDir);\n+      }\n+\n+      //move the job output to final place\n+      Path jobOutputPath \u003d \n+          new Path(outputPath, getJobAttemptBaseDirName(context));\n+      moveJobOutputs(outputFileSystem, jobOutputPath, outputPath, jobOutputPath);\n+\n+      // delete the _temporary folder and create a _done file in the o/p folder\n+      cleanupJob(context);\n+      if (shouldMarkOutputDir(context.getConfiguration())) {\n+        markOutputDirSuccessful(context);\n+      }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void commitJob(JobContext context) throws IOException {\n    if (outputPath !\u003d null) {\n      //delete the task temp directory from the current jobtempdir\n      Path tmpDir \u003d new Path(outputPath, getJobAttemptBaseDirName(context) +\n          Path.SEPARATOR + FileOutputCommitter.TEMP_DIR_NAME);\n      FileSystem fileSys \u003d tmpDir.getFileSystem(context.getConfiguration());\n      if (fileSys.exists(tmpDir)) {\n        fileSys.delete(tmpDir, true);\n      } else {\n        LOG.warn(\"Task temp dir could not be deleted \" + tmpDir);\n      }\n\n      //move the job output to final place\n      Path jobOutputPath \u003d \n          new Path(outputPath, getJobAttemptBaseDirName(context));\n      moveJobOutputs(outputFileSystem, jobOutputPath, outputPath, jobOutputPath);\n\n      // delete the _temporary folder and create a _done file in the o/p folder\n      cleanupJob(context);\n      if (shouldMarkOutputDir(context.getConfiguration())) {\n        markOutputDirSuccessful(context);\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
      "extendedDetails": {}
    },
    "f29bfa0e0e88de49f1558138eeaba1396fa5a1d2": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2702. Added a new API in OutputCommitter for recovering the outputs of tasks from a crashed job so as to support MR Application Master recovery. Contributed by Sharad Agarwal and Arun C Murthy.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1179188 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/10/11 5:15 AM",
      "commitName": "f29bfa0e0e88de49f1558138eeaba1396fa5a1d2",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 41.5,
      "commitsBetweenForRepo": 270,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,7 +1,22 @@\n   public void commitJob(JobContext context) throws IOException {\n+    //delete the task temp directory from the current jobtempdir\n+    Path tmpDir \u003d new Path(outputPath, getJobAttemptBaseDirName(context) +\n+        Path.SEPARATOR + FileOutputCommitter.TEMP_DIR_NAME);\n+    FileSystem fileSys \u003d tmpDir.getFileSystem(context.getConfiguration());\n+    if (fileSys.exists(tmpDir)) {\n+      fileSys.delete(tmpDir, true);\n+    } else {\n+      LOG.warn(\"Task temp dir could not be deleted \" + tmpDir);\n+    }\n+    \n+\t  //move the job output to final place\n+    Path jobOutputPath \u003d \n+        new Path(outputPath, getJobAttemptBaseDirName(context));\n+\t  moveJobOutputs(outputFileSystem, outputPath, jobOutputPath);\n+\t  \n     // delete the _temporary folder and create a _done file in the o/p folder\n     cleanupJob(context);\n     if (shouldMarkOutputDir(context.getConfiguration())) {\n       markOutputDirSuccessful(context);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void commitJob(JobContext context) throws IOException {\n    //delete the task temp directory from the current jobtempdir\n    Path tmpDir \u003d new Path(outputPath, getJobAttemptBaseDirName(context) +\n        Path.SEPARATOR + FileOutputCommitter.TEMP_DIR_NAME);\n    FileSystem fileSys \u003d tmpDir.getFileSystem(context.getConfiguration());\n    if (fileSys.exists(tmpDir)) {\n      fileSys.delete(tmpDir, true);\n    } else {\n      LOG.warn(\"Task temp dir could not be deleted \" + tmpDir);\n    }\n    \n\t  //move the job output to final place\n    Path jobOutputPath \u003d \n        new Path(outputPath, getJobAttemptBaseDirName(context));\n\t  moveJobOutputs(outputFileSystem, outputPath, jobOutputPath);\n\t  \n    // delete the _temporary folder and create a _done file in the o/p folder\n    cleanupJob(context);\n    if (shouldMarkOutputDir(context.getConfiguration())) {\n      markOutputDirSuccessful(context);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void commitJob(JobContext context) throws IOException {\n    // delete the _temporary folder and create a _done file in the o/p folder\n    cleanupJob(context);\n    if (shouldMarkOutputDir(context.getConfiguration())) {\n      markOutputDirSuccessful(context);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void commitJob(JobContext context) throws IOException {\n    // delete the _temporary folder and create a _done file in the o/p folder\n    cleanupJob(context);\n    if (shouldMarkOutputDir(context.getConfiguration())) {\n      markOutputDirSuccessful(context);\n    }\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
        "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,7 @@\n+  public void commitJob(JobContext context) throws IOException {\n+    // delete the _temporary folder and create a _done file in the o/p folder\n+    cleanupJob(context);\n+    if (shouldMarkOutputDir(context.getConfiguration())) {\n+      markOutputDirSuccessful(context);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void commitJob(JobContext context) throws IOException {\n    // delete the _temporary folder and create a _done file in the o/p folder\n    cleanupJob(context);\n    if (shouldMarkOutputDir(context.getConfiguration())) {\n      markOutputDirSuccessful(context);\n    }\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java"
    }
  }
}