{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "WebHdfsFileSystem.java",
  "functionName": "initialize",
  "functionId": "initialize___uri-URI__conf-Configuration",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
  "functionStartLine": 207,
  "functionEndLine": 306,
  "numCommitsSeen": 179,
  "timeTaken": 10714,
  "changeHistory": [
    "8ffc356b1e8845c370f05e97437544f75ed6e8db",
    "ba66f3b454a5f6ea84f2cf7ac0082c555e2954a7",
    "d78854b928bb877f26b11b5b212a100a79941f35",
    "cd2a1dd0630e99e6696a9f0e8dc972604d38046a",
    "e24ed47d9a19f34a4dd8d4bad9b5c78ca3dd1c2e",
    "687233f20d24c29041929dd0a99d963cec54b6df",
    "5d1889a66d91608d34ca9411fb6e9161e637e9d3",
    "239d119c6707e58c9a5e0099c6d65fe956e95140",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "559c09dc0eba28666c4b16435512cc2d35e31683",
    "837fb75e8e03b2f016bcea2f4605106a5022491c",
    "fac4e04dd359a7ff31f286d664fb06f019ec0b58",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
    "6f8003dc7bc9e8be7b0512c514d370c303faf003",
    "60da0e49e7316892d63e9c7cdc3214057e68009a",
    "af9d4fede535f0699d08e592d5c4e133a5823663",
    "bbff44cb03d0150f990acc3b77170893241cc282",
    "7ba5913797c49d5001ad95558eadd119c3361060",
    "0c5128969522cf754010c32cdcbfcfa5ebe5b3b0",
    "33ade356b35223654a077103ed7fbed89f3f2321",
    "e90687f90ee2ec76baeac49df8423524b7e83586",
    "5c381ade4e6b6428b316c41ad5f5bc145829473d",
    "0aa09f6d5a97f523e9ee6f30bb44f206433ead0a",
    "91d0b4727066241c900037f9a10e28b143489172",
    "859e425dfa2269ad33f1790e69f3ef189d6485d5",
    "f5e83a0b3e33376d3378b23f6889d954f4e975f3",
    "d8a23834614581a292aad214dddcbcc4bbe86d27",
    "3cae2ba63fe6f0765d860677a9bd9f1ea158c1c3",
    "620890fcc0fb8680a3ad282b0b7b969277deb766",
    "7a2443e9f8b95816c7df61530cda29e8b040b12e",
    "f278a491dcec249a2ec22e14b645d8f890278be5",
    "97ccd64401569a8cdabc40c5897e34a03ce4bb22",
    "50222ff52903431ec7aefa30fdf4fdaa04915c92",
    "cb787968c5deac3dd5d10291aae39c36656a1487",
    "556be2af92b68808aff71937d437ab9948164bb1",
    "e4eec269d91ae541a321ae2f28ff03310682b3fe",
    "cdbe15ccdc2f1e673f0ab538378ad05560d5b713",
    "201af204061a073fb041b9a7bbe93e35dd43c597",
    "ab8f458742ce675c352b8288cb0af177751654a4",
    "c80dbe5e09ab1eb3c1b0277055f28717895d6dd9",
    "205f0470f43303e16d6a62afd633ce8f19040810",
    "32cad9affe159ff7c6e4c7e31f57174967ef210a",
    "bf78f15ffb438cc13546328b2e85cba6f51b9422",
    "78e3821b819b441d1faf4bc66c659cdeddc6006c",
    "61d0b7530c8978c095ab6f62d9d38e168bd829c6"
  ],
  "changeHistoryShort": {
    "8ffc356b1e8845c370f05e97437544f75ed6e8db": "Ybodychange",
    "ba66f3b454a5f6ea84f2cf7ac0082c555e2954a7": "Ybodychange",
    "d78854b928bb877f26b11b5b212a100a79941f35": "Ybodychange",
    "cd2a1dd0630e99e6696a9f0e8dc972604d38046a": "Ybodychange",
    "e24ed47d9a19f34a4dd8d4bad9b5c78ca3dd1c2e": "Ybodychange",
    "687233f20d24c29041929dd0a99d963cec54b6df": "Ybodychange",
    "5d1889a66d91608d34ca9411fb6e9161e637e9d3": "Ybodychange",
    "239d119c6707e58c9a5e0099c6d65fe956e95140": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "559c09dc0eba28666c4b16435512cc2d35e31683": "Ybodychange",
    "837fb75e8e03b2f016bcea2f4605106a5022491c": "Ybodychange",
    "fac4e04dd359a7ff31f286d664fb06f019ec0b58": "Ybodychange",
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": "Ybodychange",
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35": "Ymultichange(Yfilerename,Ybodychange)",
    "6f8003dc7bc9e8be7b0512c514d370c303faf003": "Ybodychange",
    "60da0e49e7316892d63e9c7cdc3214057e68009a": "Ybodychange",
    "af9d4fede535f0699d08e592d5c4e133a5823663": "Ybodychange",
    "bbff44cb03d0150f990acc3b77170893241cc282": "Ybodychange",
    "7ba5913797c49d5001ad95558eadd119c3361060": "Ybodychange",
    "0c5128969522cf754010c32cdcbfcfa5ebe5b3b0": "Ybodychange",
    "33ade356b35223654a077103ed7fbed89f3f2321": "Ybodychange",
    "e90687f90ee2ec76baeac49df8423524b7e83586": "Ybodychange",
    "5c381ade4e6b6428b316c41ad5f5bc145829473d": "Ybodychange",
    "0aa09f6d5a97f523e9ee6f30bb44f206433ead0a": "Ybodychange",
    "91d0b4727066241c900037f9a10e28b143489172": "Ybodychange",
    "859e425dfa2269ad33f1790e69f3ef189d6485d5": "Ybodychange",
    "f5e83a0b3e33376d3378b23f6889d954f4e975f3": "Ybodychange",
    "d8a23834614581a292aad214dddcbcc4bbe86d27": "Ybodychange",
    "3cae2ba63fe6f0765d860677a9bd9f1ea158c1c3": "Ybodychange",
    "620890fcc0fb8680a3ad282b0b7b969277deb766": "Ybodychange",
    "7a2443e9f8b95816c7df61530cda29e8b040b12e": "Ybodychange",
    "f278a491dcec249a2ec22e14b645d8f890278be5": "Ybodychange",
    "97ccd64401569a8cdabc40c5897e34a03ce4bb22": "Ybodychange",
    "50222ff52903431ec7aefa30fdf4fdaa04915c92": "Ybodychange",
    "cb787968c5deac3dd5d10291aae39c36656a1487": "Ybodychange",
    "556be2af92b68808aff71937d437ab9948164bb1": "Ybodychange",
    "e4eec269d91ae541a321ae2f28ff03310682b3fe": "Ybodychange",
    "cdbe15ccdc2f1e673f0ab538378ad05560d5b713": "Ybodychange",
    "201af204061a073fb041b9a7bbe93e35dd43c597": "Ybodychange",
    "ab8f458742ce675c352b8288cb0af177751654a4": "Ybodychange",
    "c80dbe5e09ab1eb3c1b0277055f28717895d6dd9": "Ybodychange",
    "205f0470f43303e16d6a62afd633ce8f19040810": "Ybodychange",
    "32cad9affe159ff7c6e4c7e31f57174967ef210a": "Ybodychange",
    "bf78f15ffb438cc13546328b2e85cba6f51b9422": "Ybodychange",
    "78e3821b819b441d1faf4bc66c659cdeddc6006c": "Ymodifierchange",
    "61d0b7530c8978c095ab6f62d9d38e168bd829c6": "Yintroduced"
  },
  "changeHistoryDetails": {
    "8ffc356b1e8845c370f05e97437544f75ed6e8db": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"SPNEGO TLS verification\"\n\nThis reverts commit ba66f3b454a5f6ea84f2cf7ac0082c555e2954a7.\n",
      "commitDate": "13/05/20 1:14 AM",
      "commitName": "8ffc356b1e8845c370f05e97437544f75ed6e8db",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "02/05/20 5:50 AM",
      "commitNameOld": "ba66f3b454a5f6ea84f2cf7ac0082c555e2954a7",
      "commitAuthorOld": "Eric Yang",
      "daysBetweenCommits": 10.81,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,100 @@\n   public synchronized void initialize(URI uri, Configuration conf\n   ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n \n     // set user and acl patterns based on configuration file\n     UserParam.setUserPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n     AclPermissionParam.setAclPermissionPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_DEFAULT));\n \n     int connectTimeout \u003d (int) conf.getTimeDuration(\n         HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_CONNECT_TIMEOUT_KEY,\n         URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n         TimeUnit.MILLISECONDS);\n \n     int readTimeout \u003d (int) conf.getTimeDuration(\n         HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_READ_TIMEOUT_KEY,\n         URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n         TimeUnit.MILLISECONDS);\n \n \n     boolean isOAuth \u003d conf.getBoolean(\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n \n     if(isOAuth) {\n       LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newOAuth2URLConnectionFactory(connectTimeout, readTimeout, conf);\n     } else {\n       LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newDefaultURLConnectionFactory(connectTimeout, readTimeout, conf);\n     }\n \n-    this.isTLSKrb \u003d \"HTTPS_ONLY\".equals(conf.get(DFS_HTTP_POLICY_KEY));\n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n               HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.isInsecureCluster \u003d !this.canRefreshDelegationToken;\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.initializeRestCsrf(conf);\n     this.delegationToken \u003d null;\n \n     storageStatistics \u003d (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE\n         .put(DFSOpsCountStatistics.NAME,\n             new StorageStatisticsProvider() {\n               @Override\n               public StorageStatistics provide() {\n                 return new DFSOpsCountStatistics();\n               }\n             });\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n  ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n\n    // set user and acl patterns based on configuration file\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n    AclPermissionParam.setAclPermissionPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_DEFAULT));\n\n    int connectTimeout \u003d (int) conf.getTimeDuration(\n        HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_CONNECT_TIMEOUT_KEY,\n        URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n        TimeUnit.MILLISECONDS);\n\n    int readTimeout \u003d (int) conf.getTimeDuration(\n        HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_READ_TIMEOUT_KEY,\n        URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n        TimeUnit.MILLISECONDS);\n\n\n    boolean isOAuth \u003d conf.getBoolean(\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n\n    if(isOAuth) {\n      LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newOAuth2URLConnectionFactory(connectTimeout, readTimeout, conf);\n    } else {\n      LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newDefaultURLConnectionFactory(connectTimeout, readTimeout, conf);\n    }\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.isInsecureCluster \u003d !this.canRefreshDelegationToken;\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.initializeRestCsrf(conf);\n    this.delegationToken \u003d null;\n\n    storageStatistics \u003d (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE\n        .put(DFSOpsCountStatistics.NAME,\n            new StorageStatisticsProvider() {\n              @Override\n              public StorageStatistics provide() {\n                return new DFSOpsCountStatistics();\n              }\n            });\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "ba66f3b454a5f6ea84f2cf7ac0082c555e2954a7": {
      "type": "Ybodychange",
      "commitMessage": "SPNEGO TLS verification\n\nSigned-off-by: Akira Ajisaka \u003caajisaka@apache.org\u003e\n",
      "commitDate": "02/05/20 5:50 AM",
      "commitName": "ba66f3b454a5f6ea84f2cf7ac0082c555e2954a7",
      "commitAuthor": "Eric Yang",
      "commitDateOld": "17/04/20 1:02 AM",
      "commitNameOld": "37d65822235fe8285d10232589aba39ededd3be1",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 15.2,
      "commitsBetweenForRepo": 54,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,100 +1,101 @@\n   public synchronized void initialize(URI uri, Configuration conf\n   ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n \n     // set user and acl patterns based on configuration file\n     UserParam.setUserPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n     AclPermissionParam.setAclPermissionPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_DEFAULT));\n \n     int connectTimeout \u003d (int) conf.getTimeDuration(\n         HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_CONNECT_TIMEOUT_KEY,\n         URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n         TimeUnit.MILLISECONDS);\n \n     int readTimeout \u003d (int) conf.getTimeDuration(\n         HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_READ_TIMEOUT_KEY,\n         URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n         TimeUnit.MILLISECONDS);\n \n \n     boolean isOAuth \u003d conf.getBoolean(\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n \n     if(isOAuth) {\n       LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newOAuth2URLConnectionFactory(connectTimeout, readTimeout, conf);\n     } else {\n       LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newDefaultURLConnectionFactory(connectTimeout, readTimeout, conf);\n     }\n \n+    this.isTLSKrb \u003d \"HTTPS_ONLY\".equals(conf.get(DFS_HTTP_POLICY_KEY));\n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n               HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.isInsecureCluster \u003d !this.canRefreshDelegationToken;\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.initializeRestCsrf(conf);\n     this.delegationToken \u003d null;\n \n     storageStatistics \u003d (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE\n         .put(DFSOpsCountStatistics.NAME,\n             new StorageStatisticsProvider() {\n               @Override\n               public StorageStatistics provide() {\n                 return new DFSOpsCountStatistics();\n               }\n             });\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n  ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n\n    // set user and acl patterns based on configuration file\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n    AclPermissionParam.setAclPermissionPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_DEFAULT));\n\n    int connectTimeout \u003d (int) conf.getTimeDuration(\n        HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_CONNECT_TIMEOUT_KEY,\n        URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n        TimeUnit.MILLISECONDS);\n\n    int readTimeout \u003d (int) conf.getTimeDuration(\n        HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_READ_TIMEOUT_KEY,\n        URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n        TimeUnit.MILLISECONDS);\n\n\n    boolean isOAuth \u003d conf.getBoolean(\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n\n    if(isOAuth) {\n      LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newOAuth2URLConnectionFactory(connectTimeout, readTimeout, conf);\n    } else {\n      LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newDefaultURLConnectionFactory(connectTimeout, readTimeout, conf);\n    }\n\n    this.isTLSKrb \u003d \"HTTPS_ONLY\".equals(conf.get(DFS_HTTP_POLICY_KEY));\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.isInsecureCluster \u003d !this.canRefreshDelegationToken;\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.initializeRestCsrf(conf);\n    this.delegationToken \u003d null;\n\n    storageStatistics \u003d (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE\n        .put(DFSOpsCountStatistics.NAME,\n            new StorageStatisticsProvider() {\n              @Override\n              public StorageStatistics provide() {\n                return new DFSOpsCountStatistics();\n              }\n            });\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "d78854b928bb877f26b11b5b212a100a79941f35": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14434.  Ignore user.name query parameter in secure WebHDFS.\n             Contributed by KWON BYUNGCHANG\n",
      "commitDate": "28/05/19 2:31 PM",
      "commitName": "d78854b928bb877f26b11b5b212a100a79941f35",
      "commitAuthor": "Eric Yang",
      "commitDateOld": "17/05/19 10:20 AM",
      "commitNameOld": "3e5e5b028ad7e199d08e524fe7cddeee5db51a6d",
      "commitAuthorOld": "Srinivasu Majeti",
      "daysBetweenCommits": 11.17,
      "commitsBetweenForRepo": 58,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,99 +1,100 @@\n   public synchronized void initialize(URI uri, Configuration conf\n   ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n \n     // set user and acl patterns based on configuration file\n     UserParam.setUserPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n     AclPermissionParam.setAclPermissionPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_DEFAULT));\n \n     int connectTimeout \u003d (int) conf.getTimeDuration(\n         HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_CONNECT_TIMEOUT_KEY,\n         URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n         TimeUnit.MILLISECONDS);\n \n     int readTimeout \u003d (int) conf.getTimeDuration(\n         HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_READ_TIMEOUT_KEY,\n         URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n         TimeUnit.MILLISECONDS);\n \n \n     boolean isOAuth \u003d conf.getBoolean(\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n \n     if(isOAuth) {\n       LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newOAuth2URLConnectionFactory(connectTimeout, readTimeout, conf);\n     } else {\n       LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newDefaultURLConnectionFactory(connectTimeout, readTimeout, conf);\n     }\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n               HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n+    this.isInsecureCluster \u003d !this.canRefreshDelegationToken;\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.initializeRestCsrf(conf);\n     this.delegationToken \u003d null;\n \n     storageStatistics \u003d (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE\n         .put(DFSOpsCountStatistics.NAME,\n             new StorageStatisticsProvider() {\n               @Override\n               public StorageStatistics provide() {\n                 return new DFSOpsCountStatistics();\n               }\n             });\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n  ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n\n    // set user and acl patterns based on configuration file\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n    AclPermissionParam.setAclPermissionPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_DEFAULT));\n\n    int connectTimeout \u003d (int) conf.getTimeDuration(\n        HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_CONNECT_TIMEOUT_KEY,\n        URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n        TimeUnit.MILLISECONDS);\n\n    int readTimeout \u003d (int) conf.getTimeDuration(\n        HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_READ_TIMEOUT_KEY,\n        URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n        TimeUnit.MILLISECONDS);\n\n\n    boolean isOAuth \u003d conf.getBoolean(\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n\n    if(isOAuth) {\n      LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newOAuth2URLConnectionFactory(connectTimeout, readTimeout, conf);\n    } else {\n      LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newDefaultURLConnectionFactory(connectTimeout, readTimeout, conf);\n    }\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.isInsecureCluster \u003d !this.canRefreshDelegationToken;\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.initializeRestCsrf(conf);\n    this.delegationToken \u003d null;\n\n    storageStatistics \u003d (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE\n        .put(DFSOpsCountStatistics.NAME,\n            new StorageStatisticsProvider() {\n              @Override\n              public StorageStatistics provide() {\n                return new DFSOpsCountStatistics();\n              }\n            });\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "cd2a1dd0630e99e6696a9f0e8dc972604d38046a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9914. Fix configurable WebhDFS connect/read timeout. Contributed by Xiaoyu Yao.\n",
      "commitDate": "27/10/17 7:43 AM",
      "commitName": "cd2a1dd0630e99e6696a9f0e8dc972604d38046a",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "11/10/17 12:29 PM",
      "commitNameOld": "8acdf5c2742c081f3e0e96e13eb940a39964a58f",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 15.8,
      "commitsBetweenForRepo": 90,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,88 +1,99 @@\n   public synchronized void initialize(URI uri, Configuration conf\n   ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n \n     // set user and acl patterns based on configuration file\n     UserParam.setUserPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n     AclPermissionParam.setAclPermissionPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_DEFAULT));\n \n+    int connectTimeout \u003d (int) conf.getTimeDuration(\n+        HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_CONNECT_TIMEOUT_KEY,\n+        URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n+        TimeUnit.MILLISECONDS);\n+\n+    int readTimeout \u003d (int) conf.getTimeDuration(\n+        HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_READ_TIMEOUT_KEY,\n+        URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n+        TimeUnit.MILLISECONDS);\n+\n+\n     boolean isOAuth \u003d conf.getBoolean(\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n \n     if(isOAuth) {\n       LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n-          .newOAuth2URLConnectionFactory(conf);\n+          .newOAuth2URLConnectionFactory(connectTimeout, readTimeout, conf);\n     } else {\n       LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n-          .newDefaultURLConnectionFactory(conf);\n+          .newDefaultURLConnectionFactory(connectTimeout, readTimeout, conf);\n     }\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n               HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.initializeRestCsrf(conf);\n     this.delegationToken \u003d null;\n \n     storageStatistics \u003d (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE\n         .put(DFSOpsCountStatistics.NAME,\n             new StorageStatisticsProvider() {\n               @Override\n               public StorageStatistics provide() {\n                 return new DFSOpsCountStatistics();\n               }\n             });\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n  ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n\n    // set user and acl patterns based on configuration file\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n    AclPermissionParam.setAclPermissionPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_DEFAULT));\n\n    int connectTimeout \u003d (int) conf.getTimeDuration(\n        HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_CONNECT_TIMEOUT_KEY,\n        URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n        TimeUnit.MILLISECONDS);\n\n    int readTimeout \u003d (int) conf.getTimeDuration(\n        HdfsClientConfigKeys.DFS_WEBHDFS_SOCKET_READ_TIMEOUT_KEY,\n        URLConnectionFactory.DEFAULT_SOCKET_TIMEOUT,\n        TimeUnit.MILLISECONDS);\n\n\n    boolean isOAuth \u003d conf.getBoolean(\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n\n    if(isOAuth) {\n      LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newOAuth2URLConnectionFactory(connectTimeout, readTimeout, conf);\n    } else {\n      LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newDefaultURLConnectionFactory(connectTimeout, readTimeout, conf);\n    }\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.initializeRestCsrf(conf);\n    this.delegationToken \u003d null;\n\n    storageStatistics \u003d (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE\n        .put(DFSOpsCountStatistics.NAME,\n            new StorageStatisticsProvider() {\n              @Override\n              public StorageStatistics provide() {\n                return new DFSOpsCountStatistics();\n              }\n            });\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "e24ed47d9a19f34a4dd8d4bad9b5c78ca3dd1c2e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11421. Make WebHDFS\u0027 ACLs RegEx configurable. Contributed by Harsh J.\n",
      "commitDate": "24/02/17 4:49 PM",
      "commitName": "e24ed47d9a19f34a4dd8d4bad9b5c78ca3dd1c2e",
      "commitAuthor": "Xiao Chen",
      "commitDateOld": "04/01/17 9:01 PM",
      "commitNameOld": "a605ff36a53a3d1283c3f6d81eb073e4a2942143",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 50.83,
      "commitsBetweenForRepo": 260,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,88 @@\n   public synchronized void initialize(URI uri, Configuration conf\n   ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n-    /** set user pattern based on configuration file */\n+\n+    // set user and acl patterns based on configuration file\n     UserParam.setUserPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n+    AclPermissionParam.setAclPermissionPattern(conf.get(\n+        HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY,\n+        HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_DEFAULT));\n \n     boolean isOAuth \u003d conf.getBoolean(\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n \n     if(isOAuth) {\n       LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newOAuth2URLConnectionFactory(conf);\n     } else {\n       LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newDefaultURLConnectionFactory(conf);\n     }\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n               HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.initializeRestCsrf(conf);\n     this.delegationToken \u003d null;\n \n     storageStatistics \u003d (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE\n         .put(DFSOpsCountStatistics.NAME,\n             new StorageStatisticsProvider() {\n               @Override\n               public StorageStatistics provide() {\n                 return new DFSOpsCountStatistics();\n               }\n             });\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n  ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n\n    // set user and acl patterns based on configuration file\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n    AclPermissionParam.setAclPermissionPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_ACL_PERMISSION_PATTERN_DEFAULT));\n\n    boolean isOAuth \u003d conf.getBoolean(\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n\n    if(isOAuth) {\n      LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newOAuth2URLConnectionFactory(conf);\n    } else {\n      LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newDefaultURLConnectionFactory(conf);\n    }\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.initializeRestCsrf(conf);\n    this.delegationToken \u003d null;\n\n    storageStatistics \u003d (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE\n        .put(DFSOpsCountStatistics.NAME,\n            new StorageStatisticsProvider() {\n              @Override\n              public StorageStatistics provide() {\n                return new DFSOpsCountStatistics();\n              }\n            });\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "687233f20d24c29041929dd0a99d963cec54b6df": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13065. Add a new interface for retrieving FS and FC Statistics (Mingliang Liu via cmccabe)\n",
      "commitDate": "11/05/16 1:45 PM",
      "commitName": "687233f20d24c29041929dd0a99d963cec54b6df",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "23/04/16 7:37 AM",
      "commitNameOld": "6fcde2e38da04cae3aad6b13cf442af211f71506",
      "commitAuthorOld": "Masatake Iwasaki",
      "daysBetweenCommits": 18.26,
      "commitsBetweenForRepo": 102,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,84 @@\n   public synchronized void initialize(URI uri, Configuration conf\n   ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     boolean isOAuth \u003d conf.getBoolean(\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n \n     if(isOAuth) {\n       LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newOAuth2URLConnectionFactory(conf);\n     } else {\n       LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newDefaultURLConnectionFactory(conf);\n     }\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n               HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.initializeRestCsrf(conf);\n     this.delegationToken \u003d null;\n+\n+    storageStatistics \u003d (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE\n+        .put(DFSOpsCountStatistics.NAME,\n+            new StorageStatisticsProvider() {\n+              @Override\n+              public StorageStatistics provide() {\n+                return new DFSOpsCountStatistics();\n+              }\n+            });\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n  ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    boolean isOAuth \u003d conf.getBoolean(\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n\n    if(isOAuth) {\n      LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newOAuth2URLConnectionFactory(conf);\n    } else {\n      LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newDefaultURLConnectionFactory(conf);\n    }\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.initializeRestCsrf(conf);\n    this.delegationToken \u003d null;\n\n    storageStatistics \u003d (DFSOpsCountStatistics) GlobalStorageStatistics.INSTANCE\n        .put(DFSOpsCountStatistics.NAME,\n            new StorageStatisticsProvider() {\n              @Override\n              public StorageStatistics provide() {\n                return new DFSOpsCountStatistics();\n              }\n            });\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "5d1889a66d91608d34ca9411fb6e9161e637e9d3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9711. Integrate CSRF prevention filter in WebHDFS. Contributed by Chris Nauroth.\n",
      "commitDate": "18/02/16 10:07 AM",
      "commitName": "5d1889a66d91608d34ca9411fb6e9161e637e9d3",
      "commitAuthor": "cnauroth",
      "commitDateOld": "04/02/16 11:34 AM",
      "commitNameOld": "1bcfab8e7fd8562f1829ac484d2f6c91f7afe3d6",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 13.94,
      "commitsBetweenForRepo": 89,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,75 @@\n   public synchronized void initialize(URI uri, Configuration conf\n   ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     boolean isOAuth \u003d conf.getBoolean(\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n \n     if(isOAuth) {\n       LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newOAuth2URLConnectionFactory(conf);\n     } else {\n       LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newDefaultURLConnectionFactory(conf);\n     }\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n               HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n+    this.initializeRestCsrf(conf);\n     this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n  ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    boolean isOAuth \u003d conf.getBoolean(\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n\n    if(isOAuth) {\n      LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newOAuth2URLConnectionFactory(conf);\n    } else {\n      LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newDefaultURLConnectionFactory(conf);\n    }\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.initializeRestCsrf(conf);\n    this.delegationToken \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "239d119c6707e58c9a5e0099c6d65fe956e95140": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9196. Fix TestWebHdfsContentLength. Contributed by Masatake Iwasaki.\n",
      "commitDate": "06/10/15 11:54 PM",
      "commitName": "239d119c6707e58c9a5e0099c6d65fe956e95140",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "03/10/15 11:38 AM",
      "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.51,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,74 @@\n   public synchronized void initialize(URI uri, Configuration conf\n   ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     boolean isOAuth \u003d conf.getBoolean(\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n \n     if(isOAuth) {\n       LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newOAuth2URLConnectionFactory(conf);\n     } else {\n       LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newDefaultURLConnectionFactory(conf);\n     }\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n               HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n-    this.workingDir \u003d makeQualified(getHomeDirectory());\n+    this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n  ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    boolean isOAuth \u003d conf.getBoolean(\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n\n    if(isOAuth) {\n      LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newOAuth2URLConnectionFactory(conf);\n    } else {\n      LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newDefaultURLConnectionFactory(conf);\n    }\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.delegationToken \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:52 PM",
      "commitNameOld": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.74,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,74 @@\n   public synchronized void initialize(URI uri, Configuration conf\n-      ) throws IOException {\n+  ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     boolean isOAuth \u003d conf.getBoolean(\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n \n     if(isOAuth) {\n       LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newOAuth2URLConnectionFactory(conf);\n     } else {\n       LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newDefaultURLConnectionFactory(conf);\n     }\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n               HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n-    this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n+    this.workingDir \u003d makeQualified(getHomeDirectory());\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n  ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    boolean isOAuth \u003d conf.getBoolean(\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n\n    if(isOAuth) {\n      LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newOAuth2URLConnectionFactory(conf);\n    } else {\n      LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newDefaultURLConnectionFactory(conf);\n    }\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d makeQualified(getHomeDirectory());\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.delegationToken \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "559c09dc0eba28666c4b16435512cc2d35e31683": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9082. Change the log level in WebHdfsFileSystem.initialize() from INFO to DEBUG. Contributed by Santhosh Nayak.\n",
      "commitDate": "15/09/15 3:13 PM",
      "commitName": "559c09dc0eba28666c4b16435512cc2d35e31683",
      "commitAuthor": "cnauroth",
      "commitDateOld": "29/08/15 6:37 PM",
      "commitNameOld": "837fb75e8e03b2f016bcea2f4605106a5022491c",
      "commitAuthorOld": "Jakob Homan",
      "daysBetweenCommits": 16.86,
      "commitsBetweenForRepo": 102,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,74 +1,74 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     boolean isOAuth \u003d conf.getBoolean(\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n \n     if(isOAuth) {\n-      LOG.info(\"Enabling OAuth2 in WebHDFS\");\n+      LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newOAuth2URLConnectionFactory(conf);\n     } else {\n-      LOG.info(\"Not enabling OAuth2 in WebHDFS\");\n+      LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n       connectionFactory \u003d URLConnectionFactory\n           .newDefaultURLConnectionFactory(conf);\n     }\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n               HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    boolean isOAuth \u003d conf.getBoolean(\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n\n    if(isOAuth) {\n      LOG.debug(\"Enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newOAuth2URLConnectionFactory(conf);\n    } else {\n      LOG.debug(\"Not enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newDefaultURLConnectionFactory(conf);\n    }\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.delegationToken \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "837fb75e8e03b2f016bcea2f4605106a5022491c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8155. Support OAuth2 in WebHDFS.\n",
      "commitDate": "29/08/15 6:37 PM",
      "commitName": "837fb75e8e03b2f016bcea2f4605106a5022491c",
      "commitAuthor": "Jakob Homan",
      "commitDateOld": "18/08/15 5:32 PM",
      "commitNameOld": "30e342a5d32be5efffeb472cce76d4ed43642608",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 11.04,
      "commitsBetweenForRepo": 60,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,74 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n-    connectionFactory \u003d URLConnectionFactory\n-        .newDefaultURLConnectionFactory(conf);\n+    boolean isOAuth \u003d conf.getBoolean(\n+        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n+        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n+\n+    if(isOAuth) {\n+      LOG.info(\"Enabling OAuth2 in WebHDFS\");\n+      connectionFactory \u003d URLConnectionFactory\n+          .newOAuth2URLConnectionFactory(conf);\n+    } else {\n+      LOG.info(\"Not enabling OAuth2 in WebHDFS\");\n+      connectionFactory \u003d URLConnectionFactory\n+          .newDefaultURLConnectionFactory(conf);\n+    }\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n               HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    boolean isOAuth \u003d conf.getBoolean(\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_OAUTH_ENABLED_DEFAULT);\n\n    if(isOAuth) {\n      LOG.info(\"Enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newOAuth2URLConnectionFactory(conf);\n    } else {\n      LOG.info(\"Not enabling OAuth2 in WebHDFS\");\n      connectionFactory \u003d URLConnectionFactory\n          .newDefaultURLConnectionFactory(conf);\n    }\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.delegationToken \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "fac4e04dd359a7ff31f286d664fb06f019ec0b58": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8542. WebHDFS getHomeDirectory behavior does not match specification. Contributed by  Kanaka Kumar Avvaru.\n",
      "commitDate": "22/06/15 4:30 PM",
      "commitName": "fac4e04dd359a7ff31f286d664fb06f019ec0b58",
      "commitAuthor": "Jakob Homan",
      "commitDateOld": "02/05/15 10:03 AM",
      "commitNameOld": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 51.27,
      "commitsBetweenForRepo": 460,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,63 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n               HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n-    this.workingDir \u003d getHomeDirectory();\n+    this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d makeQualified(new Path(getHomeDirectoryString(ugi)));\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.delegationToken \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "6ae2a0d048e133b43249c248a75a4d77d9abb80d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8249. Separate HdfsConstants into the client and the server side class. Contributed by Haohui Mai.\n",
      "commitDate": "02/05/15 10:03 AM",
      "commitName": "6ae2a0d048e133b43249c248a75a4d77d9abb80d",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "23/04/15 5:33 PM",
      "commitNameOld": "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 8.69,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,63 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n-              HdfsConstantsClient.SAFEMODE_EXCEPTION_CLASS_NAME);\n+              HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstants.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.delegationToken \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "bcf89ddc7d52e04725caf104f5958e33d9f51b35": {
      "type": "Ymultichange(Yfilerename,Ybodychange)",
      "commitMessage": "HDFS-8052. Move WebHdfsFileSystem into hadoop-hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "23/04/15 5:33 PM",
      "commitName": "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-8052. Move WebHdfsFileSystem into hadoop-hdfs-client. Contributed by Haohui Mai.\n",
          "commitDate": "23/04/15 5:33 PM",
          "commitName": "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "23/04/15 4:40 PM",
          "commitNameOld": "0b3f8957a87ada1a275c9904b211fdbdcefafb02",
          "commitAuthorOld": "Xuan",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,63 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n-        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n-        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n+        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n+        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n-              SafeModeException.class);\n+              HdfsConstantsClient.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstantsClient.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.delegationToken \u003d null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8052. Move WebHdfsFileSystem into hadoop-hdfs-client. Contributed by Haohui Mai.\n",
          "commitDate": "23/04/15 5:33 PM",
          "commitName": "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "23/04/15 4:40 PM",
          "commitNameOld": "0b3f8957a87ada1a275c9904b211fdbdcefafb02",
          "commitAuthorOld": "Xuan",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,63 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n-        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n-        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n+        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n+        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n-              SafeModeException.class);\n+              HdfsConstantsClient.SAFEMODE_EXCEPTION_CLASS_NAME);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              HdfsConstantsClient.SAFEMODE_EXCEPTION_CLASS_NAME);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.delegationToken \u003d null;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "6f8003dc7bc9e8be7b0512c514d370c303faf003": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8185. Separate client related routines in HAUtil into a new class. Contributed by Haohui Mai.\n",
      "commitDate": "21/04/15 9:59 PM",
      "commitName": "6f8003dc7bc9e8be7b0512c514d370c303faf003",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "20/04/15 12:36 AM",
      "commitNameOld": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.89,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,63 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n-    boolean isHA \u003d HAUtil.isClientFailoverConfigured(conf, this.uri);\n-    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtil.isLogicalUri(conf, this.uri);\n+    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n+    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n-        HAUtil.buildTokenServiceForLogicalUri(uri, getScheme())\n+        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n               HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n           HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n           HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtilClient.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtilClient.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtilClient.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.delegationToken \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "60da0e49e7316892d63e9c7cdc3214057e68009a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8084. Move dfs.client.failover.* confs from DFSConfigKeys to HdfsClientConfigKeys.Failover and fix typos in the dfs.http.client.* configuration keys.\n",
      "commitDate": "10/04/15 7:38 PM",
      "commitName": "60da0e49e7316892d63e9c7cdc3214057e68009a",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "09/04/15 2:36 PM",
      "commitNameOld": "af9d4fede535f0699d08e592d5c4e133a5823663",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.21,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,63 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtil.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtil.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtil.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n-              HdfsClientConfigKeys.WebHdfsRetry.RETRY_POLICY_ENABLED_KEY,\n-              HdfsClientConfigKeys.WebHdfsRetry.RETRY_POLICY_ENABLED_DEFAULT,\n-              HdfsClientConfigKeys.WebHdfsRetry.RETRY_POLICY_SPEC_KEY,\n-              HdfsClientConfigKeys.WebHdfsRetry.RETRY_POLICY_SPEC_DEFAULT,\n+              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n+              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n+              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n+              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n-          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_MAX_ATTEMPTS_KEY,\n-          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n+          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n+          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n-          HdfsClientConfigKeys.WebHdfsRetry.RETRY_MAX_ATTEMPTS_KEY,\n-          HdfsClientConfigKeys.WebHdfsRetry.RETRY_MAX_ATTEMPTS_DEFAULT);\n+          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n+          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n-          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_SLEEPTIME_BASE_KEY,\n-          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n+          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n+          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n-          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_SLEEPTIME_MAX_KEY,\n-          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n+          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n+          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtil.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtil.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtil.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.HttpClient.RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.HttpClient.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.HttpClient.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.delegationToken \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "af9d4fede535f0699d08e592d5c4e133a5823663": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8102. Separate webhdfs retry configuration keys from DFSConfigKeys. Contributed by Haohui Mai.\n",
      "commitDate": "09/04/15 2:36 PM",
      "commitName": "af9d4fede535f0699d08e592d5c4e133a5823663",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "07/04/15 9:30 PM",
      "commitNameOld": "ab04ff9efe632b4eca6faca7407ac35e00e6a379",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 1.71,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,63 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtil.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtil.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtil.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n-              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n-              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n-              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n-              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n+              HdfsClientConfigKeys.WebHdfsRetry.RETRY_POLICY_ENABLED_KEY,\n+              HdfsClientConfigKeys.WebHdfsRetry.RETRY_POLICY_ENABLED_DEFAULT,\n+              HdfsClientConfigKeys.WebHdfsRetry.RETRY_POLICY_SPEC_KEY,\n+              HdfsClientConfigKeys.WebHdfsRetry.RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n-          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n-          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n+          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_MAX_ATTEMPTS_KEY,\n+          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n-          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n-          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n+          HdfsClientConfigKeys.WebHdfsRetry.RETRY_MAX_ATTEMPTS_KEY,\n+          HdfsClientConfigKeys.WebHdfsRetry.RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n-          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n-          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n+          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_SLEEPTIME_BASE_KEY,\n+          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n-          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n-          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n+          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_SLEEPTIME_MAX_KEY,\n+          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n         CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtil.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtil.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtil.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              HdfsClientConfigKeys.WebHdfsRetry.RETRY_POLICY_ENABLED_KEY,\n              HdfsClientConfigKeys.WebHdfsRetry.RETRY_POLICY_ENABLED_DEFAULT,\n              HdfsClientConfigKeys.WebHdfsRetry.RETRY_POLICY_SPEC_KEY,\n              HdfsClientConfigKeys.WebHdfsRetry.RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          HdfsClientConfigKeys.WebHdfsRetry.RETRY_MAX_ATTEMPTS_KEY,\n          HdfsClientConfigKeys.WebHdfsRetry.RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_SLEEPTIME_BASE_KEY,\n          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_SLEEPTIME_MAX_KEY,\n          HdfsClientConfigKeys.WebHdfsRetry.FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.delegationToken \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "bbff44cb03d0150f990acc3b77170893241cc282": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6776. Using distcp to copy data between insecure and secure cluster via webdhfs doesn\u0027t work. (yzhangal via tucu)\n",
      "commitDate": "09/09/14 10:16 PM",
      "commitName": "bbff44cb03d0150f990acc3b77170893241cc282",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "30/07/14 10:49 AM",
      "commitNameOld": "535fe14dedbf919442ec03ac573315c7a16a6dbe",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 41.48,
      "commitsBetweenForRepo": 331,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,63 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtil.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtil.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtil.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n+    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n+        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n+        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n     this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtil.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtil.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtil.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.disallowFallbackToInsecureCluster \u003d !conf.getBoolean(\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_KEY,\n        CommonConfigurationKeys.IPC_CLIENT_FALLBACK_TO_SIMPLE_AUTH_ALLOWED_DEFAULT);\n    this.delegationToken \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "7ba5913797c49d5001ad95558eadd119c3361060": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6667. In HDFS HA mode, Distcp/SLive with webhdfs on secure cluster fails with Client cannot authenticate via:[TOKEN, KERBEROS] error. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1611508 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/07/14 4:11 PM",
      "commitName": "7ba5913797c49d5001ad95558eadd119c3361060",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "20/06/14 4:58 PM",
      "commitNameOld": "0c5128969522cf754010c32cdcbfcfa5ebe5b3b0",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 26.97,
      "commitsBetweenForRepo": 176,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtil.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtil.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n-        HAUtil.buildTokenServiceForLogicalUri(uri)\n+        HAUtil.buildTokenServiceForLogicalUri(uri, getScheme())\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n     this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n     this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtil.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtil.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtil.buildTokenServiceForLogicalUri(uri, getScheme())\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.delegationToken \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "0c5128969522cf754010c32cdcbfcfa5ebe5b3b0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6222. Remove background token renewer from webhdfs. Contributed by Rushabh Shah and Daryn Sharp.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1604300 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/06/14 4:58 PM",
      "commitName": "0c5128969522cf754010c32cdcbfcfa5ebe5b3b0",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "19/06/14 4:06 PM",
      "commitNameOld": "46dc32e12568c5e254a3a2f2664095dc9de8bd55",
      "commitAuthorOld": "Alejandro Abdelnur",
      "daysBetweenCommits": 1.04,
      "commitsBetweenForRepo": 10,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,63 +1,60 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtil.isClientFailoverConfigured(conf, this.uri);\n     boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtil.isLogicalUri(conf, this.uri);\n     // In non-HA or non-logical URI case, the code needs to call\n     // getCanonicalUri() in order to handle the case where no port is\n     // specified in the URI\n     this.tokenServiceName \u003d isLogicalUri ?\n         HAUtil.buildTokenServiceForLogicalUri(uri)\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n-    initializeTokenAspect();\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n-\n-    if (UserGroupInformation.isSecurityEnabled()) {\n-      tokenAspect.initDelegationToken(ugi);\n-    }\n+    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n+    this.delegationToken \u003d null;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtil.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtil.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtil.buildTokenServiceForLogicalUri(uri)\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n    this.canRefreshDelegationToken \u003d UserGroupInformation.isSecurityEnabled();\n    this.delegationToken \u003d null;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "33ade356b35223654a077103ed7fbed89f3f2321": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6334. Client failover proxy provider for IP failover based NN HA. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594263 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/05/14 9:19 AM",
      "commitName": "33ade356b35223654a077103ed7fbed89f3f2321",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "08/05/14 7:01 PM",
      "commitNameOld": "5323d5e388a0b65b5b6670387c2efb6bed98a235",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 4.6,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,63 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n     this.nnAddrs \u003d resolveNNAddr();\n \n-    boolean isHA \u003d HAUtil.isLogicalUri(conf, this.uri);\n-    // In non-HA case, the code needs to call getCanonicalUri() in order to\n-    // handle the case where no port is specified in the URI\n-    this.tokenServiceName \u003d isHA ? HAUtil.buildTokenServiceForLogicalUri(uri)\n+    boolean isHA \u003d HAUtil.isClientFailoverConfigured(conf, this.uri);\n+    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtil.isLogicalUri(conf, this.uri);\n+    // In non-HA or non-logical URI case, the code needs to call\n+    // getCanonicalUri() in order to handle the case where no port is\n+    // specified in the URI\n+    this.tokenServiceName \u003d isLogicalUri ?\n+        HAUtil.buildTokenServiceForLogicalUri(uri)\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n     initializeTokenAspect();\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       tokenAspect.initDelegationToken(ugi);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtil.isClientFailoverConfigured(conf, this.uri);\n    boolean isLogicalUri \u003d isHA \u0026\u0026 HAUtil.isLogicalUri(conf, this.uri);\n    // In non-HA or non-logical URI case, the code needs to call\n    // getCanonicalUri() in order to handle the case where no port is\n    // specified in the URI\n    this.tokenServiceName \u003d isLogicalUri ?\n        HAUtil.buildTokenServiceForLogicalUri(uri)\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n    initializeTokenAspect();\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      tokenAspect.initDelegationToken(ugi);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "e90687f90ee2ec76baeac49df8423524b7e83586": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5321. Clean up the HTTP-related configuration in HDFS. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1574270 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/03/14 4:28 PM",
      "commitName": "e90687f90ee2ec76baeac49df8423524b7e83586",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/02/14 10:39 AM",
      "commitNameOld": "5c381ade4e6b6428b316c41ad5f5bc145829473d",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 4.24,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,60 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n         DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n \n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n-    this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n+    this.nnAddrs \u003d resolveNNAddr();\n \n     boolean isHA \u003d HAUtil.isLogicalUri(conf, this.uri);\n     // In non-HA case, the code needs to call getCanonicalUri() in order to\n     // handle the case where no port is specified in the URI\n     this.tokenServiceName \u003d isHA ? HAUtil.buildTokenServiceForLogicalUri(uri)\n         : SecurityUtil.buildTokenService(getCanonicalUri());\n     initializeTokenAspect();\n \n     if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       tokenAspect.initDelegationToken(ugi);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d resolveNNAddr();\n\n    boolean isHA \u003d HAUtil.isLogicalUri(conf, this.uri);\n    // In non-HA case, the code needs to call getCanonicalUri() in order to\n    // handle the case where no port is specified in the URI\n    this.tokenServiceName \u003d isHA ? HAUtil.buildTokenServiceForLogicalUri(uri)\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n    initializeTokenAspect();\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      tokenAspect.initDelegationToken(ugi);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "5c381ade4e6b6428b316c41ad5f5bc145829473d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5339. WebHDFS URI does not accept logical nameservices when security is enabled. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1573026 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/02/14 10:39 AM",
      "commitName": "5c381ade4e6b6428b316c41ad5f5bc145829473d",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "08/02/14 11:05 AM",
      "commitNameOld": "c6505f0499bc925fcc909f1c4eed89ba0c9098f7",
      "commitAuthorOld": "",
      "daysBetweenCommits": 19.98,
      "commitsBetweenForRepo": 160,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,57 +1,60 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n-    UserParam.setUserPattern(conf.get(DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY, DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n+    UserParam.setUserPattern(conf.get(\n+        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n+        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n+\n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n-    initializeTokenAspect();\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n+    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n+    this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n \n-    try {\n-      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n-          null, null);\n-      this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n-    } catch (URISyntaxException e) {\n-      throw new IllegalArgumentException(e);\n-    }\n+    boolean isHA \u003d HAUtil.isLogicalUri(conf, this.uri);\n+    // In non-HA case, the code needs to call getCanonicalUri() in order to\n+    // handle the case where no port is specified in the URI\n+    this.tokenServiceName \u003d isHA ? HAUtil.buildTokenServiceForLogicalUri(uri)\n+        : SecurityUtil.buildTokenService(getCanonicalUri());\n+    initializeTokenAspect();\n \n-    if (!HAUtil.isLogicalUri(conf, this.uri)) {\n+    if (!isHA) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int maxRetryAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       tokenAspect.initDelegationToken(ugi);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY,\n        DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.uri \u003d URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n    this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n\n    boolean isHA \u003d HAUtil.isLogicalUri(conf, this.uri);\n    // In non-HA case, the code needs to call getCanonicalUri() in order to\n    // handle the case where no port is specified in the URI\n    this.tokenServiceName \u003d isHA ? HAUtil.buildTokenServiceForLogicalUri(uri)\n        : SecurityUtil.buildTokenService(getCanonicalUri());\n    initializeTokenAspect();\n\n    if (!isHA) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      tokenAspect.initDelegationToken(ugi);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "0aa09f6d5a97f523e9ee6f30bb44f206433ead0a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5399. Revisit SafeModeException and corresponding retry policies. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1564629 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/02/14 8:18 PM",
      "commitName": "0aa09f6d5a97f523e9ee6f30bb44f206433ead0a",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "07/12/13 1:10 PM",
      "commitNameOld": "91d0b4727066241c900037f9a10e28b143489172",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 59.3,
      "commitsBetweenForRepo": 307,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,57 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     /** set user pattern based on configuration file */\n     UserParam.setUserPattern(conf.get(DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY, DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n     initializeTokenAspect();\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n \n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n           null, null);\n       this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n \n     if (!HAUtil.isLogicalUri(conf, this.uri)) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n+      int maxRetryAttempts \u003d conf.getInt(\n+          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n+          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n-              maxFailoverAttempts, failoverSleepBaseMillis,\n+              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       tokenAspect.initDelegationToken(ugi);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY, DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n    initializeTokenAspect();\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n          null, null);\n      this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n\n    if (!HAUtil.isLogicalUri(conf, this.uri)) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int maxRetryAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, maxRetryAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      tokenAspect.initDelegationToken(ugi);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "91d0b4727066241c900037f9a10e28b143489172": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4983. Numeric usernames do not work with WebHDFS FS. Contributed by Yongjun Zhang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548968 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "07/12/13 1:10 PM",
      "commitName": "91d0b4727066241c900037f9a10e28b143489172",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "04/12/13 4:05 PM",
      "commitNameOld": "859e425dfa2269ad33f1790e69f3ef189d6485d5",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 2.88,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,54 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n+    /** set user pattern based on configuration file */\n+    UserParam.setUserPattern(conf.get(DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY, DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n     initializeTokenAspect();\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n \n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n           null, null);\n       this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n \n     if (!HAUtil.isLogicalUri(conf, this.uri)) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       tokenAspect.initDelegationToken(ugi);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf.get(DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY, DFSConfigKeys.DFS_WEBHDFS_USER_PATTERN_DEFAULT));\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n    initializeTokenAspect();\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n          null, null);\n      this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n\n    if (!HAUtil.isLogicalUri(conf, this.uri)) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      tokenAspect.initDelegationToken(ugi);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "859e425dfa2269ad33f1790e69f3ef189d6485d5": {
      "type": "Ybodychange",
      "commitMessage": "Revert HDFS-4983\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1547970 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/12/13 4:05 PM",
      "commitName": "859e425dfa2269ad33f1790e69f3ef189d6485d5",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "04/12/13 2:24 PM",
      "commitNameOld": "f5e83a0b3e33376d3378b23f6889d954f4e975f3",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.07,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,54 +1,52 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n-    /** set user pattern based on configuration file */\n-    UserParam.setUserPattern(conf);\n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n     initializeTokenAspect();\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n \n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n           null, null);\n       this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n \n     if (!HAUtil.isLogicalUri(conf, this.uri)) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       tokenAspect.initDelegationToken(ugi);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n    initializeTokenAspect();\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n          null, null);\n      this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n\n    if (!HAUtil.isLogicalUri(conf, this.uri)) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      tokenAspect.initDelegationToken(ugi);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "f5e83a0b3e33376d3378b23f6889d954f4e975f3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4983. Numeric usernames do not work with WebHDFS FS. Contributed by Yongjun Zhang.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1547935 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/12/13 2:24 PM",
      "commitName": "f5e83a0b3e33376d3378b23f6889d954f4e975f3",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "25/11/13 5:16 PM",
      "commitNameOld": "d8a23834614581a292aad214dddcbcc4bbe86d27",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 8.88,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,52 +1,54 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n+    /** set user pattern based on configuration file */\n+    UserParam.setUserPattern(conf);\n     connectionFactory \u003d URLConnectionFactory\n         .newDefaultURLConnectionFactory(conf);\n     initializeTokenAspect();\n \n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n \n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n           null, null);\n       this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n \n     if (!HAUtil.isLogicalUri(conf, this.uri)) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       tokenAspect.initDelegationToken(ugi);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    /** set user pattern based on configuration file */\n    UserParam.setUserPattern(conf);\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n    initializeTokenAspect();\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n          null, null);\n      this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n\n    if (!HAUtil.isLogicalUri(conf, this.uri)) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      tokenAspect.initDelegationToken(ugi);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "d8a23834614581a292aad214dddcbcc4bbe86d27": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5538. URLConnectionFactory should pick up the SSL related configuration by default. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1545491 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/11/13 5:16 PM",
      "commitName": "d8a23834614581a292aad214dddcbcc4bbe86d27",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "20/11/13 1:51 PM",
      "commitNameOld": "3cae2ba63fe6f0765d860677a9bd9f1ea158c1c3",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 5.14,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,50 +1,52 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n+    connectionFactory \u003d URLConnectionFactory\n+        .newDefaultURLConnectionFactory(conf);\n     initializeTokenAspect();\n-    initializeConnectionFactory(conf);\n+\n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n \n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n           null, null);\n       this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n \n     if (!HAUtil.isLogicalUri(conf, this.uri)) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       tokenAspect.initDelegationToken(ugi);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    connectionFactory \u003d URLConnectionFactory\n        .newDefaultURLConnectionFactory(conf);\n    initializeTokenAspect();\n\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n          null, null);\n      this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n\n    if (!HAUtil.isLogicalUri(conf, this.uri)) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      tokenAspect.initDelegationToken(ugi);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "3cae2ba63fe6f0765d860677a9bd9f1ea158c1c3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3987. Support webhdfs over HTTPS. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1543962 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/11/13 1:51 PM",
      "commitName": "3cae2ba63fe6f0765d860677a9bd9f1ea158c1c3",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "14/11/13 6:11 PM",
      "commitNameOld": "620890fcc0fb8680a3ad282b0b7b969277deb766",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 5.82,
      "commitsBetweenForRepo": 34,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,50 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n+    initializeTokenAspect();\n+    initializeConnectionFactory(conf);\n+\n     ugi \u003d UserGroupInformation.getCurrentUser();\n \n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n           null, null);\n-      this.nnAddrs \u003d DFSUtil.resolve(this.uri, getDefaultPort(), conf);\n+      this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n \n     if (!HAUtil.isLogicalUri(conf, this.uri)) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       tokenAspect.initDelegationToken(ugi);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    initializeTokenAspect();\n    initializeConnectionFactory(conf);\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n          null, null);\n      this.nnAddrs \u003d DFSUtil.resolveWebHdfsUri(this.uri, conf);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n\n    if (!HAUtil.isLogicalUri(conf, this.uri)) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      tokenAspect.initDelegationToken(ugi);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "620890fcc0fb8680a3ad282b0b7b969277deb766": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5489. Use TokenAspect in WebHDFSFileSystem. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1542158 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/11/13 6:11 PM",
      "commitName": "620890fcc0fb8680a3ad282b0b7b969277deb766",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "14/11/13 10:40 AM",
      "commitNameOld": "43fa41fdeee53471e9aa34c89f58e53e8aa164e5",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.31,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,47 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     ugi \u003d UserGroupInformation.getCurrentUser();\n \n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n           null, null);\n       this.nnAddrs \u003d DFSUtil.resolve(this.uri, getDefaultPort(), conf);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n \n     if (!HAUtil.isLogicalUri(conf, this.uri)) {\n       this.retryPolicy \u003d\n           RetryUtils.getDefaultRetryPolicy(\n               conf,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n               DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n               SafeModeException.class);\n     } else {\n \n       int maxFailoverAttempts \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n       int failoverSleepBaseMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n       int failoverSleepMaxMillis \u003d conf.getInt(\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n           DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n \n       this.retryPolicy \u003d RetryPolicies\n           .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n               maxFailoverAttempts, failoverSleepBaseMillis,\n               failoverSleepMaxMillis);\n     }\n \n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n-      initDelegationToken();\n+      tokenAspect.initDelegationToken(ugi);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    ugi \u003d UserGroupInformation.getCurrentUser();\n\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n          null, null);\n      this.nnAddrs \u003d DFSUtil.resolve(this.uri, getDefaultPort(), conf);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n\n    if (!HAUtil.isLogicalUri(conf, this.uri)) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      tokenAspect.initDelegationToken(ugi);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "7a2443e9f8b95816c7df61530cda29e8b040b12e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5122. Support failover and retry in WebHdfsFileSystem for NN HA. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1524562 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/09/13 1:47 PM",
      "commitName": "7a2443e9f8b95816c7df61530cda29e8b040b12e",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "18/09/13 10:29 AM",
      "commitNameOld": "f278a491dcec249a2ec22e14b645d8f890278be5",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.14,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,47 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     ugi \u003d UserGroupInformation.getCurrentUser();\n+\n     try {\n-      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n+      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n+          null, null);\n+      this.nnAddrs \u003d DFSUtil.resolve(this.uri, getDefaultPort(), conf);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n-    this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n-    this.retryPolicy \u003d \n-        RetryUtils.getDefaultRetryPolicy(\n-            conf, \n-            DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n-            DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n-            DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n-            DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n-            SafeModeException.class);\n+\n+    if (!HAUtil.isLogicalUri(conf, this.uri)) {\n+      this.retryPolicy \u003d\n+          RetryUtils.getDefaultRetryPolicy(\n+              conf,\n+              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n+              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n+              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n+              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n+              SafeModeException.class);\n+    } else {\n+\n+      int maxFailoverAttempts \u003d conf.getInt(\n+          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n+          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n+      int failoverSleepBaseMillis \u003d conf.getInt(\n+          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n+          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n+      int failoverSleepMaxMillis \u003d conf.getInt(\n+          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n+          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n+\n+      this.retryPolicy \u003d RetryPolicies\n+          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n+              maxFailoverAttempts, failoverSleepBaseMillis,\n+              failoverSleepMaxMillis);\n+    }\n+\n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       initDelegationToken();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    ugi \u003d UserGroupInformation.getCurrentUser();\n\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null,\n          null, null);\n      this.nnAddrs \u003d DFSUtil.resolve(this.uri, getDefaultPort(), conf);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n\n    if (!HAUtil.isLogicalUri(conf, this.uri)) {\n      this.retryPolicy \u003d\n          RetryUtils.getDefaultRetryPolicy(\n              conf,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n              DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n              SafeModeException.class);\n    } else {\n\n      int maxFailoverAttempts \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_MAX_ATTEMPTS_DEFAULT);\n      int failoverSleepBaseMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_BASE_DEFAULT);\n      int failoverSleepMaxMillis \u003d conf.getInt(\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY,\n          DFSConfigKeys.DFS_HTTP_CLIENT_FAILOVER_SLEEPTIME_MAX_DEFAULT);\n\n      this.retryPolicy \u003d RetryPolicies\n          .failoverOnNetworkException(RetryPolicies.TRY_ONCE_THEN_FAIL,\n              maxFailoverAttempts, failoverSleepBaseMillis,\n              failoverSleepMaxMillis);\n    }\n\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      initDelegationToken();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "f278a491dcec249a2ec22e14b645d8f890278be5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5219. Add configuration keys for retry policy in WebHDFSFileSystem. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1524498 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/09/13 10:29 AM",
      "commitName": "f278a491dcec249a2ec22e14b645d8f890278be5",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "05/09/13 8:06 PM",
      "commitNameOld": "5eb618ee1f90ccf901edb5d89be181fad1f67d7f",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 12.6,
      "commitsBetweenForRepo": 65,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,25 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     ugi \u003d UserGroupInformation.getCurrentUser();\n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n     this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n     this.retryPolicy \u003d \n         RetryUtils.getDefaultRetryPolicy(\n             conf, \n-            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_ENABLED_KEY, \n-            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_ENABLED_DEFAULT, \n-            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_SPEC_KEY,\n-            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n+            DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n+            DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n+            DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n+            DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n             SafeModeException.class);\n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       initDelegationToken();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n    this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n    this.retryPolicy \u003d \n        RetryUtils.getDefaultRetryPolicy(\n            conf, \n            DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_KEY,\n            DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_ENABLED_DEFAULT,\n            DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_KEY,\n            DFSConfigKeys.DFS_HTTP_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n            SafeModeException.class);\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      initDelegationToken();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "97ccd64401569a8cdabc40c5897e34a03ce4bb22": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4542. Webhdfs doesn\u0027t support secure proxy users. Contributed by Daryn Sharp.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1452978 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/03/13 12:17 PM",
      "commitName": "97ccd64401569a8cdabc40c5897e34a03ce4bb22",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "29/01/13 10:51 PM",
      "commitNameOld": "481b6cccf0493cb3f740b119552bede0f7268121",
      "commitAuthorOld": "Konstantin Shvachko",
      "daysBetweenCommits": 34.56,
      "commitsBetweenForRepo": 125,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,25 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n+    ugi \u003d UserGroupInformation.getCurrentUser();\n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n     this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n     this.retryPolicy \u003d \n         RetryUtils.getDefaultRetryPolicy(\n             conf, \n             DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_ENABLED_KEY, \n             DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_ENABLED_DEFAULT, \n             DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_SPEC_KEY,\n             DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n             SafeModeException.class);\n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       initDelegationToken();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n    this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n    this.retryPolicy \u003d \n        RetryUtils.getDefaultRetryPolicy(\n            conf, \n            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_ENABLED_KEY, \n            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_ENABLED_DEFAULT, \n            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_SPEC_KEY,\n            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n            SafeModeException.class);\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      initDelegationToken();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "50222ff52903431ec7aefa30fdf4fdaa04915c92": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3871. Change NameNodeProxies to use RetryUtils.  Contributed by Arun C Murthy\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1379743 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "01/09/12 4:41 AM",
      "commitName": "50222ff52903431ec7aefa30fdf4fdaa04915c92",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "17/08/12 7:05 AM",
      "commitNameOld": "8fa10b184e607a33f59e67bd4b1fbe5a2e683941",
      "commitAuthorOld": "Daryn Sharp",
      "daysBetweenCommits": 14.9,
      "commitsBetweenForRepo": 102,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,24 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n     this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n-    this.retryPolicy \u003d NameNodeProxies.getDefaultRetryPolicy(conf);\n+    this.retryPolicy \u003d \n+        RetryUtils.getDefaultRetryPolicy(\n+            conf, \n+            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_ENABLED_KEY, \n+            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_ENABLED_DEFAULT, \n+            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_SPEC_KEY,\n+            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n+            SafeModeException.class);\n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       initDelegationToken();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n    this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n    this.retryPolicy \u003d \n        RetryUtils.getDefaultRetryPolicy(\n            conf, \n            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_ENABLED_KEY, \n            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_ENABLED_DEFAULT, \n            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_SPEC_KEY,\n            DFSConfigKeys.DFS_CLIENT_RETRY_POLICY_SPEC_DEFAULT,\n            SafeModeException.class);\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      initDelegationToken();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "cb787968c5deac3dd5d10291aae39c36656a1487": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3667.  Add retry support to WebHdfsFileSystem.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1367841 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/07/12 6:41 PM",
      "commitName": "cb787968c5deac3dd5d10291aae39c36656a1487",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "30/07/12 9:33 PM",
      "commitNameOld": "556be2af92b68808aff71937d437ab9948164bb1",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.88,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,17 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n     this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n+    this.retryPolicy \u003d NameNodeProxies.getDefaultRetryPolicy(conf);\n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       initDelegationToken();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n    this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n    this.retryPolicy \u003d NameNodeProxies.getDefaultRetryPolicy(conf);\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      initDelegationToken();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "556be2af92b68808aff71937d437ab9948164bb1": {
      "type": "Ybodychange",
      "commitMessage": "svn merge -c -1366601 for reverting HDFS-3667.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1367407 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/07/12 9:33 PM",
      "commitName": "556be2af92b68808aff71937d437ab9948164bb1",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "27/07/12 10:57 PM",
      "commitNameOld": "e4eec269d91ae541a321ae2f28ff03310682b3fe",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 2.94,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,16 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n     this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n-    this.retryPolicy \u003d NameNodeProxies.getDefaultRetryPolicy(conf);\n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       initDelegationToken();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n    this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      initDelegationToken();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "e4eec269d91ae541a321ae2f28ff03310682b3fe": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3667.  Add retry support to WebHdfsFileSystem.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1366601 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/07/12 10:57 PM",
      "commitName": "e4eec269d91ae541a321ae2f28ff03310682b3fe",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "25/07/12 4:37 PM",
      "commitNameOld": "8f395c2f78e5813e613197c3078a4ebc5d42775a",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 2.26,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,17 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n     this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n+    this.retryPolicy \u003d NameNodeProxies.getDefaultRetryPolicy(conf);\n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       initDelegationToken();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n    this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n    this.retryPolicy \u003d NameNodeProxies.getDefaultRetryPolicy(conf);\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      initDelegationToken();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "cdbe15ccdc2f1e673f0ab538378ad05560d5b713": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3308. Uses canonical URI to select delegation tokens in HftpFileSystem and WebHdfsFileSystem.  Contributed by Daryn Sharp\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1328541 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/04/12 4:59 PM",
      "commitName": "cdbe15ccdc2f1e673f0ab538378ad05560d5b713",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "20/04/12 12:32 PM",
      "commitNameOld": "201af204061a073fb041b9a7bbe93e35dd43c597",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.19,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n-    this.nnAddr \u003d NetUtils.createSocketAddrForHost(uri.getHost(), uri.getPort());\n+    this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       initDelegationToken();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n    this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      initDelegationToken();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "201af204061a073fb041b9a7bbe93e35dd43c597": {
      "type": "Ybodychange",
      "commitMessage": "Revert r1328482 for HDFS-3308.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1328487 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/04/12 12:32 PM",
      "commitName": "201af204061a073fb041b9a7bbe93e35dd43c597",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "20/04/12 12:16 PM",
      "commitNameOld": "ab8f458742ce675c352b8288cb0af177751654a4",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n-    this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n+    this.nnAddr \u003d NetUtils.createSocketAddrForHost(uri.getHost(), uri.getPort());\n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       initDelegationToken();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n    this.nnAddr \u003d NetUtils.createSocketAddrForHost(uri.getHost(), uri.getPort());\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      initDelegationToken();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "ab8f458742ce675c352b8288cb0af177751654a4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3308. Uses canonical URI to select delegation tokens in HftpFileSystem and WebHdfsFileSystem.  Contributed by Daryn Sharp\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1328482 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/04/12 12:16 PM",
      "commitName": "ab8f458742ce675c352b8288cb0af177751654a4",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "17/04/12 3:21 PM",
      "commitNameOld": "c80dbe5e09ab1eb3c1b0277055f28717895d6dd9",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 2.87,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n-    this.nnAddr \u003d NetUtils.createSocketAddrForHost(uri.getHost(), uri.getPort());\n+    this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       initDelegationToken();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n    this.nnAddr \u003d NetUtils.createSocketAddr(uri.getAuthority(), getDefaultPort());\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      initDelegationToken();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "c80dbe5e09ab1eb3c1b0277055f28717895d6dd9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2652. Add support for host-based delegation tokens.  Contributed by Daryn Sharp\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1327309 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/04/12 3:21 PM",
      "commitName": "c80dbe5e09ab1eb3c1b0277055f28717895d6dd9",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "03/02/12 10:57 PM",
      "commitNameOld": "205f0470f43303e16d6a62afd633ce8f19040810",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 73.64,
      "commitsBetweenForRepo": 574,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n     try {\n       this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n     } catch (URISyntaxException e) {\n       throw new IllegalArgumentException(e);\n     }\n-    this.nnAddr \u003d NetUtils.createSocketAddr(uri.toString());\n+    this.nnAddr \u003d NetUtils.createSocketAddrForHost(uri.getHost(), uri.getPort());\n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       initDelegationToken();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n    this.nnAddr \u003d NetUtils.createSocketAddrForHost(uri.getHost(), uri.getPort());\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      initDelegationToken();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "205f0470f43303e16d6a62afd633ce8f19040810": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2785. Update webhdfs and httpfs for host-based token support. Contributed by Robert Joseph Evans.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1240460 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/02/12 10:57 PM",
      "commitName": "205f0470f43303e16d6a62afd633ce8f19040810",
      "commitAuthor": "Jitendra Nath Pandey",
      "commitDateOld": "04/01/12 6:15 AM",
      "commitNameOld": "075122690c5c17ac443a8eb3fb7387001e4907c0",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 30.7,
      "commitsBetweenForRepo": 184,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,16 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n-\n+    try {\n+      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n+    } catch (URISyntaxException e) {\n+      throw new IllegalArgumentException(e);\n+    }\n     this.nnAddr \u003d NetUtils.createSocketAddr(uri.toString());\n     this.workingDir \u003d getHomeDirectory();\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n       initDelegationToken();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n    try {\n      this.uri \u003d new URI(uri.getScheme(), uri.getAuthority(), null, null, null);\n    } catch (URISyntaxException e) {\n      throw new IllegalArgumentException(e);\n    }\n    this.nnAddr \u003d NetUtils.createSocketAddr(uri.toString());\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      initDelegationToken();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "32cad9affe159ff7c6e4c7e31f57174967ef210a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2385. Support renew and cancel delegation tokens in webhdfs.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195656 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/10/11 1:37 PM",
      "commitName": "32cad9affe159ff7c6e4c7e31f57174967ef210a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "27/10/11 4:13 PM",
      "commitNameOld": "8cb0d4b380e0fd4437310c1dd6ef8b8995cc383d",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 3.89,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,7 +1,12 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n \n+    this.nnAddr \u003d NetUtils.createSocketAddr(uri.toString());\n     this.workingDir \u003d getHomeDirectory();\n+\n+    if (UserGroupInformation.isSecurityEnabled()) {\n+      initDelegationToken();\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n\n    this.nnAddr \u003d NetUtils.createSocketAddr(uri.toString());\n    this.workingDir \u003d getHomeDirectory();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      initDelegationToken();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "bf78f15ffb438cc13546328b2e85cba6f51b9422": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2366. Initialize WebHdfsFileSystem.ugi in object construction.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1176178 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/09/11 7:43 PM",
      "commitName": "bf78f15ffb438cc13546328b2e85cba6f51b9422",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "23/09/11 11:15 PM",
      "commitNameOld": "83a83d3b733fe18541428aaae2c2923318626e49",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 2.85,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,8 +1,7 @@\n   public synchronized void initialize(URI uri, Configuration conf\n       ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n \n-    ugi \u003d UserGroupInformation.getCurrentUser();\n     this.workingDir \u003d getHomeDirectory();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n\n    this.workingDir \u003d getHomeDirectory();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {}
    },
    "78e3821b819b441d1faf4bc66c659cdeddc6006c": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-2318. Provide authentication to webhdfs using SPNEGO and delegation tokens.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1171611 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/09/11 8:02 AM",
      "commitName": "78e3821b819b441d1faf4bc66c659cdeddc6006c",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "15/09/11 1:04 AM",
      "commitNameOld": "be2b0921fa3d1d82fed75bccfceae007d9faaea6",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.29,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,7 +1,8 @@\n-  public void initialize(URI uri, Configuration conf) throws IOException {\n+  public synchronized void initialize(URI uri, Configuration conf\n+      ) throws IOException {\n     super.initialize(uri, conf);\n     setConf(conf);\n \n     ugi \u003d UserGroupInformation.getCurrentUser();\n     this.workingDir \u003d getHomeDirectory();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void initialize(URI uri, Configuration conf\n      ) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.workingDir \u003d getHomeDirectory();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java",
      "extendedDetails": {
        "oldValue": "[public]",
        "newValue": "[public, synchronized]"
      }
    },
    "61d0b7530c8978c095ab6f62d9d38e168bd829c6": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2284. Add a new FileSystem, webhdfs://, for supporting write Http access to HDFS.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1167662 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/09/11 6:41 PM",
      "commitName": "61d0b7530c8978c095ab6f62d9d38e168bd829c6",
      "commitAuthor": "Tsz-wo Sze",
      "diff": "@@ -0,0 +1,7 @@\n+  public void initialize(URI uri, Configuration conf) throws IOException {\n+    super.initialize(uri, conf);\n+    setConf(conf);\n+\n+    ugi \u003d UserGroupInformation.getCurrentUser();\n+    this.workingDir \u003d getHomeDirectory();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void initialize(URI uri, Configuration conf) throws IOException {\n    super.initialize(uri, conf);\n    setConf(conf);\n\n    ugi \u003d UserGroupInformation.getCurrentUser();\n    this.workingDir \u003d getHomeDirectory();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java"
    }
  }
}