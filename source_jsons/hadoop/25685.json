{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "JobHistoryEventHandler.java",
  "functionName": "processEventForNewTimelineService",
  "functionId": "processEventForNewTimelineService___event-HistoryEvent__jobId-JobId__timestamp-long",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java",
  "functionStartLine": 1275,
  "functionEndLine": 1416,
  "numCommitsSeen": 69,
  "timeTaken": 3399,
  "changeHistory": [
    "092fead5d9875fb3760206bcdd76cdafec5e9481",
    "4fa1afdb883dab8786d2fb5c72a195dd2e87d711",
    "000a4d8e133920d71af304f2015d3674daed74fa",
    "f0dbd7a40f0dd9b619ca13b0a0648636c9e21b3b",
    "0d02ab8729630ad3cfb4300702927333b1d349e3",
    "6934b05c7117a12286fb2ba7a47f75e227cacb22",
    "b50a6d78f5e12cfb9e0f52e0af6efbab3618e2e5"
  ],
  "changeHistoryShort": {
    "092fead5d9875fb3760206bcdd76cdafec5e9481": "Ybodychange",
    "4fa1afdb883dab8786d2fb5c72a195dd2e87d711": "Ybodychange",
    "000a4d8e133920d71af304f2015d3674daed74fa": "Ybodychange",
    "f0dbd7a40f0dd9b619ca13b0a0648636c9e21b3b": "Ybodychange",
    "0d02ab8729630ad3cfb4300702927333b1d349e3": "Ybodychange",
    "6934b05c7117a12286fb2ba7a47f75e227cacb22": "Ybodychange",
    "b50a6d78f5e12cfb9e0f52e0af6efbab3618e2e5": "Yintroduced"
  },
  "changeHistoryDetails": {
    "092fead5d9875fb3760206bcdd76cdafec5e9481": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5792. Adopt the id prefix for YARN, MR, and DS entities. Contributed by Varun Saxena.\n",
      "commitDate": "29/08/17 10:59 PM",
      "commitName": "092fead5d9875fb3760206bcdd76cdafec5e9481",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "08/08/17 12:46 PM",
      "commitNameOld": "735fce5bec17f4e1799daf922625c475cf588114",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 21.43,
      "commitsBetweenForRepo": 142,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,126 +1,142 @@\n   private void processEventForNewTimelineService(HistoryEvent event,\n       JobId jobId, long timestamp) {\n     org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity \u003d\n         null;\n     String taskId \u003d null;\n     String taskAttemptId \u003d null;\n     boolean setCreatedTime \u003d false;\n+    long taskIdPrefix \u003d 0;\n+    long taskAttemptIdPrefix \u003d 0;\n \n     switch (event.getEventType()) {\n     // Handle job events\n     case JOB_SUBMITTED:\n       setCreatedTime \u003d true;\n       break;\n     case JOB_STATUS_CHANGED:\n     case JOB_INFO_CHANGED:\n     case JOB_INITED:\n     case JOB_PRIORITY_CHANGED:\n     case JOB_QUEUE_CHANGED:\n     case JOB_FAILED:\n     case JOB_KILLED:\n     case JOB_ERROR:\n     case JOB_FINISHED:\n     case AM_STARTED:\n     case NORMALIZED_RESOURCE:\n       break;\n     // Handle task events\n     case TASK_STARTED:\n       setCreatedTime \u003d true;\n       taskId \u003d ((TaskStartedEvent)event).getTaskId().toString();\n+      taskIdPrefix \u003d TimelineServiceHelper.\n+          invertLong(((TaskStartedEvent)event).getStartTime());\n       break;\n     case TASK_FAILED:\n       taskId \u003d ((TaskFailedEvent)event).getTaskId().toString();\n+      taskIdPrefix \u003d TimelineServiceHelper.\n+          invertLong(((TaskFailedEvent)event).getStartTime());\n       break;\n     case TASK_UPDATED:\n       taskId \u003d ((TaskUpdatedEvent)event).getTaskId().toString();\n       break;\n     case TASK_FINISHED:\n       taskId \u003d ((TaskFinishedEvent)event).getTaskId().toString();\n+      taskIdPrefix \u003d TimelineServiceHelper.\n+          invertLong(((TaskFinishedEvent)event).getStartTime());\n       break;\n     case MAP_ATTEMPT_STARTED:\n     case REDUCE_ATTEMPT_STARTED:\n       setCreatedTime \u003d true;\n       taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n           getTaskAttemptId().toString();\n+      taskAttemptIdPrefix \u003d TimelineServiceHelper.\n+          invertLong(((TaskAttemptStartedEvent)event).getStartTime());\n       break;\n     case CLEANUP_ATTEMPT_STARTED:\n     case SETUP_ATTEMPT_STARTED:\n       taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n           getTaskAttemptId().toString();\n       break;\n     case MAP_ATTEMPT_FAILED:\n     case CLEANUP_ATTEMPT_FAILED:\n     case REDUCE_ATTEMPT_FAILED:\n     case SETUP_ATTEMPT_FAILED:\n     case MAP_ATTEMPT_KILLED:\n     case CLEANUP_ATTEMPT_KILLED:\n     case REDUCE_ATTEMPT_KILLED:\n     case SETUP_ATTEMPT_KILLED:\n       taskId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n           getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n           getTaskAttemptId().toString();\n+      taskAttemptIdPrefix \u003d TimelineServiceHelper.invertLong(\n+          ((TaskAttemptUnsuccessfulCompletionEvent)event).getStartTime());\n       break;\n     case MAP_ATTEMPT_FINISHED:\n       taskId \u003d ((MapAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((MapAttemptFinishedEvent)event).\n           getAttemptId().toString();\n+      taskAttemptIdPrefix \u003d TimelineServiceHelper.\n+          invertLong(((MapAttemptFinishedEvent)event).getStartTime());\n       break;\n     case REDUCE_ATTEMPT_FINISHED:\n       taskId \u003d ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((ReduceAttemptFinishedEvent)event).\n           getAttemptId().toString();\n+      taskAttemptIdPrefix \u003d TimelineServiceHelper.\n+          invertLong(((ReduceAttemptFinishedEvent)event).getStartTime());\n       break;\n     case SETUP_ATTEMPT_FINISHED:\n     case CLEANUP_ATTEMPT_FINISHED:\n       taskId \u003d ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptFinishedEvent)event).\n           getAttemptId().toString();\n       break;\n     default:\n       LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n           \" and handled by timeline service.\");\n       return;\n     }\n \n     org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity\n         appEntityWithJobMetrics \u003d null;\n     if (taskId \u003d\u003d null) {\n       // JobEntity\n       tEntity \u003d createJobEntity(event, timestamp, jobId,\n           MAPREDUCE_JOB_ENTITY_TYPE, setCreatedTime);\n       if (event.getEventType() \u003d\u003d EventType.JOB_FINISHED\n           \u0026\u0026 event.getTimelineMetrics() !\u003d null) {\n         appEntityWithJobMetrics \u003d createAppEntityWithJobMetrics(event, jobId);\n       }\n     } else {\n       if (taskAttemptId \u003d\u003d null) {\n         // TaskEntity\n         tEntity \u003d createTaskEntity(event, timestamp, taskId,\n             MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE,\n-            jobId, setCreatedTime);\n+            jobId, setCreatedTime, taskIdPrefix);\n       } else {\n         // TaskAttemptEntity\n         tEntity \u003d createTaskAttemptEntity(event, timestamp, taskAttemptId,\n             MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n-            taskId, setCreatedTime);\n+            taskId, setCreatedTime, taskAttemptIdPrefix);\n       }\n     }\n     try {\n       if (appEntityWithJobMetrics \u003d\u003d null) {\n         timelineV2Client.putEntitiesAsync(tEntity);\n       } else {\n         timelineV2Client.putEntities(tEntity, appEntityWithJobMetrics);\n       }\n     } catch (IOException | YarnException e) {\n       LOG.error(\"Failed to process Event \" + event.getEventType()\n           + \" for the job : \" + jobId, e);\n       return;\n     }\n     if (event.getEventType() \u003d\u003d EventType.JOB_SUBMITTED) {\n       // Publish configs after main job submitted event has been posted.\n       publishConfigsOnJobSubmittedEvent((JobSubmittedEvent)event, jobId);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processEventForNewTimelineService(HistoryEvent event,\n      JobId jobId, long timestamp) {\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity \u003d\n        null;\n    String taskId \u003d null;\n    String taskAttemptId \u003d null;\n    boolean setCreatedTime \u003d false;\n    long taskIdPrefix \u003d 0;\n    long taskAttemptIdPrefix \u003d 0;\n\n    switch (event.getEventType()) {\n    // Handle job events\n    case JOB_SUBMITTED:\n      setCreatedTime \u003d true;\n      break;\n    case JOB_STATUS_CHANGED:\n    case JOB_INFO_CHANGED:\n    case JOB_INITED:\n    case JOB_PRIORITY_CHANGED:\n    case JOB_QUEUE_CHANGED:\n    case JOB_FAILED:\n    case JOB_KILLED:\n    case JOB_ERROR:\n    case JOB_FINISHED:\n    case AM_STARTED:\n    case NORMALIZED_RESOURCE:\n      break;\n    // Handle task events\n    case TASK_STARTED:\n      setCreatedTime \u003d true;\n      taskId \u003d ((TaskStartedEvent)event).getTaskId().toString();\n      taskIdPrefix \u003d TimelineServiceHelper.\n          invertLong(((TaskStartedEvent)event).getStartTime());\n      break;\n    case TASK_FAILED:\n      taskId \u003d ((TaskFailedEvent)event).getTaskId().toString();\n      taskIdPrefix \u003d TimelineServiceHelper.\n          invertLong(((TaskFailedEvent)event).getStartTime());\n      break;\n    case TASK_UPDATED:\n      taskId \u003d ((TaskUpdatedEvent)event).getTaskId().toString();\n      break;\n    case TASK_FINISHED:\n      taskId \u003d ((TaskFinishedEvent)event).getTaskId().toString();\n      taskIdPrefix \u003d TimelineServiceHelper.\n          invertLong(((TaskFinishedEvent)event).getStartTime());\n      break;\n    case MAP_ATTEMPT_STARTED:\n    case REDUCE_ATTEMPT_STARTED:\n      setCreatedTime \u003d true;\n      taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n          getTaskAttemptId().toString();\n      taskAttemptIdPrefix \u003d TimelineServiceHelper.\n          invertLong(((TaskAttemptStartedEvent)event).getStartTime());\n      break;\n    case CLEANUP_ATTEMPT_STARTED:\n    case SETUP_ATTEMPT_STARTED:\n      taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case MAP_ATTEMPT_FAILED:\n    case CLEANUP_ATTEMPT_FAILED:\n    case REDUCE_ATTEMPT_FAILED:\n    case SETUP_ATTEMPT_FAILED:\n    case MAP_ATTEMPT_KILLED:\n    case CLEANUP_ATTEMPT_KILLED:\n    case REDUCE_ATTEMPT_KILLED:\n    case SETUP_ATTEMPT_KILLED:\n      taskId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n          getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n          getTaskAttemptId().toString();\n      taskAttemptIdPrefix \u003d TimelineServiceHelper.invertLong(\n          ((TaskAttemptUnsuccessfulCompletionEvent)event).getStartTime());\n      break;\n    case MAP_ATTEMPT_FINISHED:\n      taskId \u003d ((MapAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((MapAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      taskAttemptIdPrefix \u003d TimelineServiceHelper.\n          invertLong(((MapAttemptFinishedEvent)event).getStartTime());\n      break;\n    case REDUCE_ATTEMPT_FINISHED:\n      taskId \u003d ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((ReduceAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      taskAttemptIdPrefix \u003d TimelineServiceHelper.\n          invertLong(((ReduceAttemptFinishedEvent)event).getStartTime());\n      break;\n    case SETUP_ATTEMPT_FINISHED:\n    case CLEANUP_ATTEMPT_FINISHED:\n      taskId \u003d ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    default:\n      LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n          \" and handled by timeline service.\");\n      return;\n    }\n\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity\n        appEntityWithJobMetrics \u003d null;\n    if (taskId \u003d\u003d null) {\n      // JobEntity\n      tEntity \u003d createJobEntity(event, timestamp, jobId,\n          MAPREDUCE_JOB_ENTITY_TYPE, setCreatedTime);\n      if (event.getEventType() \u003d\u003d EventType.JOB_FINISHED\n          \u0026\u0026 event.getTimelineMetrics() !\u003d null) {\n        appEntityWithJobMetrics \u003d createAppEntityWithJobMetrics(event, jobId);\n      }\n    } else {\n      if (taskAttemptId \u003d\u003d null) {\n        // TaskEntity\n        tEntity \u003d createTaskEntity(event, timestamp, taskId,\n            MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE,\n            jobId, setCreatedTime, taskIdPrefix);\n      } else {\n        // TaskAttemptEntity\n        tEntity \u003d createTaskAttemptEntity(event, timestamp, taskAttemptId,\n            MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n            taskId, setCreatedTime, taskAttemptIdPrefix);\n      }\n    }\n    try {\n      if (appEntityWithJobMetrics \u003d\u003d null) {\n        timelineV2Client.putEntitiesAsync(tEntity);\n      } else {\n        timelineV2Client.putEntities(tEntity, appEntityWithJobMetrics);\n      }\n    } catch (IOException | YarnException e) {\n      LOG.error(\"Failed to process Event \" + event.getEventType()\n          + \" for the job : \" + jobId, e);\n      return;\n    }\n    if (event.getEventType() \u003d\u003d EventType.JOB_SUBMITTED) {\n      // Publish configs after main job submitted event has been posted.\n      publishConfigsOnJobSubmittedEvent((JobSubmittedEvent)event, jobId);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java",
      "extendedDetails": {}
    },
    "4fa1afdb883dab8786d2fb5c72a195dd2e87d711": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4675. Reorganize TimelineClient and TimelineClientImpl into separate classes for ATSv1.x and ATSv2. Contributed by Naganarasimha G R.\n",
      "commitDate": "16/02/17 11:41 AM",
      "commitName": "4fa1afdb883dab8786d2fb5c72a195dd2e87d711",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "15/11/16 10:57 AM",
      "commitNameOld": "5af572b6443715b7a741296c1bd520a1840f9a7c",
      "commitAuthorOld": "Mingliang Liu",
      "daysBetweenCommits": 93.03,
      "commitsBetweenForRepo": 475,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,126 +1,126 @@\n   private void processEventForNewTimelineService(HistoryEvent event,\n       JobId jobId, long timestamp) {\n     org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity \u003d\n         null;\n     String taskId \u003d null;\n     String taskAttemptId \u003d null;\n     boolean setCreatedTime \u003d false;\n \n     switch (event.getEventType()) {\n     // Handle job events\n     case JOB_SUBMITTED:\n       setCreatedTime \u003d true;\n       break;\n     case JOB_STATUS_CHANGED:\n     case JOB_INFO_CHANGED:\n     case JOB_INITED:\n     case JOB_PRIORITY_CHANGED:\n     case JOB_QUEUE_CHANGED:\n     case JOB_FAILED:\n     case JOB_KILLED:\n     case JOB_ERROR:\n     case JOB_FINISHED:\n     case AM_STARTED:\n     case NORMALIZED_RESOURCE:\n       break;\n     // Handle task events\n     case TASK_STARTED:\n       setCreatedTime \u003d true;\n       taskId \u003d ((TaskStartedEvent)event).getTaskId().toString();\n       break;\n     case TASK_FAILED:\n       taskId \u003d ((TaskFailedEvent)event).getTaskId().toString();\n       break;\n     case TASK_UPDATED:\n       taskId \u003d ((TaskUpdatedEvent)event).getTaskId().toString();\n       break;\n     case TASK_FINISHED:\n       taskId \u003d ((TaskFinishedEvent)event).getTaskId().toString();\n       break;\n     case MAP_ATTEMPT_STARTED:\n     case REDUCE_ATTEMPT_STARTED:\n       setCreatedTime \u003d true;\n       taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n           getTaskAttemptId().toString();\n       break;\n     case CLEANUP_ATTEMPT_STARTED:\n     case SETUP_ATTEMPT_STARTED:\n       taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n           getTaskAttemptId().toString();\n       break;\n     case MAP_ATTEMPT_FAILED:\n     case CLEANUP_ATTEMPT_FAILED:\n     case REDUCE_ATTEMPT_FAILED:\n     case SETUP_ATTEMPT_FAILED:\n     case MAP_ATTEMPT_KILLED:\n     case CLEANUP_ATTEMPT_KILLED:\n     case REDUCE_ATTEMPT_KILLED:\n     case SETUP_ATTEMPT_KILLED:\n       taskId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n           getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n           getTaskAttemptId().toString();\n       break;\n     case MAP_ATTEMPT_FINISHED:\n       taskId \u003d ((MapAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((MapAttemptFinishedEvent)event).\n           getAttemptId().toString();\n       break;\n     case REDUCE_ATTEMPT_FINISHED:\n       taskId \u003d ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((ReduceAttemptFinishedEvent)event).\n           getAttemptId().toString();\n       break;\n     case SETUP_ATTEMPT_FINISHED:\n     case CLEANUP_ATTEMPT_FINISHED:\n       taskId \u003d ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptFinishedEvent)event).\n           getAttemptId().toString();\n       break;\n     default:\n       LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n           \" and handled by timeline service.\");\n       return;\n     }\n \n     org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity\n         appEntityWithJobMetrics \u003d null;\n     if (taskId \u003d\u003d null) {\n       // JobEntity\n       tEntity \u003d createJobEntity(event, timestamp, jobId,\n           MAPREDUCE_JOB_ENTITY_TYPE, setCreatedTime);\n       if (event.getEventType() \u003d\u003d EventType.JOB_FINISHED\n           \u0026\u0026 event.getTimelineMetrics() !\u003d null) {\n         appEntityWithJobMetrics \u003d createAppEntityWithJobMetrics(event, jobId);\n       }\n     } else {\n       if (taskAttemptId \u003d\u003d null) {\n         // TaskEntity\n         tEntity \u003d createTaskEntity(event, timestamp, taskId,\n             MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE,\n             jobId, setCreatedTime);\n       } else {\n         // TaskAttemptEntity\n         tEntity \u003d createTaskAttemptEntity(event, timestamp, taskAttemptId,\n             MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n             taskId, setCreatedTime);\n       }\n     }\n     try {\n       if (appEntityWithJobMetrics \u003d\u003d null) {\n-        timelineClient.putEntitiesAsync(tEntity);\n+        timelineV2Client.putEntitiesAsync(tEntity);\n       } else {\n-        timelineClient.putEntities(tEntity, appEntityWithJobMetrics);\n+        timelineV2Client.putEntities(tEntity, appEntityWithJobMetrics);\n       }\n     } catch (IOException | YarnException e) {\n       LOG.error(\"Failed to process Event \" + event.getEventType()\n           + \" for the job : \" + jobId, e);\n       return;\n     }\n     if (event.getEventType() \u003d\u003d EventType.JOB_SUBMITTED) {\n       // Publish configs after main job submitted event has been posted.\n       publishConfigsOnJobSubmittedEvent((JobSubmittedEvent)event, jobId);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processEventForNewTimelineService(HistoryEvent event,\n      JobId jobId, long timestamp) {\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity \u003d\n        null;\n    String taskId \u003d null;\n    String taskAttemptId \u003d null;\n    boolean setCreatedTime \u003d false;\n\n    switch (event.getEventType()) {\n    // Handle job events\n    case JOB_SUBMITTED:\n      setCreatedTime \u003d true;\n      break;\n    case JOB_STATUS_CHANGED:\n    case JOB_INFO_CHANGED:\n    case JOB_INITED:\n    case JOB_PRIORITY_CHANGED:\n    case JOB_QUEUE_CHANGED:\n    case JOB_FAILED:\n    case JOB_KILLED:\n    case JOB_ERROR:\n    case JOB_FINISHED:\n    case AM_STARTED:\n    case NORMALIZED_RESOURCE:\n      break;\n    // Handle task events\n    case TASK_STARTED:\n      setCreatedTime \u003d true;\n      taskId \u003d ((TaskStartedEvent)event).getTaskId().toString();\n      break;\n    case TASK_FAILED:\n      taskId \u003d ((TaskFailedEvent)event).getTaskId().toString();\n      break;\n    case TASK_UPDATED:\n      taskId \u003d ((TaskUpdatedEvent)event).getTaskId().toString();\n      break;\n    case TASK_FINISHED:\n      taskId \u003d ((TaskFinishedEvent)event).getTaskId().toString();\n      break;\n    case MAP_ATTEMPT_STARTED:\n    case REDUCE_ATTEMPT_STARTED:\n      setCreatedTime \u003d true;\n      taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case CLEANUP_ATTEMPT_STARTED:\n    case SETUP_ATTEMPT_STARTED:\n      taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case MAP_ATTEMPT_FAILED:\n    case CLEANUP_ATTEMPT_FAILED:\n    case REDUCE_ATTEMPT_FAILED:\n    case SETUP_ATTEMPT_FAILED:\n    case MAP_ATTEMPT_KILLED:\n    case CLEANUP_ATTEMPT_KILLED:\n    case REDUCE_ATTEMPT_KILLED:\n    case SETUP_ATTEMPT_KILLED:\n      taskId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n          getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case MAP_ATTEMPT_FINISHED:\n      taskId \u003d ((MapAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((MapAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    case REDUCE_ATTEMPT_FINISHED:\n      taskId \u003d ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((ReduceAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    case SETUP_ATTEMPT_FINISHED:\n    case CLEANUP_ATTEMPT_FINISHED:\n      taskId \u003d ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    default:\n      LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n          \" and handled by timeline service.\");\n      return;\n    }\n\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity\n        appEntityWithJobMetrics \u003d null;\n    if (taskId \u003d\u003d null) {\n      // JobEntity\n      tEntity \u003d createJobEntity(event, timestamp, jobId,\n          MAPREDUCE_JOB_ENTITY_TYPE, setCreatedTime);\n      if (event.getEventType() \u003d\u003d EventType.JOB_FINISHED\n          \u0026\u0026 event.getTimelineMetrics() !\u003d null) {\n        appEntityWithJobMetrics \u003d createAppEntityWithJobMetrics(event, jobId);\n      }\n    } else {\n      if (taskAttemptId \u003d\u003d null) {\n        // TaskEntity\n        tEntity \u003d createTaskEntity(event, timestamp, taskId,\n            MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE,\n            jobId, setCreatedTime);\n      } else {\n        // TaskAttemptEntity\n        tEntity \u003d createTaskAttemptEntity(event, timestamp, taskAttemptId,\n            MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n            taskId, setCreatedTime);\n      }\n    }\n    try {\n      if (appEntityWithJobMetrics \u003d\u003d null) {\n        timelineV2Client.putEntitiesAsync(tEntity);\n      } else {\n        timelineV2Client.putEntities(tEntity, appEntityWithJobMetrics);\n      }\n    } catch (IOException | YarnException e) {\n      LOG.error(\"Failed to process Event \" + event.getEventType()\n          + \" for the job : \" + jobId, e);\n      return;\n    }\n    if (event.getEventType() \u003d\u003d EventType.JOB_SUBMITTED) {\n      // Publish configs after main job submitted event has been posted.\n      publishConfigsOnJobSubmittedEvent((JobSubmittedEvent)event, jobId);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java",
      "extendedDetails": {}
    },
    "000a4d8e133920d71af304f2015d3674daed74fa": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6688. Store job configurations in Timeline Service v2 (Varun Saxena via sjlee)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "000a4d8e133920d71af304f2015d3674daed74fa",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "f0dbd7a40f0dd9b619ca13b0a0648636c9e21b3b",
      "commitAuthorOld": "Varun Saxena",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,122 +1,126 @@\n   private void processEventForNewTimelineService(HistoryEvent event,\n       JobId jobId, long timestamp) {\n     org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity \u003d\n         null;\n     String taskId \u003d null;\n     String taskAttemptId \u003d null;\n     boolean setCreatedTime \u003d false;\n \n     switch (event.getEventType()) {\n     // Handle job events\n     case JOB_SUBMITTED:\n       setCreatedTime \u003d true;\n       break;\n     case JOB_STATUS_CHANGED:\n     case JOB_INFO_CHANGED:\n     case JOB_INITED:\n     case JOB_PRIORITY_CHANGED:\n     case JOB_QUEUE_CHANGED:\n     case JOB_FAILED:\n     case JOB_KILLED:\n     case JOB_ERROR:\n     case JOB_FINISHED:\n     case AM_STARTED:\n     case NORMALIZED_RESOURCE:\n       break;\n     // Handle task events\n     case TASK_STARTED:\n       setCreatedTime \u003d true;\n       taskId \u003d ((TaskStartedEvent)event).getTaskId().toString();\n       break;\n     case TASK_FAILED:\n       taskId \u003d ((TaskFailedEvent)event).getTaskId().toString();\n       break;\n     case TASK_UPDATED:\n       taskId \u003d ((TaskUpdatedEvent)event).getTaskId().toString();\n       break;\n     case TASK_FINISHED:\n       taskId \u003d ((TaskFinishedEvent)event).getTaskId().toString();\n       break;\n     case MAP_ATTEMPT_STARTED:\n     case REDUCE_ATTEMPT_STARTED:\n       setCreatedTime \u003d true;\n       taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n           getTaskAttemptId().toString();\n       break;\n     case CLEANUP_ATTEMPT_STARTED:\n     case SETUP_ATTEMPT_STARTED:\n       taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n           getTaskAttemptId().toString();\n       break;\n     case MAP_ATTEMPT_FAILED:\n     case CLEANUP_ATTEMPT_FAILED:\n     case REDUCE_ATTEMPT_FAILED:\n     case SETUP_ATTEMPT_FAILED:\n     case MAP_ATTEMPT_KILLED:\n     case CLEANUP_ATTEMPT_KILLED:\n     case REDUCE_ATTEMPT_KILLED:\n     case SETUP_ATTEMPT_KILLED:\n       taskId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n           getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n           getTaskAttemptId().toString();\n       break;\n     case MAP_ATTEMPT_FINISHED:\n       taskId \u003d ((MapAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((MapAttemptFinishedEvent)event).\n           getAttemptId().toString();\n       break;\n     case REDUCE_ATTEMPT_FINISHED:\n       taskId \u003d ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((ReduceAttemptFinishedEvent)event).\n           getAttemptId().toString();\n       break;\n     case SETUP_ATTEMPT_FINISHED:\n     case CLEANUP_ATTEMPT_FINISHED:\n       taskId \u003d ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptFinishedEvent)event).\n           getAttemptId().toString();\n       break;\n     default:\n       LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n           \" and handled by timeline service.\");\n       return;\n     }\n \n     org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity\n         appEntityWithJobMetrics \u003d null;\n     if (taskId \u003d\u003d null) {\n       // JobEntity\n       tEntity \u003d createJobEntity(event, timestamp, jobId,\n           MAPREDUCE_JOB_ENTITY_TYPE, setCreatedTime);\n       if (event.getEventType() \u003d\u003d EventType.JOB_FINISHED\n           \u0026\u0026 event.getTimelineMetrics() !\u003d null) {\n         appEntityWithJobMetrics \u003d createAppEntityWithJobMetrics(event, jobId);\n       }\n     } else {\n       if (taskAttemptId \u003d\u003d null) {\n         // TaskEntity\n         tEntity \u003d createTaskEntity(event, timestamp, taskId,\n             MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE,\n             jobId, setCreatedTime);\n       } else {\n         // TaskAttemptEntity\n         tEntity \u003d createTaskAttemptEntity(event, timestamp, taskAttemptId,\n             MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n             taskId, setCreatedTime);\n       }\n     }\n     try {\n       if (appEntityWithJobMetrics \u003d\u003d null) {\n         timelineClient.putEntitiesAsync(tEntity);\n       } else {\n         timelineClient.putEntities(tEntity, appEntityWithJobMetrics);\n       }\n     } catch (IOException | YarnException e) {\n       LOG.error(\"Failed to process Event \" + event.getEventType()\n           + \" for the job : \" + jobId, e);\n+      return;\n     }\n-\n+    if (event.getEventType() \u003d\u003d EventType.JOB_SUBMITTED) {\n+      // Publish configs after main job submitted event has been posted.\n+      publishConfigsOnJobSubmittedEvent((JobSubmittedEvent)event, jobId);\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processEventForNewTimelineService(HistoryEvent event,\n      JobId jobId, long timestamp) {\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity \u003d\n        null;\n    String taskId \u003d null;\n    String taskAttemptId \u003d null;\n    boolean setCreatedTime \u003d false;\n\n    switch (event.getEventType()) {\n    // Handle job events\n    case JOB_SUBMITTED:\n      setCreatedTime \u003d true;\n      break;\n    case JOB_STATUS_CHANGED:\n    case JOB_INFO_CHANGED:\n    case JOB_INITED:\n    case JOB_PRIORITY_CHANGED:\n    case JOB_QUEUE_CHANGED:\n    case JOB_FAILED:\n    case JOB_KILLED:\n    case JOB_ERROR:\n    case JOB_FINISHED:\n    case AM_STARTED:\n    case NORMALIZED_RESOURCE:\n      break;\n    // Handle task events\n    case TASK_STARTED:\n      setCreatedTime \u003d true;\n      taskId \u003d ((TaskStartedEvent)event).getTaskId().toString();\n      break;\n    case TASK_FAILED:\n      taskId \u003d ((TaskFailedEvent)event).getTaskId().toString();\n      break;\n    case TASK_UPDATED:\n      taskId \u003d ((TaskUpdatedEvent)event).getTaskId().toString();\n      break;\n    case TASK_FINISHED:\n      taskId \u003d ((TaskFinishedEvent)event).getTaskId().toString();\n      break;\n    case MAP_ATTEMPT_STARTED:\n    case REDUCE_ATTEMPT_STARTED:\n      setCreatedTime \u003d true;\n      taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case CLEANUP_ATTEMPT_STARTED:\n    case SETUP_ATTEMPT_STARTED:\n      taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case MAP_ATTEMPT_FAILED:\n    case CLEANUP_ATTEMPT_FAILED:\n    case REDUCE_ATTEMPT_FAILED:\n    case SETUP_ATTEMPT_FAILED:\n    case MAP_ATTEMPT_KILLED:\n    case CLEANUP_ATTEMPT_KILLED:\n    case REDUCE_ATTEMPT_KILLED:\n    case SETUP_ATTEMPT_KILLED:\n      taskId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n          getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case MAP_ATTEMPT_FINISHED:\n      taskId \u003d ((MapAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((MapAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    case REDUCE_ATTEMPT_FINISHED:\n      taskId \u003d ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((ReduceAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    case SETUP_ATTEMPT_FINISHED:\n    case CLEANUP_ATTEMPT_FINISHED:\n      taskId \u003d ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    default:\n      LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n          \" and handled by timeline service.\");\n      return;\n    }\n\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity\n        appEntityWithJobMetrics \u003d null;\n    if (taskId \u003d\u003d null) {\n      // JobEntity\n      tEntity \u003d createJobEntity(event, timestamp, jobId,\n          MAPREDUCE_JOB_ENTITY_TYPE, setCreatedTime);\n      if (event.getEventType() \u003d\u003d EventType.JOB_FINISHED\n          \u0026\u0026 event.getTimelineMetrics() !\u003d null) {\n        appEntityWithJobMetrics \u003d createAppEntityWithJobMetrics(event, jobId);\n      }\n    } else {\n      if (taskAttemptId \u003d\u003d null) {\n        // TaskEntity\n        tEntity \u003d createTaskEntity(event, timestamp, taskId,\n            MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE,\n            jobId, setCreatedTime);\n      } else {\n        // TaskAttemptEntity\n        tEntity \u003d createTaskAttemptEntity(event, timestamp, taskAttemptId,\n            MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n            taskId, setCreatedTime);\n      }\n    }\n    try {\n      if (appEntityWithJobMetrics \u003d\u003d null) {\n        timelineClient.putEntitiesAsync(tEntity);\n      } else {\n        timelineClient.putEntities(tEntity, appEntityWithJobMetrics);\n      }\n    } catch (IOException | YarnException e) {\n      LOG.error(\"Failed to process Event \" + event.getEventType()\n          + \" for the job : \" + jobId, e);\n      return;\n    }\n    if (event.getEventType() \u003d\u003d EventType.JOB_SUBMITTED) {\n      // Publish configs after main job submitted event has been posted.\n      publishConfigsOnJobSubmittedEvent((JobSubmittedEvent)event, jobId);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java",
      "extendedDetails": {}
    },
    "f0dbd7a40f0dd9b619ca13b0a0648636c9e21b3b": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6424. Store MR counters as timeline metrics instead of event. (Naganarasimha G R via varunsaxena)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "f0dbd7a40f0dd9b619ca13b0a0648636c9e21b3b",
      "commitAuthor": "Varun Saxena",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "0d02ab8729630ad3cfb4300702927333b1d349e3",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,111 +1,122 @@\n   private void processEventForNewTimelineService(HistoryEvent event,\n       JobId jobId, long timestamp) {\n     org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity \u003d\n         null;\n     String taskId \u003d null;\n     String taskAttemptId \u003d null;\n     boolean setCreatedTime \u003d false;\n \n     switch (event.getEventType()) {\n     // Handle job events\n     case JOB_SUBMITTED:\n       setCreatedTime \u003d true;\n       break;\n     case JOB_STATUS_CHANGED:\n     case JOB_INFO_CHANGED:\n     case JOB_INITED:\n     case JOB_PRIORITY_CHANGED:\n     case JOB_QUEUE_CHANGED:\n     case JOB_FAILED:\n     case JOB_KILLED:\n     case JOB_ERROR:\n     case JOB_FINISHED:\n     case AM_STARTED:\n     case NORMALIZED_RESOURCE:\n       break;\n     // Handle task events\n     case TASK_STARTED:\n       setCreatedTime \u003d true;\n       taskId \u003d ((TaskStartedEvent)event).getTaskId().toString();\n       break;\n     case TASK_FAILED:\n       taskId \u003d ((TaskFailedEvent)event).getTaskId().toString();\n       break;\n     case TASK_UPDATED:\n       taskId \u003d ((TaskUpdatedEvent)event).getTaskId().toString();\n       break;\n     case TASK_FINISHED:\n       taskId \u003d ((TaskFinishedEvent)event).getTaskId().toString();\n       break;\n     case MAP_ATTEMPT_STARTED:\n     case REDUCE_ATTEMPT_STARTED:\n       setCreatedTime \u003d true;\n       taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n           getTaskAttemptId().toString();\n       break;\n     case CLEANUP_ATTEMPT_STARTED:\n     case SETUP_ATTEMPT_STARTED:\n       taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n           getTaskAttemptId().toString();\n       break;\n     case MAP_ATTEMPT_FAILED:\n     case CLEANUP_ATTEMPT_FAILED:\n     case REDUCE_ATTEMPT_FAILED:\n     case SETUP_ATTEMPT_FAILED:\n     case MAP_ATTEMPT_KILLED:\n     case CLEANUP_ATTEMPT_KILLED:\n     case REDUCE_ATTEMPT_KILLED:\n     case SETUP_ATTEMPT_KILLED:\n       taskId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n           getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n           getTaskAttemptId().toString();\n       break;\n     case MAP_ATTEMPT_FINISHED:\n       taskId \u003d ((MapAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((MapAttemptFinishedEvent)event).\n           getAttemptId().toString();\n       break;\n     case REDUCE_ATTEMPT_FINISHED:\n       taskId \u003d ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((ReduceAttemptFinishedEvent)event).\n           getAttemptId().toString();\n       break;\n     case SETUP_ATTEMPT_FINISHED:\n     case CLEANUP_ATTEMPT_FINISHED:\n       taskId \u003d ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptFinishedEvent)event).\n           getAttemptId().toString();\n       break;\n     default:\n       LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n           \" and handled by timeline service.\");\n       return;\n     }\n+\n+    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity\n+        appEntityWithJobMetrics \u003d null;\n     if (taskId \u003d\u003d null) {\n       // JobEntity\n       tEntity \u003d createJobEntity(event, timestamp, jobId,\n           MAPREDUCE_JOB_ENTITY_TYPE, setCreatedTime);\n+      if (event.getEventType() \u003d\u003d EventType.JOB_FINISHED\n+          \u0026\u0026 event.getTimelineMetrics() !\u003d null) {\n+        appEntityWithJobMetrics \u003d createAppEntityWithJobMetrics(event, jobId);\n+      }\n     } else {\n       if (taskAttemptId \u003d\u003d null) {\n         // TaskEntity\n         tEntity \u003d createTaskEntity(event, timestamp, taskId,\n             MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE,\n             jobId, setCreatedTime);\n       } else {\n         // TaskAttemptEntity\n         tEntity \u003d createTaskAttemptEntity(event, timestamp, taskAttemptId,\n             MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n             taskId, setCreatedTime);\n       }\n     }\n     try {\n-      timelineClient.putEntitiesAsync(tEntity);\n+      if (appEntityWithJobMetrics \u003d\u003d null) {\n+        timelineClient.putEntitiesAsync(tEntity);\n+      } else {\n+        timelineClient.putEntities(tEntity, appEntityWithJobMetrics);\n+      }\n     } catch (IOException | YarnException e) {\n       LOG.error(\"Failed to process Event \" + event.getEventType()\n           + \" for the job : \" + jobId, e);\n     }\n \n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processEventForNewTimelineService(HistoryEvent event,\n      JobId jobId, long timestamp) {\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity \u003d\n        null;\n    String taskId \u003d null;\n    String taskAttemptId \u003d null;\n    boolean setCreatedTime \u003d false;\n\n    switch (event.getEventType()) {\n    // Handle job events\n    case JOB_SUBMITTED:\n      setCreatedTime \u003d true;\n      break;\n    case JOB_STATUS_CHANGED:\n    case JOB_INFO_CHANGED:\n    case JOB_INITED:\n    case JOB_PRIORITY_CHANGED:\n    case JOB_QUEUE_CHANGED:\n    case JOB_FAILED:\n    case JOB_KILLED:\n    case JOB_ERROR:\n    case JOB_FINISHED:\n    case AM_STARTED:\n    case NORMALIZED_RESOURCE:\n      break;\n    // Handle task events\n    case TASK_STARTED:\n      setCreatedTime \u003d true;\n      taskId \u003d ((TaskStartedEvent)event).getTaskId().toString();\n      break;\n    case TASK_FAILED:\n      taskId \u003d ((TaskFailedEvent)event).getTaskId().toString();\n      break;\n    case TASK_UPDATED:\n      taskId \u003d ((TaskUpdatedEvent)event).getTaskId().toString();\n      break;\n    case TASK_FINISHED:\n      taskId \u003d ((TaskFinishedEvent)event).getTaskId().toString();\n      break;\n    case MAP_ATTEMPT_STARTED:\n    case REDUCE_ATTEMPT_STARTED:\n      setCreatedTime \u003d true;\n      taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case CLEANUP_ATTEMPT_STARTED:\n    case SETUP_ATTEMPT_STARTED:\n      taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case MAP_ATTEMPT_FAILED:\n    case CLEANUP_ATTEMPT_FAILED:\n    case REDUCE_ATTEMPT_FAILED:\n    case SETUP_ATTEMPT_FAILED:\n    case MAP_ATTEMPT_KILLED:\n    case CLEANUP_ATTEMPT_KILLED:\n    case REDUCE_ATTEMPT_KILLED:\n    case SETUP_ATTEMPT_KILLED:\n      taskId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n          getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case MAP_ATTEMPT_FINISHED:\n      taskId \u003d ((MapAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((MapAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    case REDUCE_ATTEMPT_FINISHED:\n      taskId \u003d ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((ReduceAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    case SETUP_ATTEMPT_FINISHED:\n    case CLEANUP_ATTEMPT_FINISHED:\n      taskId \u003d ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    default:\n      LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n          \" and handled by timeline service.\");\n      return;\n    }\n\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity\n        appEntityWithJobMetrics \u003d null;\n    if (taskId \u003d\u003d null) {\n      // JobEntity\n      tEntity \u003d createJobEntity(event, timestamp, jobId,\n          MAPREDUCE_JOB_ENTITY_TYPE, setCreatedTime);\n      if (event.getEventType() \u003d\u003d EventType.JOB_FINISHED\n          \u0026\u0026 event.getTimelineMetrics() !\u003d null) {\n        appEntityWithJobMetrics \u003d createAppEntityWithJobMetrics(event, jobId);\n      }\n    } else {\n      if (taskAttemptId \u003d\u003d null) {\n        // TaskEntity\n        tEntity \u003d createTaskEntity(event, timestamp, taskId,\n            MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE,\n            jobId, setCreatedTime);\n      } else {\n        // TaskAttemptEntity\n        tEntity \u003d createTaskAttemptEntity(event, timestamp, taskAttemptId,\n            MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n            taskId, setCreatedTime);\n      }\n    }\n    try {\n      if (appEntityWithJobMetrics \u003d\u003d null) {\n        timelineClient.putEntitiesAsync(tEntity);\n      } else {\n        timelineClient.putEntities(tEntity, appEntityWithJobMetrics);\n      }\n    } catch (IOException | YarnException e) {\n      LOG.error(\"Failed to process Event \" + event.getEventType()\n          + \" for the job : \" + jobId, e);\n    }\n\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java",
      "extendedDetails": {}
    },
    "0d02ab8729630ad3cfb4300702927333b1d349e3": {
      "type": "Ybodychange",
      "commitMessage": "YARN-3367. Replace starting a separate thread for post entity with event loop in TimelineClient (Naganarasimha G R via sjlee)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "0d02ab8729630ad3cfb4300702927333b1d349e3",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "960af7d4717b8a8949d0b2e43949e7daab45aa88",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,105 +1,111 @@\n   private void processEventForNewTimelineService(HistoryEvent event,\n       JobId jobId, long timestamp) {\n     org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity \u003d\n         null;\n     String taskId \u003d null;\n     String taskAttemptId \u003d null;\n     boolean setCreatedTime \u003d false;\n \n     switch (event.getEventType()) {\n     // Handle job events\n     case JOB_SUBMITTED:\n       setCreatedTime \u003d true;\n       break;\n     case JOB_STATUS_CHANGED:\n     case JOB_INFO_CHANGED:\n     case JOB_INITED:\n     case JOB_PRIORITY_CHANGED:\n     case JOB_QUEUE_CHANGED:\n     case JOB_FAILED:\n     case JOB_KILLED:\n     case JOB_ERROR:\n     case JOB_FINISHED:\n     case AM_STARTED:\n     case NORMALIZED_RESOURCE:\n       break;\n     // Handle task events\n     case TASK_STARTED:\n       setCreatedTime \u003d true;\n       taskId \u003d ((TaskStartedEvent)event).getTaskId().toString();\n       break;\n     case TASK_FAILED:\n       taskId \u003d ((TaskFailedEvent)event).getTaskId().toString();\n       break;\n     case TASK_UPDATED:\n       taskId \u003d ((TaskUpdatedEvent)event).getTaskId().toString();\n       break;\n     case TASK_FINISHED:\n       taskId \u003d ((TaskFinishedEvent)event).getTaskId().toString();\n       break;\n     case MAP_ATTEMPT_STARTED:\n     case REDUCE_ATTEMPT_STARTED:\n       setCreatedTime \u003d true;\n       taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n           getTaskAttemptId().toString();\n       break;\n     case CLEANUP_ATTEMPT_STARTED:\n     case SETUP_ATTEMPT_STARTED:\n       taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n           getTaskAttemptId().toString();\n       break;\n     case MAP_ATTEMPT_FAILED:\n     case CLEANUP_ATTEMPT_FAILED:\n     case REDUCE_ATTEMPT_FAILED:\n     case SETUP_ATTEMPT_FAILED:\n     case MAP_ATTEMPT_KILLED:\n     case CLEANUP_ATTEMPT_KILLED:\n     case REDUCE_ATTEMPT_KILLED:\n     case SETUP_ATTEMPT_KILLED:\n       taskId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n           getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n           getTaskAttemptId().toString();\n       break;\n     case MAP_ATTEMPT_FINISHED:\n       taskId \u003d ((MapAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((MapAttemptFinishedEvent)event).\n           getAttemptId().toString();\n       break;\n     case REDUCE_ATTEMPT_FINISHED:\n       taskId \u003d ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((ReduceAttemptFinishedEvent)event).\n           getAttemptId().toString();\n       break;\n     case SETUP_ATTEMPT_FINISHED:\n     case CLEANUP_ATTEMPT_FINISHED:\n       taskId \u003d ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n       taskAttemptId \u003d ((TaskAttemptFinishedEvent)event).\n           getAttemptId().toString();\n       break;\n     default:\n       LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n           \" and handled by timeline service.\");\n       return;\n     }\n     if (taskId \u003d\u003d null) {\n       // JobEntity\n       tEntity \u003d createJobEntity(event, timestamp, jobId,\n           MAPREDUCE_JOB_ENTITY_TYPE, setCreatedTime);\n     } else {\n       if (taskAttemptId \u003d\u003d null) {\n         // TaskEntity\n         tEntity \u003d createTaskEntity(event, timestamp, taskId,\n             MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE,\n             jobId, setCreatedTime);\n       } else {\n         // TaskAttemptEntity\n         tEntity \u003d createTaskAttemptEntity(event, timestamp, taskAttemptId,\n             MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n             taskId, setCreatedTime);\n       }\n     }\n-    putEntityWithoutBlocking(timelineClient, tEntity);\n+    try {\n+      timelineClient.putEntitiesAsync(tEntity);\n+    } catch (IOException | YarnException e) {\n+      LOG.error(\"Failed to process Event \" + event.getEventType()\n+          + \" for the job : \" + jobId, e);\n+    }\n+\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processEventForNewTimelineService(HistoryEvent event,\n      JobId jobId, long timestamp) {\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity \u003d\n        null;\n    String taskId \u003d null;\n    String taskAttemptId \u003d null;\n    boolean setCreatedTime \u003d false;\n\n    switch (event.getEventType()) {\n    // Handle job events\n    case JOB_SUBMITTED:\n      setCreatedTime \u003d true;\n      break;\n    case JOB_STATUS_CHANGED:\n    case JOB_INFO_CHANGED:\n    case JOB_INITED:\n    case JOB_PRIORITY_CHANGED:\n    case JOB_QUEUE_CHANGED:\n    case JOB_FAILED:\n    case JOB_KILLED:\n    case JOB_ERROR:\n    case JOB_FINISHED:\n    case AM_STARTED:\n    case NORMALIZED_RESOURCE:\n      break;\n    // Handle task events\n    case TASK_STARTED:\n      setCreatedTime \u003d true;\n      taskId \u003d ((TaskStartedEvent)event).getTaskId().toString();\n      break;\n    case TASK_FAILED:\n      taskId \u003d ((TaskFailedEvent)event).getTaskId().toString();\n      break;\n    case TASK_UPDATED:\n      taskId \u003d ((TaskUpdatedEvent)event).getTaskId().toString();\n      break;\n    case TASK_FINISHED:\n      taskId \u003d ((TaskFinishedEvent)event).getTaskId().toString();\n      break;\n    case MAP_ATTEMPT_STARTED:\n    case REDUCE_ATTEMPT_STARTED:\n      setCreatedTime \u003d true;\n      taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case CLEANUP_ATTEMPT_STARTED:\n    case SETUP_ATTEMPT_STARTED:\n      taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case MAP_ATTEMPT_FAILED:\n    case CLEANUP_ATTEMPT_FAILED:\n    case REDUCE_ATTEMPT_FAILED:\n    case SETUP_ATTEMPT_FAILED:\n    case MAP_ATTEMPT_KILLED:\n    case CLEANUP_ATTEMPT_KILLED:\n    case REDUCE_ATTEMPT_KILLED:\n    case SETUP_ATTEMPT_KILLED:\n      taskId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n          getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n          getTaskAttemptId().toString();\n      break;\n    case MAP_ATTEMPT_FINISHED:\n      taskId \u003d ((MapAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((MapAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    case REDUCE_ATTEMPT_FINISHED:\n      taskId \u003d ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((ReduceAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    case SETUP_ATTEMPT_FINISHED:\n    case CLEANUP_ATTEMPT_FINISHED:\n      taskId \u003d ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n      taskAttemptId \u003d ((TaskAttemptFinishedEvent)event).\n          getAttemptId().toString();\n      break;\n    default:\n      LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n          \" and handled by timeline service.\");\n      return;\n    }\n    if (taskId \u003d\u003d null) {\n      // JobEntity\n      tEntity \u003d createJobEntity(event, timestamp, jobId,\n          MAPREDUCE_JOB_ENTITY_TYPE, setCreatedTime);\n    } else {\n      if (taskAttemptId \u003d\u003d null) {\n        // TaskEntity\n        tEntity \u003d createTaskEntity(event, timestamp, taskId,\n            MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE,\n            jobId, setCreatedTime);\n      } else {\n        // TaskAttemptEntity\n        tEntity \u003d createTaskAttemptEntity(event, timestamp, taskAttemptId,\n            MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n            taskId, setCreatedTime);\n      }\n    }\n    try {\n      timelineClient.putEntitiesAsync(tEntity);\n    } catch (IOException | YarnException e) {\n      LOG.error(\"Failed to process Event \" + event.getEventType()\n          + \" for the job : \" + jobId, e);\n    }\n\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java",
      "extendedDetails": {}
    },
    "6934b05c7117a12286fb2ba7a47f75e227cacb22": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4238. createdTime and modifiedTime is not reported while publishing entities to ATSv2. (Varun Saxena via Naganarasimha G R)\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "6934b05c7117a12286fb2ba7a47f75e227cacb22",
      "commitAuthor": "Naganarasimha",
      "commitDateOld": "10/07/16 8:45 AM",
      "commitNameOld": "89e5c44f9e891a3579384c3fa3766937cd4970f1",
      "commitAuthorOld": "Li Lu",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,91 +1,101 @@\n   private void processEventForNewTimelineService(HistoryEvent event, JobId jobId,\n       long timestamp) {\n     org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity \u003d null;\n     String taskId \u003d null;\n     String taskAttemptId \u003d null;\n+    boolean setCreatedTime \u003d false;\n \n     switch (event.getEventType()) {\n       // Handle job events\n       case JOB_SUBMITTED:\n+        setCreatedTime \u003d true;\n+        break;\n       case JOB_STATUS_CHANGED:\n       case JOB_INFO_CHANGED:\n       case JOB_INITED:\n       case JOB_PRIORITY_CHANGED:\n       case JOB_QUEUE_CHANGED:\n       case JOB_FAILED:\n       case JOB_KILLED:\n       case JOB_ERROR:\n       case JOB_FINISHED:\n       case AM_STARTED:\n       case NORMALIZED_RESOURCE:\n         break;\n       // Handle task events\n       case TASK_STARTED:\n+        setCreatedTime \u003d true;\n         taskId \u003d ((TaskStartedEvent)event).getTaskId().toString();\n         break;\n       case TASK_FAILED:\n         taskId \u003d ((TaskFailedEvent)event).getTaskId().toString();\n         break;\n       case TASK_UPDATED:\n         taskId \u003d ((TaskUpdatedEvent)event).getTaskId().toString();\n         break;\n       case TASK_FINISHED:\n         taskId \u003d ((TaskFinishedEvent)event).getTaskId().toString();\n         break;\n       case MAP_ATTEMPT_STARTED:\n-      case CLEANUP_ATTEMPT_STARTED:\n       case REDUCE_ATTEMPT_STARTED:\n+        setCreatedTime \u003d true;\n+        taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n+        taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n+            getTaskAttemptId().toString();\n+        break;\n+      case CLEANUP_ATTEMPT_STARTED:\n       case SETUP_ATTEMPT_STARTED:\n         taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n         taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n             getTaskAttemptId().toString();\n         break;\n       case MAP_ATTEMPT_FAILED:\n       case CLEANUP_ATTEMPT_FAILED:\n       case REDUCE_ATTEMPT_FAILED:\n       case SETUP_ATTEMPT_FAILED:\n       case MAP_ATTEMPT_KILLED:\n       case CLEANUP_ATTEMPT_KILLED:\n       case REDUCE_ATTEMPT_KILLED:\n       case SETUP_ATTEMPT_KILLED:\n         taskId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).getTaskId().toString();\n         taskAttemptId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n             getTaskAttemptId().toString();\n         break;\n       case MAP_ATTEMPT_FINISHED:\n         taskId \u003d ((MapAttemptFinishedEvent)event).getTaskId().toString();\n         taskAttemptId \u003d ((MapAttemptFinishedEvent)event).getAttemptId().toString();\n         break;\n       case REDUCE_ATTEMPT_FINISHED:\n         taskId \u003d ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n         taskAttemptId \u003d ((ReduceAttemptFinishedEvent)event).getAttemptId().toString();\n         break;\n       case SETUP_ATTEMPT_FINISHED:\n       case CLEANUP_ATTEMPT_FINISHED:\n         taskId \u003d ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n         taskAttemptId \u003d ((TaskAttemptFinishedEvent)event).getAttemptId().toString();\n         break;\n       default:\n         LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n             \" and handled by timeline service.\");\n         return;\n     }\n     if (taskId \u003d\u003d null) {\n       // JobEntity\n       tEntity \u003d createJobEntity(event, timestamp, jobId,\n-          MAPREDUCE_JOB_ENTITY_TYPE);\n+          MAPREDUCE_JOB_ENTITY_TYPE, setCreatedTime);\n     } else {\n       if (taskAttemptId \u003d\u003d null) {\n         // TaskEntity\n         tEntity \u003d createTaskEntity(event, timestamp, taskId,\n-            MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE, jobId);\n+            MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE,\n+            jobId, setCreatedTime);\n       } else {\n         // TaskAttemptEntity\n         tEntity \u003d createTaskAttemptEntity(event, timestamp, taskAttemptId,\n             MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n-            taskId);\n+            taskId, setCreatedTime);\n       }\n     }\n \n     putEntityWithoutBlocking(timelineClient, tEntity);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processEventForNewTimelineService(HistoryEvent event, JobId jobId,\n      long timestamp) {\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity \u003d null;\n    String taskId \u003d null;\n    String taskAttemptId \u003d null;\n    boolean setCreatedTime \u003d false;\n\n    switch (event.getEventType()) {\n      // Handle job events\n      case JOB_SUBMITTED:\n        setCreatedTime \u003d true;\n        break;\n      case JOB_STATUS_CHANGED:\n      case JOB_INFO_CHANGED:\n      case JOB_INITED:\n      case JOB_PRIORITY_CHANGED:\n      case JOB_QUEUE_CHANGED:\n      case JOB_FAILED:\n      case JOB_KILLED:\n      case JOB_ERROR:\n      case JOB_FINISHED:\n      case AM_STARTED:\n      case NORMALIZED_RESOURCE:\n        break;\n      // Handle task events\n      case TASK_STARTED:\n        setCreatedTime \u003d true;\n        taskId \u003d ((TaskStartedEvent)event).getTaskId().toString();\n        break;\n      case TASK_FAILED:\n        taskId \u003d ((TaskFailedEvent)event).getTaskId().toString();\n        break;\n      case TASK_UPDATED:\n        taskId \u003d ((TaskUpdatedEvent)event).getTaskId().toString();\n        break;\n      case TASK_FINISHED:\n        taskId \u003d ((TaskFinishedEvent)event).getTaskId().toString();\n        break;\n      case MAP_ATTEMPT_STARTED:\n      case REDUCE_ATTEMPT_STARTED:\n        setCreatedTime \u003d true;\n        taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n        taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n            getTaskAttemptId().toString();\n        break;\n      case CLEANUP_ATTEMPT_STARTED:\n      case SETUP_ATTEMPT_STARTED:\n        taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n        taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n            getTaskAttemptId().toString();\n        break;\n      case MAP_ATTEMPT_FAILED:\n      case CLEANUP_ATTEMPT_FAILED:\n      case REDUCE_ATTEMPT_FAILED:\n      case SETUP_ATTEMPT_FAILED:\n      case MAP_ATTEMPT_KILLED:\n      case CLEANUP_ATTEMPT_KILLED:\n      case REDUCE_ATTEMPT_KILLED:\n      case SETUP_ATTEMPT_KILLED:\n        taskId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).getTaskId().toString();\n        taskAttemptId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n            getTaskAttemptId().toString();\n        break;\n      case MAP_ATTEMPT_FINISHED:\n        taskId \u003d ((MapAttemptFinishedEvent)event).getTaskId().toString();\n        taskAttemptId \u003d ((MapAttemptFinishedEvent)event).getAttemptId().toString();\n        break;\n      case REDUCE_ATTEMPT_FINISHED:\n        taskId \u003d ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n        taskAttemptId \u003d ((ReduceAttemptFinishedEvent)event).getAttemptId().toString();\n        break;\n      case SETUP_ATTEMPT_FINISHED:\n      case CLEANUP_ATTEMPT_FINISHED:\n        taskId \u003d ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n        taskAttemptId \u003d ((TaskAttemptFinishedEvent)event).getAttemptId().toString();\n        break;\n      default:\n        LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n            \" and handled by timeline service.\");\n        return;\n    }\n    if (taskId \u003d\u003d null) {\n      // JobEntity\n      tEntity \u003d createJobEntity(event, timestamp, jobId,\n          MAPREDUCE_JOB_ENTITY_TYPE, setCreatedTime);\n    } else {\n      if (taskAttemptId \u003d\u003d null) {\n        // TaskEntity\n        tEntity \u003d createTaskEntity(event, timestamp, taskId,\n            MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE,\n            jobId, setCreatedTime);\n      } else {\n        // TaskAttemptEntity\n        tEntity \u003d createTaskAttemptEntity(event, timestamp, taskAttemptId,\n            MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n            taskId, setCreatedTime);\n      }\n    }\n\n    putEntityWithoutBlocking(timelineClient, tEntity);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java",
      "extendedDetails": {}
    },
    "b50a6d78f5e12cfb9e0f52e0af6efbab3618e2e5": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-6327. Made MR AM use timeline service v2 API to write history events and counters. Contributed by Junping Du.\n",
      "commitDate": "10/07/16 8:45 AM",
      "commitName": "b50a6d78f5e12cfb9e0f52e0af6efbab3618e2e5",
      "commitAuthor": "Zhijie Shen",
      "diff": "@@ -0,0 +1,91 @@\n+  private void processEventForNewTimelineService(HistoryEvent event, JobId jobId,\n+      long timestamp) {\n+    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity \u003d null;\n+    String taskId \u003d null;\n+    String taskAttemptId \u003d null;\n+\n+    switch (event.getEventType()) {\n+      // Handle job events\n+      case JOB_SUBMITTED:\n+      case JOB_STATUS_CHANGED:\n+      case JOB_INFO_CHANGED:\n+      case JOB_INITED:\n+      case JOB_PRIORITY_CHANGED:\n+      case JOB_QUEUE_CHANGED:\n+      case JOB_FAILED:\n+      case JOB_KILLED:\n+      case JOB_ERROR:\n+      case JOB_FINISHED:\n+      case AM_STARTED:\n+      case NORMALIZED_RESOURCE:\n+        break;\n+      // Handle task events\n+      case TASK_STARTED:\n+        taskId \u003d ((TaskStartedEvent)event).getTaskId().toString();\n+        break;\n+      case TASK_FAILED:\n+        taskId \u003d ((TaskFailedEvent)event).getTaskId().toString();\n+        break;\n+      case TASK_UPDATED:\n+        taskId \u003d ((TaskUpdatedEvent)event).getTaskId().toString();\n+        break;\n+      case TASK_FINISHED:\n+        taskId \u003d ((TaskFinishedEvent)event).getTaskId().toString();\n+        break;\n+      case MAP_ATTEMPT_STARTED:\n+      case CLEANUP_ATTEMPT_STARTED:\n+      case REDUCE_ATTEMPT_STARTED:\n+      case SETUP_ATTEMPT_STARTED:\n+        taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n+        taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n+            getTaskAttemptId().toString();\n+        break;\n+      case MAP_ATTEMPT_FAILED:\n+      case CLEANUP_ATTEMPT_FAILED:\n+      case REDUCE_ATTEMPT_FAILED:\n+      case SETUP_ATTEMPT_FAILED:\n+      case MAP_ATTEMPT_KILLED:\n+      case CLEANUP_ATTEMPT_KILLED:\n+      case REDUCE_ATTEMPT_KILLED:\n+      case SETUP_ATTEMPT_KILLED:\n+        taskId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).getTaskId().toString();\n+        taskAttemptId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n+            getTaskAttemptId().toString();\n+        break;\n+      case MAP_ATTEMPT_FINISHED:\n+        taskId \u003d ((MapAttemptFinishedEvent)event).getTaskId().toString();\n+        taskAttemptId \u003d ((MapAttemptFinishedEvent)event).getAttemptId().toString();\n+        break;\n+      case REDUCE_ATTEMPT_FINISHED:\n+        taskId \u003d ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n+        taskAttemptId \u003d ((ReduceAttemptFinishedEvent)event).getAttemptId().toString();\n+        break;\n+      case SETUP_ATTEMPT_FINISHED:\n+      case CLEANUP_ATTEMPT_FINISHED:\n+        taskId \u003d ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n+        taskAttemptId \u003d ((TaskAttemptFinishedEvent)event).getAttemptId().toString();\n+        break;\n+      default:\n+        LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n+            \" and handled by timeline service.\");\n+        return;\n+    }\n+    if (taskId \u003d\u003d null) {\n+      // JobEntity\n+      tEntity \u003d createJobEntity(event, timestamp, jobId,\n+          MAPREDUCE_JOB_ENTITY_TYPE);\n+    } else {\n+      if (taskAttemptId \u003d\u003d null) {\n+        // TaskEntity\n+        tEntity \u003d createTaskEntity(event, timestamp, taskId,\n+            MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE, jobId);\n+      } else {\n+        // TaskAttemptEntity\n+        tEntity \u003d createTaskAttemptEntity(event, timestamp, taskAttemptId,\n+            MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n+            taskId);\n+      }\n+    }\n+\n+    putEntityWithoutBlocking(timelineClient, tEntity);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void processEventForNewTimelineService(HistoryEvent event, JobId jobId,\n      long timestamp) {\n    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity tEntity \u003d null;\n    String taskId \u003d null;\n    String taskAttemptId \u003d null;\n\n    switch (event.getEventType()) {\n      // Handle job events\n      case JOB_SUBMITTED:\n      case JOB_STATUS_CHANGED:\n      case JOB_INFO_CHANGED:\n      case JOB_INITED:\n      case JOB_PRIORITY_CHANGED:\n      case JOB_QUEUE_CHANGED:\n      case JOB_FAILED:\n      case JOB_KILLED:\n      case JOB_ERROR:\n      case JOB_FINISHED:\n      case AM_STARTED:\n      case NORMALIZED_RESOURCE:\n        break;\n      // Handle task events\n      case TASK_STARTED:\n        taskId \u003d ((TaskStartedEvent)event).getTaskId().toString();\n        break;\n      case TASK_FAILED:\n        taskId \u003d ((TaskFailedEvent)event).getTaskId().toString();\n        break;\n      case TASK_UPDATED:\n        taskId \u003d ((TaskUpdatedEvent)event).getTaskId().toString();\n        break;\n      case TASK_FINISHED:\n        taskId \u003d ((TaskFinishedEvent)event).getTaskId().toString();\n        break;\n      case MAP_ATTEMPT_STARTED:\n      case CLEANUP_ATTEMPT_STARTED:\n      case REDUCE_ATTEMPT_STARTED:\n      case SETUP_ATTEMPT_STARTED:\n        taskId \u003d ((TaskAttemptStartedEvent)event).getTaskId().toString();\n        taskAttemptId \u003d ((TaskAttemptStartedEvent)event).\n            getTaskAttemptId().toString();\n        break;\n      case MAP_ATTEMPT_FAILED:\n      case CLEANUP_ATTEMPT_FAILED:\n      case REDUCE_ATTEMPT_FAILED:\n      case SETUP_ATTEMPT_FAILED:\n      case MAP_ATTEMPT_KILLED:\n      case CLEANUP_ATTEMPT_KILLED:\n      case REDUCE_ATTEMPT_KILLED:\n      case SETUP_ATTEMPT_KILLED:\n        taskId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).getTaskId().toString();\n        taskAttemptId \u003d ((TaskAttemptUnsuccessfulCompletionEvent)event).\n            getTaskAttemptId().toString();\n        break;\n      case MAP_ATTEMPT_FINISHED:\n        taskId \u003d ((MapAttemptFinishedEvent)event).getTaskId().toString();\n        taskAttemptId \u003d ((MapAttemptFinishedEvent)event).getAttemptId().toString();\n        break;\n      case REDUCE_ATTEMPT_FINISHED:\n        taskId \u003d ((ReduceAttemptFinishedEvent)event).getTaskId().toString();\n        taskAttemptId \u003d ((ReduceAttemptFinishedEvent)event).getAttemptId().toString();\n        break;\n      case SETUP_ATTEMPT_FINISHED:\n      case CLEANUP_ATTEMPT_FINISHED:\n        taskId \u003d ((TaskAttemptFinishedEvent)event).getTaskId().toString();\n        taskAttemptId \u003d ((TaskAttemptFinishedEvent)event).getAttemptId().toString();\n        break;\n      default:\n        LOG.warn(\"EventType: \" + event.getEventType() + \" cannot be recognized\" +\n            \" and handled by timeline service.\");\n        return;\n    }\n    if (taskId \u003d\u003d null) {\n      // JobEntity\n      tEntity \u003d createJobEntity(event, timestamp, jobId,\n          MAPREDUCE_JOB_ENTITY_TYPE);\n    } else {\n      if (taskAttemptId \u003d\u003d null) {\n        // TaskEntity\n        tEntity \u003d createTaskEntity(event, timestamp, taskId,\n            MAPREDUCE_TASK_ENTITY_TYPE, MAPREDUCE_JOB_ENTITY_TYPE, jobId);\n      } else {\n        // TaskAttemptEntity\n        tEntity \u003d createTaskAttemptEntity(event, timestamp, taskAttemptId,\n            MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE, MAPREDUCE_TASK_ENTITY_TYPE,\n            taskId);\n      }\n    }\n\n    putEntityWithoutBlocking(timelineClient, tEntity);\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java"
    }
  }
}