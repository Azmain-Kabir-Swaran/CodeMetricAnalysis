{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSNamesystem.java",
  "functionName": "concat",
  "functionId": "concat___target-String__srcs-String[]__logRetryCache-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
  "functionStartLine": 2191,
  "functionEndLine": 2215,
  "numCommitsSeen": 1366,
  "timeTaken": 61070,
  "changeHistory": [
    "1824aee9da4056de0fb638906b2172e486bbebe7",
    "4b95c242eca540455a4d5d0899aaf73b6064b5ea",
    "5527d79adb9b1e2f2779c283f81d6a3d5447babc",
    "f600fbb6c4987c69292faea6b5abf022bb213ffd",
    "ff013d2c952272f3176dcf624251b05d610503b5",
    "84a1321f6aa0af6895564a7c47f8f264656f0294",
    "9b90e52f1ec22c18cd535af2a569defcef65b093",
    "ff0b99eafeda035ebe0dc82cfe689808047a8893",
    "d27d7fc72e279614212c1eae52a84675073e89fb",
    "3fa33b5c2c289ceaced30c6c5451f3569110459d",
    "7817674a3a4d097b647dd77f1345787dd376d5ea",
    "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
    "8e253cb93030642f5a7324bad0f161cd0ad33206",
    "78b9321539f973c7a1da5ce14acb49cdab41737a",
    "8c7a7e619699386f9e6991842558d78aa0c8053d",
    "1b531c1dbb452a6192fad411605d2baaa3831bcd",
    "fd1000bcefa07992ff5c6fae3508f3e33b7955c6",
    "d8ca9c655b3582596c756781f83253f644d1053f",
    "df2fb006b28bf1907fe3c54255e5f6bbb7698285",
    "d866f81edbd70121a9e29e5d25be67e1c464397e",
    "a85a0293c77f7cf0471d242458f04ec61e0129bc",
    "7d1c8d92f9b4b83c6ee154cd9ff70724bc61599f",
    "0270889b4e7f241620b2c3c297ec6530d96a7db5",
    "36d1c49486587c2dbb193e8538b1d4510c462fa6",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "1824aee9da4056de0fb638906b2172e486bbebe7": "Ybodychange",
    "4b95c242eca540455a4d5d0899aaf73b6064b5ea": "Ybodychange",
    "5527d79adb9b1e2f2779c283f81d6a3d5447babc": "Ybodychange",
    "f600fbb6c4987c69292faea6b5abf022bb213ffd": "Ybodychange",
    "ff013d2c952272f3176dcf624251b05d610503b5": "Ybodychange",
    "84a1321f6aa0af6895564a7c47f8f264656f0294": "Ybodychange",
    "9b90e52f1ec22c18cd535af2a569defcef65b093": "Ybodychange",
    "ff0b99eafeda035ebe0dc82cfe689808047a8893": "Ybodychange",
    "d27d7fc72e279614212c1eae52a84675073e89fb": "Ybodychange",
    "3fa33b5c2c289ceaced30c6c5451f3569110459d": "Ybodychange",
    "7817674a3a4d097b647dd77f1345787dd376d5ea": "Ybodychange",
    "8caf537afabc70b0c74e0a29aea0cc2935ecb162": "Ymultichange(Yexceptionschange,Ybodychange)",
    "8e253cb93030642f5a7324bad0f161cd0ad33206": "Ymultichange(Yparameterchange,Ybodychange)",
    "78b9321539f973c7a1da5ce14acb49cdab41737a": "Ybodychange",
    "8c7a7e619699386f9e6991842558d78aa0c8053d": "Ybodychange",
    "1b531c1dbb452a6192fad411605d2baaa3831bcd": "Ybodychange",
    "fd1000bcefa07992ff5c6fae3508f3e33b7955c6": "Ybodychange",
    "d8ca9c655b3582596c756781f83253f644d1053f": "Ybodychange",
    "df2fb006b28bf1907fe3c54255e5f6bbb7698285": "Ybodychange",
    "d866f81edbd70121a9e29e5d25be67e1c464397e": "Ybodychange",
    "a85a0293c77f7cf0471d242458f04ec61e0129bc": "Ybodychange",
    "7d1c8d92f9b4b83c6ee154cd9ff70724bc61599f": "Ybodychange",
    "0270889b4e7f241620b2c3c297ec6530d96a7db5": "Ybodychange",
    "36d1c49486587c2dbb193e8538b1d4510c462fa6": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee": "Ymodifierchange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "1824aee9da4056de0fb638906b2172e486bbebe7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15217 Add more information to longest write/read lock held log\n\n",
      "commitDate": "18/04/20 1:52 PM",
      "commitName": "1824aee9da4056de0fb638906b2172e486bbebe7",
      "commitAuthor": "Toshihiro Suzuki",
      "commitDateOld": "25/03/20 10:28 AM",
      "commitNameOld": "a700803a18fb957d2799001a2ce1dcb70f75c080",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 24.14,
      "commitsBetweenForRepo": 78,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,25 @@\n   void concat(String target, String [] srcs, boolean logRetryCache)\n       throws IOException {\n     final String operationName \u003d \"concat\";\n     FileStatus stat \u003d null;\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n     FSPermissionChecker.setOperationType(operationName);\n     checkOperation(OperationCategory.WRITE);\n+    String srcsStr \u003d Arrays.toString(srcs);\n     try {\n       writeLock();\n       try {\n         checkOperation(OperationCategory.WRITE);\n         checkNameNodeSafeMode(\"Cannot concat \" + target);\n         stat \u003d FSDirConcatOp.concat(dir, pc, target, srcs, logRetryCache);\n       } finally {\n-        writeUnlock(operationName);\n+        writeUnlock(operationName,\n+            getLockReportInfoSupplier(srcsStr, target, stat));\n       }\n     } catch (AccessControlException ace) {\n-      logAuditEvent(false, operationName, Arrays.toString(srcs),\n-          target, stat);\n+      logAuditEvent(false, operationName, srcsStr, target, stat);\n       throw ace;\n     }\n     getEditLog().logSync();\n-    logAuditEvent(true, operationName, Arrays.toString(srcs), target, stat);\n+    logAuditEvent(true, operationName, srcsStr, target, stat);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException {\n    final String operationName \u003d \"concat\";\n    FileStatus stat \u003d null;\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    FSPermissionChecker.setOperationType(operationName);\n    checkOperation(OperationCategory.WRITE);\n    String srcsStr \u003d Arrays.toString(srcs);\n    try {\n      writeLock();\n      try {\n        checkOperation(OperationCategory.WRITE);\n        checkNameNodeSafeMode(\"Cannot concat \" + target);\n        stat \u003d FSDirConcatOp.concat(dir, pc, target, srcs, logRetryCache);\n      } finally {\n        writeUnlock(operationName,\n            getLockReportInfoSupplier(srcsStr, target, stat));\n      }\n    } catch (AccessControlException ace) {\n      logAuditEvent(false, operationName, srcsStr, target, stat);\n      throw ace;\n    }\n    getEditLog().logSync();\n    logAuditEvent(true, operationName, srcsStr, target, stat);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "4b95c242eca540455a4d5d0899aaf73b6064b5ea": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14743. Enhance INodeAttributeProvider/ AccessControlEnforcer Interface in HDFS to support Authorization of mkdir, rm, rmdir, copy, move etc... (#1829)\n\nReviewed-by: Xiaoyu Yao \u003cxyao@apache.org\u003e",
      "commitDate": "13/03/20 11:29 AM",
      "commitName": "4b95c242eca540455a4d5d0899aaf73b6064b5ea",
      "commitAuthor": "Wei-Chiu Chuang",
      "commitDateOld": "27/02/20 8:49 AM",
      "commitNameOld": "cd2c6b1aac470991b9b90339ce2721ba179e7c48",
      "commitAuthorOld": "Ayush Saxena",
      "daysBetweenCommits": 15.07,
      "commitsBetweenForRepo": 51,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,24 @@\n   void concat(String target, String [] srcs, boolean logRetryCache)\n       throws IOException {\n     final String operationName \u003d \"concat\";\n     FileStatus stat \u003d null;\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n+    FSPermissionChecker.setOperationType(operationName);\n     checkOperation(OperationCategory.WRITE);\n     try {\n       writeLock();\n       try {\n         checkOperation(OperationCategory.WRITE);\n         checkNameNodeSafeMode(\"Cannot concat \" + target);\n         stat \u003d FSDirConcatOp.concat(dir, pc, target, srcs, logRetryCache);\n       } finally {\n         writeUnlock(operationName);\n       }\n     } catch (AccessControlException ace) {\n       logAuditEvent(false, operationName, Arrays.toString(srcs),\n           target, stat);\n       throw ace;\n     }\n     getEditLog().logSync();\n     logAuditEvent(true, operationName, Arrays.toString(srcs), target, stat);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException {\n    final String operationName \u003d \"concat\";\n    FileStatus stat \u003d null;\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    FSPermissionChecker.setOperationType(operationName);\n    checkOperation(OperationCategory.WRITE);\n    try {\n      writeLock();\n      try {\n        checkOperation(OperationCategory.WRITE);\n        checkNameNodeSafeMode(\"Cannot concat \" + target);\n        stat \u003d FSDirConcatOp.concat(dir, pc, target, srcs, logRetryCache);\n      } finally {\n        writeUnlock(operationName);\n      }\n    } catch (AccessControlException ace) {\n      logAuditEvent(false, operationName, Arrays.toString(srcs),\n          target, stat);\n      throw ace;\n    }\n    getEditLog().logSync();\n    logAuditEvent(true, operationName, Arrays.toString(srcs), target, stat);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "5527d79adb9b1e2f2779c283f81d6a3d5447babc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14810. Review FSNameSystem editlog sync. Contributed by Xiaoqiao He.\n",
      "commitDate": "17/10/19 9:26 AM",
      "commitName": "5527d79adb9b1e2f2779c283f81d6a3d5447babc",
      "commitAuthor": "Ayush Saxena",
      "commitDateOld": "06/09/19 5:42 AM",
      "commitNameOld": "d98c54816d21d59c4d877ae4b1917b22268ffcef",
      "commitAuthorOld": "Surendra Singh Lilhore",
      "daysBetweenCommits": 41.16,
      "commitsBetweenForRepo": 321,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,23 @@\n   void concat(String target, String [] srcs, boolean logRetryCache)\n       throws IOException {\n     final String operationName \u003d \"concat\";\n     FileStatus stat \u003d null;\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n     try {\n       writeLock();\n       try {\n         checkOperation(OperationCategory.WRITE);\n         checkNameNodeSafeMode(\"Cannot concat \" + target);\n         stat \u003d FSDirConcatOp.concat(dir, pc, target, srcs, logRetryCache);\n       } finally {\n         writeUnlock(operationName);\n       }\n     } catch (AccessControlException ace) {\n       logAuditEvent(false, operationName, Arrays.toString(srcs),\n           target, stat);\n       throw ace;\n-    } finally {\n-      getEditLog().logSync();\n     }\n+    getEditLog().logSync();\n     logAuditEvent(true, operationName, Arrays.toString(srcs), target, stat);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException {\n    final String operationName \u003d \"concat\";\n    FileStatus stat \u003d null;\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    try {\n      writeLock();\n      try {\n        checkOperation(OperationCategory.WRITE);\n        checkNameNodeSafeMode(\"Cannot concat \" + target);\n        stat \u003d FSDirConcatOp.concat(dir, pc, target, srcs, logRetryCache);\n      } finally {\n        writeUnlock(operationName);\n      }\n    } catch (AccessControlException ace) {\n      logAuditEvent(false, operationName, Arrays.toString(srcs),\n          target, stat);\n      throw ace;\n    }\n    getEditLog().logSync();\n    logAuditEvent(true, operationName, Arrays.toString(srcs), target, stat);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "f600fbb6c4987c69292faea6b5abf022bb213ffd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11246. FSNameSystem#logAuditEvent should be called outside the read or write locks. Contributed by He Xiaoqiao, Kuhu Shukla.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\nCo-authored-by: Kuhu Shukla \u003ckshukla@apache.org\u003e\n",
      "commitDate": "29/08/19 10:10 AM",
      "commitName": "f600fbb6c4987c69292faea6b5abf022bb213ffd",
      "commitAuthor": "He Xiaoqiao",
      "commitDateOld": "27/08/19 3:26 PM",
      "commitNameOld": "dde9399b37bffb77da17c025f0b9b673d7088bc6",
      "commitAuthorOld": "He Xiaoqiao",
      "daysBetweenCommits": 1.78,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,24 @@\n   void concat(String target, String [] srcs, boolean logRetryCache)\n       throws IOException {\n     final String operationName \u003d \"concat\";\n     FileStatus stat \u003d null;\n-    boolean success \u003d false;\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n     checkOperation(OperationCategory.WRITE);\n-    writeLock();\n     try {\n-      checkOperation(OperationCategory.WRITE);\n-      checkNameNodeSafeMode(\"Cannot concat \" + target);\n-      stat \u003d FSDirConcatOp.concat(dir, pc, target, srcs, logRetryCache);\n-      success \u003d true;\n+      writeLock();\n+      try {\n+        checkOperation(OperationCategory.WRITE);\n+        checkNameNodeSafeMode(\"Cannot concat \" + target);\n+        stat \u003d FSDirConcatOp.concat(dir, pc, target, srcs, logRetryCache);\n+      } finally {\n+        writeUnlock(operationName);\n+      }\n     } catch (AccessControlException ace) {\n-      logAuditEvent(success, operationName, Arrays.toString(srcs),\n+      logAuditEvent(false, operationName, Arrays.toString(srcs),\n           target, stat);\n       throw ace;\n     } finally {\n-      writeUnlock(operationName);\n-      if (success) {\n-        getEditLog().logSync();\n-      }\n+      getEditLog().logSync();\n     }\n-    logAuditEvent(success, operationName, Arrays.toString(srcs), target, stat);\n+    logAuditEvent(true, operationName, Arrays.toString(srcs), target, stat);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException {\n    final String operationName \u003d \"concat\";\n    FileStatus stat \u003d null;\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    try {\n      writeLock();\n      try {\n        checkOperation(OperationCategory.WRITE);\n        checkNameNodeSafeMode(\"Cannot concat \" + target);\n        stat \u003d FSDirConcatOp.concat(dir, pc, target, srcs, logRetryCache);\n      } finally {\n        writeUnlock(operationName);\n      }\n    } catch (AccessControlException ace) {\n      logAuditEvent(false, operationName, Arrays.toString(srcs),\n          target, stat);\n      throw ace;\n    } finally {\n      getEditLog().logSync();\n    }\n    logAuditEvent(true, operationName, Arrays.toString(srcs), target, stat);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "ff013d2c952272f3176dcf624251b05d610503b5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13602. Add checkOperation(WRITE) checks in FSNamesystem. Contributed by Chao Sun.",
      "commitDate": "31/05/18 5:37 PM",
      "commitName": "ff013d2c952272f3176dcf624251b05d610503b5",
      "commitAuthor": "Chao Sun",
      "commitDateOld": "26/04/18 1:52 PM",
      "commitNameOld": "2adda92de1535c0472c0df33a145fa1814703f4f",
      "commitAuthorOld": "Owen O\u0027Malley",
      "daysBetweenCommits": 35.16,
      "commitsBetweenForRepo": 296,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,25 @@\n   void concat(String target, String [] srcs, boolean logRetryCache)\n       throws IOException {\n     final String operationName \u003d \"concat\";\n     FileStatus stat \u003d null;\n     boolean success \u003d false;\n     final FSPermissionChecker pc \u003d getPermissionChecker();\n+    checkOperation(OperationCategory.WRITE);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot concat \" + target);\n       stat \u003d FSDirConcatOp.concat(dir, pc, target, srcs, logRetryCache);\n       success \u003d true;\n     } catch (AccessControlException ace) {\n       logAuditEvent(success, operationName, Arrays.toString(srcs),\n           target, stat);\n       throw ace;\n     } finally {\n       writeUnlock(operationName);\n       if (success) {\n         getEditLog().logSync();\n       }\n     }\n     logAuditEvent(success, operationName, Arrays.toString(srcs), target, stat);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException {\n    final String operationName \u003d \"concat\";\n    FileStatus stat \u003d null;\n    boolean success \u003d false;\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot concat \" + target);\n      stat \u003d FSDirConcatOp.concat(dir, pc, target, srcs, logRetryCache);\n      success \u003d true;\n    } catch (AccessControlException ace) {\n      logAuditEvent(success, operationName, Arrays.toString(srcs),\n          target, stat);\n      throw ace;\n    } finally {\n      writeUnlock(operationName);\n      if (success) {\n        getEditLog().logSync();\n      }\n    }\n    logAuditEvent(success, operationName, Arrays.toString(srcs), target, stat);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "84a1321f6aa0af6895564a7c47f8f264656f0294": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13136. Avoid taking FSN lock while doing group member lookup for FSD permission check. Contributed by Xiaoyu Yao.\n",
      "commitDate": "22/02/18 11:32 AM",
      "commitName": "84a1321f6aa0af6895564a7c47f8f264656f0294",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "15/02/18 1:32 PM",
      "commitNameOld": "47473952e56b0380147d42f4110ad03c2276c961",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 6.92,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,24 @@\n   void concat(String target, String [] srcs, boolean logRetryCache)\n       throws IOException {\n     final String operationName \u003d \"concat\";\n     FileStatus stat \u003d null;\n     boolean success \u003d false;\n+    final FSPermissionChecker pc \u003d getPermissionChecker();\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot concat \" + target);\n-      stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n+      stat \u003d FSDirConcatOp.concat(dir, pc, target, srcs, logRetryCache);\n       success \u003d true;\n     } catch (AccessControlException ace) {\n       logAuditEvent(success, operationName, Arrays.toString(srcs),\n           target, stat);\n       throw ace;\n     } finally {\n       writeUnlock(operationName);\n       if (success) {\n         getEditLog().logSync();\n       }\n     }\n     logAuditEvent(success, operationName, Arrays.toString(srcs), target, stat);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException {\n    final String operationName \u003d \"concat\";\n    FileStatus stat \u003d null;\n    boolean success \u003d false;\n    final FSPermissionChecker pc \u003d getPermissionChecker();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot concat \" + target);\n      stat \u003d FSDirConcatOp.concat(dir, pc, target, srcs, logRetryCache);\n      success \u003d true;\n    } catch (AccessControlException ace) {\n      logAuditEvent(success, operationName, Arrays.toString(srcs),\n          target, stat);\n      throw ace;\n    } finally {\n      writeUnlock(operationName);\n      if (success) {\n        getEditLog().logSync();\n      }\n    }\n    logAuditEvent(success, operationName, Arrays.toString(srcs), target, stat);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "9b90e52f1ec22c18cd535af2a569defcef65b093": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11641. Reduce cost of audit logging by using FileStatus instead of HdfsFileStatus. Contributed by Daryn Sharp.\n",
      "commitDate": "16/05/17 9:28 AM",
      "commitName": "9b90e52f1ec22c18cd535af2a569defcef65b093",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "04/05/17 11:39 AM",
      "commitNameOld": "c2a52ef9c29459ff9ef3e23b29e14912bfdb1405",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 11.91,
      "commitsBetweenForRepo": 58,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   void concat(String target, String [] srcs, boolean logRetryCache)\n       throws IOException {\n     final String operationName \u003d \"concat\";\n-    HdfsFileStatus stat \u003d null;\n+    FileStatus stat \u003d null;\n     boolean success \u003d false;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot concat \" + target);\n       stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n       success \u003d true;\n     } catch (AccessControlException ace) {\n       logAuditEvent(success, operationName, Arrays.toString(srcs),\n           target, stat);\n       throw ace;\n     } finally {\n       writeUnlock(operationName);\n       if (success) {\n         getEditLog().logSync();\n       }\n     }\n     logAuditEvent(success, operationName, Arrays.toString(srcs), target, stat);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException {\n    final String operationName \u003d \"concat\";\n    FileStatus stat \u003d null;\n    boolean success \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot concat \" + target);\n      stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n      success \u003d true;\n    } catch (AccessControlException ace) {\n      logAuditEvent(success, operationName, Arrays.toString(srcs),\n          target, stat);\n      throw ace;\n    } finally {\n      writeUnlock(operationName);\n      if (success) {\n        getEditLog().logSync();\n      }\n    }\n    logAuditEvent(success, operationName, Arrays.toString(srcs), target, stat);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "ff0b99eafeda035ebe0dc82cfe689808047a8893": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10872. Add MutableRate metrics for FSNamesystemLock operations. Contributed by Erik Krogen.\n",
      "commitDate": "14/11/16 11:05 AM",
      "commitName": "ff0b99eafeda035ebe0dc82cfe689808047a8893",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "08/11/16 6:17 PM",
      "commitNameOld": "ed0bebabaaf27cd730f7f8eb002d92c9c7db327d",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 5.7,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,23 @@\n   void concat(String target, String [] srcs, boolean logRetryCache)\n       throws IOException {\n+    final String operationName \u003d \"concat\";\n     HdfsFileStatus stat \u003d null;\n     boolean success \u003d false;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot concat \" + target);\n       stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n       success \u003d true;\n     } catch (AccessControlException ace) {\n-      logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n+      logAuditEvent(success, operationName, Arrays.toString(srcs),\n+          target, stat);\n       throw ace;\n     } finally {\n-      writeUnlock();\n+      writeUnlock(operationName);\n       if (success) {\n         getEditLog().logSync();\n       }\n     }\n-    logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n+    logAuditEvent(success, operationName, Arrays.toString(srcs), target, stat);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException {\n    final String operationName \u003d \"concat\";\n    HdfsFileStatus stat \u003d null;\n    boolean success \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot concat \" + target);\n      stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n      success \u003d true;\n    } catch (AccessControlException ace) {\n      logAuditEvent(success, operationName, Arrays.toString(srcs),\n          target, stat);\n      throw ace;\n    } finally {\n      writeUnlock(operationName);\n      if (success) {\n        getEditLog().logSync();\n      }\n    }\n    logAuditEvent(success, operationName, Arrays.toString(srcs), target, stat);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "d27d7fc72e279614212c1eae52a84675073e89fb": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9395. Make HDFS audit logging consistant. Contributed by Kuhu Shukla.\n",
      "commitDate": "24/02/16 10:04 AM",
      "commitName": "d27d7fc72e279614212c1eae52a84675073e89fb",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "21/02/16 7:51 PM",
      "commitNameOld": "f313516731d787f6be64c9406ca83d941d47ee99",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 2.59,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,21 @@\n   void concat(String target, String [] srcs, boolean logRetryCache)\n       throws IOException {\n     HdfsFileStatus stat \u003d null;\n     boolean success \u003d false;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot concat \" + target);\n       stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n       success \u003d true;\n+    } catch (AccessControlException ace) {\n+      logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n+      throw ace;\n     } finally {\n       writeUnlock();\n       if (success) {\n         getEditLog().logSync();\n       }\n-      logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n     }\n+    logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException {\n    HdfsFileStatus stat \u003d null;\n    boolean success \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot concat \" + target);\n      stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n      success \u003d true;\n    } catch (AccessControlException ace) {\n      logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n      throw ace;\n    } finally {\n      writeUnlock();\n      if (success) {\n        getEditLog().logSync();\n      }\n    }\n    logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "3fa33b5c2c289ceaced30c6c5451f3569110459d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9430 Remove waitForLoadingFSImage since checkNNStartup has ensured image loaded and namenode started. (Brahma Reddy Battula via mingma)\n",
      "commitDate": "04/12/15 9:47 AM",
      "commitName": "3fa33b5c2c289ceaced30c6c5451f3569110459d",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "01/12/15 4:09 PM",
      "commitNameOld": "a49cc74b4c72195dee1dfb6f9548e5e411dff553",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.74,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,18 @@\n   void concat(String target, String [] srcs, boolean logRetryCache)\n       throws IOException {\n-    waitForLoadingFSImage();\n     HdfsFileStatus stat \u003d null;\n     boolean success \u003d false;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot concat \" + target);\n       stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n       success \u003d true;\n     } finally {\n       writeUnlock();\n       if (success) {\n         getEditLog().logSync();\n       }\n       logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException {\n    HdfsFileStatus stat \u003d null;\n    boolean success \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot concat \" + target);\n      stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n      success \u003d true;\n    } finally {\n      writeUnlock();\n      if (success) {\n        getEditLog().logSync();\n      }\n      logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "7817674a3a4d097b647dd77f1345787dd376d5ea": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7609. Avoid retry cache collision when Standby NameNode loading edits. Contributed by Ming Ma.\n",
      "commitDate": "29/05/15 11:05 AM",
      "commitName": "7817674a3a4d097b647dd77f1345787dd376d5ea",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "27/05/15 3:42 PM",
      "commitNameOld": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.81,
      "commitsBetweenForRepo": 27,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,19 @@\n   void concat(String target, String [] srcs, boolean logRetryCache)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n     waitForLoadingFSImage();\n     HdfsFileStatus stat \u003d null;\n     boolean success \u003d false;\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot concat \" + target);\n       stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n       success \u003d true;\n     } finally {\n       writeUnlock();\n       if (success) {\n         getEditLog().logSync();\n       }\n       logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException {\n    waitForLoadingFSImage();\n    HdfsFileStatus stat \u003d null;\n    boolean success \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot concat \" + target);\n      stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n      success \u003d true;\n    } finally {\n      writeUnlock();\n      if (success) {\n        getEditLog().logSync();\n      }\n      logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "8caf537afabc70b0c74e0a29aea0cc2935ecb162": {
      "type": "Ymultichange(Yexceptionschange,Ybodychange)",
      "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
      "commitDate": "24/11/14 3:42 PM",
      "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 3:42 PM",
          "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 11:11 AM",
          "commitNameOld": "8e253cb93030642f5a7324bad0f161cd0ad33206",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 0.19,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,20 @@\n   void concat(String target, String [] srcs, boolean logRetryCache)\n-      throws IOException, UnresolvedLinkException {\n-    if(FSNamesystem.LOG.isDebugEnabled()) {\n-      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n-          \" to \" + target);\n-    }\n-\n+      throws IOException {\n+    checkOperation(OperationCategory.WRITE);\n+    waitForLoadingFSImage();\n+    HdfsFileStatus stat \u003d null;\n+    boolean success \u003d false;\n+    writeLock();\n     try {\n-      concatInt(target, srcs, logRetryCache);\n-    } catch (AccessControlException e) {\n-      logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n-      throw e;\n+      checkOperation(OperationCategory.WRITE);\n+      checkNameNodeSafeMode(\"Cannot concat \" + target);\n+      stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n+      success \u003d true;\n+    } finally {\n+      writeUnlock();\n+      if (success) {\n+        getEditLog().logSync();\n+      }\n+      logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException {\n    checkOperation(OperationCategory.WRITE);\n    waitForLoadingFSImage();\n    HdfsFileStatus stat \u003d null;\n    boolean success \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot concat \" + target);\n      stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n      success \u003d true;\n    } finally {\n      writeUnlock();\n      if (success) {\n        getEditLog().logSync();\n      }\n      logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[IOException, UnresolvedLinkException]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 3:42 PM",
          "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 11:11 AM",
          "commitNameOld": "8e253cb93030642f5a7324bad0f161cd0ad33206",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 0.19,
          "commitsBetweenForRepo": 6,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,14 +1,20 @@\n   void concat(String target, String [] srcs, boolean logRetryCache)\n-      throws IOException, UnresolvedLinkException {\n-    if(FSNamesystem.LOG.isDebugEnabled()) {\n-      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n-          \" to \" + target);\n-    }\n-\n+      throws IOException {\n+    checkOperation(OperationCategory.WRITE);\n+    waitForLoadingFSImage();\n+    HdfsFileStatus stat \u003d null;\n+    boolean success \u003d false;\n+    writeLock();\n     try {\n-      concatInt(target, srcs, logRetryCache);\n-    } catch (AccessControlException e) {\n-      logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n-      throw e;\n+      checkOperation(OperationCategory.WRITE);\n+      checkNameNodeSafeMode(\"Cannot concat \" + target);\n+      stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n+      success \u003d true;\n+    } finally {\n+      writeUnlock();\n+      if (success) {\n+        getEditLog().logSync();\n+      }\n+      logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException {\n    checkOperation(OperationCategory.WRITE);\n    waitForLoadingFSImage();\n    HdfsFileStatus stat \u003d null;\n    boolean success \u003d false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot concat \" + target);\n      stat \u003d FSDirConcatOp.concat(dir, target, srcs, logRetryCache);\n      success \u003d true;\n    } finally {\n      writeUnlock();\n      if (success) {\n        getEditLog().logSync();\n      }\n      logAuditEvent(success, \"concat\", Arrays.toString(srcs), target, stat);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "8e253cb93030642f5a7324bad0f161cd0ad33206": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7412. Move RetryCache to NameNodeRpcServer. Contributed by Haohui Mai.\n",
      "commitDate": "24/11/14 11:11 AM",
      "commitName": "8e253cb93030642f5a7324bad0f161cd0ad33206",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7412. Move RetryCache to NameNodeRpcServer. Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 11:11 AM",
          "commitName": "8e253cb93030642f5a7324bad0f161cd0ad33206",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 10:46 AM",
          "commitNameOld": "daacbc18d739d030822df0b75205eeb067f89850",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,14 @@\n-  void concat(String target, String [] srcs) \n+  void concat(String target, String [] srcs, boolean logRetryCache)\n       throws IOException, UnresolvedLinkException {\n-    CacheEntry cacheEntry \u003d RetryCache.waitForCompletion(retryCache);\n-    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n-      return; // Return previous response\n-    }\n-    \n-    // Either there is no previous request in progress or it has failed\n     if(FSNamesystem.LOG.isDebugEnabled()) {\n       FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n           \" to \" + target);\n     }\n-    \n-    boolean success \u003d false;\n+\n     try {\n-      concatInt(target, srcs, cacheEntry !\u003d null);\n-      success \u003d true;\n+      concatInt(target, srcs, logRetryCache);\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n       throw e;\n-    } finally {\n-      RetryCache.setState(cacheEntry, success);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException, UnresolvedLinkException {\n    if(FSNamesystem.LOG.isDebugEnabled()) {\n      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n          \" to \" + target);\n    }\n\n    try {\n      concatInt(target, srcs, logRetryCache);\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n      throw e;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[target-String, srcs-String[]]",
            "newValue": "[target-String, srcs-String[], logRetryCache-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7412. Move RetryCache to NameNodeRpcServer. Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 11:11 AM",
          "commitName": "8e253cb93030642f5a7324bad0f161cd0ad33206",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 10:46 AM",
          "commitNameOld": "daacbc18d739d030822df0b75205eeb067f89850",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 2,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,24 +1,14 @@\n-  void concat(String target, String [] srcs) \n+  void concat(String target, String [] srcs, boolean logRetryCache)\n       throws IOException, UnresolvedLinkException {\n-    CacheEntry cacheEntry \u003d RetryCache.waitForCompletion(retryCache);\n-    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n-      return; // Return previous response\n-    }\n-    \n-    // Either there is no previous request in progress or it has failed\n     if(FSNamesystem.LOG.isDebugEnabled()) {\n       FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n           \" to \" + target);\n     }\n-    \n-    boolean success \u003d false;\n+\n     try {\n-      concatInt(target, srcs, cacheEntry !\u003d null);\n-      success \u003d true;\n+      concatInt(target, srcs, logRetryCache);\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n       throw e;\n-    } finally {\n-      RetryCache.setState(cacheEntry, success);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void concat(String target, String [] srcs, boolean logRetryCache)\n      throws IOException, UnresolvedLinkException {\n    if(FSNamesystem.LOG.isDebugEnabled()) {\n      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n          \" to \" + target);\n    }\n\n    try {\n      concatInt(target, srcs, logRetryCache);\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n      throw e;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "78b9321539f973c7a1da5ce14acb49cdab41737a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6119. FSNamesystem code cleanup. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582073 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/03/14 2:32 PM",
      "commitName": "78b9321539f973c7a1da5ce14acb49cdab41737a",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "26/03/14 9:32 AM",
      "commitNameOld": "c00703dd082474fea98a63b871c2183ca01147ed",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.21,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   void concat(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     CacheEntry cacheEntry \u003d RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n       return; // Return previous response\n     }\n     \n-    // Either there is no previous request in progres or it has failed\n+    // Either there is no previous request in progress or it has failed\n     if(FSNamesystem.LOG.isDebugEnabled()) {\n       FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n           \" to \" + target);\n     }\n     \n     boolean success \u003d false;\n     try {\n       concatInt(target, srcs, cacheEntry !\u003d null);\n       success \u003d true;\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n       throw e;\n     } finally {\n       RetryCache.setState(cacheEntry, success);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    CacheEntry cacheEntry \u003d RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    \n    // Either there is no previous request in progress or it has failed\n    if(FSNamesystem.LOG.isDebugEnabled()) {\n      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n          \" to \" + target);\n    }\n    \n    boolean success \u003d false;\n    try {\n      concatInt(target, srcs, cacheEntry !\u003d null);\n      success \u003d true;\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n      throw e;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "8c7a7e619699386f9e6991842558d78aa0c8053d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/07/13 12:51 AM",
      "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "26/07/13 4:59 PM",
      "commitNameOld": "dc17bda4b677e30c02c2a9a053895a43e41f7a12",
      "commitAuthorOld": "Konstantin Boudnik",
      "daysBetweenCommits": 3.33,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   void concat(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     CacheEntry cacheEntry \u003d RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n       return; // Return previous response\n     }\n     \n     // Either there is no previous request in progres or it has failed\n     if(FSNamesystem.LOG.isDebugEnabled()) {\n       FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n           \" to \" + target);\n     }\n     \n     boolean success \u003d false;\n     try {\n-      concatInt(target, srcs);\n+      concatInt(target, srcs, cacheEntry !\u003d null);\n       success \u003d true;\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n       throw e;\n     } finally {\n       RetryCache.setState(cacheEntry, success);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    CacheEntry cacheEntry \u003d RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    \n    // Either there is no previous request in progres or it has failed\n    if(FSNamesystem.LOG.isDebugEnabled()) {\n      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n          \" to \" + target);\n    }\n    \n    boolean success \u003d false;\n    try {\n      concatInt(target, srcs, cacheEntry !\u003d null);\n      success \u003d true;\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n      throw e;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "1b531c1dbb452a6192fad411605d2baaa3831bcd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4979. Implement retry cache on Namenode. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1507170 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/07/13 6:09 PM",
      "commitName": "1b531c1dbb452a6192fad411605d2baaa3831bcd",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "24/07/13 5:32 PM",
      "commitNameOld": "f138ae68f9be0ae072a6a4ee50e94a1608c90edb",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.03,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,24 @@\n   void concat(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n+    CacheEntry cacheEntry \u003d RetryCache.waitForCompletion(retryCache);\n+    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n+      return; // Return previous response\n+    }\n+    \n+    // Either there is no previous request in progres or it has failed\n     if(FSNamesystem.LOG.isDebugEnabled()) {\n       FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n           \" to \" + target);\n     }\n+    \n+    boolean success \u003d false;\n     try {\n       concatInt(target, srcs);\n+      success \u003d true;\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n       throw e;\n+    } finally {\n+      RetryCache.setState(cacheEntry, success);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    CacheEntry cacheEntry \u003d RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry !\u003d null \u0026\u0026 cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    \n    // Either there is no previous request in progres or it has failed\n    if(FSNamesystem.LOG.isDebugEnabled()) {\n      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n          \" to \" + target);\n    }\n    \n    boolean success \u003d false;\n    try {\n      concatInt(target, srcs);\n      success \u003d true;\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n      throw e;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "fd1000bcefa07992ff5c6fae3508f3e33b7955c6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4679. Namenode operation checks should be done in a consistent manner. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1466721 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/04/13 3:45 PM",
      "commitName": "fd1000bcefa07992ff5c6fae3508f3e33b7955c6",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "08/04/13 6:21 PM",
      "commitNameOld": "f680865d994b8b75c11fa85f3241b1b9c6851187",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 1.89,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,9 +1,13 @@\n   void concat(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n+    if(FSNamesystem.LOG.isDebugEnabled()) {\n+      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n+          \" to \" + target);\n+    }\n     try {\n       concatInt(target, srcs);\n     } catch (AccessControlException e) {\n       logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n       throw e;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    if(FSNamesystem.LOG.isDebugEnabled()) {\n      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n          \" to \" + target);\n    }\n    try {\n      concatInt(target, srcs);\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "d8ca9c655b3582596c756781f83253f644d1053f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4532. RPC call queue may fill due to current user lookup (daryn)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1452435 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/03/13 10:36 AM",
      "commitName": "d8ca9c655b3582596c756781f83253f644d1053f",
      "commitAuthor": "Daryn Sharp",
      "commitDateOld": "28/02/13 1:13 PM",
      "commitNameOld": "2e02b926644ba80243ba7421cb609133db41d583",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 3.89,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,9 @@\n   void concat(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     try {\n       concatInt(target, srcs);\n     } catch (AccessControlException e) {\n-      if (isAuditEnabled() \u0026\u0026 isExternalInvocation()) {\n-        logAuditEvent(false, UserGroupInformation.getLoginUser(),\n-                      getRemoteIp(),\n-                      \"concat\", Arrays.toString(srcs), target, null);\n-      }\n+      logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n       throw e;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    try {\n      concatInt(target, srcs);\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"concat\", Arrays.toString(srcs), target, null);\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "df2fb006b28bf1907fe3c54255e5f6bbb7698285": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3680. Allow customized audit logging in HDFS FSNamesystem. Contributed by Marcelo Vanzin.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1418114 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/12/12 2:27 PM",
      "commitName": "df2fb006b28bf1907fe3c54255e5f6bbb7698285",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "05/12/12 11:20 PM",
      "commitNameOld": "8bb0dc34e4f14698bea104be6294acb4954358ca",
      "commitAuthorOld": "Konstantin Shvachko",
      "daysBetweenCommits": 0.63,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n   void concat(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     try {\n       concatInt(target, srcs);\n     } catch (AccessControlException e) {\n-      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n+      if (isAuditEnabled() \u0026\u0026 isExternalInvocation()) {\n         logAuditEvent(false, UserGroupInformation.getLoginUser(),\n                       getRemoteIp(),\n                       \"concat\", Arrays.toString(srcs), target, null);\n       }\n       throw e;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    try {\n      concatInt(target, srcs);\n    } catch (AccessControlException e) {\n      if (isAuditEnabled() \u0026\u0026 isExternalInvocation()) {\n        logAuditEvent(false, UserGroupInformation.getLoginUser(),\n                      getRemoteIp(),\n                      \"concat\", Arrays.toString(srcs), target, null);\n      }\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "d866f81edbd70121a9e29e5d25be67e1c464397e": {
      "type": "Ybodychange",
      "commitMessage": "Reverting initial commit of HDFS-3680 pending further comments.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1415797 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/11/12 11:19 AM",
      "commitName": "d866f81edbd70121a9e29e5d25be67e1c464397e",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "30/11/12 11:11 AM",
      "commitNameOld": "a85a0293c77f7cf0471d242458f04ec61e0129bc",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n   void concat(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     try {\n       concatInt(target, srcs);\n     } catch (AccessControlException e) {\n-      if (isAuditEnabled() \u0026\u0026 isExternalInvocation()) {\n+      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n         logAuditEvent(false, UserGroupInformation.getLoginUser(),\n                       getRemoteIp(),\n                       \"concat\", Arrays.toString(srcs), target, null);\n       }\n       throw e;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    try {\n      concatInt(target, srcs);\n    } catch (AccessControlException e) {\n      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        logAuditEvent(false, UserGroupInformation.getLoginUser(),\n                      getRemoteIp(),\n                      \"concat\", Arrays.toString(srcs), target, null);\n      }\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "a85a0293c77f7cf0471d242458f04ec61e0129bc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3680. Allow customized audit logging in HDFS FSNamesystem. Contributed by Marcelo Vanzin.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1415794 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/11/12 11:11 AM",
      "commitName": "a85a0293c77f7cf0471d242458f04ec61e0129bc",
      "commitAuthor": "Aaron Myers",
      "commitDateOld": "19/11/12 6:00 PM",
      "commitNameOld": "573c41c2666e084f3988a288bb40d2305fc23d8f",
      "commitAuthorOld": "Konstantin Shvachko",
      "daysBetweenCommits": 10.72,
      "commitsBetweenForRepo": 30,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n   void concat(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     try {\n       concatInt(target, srcs);\n     } catch (AccessControlException e) {\n-      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n+      if (isAuditEnabled() \u0026\u0026 isExternalInvocation()) {\n         logAuditEvent(false, UserGroupInformation.getLoginUser(),\n                       getRemoteIp(),\n                       \"concat\", Arrays.toString(srcs), target, null);\n       }\n       throw e;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    try {\n      concatInt(target, srcs);\n    } catch (AccessControlException e) {\n      if (isAuditEnabled() \u0026\u0026 isExternalInvocation()) {\n        logAuditEvent(false, UserGroupInformation.getLoginUser(),\n                      getRemoteIp(),\n                      \"concat\", Arrays.toString(srcs), target, null);\n      }\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "7d1c8d92f9b4b83c6ee154cd9ff70724bc61599f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3733. Audit logs should include WebHDFS access. Contributed by Andy Isaacson\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1379278 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/08/12 9:08 PM",
      "commitName": "7d1c8d92f9b4b83c6ee154cd9ff70724bc61599f",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "28/08/12 3:09 PM",
      "commitNameOld": "d4d2bf73a9181a5bfdc0fd99328c7ee4ec998b4e",
      "commitAuthorOld": "Aaron Myers",
      "daysBetweenCommits": 2.25,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,13 @@\n   void concat(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     try {\n       concatInt(target, srcs);\n     } catch (AccessControlException e) {\n       if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n         logAuditEvent(false, UserGroupInformation.getLoginUser(),\n-                      Server.getRemoteIp(),\n+                      getRemoteIp(),\n                       \"concat\", Arrays.toString(srcs), target, null);\n       }\n       throw e;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    try {\n      concatInt(target, srcs);\n    } catch (AccessControlException e) {\n      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        logAuditEvent(false, UserGroupInformation.getLoginUser(),\n                      getRemoteIp(),\n                      \"concat\", Arrays.toString(srcs), target, null);\n      }\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "0270889b4e7f241620b2c3c297ec6530d96a7db5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3535. Audit logging should log denied accesses. Contributed by Andy Isaacson\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1354144 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/06/12 11:14 AM",
      "commitName": "0270889b4e7f241620b2c3c297ec6530d96a7db5",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "11/06/12 6:55 PM",
      "commitNameOld": "543f86631bf07053a045d5dabcad16fb8f9eff97",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 14.68,
      "commitsBetweenForRepo": 53,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,13 @@\n   void concat(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n-    if(FSNamesystem.LOG.isDebugEnabled()) {\n-      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n-          \" to \" + target);\n-    }\n-    \n-    // verify args\n-    if(target.isEmpty()) {\n-      throw new IllegalArgumentException(\"Target file name is empty\");\n-    }\n-    if(srcs \u003d\u003d null || srcs.length \u003d\u003d 0) {\n-      throw new IllegalArgumentException(\"No sources given\");\n-    }\n-    \n-    // We require all files be in the same directory\n-    String trgParent \u003d \n-      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n-    for (String s : srcs) {\n-      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n-      if (!srcParent.equals(trgParent)) {\n-        throw new IllegalArgumentException(\n-           \"Sources and target are not in the same directory\");\n-      }\n-    }\n-\n-    HdfsFileStatus resultingStat \u003d null;\n-    writeLock();\n     try {\n-      checkOperation(OperationCategory.WRITE);\n-      if (isInSafeMode()) {\n-        throw new SafeModeException(\"Cannot concat \" + target, safeMode);\n-      }\n-      concatInternal(target, srcs);\n+      concatInt(target, srcs);\n+    } catch (AccessControlException e) {\n       if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n-        resultingStat \u003d dir.getFileInfo(target, false);\n+        logAuditEvent(false, UserGroupInformation.getLoginUser(),\n+                      Server.getRemoteIp(),\n+                      \"concat\", Arrays.toString(srcs), target, null);\n       }\n-    } finally {\n-      writeUnlock();\n-    }\n-    getEditLog().logSync();\n-    if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n-      logAuditEvent(UserGroupInformation.getLoginUser(),\n-                    Server.getRemoteIp(),\n-                    \"concat\", Arrays.toString(srcs), target, resultingStat);\n+      throw e;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    try {\n      concatInt(target, srcs);\n    } catch (AccessControlException e) {\n      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        logAuditEvent(false, UserGroupInformation.getLoginUser(),\n                      Server.getRemoteIp(),\n                      \"concat\", Arrays.toString(srcs), target, null);\n      }\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "36d1c49486587c2dbb193e8538b1d4510c462fa6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2693. Fix synchronization issues around state transition. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1221582 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/11 7:03 PM",
      "commitName": "36d1c49486587c2dbb193e8538b1d4510c462fa6",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "16/12/11 10:36 AM",
      "commitNameOld": "371f4228e86f5ebffb3d8647fb30b8bdc2b777c4",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 4.35,
      "commitsBetweenForRepo": 20,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,47 @@\n   void concat(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     if(FSNamesystem.LOG.isDebugEnabled()) {\n       FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n           \" to \" + target);\n     }\n     \n     // verify args\n     if(target.isEmpty()) {\n       throw new IllegalArgumentException(\"Target file name is empty\");\n     }\n     if(srcs \u003d\u003d null || srcs.length \u003d\u003d 0) {\n       throw new IllegalArgumentException(\"No sources given\");\n     }\n     \n     // We require all files be in the same directory\n     String trgParent \u003d \n       target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n     for (String s : srcs) {\n       String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n       if (!srcParent.equals(trgParent)) {\n         throw new IllegalArgumentException(\n            \"Sources and target are not in the same directory\");\n       }\n     }\n \n     HdfsFileStatus resultingStat \u003d null;\n     writeLock();\n     try {\n+      checkOperation(OperationCategory.WRITE);\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot concat \" + target, safeMode);\n       }\n       concatInternal(target, srcs);\n       if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n         resultingStat \u003d dir.getFileInfo(target, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n       logAuditEvent(UserGroupInformation.getLoginUser(),\n                     Server.getRemoteIp(),\n                     \"concat\", Arrays.toString(srcs), target, resultingStat);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    if(FSNamesystem.LOG.isDebugEnabled()) {\n      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n          \" to \" + target);\n    }\n    \n    // verify args\n    if(target.isEmpty()) {\n      throw new IllegalArgumentException(\"Target file name is empty\");\n    }\n    if(srcs \u003d\u003d null || srcs.length \u003d\u003d 0) {\n      throw new IllegalArgumentException(\"No sources given\");\n    }\n    \n    // We require all files be in the same directory\n    String trgParent \u003d \n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    HdfsFileStatus resultingStat \u003d null;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot concat \" + target, safeMode);\n      }\n      concatInternal(target, srcs);\n      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        resultingStat \u003d dir.getFileInfo(target, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n      logAuditEvent(UserGroupInformation.getLoginUser(),\n                    Server.getRemoteIp(),\n                    \"concat\", Arrays.toString(srcs), target, resultingStat);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    if(FSNamesystem.LOG.isDebugEnabled()) {\n      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n          \" to \" + target);\n    }\n    \n    // verify args\n    if(target.isEmpty()) {\n      throw new IllegalArgumentException(\"Target file name is empty\");\n    }\n    if(srcs \u003d\u003d null || srcs.length \u003d\u003d 0) {\n      throw new IllegalArgumentException(\"No sources given\");\n    }\n    \n    // We require all files be in the same directory\n    String trgParent \u003d \n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    HdfsFileStatus resultingStat \u003d null;\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot concat \" + target, safeMode);\n      }\n      concatInternal(target, srcs);\n      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        resultingStat \u003d dir.getFileInfo(target, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n      logAuditEvent(UserGroupInformation.getLoginUser(),\n                    Server.getRemoteIp(),\n                    \"concat\", Arrays.toString(srcs), target, resultingStat);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    if(FSNamesystem.LOG.isDebugEnabled()) {\n      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n          \" to \" + target);\n    }\n    \n    // verify args\n    if(target.isEmpty()) {\n      throw new IllegalArgumentException(\"Target file name is empty\");\n    }\n    if(srcs \u003d\u003d null || srcs.length \u003d\u003d 0) {\n      throw new IllegalArgumentException(\"No sources given\");\n    }\n    \n    // We require all files be in the same directory\n    String trgParent \u003d \n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    HdfsFileStatus resultingStat \u003d null;\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot concat \" + target, safeMode);\n      }\n      concatInternal(target, srcs);\n      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        resultingStat \u003d dir.getFileInfo(target, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n      logAuditEvent(UserGroupInformation.getLoginUser(),\n                    Server.getRemoteIp(),\n                    \"concat\", Arrays.toString(srcs), target, resultingStat);\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-2239. Reduce access levels of the fields and methods in FSNamesystem.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1155998 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/08/11 6:50 PM",
      "commitName": "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/08/11 3:06 AM",
      "commitNameOld": "371f4a59059322000a40eb4bdf5386b96b626ece",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.66,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,46 @@\n-  public void concat(String target, String [] srcs) \n+  void concat(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     if(FSNamesystem.LOG.isDebugEnabled()) {\n       FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n           \" to \" + target);\n     }\n     \n     // verify args\n     if(target.isEmpty()) {\n       throw new IllegalArgumentException(\"Target file name is empty\");\n     }\n     if(srcs \u003d\u003d null || srcs.length \u003d\u003d 0) {\n       throw new IllegalArgumentException(\"No sources given\");\n     }\n     \n     // We require all files be in the same directory\n     String trgParent \u003d \n       target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n     for (String s : srcs) {\n       String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n       if (!srcParent.equals(trgParent)) {\n         throw new IllegalArgumentException(\n            \"Sources and target are not in the same directory\");\n       }\n     }\n \n     HdfsFileStatus resultingStat \u003d null;\n     writeLock();\n     try {\n       if (isInSafeMode()) {\n         throw new SafeModeException(\"Cannot concat \" + target, safeMode);\n       }\n       concatInternal(target, srcs);\n       if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n         resultingStat \u003d dir.getFileInfo(target, false);\n       }\n     } finally {\n       writeUnlock();\n     }\n     getEditLog().logSync();\n     if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n       logAuditEvent(UserGroupInformation.getLoginUser(),\n                     Server.getRemoteIp(),\n                     \"concat\", Arrays.toString(srcs), target, resultingStat);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    if(FSNamesystem.LOG.isDebugEnabled()) {\n      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n          \" to \" + target);\n    }\n    \n    // verify args\n    if(target.isEmpty()) {\n      throw new IllegalArgumentException(\"Target file name is empty\");\n    }\n    if(srcs \u003d\u003d null || srcs.length \u003d\u003d 0) {\n      throw new IllegalArgumentException(\"No sources given\");\n    }\n    \n    // We require all files be in the same directory\n    String trgParent \u003d \n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    HdfsFileStatus resultingStat \u003d null;\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot concat \" + target, safeMode);\n      }\n      concatInternal(target, srcs);\n      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        resultingStat \u003d dir.getFileInfo(target, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n      logAuditEvent(UserGroupInformation.getLoginUser(),\n                    Server.getRemoteIp(),\n                    \"concat\", Arrays.toString(srcs), target, resultingStat);\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldValue": "[public]",
        "newValue": "[]"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,46 @@\n+  public void concat(String target, String [] srcs) \n+      throws IOException, UnresolvedLinkException {\n+    if(FSNamesystem.LOG.isDebugEnabled()) {\n+      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n+          \" to \" + target);\n+    }\n+    \n+    // verify args\n+    if(target.isEmpty()) {\n+      throw new IllegalArgumentException(\"Target file name is empty\");\n+    }\n+    if(srcs \u003d\u003d null || srcs.length \u003d\u003d 0) {\n+      throw new IllegalArgumentException(\"No sources given\");\n+    }\n+    \n+    // We require all files be in the same directory\n+    String trgParent \u003d \n+      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n+    for (String s : srcs) {\n+      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n+      if (!srcParent.equals(trgParent)) {\n+        throw new IllegalArgumentException(\n+           \"Sources and target are not in the same directory\");\n+      }\n+    }\n+\n+    HdfsFileStatus resultingStat \u003d null;\n+    writeLock();\n+    try {\n+      if (isInSafeMode()) {\n+        throw new SafeModeException(\"Cannot concat \" + target, safeMode);\n+      }\n+      concatInternal(target, srcs);\n+      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n+        resultingStat \u003d dir.getFileInfo(target, false);\n+      }\n+    } finally {\n+      writeUnlock();\n+    }\n+    getEditLog().logSync();\n+    if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n+      logAuditEvent(UserGroupInformation.getLoginUser(),\n+                    Server.getRemoteIp(),\n+                    \"concat\", Arrays.toString(srcs), target, resultingStat);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void concat(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    if(FSNamesystem.LOG.isDebugEnabled()) {\n      FSNamesystem.LOG.debug(\"concat \" + Arrays.toString(srcs) +\n          \" to \" + target);\n    }\n    \n    // verify args\n    if(target.isEmpty()) {\n      throw new IllegalArgumentException(\"Target file name is empty\");\n    }\n    if(srcs \u003d\u003d null || srcs.length \u003d\u003d 0) {\n      throw new IllegalArgumentException(\"No sources given\");\n    }\n    \n    // We require all files be in the same directory\n    String trgParent \u003d \n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    HdfsFileStatus resultingStat \u003d null;\n    writeLock();\n    try {\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot concat \" + target, safeMode);\n      }\n      concatInternal(target, srcs);\n      if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n        resultingStat \u003d dir.getFileInfo(target, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (auditLog.isInfoEnabled() \u0026\u0026 isExternalInvocation()) {\n      logAuditEvent(UserGroupInformation.getLoginUser(),\n                    Server.getRemoteIp(),\n                    \"concat\", Arrays.toString(srcs), target, resultingStat);\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
    }
  }
}