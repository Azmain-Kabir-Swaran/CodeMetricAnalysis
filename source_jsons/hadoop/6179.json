{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSImageFormat.java",
  "functionName": "saveChildren",
  "functionId": "saveChildren___children-ReadOnlyList__INode____out-DataOutputStream__inSnapshot-boolean__counter-Counter",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
  "functionStartLine": 1337,
  "functionEndLine": 1360,
  "numCommitsSeen": 127,
  "timeTaken": 2579,
  "changeHistory": [
    "f43a20c529ac3f104add95b222de6580757b3763",
    "97f58955a6045b373ab73653bf26ab5922b00cf3"
  ],
  "changeHistoryShort": {
    "f43a20c529ac3f104add95b222de6580757b3763": "Ybodychange",
    "97f58955a6045b373ab73653bf26ab5922b00cf3": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f43a20c529ac3f104add95b222de6580757b3763": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7097. Allow block reports to be processed during checkpointing on standby name node. (kihwal via wang)\n",
      "commitDate": "25/11/14 3:37 PM",
      "commitName": "f43a20c529ac3f104add95b222de6580757b3763",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "11/11/14 12:42 PM",
      "commitNameOld": "571e9c623241106dad5521a870fb8daef3f2b00a",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 14.12,
      "commitsBetweenForRepo": 103,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,24 @@\n     private int saveChildren(ReadOnlyList\u003cINode\u003e children,\n         DataOutputStream out, boolean inSnapshot, Counter counter)\n         throws IOException {\n       // Write normal children INode.\n       out.writeInt(children.size());\n       int dirNum \u003d 0;\n-      int i \u003d 0;\n       for(INode child : children) {\n         // print all children first\n         // TODO: for HDFS-5428, we cannot change the format/content of fsimage\n         // here, thus even if the parent directory is in snapshot, we still\n         // do not handle INodeUC as those stored in deleted list\n         saveINode2Image(child, out, false, referenceMap, counter);\n         if (child.isDirectory()) {\n           dirNum++;\n         } else if (inSnapshot \u0026\u0026 child.isFile()\n             \u0026\u0026 child.asFile().isUnderConstruction()) {\n           this.snapshotUCMap.put(child.getId(), child.asFile());\n         }\n-        if (i++ % 50 \u003d\u003d 0) {\n+        if (checkCancelCounter++ % CHECK_CANCEL_INTERVAL \u003d\u003d 0) {\n           context.checkCancelled();\n         }\n       }\n       return dirNum;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private int saveChildren(ReadOnlyList\u003cINode\u003e children,\n        DataOutputStream out, boolean inSnapshot, Counter counter)\n        throws IOException {\n      // Write normal children INode.\n      out.writeInt(children.size());\n      int dirNum \u003d 0;\n      for(INode child : children) {\n        // print all children first\n        // TODO: for HDFS-5428, we cannot change the format/content of fsimage\n        // here, thus even if the parent directory is in snapshot, we still\n        // do not handle INodeUC as those stored in deleted list\n        saveINode2Image(child, out, false, referenceMap, counter);\n        if (child.isDirectory()) {\n          dirNum++;\n        } else if (inSnapshot \u0026\u0026 child.isFile()\n            \u0026\u0026 child.asFile().isUnderConstruction()) {\n          this.snapshotUCMap.put(child.getId(), child.asFile());\n        }\n        if (checkCancelCounter++ % CHECK_CANCEL_INTERVAL \u003d\u003d 0) {\n          context.checkCancelled();\n        }\n      }\n      return dirNum;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
      "extendedDetails": {}
    },
    "97f58955a6045b373ab73653bf26ab5922b00cf3": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-6293. Issues with OIV processing PB-based fsimages. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594439 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/05/14 6:15 PM",
      "commitName": "97f58955a6045b373ab73653bf26ab5922b00cf3",
      "commitAuthor": "Kihwal Lee",
      "diff": "@@ -0,0 +1,25 @@\n+    private int saveChildren(ReadOnlyList\u003cINode\u003e children,\n+        DataOutputStream out, boolean inSnapshot, Counter counter)\n+        throws IOException {\n+      // Write normal children INode.\n+      out.writeInt(children.size());\n+      int dirNum \u003d 0;\n+      int i \u003d 0;\n+      for(INode child : children) {\n+        // print all children first\n+        // TODO: for HDFS-5428, we cannot change the format/content of fsimage\n+        // here, thus even if the parent directory is in snapshot, we still\n+        // do not handle INodeUC as those stored in deleted list\n+        saveINode2Image(child, out, false, referenceMap, counter);\n+        if (child.isDirectory()) {\n+          dirNum++;\n+        } else if (inSnapshot \u0026\u0026 child.isFile()\n+            \u0026\u0026 child.asFile().isUnderConstruction()) {\n+          this.snapshotUCMap.put(child.getId(), child.asFile());\n+        }\n+        if (i++ % 50 \u003d\u003d 0) {\n+          context.checkCancelled();\n+        }\n+      }\n+      return dirNum;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private int saveChildren(ReadOnlyList\u003cINode\u003e children,\n        DataOutputStream out, boolean inSnapshot, Counter counter)\n        throws IOException {\n      // Write normal children INode.\n      out.writeInt(children.size());\n      int dirNum \u003d 0;\n      int i \u003d 0;\n      for(INode child : children) {\n        // print all children first\n        // TODO: for HDFS-5428, we cannot change the format/content of fsimage\n        // here, thus even if the parent directory is in snapshot, we still\n        // do not handle INodeUC as those stored in deleted list\n        saveINode2Image(child, out, false, referenceMap, counter);\n        if (child.isDirectory()) {\n          dirNum++;\n        } else if (inSnapshot \u0026\u0026 child.isFile()\n            \u0026\u0026 child.asFile().isUnderConstruction()) {\n          this.snapshotUCMap.put(child.getId(), child.asFile());\n        }\n        if (i++ % 50 \u003d\u003d 0) {\n          context.checkCancelled();\n        }\n      }\n      return dirNum;\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java"
    }
  }
}