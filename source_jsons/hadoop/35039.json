{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DistributedScheduler.java",
  "functionName": "allocateForDistributedScheduling",
  "functionId": "allocateForDistributedScheduling___request-DistributedSchedulingAllocateRequest",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/DistributedScheduler.java",
  "functionStartLine": 220,
  "functionEndLine": 259,
  "numCommitsSeen": 15,
  "timeTaken": 5721,
  "changeHistory": [
    "2064ca015d1584263aac0cc20c60b925a3aff612",
    "ac1e5d4f77e3b9df8dcacb0b1f72eecc27931eb8",
    "283fa33febe043bd7b4fa87546be26c9c5a8f8b5",
    "10be45986cdf86a89055065b752959bd6369d54f",
    "82c9e061017c32e633e0b0cbb7978749a6df4fb2",
    "e5766b1dbee02ae0ef89618e172f3fb227af19e8",
    "99e5dd68d0f44109c169d74824fa45a7396a5990",
    "1597630681c784a3d59f5605b87e96197b8139d7",
    "c282a08f3892e2e8ceb58e1e9a411062fbd1fb2b"
  ],
  "changeHistoryShort": {
    "2064ca015d1584263aac0cc20c60b925a3aff612": "Ybodychange",
    "ac1e5d4f77e3b9df8dcacb0b1f72eecc27931eb8": "Ybodychange",
    "283fa33febe043bd7b4fa87546be26c9c5a8f8b5": "Ybodychange",
    "10be45986cdf86a89055065b752959bd6369d54f": "Ybodychange",
    "82c9e061017c32e633e0b0cbb7978749a6df4fb2": "Ybodychange",
    "e5766b1dbee02ae0ef89618e172f3fb227af19e8": "Ymultichange(Yfilerename,Yreturntypechange,Ybodychange,Yparameterchange)",
    "99e5dd68d0f44109c169d74824fa45a7396a5990": "Ymultichange(Yparameterchange,Ybodychange)",
    "1597630681c784a3d59f5605b87e96197b8139d7": "Ybodychange",
    "c282a08f3892e2e8ceb58e1e9a411062fbd1fb2b": "Yintroduced"
  },
  "changeHistoryDetails": {
    "2064ca015d1584263aac0cc20c60b925a3aff612": {
      "type": "Ybodychange",
      "commitMessage": "YARN-9349.  Changed logging to use slf4j api.\n            Contributed by Prabhu Joseph\n",
      "commitDate": "15/03/19 4:20 PM",
      "commitName": "2064ca015d1584263aac0cc20c60b925a3aff612",
      "commitAuthor": "Eric Yang",
      "commitDateOld": "27/12/16 12:40 PM",
      "commitNameOld": "ac1e5d4f77e3b9df8dcacb0b1f72eecc27931eb8",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 808.11,
      "commitsBetweenForRepo": 6278,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,42 +1,40 @@\n   public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n       DistributedSchedulingAllocateRequest request)\n       throws YarnException, IOException {\n \n     // Partition requests to GUARANTEED and OPPORTUNISTIC.\n     OpportunisticContainerAllocator.PartitionedResourceRequests\n         partitionedAsks \u003d containerAllocator\n         .partitionAskList(request.getAllocateRequest().getAskList());\n \n     // Allocate OPPORTUNISTIC containers.\n     List\u003cContainer\u003e allocatedContainers \u003d\n         containerAllocator.allocateContainers(\n             request.getAllocateRequest().getResourceBlacklistRequest(),\n             partitionedAsks.getOpportunistic(), applicationAttemptId,\n             oppContainerContext, rmIdentifier, appSubmitter);\n \n     // Prepare request for sending to RM for scheduling GUARANTEED containers.\n     request.setAllocatedContainers(allocatedContainers);\n     request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n \n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Forwarding allocate request to the\" +\n+    LOG.debug(\"Forwarding allocate request to the\" +\n           \"Distributed Scheduler Service on YARN RM\");\n-    }\n \n     DistributedSchedulingAllocateResponse dsResp \u003d\n         getNextInterceptor().allocateForDistributedScheduling(request);\n \n     // Update host to nodeId mapping\n     setNodeList(dsResp.getNodesForScheduling());\n     List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n     for (NMToken nmToken : nmTokens) {\n       nodeTokens.put(nmToken.getNodeId(), nmToken);\n     }\n \n     // Check if we have NM tokens for all the allocated containers. If not\n     // generate one and update the response.\n     updateAllocateResponse(\n         dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n \n     return dsResp;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n      DistributedSchedulingAllocateRequest request)\n      throws YarnException, IOException {\n\n    // Partition requests to GUARANTEED and OPPORTUNISTIC.\n    OpportunisticContainerAllocator.PartitionedResourceRequests\n        partitionedAsks \u003d containerAllocator\n        .partitionAskList(request.getAllocateRequest().getAskList());\n\n    // Allocate OPPORTUNISTIC containers.\n    List\u003cContainer\u003e allocatedContainers \u003d\n        containerAllocator.allocateContainers(\n            request.getAllocateRequest().getResourceBlacklistRequest(),\n            partitionedAsks.getOpportunistic(), applicationAttemptId,\n            oppContainerContext, rmIdentifier, appSubmitter);\n\n    // Prepare request for sending to RM for scheduling GUARANTEED containers.\n    request.setAllocatedContainers(allocatedContainers);\n    request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n\n    LOG.debug(\"Forwarding allocate request to the\" +\n          \"Distributed Scheduler Service on YARN RM\");\n\n    DistributedSchedulingAllocateResponse dsResp \u003d\n        getNextInterceptor().allocateForDistributedScheduling(request);\n\n    // Update host to nodeId mapping\n    setNodeList(dsResp.getNodesForScheduling());\n    List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n    for (NMToken nmToken : nmTokens) {\n      nodeTokens.put(nmToken.getNodeId(), nmToken);\n    }\n\n    // Check if we have NM tokens for all the allocated containers. If not\n    // generate one and update the response.\n    updateAllocateResponse(\n        dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n\n    return dsResp;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/DistributedScheduler.java",
      "extendedDetails": {}
    },
    "ac1e5d4f77e3b9df8dcacb0b1f72eecc27931eb8": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5938. Refactoring OpportunisticContainerAllocator to use SchedulerRequestKey instead of Priority and other misc fixes (asuresh)\n",
      "commitDate": "27/12/16 12:40 PM",
      "commitName": "ac1e5d4f77e3b9df8dcacb0b1f72eecc27931eb8",
      "commitAuthor": "Arun Suresh",
      "commitDateOld": "09/11/16 12:11 AM",
      "commitNameOld": "283fa33febe043bd7b4fa87546be26c9c5a8f8b5",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 48.52,
      "commitsBetweenForRepo": 269,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,49 +1,42 @@\n   public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n       DistributedSchedulingAllocateRequest request)\n       throws YarnException, IOException {\n \n     // Partition requests to GUARANTEED and OPPORTUNISTIC.\n     OpportunisticContainerAllocator.PartitionedResourceRequests\n         partitionedAsks \u003d containerAllocator\n         .partitionAskList(request.getAllocateRequest().getAskList());\n \n     // Allocate OPPORTUNISTIC containers.\n-    request.getAllocateRequest().setAskList(partitionedAsks.getOpportunistic());\n     List\u003cContainer\u003e allocatedContainers \u003d\n         containerAllocator.allocateContainers(\n-            request.getAllocateRequest(), applicationAttemptId,\n+            request.getAllocateRequest().getResourceBlacklistRequest(),\n+            partitionedAsks.getOpportunistic(), applicationAttemptId,\n             oppContainerContext, rmIdentifier, appSubmitter);\n \n     // Prepare request for sending to RM for scheduling GUARANTEED containers.\n     request.setAllocatedContainers(allocatedContainers);\n     request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Forwarding allocate request to the\" +\n           \"Distributed Scheduler Service on YARN RM\");\n     }\n \n     DistributedSchedulingAllocateResponse dsResp \u003d\n         getNextInterceptor().allocateForDistributedScheduling(request);\n \n     // Update host to nodeId mapping\n     setNodeList(dsResp.getNodesForScheduling());\n     List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n     for (NMToken nmToken : nmTokens) {\n       nodeTokens.put(nmToken.getNodeId(), nmToken);\n     }\n \n-    oppContainerContext.updateCompletedContainers(dsResp.getAllocateResponse());\n-\n     // Check if we have NM tokens for all the allocated containers. If not\n     // generate one and update the response.\n     updateAllocateResponse(\n         dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n \n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Number of opportunistic containers currently\" +\n-          \"allocated by application: \" + oppContainerContext\n-          .getContainersAllocated().size());\n-    }\n     return dsResp;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n      DistributedSchedulingAllocateRequest request)\n      throws YarnException, IOException {\n\n    // Partition requests to GUARANTEED and OPPORTUNISTIC.\n    OpportunisticContainerAllocator.PartitionedResourceRequests\n        partitionedAsks \u003d containerAllocator\n        .partitionAskList(request.getAllocateRequest().getAskList());\n\n    // Allocate OPPORTUNISTIC containers.\n    List\u003cContainer\u003e allocatedContainers \u003d\n        containerAllocator.allocateContainers(\n            request.getAllocateRequest().getResourceBlacklistRequest(),\n            partitionedAsks.getOpportunistic(), applicationAttemptId,\n            oppContainerContext, rmIdentifier, appSubmitter);\n\n    // Prepare request for sending to RM for scheduling GUARANTEED containers.\n    request.setAllocatedContainers(allocatedContainers);\n    request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Forwarding allocate request to the\" +\n          \"Distributed Scheduler Service on YARN RM\");\n    }\n\n    DistributedSchedulingAllocateResponse dsResp \u003d\n        getNextInterceptor().allocateForDistributedScheduling(request);\n\n    // Update host to nodeId mapping\n    setNodeList(dsResp.getNodesForScheduling());\n    List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n    for (NMToken nmToken : nmTokens) {\n      nodeTokens.put(nmToken.getNodeId(), nmToken);\n    }\n\n    // Check if we have NM tokens for all the allocated containers. If not\n    // generate one and update the response.\n    updateAllocateResponse(\n        dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n\n    return dsResp;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/DistributedScheduler.java",
      "extendedDetails": {}
    },
    "283fa33febe043bd7b4fa87546be26c9c5a8f8b5": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5823. Update NMTokens in case of requests with only opportunistic containers. (Konstantinos Karanasos via asuresh)\n",
      "commitDate": "09/11/16 12:11 AM",
      "commitName": "283fa33febe043bd7b4fa87546be26c9c5a8f8b5",
      "commitAuthor": "Arun Suresh",
      "commitDateOld": "29/10/16 2:03 AM",
      "commitNameOld": "aa3cab1eb29c56368d15882d7260a994e615e8d8",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 10.96,
      "commitsBetweenForRepo": 107,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,49 @@\n   public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n       DistributedSchedulingAllocateRequest request)\n       throws YarnException, IOException {\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Forwarding allocate request to the\" +\n-          \"Distributed Scheduler Service on YARN RM\");\n-    }\n+\n+    // Partition requests to GUARANTEED and OPPORTUNISTIC.\n+    OpportunisticContainerAllocator.PartitionedResourceRequests\n+        partitionedAsks \u003d containerAllocator\n+        .partitionAskList(request.getAllocateRequest().getAskList());\n+\n+    // Allocate OPPORTUNISTIC containers.\n+    request.getAllocateRequest().setAskList(partitionedAsks.getOpportunistic());\n     List\u003cContainer\u003e allocatedContainers \u003d\n         containerAllocator.allocateContainers(\n             request.getAllocateRequest(), applicationAttemptId,\n             oppContainerContext, rmIdentifier, appSubmitter);\n \n+    // Prepare request for sending to RM for scheduling GUARANTEED containers.\n     request.setAllocatedContainers(allocatedContainers);\n+    request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n+\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Forwarding allocate request to the\" +\n+          \"Distributed Scheduler Service on YARN RM\");\n+    }\n \n     DistributedSchedulingAllocateResponse dsResp \u003d\n         getNextInterceptor().allocateForDistributedScheduling(request);\n \n     // Update host to nodeId mapping\n     setNodeList(dsResp.getNodesForScheduling());\n     List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n     for (NMToken nmToken : nmTokens) {\n       nodeTokens.put(nmToken.getNodeId(), nmToken);\n     }\n \n     oppContainerContext.updateCompletedContainers(dsResp.getAllocateResponse());\n \n     // Check if we have NM tokens for all the allocated containers. If not\n     // generate one and update the response.\n     updateAllocateResponse(\n         dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Number of opportunistic containers currently\" +\n           \"allocated by application: \" + oppContainerContext\n           .getContainersAllocated().size());\n     }\n     return dsResp;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n      DistributedSchedulingAllocateRequest request)\n      throws YarnException, IOException {\n\n    // Partition requests to GUARANTEED and OPPORTUNISTIC.\n    OpportunisticContainerAllocator.PartitionedResourceRequests\n        partitionedAsks \u003d containerAllocator\n        .partitionAskList(request.getAllocateRequest().getAskList());\n\n    // Allocate OPPORTUNISTIC containers.\n    request.getAllocateRequest().setAskList(partitionedAsks.getOpportunistic());\n    List\u003cContainer\u003e allocatedContainers \u003d\n        containerAllocator.allocateContainers(\n            request.getAllocateRequest(), applicationAttemptId,\n            oppContainerContext, rmIdentifier, appSubmitter);\n\n    // Prepare request for sending to RM for scheduling GUARANTEED containers.\n    request.setAllocatedContainers(allocatedContainers);\n    request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Forwarding allocate request to the\" +\n          \"Distributed Scheduler Service on YARN RM\");\n    }\n\n    DistributedSchedulingAllocateResponse dsResp \u003d\n        getNextInterceptor().allocateForDistributedScheduling(request);\n\n    // Update host to nodeId mapping\n    setNodeList(dsResp.getNodesForScheduling());\n    List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n    for (NMToken nmToken : nmTokens) {\n      nodeTokens.put(nmToken.getNodeId(), nmToken);\n    }\n\n    oppContainerContext.updateCompletedContainers(dsResp.getAllocateResponse());\n\n    // Check if we have NM tokens for all the allocated containers. If not\n    // generate one and update the response.\n    updateAllocateResponse(\n        dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Number of opportunistic containers currently\" +\n          \"allocated by application: \" + oppContainerContext\n          .getContainersAllocated().size());\n    }\n    return dsResp;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/DistributedScheduler.java",
      "extendedDetails": {}
    },
    "10be45986cdf86a89055065b752959bd6369d54f": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5486. Update OpportunisticContainerAllocatorAMService::allocate method to handle OPPORTUNISTIC container requests. (Konstantinos Karanasos via asuresh)\n",
      "commitDate": "29/09/16 3:11 PM",
      "commitName": "10be45986cdf86a89055065b752959bd6369d54f",
      "commitAuthor": "Arun Suresh",
      "commitDateOld": "09/08/16 12:42 AM",
      "commitNameOld": "82c9e061017c32e633e0b0cbb7978749a6df4fb2",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 51.6,
      "commitsBetweenForRepo": 319,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,47 +1,38 @@\n   public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n       DistributedSchedulingAllocateRequest request)\n       throws YarnException, IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Forwarding allocate request to the\" +\n           \"Distributed Scheduler Service on YARN RM\");\n     }\n     List\u003cContainer\u003e allocatedContainers \u003d\n         containerAllocator.allocateContainers(\n             request.getAllocateRequest(), applicationAttemptId,\n             oppContainerContext, rmIdentifier, appSubmitter);\n \n     request.setAllocatedContainers(allocatedContainers);\n \n     DistributedSchedulingAllocateResponse dsResp \u003d\n         getNextInterceptor().allocateForDistributedScheduling(request);\n \n     // Update host to nodeId mapping\n     setNodeList(dsResp.getNodesForScheduling());\n     List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n     for (NMToken nmToken : nmTokens) {\n-      oppContainerContext.getNodeTokens().put(nmToken.getNodeId(), nmToken);\n+      nodeTokens.put(nmToken.getNodeId(), nmToken);\n     }\n \n-    List\u003cContainerStatus\u003e completedContainers \u003d\n-        dsResp.getAllocateResponse().getCompletedContainersStatuses();\n-\n-    // Only account for opportunistic containers\n-    for (ContainerStatus cs : completedContainers) {\n-      if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n-        oppContainerContext.getContainersAllocated()\n-            .remove(cs.getContainerId());\n-      }\n-    }\n+    oppContainerContext.updateCompletedContainers(dsResp.getAllocateResponse());\n \n     // Check if we have NM tokens for all the allocated containers. If not\n     // generate one and update the response.\n-    updateResponseWithNMTokens(\n+    updateAllocateResponse(\n         dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Number of opportunistic containers currently\" +\n           \"allocated by application: \" + oppContainerContext\n           .getContainersAllocated().size());\n     }\n     return dsResp;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n      DistributedSchedulingAllocateRequest request)\n      throws YarnException, IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Forwarding allocate request to the\" +\n          \"Distributed Scheduler Service on YARN RM\");\n    }\n    List\u003cContainer\u003e allocatedContainers \u003d\n        containerAllocator.allocateContainers(\n            request.getAllocateRequest(), applicationAttemptId,\n            oppContainerContext, rmIdentifier, appSubmitter);\n\n    request.setAllocatedContainers(allocatedContainers);\n\n    DistributedSchedulingAllocateResponse dsResp \u003d\n        getNextInterceptor().allocateForDistributedScheduling(request);\n\n    // Update host to nodeId mapping\n    setNodeList(dsResp.getNodesForScheduling());\n    List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n    for (NMToken nmToken : nmTokens) {\n      nodeTokens.put(nmToken.getNodeId(), nmToken);\n    }\n\n    oppContainerContext.updateCompletedContainers(dsResp.getAllocateResponse());\n\n    // Check if we have NM tokens for all the allocated containers. If not\n    // generate one and update the response.\n    updateAllocateResponse(\n        dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Number of opportunistic containers currently\" +\n          \"allocated by application: \" + oppContainerContext\n          .getContainersAllocated().size());\n    }\n    return dsResp;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/DistributedScheduler.java",
      "extendedDetails": {}
    },
    "82c9e061017c32e633e0b0cbb7978749a6df4fb2": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5457. Refactor DistributedScheduling framework to pull out common functionality. (asuresh)\n",
      "commitDate": "09/08/16 12:42 AM",
      "commitName": "82c9e061017c32e633e0b0cbb7978749a6df4fb2",
      "commitAuthor": "Arun Suresh",
      "commitDateOld": "31/07/16 11:48 AM",
      "commitNameOld": "e5766b1dbee02ae0ef89618e172f3fb227af19e8",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 8.54,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,47 @@\n   public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n       DistributedSchedulingAllocateRequest request)\n       throws YarnException, IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Forwarding allocate request to the\" +\n           \"Distributed Scheduler Service on YARN RM\");\n     }\n-    // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n-    PartitionedResourceRequests partitionedAsks \u003d\n-        partitionAskList(request.getAllocateRequest().getAskList());\n-\n-    List\u003cContainerId\u003e releasedContainers \u003d\n-        request.getAllocateRequest().getReleaseList();\n-    int numReleasedContainers \u003d releasedContainers.size();\n-    if (numReleasedContainers \u003e 0) {\n-      LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n-          + numReleasedContainers);\n-      containersAllocated.removeAll(releasedContainers);\n-    }\n-\n-    // Also, update black list\n-    ResourceBlacklistRequest rbr \u003d\n-        request.getAllocateRequest().getResourceBlacklistRequest();\n-    if (rbr !\u003d null) {\n-      blacklist.removeAll(rbr.getBlacklistRemovals());\n-      blacklist.addAll(rbr.getBlacklistAdditions());\n-    }\n-\n-    // Add OPPORTUNISTIC reqs to the outstanding reqs\n-    addToOutstandingReqs(partitionedAsks.getOpportunistic());\n-\n-    List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n-    for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n-      // Allocated containers :\n-      //  Key \u003d Requested Capability,\n-      //  Value \u003d List of Containers of given Cap (The actual container size\n-      //          might be different than what is requested.. which is why\n-      //          we need the requested capability (key) to match against\n-      //          the outstanding reqs)\n-      Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n-          containerAllocator.allocate(this.appParams, containerIdCounter,\n-              outstandingOpReqs.get(priority).values(), blacklist,\n-              applicationAttemptId, nodeList, appSubmitter);\n-      for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n-        matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n-        allocatedContainers.addAll(e.getValue());\n-      }\n-    }\n+    List\u003cContainer\u003e allocatedContainers \u003d\n+        containerAllocator.allocateContainers(\n+            request.getAllocateRequest(), applicationAttemptId,\n+            oppContainerContext, rmIdentifier, appSubmitter);\n \n     request.setAllocatedContainers(allocatedContainers);\n \n-    // Send all the GUARANTEED Reqs to RM\n-    request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n     DistributedSchedulingAllocateResponse dsResp \u003d\n         getNextInterceptor().allocateForDistributedScheduling(request);\n \n     // Update host to nodeId mapping\n     setNodeList(dsResp.getNodesForScheduling());\n     List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n     for (NMToken nmToken : nmTokens) {\n-      nodeTokens.put(nmToken.getNodeId(), nmToken);\n+      oppContainerContext.getNodeTokens().put(nmToken.getNodeId(), nmToken);\n     }\n \n     List\u003cContainerStatus\u003e completedContainers \u003d\n         dsResp.getAllocateResponse().getCompletedContainersStatuses();\n \n     // Only account for opportunistic containers\n     for (ContainerStatus cs : completedContainers) {\n       if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n-        containersAllocated.remove(cs.getContainerId());\n+        oppContainerContext.getContainersAllocated()\n+            .remove(cs.getContainerId());\n       }\n     }\n \n     // Check if we have NM tokens for all the allocated containers. If not\n     // generate one and update the response.\n     updateResponseWithNMTokens(\n         dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n \n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\n-          \"Number of opportunistic containers currently allocated by\" +\n-              \"application: \" + containersAllocated.size());\n+      LOG.debug(\"Number of opportunistic containers currently\" +\n+          \"allocated by application: \" + oppContainerContext\n+          .getContainersAllocated().size());\n     }\n     return dsResp;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n      DistributedSchedulingAllocateRequest request)\n      throws YarnException, IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Forwarding allocate request to the\" +\n          \"Distributed Scheduler Service on YARN RM\");\n    }\n    List\u003cContainer\u003e allocatedContainers \u003d\n        containerAllocator.allocateContainers(\n            request.getAllocateRequest(), applicationAttemptId,\n            oppContainerContext, rmIdentifier, appSubmitter);\n\n    request.setAllocatedContainers(allocatedContainers);\n\n    DistributedSchedulingAllocateResponse dsResp \u003d\n        getNextInterceptor().allocateForDistributedScheduling(request);\n\n    // Update host to nodeId mapping\n    setNodeList(dsResp.getNodesForScheduling());\n    List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n    for (NMToken nmToken : nmTokens) {\n      oppContainerContext.getNodeTokens().put(nmToken.getNodeId(), nmToken);\n    }\n\n    List\u003cContainerStatus\u003e completedContainers \u003d\n        dsResp.getAllocateResponse().getCompletedContainersStatuses();\n\n    // Only account for opportunistic containers\n    for (ContainerStatus cs : completedContainers) {\n      if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n        oppContainerContext.getContainersAllocated()\n            .remove(cs.getContainerId());\n      }\n    }\n\n    // Check if we have NM tokens for all the allocated containers. If not\n    // generate one and update the response.\n    updateResponseWithNMTokens(\n        dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Number of opportunistic containers currently\" +\n          \"allocated by application: \" + oppContainerContext\n          .getContainersAllocated().size());\n    }\n    return dsResp;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/DistributedScheduler.java",
      "extendedDetails": {}
    },
    "e5766b1dbee02ae0ef89618e172f3fb227af19e8": {
      "type": "Ymultichange(Yfilerename,Yreturntypechange,Ybodychange,Yparameterchange)",
      "commitMessage": "YARN-5113. Refactoring and other clean-up for distributed scheduling. (Konstantinos Karanasos via asuresh)\n",
      "commitDate": "31/07/16 11:48 AM",
      "commitName": "e5766b1dbee02ae0ef89618e172f3fb227af19e8",
      "commitAuthor": "Arun Suresh",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "YARN-5113. Refactoring and other clean-up for distributed scheduling. (Konstantinos Karanasos via asuresh)\n",
          "commitDate": "31/07/16 11:48 AM",
          "commitName": "e5766b1dbee02ae0ef89618e172f3fb227af19e8",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "30/07/16 7:29 PM",
          "commitNameOld": "3d191cc15244e1e29f837b34a9bd1d029e003064",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 0.68,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,83 +1,85 @@\n-  public DistSchedAllocateResponse allocateForDistributedScheduling(\n-      DistSchedAllocateRequest request) throws YarnException, IOException {\n+  public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n+      DistributedSchedulingAllocateRequest request)\n+      throws YarnException, IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Forwarding allocate request to the\" +\n           \"Distributed Scheduler Service on YARN RM\");\n     }\n     // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n-    PartitionedResourceRequests partitionedAsks \u003d partitionAskList(\n-        request.getAllocateRequest().getAskList());\n+    PartitionedResourceRequests partitionedAsks \u003d\n+        partitionAskList(request.getAllocateRequest().getAskList());\n \n     List\u003cContainerId\u003e releasedContainers \u003d\n         request.getAllocateRequest().getReleaseList();\n     int numReleasedContainers \u003d releasedContainers.size();\n     if (numReleasedContainers \u003e 0) {\n       LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n           + numReleasedContainers);\n       containersAllocated.removeAll(releasedContainers);\n     }\n \n     // Also, update black list\n     ResourceBlacklistRequest rbr \u003d\n         request.getAllocateRequest().getResourceBlacklistRequest();\n     if (rbr !\u003d null) {\n       blacklist.removeAll(rbr.getBlacklistRemovals());\n       blacklist.addAll(rbr.getBlacklistAdditions());\n     }\n \n     // Add OPPORTUNISTIC reqs to the outstanding reqs\n     addToOutstandingReqs(partitionedAsks.getOpportunistic());\n \n     List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n     for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n       // Allocated containers :\n       //  Key \u003d Requested Capability,\n       //  Value \u003d List of Containers of given Cap (The actual container size\n       //          might be different than what is requested.. which is why\n       //          we need the requested capability (key) to match against\n       //          the outstanding reqs)\n       Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n           containerAllocator.allocate(this.appParams, containerIdCounter,\n               outstandingOpReqs.get(priority).values(), blacklist,\n               applicationAttemptId, nodeList, appSubmitter);\n       for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n         matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n         allocatedContainers.addAll(e.getValue());\n       }\n     }\n+\n     request.setAllocatedContainers(allocatedContainers);\n \n     // Send all the GUARANTEED Reqs to RM\n     request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n-    DistSchedAllocateResponse dsResp \u003d\n+    DistributedSchedulingAllocateResponse dsResp \u003d\n         getNextInterceptor().allocateForDistributedScheduling(request);\n \n     // Update host to nodeId mapping\n     setNodeList(dsResp.getNodesForScheduling());\n     List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n     for (NMToken nmToken : nmTokens) {\n       nodeTokens.put(nmToken.getNodeId(), nmToken);\n     }\n \n     List\u003cContainerStatus\u003e completedContainers \u003d\n         dsResp.getAllocateResponse().getCompletedContainersStatuses();\n \n     // Only account for opportunistic containers\n     for (ContainerStatus cs : completedContainers) {\n       if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n         containersAllocated.remove(cs.getContainerId());\n       }\n     }\n \n     // Check if we have NM tokens for all the allocated containers. If not\n     // generate one and update the response.\n     updateResponseWithNMTokens(\n         dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\n           \"Number of opportunistic containers currently allocated by\" +\n               \"application: \" + containersAllocated.size());\n     }\n     return dsResp;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n      DistributedSchedulingAllocateRequest request)\n      throws YarnException, IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Forwarding allocate request to the\" +\n          \"Distributed Scheduler Service on YARN RM\");\n    }\n    // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n    PartitionedResourceRequests partitionedAsks \u003d\n        partitionAskList(request.getAllocateRequest().getAskList());\n\n    List\u003cContainerId\u003e releasedContainers \u003d\n        request.getAllocateRequest().getReleaseList();\n    int numReleasedContainers \u003d releasedContainers.size();\n    if (numReleasedContainers \u003e 0) {\n      LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n          + numReleasedContainers);\n      containersAllocated.removeAll(releasedContainers);\n    }\n\n    // Also, update black list\n    ResourceBlacklistRequest rbr \u003d\n        request.getAllocateRequest().getResourceBlacklistRequest();\n    if (rbr !\u003d null) {\n      blacklist.removeAll(rbr.getBlacklistRemovals());\n      blacklist.addAll(rbr.getBlacklistAdditions());\n    }\n\n    // Add OPPORTUNISTIC reqs to the outstanding reqs\n    addToOutstandingReqs(partitionedAsks.getOpportunistic());\n\n    List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n    for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n      // Allocated containers :\n      //  Key \u003d Requested Capability,\n      //  Value \u003d List of Containers of given Cap (The actual container size\n      //          might be different than what is requested.. which is why\n      //          we need the requested capability (key) to match against\n      //          the outstanding reqs)\n      Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n          containerAllocator.allocate(this.appParams, containerIdCounter,\n              outstandingOpReqs.get(priority).values(), blacklist,\n              applicationAttemptId, nodeList, appSubmitter);\n      for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n        matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n        allocatedContainers.addAll(e.getValue());\n      }\n    }\n\n    request.setAllocatedContainers(allocatedContainers);\n\n    // Send all the GUARANTEED Reqs to RM\n    request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n    DistributedSchedulingAllocateResponse dsResp \u003d\n        getNextInterceptor().allocateForDistributedScheduling(request);\n\n    // Update host to nodeId mapping\n    setNodeList(dsResp.getNodesForScheduling());\n    List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n    for (NMToken nmToken : nmTokens) {\n      nodeTokens.put(nmToken.getNodeId(), nmToken);\n    }\n\n    List\u003cContainerStatus\u003e completedContainers \u003d\n        dsResp.getAllocateResponse().getCompletedContainersStatuses();\n\n    // Only account for opportunistic containers\n    for (ContainerStatus cs : completedContainers) {\n      if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n        containersAllocated.remove(cs.getContainerId());\n      }\n    }\n\n    // Check if we have NM tokens for all the allocated containers. If not\n    // generate one and update the response.\n    updateResponseWithNMTokens(\n        dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\n          \"Number of opportunistic containers currently allocated by\" +\n              \"application: \" + containersAllocated.size());\n    }\n    return dsResp;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/DistributedScheduler.java",
          "extendedDetails": {
            "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/LocalScheduler.java",
            "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/DistributedScheduler.java"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "YARN-5113. Refactoring and other clean-up for distributed scheduling. (Konstantinos Karanasos via asuresh)\n",
          "commitDate": "31/07/16 11:48 AM",
          "commitName": "e5766b1dbee02ae0ef89618e172f3fb227af19e8",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "30/07/16 7:29 PM",
          "commitNameOld": "3d191cc15244e1e29f837b34a9bd1d029e003064",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 0.68,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,83 +1,85 @@\n-  public DistSchedAllocateResponse allocateForDistributedScheduling(\n-      DistSchedAllocateRequest request) throws YarnException, IOException {\n+  public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n+      DistributedSchedulingAllocateRequest request)\n+      throws YarnException, IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Forwarding allocate request to the\" +\n           \"Distributed Scheduler Service on YARN RM\");\n     }\n     // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n-    PartitionedResourceRequests partitionedAsks \u003d partitionAskList(\n-        request.getAllocateRequest().getAskList());\n+    PartitionedResourceRequests partitionedAsks \u003d\n+        partitionAskList(request.getAllocateRequest().getAskList());\n \n     List\u003cContainerId\u003e releasedContainers \u003d\n         request.getAllocateRequest().getReleaseList();\n     int numReleasedContainers \u003d releasedContainers.size();\n     if (numReleasedContainers \u003e 0) {\n       LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n           + numReleasedContainers);\n       containersAllocated.removeAll(releasedContainers);\n     }\n \n     // Also, update black list\n     ResourceBlacklistRequest rbr \u003d\n         request.getAllocateRequest().getResourceBlacklistRequest();\n     if (rbr !\u003d null) {\n       blacklist.removeAll(rbr.getBlacklistRemovals());\n       blacklist.addAll(rbr.getBlacklistAdditions());\n     }\n \n     // Add OPPORTUNISTIC reqs to the outstanding reqs\n     addToOutstandingReqs(partitionedAsks.getOpportunistic());\n \n     List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n     for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n       // Allocated containers :\n       //  Key \u003d Requested Capability,\n       //  Value \u003d List of Containers of given Cap (The actual container size\n       //          might be different than what is requested.. which is why\n       //          we need the requested capability (key) to match against\n       //          the outstanding reqs)\n       Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n           containerAllocator.allocate(this.appParams, containerIdCounter,\n               outstandingOpReqs.get(priority).values(), blacklist,\n               applicationAttemptId, nodeList, appSubmitter);\n       for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n         matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n         allocatedContainers.addAll(e.getValue());\n       }\n     }\n+\n     request.setAllocatedContainers(allocatedContainers);\n \n     // Send all the GUARANTEED Reqs to RM\n     request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n-    DistSchedAllocateResponse dsResp \u003d\n+    DistributedSchedulingAllocateResponse dsResp \u003d\n         getNextInterceptor().allocateForDistributedScheduling(request);\n \n     // Update host to nodeId mapping\n     setNodeList(dsResp.getNodesForScheduling());\n     List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n     for (NMToken nmToken : nmTokens) {\n       nodeTokens.put(nmToken.getNodeId(), nmToken);\n     }\n \n     List\u003cContainerStatus\u003e completedContainers \u003d\n         dsResp.getAllocateResponse().getCompletedContainersStatuses();\n \n     // Only account for opportunistic containers\n     for (ContainerStatus cs : completedContainers) {\n       if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n         containersAllocated.remove(cs.getContainerId());\n       }\n     }\n \n     // Check if we have NM tokens for all the allocated containers. If not\n     // generate one and update the response.\n     updateResponseWithNMTokens(\n         dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\n           \"Number of opportunistic containers currently allocated by\" +\n               \"application: \" + containersAllocated.size());\n     }\n     return dsResp;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n      DistributedSchedulingAllocateRequest request)\n      throws YarnException, IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Forwarding allocate request to the\" +\n          \"Distributed Scheduler Service on YARN RM\");\n    }\n    // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n    PartitionedResourceRequests partitionedAsks \u003d\n        partitionAskList(request.getAllocateRequest().getAskList());\n\n    List\u003cContainerId\u003e releasedContainers \u003d\n        request.getAllocateRequest().getReleaseList();\n    int numReleasedContainers \u003d releasedContainers.size();\n    if (numReleasedContainers \u003e 0) {\n      LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n          + numReleasedContainers);\n      containersAllocated.removeAll(releasedContainers);\n    }\n\n    // Also, update black list\n    ResourceBlacklistRequest rbr \u003d\n        request.getAllocateRequest().getResourceBlacklistRequest();\n    if (rbr !\u003d null) {\n      blacklist.removeAll(rbr.getBlacklistRemovals());\n      blacklist.addAll(rbr.getBlacklistAdditions());\n    }\n\n    // Add OPPORTUNISTIC reqs to the outstanding reqs\n    addToOutstandingReqs(partitionedAsks.getOpportunistic());\n\n    List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n    for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n      // Allocated containers :\n      //  Key \u003d Requested Capability,\n      //  Value \u003d List of Containers of given Cap (The actual container size\n      //          might be different than what is requested.. which is why\n      //          we need the requested capability (key) to match against\n      //          the outstanding reqs)\n      Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n          containerAllocator.allocate(this.appParams, containerIdCounter,\n              outstandingOpReqs.get(priority).values(), blacklist,\n              applicationAttemptId, nodeList, appSubmitter);\n      for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n        matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n        allocatedContainers.addAll(e.getValue());\n      }\n    }\n\n    request.setAllocatedContainers(allocatedContainers);\n\n    // Send all the GUARANTEED Reqs to RM\n    request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n    DistributedSchedulingAllocateResponse dsResp \u003d\n        getNextInterceptor().allocateForDistributedScheduling(request);\n\n    // Update host to nodeId mapping\n    setNodeList(dsResp.getNodesForScheduling());\n    List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n    for (NMToken nmToken : nmTokens) {\n      nodeTokens.put(nmToken.getNodeId(), nmToken);\n    }\n\n    List\u003cContainerStatus\u003e completedContainers \u003d\n        dsResp.getAllocateResponse().getCompletedContainersStatuses();\n\n    // Only account for opportunistic containers\n    for (ContainerStatus cs : completedContainers) {\n      if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n        containersAllocated.remove(cs.getContainerId());\n      }\n    }\n\n    // Check if we have NM tokens for all the allocated containers. If not\n    // generate one and update the response.\n    updateResponseWithNMTokens(\n        dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\n          \"Number of opportunistic containers currently allocated by\" +\n              \"application: \" + containersAllocated.size());\n    }\n    return dsResp;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/DistributedScheduler.java",
          "extendedDetails": {
            "oldValue": "DistSchedAllocateResponse",
            "newValue": "DistributedSchedulingAllocateResponse"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-5113. Refactoring and other clean-up for distributed scheduling. (Konstantinos Karanasos via asuresh)\n",
          "commitDate": "31/07/16 11:48 AM",
          "commitName": "e5766b1dbee02ae0ef89618e172f3fb227af19e8",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "30/07/16 7:29 PM",
          "commitNameOld": "3d191cc15244e1e29f837b34a9bd1d029e003064",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 0.68,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,83 +1,85 @@\n-  public DistSchedAllocateResponse allocateForDistributedScheduling(\n-      DistSchedAllocateRequest request) throws YarnException, IOException {\n+  public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n+      DistributedSchedulingAllocateRequest request)\n+      throws YarnException, IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Forwarding allocate request to the\" +\n           \"Distributed Scheduler Service on YARN RM\");\n     }\n     // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n-    PartitionedResourceRequests partitionedAsks \u003d partitionAskList(\n-        request.getAllocateRequest().getAskList());\n+    PartitionedResourceRequests partitionedAsks \u003d\n+        partitionAskList(request.getAllocateRequest().getAskList());\n \n     List\u003cContainerId\u003e releasedContainers \u003d\n         request.getAllocateRequest().getReleaseList();\n     int numReleasedContainers \u003d releasedContainers.size();\n     if (numReleasedContainers \u003e 0) {\n       LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n           + numReleasedContainers);\n       containersAllocated.removeAll(releasedContainers);\n     }\n \n     // Also, update black list\n     ResourceBlacklistRequest rbr \u003d\n         request.getAllocateRequest().getResourceBlacklistRequest();\n     if (rbr !\u003d null) {\n       blacklist.removeAll(rbr.getBlacklistRemovals());\n       blacklist.addAll(rbr.getBlacklistAdditions());\n     }\n \n     // Add OPPORTUNISTIC reqs to the outstanding reqs\n     addToOutstandingReqs(partitionedAsks.getOpportunistic());\n \n     List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n     for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n       // Allocated containers :\n       //  Key \u003d Requested Capability,\n       //  Value \u003d List of Containers of given Cap (The actual container size\n       //          might be different than what is requested.. which is why\n       //          we need the requested capability (key) to match against\n       //          the outstanding reqs)\n       Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n           containerAllocator.allocate(this.appParams, containerIdCounter,\n               outstandingOpReqs.get(priority).values(), blacklist,\n               applicationAttemptId, nodeList, appSubmitter);\n       for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n         matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n         allocatedContainers.addAll(e.getValue());\n       }\n     }\n+\n     request.setAllocatedContainers(allocatedContainers);\n \n     // Send all the GUARANTEED Reqs to RM\n     request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n-    DistSchedAllocateResponse dsResp \u003d\n+    DistributedSchedulingAllocateResponse dsResp \u003d\n         getNextInterceptor().allocateForDistributedScheduling(request);\n \n     // Update host to nodeId mapping\n     setNodeList(dsResp.getNodesForScheduling());\n     List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n     for (NMToken nmToken : nmTokens) {\n       nodeTokens.put(nmToken.getNodeId(), nmToken);\n     }\n \n     List\u003cContainerStatus\u003e completedContainers \u003d\n         dsResp.getAllocateResponse().getCompletedContainersStatuses();\n \n     // Only account for opportunistic containers\n     for (ContainerStatus cs : completedContainers) {\n       if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n         containersAllocated.remove(cs.getContainerId());\n       }\n     }\n \n     // Check if we have NM tokens for all the allocated containers. If not\n     // generate one and update the response.\n     updateResponseWithNMTokens(\n         dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\n           \"Number of opportunistic containers currently allocated by\" +\n               \"application: \" + containersAllocated.size());\n     }\n     return dsResp;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n      DistributedSchedulingAllocateRequest request)\n      throws YarnException, IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Forwarding allocate request to the\" +\n          \"Distributed Scheduler Service on YARN RM\");\n    }\n    // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n    PartitionedResourceRequests partitionedAsks \u003d\n        partitionAskList(request.getAllocateRequest().getAskList());\n\n    List\u003cContainerId\u003e releasedContainers \u003d\n        request.getAllocateRequest().getReleaseList();\n    int numReleasedContainers \u003d releasedContainers.size();\n    if (numReleasedContainers \u003e 0) {\n      LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n          + numReleasedContainers);\n      containersAllocated.removeAll(releasedContainers);\n    }\n\n    // Also, update black list\n    ResourceBlacklistRequest rbr \u003d\n        request.getAllocateRequest().getResourceBlacklistRequest();\n    if (rbr !\u003d null) {\n      blacklist.removeAll(rbr.getBlacklistRemovals());\n      blacklist.addAll(rbr.getBlacklistAdditions());\n    }\n\n    // Add OPPORTUNISTIC reqs to the outstanding reqs\n    addToOutstandingReqs(partitionedAsks.getOpportunistic());\n\n    List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n    for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n      // Allocated containers :\n      //  Key \u003d Requested Capability,\n      //  Value \u003d List of Containers of given Cap (The actual container size\n      //          might be different than what is requested.. which is why\n      //          we need the requested capability (key) to match against\n      //          the outstanding reqs)\n      Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n          containerAllocator.allocate(this.appParams, containerIdCounter,\n              outstandingOpReqs.get(priority).values(), blacklist,\n              applicationAttemptId, nodeList, appSubmitter);\n      for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n        matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n        allocatedContainers.addAll(e.getValue());\n      }\n    }\n\n    request.setAllocatedContainers(allocatedContainers);\n\n    // Send all the GUARANTEED Reqs to RM\n    request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n    DistributedSchedulingAllocateResponse dsResp \u003d\n        getNextInterceptor().allocateForDistributedScheduling(request);\n\n    // Update host to nodeId mapping\n    setNodeList(dsResp.getNodesForScheduling());\n    List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n    for (NMToken nmToken : nmTokens) {\n      nodeTokens.put(nmToken.getNodeId(), nmToken);\n    }\n\n    List\u003cContainerStatus\u003e completedContainers \u003d\n        dsResp.getAllocateResponse().getCompletedContainersStatuses();\n\n    // Only account for opportunistic containers\n    for (ContainerStatus cs : completedContainers) {\n      if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n        containersAllocated.remove(cs.getContainerId());\n      }\n    }\n\n    // Check if we have NM tokens for all the allocated containers. If not\n    // generate one and update the response.\n    updateResponseWithNMTokens(\n        dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\n          \"Number of opportunistic containers currently allocated by\" +\n              \"application: \" + containersAllocated.size());\n    }\n    return dsResp;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/DistributedScheduler.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-5113. Refactoring and other clean-up for distributed scheduling. (Konstantinos Karanasos via asuresh)\n",
          "commitDate": "31/07/16 11:48 AM",
          "commitName": "e5766b1dbee02ae0ef89618e172f3fb227af19e8",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "30/07/16 7:29 PM",
          "commitNameOld": "3d191cc15244e1e29f837b34a9bd1d029e003064",
          "commitAuthorOld": "Akira Ajisaka",
          "daysBetweenCommits": 0.68,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,83 +1,85 @@\n-  public DistSchedAllocateResponse allocateForDistributedScheduling(\n-      DistSchedAllocateRequest request) throws YarnException, IOException {\n+  public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n+      DistributedSchedulingAllocateRequest request)\n+      throws YarnException, IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Forwarding allocate request to the\" +\n           \"Distributed Scheduler Service on YARN RM\");\n     }\n     // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n-    PartitionedResourceRequests partitionedAsks \u003d partitionAskList(\n-        request.getAllocateRequest().getAskList());\n+    PartitionedResourceRequests partitionedAsks \u003d\n+        partitionAskList(request.getAllocateRequest().getAskList());\n \n     List\u003cContainerId\u003e releasedContainers \u003d\n         request.getAllocateRequest().getReleaseList();\n     int numReleasedContainers \u003d releasedContainers.size();\n     if (numReleasedContainers \u003e 0) {\n       LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n           + numReleasedContainers);\n       containersAllocated.removeAll(releasedContainers);\n     }\n \n     // Also, update black list\n     ResourceBlacklistRequest rbr \u003d\n         request.getAllocateRequest().getResourceBlacklistRequest();\n     if (rbr !\u003d null) {\n       blacklist.removeAll(rbr.getBlacklistRemovals());\n       blacklist.addAll(rbr.getBlacklistAdditions());\n     }\n \n     // Add OPPORTUNISTIC reqs to the outstanding reqs\n     addToOutstandingReqs(partitionedAsks.getOpportunistic());\n \n     List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n     for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n       // Allocated containers :\n       //  Key \u003d Requested Capability,\n       //  Value \u003d List of Containers of given Cap (The actual container size\n       //          might be different than what is requested.. which is why\n       //          we need the requested capability (key) to match against\n       //          the outstanding reqs)\n       Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n           containerAllocator.allocate(this.appParams, containerIdCounter,\n               outstandingOpReqs.get(priority).values(), blacklist,\n               applicationAttemptId, nodeList, appSubmitter);\n       for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n         matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n         allocatedContainers.addAll(e.getValue());\n       }\n     }\n+\n     request.setAllocatedContainers(allocatedContainers);\n \n     // Send all the GUARANTEED Reqs to RM\n     request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n-    DistSchedAllocateResponse dsResp \u003d\n+    DistributedSchedulingAllocateResponse dsResp \u003d\n         getNextInterceptor().allocateForDistributedScheduling(request);\n \n     // Update host to nodeId mapping\n     setNodeList(dsResp.getNodesForScheduling());\n     List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n     for (NMToken nmToken : nmTokens) {\n       nodeTokens.put(nmToken.getNodeId(), nmToken);\n     }\n \n     List\u003cContainerStatus\u003e completedContainers \u003d\n         dsResp.getAllocateResponse().getCompletedContainersStatuses();\n \n     // Only account for opportunistic containers\n     for (ContainerStatus cs : completedContainers) {\n       if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n         containersAllocated.remove(cs.getContainerId());\n       }\n     }\n \n     // Check if we have NM tokens for all the allocated containers. If not\n     // generate one and update the response.\n     updateResponseWithNMTokens(\n         dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\n           \"Number of opportunistic containers currently allocated by\" +\n               \"application: \" + containersAllocated.size());\n     }\n     return dsResp;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public DistributedSchedulingAllocateResponse allocateForDistributedScheduling(\n      DistributedSchedulingAllocateRequest request)\n      throws YarnException, IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Forwarding allocate request to the\" +\n          \"Distributed Scheduler Service on YARN RM\");\n    }\n    // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n    PartitionedResourceRequests partitionedAsks \u003d\n        partitionAskList(request.getAllocateRequest().getAskList());\n\n    List\u003cContainerId\u003e releasedContainers \u003d\n        request.getAllocateRequest().getReleaseList();\n    int numReleasedContainers \u003d releasedContainers.size();\n    if (numReleasedContainers \u003e 0) {\n      LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n          + numReleasedContainers);\n      containersAllocated.removeAll(releasedContainers);\n    }\n\n    // Also, update black list\n    ResourceBlacklistRequest rbr \u003d\n        request.getAllocateRequest().getResourceBlacklistRequest();\n    if (rbr !\u003d null) {\n      blacklist.removeAll(rbr.getBlacklistRemovals());\n      blacklist.addAll(rbr.getBlacklistAdditions());\n    }\n\n    // Add OPPORTUNISTIC reqs to the outstanding reqs\n    addToOutstandingReqs(partitionedAsks.getOpportunistic());\n\n    List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n    for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n      // Allocated containers :\n      //  Key \u003d Requested Capability,\n      //  Value \u003d List of Containers of given Cap (The actual container size\n      //          might be different than what is requested.. which is why\n      //          we need the requested capability (key) to match against\n      //          the outstanding reqs)\n      Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n          containerAllocator.allocate(this.appParams, containerIdCounter,\n              outstandingOpReqs.get(priority).values(), blacklist,\n              applicationAttemptId, nodeList, appSubmitter);\n      for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n        matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n        allocatedContainers.addAll(e.getValue());\n      }\n    }\n\n    request.setAllocatedContainers(allocatedContainers);\n\n    // Send all the GUARANTEED Reqs to RM\n    request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n    DistributedSchedulingAllocateResponse dsResp \u003d\n        getNextInterceptor().allocateForDistributedScheduling(request);\n\n    // Update host to nodeId mapping\n    setNodeList(dsResp.getNodesForScheduling());\n    List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n    for (NMToken nmToken : nmTokens) {\n      nodeTokens.put(nmToken.getNodeId(), nmToken);\n    }\n\n    List\u003cContainerStatus\u003e completedContainers \u003d\n        dsResp.getAllocateResponse().getCompletedContainersStatuses();\n\n    // Only account for opportunistic containers\n    for (ContainerStatus cs : completedContainers) {\n      if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n        containersAllocated.remove(cs.getContainerId());\n      }\n    }\n\n    // Check if we have NM tokens for all the allocated containers. If not\n    // generate one and update the response.\n    updateResponseWithNMTokens(\n        dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\n          \"Number of opportunistic containers currently allocated by\" +\n              \"application: \" + containersAllocated.size());\n    }\n    return dsResp;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/DistributedScheduler.java",
          "extendedDetails": {
            "oldValue": "[request-DistSchedAllocateRequest]",
            "newValue": "[request-DistributedSchedulingAllocateRequest]"
          }
        }
      ]
    },
    "99e5dd68d0f44109c169d74824fa45a7396a5990": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-5171. Extend DistributedSchedulerProtocol to notify RM of containers allocated by the Node. (Inigo Goiri via asuresh)\n",
      "commitDate": "22/06/16 7:04 PM",
      "commitName": "99e5dd68d0f44109c169d74824fa45a7396a5990",
      "commitAuthor": "Arun Suresh",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-5171. Extend DistributedSchedulerProtocol to notify RM of containers allocated by the Node. (Inigo Goiri via asuresh)\n",
          "commitDate": "22/06/16 7:04 PM",
          "commitName": "99e5dd68d0f44109c169d74824fa45a7396a5990",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "02/06/16 9:01 AM",
          "commitNameOld": "dc26601d8fe27a4223a50601bf7522cc42e8e2f3",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 20.42,
          "commitsBetweenForRepo": 130,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,80 +1,83 @@\n-  public DistSchedAllocateResponse allocateForDistributedScheduling\n-      (AllocateRequest request) throws YarnException, IOException {\n+  public DistSchedAllocateResponse allocateForDistributedScheduling(\n+      DistSchedAllocateRequest request) throws YarnException, IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Forwarding allocate request to the\" +\n           \"Distributed Scheduler Service on YARN RM\");\n     }\n     // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n-    PartitionedResourceRequests partitionedAsks \u003d partitionAskList(request\n-        .getAskList());\n+    PartitionedResourceRequests partitionedAsks \u003d partitionAskList(\n+        request.getAllocateRequest().getAskList());\n \n-    List\u003cContainerId\u003e releasedContainers \u003d request.getReleaseList();\n+    List\u003cContainerId\u003e releasedContainers \u003d\n+        request.getAllocateRequest().getReleaseList();\n     int numReleasedContainers \u003d releasedContainers.size();\n     if (numReleasedContainers \u003e 0) {\n       LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n           + numReleasedContainers);\n       containersAllocated.removeAll(releasedContainers);\n     }\n \n     // Also, update black list\n-    ResourceBlacklistRequest rbr \u003d request.getResourceBlacklistRequest();\n+    ResourceBlacklistRequest rbr \u003d\n+        request.getAllocateRequest().getResourceBlacklistRequest();\n     if (rbr !\u003d null) {\n       blacklist.removeAll(rbr.getBlacklistRemovals());\n       blacklist.addAll(rbr.getBlacklistAdditions());\n     }\n \n     // Add OPPORTUNISTIC reqs to the outstanding reqs\n     addToOutstandingReqs(partitionedAsks.getOpportunistic());\n \n     List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n     for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n       // Allocated containers :\n       //  Key \u003d Requested Capability,\n       //  Value \u003d List of Containers of given Cap (The actual container size\n       //          might be different than what is requested.. which is why\n       //          we need the requested capability (key) to match against\n       //          the outstanding reqs)\n       Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n           containerAllocator.allocate(this.appParams, containerIdCounter,\n               outstandingOpReqs.get(priority).values(), blacklist,\n               applicationAttemptId, nodeList, appSubmitter);\n       for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n         matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n         allocatedContainers.addAll(e.getValue());\n       }\n     }\n+    request.setAllocatedContainers(allocatedContainers);\n \n     // Send all the GUARANTEED Reqs to RM\n-    request.setAskList(partitionedAsks.getGuaranteed());\n+    request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n     DistSchedAllocateResponse dsResp \u003d\n         getNextInterceptor().allocateForDistributedScheduling(request);\n \n     // Update host to nodeId mapping\n     setNodeList(dsResp.getNodesForScheduling());\n     List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n     for (NMToken nmToken : nmTokens) {\n       nodeTokens.put(nmToken.getNodeId(), nmToken);\n     }\n \n     List\u003cContainerStatus\u003e completedContainers \u003d\n         dsResp.getAllocateResponse().getCompletedContainersStatuses();\n \n     // Only account for opportunistic containers\n     for (ContainerStatus cs : completedContainers) {\n       if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n         containersAllocated.remove(cs.getContainerId());\n       }\n     }\n \n     // Check if we have NM tokens for all the allocated containers. If not\n     // generate one and update the response.\n     updateResponseWithNMTokens(\n         dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\n           \"Number of opportunistic containers currently allocated by\" +\n               \"application: \" + containersAllocated.size());\n     }\n     return dsResp;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public DistSchedAllocateResponse allocateForDistributedScheduling(\n      DistSchedAllocateRequest request) throws YarnException, IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Forwarding allocate request to the\" +\n          \"Distributed Scheduler Service on YARN RM\");\n    }\n    // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n    PartitionedResourceRequests partitionedAsks \u003d partitionAskList(\n        request.getAllocateRequest().getAskList());\n\n    List\u003cContainerId\u003e releasedContainers \u003d\n        request.getAllocateRequest().getReleaseList();\n    int numReleasedContainers \u003d releasedContainers.size();\n    if (numReleasedContainers \u003e 0) {\n      LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n          + numReleasedContainers);\n      containersAllocated.removeAll(releasedContainers);\n    }\n\n    // Also, update black list\n    ResourceBlacklistRequest rbr \u003d\n        request.getAllocateRequest().getResourceBlacklistRequest();\n    if (rbr !\u003d null) {\n      blacklist.removeAll(rbr.getBlacklistRemovals());\n      blacklist.addAll(rbr.getBlacklistAdditions());\n    }\n\n    // Add OPPORTUNISTIC reqs to the outstanding reqs\n    addToOutstandingReqs(partitionedAsks.getOpportunistic());\n\n    List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n    for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n      // Allocated containers :\n      //  Key \u003d Requested Capability,\n      //  Value \u003d List of Containers of given Cap (The actual container size\n      //          might be different than what is requested.. which is why\n      //          we need the requested capability (key) to match against\n      //          the outstanding reqs)\n      Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n          containerAllocator.allocate(this.appParams, containerIdCounter,\n              outstandingOpReqs.get(priority).values(), blacklist,\n              applicationAttemptId, nodeList, appSubmitter);\n      for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n        matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n        allocatedContainers.addAll(e.getValue());\n      }\n    }\n    request.setAllocatedContainers(allocatedContainers);\n\n    // Send all the GUARANTEED Reqs to RM\n    request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n    DistSchedAllocateResponse dsResp \u003d\n        getNextInterceptor().allocateForDistributedScheduling(request);\n\n    // Update host to nodeId mapping\n    setNodeList(dsResp.getNodesForScheduling());\n    List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n    for (NMToken nmToken : nmTokens) {\n      nodeTokens.put(nmToken.getNodeId(), nmToken);\n    }\n\n    List\u003cContainerStatus\u003e completedContainers \u003d\n        dsResp.getAllocateResponse().getCompletedContainersStatuses();\n\n    // Only account for opportunistic containers\n    for (ContainerStatus cs : completedContainers) {\n      if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n        containersAllocated.remove(cs.getContainerId());\n      }\n    }\n\n    // Check if we have NM tokens for all the allocated containers. If not\n    // generate one and update the response.\n    updateResponseWithNMTokens(\n        dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\n          \"Number of opportunistic containers currently allocated by\" +\n              \"application: \" + containersAllocated.size());\n    }\n    return dsResp;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/LocalScheduler.java",
          "extendedDetails": {
            "oldValue": "[request-AllocateRequest]",
            "newValue": "[request-DistSchedAllocateRequest]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-5171. Extend DistributedSchedulerProtocol to notify RM of containers allocated by the Node. (Inigo Goiri via asuresh)\n",
          "commitDate": "22/06/16 7:04 PM",
          "commitName": "99e5dd68d0f44109c169d74824fa45a7396a5990",
          "commitAuthor": "Arun Suresh",
          "commitDateOld": "02/06/16 9:01 AM",
          "commitNameOld": "dc26601d8fe27a4223a50601bf7522cc42e8e2f3",
          "commitAuthorOld": "Arun Suresh",
          "daysBetweenCommits": 20.42,
          "commitsBetweenForRepo": 130,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,80 +1,83 @@\n-  public DistSchedAllocateResponse allocateForDistributedScheduling\n-      (AllocateRequest request) throws YarnException, IOException {\n+  public DistSchedAllocateResponse allocateForDistributedScheduling(\n+      DistSchedAllocateRequest request) throws YarnException, IOException {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Forwarding allocate request to the\" +\n           \"Distributed Scheduler Service on YARN RM\");\n     }\n     // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n-    PartitionedResourceRequests partitionedAsks \u003d partitionAskList(request\n-        .getAskList());\n+    PartitionedResourceRequests partitionedAsks \u003d partitionAskList(\n+        request.getAllocateRequest().getAskList());\n \n-    List\u003cContainerId\u003e releasedContainers \u003d request.getReleaseList();\n+    List\u003cContainerId\u003e releasedContainers \u003d\n+        request.getAllocateRequest().getReleaseList();\n     int numReleasedContainers \u003d releasedContainers.size();\n     if (numReleasedContainers \u003e 0) {\n       LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n           + numReleasedContainers);\n       containersAllocated.removeAll(releasedContainers);\n     }\n \n     // Also, update black list\n-    ResourceBlacklistRequest rbr \u003d request.getResourceBlacklistRequest();\n+    ResourceBlacklistRequest rbr \u003d\n+        request.getAllocateRequest().getResourceBlacklistRequest();\n     if (rbr !\u003d null) {\n       blacklist.removeAll(rbr.getBlacklistRemovals());\n       blacklist.addAll(rbr.getBlacklistAdditions());\n     }\n \n     // Add OPPORTUNISTIC reqs to the outstanding reqs\n     addToOutstandingReqs(partitionedAsks.getOpportunistic());\n \n     List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n     for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n       // Allocated containers :\n       //  Key \u003d Requested Capability,\n       //  Value \u003d List of Containers of given Cap (The actual container size\n       //          might be different than what is requested.. which is why\n       //          we need the requested capability (key) to match against\n       //          the outstanding reqs)\n       Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n           containerAllocator.allocate(this.appParams, containerIdCounter,\n               outstandingOpReqs.get(priority).values(), blacklist,\n               applicationAttemptId, nodeList, appSubmitter);\n       for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n         matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n         allocatedContainers.addAll(e.getValue());\n       }\n     }\n+    request.setAllocatedContainers(allocatedContainers);\n \n     // Send all the GUARANTEED Reqs to RM\n-    request.setAskList(partitionedAsks.getGuaranteed());\n+    request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n     DistSchedAllocateResponse dsResp \u003d\n         getNextInterceptor().allocateForDistributedScheduling(request);\n \n     // Update host to nodeId mapping\n     setNodeList(dsResp.getNodesForScheduling());\n     List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n     for (NMToken nmToken : nmTokens) {\n       nodeTokens.put(nmToken.getNodeId(), nmToken);\n     }\n \n     List\u003cContainerStatus\u003e completedContainers \u003d\n         dsResp.getAllocateResponse().getCompletedContainersStatuses();\n \n     // Only account for opportunistic containers\n     for (ContainerStatus cs : completedContainers) {\n       if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n         containersAllocated.remove(cs.getContainerId());\n       }\n     }\n \n     // Check if we have NM tokens for all the allocated containers. If not\n     // generate one and update the response.\n     updateResponseWithNMTokens(\n         dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\n           \"Number of opportunistic containers currently allocated by\" +\n               \"application: \" + containersAllocated.size());\n     }\n     return dsResp;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public DistSchedAllocateResponse allocateForDistributedScheduling(\n      DistSchedAllocateRequest request) throws YarnException, IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Forwarding allocate request to the\" +\n          \"Distributed Scheduler Service on YARN RM\");\n    }\n    // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n    PartitionedResourceRequests partitionedAsks \u003d partitionAskList(\n        request.getAllocateRequest().getAskList());\n\n    List\u003cContainerId\u003e releasedContainers \u003d\n        request.getAllocateRequest().getReleaseList();\n    int numReleasedContainers \u003d releasedContainers.size();\n    if (numReleasedContainers \u003e 0) {\n      LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n          + numReleasedContainers);\n      containersAllocated.removeAll(releasedContainers);\n    }\n\n    // Also, update black list\n    ResourceBlacklistRequest rbr \u003d\n        request.getAllocateRequest().getResourceBlacklistRequest();\n    if (rbr !\u003d null) {\n      blacklist.removeAll(rbr.getBlacklistRemovals());\n      blacklist.addAll(rbr.getBlacklistAdditions());\n    }\n\n    // Add OPPORTUNISTIC reqs to the outstanding reqs\n    addToOutstandingReqs(partitionedAsks.getOpportunistic());\n\n    List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n    for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n      // Allocated containers :\n      //  Key \u003d Requested Capability,\n      //  Value \u003d List of Containers of given Cap (The actual container size\n      //          might be different than what is requested.. which is why\n      //          we need the requested capability (key) to match against\n      //          the outstanding reqs)\n      Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n          containerAllocator.allocate(this.appParams, containerIdCounter,\n              outstandingOpReqs.get(priority).values(), blacklist,\n              applicationAttemptId, nodeList, appSubmitter);\n      for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n        matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n        allocatedContainers.addAll(e.getValue());\n      }\n    }\n    request.setAllocatedContainers(allocatedContainers);\n\n    // Send all the GUARANTEED Reqs to RM\n    request.getAllocateRequest().setAskList(partitionedAsks.getGuaranteed());\n    DistSchedAllocateResponse dsResp \u003d\n        getNextInterceptor().allocateForDistributedScheduling(request);\n\n    // Update host to nodeId mapping\n    setNodeList(dsResp.getNodesForScheduling());\n    List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n    for (NMToken nmToken : nmTokens) {\n      nodeTokens.put(nmToken.getNodeId(), nmToken);\n    }\n\n    List\u003cContainerStatus\u003e completedContainers \u003d\n        dsResp.getAllocateResponse().getCompletedContainersStatuses();\n\n    // Only account for opportunistic containers\n    for (ContainerStatus cs : completedContainers) {\n      if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n        containersAllocated.remove(cs.getContainerId());\n      }\n    }\n\n    // Check if we have NM tokens for all the allocated containers. If not\n    // generate one and update the response.\n    updateResponseWithNMTokens(\n        dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\n          \"Number of opportunistic containers currently allocated by\" +\n              \"application: \" + containersAllocated.size());\n    }\n    return dsResp;\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/LocalScheduler.java",
          "extendedDetails": {}
        }
      ]
    },
    "1597630681c784a3d59f5605b87e96197b8139d7": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5110. Fix OpportunisticContainerAllocator to insert complete HostAddress in issued ContainerTokenIds. (Konstantinos Karanasos via asuresh)\n",
      "commitDate": "18/05/16 6:46 PM",
      "commitName": "1597630681c784a3d59f5605b87e96197b8139d7",
      "commitAuthor": "Arun Suresh",
      "commitDateOld": "24/04/16 10:38 PM",
      "commitNameOld": "c282a08f3892e2e8ceb58e1e9a411062fbd1fb2b",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 23.84,
      "commitsBetweenForRepo": 167,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,80 @@\n   public DistSchedAllocateResponse allocateForDistributedScheduling\n       (AllocateRequest request) throws YarnException, IOException {\n-    LOG.info(\"Forwarding allocate request to the\" +\n-        \"Distributed Scheduler Service on YARN RM\");\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Forwarding allocate request to the\" +\n+          \"Distributed Scheduler Service on YARN RM\");\n+    }\n     // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n     PartitionedResourceRequests partitionedAsks \u003d partitionAskList(request\n         .getAskList());\n \n     List\u003cContainerId\u003e releasedContainers \u003d request.getReleaseList();\n     int numReleasedContainers \u003d releasedContainers.size();\n     if (numReleasedContainers \u003e 0) {\n       LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n           + numReleasedContainers);\n       containersAllocated.removeAll(releasedContainers);\n     }\n \n     // Also, update black list\n     ResourceBlacklistRequest rbr \u003d request.getResourceBlacklistRequest();\n     if (rbr !\u003d null) {\n       blacklist.removeAll(rbr.getBlacklistRemovals());\n       blacklist.addAll(rbr.getBlacklistAdditions());\n     }\n \n     // Add OPPORTUNISTIC reqs to the outstanding reqs\n     addToOutstandingReqs(partitionedAsks.getOpportunistic());\n \n     List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n     for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n       // Allocated containers :\n       //  Key \u003d Requested Capability,\n       //  Value \u003d List of Containers of given Cap (The actual container size\n       //          might be different than what is requested.. which is why\n       //          we need the requested capability (key) to match against\n       //          the outstanding reqs)\n       Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n           containerAllocator.allocate(this.appParams, containerIdCounter,\n               outstandingOpReqs.get(priority).values(), blacklist,\n               applicationAttemptId, nodeList, appSubmitter);\n       for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n         matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n         allocatedContainers.addAll(e.getValue());\n       }\n     }\n \n     // Send all the GUARANTEED Reqs to RM\n     request.setAskList(partitionedAsks.getGuaranteed());\n     DistSchedAllocateResponse dsResp \u003d\n         getNextInterceptor().allocateForDistributedScheduling(request);\n \n     // Update host to nodeId mapping\n     setNodeList(dsResp.getNodesForScheduling());\n     List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n     for (NMToken nmToken : nmTokens) {\n       nodeTokens.put(nmToken.getNodeId(), nmToken);\n     }\n \n     List\u003cContainerStatus\u003e completedContainers \u003d\n         dsResp.getAllocateResponse().getCompletedContainersStatuses();\n \n     // Only account for opportunistic containers\n     for (ContainerStatus cs : completedContainers) {\n       if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n         containersAllocated.remove(cs.getContainerId());\n       }\n     }\n \n     // Check if we have NM tokens for all the allocated containers. If not\n     // generate one and update the response.\n     updateResponseWithNMTokens(\n         dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n \n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\n           \"Number of opportunistic containers currently allocated by\" +\n               \"application: \" + containersAllocated.size());\n     }\n     return dsResp;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DistSchedAllocateResponse allocateForDistributedScheduling\n      (AllocateRequest request) throws YarnException, IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Forwarding allocate request to the\" +\n          \"Distributed Scheduler Service on YARN RM\");\n    }\n    // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n    PartitionedResourceRequests partitionedAsks \u003d partitionAskList(request\n        .getAskList());\n\n    List\u003cContainerId\u003e releasedContainers \u003d request.getReleaseList();\n    int numReleasedContainers \u003d releasedContainers.size();\n    if (numReleasedContainers \u003e 0) {\n      LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n          + numReleasedContainers);\n      containersAllocated.removeAll(releasedContainers);\n    }\n\n    // Also, update black list\n    ResourceBlacklistRequest rbr \u003d request.getResourceBlacklistRequest();\n    if (rbr !\u003d null) {\n      blacklist.removeAll(rbr.getBlacklistRemovals());\n      blacklist.addAll(rbr.getBlacklistAdditions());\n    }\n\n    // Add OPPORTUNISTIC reqs to the outstanding reqs\n    addToOutstandingReqs(partitionedAsks.getOpportunistic());\n\n    List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n    for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n      // Allocated containers :\n      //  Key \u003d Requested Capability,\n      //  Value \u003d List of Containers of given Cap (The actual container size\n      //          might be different than what is requested.. which is why\n      //          we need the requested capability (key) to match against\n      //          the outstanding reqs)\n      Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n          containerAllocator.allocate(this.appParams, containerIdCounter,\n              outstandingOpReqs.get(priority).values(), blacklist,\n              applicationAttemptId, nodeList, appSubmitter);\n      for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n        matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n        allocatedContainers.addAll(e.getValue());\n      }\n    }\n\n    // Send all the GUARANTEED Reqs to RM\n    request.setAskList(partitionedAsks.getGuaranteed());\n    DistSchedAllocateResponse dsResp \u003d\n        getNextInterceptor().allocateForDistributedScheduling(request);\n\n    // Update host to nodeId mapping\n    setNodeList(dsResp.getNodesForScheduling());\n    List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n    for (NMToken nmToken : nmTokens) {\n      nodeTokens.put(nmToken.getNodeId(), nmToken);\n    }\n\n    List\u003cContainerStatus\u003e completedContainers \u003d\n        dsResp.getAllocateResponse().getCompletedContainersStatuses();\n\n    // Only account for opportunistic containers\n    for (ContainerStatus cs : completedContainers) {\n      if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n        containersAllocated.remove(cs.getContainerId());\n      }\n    }\n\n    // Check if we have NM tokens for all the allocated containers. If not\n    // generate one and update the response.\n    updateResponseWithNMTokens(\n        dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\n          \"Number of opportunistic containers currently allocated by\" +\n              \"application: \" + containersAllocated.size());\n    }\n    return dsResp;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/LocalScheduler.java",
      "extendedDetails": {}
    },
    "c282a08f3892e2e8ceb58e1e9a411062fbd1fb2b": {
      "type": "Yintroduced",
      "commitMessage": "YARN-2885. Create AMRMProxy request interceptor and ContainerAllocator to distribute OPPORTUNISTIC containers to appropriate Nodes (asuresh)\n\n(cherry picked from commit 2bf025278a318b0452fdc9ece4427b4c42124e39)\n",
      "commitDate": "24/04/16 10:38 PM",
      "commitName": "c282a08f3892e2e8ceb58e1e9a411062fbd1fb2b",
      "commitAuthor": "Arun Suresh",
      "diff": "@@ -0,0 +1,78 @@\n+  public DistSchedAllocateResponse allocateForDistributedScheduling\n+      (AllocateRequest request) throws YarnException, IOException {\n+    LOG.info(\"Forwarding allocate request to the\" +\n+        \"Distributed Scheduler Service on YARN RM\");\n+    // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n+    PartitionedResourceRequests partitionedAsks \u003d partitionAskList(request\n+        .getAskList());\n+\n+    List\u003cContainerId\u003e releasedContainers \u003d request.getReleaseList();\n+    int numReleasedContainers \u003d releasedContainers.size();\n+    if (numReleasedContainers \u003e 0) {\n+      LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n+          + numReleasedContainers);\n+      containersAllocated.removeAll(releasedContainers);\n+    }\n+\n+    // Also, update black list\n+    ResourceBlacklistRequest rbr \u003d request.getResourceBlacklistRequest();\n+    if (rbr !\u003d null) {\n+      blacklist.removeAll(rbr.getBlacklistRemovals());\n+      blacklist.addAll(rbr.getBlacklistAdditions());\n+    }\n+\n+    // Add OPPORTUNISTIC reqs to the outstanding reqs\n+    addToOutstandingReqs(partitionedAsks.getOpportunistic());\n+\n+    List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n+    for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n+      // Allocated containers :\n+      //  Key \u003d Requested Capability,\n+      //  Value \u003d List of Containers of given Cap (The actual container size\n+      //          might be different than what is requested.. which is why\n+      //          we need the requested capability (key) to match against\n+      //          the outstanding reqs)\n+      Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n+          containerAllocator.allocate(this.appParams, containerIdCounter,\n+              outstandingOpReqs.get(priority).values(), blacklist,\n+              applicationAttemptId, nodeList, appSubmitter);\n+      for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n+        matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n+        allocatedContainers.addAll(e.getValue());\n+      }\n+    }\n+\n+    // Send all the GUARANTEED Reqs to RM\n+    request.setAskList(partitionedAsks.getGuaranteed());\n+    DistSchedAllocateResponse dsResp \u003d\n+        getNextInterceptor().allocateForDistributedScheduling(request);\n+\n+    // Update host to nodeId mapping\n+    setNodeList(dsResp.getNodesForScheduling());\n+    List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n+    for (NMToken nmToken : nmTokens) {\n+      nodeTokens.put(nmToken.getNodeId(), nmToken);\n+    }\n+\n+    List\u003cContainerStatus\u003e completedContainers \u003d\n+        dsResp.getAllocateResponse().getCompletedContainersStatuses();\n+\n+    // Only account for opportunistic containers\n+    for (ContainerStatus cs : completedContainers) {\n+      if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n+        containersAllocated.remove(cs.getContainerId());\n+      }\n+    }\n+\n+    // Check if we have NM tokens for all the allocated containers. If not\n+    // generate one and update the response.\n+    updateResponseWithNMTokens(\n+        dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n+\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\n+          \"Number of opportunistic containers currently allocated by\" +\n+              \"application: \" + containersAllocated.size());\n+    }\n+    return dsResp;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public DistSchedAllocateResponse allocateForDistributedScheduling\n      (AllocateRequest request) throws YarnException, IOException {\n    LOG.info(\"Forwarding allocate request to the\" +\n        \"Distributed Scheduler Service on YARN RM\");\n    // Partition requests into GUARANTEED and OPPORTUNISTIC reqs\n    PartitionedResourceRequests partitionedAsks \u003d partitionAskList(request\n        .getAskList());\n\n    List\u003cContainerId\u003e releasedContainers \u003d request.getReleaseList();\n    int numReleasedContainers \u003d releasedContainers.size();\n    if (numReleasedContainers \u003e 0) {\n      LOG.info(\"AttemptID: \" + applicationAttemptId + \" released: \"\n          + numReleasedContainers);\n      containersAllocated.removeAll(releasedContainers);\n    }\n\n    // Also, update black list\n    ResourceBlacklistRequest rbr \u003d request.getResourceBlacklistRequest();\n    if (rbr !\u003d null) {\n      blacklist.removeAll(rbr.getBlacklistRemovals());\n      blacklist.addAll(rbr.getBlacklistAdditions());\n    }\n\n    // Add OPPORTUNISTIC reqs to the outstanding reqs\n    addToOutstandingReqs(partitionedAsks.getOpportunistic());\n\n    List\u003cContainer\u003e allocatedContainers \u003d new ArrayList\u003c\u003e();\n    for (Priority priority : outstandingOpReqs.descendingKeySet()) {\n      // Allocated containers :\n      //  Key \u003d Requested Capability,\n      //  Value \u003d List of Containers of given Cap (The actual container size\n      //          might be different than what is requested.. which is why\n      //          we need the requested capability (key) to match against\n      //          the outstanding reqs)\n      Map\u003cResource, List\u003cContainer\u003e\u003e allocated \u003d\n          containerAllocator.allocate(this.appParams, containerIdCounter,\n              outstandingOpReqs.get(priority).values(), blacklist,\n              applicationAttemptId, nodeList, appSubmitter);\n      for (Map.Entry\u003cResource, List\u003cContainer\u003e\u003e e : allocated.entrySet()) {\n        matchAllocationToOutstandingRequest(e.getKey(), e.getValue());\n        allocatedContainers.addAll(e.getValue());\n      }\n    }\n\n    // Send all the GUARANTEED Reqs to RM\n    request.setAskList(partitionedAsks.getGuaranteed());\n    DistSchedAllocateResponse dsResp \u003d\n        getNextInterceptor().allocateForDistributedScheduling(request);\n\n    // Update host to nodeId mapping\n    setNodeList(dsResp.getNodesForScheduling());\n    List\u003cNMToken\u003e nmTokens \u003d dsResp.getAllocateResponse().getNMTokens();\n    for (NMToken nmToken : nmTokens) {\n      nodeTokens.put(nmToken.getNodeId(), nmToken);\n    }\n\n    List\u003cContainerStatus\u003e completedContainers \u003d\n        dsResp.getAllocateResponse().getCompletedContainersStatuses();\n\n    // Only account for opportunistic containers\n    for (ContainerStatus cs : completedContainers) {\n      if (cs.getExecutionType() \u003d\u003d ExecutionType.OPPORTUNISTIC) {\n        containersAllocated.remove(cs.getContainerId());\n      }\n    }\n\n    // Check if we have NM tokens for all the allocated containers. If not\n    // generate one and update the response.\n    updateResponseWithNMTokens(\n        dsResp.getAllocateResponse(), nmTokens, allocatedContainers);\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\n          \"Number of opportunistic containers currently allocated by\" +\n              \"application: \" + containersAllocated.size());\n    }\n    return dsResp;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/scheduler/LocalScheduler.java"
    }
  }
}