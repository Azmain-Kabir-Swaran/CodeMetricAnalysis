{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "XORRawDecoder.java",
  "functionName": "doDecode",
  "functionId": "doDecode___decodingState-ByteBufferDecodingState",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java",
  "functionStartLine": 40,
  "functionEndLine": 62,
  "numCommitsSeen": 16,
  "timeTaken": 2592,
  "changeHistory": [
    "77202fa1035a54496d11d07472fbc399148ff630",
    "5eca6dece67620f990f3306b6caaf09f317b38f6",
    "4ad484883f773c702a1874fc12816ef1a4a54136",
    "343c0e76fcd95ac739ca7cd6742c9d617e19fc37",
    "09c3a375bafa481e88d1317388a73c46950164c9",
    "17f7cdc04764524c091bb0e9eb43399f88ac0e6b",
    "b29f3bde4d2fd2f2c4abd6d7b5f97a81bb50efb2"
  ],
  "changeHistoryShort": {
    "77202fa1035a54496d11d07472fbc399148ff630": "Ymultichange(Yparameterchange,Ybodychange)",
    "5eca6dece67620f990f3306b6caaf09f317b38f6": "Ybodychange",
    "4ad484883f773c702a1874fc12816ef1a4a54136": "Ybodychange",
    "343c0e76fcd95ac739ca7cd6742c9d617e19fc37": "Ybodychange",
    "09c3a375bafa481e88d1317388a73c46950164c9": "Ybodychange",
    "17f7cdc04764524c091bb0e9eb43399f88ac0e6b": "Yfilerename",
    "b29f3bde4d2fd2f2c4abd6d7b5f97a81bb50efb2": "Yintroduced"
  },
  "changeHistoryDetails": {
    "77202fa1035a54496d11d07472fbc399148ff630": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HADOOP-13010. Refactor raw erasure coders. Contributed by Kai Zheng\n",
      "commitDate": "26/05/16 10:23 PM",
      "commitName": "77202fa1035a54496d11d07472fbc399148ff630",
      "commitAuthor": "Kai Zheng",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HADOOP-13010. Refactor raw erasure coders. Contributed by Kai Zheng\n",
          "commitDate": "26/05/16 10:23 PM",
          "commitName": "77202fa1035a54496d11d07472fbc399148ff630",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "29/10/15 12:04 AM",
          "commitNameOld": "5eca6dece67620f990f3306b6caaf09f317b38f6",
          "commitAuthorOld": "Walter Su",
          "daysBetweenCommits": 210.93,
          "commitsBetweenForRepo": 1340,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,23 @@\n-  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n-                          ByteBuffer[] outputs) {\n-    ByteBuffer output \u003d outputs[0];\n+  protected void doDecode(ByteBufferDecodingState decodingState) {\n+    CoderUtil.resetOutputBuffers(decodingState.outputs,\n+        decodingState.decodeLength);\n+    ByteBuffer output \u003d decodingState.outputs[0];\n \n-    int erasedIdx \u003d erasedIndexes[0];\n+    int erasedIdx \u003d decodingState.erasedIndexes[0];\n \n     // Process the inputs.\n     int iIdx, oIdx;\n-    for (int i \u003d 0; i \u003c inputs.length; i++) {\n+    for (int i \u003d 0; i \u003c decodingState.inputs.length; i++) {\n       // Skip the erased location.\n       if (i \u003d\u003d erasedIdx) {\n         continue;\n       }\n \n-      for (iIdx \u003d inputs[i].position(), oIdx \u003d output.position();\n-           iIdx \u003c inputs[i].limit();\n+      for (iIdx \u003d decodingState.inputs[i].position(), oIdx \u003d output.position();\n+           iIdx \u003c decodingState.inputs[i].limit();\n            iIdx++, oIdx++) {\n-        output.put(oIdx, (byte) (output.get(oIdx) ^ inputs[i].get(iIdx)));\n+        output.put(oIdx, (byte) (output.get(oIdx) ^\n+            decodingState.inputs[i].get(iIdx)));\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void doDecode(ByteBufferDecodingState decodingState) {\n    CoderUtil.resetOutputBuffers(decodingState.outputs,\n        decodingState.decodeLength);\n    ByteBuffer output \u003d decodingState.outputs[0];\n\n    int erasedIdx \u003d decodingState.erasedIndexes[0];\n\n    // Process the inputs.\n    int iIdx, oIdx;\n    for (int i \u003d 0; i \u003c decodingState.inputs.length; i++) {\n      // Skip the erased location.\n      if (i \u003d\u003d erasedIdx) {\n        continue;\n      }\n\n      for (iIdx \u003d decodingState.inputs[i].position(), oIdx \u003d output.position();\n           iIdx \u003c decodingState.inputs[i].limit();\n           iIdx++, oIdx++) {\n        output.put(oIdx, (byte) (output.get(oIdx) ^\n            decodingState.inputs[i].get(iIdx)));\n      }\n    }\n  }",
          "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java",
          "extendedDetails": {
            "oldValue": "[inputs-ByteBuffer[], erasedIndexes-int[], outputs-ByteBuffer[]]",
            "newValue": "[decodingState-ByteBufferDecodingState]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-13010. Refactor raw erasure coders. Contributed by Kai Zheng\n",
          "commitDate": "26/05/16 10:23 PM",
          "commitName": "77202fa1035a54496d11d07472fbc399148ff630",
          "commitAuthor": "Kai Zheng",
          "commitDateOld": "29/10/15 12:04 AM",
          "commitNameOld": "5eca6dece67620f990f3306b6caaf09f317b38f6",
          "commitAuthorOld": "Walter Su",
          "daysBetweenCommits": 210.93,
          "commitsBetweenForRepo": 1340,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,21 +1,23 @@\n-  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n-                          ByteBuffer[] outputs) {\n-    ByteBuffer output \u003d outputs[0];\n+  protected void doDecode(ByteBufferDecodingState decodingState) {\n+    CoderUtil.resetOutputBuffers(decodingState.outputs,\n+        decodingState.decodeLength);\n+    ByteBuffer output \u003d decodingState.outputs[0];\n \n-    int erasedIdx \u003d erasedIndexes[0];\n+    int erasedIdx \u003d decodingState.erasedIndexes[0];\n \n     // Process the inputs.\n     int iIdx, oIdx;\n-    for (int i \u003d 0; i \u003c inputs.length; i++) {\n+    for (int i \u003d 0; i \u003c decodingState.inputs.length; i++) {\n       // Skip the erased location.\n       if (i \u003d\u003d erasedIdx) {\n         continue;\n       }\n \n-      for (iIdx \u003d inputs[i].position(), oIdx \u003d output.position();\n-           iIdx \u003c inputs[i].limit();\n+      for (iIdx \u003d decodingState.inputs[i].position(), oIdx \u003d output.position();\n+           iIdx \u003c decodingState.inputs[i].limit();\n            iIdx++, oIdx++) {\n-        output.put(oIdx, (byte) (output.get(oIdx) ^ inputs[i].get(iIdx)));\n+        output.put(oIdx, (byte) (output.get(oIdx) ^\n+            decodingState.inputs[i].get(iIdx)));\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void doDecode(ByteBufferDecodingState decodingState) {\n    CoderUtil.resetOutputBuffers(decodingState.outputs,\n        decodingState.decodeLength);\n    ByteBuffer output \u003d decodingState.outputs[0];\n\n    int erasedIdx \u003d decodingState.erasedIndexes[0];\n\n    // Process the inputs.\n    int iIdx, oIdx;\n    for (int i \u003d 0; i \u003c decodingState.inputs.length; i++) {\n      // Skip the erased location.\n      if (i \u003d\u003d erasedIdx) {\n        continue;\n      }\n\n      for (iIdx \u003d decodingState.inputs[i].position(), oIdx \u003d output.position();\n           iIdx \u003c decodingState.inputs[i].limit();\n           iIdx++, oIdx++) {\n        output.put(oIdx, (byte) (output.get(oIdx) ^\n            decodingState.inputs[i].get(iIdx)));\n      }\n    }\n  }",
          "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java",
          "extendedDetails": {}
        }
      ]
    },
    "5eca6dece67620f990f3306b6caaf09f317b38f6": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12327. Initialize output buffers with ZERO bytes in erasure coder. Contributed by Kai Zheng.\n",
      "commitDate": "29/10/15 12:04 AM",
      "commitName": "5eca6dece67620f990f3306b6caaf09f317b38f6",
      "commitAuthor": "Walter Su",
      "commitDateOld": "07/10/15 6:12 PM",
      "commitNameOld": "66e2cfa1a0285f2b4f62a4ffb4d5c1ee54f76156",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 21.24,
      "commitsBetweenForRepo": 199,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,21 @@\n   protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                           ByteBuffer[] outputs) {\n     ByteBuffer output \u003d outputs[0];\n-    resetBuffer(output);\n \n     int erasedIdx \u003d erasedIndexes[0];\n \n     // Process the inputs.\n     int iIdx, oIdx;\n     for (int i \u003d 0; i \u003c inputs.length; i++) {\n       // Skip the erased location.\n       if (i \u003d\u003d erasedIdx) {\n         continue;\n       }\n \n       for (iIdx \u003d inputs[i].position(), oIdx \u003d output.position();\n            iIdx \u003c inputs[i].limit();\n            iIdx++, oIdx++) {\n         output.put(oIdx, (byte) (output.get(oIdx) ^ inputs[i].get(iIdx)));\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                          ByteBuffer[] outputs) {\n    ByteBuffer output \u003d outputs[0];\n\n    int erasedIdx \u003d erasedIndexes[0];\n\n    // Process the inputs.\n    int iIdx, oIdx;\n    for (int i \u003d 0; i \u003c inputs.length; i++) {\n      // Skip the erased location.\n      if (i \u003d\u003d erasedIdx) {\n        continue;\n      }\n\n      for (iIdx \u003d inputs[i].position(), oIdx \u003d output.position();\n           iIdx \u003c inputs[i].limit();\n           iIdx++, oIdx++) {\n        output.put(oIdx, (byte) (output.get(oIdx) ^ inputs[i].get(iIdx)));\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java",
      "extendedDetails": {}
    },
    "4ad484883f773c702a1874fc12816ef1a4a54136": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11847 Enhance raw coder allowing to read least required inputs in decoding. Contributed by Kai Zheng\n",
      "commitDate": "26/05/15 12:07 PM",
      "commitName": "4ad484883f773c702a1874fc12816ef1a4a54136",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "26/05/15 12:07 PM",
      "commitNameOld": "b30e96bfb4b8ce5537671c97f0c9c56cd195bfdc",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,22 @@\n   protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                           ByteBuffer[] outputs) {\n     ByteBuffer output \u003d outputs[0];\n-    resetOutputBuffer(output);\n+    resetBuffer(output);\n \n     int erasedIdx \u003d erasedIndexes[0];\n \n     // Process the inputs.\n     int iIdx, oIdx;\n     for (int i \u003d 0; i \u003c inputs.length; i++) {\n       // Skip the erased location.\n       if (i \u003d\u003d erasedIdx) {\n         continue;\n       }\n \n       for (iIdx \u003d inputs[i].position(), oIdx \u003d output.position();\n            iIdx \u003c inputs[i].limit();\n            iIdx++, oIdx++) {\n         output.put(oIdx, (byte) (output.get(oIdx) ^ inputs[i].get(iIdx)));\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                          ByteBuffer[] outputs) {\n    ByteBuffer output \u003d outputs[0];\n    resetBuffer(output);\n\n    int erasedIdx \u003d erasedIndexes[0];\n\n    // Process the inputs.\n    int iIdx, oIdx;\n    for (int i \u003d 0; i \u003c inputs.length; i++) {\n      // Skip the erased location.\n      if (i \u003d\u003d erasedIdx) {\n        continue;\n      }\n\n      for (iIdx \u003d inputs[i].position(), oIdx \u003d output.position();\n           iIdx \u003c inputs[i].limit();\n           iIdx++, oIdx++) {\n        output.put(oIdx, (byte) (output.get(oIdx) ^ inputs[i].get(iIdx)));\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java",
      "extendedDetails": {}
    },
    "343c0e76fcd95ac739ca7cd6742c9d617e19fc37": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11938. Enhance ByteBuffer version encode/decode API of raw erasure coder. Contributed by Kai Zheng.\n",
      "commitDate": "26/05/15 12:02 PM",
      "commitName": "343c0e76fcd95ac739ca7cd6742c9d617e19fc37",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "26/05/15 12:02 PM",
      "commitNameOld": "09c3a375bafa481e88d1317388a73c46950164c9",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,19 +1,22 @@\n   protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                           ByteBuffer[] outputs) {\n-    resetBuffer(outputs[0]);\n+    ByteBuffer output \u003d outputs[0];\n+    resetOutputBuffer(output);\n \n-    int bufSize \u003d getChunkSize();\n     int erasedIdx \u003d erasedIndexes[0];\n \n     // Process the inputs.\n+    int iIdx, oIdx;\n     for (int i \u003d 0; i \u003c inputs.length; i++) {\n       // Skip the erased location.\n       if (i \u003d\u003d erasedIdx) {\n         continue;\n       }\n \n-      for (int j \u003d 0; j \u003c bufSize; j++) {\n-        outputs[0].put(j, (byte) (outputs[0].get(j) ^ inputs[i].get(j)));\n+      for (iIdx \u003d inputs[i].position(), oIdx \u003d output.position();\n+           iIdx \u003c inputs[i].limit();\n+           iIdx++, oIdx++) {\n+        output.put(oIdx, (byte) (output.get(oIdx) ^ inputs[i].get(iIdx)));\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                          ByteBuffer[] outputs) {\n    ByteBuffer output \u003d outputs[0];\n    resetOutputBuffer(output);\n\n    int erasedIdx \u003d erasedIndexes[0];\n\n    // Process the inputs.\n    int iIdx, oIdx;\n    for (int i \u003d 0; i \u003c inputs.length; i++) {\n      // Skip the erased location.\n      if (i \u003d\u003d erasedIdx) {\n        continue;\n      }\n\n      for (iIdx \u003d inputs[i].position(), oIdx \u003d output.position();\n           iIdx \u003c inputs[i].limit();\n           iIdx++, oIdx++) {\n        output.put(oIdx, (byte) (output.get(oIdx) ^ inputs[i].get(iIdx)));\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java",
      "extendedDetails": {}
    },
    "09c3a375bafa481e88d1317388a73c46950164c9": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11920. Refactor some codes for erasure coders. Contributed by Kai Zheng.\n",
      "commitDate": "26/05/15 12:02 PM",
      "commitName": "09c3a375bafa481e88d1317388a73c46950164c9",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "26/05/15 11:55 AM",
      "commitNameOld": "17f7cdc04764524c091bb0e9eb43399f88ac0e6b",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 74,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,19 @@\n   protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                           ByteBuffer[] outputs) {\n-    assert(erasedIndexes.length \u003d\u003d outputs.length);\n-    assert(erasedIndexes.length \u003c\u003d 1);\n+    resetBuffer(outputs[0]);\n \n-    int bufSize \u003d inputs[0].remaining();\n+    int bufSize \u003d getChunkSize();\n     int erasedIdx \u003d erasedIndexes[0];\n \n-    // Set the output to zeros.\n-    for (int j \u003d 0; j \u003c bufSize; j++) {\n-      outputs[0].put(j, (byte) 0);\n-    }\n-\n     // Process the inputs.\n     for (int i \u003d 0; i \u003c inputs.length; i++) {\n       // Skip the erased location.\n       if (i \u003d\u003d erasedIdx) {\n         continue;\n       }\n \n       for (int j \u003d 0; j \u003c bufSize; j++) {\n         outputs[0].put(j, (byte) (outputs[0].get(j) ^ inputs[i].get(j)));\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                          ByteBuffer[] outputs) {\n    resetBuffer(outputs[0]);\n\n    int bufSize \u003d getChunkSize();\n    int erasedIdx \u003d erasedIndexes[0];\n\n    // Process the inputs.\n    for (int i \u003d 0; i \u003c inputs.length; i++) {\n      // Skip the erased location.\n      if (i \u003d\u003d erasedIdx) {\n        continue;\n      }\n\n      for (int j \u003d 0; j \u003c bufSize; j++) {\n        outputs[0].put(j, (byte) (outputs[0].get(j) ^ inputs[i].get(j)));\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java",
      "extendedDetails": {}
    },
    "17f7cdc04764524c091bb0e9eb43399f88ac0e6b": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-11805 Better to rename some raw erasure coders. Contributed by Kai Zheng\n",
      "commitDate": "26/05/15 11:55 AM",
      "commitName": "17f7cdc04764524c091bb0e9eb43399f88ac0e6b",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "26/05/15 11:55 AM",
      "commitNameOld": "146ce7a9784e52432b76164009336a4b2cf2860e",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                          ByteBuffer[] outputs) {\n    assert(erasedIndexes.length \u003d\u003d outputs.length);\n    assert(erasedIndexes.length \u003c\u003d 1);\n\n    int bufSize \u003d inputs[0].remaining();\n    int erasedIdx \u003d erasedIndexes[0];\n\n    // Set the output to zeros.\n    for (int j \u003d 0; j \u003c bufSize; j++) {\n      outputs[0].put(j, (byte) 0);\n    }\n\n    // Process the inputs.\n    for (int i \u003d 0; i \u003c inputs.length; i++) {\n      // Skip the erased location.\n      if (i \u003d\u003d erasedIdx) {\n        continue;\n      }\n\n      for (int j \u003d 0; j \u003c bufSize; j++) {\n        outputs[0].put(j, (byte) (outputs[0].get(j) ^ inputs[i].get(j)));\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java",
      "extendedDetails": {
        "oldPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/XorRawDecoder.java",
        "newPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java"
      }
    },
    "b29f3bde4d2fd2f2c4abd6d7b5f97a81bb50efb2": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-11541. Raw XOR coder\n",
      "commitDate": "26/05/15 11:03 AM",
      "commitName": "b29f3bde4d2fd2f2c4abd6d7b5f97a81bb50efb2",
      "commitAuthor": "Kai Zheng",
      "diff": "@@ -0,0 +1,25 @@\n+  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n+                          ByteBuffer[] outputs) {\n+    assert(erasedIndexes.length \u003d\u003d outputs.length);\n+    assert(erasedIndexes.length \u003c\u003d 1);\n+\n+    int bufSize \u003d inputs[0].remaining();\n+    int erasedIdx \u003d erasedIndexes[0];\n+\n+    // Set the output to zeros.\n+    for (int j \u003d 0; j \u003c bufSize; j++) {\n+      outputs[0].put(j, (byte) 0);\n+    }\n+\n+    // Process the inputs.\n+    for (int i \u003d 0; i \u003c inputs.length; i++) {\n+      // Skip the erased location.\n+      if (i \u003d\u003d erasedIdx) {\n+        continue;\n+      }\n+\n+      for (int j \u003d 0; j \u003c bufSize; j++) {\n+        outputs[0].put(j, (byte) (outputs[0].get(j) ^ inputs[i].get(j)));\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected void doDecode(ByteBuffer[] inputs, int[] erasedIndexes,\n                          ByteBuffer[] outputs) {\n    assert(erasedIndexes.length \u003d\u003d outputs.length);\n    assert(erasedIndexes.length \u003c\u003d 1);\n\n    int bufSize \u003d inputs[0].remaining();\n    int erasedIdx \u003d erasedIndexes[0];\n\n    // Set the output to zeros.\n    for (int j \u003d 0; j \u003c bufSize; j++) {\n      outputs[0].put(j, (byte) 0);\n    }\n\n    // Process the inputs.\n    for (int i \u003d 0; i \u003c inputs.length; i++) {\n      // Skip the erased location.\n      if (i \u003d\u003d erasedIdx) {\n        continue;\n      }\n\n      for (int j \u003d 0; j \u003c bufSize; j++) {\n        outputs[0].put(j, (byte) (outputs[0].get(j) ^ inputs[i].get(j)));\n      }\n    }\n  }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/erasurecode/rawcoder/XorRawDecoder.java"
    }
  }
}