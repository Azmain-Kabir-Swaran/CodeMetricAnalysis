{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CacheManager.java",
  "functionName": "loadState",
  "functionId": "loadState___s-PersistState",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
  "functionStartLine": 1129,
  "functionEndLine": 1164,
  "numCommitsSeen": 56,
  "timeTaken": 2829,
  "changeHistory": [
    "3481895f8a9ea9f6e217a0ba158c48da89b3faf2",
    "5f6b4157a40e974ccc6a56c39dbd35c54f393fbd",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305"
  ],
  "changeHistoryShort": {
    "3481895f8a9ea9f6e217a0ba158c48da89b3faf2": "Ybodychange",
    "5f6b4157a40e974ccc6a56c39dbd35c54f393fbd": "Ybodychange",
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": "Yintroduced"
  },
  "changeHistoryDetails": {
    "3481895f8a9ea9f6e217a0ba158c48da89b3faf2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15283. Cache pool MAXTTL is not persisted and restored on cluster restart. Contributed by Stephen O\u0027Donnell.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "16/04/20 8:18 PM",
      "commitName": "3481895f8a9ea9f6e217a0ba158c48da89b3faf2",
      "commitAuthor": "Stephen O\u0027Donnell",
      "commitDateOld": "13/12/18 9:36 PM",
      "commitNameOld": "ca379e1c43fd733a34f3ece6172c96d74c890422",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 489.9,
      "commitsBetweenForRepo": 2956,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,32 +1,36 @@\n   public void loadState(PersistState s) throws IOException {\n     nextDirectiveId \u003d s.section.getNextDirectiveId();\n     for (CachePoolInfoProto p : s.pools) {\n       CachePoolInfo info \u003d new CachePoolInfo(p.getPoolName());\n       if (p.hasOwnerName())\n         info.setOwnerName(p.getOwnerName());\n \n       if (p.hasGroupName())\n         info.setGroupName(p.getGroupName());\n \n       if (p.hasMode())\n         info.setMode(new FsPermission((short) p.getMode()));\n \n       if (p.hasDefaultReplication()) {\n         info.setDefaultReplication((short) p.getDefaultReplication());\n       }\n \n       if (p.hasLimit())\n         info.setLimit(p.getLimit());\n \n+      if (p.hasMaxRelativeExpiry()) {\n+        info.setMaxRelativeExpiryMs(p.getMaxRelativeExpiry());\n+      }\n+\n       addCachePool(info);\n     }\n \n     for (CacheDirectiveInfoProto p : s.directives) {\n       // Get pool reference by looking it up in the map\n       final String poolName \u003d p.getPool();\n       CacheDirective directive \u003d new CacheDirective(p.getId(), new Path(\n           p.getPath()).toUri().getPath(), (short) p.getReplication(), p\n           .getExpiration().getMillis());\n       addCacheDirective(poolName, directive);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void loadState(PersistState s) throws IOException {\n    nextDirectiveId \u003d s.section.getNextDirectiveId();\n    for (CachePoolInfoProto p : s.pools) {\n      CachePoolInfo info \u003d new CachePoolInfo(p.getPoolName());\n      if (p.hasOwnerName())\n        info.setOwnerName(p.getOwnerName());\n\n      if (p.hasGroupName())\n        info.setGroupName(p.getGroupName());\n\n      if (p.hasMode())\n        info.setMode(new FsPermission((short) p.getMode()));\n\n      if (p.hasDefaultReplication()) {\n        info.setDefaultReplication((short) p.getDefaultReplication());\n      }\n\n      if (p.hasLimit())\n        info.setLimit(p.getLimit());\n\n      if (p.hasMaxRelativeExpiry()) {\n        info.setMaxRelativeExpiryMs(p.getMaxRelativeExpiry());\n      }\n\n      addCachePool(info);\n    }\n\n    for (CacheDirectiveInfoProto p : s.directives) {\n      // Get pool reference by looking it up in the map\n      final String poolName \u003d p.getPool();\n      CacheDirective directive \u003d new CacheDirective(p.getId(), new Path(\n          p.getPath()).toUri().getPath(), (short) p.getReplication(), p\n          .getExpiration().getMillis());\n      addCacheDirective(poolName, directive);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "5f6b4157a40e974ccc6a56c39dbd35c54f393fbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10328. Add per-cache-pool default replication num configuration (xupeng via cmccabe)\n",
      "commitDate": "20/06/16 10:42 AM",
      "commitName": "5f6b4157a40e974ccc6a56c39dbd35c54f393fbd",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "16/12/15 6:16 PM",
      "commitNameOld": "f741476146574550a1a208d58ef8be76639e5ddc",
      "commitAuthorOld": "Uma Mahesh",
      "daysBetweenCommits": 186.64,
      "commitsBetweenForRepo": 1171,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,32 @@\n   public void loadState(PersistState s) throws IOException {\n     nextDirectiveId \u003d s.section.getNextDirectiveId();\n     for (CachePoolInfoProto p : s.pools) {\n       CachePoolInfo info \u003d new CachePoolInfo(p.getPoolName());\n       if (p.hasOwnerName())\n         info.setOwnerName(p.getOwnerName());\n \n       if (p.hasGroupName())\n         info.setGroupName(p.getGroupName());\n \n       if (p.hasMode())\n         info.setMode(new FsPermission((short) p.getMode()));\n \n+      if (p.hasDefaultReplication()) {\n+        info.setDefaultReplication((short) p.getDefaultReplication());\n+      }\n+\n       if (p.hasLimit())\n         info.setLimit(p.getLimit());\n \n       addCachePool(info);\n     }\n \n     for (CacheDirectiveInfoProto p : s.directives) {\n       // Get pool reference by looking it up in the map\n       final String poolName \u003d p.getPool();\n       CacheDirective directive \u003d new CacheDirective(p.getId(), new Path(\n           p.getPath()).toUri().getPath(), (short) p.getReplication(), p\n           .getExpiration().getMillis());\n       addCacheDirective(poolName, directive);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void loadState(PersistState s) throws IOException {\n    nextDirectiveId \u003d s.section.getNextDirectiveId();\n    for (CachePoolInfoProto p : s.pools) {\n      CachePoolInfo info \u003d new CachePoolInfo(p.getPoolName());\n      if (p.hasOwnerName())\n        info.setOwnerName(p.getOwnerName());\n\n      if (p.hasGroupName())\n        info.setGroupName(p.getGroupName());\n\n      if (p.hasMode())\n        info.setMode(new FsPermission((short) p.getMode()));\n\n      if (p.hasDefaultReplication()) {\n        info.setDefaultReplication((short) p.getDefaultReplication());\n      }\n\n      if (p.hasLimit())\n        info.setLimit(p.getLimit());\n\n      addCachePool(info);\n    }\n\n    for (CacheDirectiveInfoProto p : s.directives) {\n      // Get pool reference by looking it up in the map\n      final String poolName \u003d p.getPool();\n      CacheDirective directive \u003d new CacheDirective(p.getId(), new Path(\n          p.getPath()).toUri().getPath(), (short) p.getReplication(), p\n          .getExpiration().getMillis());\n      addCacheDirective(poolName, directive);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "a2edb11b68ae01a44092cb14ac2717a6aad93305": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5698. Use protobuf to serialize / deserialize FSImage. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1566359 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/02/14 11:18 AM",
      "commitName": "a2edb11b68ae01a44092cb14ac2717a6aad93305",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,28 @@\n+  public void loadState(PersistState s) throws IOException {\n+    nextDirectiveId \u003d s.section.getNextDirectiveId();\n+    for (CachePoolInfoProto p : s.pools) {\n+      CachePoolInfo info \u003d new CachePoolInfo(p.getPoolName());\n+      if (p.hasOwnerName())\n+        info.setOwnerName(p.getOwnerName());\n+\n+      if (p.hasGroupName())\n+        info.setGroupName(p.getGroupName());\n+\n+      if (p.hasMode())\n+        info.setMode(new FsPermission((short) p.getMode()));\n+\n+      if (p.hasLimit())\n+        info.setLimit(p.getLimit());\n+\n+      addCachePool(info);\n+    }\n+\n+    for (CacheDirectiveInfoProto p : s.directives) {\n+      // Get pool reference by looking it up in the map\n+      final String poolName \u003d p.getPool();\n+      CacheDirective directive \u003d new CacheDirective(p.getId(), new Path(\n+          p.getPath()).toUri().getPath(), (short) p.getReplication(), p\n+          .getExpiration().getMillis());\n+      addCacheDirective(poolName, directive);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void loadState(PersistState s) throws IOException {\n    nextDirectiveId \u003d s.section.getNextDirectiveId();\n    for (CachePoolInfoProto p : s.pools) {\n      CachePoolInfo info \u003d new CachePoolInfo(p.getPoolName());\n      if (p.hasOwnerName())\n        info.setOwnerName(p.getOwnerName());\n\n      if (p.hasGroupName())\n        info.setGroupName(p.getGroupName());\n\n      if (p.hasMode())\n        info.setMode(new FsPermission((short) p.getMode()));\n\n      if (p.hasLimit())\n        info.setLimit(p.getLimit());\n\n      addCachePool(info);\n    }\n\n    for (CacheDirectiveInfoProto p : s.directives) {\n      // Get pool reference by looking it up in the map\n      final String poolName \u003d p.getPool();\n      CacheDirective directive \u003d new CacheDirective(p.getId(), new Path(\n          p.getPath()).toUri().getPath(), (short) p.getReplication(), p\n          .getExpiration().getMillis());\n      addCacheDirective(poolName, directive);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java"
    }
  }
}