{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "JobResourceUploader.java",
  "functionName": "uploadResourcesInternal",
  "functionId": "uploadResourcesInternal___job-Job__submitJobDir-Path",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
  "functionStartLine": 141,
  "functionEndLine": 227,
  "numCommitsSeen": 33,
  "timeTaken": 5154,
  "changeHistory": [
    "6e6945cd78d76c6beaec85c963f27e28bf96c0f2",
    "e46d5bb962b0c942f993afc505b165b1cd96e51b",
    "0adc0471d0c06f66a31060f270dcb50a7b4ffafa",
    "5f496683fb00ba26a6bf5a506ae87d4bc4088727",
    "3a8d57a0a2e047b34be82f602a2b6cf5593d2125",
    "fc0885da294490c3984c2231a4d35f89b3b520d4",
    "f80a7298325a4626638ee24467e2012442e480d4",
    "8f0d3d69d65a252439610e6f13d679808d768569",
    "605b4b61364781fc99ed27035c793153a20d8f71",
    "d50e8f09287deeb51012d08e326a2ed71a6da869",
    "c66c3ac6bf9f63177279feec3f2917e4b882e2bc"
  ],
  "changeHistoryShort": {
    "6e6945cd78d76c6beaec85c963f27e28bf96c0f2": "Ybodychange",
    "e46d5bb962b0c942f993afc505b165b1cd96e51b": "Ymultichange(Yrename,Ymodifierchange,Ybodychange)",
    "0adc0471d0c06f66a31060f270dcb50a7b4ffafa": "Ybodychange",
    "5f496683fb00ba26a6bf5a506ae87d4bc4088727": "Ybodychange",
    "3a8d57a0a2e047b34be82f602a2b6cf5593d2125": "Ybodychange",
    "fc0885da294490c3984c2231a4d35f89b3b520d4": "Ybodychange",
    "f80a7298325a4626638ee24467e2012442e480d4": "Ybodychange",
    "8f0d3d69d65a252439610e6f13d679808d768569": "Ymultichange(Yrename,Ybodychange)",
    "605b4b61364781fc99ed27035c793153a20d8f71": "Ybodychange",
    "d50e8f09287deeb51012d08e326a2ed71a6da869": "Ybodychange",
    "c66c3ac6bf9f63177279feec3f2917e4b882e2bc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6e6945cd78d76c6beaec85c963f27e28bf96c0f2": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-7059. Downward Compatibility issue: MR job fails because of unknown setErasureCodingPolicy method from 3.x client to HDFS 2.x cluster. Contributed by Jiandan Yang.\n",
      "commitDate": "28/02/18 6:18 PM",
      "commitName": "6e6945cd78d76c6beaec85c963f27e28bf96c0f2",
      "commitAuthor": "Weiwei Yang",
      "commitDateOld": "02/11/17 1:43 AM",
      "commitNameOld": "178751ed8c9d47038acf8616c226f1f52e884feb",
      "commitAuthorOld": "Akira Ajisaka",
      "daysBetweenCommits": 118.73,
      "commitsBetweenForRepo": 784,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,87 @@\n   private void uploadResourcesInternal(Job job, Path submitJobDir)\n       throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n     short replication \u003d\n         (short) conf.getInt(Job.SUBMIT_REPLICATION,\n             Job.DEFAULT_SUBMIT_REPLICATION);\n \n     if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n       LOG.warn(\"Hadoop command-line option parsing not performed. \"\n           + \"Implement the Tool interface and execute your application \"\n           + \"with ToolRunner to remedy this.\");\n     }\n \n     //\n     // Figure out what fs the JobTracker is using. Copy the\n     // job to it, under a temporary name. This allows DFS to work,\n     // and under the local fs also provides UNIX-like object loading\n     // semantics. (that is, if the job file is deleted right after\n     // submission, we can still run the submission to completion)\n     //\n \n     // Create a number of filenames in the JobTracker\u0027s fs namespace\n     LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n     if (jtFs.exists(submitJobDir)) {\n       throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n           + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n           + \" that directory\");\n     }\n     // Create the submission directory for the MapReduce job.\n     submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n     submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n     FsPermission mapredSysPerms \u003d\n         new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     mkdirs(jtFs, submitJobDir, mapredSysPerms);\n \n     if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,\n         MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\n-      disableErasureCodingForPath(jtFs, submitJobDir);\n+      disableErasureCodingForPath(submitJobDir);\n     }\n \n     // Get the resources that have been added via command line arguments in the\n     // GenericOptionsParser (i.e. files, libjars, archives).\n     Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n     Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n     Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n     String jobJar \u003d job.getJar();\n \n     // Merge resources that have been programmatically specified for the shared\n     // cache via the Job API.\n     files.addAll(conf.getStringCollection(MRJobConfig.FILES_FOR_SHARED_CACHE));\n     libjars.addAll(conf.getStringCollection(\n             MRJobConfig.FILES_FOR_CLASSPATH_AND_SHARED_CACHE));\n     archives.addAll(conf\n         .getStringCollection(MRJobConfig.ARCHIVES_FOR_SHARED_CACHE));\n \n \n     Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n     checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n \n     Map\u003cString, Boolean\u003e fileSCUploadPolicies \u003d\n         new LinkedHashMap\u003cString, Boolean\u003e();\n     Map\u003cString, Boolean\u003e archiveSCUploadPolicies \u003d\n         new LinkedHashMap\u003cString, Boolean\u003e();\n \n     uploadFiles(job, files, submitJobDir, mapredSysPerms, replication,\n         fileSCUploadPolicies, statCache);\n     uploadLibJars(job, libjars, submitJobDir, mapredSysPerms, replication,\n         fileSCUploadPolicies, statCache);\n     uploadArchives(job, archives, submitJobDir, mapredSysPerms, replication,\n         archiveSCUploadPolicies, statCache);\n     uploadJobJar(job, jobJar, submitJobDir, replication, statCache);\n     addLog4jToDistributedCache(job, submitJobDir);\n \n     // Note, we do not consider resources in the distributed cache for the\n     // shared cache at this time. Only resources specified via the\n     // GenericOptionsParser or the jobjar.\n     Job.setFileSharedCacheUploadPolicies(conf, fileSCUploadPolicies);\n     Job.setArchiveSharedCacheUploadPolicies(conf, archiveSCUploadPolicies);\n \n     // set the timestamps of the archives and files\n     // set the public/private visibility of the archives and files\n     ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n         statCache);\n     // get DelegationToken for cached file\n     ClientDistributedCacheManager.getDelegationTokens(conf,\n         job.getCredentials());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void uploadResourcesInternal(Job job, Path submitJobDir)\n      throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n    short replication \u003d\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker\u0027s fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n          + \" that directory\");\n    }\n    // Create the submission directory for the MapReduce job.\n    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms \u003d\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    mkdirs(jtFs, submitJobDir, mapredSysPerms);\n\n    if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,\n        MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\n      disableErasureCodingForPath(submitJobDir);\n    }\n\n    // Get the resources that have been added via command line arguments in the\n    // GenericOptionsParser (i.e. files, libjars, archives).\n    Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n    Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n    Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n    String jobJar \u003d job.getJar();\n\n    // Merge resources that have been programmatically specified for the shared\n    // cache via the Job API.\n    files.addAll(conf.getStringCollection(MRJobConfig.FILES_FOR_SHARED_CACHE));\n    libjars.addAll(conf.getStringCollection(\n            MRJobConfig.FILES_FOR_CLASSPATH_AND_SHARED_CACHE));\n    archives.addAll(conf\n        .getStringCollection(MRJobConfig.ARCHIVES_FOR_SHARED_CACHE));\n\n\n    Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n    checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n\n    Map\u003cString, Boolean\u003e fileSCUploadPolicies \u003d\n        new LinkedHashMap\u003cString, Boolean\u003e();\n    Map\u003cString, Boolean\u003e archiveSCUploadPolicies \u003d\n        new LinkedHashMap\u003cString, Boolean\u003e();\n\n    uploadFiles(job, files, submitJobDir, mapredSysPerms, replication,\n        fileSCUploadPolicies, statCache);\n    uploadLibJars(job, libjars, submitJobDir, mapredSysPerms, replication,\n        fileSCUploadPolicies, statCache);\n    uploadArchives(job, archives, submitJobDir, mapredSysPerms, replication,\n        archiveSCUploadPolicies, statCache);\n    uploadJobJar(job, jobJar, submitJobDir, replication, statCache);\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // Note, we do not consider resources in the distributed cache for the\n    // shared cache at this time. Only resources specified via the\n    // GenericOptionsParser or the jobjar.\n    Job.setFileSharedCacheUploadPolicies(conf, fileSCUploadPolicies);\n    Job.setArchiveSharedCacheUploadPolicies(conf, archiveSCUploadPolicies);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n        statCache);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
      "extendedDetails": {}
    },
    "e46d5bb962b0c942f993afc505b165b1cd96e51b": {
      "type": "Ymultichange(Yrename,Ymodifierchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-5951. Add support for the YARN Shared Cache.\n",
      "commitDate": "12/10/17 10:59 AM",
      "commitName": "e46d5bb962b0c942f993afc505b165b1cd96e51b",
      "commitAuthor": "Chris Trezzo",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "MAPREDUCE-5951. Add support for the YARN Shared Cache.\n",
          "commitDate": "12/10/17 10:59 AM",
          "commitName": "e46d5bb962b0c942f993afc505b165b1cd96e51b",
          "commitAuthor": "Chris Trezzo",
          "commitDateOld": "18/09/17 10:40 AM",
          "commitNameOld": "0adc0471d0c06f66a31060f270dcb50a7b4ffafa",
          "commitAuthorOld": "Robert Kanter",
          "daysBetweenCommits": 24.01,
          "commitsBetweenForRepo": 206,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,87 @@\n-  public void uploadResources(Job job, Path submitJobDir) throws IOException {\n+  private void uploadResourcesInternal(Job job, Path submitJobDir)\n+      throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n     short replication \u003d\n         (short) conf.getInt(Job.SUBMIT_REPLICATION,\n             Job.DEFAULT_SUBMIT_REPLICATION);\n \n     if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n       LOG.warn(\"Hadoop command-line option parsing not performed. \"\n           + \"Implement the Tool interface and execute your application \"\n           + \"with ToolRunner to remedy this.\");\n     }\n \n     //\n     // Figure out what fs the JobTracker is using. Copy the\n     // job to it, under a temporary name. This allows DFS to work,\n     // and under the local fs also provides UNIX-like object loading\n     // semantics. (that is, if the job file is deleted right after\n     // submission, we can still run the submission to completion)\n     //\n \n     // Create a number of filenames in the JobTracker\u0027s fs namespace\n     LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n     if (jtFs.exists(submitJobDir)) {\n       throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n           + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n           + \" that directory\");\n     }\n+    // Create the submission directory for the MapReduce job.\n     submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n     submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n     FsPermission mapredSysPerms \u003d\n         new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     mkdirs(jtFs, submitJobDir, mapredSysPerms);\n \n     if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,\n         MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\n       disableErasureCodingForPath(jtFs, submitJobDir);\n     }\n \n+    // Get the resources that have been added via command line arguments in the\n+    // GenericOptionsParser (i.e. files, libjars, archives).\n     Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n     Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n     Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n     String jobJar \u003d job.getJar();\n \n+    // Merge resources that have been programmatically specified for the shared\n+    // cache via the Job API.\n+    files.addAll(conf.getStringCollection(MRJobConfig.FILES_FOR_SHARED_CACHE));\n+    libjars.addAll(conf.getStringCollection(\n+            MRJobConfig.FILES_FOR_CLASSPATH_AND_SHARED_CACHE));\n+    archives.addAll(conf\n+        .getStringCollection(MRJobConfig.ARCHIVES_FOR_SHARED_CACHE));\n+\n+\n     Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n     checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n \n-    uploadFiles(conf, files, submitJobDir, mapredSysPerms, replication);\n-    uploadLibJars(conf, libjars, submitJobDir, mapredSysPerms, replication);\n-    uploadArchives(conf, archives, submitJobDir, mapredSysPerms, replication);\n-    uploadJobJar(job, jobJar, submitJobDir, replication);\n+    Map\u003cString, Boolean\u003e fileSCUploadPolicies \u003d\n+        new LinkedHashMap\u003cString, Boolean\u003e();\n+    Map\u003cString, Boolean\u003e archiveSCUploadPolicies \u003d\n+        new LinkedHashMap\u003cString, Boolean\u003e();\n+\n+    uploadFiles(job, files, submitJobDir, mapredSysPerms, replication,\n+        fileSCUploadPolicies, statCache);\n+    uploadLibJars(job, libjars, submitJobDir, mapredSysPerms, replication,\n+        fileSCUploadPolicies, statCache);\n+    uploadArchives(job, archives, submitJobDir, mapredSysPerms, replication,\n+        archiveSCUploadPolicies, statCache);\n+    uploadJobJar(job, jobJar, submitJobDir, replication, statCache);\n     addLog4jToDistributedCache(job, submitJobDir);\n \n+    // Note, we do not consider resources in the distributed cache for the\n+    // shared cache at this time. Only resources specified via the\n+    // GenericOptionsParser or the jobjar.\n+    Job.setFileSharedCacheUploadPolicies(conf, fileSCUploadPolicies);\n+    Job.setArchiveSharedCacheUploadPolicies(conf, archiveSCUploadPolicies);\n+\n     // set the timestamps of the archives and files\n     // set the public/private visibility of the archives and files\n     ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n         statCache);\n     // get DelegationToken for cached file\n     ClientDistributedCacheManager.getDelegationTokens(conf,\n         job.getCredentials());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void uploadResourcesInternal(Job job, Path submitJobDir)\n      throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n    short replication \u003d\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker\u0027s fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n          + \" that directory\");\n    }\n    // Create the submission directory for the MapReduce job.\n    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms \u003d\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    mkdirs(jtFs, submitJobDir, mapredSysPerms);\n\n    if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,\n        MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\n      disableErasureCodingForPath(jtFs, submitJobDir);\n    }\n\n    // Get the resources that have been added via command line arguments in the\n    // GenericOptionsParser (i.e. files, libjars, archives).\n    Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n    Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n    Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n    String jobJar \u003d job.getJar();\n\n    // Merge resources that have been programmatically specified for the shared\n    // cache via the Job API.\n    files.addAll(conf.getStringCollection(MRJobConfig.FILES_FOR_SHARED_CACHE));\n    libjars.addAll(conf.getStringCollection(\n            MRJobConfig.FILES_FOR_CLASSPATH_AND_SHARED_CACHE));\n    archives.addAll(conf\n        .getStringCollection(MRJobConfig.ARCHIVES_FOR_SHARED_CACHE));\n\n\n    Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n    checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n\n    Map\u003cString, Boolean\u003e fileSCUploadPolicies \u003d\n        new LinkedHashMap\u003cString, Boolean\u003e();\n    Map\u003cString, Boolean\u003e archiveSCUploadPolicies \u003d\n        new LinkedHashMap\u003cString, Boolean\u003e();\n\n    uploadFiles(job, files, submitJobDir, mapredSysPerms, replication,\n        fileSCUploadPolicies, statCache);\n    uploadLibJars(job, libjars, submitJobDir, mapredSysPerms, replication,\n        fileSCUploadPolicies, statCache);\n    uploadArchives(job, archives, submitJobDir, mapredSysPerms, replication,\n        archiveSCUploadPolicies, statCache);\n    uploadJobJar(job, jobJar, submitJobDir, replication, statCache);\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // Note, we do not consider resources in the distributed cache for the\n    // shared cache at this time. Only resources specified via the\n    // GenericOptionsParser or the jobjar.\n    Job.setFileSharedCacheUploadPolicies(conf, fileSCUploadPolicies);\n    Job.setArchiveSharedCacheUploadPolicies(conf, archiveSCUploadPolicies);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n        statCache);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
          "extendedDetails": {
            "oldValue": "uploadResources",
            "newValue": "uploadResourcesInternal"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "MAPREDUCE-5951. Add support for the YARN Shared Cache.\n",
          "commitDate": "12/10/17 10:59 AM",
          "commitName": "e46d5bb962b0c942f993afc505b165b1cd96e51b",
          "commitAuthor": "Chris Trezzo",
          "commitDateOld": "18/09/17 10:40 AM",
          "commitNameOld": "0adc0471d0c06f66a31060f270dcb50a7b4ffafa",
          "commitAuthorOld": "Robert Kanter",
          "daysBetweenCommits": 24.01,
          "commitsBetweenForRepo": 206,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,87 @@\n-  public void uploadResources(Job job, Path submitJobDir) throws IOException {\n+  private void uploadResourcesInternal(Job job, Path submitJobDir)\n+      throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n     short replication \u003d\n         (short) conf.getInt(Job.SUBMIT_REPLICATION,\n             Job.DEFAULT_SUBMIT_REPLICATION);\n \n     if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n       LOG.warn(\"Hadoop command-line option parsing not performed. \"\n           + \"Implement the Tool interface and execute your application \"\n           + \"with ToolRunner to remedy this.\");\n     }\n \n     //\n     // Figure out what fs the JobTracker is using. Copy the\n     // job to it, under a temporary name. This allows DFS to work,\n     // and under the local fs also provides UNIX-like object loading\n     // semantics. (that is, if the job file is deleted right after\n     // submission, we can still run the submission to completion)\n     //\n \n     // Create a number of filenames in the JobTracker\u0027s fs namespace\n     LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n     if (jtFs.exists(submitJobDir)) {\n       throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n           + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n           + \" that directory\");\n     }\n+    // Create the submission directory for the MapReduce job.\n     submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n     submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n     FsPermission mapredSysPerms \u003d\n         new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     mkdirs(jtFs, submitJobDir, mapredSysPerms);\n \n     if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,\n         MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\n       disableErasureCodingForPath(jtFs, submitJobDir);\n     }\n \n+    // Get the resources that have been added via command line arguments in the\n+    // GenericOptionsParser (i.e. files, libjars, archives).\n     Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n     Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n     Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n     String jobJar \u003d job.getJar();\n \n+    // Merge resources that have been programmatically specified for the shared\n+    // cache via the Job API.\n+    files.addAll(conf.getStringCollection(MRJobConfig.FILES_FOR_SHARED_CACHE));\n+    libjars.addAll(conf.getStringCollection(\n+            MRJobConfig.FILES_FOR_CLASSPATH_AND_SHARED_CACHE));\n+    archives.addAll(conf\n+        .getStringCollection(MRJobConfig.ARCHIVES_FOR_SHARED_CACHE));\n+\n+\n     Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n     checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n \n-    uploadFiles(conf, files, submitJobDir, mapredSysPerms, replication);\n-    uploadLibJars(conf, libjars, submitJobDir, mapredSysPerms, replication);\n-    uploadArchives(conf, archives, submitJobDir, mapredSysPerms, replication);\n-    uploadJobJar(job, jobJar, submitJobDir, replication);\n+    Map\u003cString, Boolean\u003e fileSCUploadPolicies \u003d\n+        new LinkedHashMap\u003cString, Boolean\u003e();\n+    Map\u003cString, Boolean\u003e archiveSCUploadPolicies \u003d\n+        new LinkedHashMap\u003cString, Boolean\u003e();\n+\n+    uploadFiles(job, files, submitJobDir, mapredSysPerms, replication,\n+        fileSCUploadPolicies, statCache);\n+    uploadLibJars(job, libjars, submitJobDir, mapredSysPerms, replication,\n+        fileSCUploadPolicies, statCache);\n+    uploadArchives(job, archives, submitJobDir, mapredSysPerms, replication,\n+        archiveSCUploadPolicies, statCache);\n+    uploadJobJar(job, jobJar, submitJobDir, replication, statCache);\n     addLog4jToDistributedCache(job, submitJobDir);\n \n+    // Note, we do not consider resources in the distributed cache for the\n+    // shared cache at this time. Only resources specified via the\n+    // GenericOptionsParser or the jobjar.\n+    Job.setFileSharedCacheUploadPolicies(conf, fileSCUploadPolicies);\n+    Job.setArchiveSharedCacheUploadPolicies(conf, archiveSCUploadPolicies);\n+\n     // set the timestamps of the archives and files\n     // set the public/private visibility of the archives and files\n     ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n         statCache);\n     // get DelegationToken for cached file\n     ClientDistributedCacheManager.getDelegationTokens(conf,\n         job.getCredentials());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void uploadResourcesInternal(Job job, Path submitJobDir)\n      throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n    short replication \u003d\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker\u0027s fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n          + \" that directory\");\n    }\n    // Create the submission directory for the MapReduce job.\n    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms \u003d\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    mkdirs(jtFs, submitJobDir, mapredSysPerms);\n\n    if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,\n        MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\n      disableErasureCodingForPath(jtFs, submitJobDir);\n    }\n\n    // Get the resources that have been added via command line arguments in the\n    // GenericOptionsParser (i.e. files, libjars, archives).\n    Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n    Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n    Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n    String jobJar \u003d job.getJar();\n\n    // Merge resources that have been programmatically specified for the shared\n    // cache via the Job API.\n    files.addAll(conf.getStringCollection(MRJobConfig.FILES_FOR_SHARED_CACHE));\n    libjars.addAll(conf.getStringCollection(\n            MRJobConfig.FILES_FOR_CLASSPATH_AND_SHARED_CACHE));\n    archives.addAll(conf\n        .getStringCollection(MRJobConfig.ARCHIVES_FOR_SHARED_CACHE));\n\n\n    Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n    checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n\n    Map\u003cString, Boolean\u003e fileSCUploadPolicies \u003d\n        new LinkedHashMap\u003cString, Boolean\u003e();\n    Map\u003cString, Boolean\u003e archiveSCUploadPolicies \u003d\n        new LinkedHashMap\u003cString, Boolean\u003e();\n\n    uploadFiles(job, files, submitJobDir, mapredSysPerms, replication,\n        fileSCUploadPolicies, statCache);\n    uploadLibJars(job, libjars, submitJobDir, mapredSysPerms, replication,\n        fileSCUploadPolicies, statCache);\n    uploadArchives(job, archives, submitJobDir, mapredSysPerms, replication,\n        archiveSCUploadPolicies, statCache);\n    uploadJobJar(job, jobJar, submitJobDir, replication, statCache);\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // Note, we do not consider resources in the distributed cache for the\n    // shared cache at this time. Only resources specified via the\n    // GenericOptionsParser or the jobjar.\n    Job.setFileSharedCacheUploadPolicies(conf, fileSCUploadPolicies);\n    Job.setArchiveSharedCacheUploadPolicies(conf, archiveSCUploadPolicies);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n        statCache);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-5951. Add support for the YARN Shared Cache.\n",
          "commitDate": "12/10/17 10:59 AM",
          "commitName": "e46d5bb962b0c942f993afc505b165b1cd96e51b",
          "commitAuthor": "Chris Trezzo",
          "commitDateOld": "18/09/17 10:40 AM",
          "commitNameOld": "0adc0471d0c06f66a31060f270dcb50a7b4ffafa",
          "commitAuthorOld": "Robert Kanter",
          "daysBetweenCommits": 24.01,
          "commitsBetweenForRepo": 206,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,60 +1,87 @@\n-  public void uploadResources(Job job, Path submitJobDir) throws IOException {\n+  private void uploadResourcesInternal(Job job, Path submitJobDir)\n+      throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n     short replication \u003d\n         (short) conf.getInt(Job.SUBMIT_REPLICATION,\n             Job.DEFAULT_SUBMIT_REPLICATION);\n \n     if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n       LOG.warn(\"Hadoop command-line option parsing not performed. \"\n           + \"Implement the Tool interface and execute your application \"\n           + \"with ToolRunner to remedy this.\");\n     }\n \n     //\n     // Figure out what fs the JobTracker is using. Copy the\n     // job to it, under a temporary name. This allows DFS to work,\n     // and under the local fs also provides UNIX-like object loading\n     // semantics. (that is, if the job file is deleted right after\n     // submission, we can still run the submission to completion)\n     //\n \n     // Create a number of filenames in the JobTracker\u0027s fs namespace\n     LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n     if (jtFs.exists(submitJobDir)) {\n       throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n           + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n           + \" that directory\");\n     }\n+    // Create the submission directory for the MapReduce job.\n     submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n     submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n     FsPermission mapredSysPerms \u003d\n         new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     mkdirs(jtFs, submitJobDir, mapredSysPerms);\n \n     if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,\n         MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\n       disableErasureCodingForPath(jtFs, submitJobDir);\n     }\n \n+    // Get the resources that have been added via command line arguments in the\n+    // GenericOptionsParser (i.e. files, libjars, archives).\n     Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n     Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n     Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n     String jobJar \u003d job.getJar();\n \n+    // Merge resources that have been programmatically specified for the shared\n+    // cache via the Job API.\n+    files.addAll(conf.getStringCollection(MRJobConfig.FILES_FOR_SHARED_CACHE));\n+    libjars.addAll(conf.getStringCollection(\n+            MRJobConfig.FILES_FOR_CLASSPATH_AND_SHARED_CACHE));\n+    archives.addAll(conf\n+        .getStringCollection(MRJobConfig.ARCHIVES_FOR_SHARED_CACHE));\n+\n+\n     Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n     checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n \n-    uploadFiles(conf, files, submitJobDir, mapredSysPerms, replication);\n-    uploadLibJars(conf, libjars, submitJobDir, mapredSysPerms, replication);\n-    uploadArchives(conf, archives, submitJobDir, mapredSysPerms, replication);\n-    uploadJobJar(job, jobJar, submitJobDir, replication);\n+    Map\u003cString, Boolean\u003e fileSCUploadPolicies \u003d\n+        new LinkedHashMap\u003cString, Boolean\u003e();\n+    Map\u003cString, Boolean\u003e archiveSCUploadPolicies \u003d\n+        new LinkedHashMap\u003cString, Boolean\u003e();\n+\n+    uploadFiles(job, files, submitJobDir, mapredSysPerms, replication,\n+        fileSCUploadPolicies, statCache);\n+    uploadLibJars(job, libjars, submitJobDir, mapredSysPerms, replication,\n+        fileSCUploadPolicies, statCache);\n+    uploadArchives(job, archives, submitJobDir, mapredSysPerms, replication,\n+        archiveSCUploadPolicies, statCache);\n+    uploadJobJar(job, jobJar, submitJobDir, replication, statCache);\n     addLog4jToDistributedCache(job, submitJobDir);\n \n+    // Note, we do not consider resources in the distributed cache for the\n+    // shared cache at this time. Only resources specified via the\n+    // GenericOptionsParser or the jobjar.\n+    Job.setFileSharedCacheUploadPolicies(conf, fileSCUploadPolicies);\n+    Job.setArchiveSharedCacheUploadPolicies(conf, archiveSCUploadPolicies);\n+\n     // set the timestamps of the archives and files\n     // set the public/private visibility of the archives and files\n     ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n         statCache);\n     // get DelegationToken for cached file\n     ClientDistributedCacheManager.getDelegationTokens(conf,\n         job.getCredentials());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void uploadResourcesInternal(Job job, Path submitJobDir)\n      throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n    short replication \u003d\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker\u0027s fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n          + \" that directory\");\n    }\n    // Create the submission directory for the MapReduce job.\n    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms \u003d\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    mkdirs(jtFs, submitJobDir, mapredSysPerms);\n\n    if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,\n        MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\n      disableErasureCodingForPath(jtFs, submitJobDir);\n    }\n\n    // Get the resources that have been added via command line arguments in the\n    // GenericOptionsParser (i.e. files, libjars, archives).\n    Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n    Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n    Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n    String jobJar \u003d job.getJar();\n\n    // Merge resources that have been programmatically specified for the shared\n    // cache via the Job API.\n    files.addAll(conf.getStringCollection(MRJobConfig.FILES_FOR_SHARED_CACHE));\n    libjars.addAll(conf.getStringCollection(\n            MRJobConfig.FILES_FOR_CLASSPATH_AND_SHARED_CACHE));\n    archives.addAll(conf\n        .getStringCollection(MRJobConfig.ARCHIVES_FOR_SHARED_CACHE));\n\n\n    Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n    checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n\n    Map\u003cString, Boolean\u003e fileSCUploadPolicies \u003d\n        new LinkedHashMap\u003cString, Boolean\u003e();\n    Map\u003cString, Boolean\u003e archiveSCUploadPolicies \u003d\n        new LinkedHashMap\u003cString, Boolean\u003e();\n\n    uploadFiles(job, files, submitJobDir, mapredSysPerms, replication,\n        fileSCUploadPolicies, statCache);\n    uploadLibJars(job, libjars, submitJobDir, mapredSysPerms, replication,\n        fileSCUploadPolicies, statCache);\n    uploadArchives(job, archives, submitJobDir, mapredSysPerms, replication,\n        archiveSCUploadPolicies, statCache);\n    uploadJobJar(job, jobJar, submitJobDir, replication, statCache);\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // Note, we do not consider resources in the distributed cache for the\n    // shared cache at this time. Only resources specified via the\n    // GenericOptionsParser or the jobjar.\n    Job.setFileSharedCacheUploadPolicies(conf, fileSCUploadPolicies);\n    Job.setArchiveSharedCacheUploadPolicies(conf, archiveSCUploadPolicies);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n        statCache);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
          "extendedDetails": {}
        }
      ]
    },
    "0adc0471d0c06f66a31060f270dcb50a7b4ffafa": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6954. Disable erasure coding for files that are uploaded to the MR staging area (pbacsko via rkanter)\n",
      "commitDate": "18/09/17 10:40 AM",
      "commitName": "0adc0471d0c06f66a31060f270dcb50a7b4ffafa",
      "commitAuthor": "Robert Kanter",
      "commitDateOld": "18/09/17 10:32 AM",
      "commitNameOld": "5f496683fb00ba26a6bf5a506ae87d4bc4088727",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,60 @@\n   public void uploadResources(Job job, Path submitJobDir) throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n     short replication \u003d\n         (short) conf.getInt(Job.SUBMIT_REPLICATION,\n             Job.DEFAULT_SUBMIT_REPLICATION);\n \n     if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n       LOG.warn(\"Hadoop command-line option parsing not performed. \"\n           + \"Implement the Tool interface and execute your application \"\n           + \"with ToolRunner to remedy this.\");\n     }\n \n     //\n     // Figure out what fs the JobTracker is using. Copy the\n     // job to it, under a temporary name. This allows DFS to work,\n     // and under the local fs also provides UNIX-like object loading\n     // semantics. (that is, if the job file is deleted right after\n     // submission, we can still run the submission to completion)\n     //\n \n     // Create a number of filenames in the JobTracker\u0027s fs namespace\n     LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n     if (jtFs.exists(submitJobDir)) {\n       throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n           + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n           + \" that directory\");\n     }\n     submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n     submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n     FsPermission mapredSysPerms \u003d\n         new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     mkdirs(jtFs, submitJobDir, mapredSysPerms);\n \n+    if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,\n+        MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\n+      disableErasureCodingForPath(jtFs, submitJobDir);\n+    }\n+\n     Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n     Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n     Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n     String jobJar \u003d job.getJar();\n \n     Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n     checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n \n     uploadFiles(conf, files, submitJobDir, mapredSysPerms, replication);\n     uploadLibJars(conf, libjars, submitJobDir, mapredSysPerms, replication);\n     uploadArchives(conf, archives, submitJobDir, mapredSysPerms, replication);\n     uploadJobJar(job, jobJar, submitJobDir, replication);\n     addLog4jToDistributedCache(job, submitJobDir);\n \n     // set the timestamps of the archives and files\n     // set the public/private visibility of the archives and files\n     ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n         statCache);\n     // get DelegationToken for cached file\n     ClientDistributedCacheManager.getDelegationTokens(conf,\n         job.getCredentials());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void uploadResources(Job job, Path submitJobDir) throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n    short replication \u003d\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker\u0027s fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n          + \" that directory\");\n    }\n    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms \u003d\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    mkdirs(jtFs, submitJobDir, mapredSysPerms);\n\n    if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,\n        MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\n      disableErasureCodingForPath(jtFs, submitJobDir);\n    }\n\n    Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n    Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n    Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n    String jobJar \u003d job.getJar();\n\n    Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n    checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n\n    uploadFiles(conf, files, submitJobDir, mapredSysPerms, replication);\n    uploadLibJars(conf, libjars, submitJobDir, mapredSysPerms, replication);\n    uploadArchives(conf, archives, submitJobDir, mapredSysPerms, replication);\n    uploadJobJar(job, jobJar, submitJobDir, replication);\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n        statCache);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
      "extendedDetails": {}
    },
    "5f496683fb00ba26a6bf5a506ae87d4bc4088727": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"YARN-7162. Remove XML excludes file format (rkanter)\" - wrong commit message\n\nThis reverts commit 3a8d57a0a2e047b34be82f602a2b6cf5593d2125.\n",
      "commitDate": "18/09/17 10:32 AM",
      "commitName": "5f496683fb00ba26a6bf5a506ae87d4bc4088727",
      "commitAuthor": "Robert Kanter",
      "commitDateOld": "15/09/17 12:00 PM",
      "commitNameOld": "3a8d57a0a2e047b34be82f602a2b6cf5593d2125",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 2.94,
      "commitsBetweenForRepo": 14,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,60 +1,55 @@\n   public void uploadResources(Job job, Path submitJobDir) throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n     short replication \u003d\n         (short) conf.getInt(Job.SUBMIT_REPLICATION,\n             Job.DEFAULT_SUBMIT_REPLICATION);\n \n     if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n       LOG.warn(\"Hadoop command-line option parsing not performed. \"\n           + \"Implement the Tool interface and execute your application \"\n           + \"with ToolRunner to remedy this.\");\n     }\n \n     //\n     // Figure out what fs the JobTracker is using. Copy the\n     // job to it, under a temporary name. This allows DFS to work,\n     // and under the local fs also provides UNIX-like object loading\n     // semantics. (that is, if the job file is deleted right after\n     // submission, we can still run the submission to completion)\n     //\n \n     // Create a number of filenames in the JobTracker\u0027s fs namespace\n     LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n     if (jtFs.exists(submitJobDir)) {\n       throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n           + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n           + \" that directory\");\n     }\n     submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n     submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n     FsPermission mapredSysPerms \u003d\n         new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     mkdirs(jtFs, submitJobDir, mapredSysPerms);\n \n-    if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,\n-        MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\n-      disableErasureCodingForPath(jtFs, submitJobDir);\n-    }\n-\n     Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n     Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n     Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n     String jobJar \u003d job.getJar();\n \n     Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n     checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n \n     uploadFiles(conf, files, submitJobDir, mapredSysPerms, replication);\n     uploadLibJars(conf, libjars, submitJobDir, mapredSysPerms, replication);\n     uploadArchives(conf, archives, submitJobDir, mapredSysPerms, replication);\n     uploadJobJar(job, jobJar, submitJobDir, replication);\n     addLog4jToDistributedCache(job, submitJobDir);\n \n     // set the timestamps of the archives and files\n     // set the public/private visibility of the archives and files\n     ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n         statCache);\n     // get DelegationToken for cached file\n     ClientDistributedCacheManager.getDelegationTokens(conf,\n         job.getCredentials());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void uploadResources(Job job, Path submitJobDir) throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n    short replication \u003d\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker\u0027s fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n          + \" that directory\");\n    }\n    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms \u003d\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    mkdirs(jtFs, submitJobDir, mapredSysPerms);\n\n    Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n    Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n    Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n    String jobJar \u003d job.getJar();\n\n    Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n    checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n\n    uploadFiles(conf, files, submitJobDir, mapredSysPerms, replication);\n    uploadLibJars(conf, libjars, submitJobDir, mapredSysPerms, replication);\n    uploadArchives(conf, archives, submitJobDir, mapredSysPerms, replication);\n    uploadJobJar(job, jobJar, submitJobDir, replication);\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n        statCache);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
      "extendedDetails": {}
    },
    "3a8d57a0a2e047b34be82f602a2b6cf5593d2125": {
      "type": "Ybodychange",
      "commitMessage": "YARN-7162. Remove XML excludes file format (rkanter)\n",
      "commitDate": "15/09/17 12:00 PM",
      "commitName": "3a8d57a0a2e047b34be82f602a2b6cf5593d2125",
      "commitAuthor": "Robert Kanter",
      "commitDateOld": "05/04/17 5:25 PM",
      "commitNameOld": "fc0885da294490c3984c2231a4d35f89b3b520d4",
      "commitAuthorOld": "Daniel Templeton",
      "daysBetweenCommits": 162.77,
      "commitsBetweenForRepo": 1059,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,60 @@\n   public void uploadResources(Job job, Path submitJobDir) throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n     short replication \u003d\n         (short) conf.getInt(Job.SUBMIT_REPLICATION,\n             Job.DEFAULT_SUBMIT_REPLICATION);\n \n     if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n       LOG.warn(\"Hadoop command-line option parsing not performed. \"\n           + \"Implement the Tool interface and execute your application \"\n           + \"with ToolRunner to remedy this.\");\n     }\n \n     //\n     // Figure out what fs the JobTracker is using. Copy the\n     // job to it, under a temporary name. This allows DFS to work,\n     // and under the local fs also provides UNIX-like object loading\n     // semantics. (that is, if the job file is deleted right after\n     // submission, we can still run the submission to completion)\n     //\n \n     // Create a number of filenames in the JobTracker\u0027s fs namespace\n     LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n     if (jtFs.exists(submitJobDir)) {\n       throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n           + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n           + \" that directory\");\n     }\n     submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n     submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n     FsPermission mapredSysPerms \u003d\n         new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     mkdirs(jtFs, submitJobDir, mapredSysPerms);\n \n+    if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,\n+        MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\n+      disableErasureCodingForPath(jtFs, submitJobDir);\n+    }\n+\n     Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n     Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n     Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n     String jobJar \u003d job.getJar();\n \n     Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n     checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n \n     uploadFiles(conf, files, submitJobDir, mapredSysPerms, replication);\n     uploadLibJars(conf, libjars, submitJobDir, mapredSysPerms, replication);\n     uploadArchives(conf, archives, submitJobDir, mapredSysPerms, replication);\n     uploadJobJar(job, jobJar, submitJobDir, replication);\n     addLog4jToDistributedCache(job, submitJobDir);\n \n     // set the timestamps of the archives and files\n     // set the public/private visibility of the archives and files\n     ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n         statCache);\n     // get DelegationToken for cached file\n     ClientDistributedCacheManager.getDelegationTokens(conf,\n         job.getCredentials());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void uploadResources(Job job, Path submitJobDir) throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n    short replication \u003d\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker\u0027s fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n          + \" that directory\");\n    }\n    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms \u003d\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    mkdirs(jtFs, submitJobDir, mapredSysPerms);\n\n    if (!conf.getBoolean(MRJobConfig.MR_AM_STAGING_DIR_ERASURECODING_ENABLED,\n        MRJobConfig.DEFAULT_MR_AM_STAGING_ERASURECODING_ENABLED)) {\n      disableErasureCodingForPath(jtFs, submitJobDir);\n    }\n\n    Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n    Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n    Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n    String jobJar \u003d job.getJar();\n\n    Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n    checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n\n    uploadFiles(conf, files, submitJobDir, mapredSysPerms, replication);\n    uploadLibJars(conf, libjars, submitJobDir, mapredSysPerms, replication);\n    uploadArchives(conf, archives, submitJobDir, mapredSysPerms, replication);\n    uploadJobJar(job, jobJar, submitJobDir, replication);\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n        statCache);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
      "extendedDetails": {}
    },
    "fc0885da294490c3984c2231a4d35f89b3b520d4": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6846. Fragments specified for libjar paths are not handled correctly\n(Contributed by Chris Trezzo via Daniel Templeton)\n",
      "commitDate": "05/04/17 5:25 PM",
      "commitName": "fc0885da294490c3984c2231a4d35f89b3b520d4",
      "commitAuthor": "Daniel Templeton",
      "commitDateOld": "29/03/17 5:41 PM",
      "commitNameOld": "ceab00ac62f8057a07b4b936799e6f04271e6e41",
      "commitAuthorOld": "Ming Ma",
      "daysBetweenCommits": 6.99,
      "commitsBetweenForRepo": 46,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,55 +1,55 @@\n   public void uploadResources(Job job, Path submitJobDir) throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n     short replication \u003d\n         (short) conf.getInt(Job.SUBMIT_REPLICATION,\n             Job.DEFAULT_SUBMIT_REPLICATION);\n \n     if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n       LOG.warn(\"Hadoop command-line option parsing not performed. \"\n           + \"Implement the Tool interface and execute your application \"\n           + \"with ToolRunner to remedy this.\");\n     }\n \n     //\n     // Figure out what fs the JobTracker is using. Copy the\n     // job to it, under a temporary name. This allows DFS to work,\n     // and under the local fs also provides UNIX-like object loading\n     // semantics. (that is, if the job file is deleted right after\n     // submission, we can still run the submission to completion)\n     //\n \n     // Create a number of filenames in the JobTracker\u0027s fs namespace\n     LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n     if (jtFs.exists(submitJobDir)) {\n       throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n           + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n           + \" that directory\");\n     }\n     submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n     submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n     FsPermission mapredSysPerms \u003d\n         new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n-    FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms);\n+    mkdirs(jtFs, submitJobDir, mapredSysPerms);\n \n     Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n     Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n     Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n     String jobJar \u003d job.getJar();\n \n     Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n     checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n \n     uploadFiles(conf, files, submitJobDir, mapredSysPerms, replication);\n     uploadLibJars(conf, libjars, submitJobDir, mapredSysPerms, replication);\n     uploadArchives(conf, archives, submitJobDir, mapredSysPerms, replication);\n     uploadJobJar(job, jobJar, submitJobDir, replication);\n     addLog4jToDistributedCache(job, submitJobDir);\n \n     // set the timestamps of the archives and files\n     // set the public/private visibility of the archives and files\n     ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n         statCache);\n     // get DelegationToken for cached file\n     ClientDistributedCacheManager.getDelegationTokens(conf,\n         job.getCredentials());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void uploadResources(Job job, Path submitJobDir) throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n    short replication \u003d\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker\u0027s fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n          + \" that directory\");\n    }\n    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms \u003d\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    mkdirs(jtFs, submitJobDir, mapredSysPerms);\n\n    Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n    Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n    Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n    String jobJar \u003d job.getJar();\n\n    Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n    checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n\n    uploadFiles(conf, files, submitJobDir, mapredSysPerms, replication);\n    uploadLibJars(conf, libjars, submitJobDir, mapredSysPerms, replication);\n    uploadArchives(conf, archives, submitJobDir, mapredSysPerms, replication);\n    uploadJobJar(job, jobJar, submitJobDir, replication);\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n        statCache);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
      "extendedDetails": {}
    },
    "f80a7298325a4626638ee24467e2012442e480d4": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6690. Limit the number of resources a single map reduce job can submit for localization. Contributed by Chris Trezzo\n",
      "commitDate": "17/08/16 9:22 AM",
      "commitName": "f80a7298325a4626638ee24467e2012442e480d4",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "19/07/16 8:15 PM",
      "commitNameOld": "8f0d3d69d65a252439610e6f13d679808d768569",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 28.55,
      "commitsBetweenForRepo": 200,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,48 +1,55 @@\n   public void uploadResources(Job job, Path submitJobDir) throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n     short replication \u003d\n         (short) conf.getInt(Job.SUBMIT_REPLICATION,\n             Job.DEFAULT_SUBMIT_REPLICATION);\n \n     if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n       LOG.warn(\"Hadoop command-line option parsing not performed. \"\n           + \"Implement the Tool interface and execute your application \"\n           + \"with ToolRunner to remedy this.\");\n     }\n \n     //\n     // Figure out what fs the JobTracker is using. Copy the\n     // job to it, under a temporary name. This allows DFS to work,\n     // and under the local fs also provides UNIX-like object loading\n     // semantics. (that is, if the job file is deleted right after\n     // submission, we can still run the submission to completion)\n     //\n \n     // Create a number of filenames in the JobTracker\u0027s fs namespace\n     LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n     if (jtFs.exists(submitJobDir)) {\n       throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n           + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n           + \" that directory\");\n     }\n     submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n     submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n     FsPermission mapredSysPerms \u003d\n         new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms);\n-    // add all the command line files/ jars and archive\n-    // first copy them to jobtrackers filesystem\n \n-    uploadFiles(conf, submitJobDir, mapredSysPerms, replication);\n-    uploadLibJars(conf, submitJobDir, mapredSysPerms, replication);\n-    uploadArchives(conf, submitJobDir, mapredSysPerms, replication);\n-    uploadJobJar(job, submitJobDir, replication);\n+    Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n+    Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n+    Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n+    String jobJar \u003d job.getJar();\n+\n+    Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n+    checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n+\n+    uploadFiles(conf, files, submitJobDir, mapredSysPerms, replication);\n+    uploadLibJars(conf, libjars, submitJobDir, mapredSysPerms, replication);\n+    uploadArchives(conf, archives, submitJobDir, mapredSysPerms, replication);\n+    uploadJobJar(job, jobJar, submitJobDir, replication);\n     addLog4jToDistributedCache(job, submitJobDir);\n \n     // set the timestamps of the archives and files\n     // set the public/private visibility of the archives and files\n-    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf);\n+    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n+        statCache);\n     // get DelegationToken for cached file\n     ClientDistributedCacheManager.getDelegationTokens(conf,\n         job.getCredentials());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void uploadResources(Job job, Path submitJobDir) throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n    short replication \u003d\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker\u0027s fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n          + \" that directory\");\n    }\n    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms \u003d\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms);\n\n    Collection\u003cString\u003e files \u003d conf.getStringCollection(\"tmpfiles\");\n    Collection\u003cString\u003e libjars \u003d conf.getStringCollection(\"tmpjars\");\n    Collection\u003cString\u003e archives \u003d conf.getStringCollection(\"tmparchives\");\n    String jobJar \u003d job.getJar();\n\n    Map\u003cURI, FileStatus\u003e statCache \u003d new HashMap\u003cURI, FileStatus\u003e();\n    checkLocalizationLimits(conf, files, libjars, archives, jobJar, statCache);\n\n    uploadFiles(conf, files, submitJobDir, mapredSysPerms, replication);\n    uploadLibJars(conf, libjars, submitJobDir, mapredSysPerms, replication);\n    uploadArchives(conf, archives, submitJobDir, mapredSysPerms, replication);\n    uploadJobJar(job, jobJar, submitJobDir, replication);\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf,\n        statCache);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
      "extendedDetails": {}
    },
    "8f0d3d69d65a252439610e6f13d679808d768569": {
      "type": "Ymultichange(Yrename,Ybodychange)",
      "commitMessage": "MAPREDUCE-6365. Refactor JobResourceUploader#uploadFilesInternal (Chris Trezzo via sjlee)\n",
      "commitDate": "19/07/16 8:15 PM",
      "commitName": "8f0d3d69d65a252439610e6f13d679808d768569",
      "commitAuthor": "Sangjin Lee",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "MAPREDUCE-6365. Refactor JobResourceUploader#uploadFilesInternal (Chris Trezzo via sjlee)\n",
          "commitDate": "19/07/16 8:15 PM",
          "commitName": "8f0d3d69d65a252439610e6f13d679808d768569",
          "commitAuthor": "Sangjin Lee",
          "commitDateOld": "21/06/16 11:25 AM",
          "commitNameOld": "605b4b61364781fc99ed27035c793153a20d8f71",
          "commitAuthorOld": "Sangjin Lee",
          "daysBetweenCommits": 28.37,
          "commitsBetweenForRepo": 320,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,137 +1,48 @@\n-  public void uploadFiles(Job job, Path submitJobDir) throws IOException {\n+  public void uploadResources(Job job, Path submitJobDir) throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n     short replication \u003d\n         (short) conf.getInt(Job.SUBMIT_REPLICATION,\n             Job.DEFAULT_SUBMIT_REPLICATION);\n \n     if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n       LOG.warn(\"Hadoop command-line option parsing not performed. \"\n           + \"Implement the Tool interface and execute your application \"\n           + \"with ToolRunner to remedy this.\");\n     }\n \n-    // get all the command line arguments passed in by the user conf\n-    String files \u003d conf.get(\"tmpfiles\");\n-    String libjars \u003d conf.get(\"tmpjars\");\n-    String archives \u003d conf.get(\"tmparchives\");\n-    String jobJar \u003d job.getJar();\n-\n     //\n     // Figure out what fs the JobTracker is using. Copy the\n     // job to it, under a temporary name. This allows DFS to work,\n     // and under the local fs also provides UNIX-like object loading\n     // semantics. (that is, if the job file is deleted right after\n     // submission, we can still run the submission to completion)\n     //\n \n     // Create a number of filenames in the JobTracker\u0027s fs namespace\n     LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n     if (jtFs.exists(submitJobDir)) {\n       throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n           + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n           + \" that directory\");\n     }\n     submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n     submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n     FsPermission mapredSysPerms \u003d\n         new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms);\n-    Path filesDir \u003d JobSubmissionFiles.getJobDistCacheFiles(submitJobDir);\n-    Path archivesDir \u003d JobSubmissionFiles.getJobDistCacheArchives(submitJobDir);\n-    Path libjarsDir \u003d JobSubmissionFiles.getJobDistCacheLibjars(submitJobDir);\n     // add all the command line files/ jars and archive\n     // first copy them to jobtrackers filesystem\n \n-    if (files !\u003d null) {\n-      FileSystem.mkdirs(jtFs, filesDir, mapredSysPerms);\n-      String[] fileArr \u003d files.split(\",\");\n-      for (String tmpFile : fileArr) {\n-        URI tmpURI \u003d null;\n-        try {\n-          tmpURI \u003d new URI(tmpFile);\n-        } catch (URISyntaxException e) {\n-          throw new IllegalArgumentException(e);\n-        }\n-        Path tmp \u003d new Path(tmpURI);\n-        Path newPath \u003d copyRemoteFiles(filesDir, tmp, conf, replication);\n-        try {\n-          URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n-          DistributedCache.addCacheFile(pathURI, conf);\n-        } catch (URISyntaxException ue) {\n-          // should not throw a uri exception\n-          throw new IOException(\"Failed to create uri for \" + tmpFile, ue);\n-        }\n-      }\n-    }\n-\n-    if (libjars !\u003d null) {\n-      FileSystem.mkdirs(jtFs, libjarsDir, mapredSysPerms);\n-      String[] libjarsArr \u003d libjars.split(\",\");\n-      for (String tmpjars : libjarsArr) {\n-        Path tmp \u003d new Path(tmpjars);\n-        Path newPath \u003d copyRemoteFiles(libjarsDir, tmp, conf, replication);\n-\n-        // Add each file to the classpath\n-        DistributedCache.addFileToClassPath(\n-            new Path(newPath.toUri().getPath()), conf, jtFs, !useWildcard);\n-      }\n-\n-      if (useWildcard) {\n-        // Add the whole directory to the cache\n-        Path libJarsDirWildcard \u003d\n-            jtFs.makeQualified(new Path(libjarsDir, DistributedCache.WILDCARD));\n-\n-        DistributedCache.addCacheFile(libJarsDirWildcard.toUri(), conf);\n-      }\n-    }\n-\n-    if (archives !\u003d null) {\n-      FileSystem.mkdirs(jtFs, archivesDir, mapredSysPerms);\n-      String[] archivesArr \u003d archives.split(\",\");\n-      for (String tmpArchives : archivesArr) {\n-        URI tmpURI;\n-        try {\n-          tmpURI \u003d new URI(tmpArchives);\n-        } catch (URISyntaxException e) {\n-          throw new IllegalArgumentException(e);\n-        }\n-        Path tmp \u003d new Path(tmpURI);\n-        Path newPath \u003d copyRemoteFiles(archivesDir, tmp, conf, replication);\n-        try {\n-          URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n-          DistributedCache.addCacheArchive(pathURI, conf);\n-        } catch (URISyntaxException ue) {\n-          // should not throw an uri excpetion\n-          throw new IOException(\"Failed to create uri for \" + tmpArchives, ue);\n-        }\n-      }\n-    }\n-\n-    if (jobJar !\u003d null) { // copy jar to JobTracker\u0027s fs\n-      // use jar name if job is not named.\n-      if (\"\".equals(job.getJobName())) {\n-        job.setJobName(new Path(jobJar).getName());\n-      }\n-      Path jobJarPath \u003d new Path(jobJar);\n-      URI jobJarURI \u003d jobJarPath.toUri();\n-      // If the job jar is already in a global fs,\n-      // we don\u0027t need to copy it from local fs\n-      if (jobJarURI.getScheme() \u003d\u003d null || jobJarURI.getScheme().equals(\"file\")) {\n-        copyJar(jobJarPath, JobSubmissionFiles.getJobJar(submitJobDir),\n-            replication);\n-        job.setJar(JobSubmissionFiles.getJobJar(submitJobDir).toString());\n-      }\n-    } else {\n-      LOG.warn(\"No job jar file set.  User classes may not be found. \"\n-          + \"See Job or Job#setJar(String).\");\n-    }\n-\n+    uploadFiles(conf, submitJobDir, mapredSysPerms, replication);\n+    uploadLibJars(conf, submitJobDir, mapredSysPerms, replication);\n+    uploadArchives(conf, submitJobDir, mapredSysPerms, replication);\n+    uploadJobJar(job, submitJobDir, replication);\n     addLog4jToDistributedCache(job, submitJobDir);\n \n     // set the timestamps of the archives and files\n     // set the public/private visibility of the archives and files\n     ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf);\n     // get DelegationToken for cached file\n     ClientDistributedCacheManager.getDelegationTokens(conf,\n         job.getCredentials());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void uploadResources(Job job, Path submitJobDir) throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n    short replication \u003d\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker\u0027s fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n          + \" that directory\");\n    }\n    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms \u003d\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms);\n    // add all the command line files/ jars and archive\n    // first copy them to jobtrackers filesystem\n\n    uploadFiles(conf, submitJobDir, mapredSysPerms, replication);\n    uploadLibJars(conf, submitJobDir, mapredSysPerms, replication);\n    uploadArchives(conf, submitJobDir, mapredSysPerms, replication);\n    uploadJobJar(job, submitJobDir, replication);\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
          "extendedDetails": {
            "oldValue": "uploadFiles",
            "newValue": "uploadResources"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-6365. Refactor JobResourceUploader#uploadFilesInternal (Chris Trezzo via sjlee)\n",
          "commitDate": "19/07/16 8:15 PM",
          "commitName": "8f0d3d69d65a252439610e6f13d679808d768569",
          "commitAuthor": "Sangjin Lee",
          "commitDateOld": "21/06/16 11:25 AM",
          "commitNameOld": "605b4b61364781fc99ed27035c793153a20d8f71",
          "commitAuthorOld": "Sangjin Lee",
          "daysBetweenCommits": 28.37,
          "commitsBetweenForRepo": 320,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,137 +1,48 @@\n-  public void uploadFiles(Job job, Path submitJobDir) throws IOException {\n+  public void uploadResources(Job job, Path submitJobDir) throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n     short replication \u003d\n         (short) conf.getInt(Job.SUBMIT_REPLICATION,\n             Job.DEFAULT_SUBMIT_REPLICATION);\n \n     if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n       LOG.warn(\"Hadoop command-line option parsing not performed. \"\n           + \"Implement the Tool interface and execute your application \"\n           + \"with ToolRunner to remedy this.\");\n     }\n \n-    // get all the command line arguments passed in by the user conf\n-    String files \u003d conf.get(\"tmpfiles\");\n-    String libjars \u003d conf.get(\"tmpjars\");\n-    String archives \u003d conf.get(\"tmparchives\");\n-    String jobJar \u003d job.getJar();\n-\n     //\n     // Figure out what fs the JobTracker is using. Copy the\n     // job to it, under a temporary name. This allows DFS to work,\n     // and under the local fs also provides UNIX-like object loading\n     // semantics. (that is, if the job file is deleted right after\n     // submission, we can still run the submission to completion)\n     //\n \n     // Create a number of filenames in the JobTracker\u0027s fs namespace\n     LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n     if (jtFs.exists(submitJobDir)) {\n       throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n           + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n           + \" that directory\");\n     }\n     submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n     submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n     FsPermission mapredSysPerms \u003d\n         new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms);\n-    Path filesDir \u003d JobSubmissionFiles.getJobDistCacheFiles(submitJobDir);\n-    Path archivesDir \u003d JobSubmissionFiles.getJobDistCacheArchives(submitJobDir);\n-    Path libjarsDir \u003d JobSubmissionFiles.getJobDistCacheLibjars(submitJobDir);\n     // add all the command line files/ jars and archive\n     // first copy them to jobtrackers filesystem\n \n-    if (files !\u003d null) {\n-      FileSystem.mkdirs(jtFs, filesDir, mapredSysPerms);\n-      String[] fileArr \u003d files.split(\",\");\n-      for (String tmpFile : fileArr) {\n-        URI tmpURI \u003d null;\n-        try {\n-          tmpURI \u003d new URI(tmpFile);\n-        } catch (URISyntaxException e) {\n-          throw new IllegalArgumentException(e);\n-        }\n-        Path tmp \u003d new Path(tmpURI);\n-        Path newPath \u003d copyRemoteFiles(filesDir, tmp, conf, replication);\n-        try {\n-          URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n-          DistributedCache.addCacheFile(pathURI, conf);\n-        } catch (URISyntaxException ue) {\n-          // should not throw a uri exception\n-          throw new IOException(\"Failed to create uri for \" + tmpFile, ue);\n-        }\n-      }\n-    }\n-\n-    if (libjars !\u003d null) {\n-      FileSystem.mkdirs(jtFs, libjarsDir, mapredSysPerms);\n-      String[] libjarsArr \u003d libjars.split(\",\");\n-      for (String tmpjars : libjarsArr) {\n-        Path tmp \u003d new Path(tmpjars);\n-        Path newPath \u003d copyRemoteFiles(libjarsDir, tmp, conf, replication);\n-\n-        // Add each file to the classpath\n-        DistributedCache.addFileToClassPath(\n-            new Path(newPath.toUri().getPath()), conf, jtFs, !useWildcard);\n-      }\n-\n-      if (useWildcard) {\n-        // Add the whole directory to the cache\n-        Path libJarsDirWildcard \u003d\n-            jtFs.makeQualified(new Path(libjarsDir, DistributedCache.WILDCARD));\n-\n-        DistributedCache.addCacheFile(libJarsDirWildcard.toUri(), conf);\n-      }\n-    }\n-\n-    if (archives !\u003d null) {\n-      FileSystem.mkdirs(jtFs, archivesDir, mapredSysPerms);\n-      String[] archivesArr \u003d archives.split(\",\");\n-      for (String tmpArchives : archivesArr) {\n-        URI tmpURI;\n-        try {\n-          tmpURI \u003d new URI(tmpArchives);\n-        } catch (URISyntaxException e) {\n-          throw new IllegalArgumentException(e);\n-        }\n-        Path tmp \u003d new Path(tmpURI);\n-        Path newPath \u003d copyRemoteFiles(archivesDir, tmp, conf, replication);\n-        try {\n-          URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n-          DistributedCache.addCacheArchive(pathURI, conf);\n-        } catch (URISyntaxException ue) {\n-          // should not throw an uri excpetion\n-          throw new IOException(\"Failed to create uri for \" + tmpArchives, ue);\n-        }\n-      }\n-    }\n-\n-    if (jobJar !\u003d null) { // copy jar to JobTracker\u0027s fs\n-      // use jar name if job is not named.\n-      if (\"\".equals(job.getJobName())) {\n-        job.setJobName(new Path(jobJar).getName());\n-      }\n-      Path jobJarPath \u003d new Path(jobJar);\n-      URI jobJarURI \u003d jobJarPath.toUri();\n-      // If the job jar is already in a global fs,\n-      // we don\u0027t need to copy it from local fs\n-      if (jobJarURI.getScheme() \u003d\u003d null || jobJarURI.getScheme().equals(\"file\")) {\n-        copyJar(jobJarPath, JobSubmissionFiles.getJobJar(submitJobDir),\n-            replication);\n-        job.setJar(JobSubmissionFiles.getJobJar(submitJobDir).toString());\n-      }\n-    } else {\n-      LOG.warn(\"No job jar file set.  User classes may not be found. \"\n-          + \"See Job or Job#setJar(String).\");\n-    }\n-\n+    uploadFiles(conf, submitJobDir, mapredSysPerms, replication);\n+    uploadLibJars(conf, submitJobDir, mapredSysPerms, replication);\n+    uploadArchives(conf, submitJobDir, mapredSysPerms, replication);\n+    uploadJobJar(job, submitJobDir, replication);\n     addLog4jToDistributedCache(job, submitJobDir);\n \n     // set the timestamps of the archives and files\n     // set the public/private visibility of the archives and files\n     ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf);\n     // get DelegationToken for cached file\n     ClientDistributedCacheManager.getDelegationTokens(conf,\n         job.getCredentials());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void uploadResources(Job job, Path submitJobDir) throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n    short replication \u003d\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker\u0027s fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n          + \" that directory\");\n    }\n    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms \u003d\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms);\n    // add all the command line files/ jars and archive\n    // first copy them to jobtrackers filesystem\n\n    uploadFiles(conf, submitJobDir, mapredSysPerms, replication);\n    uploadLibJars(conf, submitJobDir, mapredSysPerms, replication);\n    uploadArchives(conf, submitJobDir, mapredSysPerms, replication);\n    uploadJobJar(job, submitJobDir, replication);\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
          "extendedDetails": {}
        }
      ]
    },
    "605b4b61364781fc99ed27035c793153a20d8f71": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6719. The list of -libjars archives should be replaced with a wildcard in the distributed cache to reduce the application footprint in the state store (Daniel Templeton via sjlee)\n",
      "commitDate": "21/06/16 11:25 AM",
      "commitName": "605b4b61364781fc99ed27035c793153a20d8f71",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "30/06/15 4:49 PM",
      "commitNameOld": "3a72bfd08281fd271bef4f41289125d39c41928c",
      "commitAuthorOld": "Robert Kanter",
      "daysBetweenCommits": 356.77,
      "commitsBetweenForRepo": 2336,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,127 +1,137 @@\n   public void uploadFiles(Job job, Path submitJobDir) throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n     short replication \u003d\n         (short) conf.getInt(Job.SUBMIT_REPLICATION,\n             Job.DEFAULT_SUBMIT_REPLICATION);\n \n     if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n       LOG.warn(\"Hadoop command-line option parsing not performed. \"\n           + \"Implement the Tool interface and execute your application \"\n           + \"with ToolRunner to remedy this.\");\n     }\n \n     // get all the command line arguments passed in by the user conf\n     String files \u003d conf.get(\"tmpfiles\");\n     String libjars \u003d conf.get(\"tmpjars\");\n     String archives \u003d conf.get(\"tmparchives\");\n     String jobJar \u003d job.getJar();\n \n     //\n     // Figure out what fs the JobTracker is using. Copy the\n     // job to it, under a temporary name. This allows DFS to work,\n     // and under the local fs also provides UNIX-like object loading\n     // semantics. (that is, if the job file is deleted right after\n     // submission, we can still run the submission to completion)\n     //\n \n     // Create a number of filenames in the JobTracker\u0027s fs namespace\n     LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n     if (jtFs.exists(submitJobDir)) {\n       throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n           + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n           + \" that directory\");\n     }\n     submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n     submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n     FsPermission mapredSysPerms \u003d\n         new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms);\n     Path filesDir \u003d JobSubmissionFiles.getJobDistCacheFiles(submitJobDir);\n     Path archivesDir \u003d JobSubmissionFiles.getJobDistCacheArchives(submitJobDir);\n     Path libjarsDir \u003d JobSubmissionFiles.getJobDistCacheLibjars(submitJobDir);\n     // add all the command line files/ jars and archive\n     // first copy them to jobtrackers filesystem\n \n     if (files !\u003d null) {\n       FileSystem.mkdirs(jtFs, filesDir, mapredSysPerms);\n       String[] fileArr \u003d files.split(\",\");\n       for (String tmpFile : fileArr) {\n         URI tmpURI \u003d null;\n         try {\n           tmpURI \u003d new URI(tmpFile);\n         } catch (URISyntaxException e) {\n           throw new IllegalArgumentException(e);\n         }\n         Path tmp \u003d new Path(tmpURI);\n         Path newPath \u003d copyRemoteFiles(filesDir, tmp, conf, replication);\n         try {\n           URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n           DistributedCache.addCacheFile(pathURI, conf);\n         } catch (URISyntaxException ue) {\n           // should not throw a uri exception\n           throw new IOException(\"Failed to create uri for \" + tmpFile, ue);\n         }\n       }\n     }\n \n     if (libjars !\u003d null) {\n       FileSystem.mkdirs(jtFs, libjarsDir, mapredSysPerms);\n       String[] libjarsArr \u003d libjars.split(\",\");\n       for (String tmpjars : libjarsArr) {\n         Path tmp \u003d new Path(tmpjars);\n         Path newPath \u003d copyRemoteFiles(libjarsDir, tmp, conf, replication);\n+\n+        // Add each file to the classpath\n         DistributedCache.addFileToClassPath(\n-            new Path(newPath.toUri().getPath()), conf, jtFs);\n+            new Path(newPath.toUri().getPath()), conf, jtFs, !useWildcard);\n+      }\n+\n+      if (useWildcard) {\n+        // Add the whole directory to the cache\n+        Path libJarsDirWildcard \u003d\n+            jtFs.makeQualified(new Path(libjarsDir, DistributedCache.WILDCARD));\n+\n+        DistributedCache.addCacheFile(libJarsDirWildcard.toUri(), conf);\n       }\n     }\n \n     if (archives !\u003d null) {\n       FileSystem.mkdirs(jtFs, archivesDir, mapredSysPerms);\n       String[] archivesArr \u003d archives.split(\",\");\n       for (String tmpArchives : archivesArr) {\n         URI tmpURI;\n         try {\n           tmpURI \u003d new URI(tmpArchives);\n         } catch (URISyntaxException e) {\n           throw new IllegalArgumentException(e);\n         }\n         Path tmp \u003d new Path(tmpURI);\n         Path newPath \u003d copyRemoteFiles(archivesDir, tmp, conf, replication);\n         try {\n           URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n           DistributedCache.addCacheArchive(pathURI, conf);\n         } catch (URISyntaxException ue) {\n           // should not throw an uri excpetion\n           throw new IOException(\"Failed to create uri for \" + tmpArchives, ue);\n         }\n       }\n     }\n \n     if (jobJar !\u003d null) { // copy jar to JobTracker\u0027s fs\n       // use jar name if job is not named.\n       if (\"\".equals(job.getJobName())) {\n         job.setJobName(new Path(jobJar).getName());\n       }\n       Path jobJarPath \u003d new Path(jobJar);\n       URI jobJarURI \u003d jobJarPath.toUri();\n       // If the job jar is already in a global fs,\n       // we don\u0027t need to copy it from local fs\n       if (jobJarURI.getScheme() \u003d\u003d null || jobJarURI.getScheme().equals(\"file\")) {\n         copyJar(jobJarPath, JobSubmissionFiles.getJobJar(submitJobDir),\n             replication);\n         job.setJar(JobSubmissionFiles.getJobJar(submitJobDir).toString());\n       }\n     } else {\n       LOG.warn(\"No job jar file set.  User classes may not be found. \"\n           + \"See Job or Job#setJar(String).\");\n     }\n \n     addLog4jToDistributedCache(job, submitJobDir);\n \n     // set the timestamps of the archives and files\n     // set the public/private visibility of the archives and files\n     ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf);\n     // get DelegationToken for cached file\n     ClientDistributedCacheManager.getDelegationTokens(conf,\n         job.getCredentials());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void uploadFiles(Job job, Path submitJobDir) throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n    short replication \u003d\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    // get all the command line arguments passed in by the user conf\n    String files \u003d conf.get(\"tmpfiles\");\n    String libjars \u003d conf.get(\"tmpjars\");\n    String archives \u003d conf.get(\"tmparchives\");\n    String jobJar \u003d job.getJar();\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker\u0027s fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n          + \" that directory\");\n    }\n    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms \u003d\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms);\n    Path filesDir \u003d JobSubmissionFiles.getJobDistCacheFiles(submitJobDir);\n    Path archivesDir \u003d JobSubmissionFiles.getJobDistCacheArchives(submitJobDir);\n    Path libjarsDir \u003d JobSubmissionFiles.getJobDistCacheLibjars(submitJobDir);\n    // add all the command line files/ jars and archive\n    // first copy them to jobtrackers filesystem\n\n    if (files !\u003d null) {\n      FileSystem.mkdirs(jtFs, filesDir, mapredSysPerms);\n      String[] fileArr \u003d files.split(\",\");\n      for (String tmpFile : fileArr) {\n        URI tmpURI \u003d null;\n        try {\n          tmpURI \u003d new URI(tmpFile);\n        } catch (URISyntaxException e) {\n          throw new IllegalArgumentException(e);\n        }\n        Path tmp \u003d new Path(tmpURI);\n        Path newPath \u003d copyRemoteFiles(filesDir, tmp, conf, replication);\n        try {\n          URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n          DistributedCache.addCacheFile(pathURI, conf);\n        } catch (URISyntaxException ue) {\n          // should not throw a uri exception\n          throw new IOException(\"Failed to create uri for \" + tmpFile, ue);\n        }\n      }\n    }\n\n    if (libjars !\u003d null) {\n      FileSystem.mkdirs(jtFs, libjarsDir, mapredSysPerms);\n      String[] libjarsArr \u003d libjars.split(\",\");\n      for (String tmpjars : libjarsArr) {\n        Path tmp \u003d new Path(tmpjars);\n        Path newPath \u003d copyRemoteFiles(libjarsDir, tmp, conf, replication);\n\n        // Add each file to the classpath\n        DistributedCache.addFileToClassPath(\n            new Path(newPath.toUri().getPath()), conf, jtFs, !useWildcard);\n      }\n\n      if (useWildcard) {\n        // Add the whole directory to the cache\n        Path libJarsDirWildcard \u003d\n            jtFs.makeQualified(new Path(libjarsDir, DistributedCache.WILDCARD));\n\n        DistributedCache.addCacheFile(libJarsDirWildcard.toUri(), conf);\n      }\n    }\n\n    if (archives !\u003d null) {\n      FileSystem.mkdirs(jtFs, archivesDir, mapredSysPerms);\n      String[] archivesArr \u003d archives.split(\",\");\n      for (String tmpArchives : archivesArr) {\n        URI tmpURI;\n        try {\n          tmpURI \u003d new URI(tmpArchives);\n        } catch (URISyntaxException e) {\n          throw new IllegalArgumentException(e);\n        }\n        Path tmp \u003d new Path(tmpURI);\n        Path newPath \u003d copyRemoteFiles(archivesDir, tmp, conf, replication);\n        try {\n          URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n          DistributedCache.addCacheArchive(pathURI, conf);\n        } catch (URISyntaxException ue) {\n          // should not throw an uri excpetion\n          throw new IOException(\"Failed to create uri for \" + tmpArchives, ue);\n        }\n      }\n    }\n\n    if (jobJar !\u003d null) { // copy jar to JobTracker\u0027s fs\n      // use jar name if job is not named.\n      if (\"\".equals(job.getJobName())) {\n        job.setJobName(new Path(jobJar).getName());\n      }\n      Path jobJarPath \u003d new Path(jobJar);\n      URI jobJarURI \u003d jobJarPath.toUri();\n      // If the job jar is already in a global fs,\n      // we don\u0027t need to copy it from local fs\n      if (jobJarURI.getScheme() \u003d\u003d null || jobJarURI.getScheme().equals(\"file\")) {\n        copyJar(jobJarPath, JobSubmissionFiles.getJobJar(submitJobDir),\n            replication);\n        job.setJar(JobSubmissionFiles.getJobJar(submitJobDir).toString());\n      }\n    } else {\n      LOG.warn(\"No job jar file set.  User classes may not be found. \"\n          + \"See Job or Job#setJar(String).\");\n    }\n\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
      "extendedDetails": {}
    },
    "d50e8f09287deeb51012d08e326a2ed71a6da869": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6238. MR2 can\u0027t run local jobs with -libjars command options which is a regression from MR1 (zxu via rkanter)\n",
      "commitDate": "20/04/15 2:14 PM",
      "commitName": "d50e8f09287deeb51012d08e326a2ed71a6da869",
      "commitAuthor": "Robert Kanter",
      "commitDateOld": "04/03/15 2:42 PM",
      "commitNameOld": "c66c3ac6bf9f63177279feec3f2917e4b882e2bc",
      "commitAuthorOld": "Karthik Kambatla",
      "daysBetweenCommits": 46.94,
      "commitsBetweenForRepo": 384,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,127 +1,127 @@\n   public void uploadFiles(Job job, Path submitJobDir) throws IOException {\n     Configuration conf \u003d job.getConfiguration();\n     short replication \u003d\n         (short) conf.getInt(Job.SUBMIT_REPLICATION,\n             Job.DEFAULT_SUBMIT_REPLICATION);\n \n     if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n       LOG.warn(\"Hadoop command-line option parsing not performed. \"\n           + \"Implement the Tool interface and execute your application \"\n           + \"with ToolRunner to remedy this.\");\n     }\n \n     // get all the command line arguments passed in by the user conf\n     String files \u003d conf.get(\"tmpfiles\");\n     String libjars \u003d conf.get(\"tmpjars\");\n     String archives \u003d conf.get(\"tmparchives\");\n     String jobJar \u003d job.getJar();\n \n     //\n     // Figure out what fs the JobTracker is using. Copy the\n     // job to it, under a temporary name. This allows DFS to work,\n     // and under the local fs also provides UNIX-like object loading\n     // semantics. (that is, if the job file is deleted right after\n     // submission, we can still run the submission to completion)\n     //\n \n     // Create a number of filenames in the JobTracker\u0027s fs namespace\n     LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n     if (jtFs.exists(submitJobDir)) {\n       throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n           + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n           + \" that directory\");\n     }\n     submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n     submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n     FsPermission mapredSysPerms \u003d\n         new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms);\n     Path filesDir \u003d JobSubmissionFiles.getJobDistCacheFiles(submitJobDir);\n     Path archivesDir \u003d JobSubmissionFiles.getJobDistCacheArchives(submitJobDir);\n     Path libjarsDir \u003d JobSubmissionFiles.getJobDistCacheLibjars(submitJobDir);\n     // add all the command line files/ jars and archive\n     // first copy them to jobtrackers filesystem\n \n     if (files !\u003d null) {\n       FileSystem.mkdirs(jtFs, filesDir, mapredSysPerms);\n       String[] fileArr \u003d files.split(\",\");\n       for (String tmpFile : fileArr) {\n         URI tmpURI \u003d null;\n         try {\n           tmpURI \u003d new URI(tmpFile);\n         } catch (URISyntaxException e) {\n           throw new IllegalArgumentException(e);\n         }\n         Path tmp \u003d new Path(tmpURI);\n         Path newPath \u003d copyRemoteFiles(filesDir, tmp, conf, replication);\n         try {\n           URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n           DistributedCache.addCacheFile(pathURI, conf);\n         } catch (URISyntaxException ue) {\n           // should not throw a uri exception\n           throw new IOException(\"Failed to create uri for \" + tmpFile, ue);\n         }\n       }\n     }\n \n     if (libjars !\u003d null) {\n       FileSystem.mkdirs(jtFs, libjarsDir, mapredSysPerms);\n       String[] libjarsArr \u003d libjars.split(\",\");\n       for (String tmpjars : libjarsArr) {\n         Path tmp \u003d new Path(tmpjars);\n         Path newPath \u003d copyRemoteFiles(libjarsDir, tmp, conf, replication);\n         DistributedCache.addFileToClassPath(\n-            new Path(newPath.toUri().getPath()), conf);\n+            new Path(newPath.toUri().getPath()), conf, jtFs);\n       }\n     }\n \n     if (archives !\u003d null) {\n       FileSystem.mkdirs(jtFs, archivesDir, mapredSysPerms);\n       String[] archivesArr \u003d archives.split(\",\");\n       for (String tmpArchives : archivesArr) {\n         URI tmpURI;\n         try {\n           tmpURI \u003d new URI(tmpArchives);\n         } catch (URISyntaxException e) {\n           throw new IllegalArgumentException(e);\n         }\n         Path tmp \u003d new Path(tmpURI);\n         Path newPath \u003d copyRemoteFiles(archivesDir, tmp, conf, replication);\n         try {\n           URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n           DistributedCache.addCacheArchive(pathURI, conf);\n         } catch (URISyntaxException ue) {\n           // should not throw an uri excpetion\n           throw new IOException(\"Failed to create uri for \" + tmpArchives, ue);\n         }\n       }\n     }\n \n     if (jobJar !\u003d null) { // copy jar to JobTracker\u0027s fs\n       // use jar name if job is not named.\n       if (\"\".equals(job.getJobName())) {\n         job.setJobName(new Path(jobJar).getName());\n       }\n       Path jobJarPath \u003d new Path(jobJar);\n       URI jobJarURI \u003d jobJarPath.toUri();\n       // If the job jar is already in a global fs,\n       // we don\u0027t need to copy it from local fs\n       if (jobJarURI.getScheme() \u003d\u003d null || jobJarURI.getScheme().equals(\"file\")) {\n         copyJar(jobJarPath, JobSubmissionFiles.getJobJar(submitJobDir),\n             replication);\n         job.setJar(JobSubmissionFiles.getJobJar(submitJobDir).toString());\n       }\n     } else {\n       LOG.warn(\"No job jar file set.  User classes may not be found. \"\n           + \"See Job or Job#setJar(String).\");\n     }\n \n     addLog4jToDistributedCache(job, submitJobDir);\n \n     // set the timestamps of the archives and files\n     // set the public/private visibility of the archives and files\n     ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf);\n     // get DelegationToken for cached file\n     ClientDistributedCacheManager.getDelegationTokens(conf,\n         job.getCredentials());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void uploadFiles(Job job, Path submitJobDir) throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n    short replication \u003d\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    // get all the command line arguments passed in by the user conf\n    String files \u003d conf.get(\"tmpfiles\");\n    String libjars \u003d conf.get(\"tmpjars\");\n    String archives \u003d conf.get(\"tmparchives\");\n    String jobJar \u003d job.getJar();\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker\u0027s fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n          + \" that directory\");\n    }\n    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms \u003d\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms);\n    Path filesDir \u003d JobSubmissionFiles.getJobDistCacheFiles(submitJobDir);\n    Path archivesDir \u003d JobSubmissionFiles.getJobDistCacheArchives(submitJobDir);\n    Path libjarsDir \u003d JobSubmissionFiles.getJobDistCacheLibjars(submitJobDir);\n    // add all the command line files/ jars and archive\n    // first copy them to jobtrackers filesystem\n\n    if (files !\u003d null) {\n      FileSystem.mkdirs(jtFs, filesDir, mapredSysPerms);\n      String[] fileArr \u003d files.split(\",\");\n      for (String tmpFile : fileArr) {\n        URI tmpURI \u003d null;\n        try {\n          tmpURI \u003d new URI(tmpFile);\n        } catch (URISyntaxException e) {\n          throw new IllegalArgumentException(e);\n        }\n        Path tmp \u003d new Path(tmpURI);\n        Path newPath \u003d copyRemoteFiles(filesDir, tmp, conf, replication);\n        try {\n          URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n          DistributedCache.addCacheFile(pathURI, conf);\n        } catch (URISyntaxException ue) {\n          // should not throw a uri exception\n          throw new IOException(\"Failed to create uri for \" + tmpFile, ue);\n        }\n      }\n    }\n\n    if (libjars !\u003d null) {\n      FileSystem.mkdirs(jtFs, libjarsDir, mapredSysPerms);\n      String[] libjarsArr \u003d libjars.split(\",\");\n      for (String tmpjars : libjarsArr) {\n        Path tmp \u003d new Path(tmpjars);\n        Path newPath \u003d copyRemoteFiles(libjarsDir, tmp, conf, replication);\n        DistributedCache.addFileToClassPath(\n            new Path(newPath.toUri().getPath()), conf, jtFs);\n      }\n    }\n\n    if (archives !\u003d null) {\n      FileSystem.mkdirs(jtFs, archivesDir, mapredSysPerms);\n      String[] archivesArr \u003d archives.split(\",\");\n      for (String tmpArchives : archivesArr) {\n        URI tmpURI;\n        try {\n          tmpURI \u003d new URI(tmpArchives);\n        } catch (URISyntaxException e) {\n          throw new IllegalArgumentException(e);\n        }\n        Path tmp \u003d new Path(tmpURI);\n        Path newPath \u003d copyRemoteFiles(archivesDir, tmp, conf, replication);\n        try {\n          URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n          DistributedCache.addCacheArchive(pathURI, conf);\n        } catch (URISyntaxException ue) {\n          // should not throw an uri excpetion\n          throw new IOException(\"Failed to create uri for \" + tmpArchives, ue);\n        }\n      }\n    }\n\n    if (jobJar !\u003d null) { // copy jar to JobTracker\u0027s fs\n      // use jar name if job is not named.\n      if (\"\".equals(job.getJobName())) {\n        job.setJobName(new Path(jobJar).getName());\n      }\n      Path jobJarPath \u003d new Path(jobJar);\n      URI jobJarURI \u003d jobJarPath.toUri();\n      // If the job jar is already in a global fs,\n      // we don\u0027t need to copy it from local fs\n      if (jobJarURI.getScheme() \u003d\u003d null || jobJarURI.getScheme().equals(\"file\")) {\n        copyJar(jobJarPath, JobSubmissionFiles.getJobJar(submitJobDir),\n            replication);\n        job.setJar(JobSubmissionFiles.getJobJar(submitJobDir).toString());\n      }\n    } else {\n      LOG.warn(\"No job jar file set.  User classes may not be found. \"\n          + \"See Job or Job#setJar(String).\");\n    }\n\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
      "extendedDetails": {}
    },
    "c66c3ac6bf9f63177279feec3f2917e4b882e2bc": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-6267. Refactor JobSubmitter#copyAndConfigureFiles into it\u0027s own class. (Chris Trezzo via kasha)\n",
      "commitDate": "04/03/15 2:42 PM",
      "commitName": "c66c3ac6bf9f63177279feec3f2917e4b882e2bc",
      "commitAuthor": "Karthik Kambatla",
      "diff": "@@ -0,0 +1,127 @@\n+  public void uploadFiles(Job job, Path submitJobDir) throws IOException {\n+    Configuration conf \u003d job.getConfiguration();\n+    short replication \u003d\n+        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n+            Job.DEFAULT_SUBMIT_REPLICATION);\n+\n+    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n+      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n+          + \"Implement the Tool interface and execute your application \"\n+          + \"with ToolRunner to remedy this.\");\n+    }\n+\n+    // get all the command line arguments passed in by the user conf\n+    String files \u003d conf.get(\"tmpfiles\");\n+    String libjars \u003d conf.get(\"tmpjars\");\n+    String archives \u003d conf.get(\"tmparchives\");\n+    String jobJar \u003d job.getJar();\n+\n+    //\n+    // Figure out what fs the JobTracker is using. Copy the\n+    // job to it, under a temporary name. This allows DFS to work,\n+    // and under the local fs also provides UNIX-like object loading\n+    // semantics. (that is, if the job file is deleted right after\n+    // submission, we can still run the submission to completion)\n+    //\n+\n+    // Create a number of filenames in the JobTracker\u0027s fs namespace\n+    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n+    if (jtFs.exists(submitJobDir)) {\n+      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n+          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n+          + \" that directory\");\n+    }\n+    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n+    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n+    FsPermission mapredSysPerms \u003d\n+        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n+    FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms);\n+    Path filesDir \u003d JobSubmissionFiles.getJobDistCacheFiles(submitJobDir);\n+    Path archivesDir \u003d JobSubmissionFiles.getJobDistCacheArchives(submitJobDir);\n+    Path libjarsDir \u003d JobSubmissionFiles.getJobDistCacheLibjars(submitJobDir);\n+    // add all the command line files/ jars and archive\n+    // first copy them to jobtrackers filesystem\n+\n+    if (files !\u003d null) {\n+      FileSystem.mkdirs(jtFs, filesDir, mapredSysPerms);\n+      String[] fileArr \u003d files.split(\",\");\n+      for (String tmpFile : fileArr) {\n+        URI tmpURI \u003d null;\n+        try {\n+          tmpURI \u003d new URI(tmpFile);\n+        } catch (URISyntaxException e) {\n+          throw new IllegalArgumentException(e);\n+        }\n+        Path tmp \u003d new Path(tmpURI);\n+        Path newPath \u003d copyRemoteFiles(filesDir, tmp, conf, replication);\n+        try {\n+          URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n+          DistributedCache.addCacheFile(pathURI, conf);\n+        } catch (URISyntaxException ue) {\n+          // should not throw a uri exception\n+          throw new IOException(\"Failed to create uri for \" + tmpFile, ue);\n+        }\n+      }\n+    }\n+\n+    if (libjars !\u003d null) {\n+      FileSystem.mkdirs(jtFs, libjarsDir, mapredSysPerms);\n+      String[] libjarsArr \u003d libjars.split(\",\");\n+      for (String tmpjars : libjarsArr) {\n+        Path tmp \u003d new Path(tmpjars);\n+        Path newPath \u003d copyRemoteFiles(libjarsDir, tmp, conf, replication);\n+        DistributedCache.addFileToClassPath(\n+            new Path(newPath.toUri().getPath()), conf);\n+      }\n+    }\n+\n+    if (archives !\u003d null) {\n+      FileSystem.mkdirs(jtFs, archivesDir, mapredSysPerms);\n+      String[] archivesArr \u003d archives.split(\",\");\n+      for (String tmpArchives : archivesArr) {\n+        URI tmpURI;\n+        try {\n+          tmpURI \u003d new URI(tmpArchives);\n+        } catch (URISyntaxException e) {\n+          throw new IllegalArgumentException(e);\n+        }\n+        Path tmp \u003d new Path(tmpURI);\n+        Path newPath \u003d copyRemoteFiles(archivesDir, tmp, conf, replication);\n+        try {\n+          URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n+          DistributedCache.addCacheArchive(pathURI, conf);\n+        } catch (URISyntaxException ue) {\n+          // should not throw an uri excpetion\n+          throw new IOException(\"Failed to create uri for \" + tmpArchives, ue);\n+        }\n+      }\n+    }\n+\n+    if (jobJar !\u003d null) { // copy jar to JobTracker\u0027s fs\n+      // use jar name if job is not named.\n+      if (\"\".equals(job.getJobName())) {\n+        job.setJobName(new Path(jobJar).getName());\n+      }\n+      Path jobJarPath \u003d new Path(jobJar);\n+      URI jobJarURI \u003d jobJarPath.toUri();\n+      // If the job jar is already in a global fs,\n+      // we don\u0027t need to copy it from local fs\n+      if (jobJarURI.getScheme() \u003d\u003d null || jobJarURI.getScheme().equals(\"file\")) {\n+        copyJar(jobJarPath, JobSubmissionFiles.getJobJar(submitJobDir),\n+            replication);\n+        job.setJar(JobSubmissionFiles.getJobJar(submitJobDir).toString());\n+      }\n+    } else {\n+      LOG.warn(\"No job jar file set.  User classes may not be found. \"\n+          + \"See Job or Job#setJar(String).\");\n+    }\n+\n+    addLog4jToDistributedCache(job, submitJobDir);\n+\n+    // set the timestamps of the archives and files\n+    // set the public/private visibility of the archives and files\n+    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf);\n+    // get DelegationToken for cached file\n+    ClientDistributedCacheManager.getDelegationTokens(conf,\n+        job.getCredentials());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void uploadFiles(Job job, Path submitJobDir) throws IOException {\n    Configuration conf \u003d job.getConfiguration();\n    short replication \u003d\n        (short) conf.getInt(Job.SUBMIT_REPLICATION,\n            Job.DEFAULT_SUBMIT_REPLICATION);\n\n    if (!(conf.getBoolean(Job.USED_GENERIC_PARSER, false))) {\n      LOG.warn(\"Hadoop command-line option parsing not performed. \"\n          + \"Implement the Tool interface and execute your application \"\n          + \"with ToolRunner to remedy this.\");\n    }\n\n    // get all the command line arguments passed in by the user conf\n    String files \u003d conf.get(\"tmpfiles\");\n    String libjars \u003d conf.get(\"tmpjars\");\n    String archives \u003d conf.get(\"tmparchives\");\n    String jobJar \u003d job.getJar();\n\n    //\n    // Figure out what fs the JobTracker is using. Copy the\n    // job to it, under a temporary name. This allows DFS to work,\n    // and under the local fs also provides UNIX-like object loading\n    // semantics. (that is, if the job file is deleted right after\n    // submission, we can still run the submission to completion)\n    //\n\n    // Create a number of filenames in the JobTracker\u0027s fs namespace\n    LOG.debug(\"default FileSystem: \" + jtFs.getUri());\n    if (jtFs.exists(submitJobDir)) {\n      throw new IOException(\"Not submitting job. Job directory \" + submitJobDir\n          + \" already exists!! This is unexpected.Please check what\u0027s there in\"\n          + \" that directory\");\n    }\n    submitJobDir \u003d jtFs.makeQualified(submitJobDir);\n    submitJobDir \u003d new Path(submitJobDir.toUri().getPath());\n    FsPermission mapredSysPerms \u003d\n        new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jtFs, submitJobDir, mapredSysPerms);\n    Path filesDir \u003d JobSubmissionFiles.getJobDistCacheFiles(submitJobDir);\n    Path archivesDir \u003d JobSubmissionFiles.getJobDistCacheArchives(submitJobDir);\n    Path libjarsDir \u003d JobSubmissionFiles.getJobDistCacheLibjars(submitJobDir);\n    // add all the command line files/ jars and archive\n    // first copy them to jobtrackers filesystem\n\n    if (files !\u003d null) {\n      FileSystem.mkdirs(jtFs, filesDir, mapredSysPerms);\n      String[] fileArr \u003d files.split(\",\");\n      for (String tmpFile : fileArr) {\n        URI tmpURI \u003d null;\n        try {\n          tmpURI \u003d new URI(tmpFile);\n        } catch (URISyntaxException e) {\n          throw new IllegalArgumentException(e);\n        }\n        Path tmp \u003d new Path(tmpURI);\n        Path newPath \u003d copyRemoteFiles(filesDir, tmp, conf, replication);\n        try {\n          URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n          DistributedCache.addCacheFile(pathURI, conf);\n        } catch (URISyntaxException ue) {\n          // should not throw a uri exception\n          throw new IOException(\"Failed to create uri for \" + tmpFile, ue);\n        }\n      }\n    }\n\n    if (libjars !\u003d null) {\n      FileSystem.mkdirs(jtFs, libjarsDir, mapredSysPerms);\n      String[] libjarsArr \u003d libjars.split(\",\");\n      for (String tmpjars : libjarsArr) {\n        Path tmp \u003d new Path(tmpjars);\n        Path newPath \u003d copyRemoteFiles(libjarsDir, tmp, conf, replication);\n        DistributedCache.addFileToClassPath(\n            new Path(newPath.toUri().getPath()), conf);\n      }\n    }\n\n    if (archives !\u003d null) {\n      FileSystem.mkdirs(jtFs, archivesDir, mapredSysPerms);\n      String[] archivesArr \u003d archives.split(\",\");\n      for (String tmpArchives : archivesArr) {\n        URI tmpURI;\n        try {\n          tmpURI \u003d new URI(tmpArchives);\n        } catch (URISyntaxException e) {\n          throw new IllegalArgumentException(e);\n        }\n        Path tmp \u003d new Path(tmpURI);\n        Path newPath \u003d copyRemoteFiles(archivesDir, tmp, conf, replication);\n        try {\n          URI pathURI \u003d getPathURI(newPath, tmpURI.getFragment());\n          DistributedCache.addCacheArchive(pathURI, conf);\n        } catch (URISyntaxException ue) {\n          // should not throw an uri excpetion\n          throw new IOException(\"Failed to create uri for \" + tmpArchives, ue);\n        }\n      }\n    }\n\n    if (jobJar !\u003d null) { // copy jar to JobTracker\u0027s fs\n      // use jar name if job is not named.\n      if (\"\".equals(job.getJobName())) {\n        job.setJobName(new Path(jobJar).getName());\n      }\n      Path jobJarPath \u003d new Path(jobJar);\n      URI jobJarURI \u003d jobJarPath.toUri();\n      // If the job jar is already in a global fs,\n      // we don\u0027t need to copy it from local fs\n      if (jobJarURI.getScheme() \u003d\u003d null || jobJarURI.getScheme().equals(\"file\")) {\n        copyJar(jobJarPath, JobSubmissionFiles.getJobJar(submitJobDir),\n            replication);\n        job.setJar(JobSubmissionFiles.getJobJar(submitJobDir).toString());\n      }\n    } else {\n      LOG.warn(\"No job jar file set.  User classes may not be found. \"\n          + \"See Job or Job#setJar(String).\");\n    }\n\n    addLog4jToDistributedCache(job, submitJobDir);\n\n    // set the timestamps of the archives and files\n    // set the public/private visibility of the archives and files\n    ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf);\n    // get DelegationToken for cached file\n    ClientDistributedCacheManager.getDelegationTokens(conf,\n        job.getCredentials());\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java"
    }
  }
}