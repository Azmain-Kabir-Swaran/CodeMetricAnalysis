{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LightWeightHashSet.java",
  "functionName": "removeElem",
  "functionId": "removeElem___key-T(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/LightWeightHashSet.java",
  "functionStartLine": 298,
  "functionEndLine": 331,
  "numCommitsSeen": 9,
  "timeTaken": 1084,
  "changeHistory": [
    "9a3f147fdd5421460889b266ead3a2300323cda2"
  ],
  "changeHistoryShort": {
    "9a3f147fdd5421460889b266ead3a2300323cda2": "Yintroduced"
  },
  "changeHistoryDetails": {
    "9a3f147fdd5421460889b266ead3a2300323cda2": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-2476. More CPU efficient data structure for under-replicated, over-replicated, and invalidated blocks. Contributed by Tomasz Nykiel.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1201991 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/11/11 5:13 PM",
      "commitName": "9a3f147fdd5421460889b266ead3a2300323cda2",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,34 @@\n+  protected LinkedElement\u003cT\u003e removeElem(final T key) {\n+    LinkedElement\u003cT\u003e found \u003d null;\n+    final int hashCode \u003d key.hashCode();\n+    final int index \u003d getIndex(hashCode);\n+    if (entries[index] \u003d\u003d null) {\n+      return null;\n+    } else if (hashCode \u003d\u003d entries[index].hashCode \u0026\u0026\n+            entries[index].element.equals(key)) {\n+      // remove the head of the bucket linked list\n+      modification++;\n+      size--;\n+      found \u003d entries[index];\n+      entries[index] \u003d found.next;\n+    } else {\n+      // head !\u003d null and key is not equal to head\n+      // search the element\n+      LinkedElement\u003cT\u003e prev \u003d entries[index];\n+      for (found \u003d prev.next; found !\u003d null;) {\n+        if (hashCode \u003d\u003d found.hashCode \u0026\u0026\n+                found.element.equals(key)) {\n+          // found the element, remove it\n+          modification++;\n+          size--;\n+          prev.next \u003d found.next;\n+          found.next \u003d null;\n+          break;\n+        } else {\n+          prev \u003d found;\n+          found \u003d found.next;\n+        }\n+      }\n+    }\n+    return found;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  protected LinkedElement\u003cT\u003e removeElem(final T key) {\n    LinkedElement\u003cT\u003e found \u003d null;\n    final int hashCode \u003d key.hashCode();\n    final int index \u003d getIndex(hashCode);\n    if (entries[index] \u003d\u003d null) {\n      return null;\n    } else if (hashCode \u003d\u003d entries[index].hashCode \u0026\u0026\n            entries[index].element.equals(key)) {\n      // remove the head of the bucket linked list\n      modification++;\n      size--;\n      found \u003d entries[index];\n      entries[index] \u003d found.next;\n    } else {\n      // head !\u003d null and key is not equal to head\n      // search the element\n      LinkedElement\u003cT\u003e prev \u003d entries[index];\n      for (found \u003d prev.next; found !\u003d null;) {\n        if (hashCode \u003d\u003d found.hashCode \u0026\u0026\n                found.element.equals(key)) {\n          // found the element, remove it\n          modification++;\n          size--;\n          prev.next \u003d found.next;\n          found.next \u003d null;\n          break;\n        } else {\n          prev \u003d found;\n          found \u003d found.next;\n        }\n      }\n    }\n    return found;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/LightWeightHashSet.java"
    }
  }
}