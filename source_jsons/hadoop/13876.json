{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "processExtraRedundancyBlock",
  "functionId": "processExtraRedundancyBlock___block-BlockInfo(modifiers-final)__replication-short(modifiers-final)__addedNode-DatanodeDescriptor(modifiers-final)__delNodeHint-DatanodeDescriptor",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 3857,
  "functionEndLine": 3890,
  "numCommitsSeen": 804,
  "timeTaken": 11255,
  "changeHistory": [
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "5865fe2bf01284993572ea60b3ec3bf8b4492818",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
    "6979cbfc1f4c28440816b56f5624765872b0be49",
    "e418bd1fb0568ce7ae22f588fea2dd9c95567383",
    "8e03e855b6a0cf650f43aac47b1ec642caf493f5",
    "73b86a5046fe3262dde7b05be46b18575e35fd5f"
  ],
  "changeHistoryShort": {
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ybodychange",
    "5865fe2bf01284993572ea60b3ec3bf8b4492818": "Ybodychange",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": "Yrename",
    "6979cbfc1f4c28440816b56f5624765872b0be49": "Ybodychange",
    "e418bd1fb0568ce7ae22f588fea2dd9c95567383": "Ybodychange",
    "8e03e855b6a0cf650f43aac47b1ec642caf493f5": "Ybodychange",
    "73b86a5046fe3262dde7b05be46b18575e35fd5f": "Ybodychange"
  },
  "changeHistoryDetails": {
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "14/10/16 6:13 PM",
      "commitNameOld": "391ce535a739dc92cb90017d759217265a4fd969",
      "commitAuthorOld": "Vinitha Reddy Gankidi",
      "daysBetweenCommits": 2.98,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   private void processExtraRedundancyBlock(final BlockInfo block,\n       final short replication, final DatanodeDescriptor addedNode,\n       DatanodeDescriptor delNodeHint) {\n     assert namesystem.hasWriteLock();\n     if (addedNode \u003d\u003d delNodeHint) {\n       delNodeHint \u003d null;\n     }\n     Collection\u003cDatanodeStorageInfo\u003e nonExcess \u003d new ArrayList\u003c\u003e();\n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n         .getNodes(block);\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       if (storage.getState() !\u003d State.NORMAL) {\n         continue;\n       }\n       final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n       if (storage.areBlockContentsStale()) {\n-        LOG.trace(\"BLOCK* processOverReplicatedBlock: Postponing {}\"\n+        LOG.trace(\"BLOCK* processExtraRedundancyBlock: Postponing {}\"\n             + \" since storage {} does not yet have up-to-date information.\",\n             block, storage);\n         postponeBlock(block);\n         return;\n       }\n       if (!isExcess(cur, block)) {\n-        if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()) {\n+        if (cur.isInService()) {\n           // exclude corrupt replicas\n           if (corruptNodes \u003d\u003d null || !corruptNodes.contains(cur)) {\n             nonExcess.add(storage);\n           }\n         }\n       }\n     }\n     chooseExcessRedundancies(nonExcess, block, replication, addedNode,\n         delNodeHint);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processExtraRedundancyBlock(final BlockInfo block,\n      final short replication, final DatanodeDescriptor addedNode,\n      DatanodeDescriptor delNodeHint) {\n    assert namesystem.hasWriteLock();\n    if (addedNode \u003d\u003d delNodeHint) {\n      delNodeHint \u003d null;\n    }\n    Collection\u003cDatanodeStorageInfo\u003e nonExcess \u003d new ArrayList\u003c\u003e();\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n        .getNodes(block);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      if (storage.getState() !\u003d State.NORMAL) {\n        continue;\n      }\n      final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n      if (storage.areBlockContentsStale()) {\n        LOG.trace(\"BLOCK* processExtraRedundancyBlock: Postponing {}\"\n            + \" since storage {} does not yet have up-to-date information.\",\n            block, storage);\n        postponeBlock(block);\n        return;\n      }\n      if (!isExcess(cur, block)) {\n        if (cur.isInService()) {\n          // exclude corrupt replicas\n          if (corruptNodes \u003d\u003d null || !corruptNodes.contains(cur)) {\n            nonExcess.add(storage);\n          }\n        }\n      }\n    }\n    chooseExcessRedundancies(nonExcess, block, replication, addedNode,\n        delNodeHint);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "5865fe2bf01284993572ea60b3ec3bf8b4492818": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9869. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-2]. Contributed by Rakesh R.\n",
      "commitDate": "25/04/16 10:01 PM",
      "commitName": "5865fe2bf01284993572ea60b3ec3bf8b4492818",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "17/04/16 6:28 PM",
      "commitNameOld": "67523ffcf491f4f2db5335899c00a174d0caaa9b",
      "commitAuthorOld": "Walter Su",
      "daysBetweenCommits": 8.15,
      "commitsBetweenForRepo": 47,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n   private void processExtraRedundancyBlock(final BlockInfo block,\n       final short replication, final DatanodeDescriptor addedNode,\n       DatanodeDescriptor delNodeHint) {\n     assert namesystem.hasWriteLock();\n     if (addedNode \u003d\u003d delNodeHint) {\n       delNodeHint \u003d null;\n     }\n     Collection\u003cDatanodeStorageInfo\u003e nonExcess \u003d new ArrayList\u003c\u003e();\n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n         .getNodes(block);\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       if (storage.getState() !\u003d State.NORMAL) {\n         continue;\n       }\n       final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n       if (storage.areBlockContentsStale()) {\n         LOG.trace(\"BLOCK* processOverReplicatedBlock: Postponing {}\"\n             + \" since storage {} does not yet have up-to-date information.\",\n             block, storage);\n         postponeBlock(block);\n         return;\n       }\n       if (!isExcess(cur, block)) {\n         if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()) {\n           // exclude corrupt replicas\n           if (corruptNodes \u003d\u003d null || !corruptNodes.contains(cur)) {\n             nonExcess.add(storage);\n           }\n         }\n       }\n     }\n-    chooseExcessReplicates(nonExcess, block, replication, addedNode,\n+    chooseExcessRedundancies(nonExcess, block, replication, addedNode,\n         delNodeHint);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processExtraRedundancyBlock(final BlockInfo block,\n      final short replication, final DatanodeDescriptor addedNode,\n      DatanodeDescriptor delNodeHint) {\n    assert namesystem.hasWriteLock();\n    if (addedNode \u003d\u003d delNodeHint) {\n      delNodeHint \u003d null;\n    }\n    Collection\u003cDatanodeStorageInfo\u003e nonExcess \u003d new ArrayList\u003c\u003e();\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n        .getNodes(block);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      if (storage.getState() !\u003d State.NORMAL) {\n        continue;\n      }\n      final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n      if (storage.areBlockContentsStale()) {\n        LOG.trace(\"BLOCK* processOverReplicatedBlock: Postponing {}\"\n            + \" since storage {} does not yet have up-to-date information.\",\n            block, storage);\n        postponeBlock(block);\n        return;\n      }\n      if (!isExcess(cur, block)) {\n        if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()) {\n          // exclude corrupt replicas\n          if (corruptNodes \u003d\u003d null || !corruptNodes.contains(cur)) {\n            nonExcess.add(storage);\n          }\n        }\n      }\n    }\n    chooseExcessRedundancies(nonExcess, block, replication, addedNode,\n        delNodeHint);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": {
      "type": "Yrename",
      "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
      "commitDate": "16/03/16 4:53 PM",
      "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "10/03/16 7:03 PM",
      "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 5.87,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n-  private void processOverReplicatedBlock(final BlockInfo block,\n+  private void processExtraRedundancyBlock(final BlockInfo block,\n       final short replication, final DatanodeDescriptor addedNode,\n       DatanodeDescriptor delNodeHint) {\n     assert namesystem.hasWriteLock();\n     if (addedNode \u003d\u003d delNodeHint) {\n       delNodeHint \u003d null;\n     }\n     Collection\u003cDatanodeStorageInfo\u003e nonExcess \u003d new ArrayList\u003c\u003e();\n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n         .getNodes(block);\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       if (storage.getState() !\u003d State.NORMAL) {\n         continue;\n       }\n       final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n       if (storage.areBlockContentsStale()) {\n         LOG.trace(\"BLOCK* processOverReplicatedBlock: Postponing {}\"\n             + \" since storage {} does not yet have up-to-date information.\",\n             block, storage);\n         postponeBlock(block);\n         return;\n       }\n       if (!isExcess(cur, block)) {\n         if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()) {\n           // exclude corrupt replicas\n           if (corruptNodes \u003d\u003d null || !corruptNodes.contains(cur)) {\n             nonExcess.add(storage);\n           }\n         }\n       }\n     }\n     chooseExcessReplicates(nonExcess, block, replication, addedNode,\n         delNodeHint);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processExtraRedundancyBlock(final BlockInfo block,\n      final short replication, final DatanodeDescriptor addedNode,\n      DatanodeDescriptor delNodeHint) {\n    assert namesystem.hasWriteLock();\n    if (addedNode \u003d\u003d delNodeHint) {\n      delNodeHint \u003d null;\n    }\n    Collection\u003cDatanodeStorageInfo\u003e nonExcess \u003d new ArrayList\u003c\u003e();\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n        .getNodes(block);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      if (storage.getState() !\u003d State.NORMAL) {\n        continue;\n      }\n      final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n      if (storage.areBlockContentsStale()) {\n        LOG.trace(\"BLOCK* processOverReplicatedBlock: Postponing {}\"\n            + \" since storage {} does not yet have up-to-date information.\",\n            block, storage);\n        postponeBlock(block);\n        return;\n      }\n      if (!isExcess(cur, block)) {\n        if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()) {\n          // exclude corrupt replicas\n          if (corruptNodes \u003d\u003d null || !corruptNodes.contains(cur)) {\n            nonExcess.add(storage);\n          }\n        }\n      }\n    }\n    chooseExcessReplicates(nonExcess, block, replication, addedNode,\n        delNodeHint);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldValue": "processOverReplicatedBlock",
        "newValue": "processExtraRedundancyBlock"
      }
    },
    "6979cbfc1f4c28440816b56f5624765872b0be49": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9838. Refactor the excessReplicateMap to a class.\n",
      "commitDate": "24/02/16 7:42 PM",
      "commitName": "6979cbfc1f4c28440816b56f5624765872b0be49",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "24/02/16 3:13 PM",
      "commitNameOld": "47b92f2b6f2dafc129a41b247f35e77c8e47ffba",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.19,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,34 @@\n   private void processOverReplicatedBlock(final BlockInfo block,\n       final short replication, final DatanodeDescriptor addedNode,\n       DatanodeDescriptor delNodeHint) {\n     assert namesystem.hasWriteLock();\n     if (addedNode \u003d\u003d delNodeHint) {\n       delNodeHint \u003d null;\n     }\n     Collection\u003cDatanodeStorageInfo\u003e nonExcess \u003d new ArrayList\u003c\u003e();\n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n         .getNodes(block);\n     for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n       if (storage.getState() !\u003d State.NORMAL) {\n         continue;\n       }\n       final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n       if (storage.areBlockContentsStale()) {\n         LOG.trace(\"BLOCK* processOverReplicatedBlock: Postponing {}\"\n             + \" since storage {} does not yet have up-to-date information.\",\n             block, storage);\n         postponeBlock(block);\n         return;\n       }\n-      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n-          cur.getDatanodeUuid());\n-      if (excessBlocks \u003d\u003d null || !excessBlocks.contains(block)) {\n+      if (!isExcess(cur, block)) {\n         if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()) {\n           // exclude corrupt replicas\n           if (corruptNodes \u003d\u003d null || !corruptNodes.contains(cur)) {\n             nonExcess.add(storage);\n           }\n         }\n       }\n     }\n     chooseExcessReplicates(nonExcess, block, replication, addedNode,\n         delNodeHint);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processOverReplicatedBlock(final BlockInfo block,\n      final short replication, final DatanodeDescriptor addedNode,\n      DatanodeDescriptor delNodeHint) {\n    assert namesystem.hasWriteLock();\n    if (addedNode \u003d\u003d delNodeHint) {\n      delNodeHint \u003d null;\n    }\n    Collection\u003cDatanodeStorageInfo\u003e nonExcess \u003d new ArrayList\u003c\u003e();\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n        .getNodes(block);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      if (storage.getState() !\u003d State.NORMAL) {\n        continue;\n      }\n      final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n      if (storage.areBlockContentsStale()) {\n        LOG.trace(\"BLOCK* processOverReplicatedBlock: Postponing {}\"\n            + \" since storage {} does not yet have up-to-date information.\",\n            block, storage);\n        postponeBlock(block);\n        return;\n      }\n      if (!isExcess(cur, block)) {\n        if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()) {\n          // exclude corrupt replicas\n          if (corruptNodes \u003d\u003d null || !corruptNodes.contains(cur)) {\n            nonExcess.add(storage);\n          }\n        }\n      }\n    }\n    chooseExcessReplicates(nonExcess, block, replication, addedNode,\n        delNodeHint);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "e418bd1fb0568ce7ae22f588fea2dd9c95567383": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9566. Remove expensive \u0027BlocksMap#getStorages(Block b, final DatanodeStorage.State state)\u0027 method (Contributed by Daryn Sharp)\n",
      "commitDate": "31/01/16 11:54 PM",
      "commitName": "e418bd1fb0568ce7ae22f588fea2dd9c95567383",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "27/01/16 4:31 PM",
      "commitNameOld": "3a9571308e99cc374681bbc451a517d41a150aa0",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 4.31,
      "commitsBetweenForRepo": 22,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,33 +1,36 @@\n   private void processOverReplicatedBlock(final BlockInfo block,\n       final short replication, final DatanodeDescriptor addedNode,\n       DatanodeDescriptor delNodeHint) {\n     assert namesystem.hasWriteLock();\n     if (addedNode \u003d\u003d delNodeHint) {\n       delNodeHint \u003d null;\n     }\n     Collection\u003cDatanodeStorageInfo\u003e nonExcess \u003d new ArrayList\u003c\u003e();\n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n         .getNodes(block);\n-    for(DatanodeStorageInfo storage : blocksMap.getStorages(block, State.NORMAL)) {\n+    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n+      if (storage.getState() !\u003d State.NORMAL) {\n+        continue;\n+      }\n       final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n       if (storage.areBlockContentsStale()) {\n         LOG.trace(\"BLOCK* processOverReplicatedBlock: Postponing {}\"\n             + \" since storage {} does not yet have up-to-date information.\",\n             block, storage);\n         postponeBlock(block);\n         return;\n       }\n       LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n           cur.getDatanodeUuid());\n       if (excessBlocks \u003d\u003d null || !excessBlocks.contains(block)) {\n         if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()) {\n           // exclude corrupt replicas\n           if (corruptNodes \u003d\u003d null || !corruptNodes.contains(cur)) {\n             nonExcess.add(storage);\n           }\n         }\n       }\n     }\n     chooseExcessReplicates(nonExcess, block, replication, addedNode,\n         delNodeHint);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processOverReplicatedBlock(final BlockInfo block,\n      final short replication, final DatanodeDescriptor addedNode,\n      DatanodeDescriptor delNodeHint) {\n    assert namesystem.hasWriteLock();\n    if (addedNode \u003d\u003d delNodeHint) {\n      delNodeHint \u003d null;\n    }\n    Collection\u003cDatanodeStorageInfo\u003e nonExcess \u003d new ArrayList\u003c\u003e();\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n        .getNodes(block);\n    for (DatanodeStorageInfo storage : blocksMap.getStorages(block)) {\n      if (storage.getState() !\u003d State.NORMAL) {\n        continue;\n      }\n      final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n      if (storage.areBlockContentsStale()) {\n        LOG.trace(\"BLOCK* processOverReplicatedBlock: Postponing {}\"\n            + \" since storage {} does not yet have up-to-date information.\",\n            block, storage);\n        postponeBlock(block);\n        return;\n      }\n      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n          cur.getDatanodeUuid());\n      if (excessBlocks \u003d\u003d null || !excessBlocks.contains(block)) {\n        if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()) {\n          // exclude corrupt replicas\n          if (corruptNodes \u003d\u003d null || !corruptNodes.contains(cur)) {\n            nonExcess.add(storage);\n          }\n        }\n      }\n    }\n    chooseExcessReplicates(nonExcess, block, replication, addedNode,\n        delNodeHint);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "8e03e855b6a0cf650f43aac47b1ec642caf493f5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9434. Recommission a datanode with 500k blocks may pause NN for 30 seconds for printing info log messags.\n",
      "commitDate": "19/11/15 2:54 PM",
      "commitName": "8e03e855b6a0cf650f43aac47b1ec642caf493f5",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "03/11/15 1:34 PM",
      "commitNameOld": "dac0463a4e20dfb3a802355919fc22b8e017a4e1",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 16.06,
      "commitsBetweenForRepo": 93,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,33 @@\n   private void processOverReplicatedBlock(final BlockInfo block,\n       final short replication, final DatanodeDescriptor addedNode,\n       DatanodeDescriptor delNodeHint) {\n     assert namesystem.hasWriteLock();\n     if (addedNode \u003d\u003d delNodeHint) {\n       delNodeHint \u003d null;\n     }\n     Collection\u003cDatanodeStorageInfo\u003e nonExcess \u003d new ArrayList\u003c\u003e();\n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n         .getNodes(block);\n     for(DatanodeStorageInfo storage : blocksMap.getStorages(block, State.NORMAL)) {\n       final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n       if (storage.areBlockContentsStale()) {\n-        LOG.info(\"BLOCK* processOverReplicatedBlock: \" +\n-            \"Postponing processing of over-replicated \" +\n-            block + \" since storage + \" + storage\n-            + \"datanode \" + cur + \" does not yet have up-to-date \" +\n-            \"block information.\");\n+        LOG.trace(\"BLOCK* processOverReplicatedBlock: Postponing {}\"\n+            + \" since storage {} does not yet have up-to-date information.\",\n+            block, storage);\n         postponeBlock(block);\n         return;\n       }\n       LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n           cur.getDatanodeUuid());\n       if (excessBlocks \u003d\u003d null || !excessBlocks.contains(block)) {\n         if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()) {\n           // exclude corrupt replicas\n           if (corruptNodes \u003d\u003d null || !corruptNodes.contains(cur)) {\n             nonExcess.add(storage);\n           }\n         }\n       }\n     }\n     chooseExcessReplicates(nonExcess, block, replication, addedNode,\n         delNodeHint);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processOverReplicatedBlock(final BlockInfo block,\n      final short replication, final DatanodeDescriptor addedNode,\n      DatanodeDescriptor delNodeHint) {\n    assert namesystem.hasWriteLock();\n    if (addedNode \u003d\u003d delNodeHint) {\n      delNodeHint \u003d null;\n    }\n    Collection\u003cDatanodeStorageInfo\u003e nonExcess \u003d new ArrayList\u003c\u003e();\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n        .getNodes(block);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(block, State.NORMAL)) {\n      final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n      if (storage.areBlockContentsStale()) {\n        LOG.trace(\"BLOCK* processOverReplicatedBlock: Postponing {}\"\n            + \" since storage {} does not yet have up-to-date information.\",\n            block, storage);\n        postponeBlock(block);\n        return;\n      }\n      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n          cur.getDatanodeUuid());\n      if (excessBlocks \u003d\u003d null || !excessBlocks.contains(block)) {\n        if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()) {\n          // exclude corrupt replicas\n          if (corruptNodes \u003d\u003d null || !corruptNodes.contains(cur)) {\n            nonExcess.add(storage);\n          }\n        }\n      }\n    }\n    chooseExcessReplicates(nonExcess, block, replication, addedNode,\n        delNodeHint);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "73b86a5046fe3262dde7b05be46b18575e35fd5f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8988. Use LightWeightHashSet instead of LightWeightLinkedSet in BlockManager#excessReplicateMap. (yliu)\n",
      "commitDate": "11/10/15 11:40 PM",
      "commitName": "73b86a5046fe3262dde7b05be46b18575e35fd5f",
      "commitAuthor": "yliu",
      "commitDateOld": "23/09/15 1:34 PM",
      "commitNameOld": "c09dc258a8f64fab852bf6f26187163480dbee3c",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 18.42,
      "commitsBetweenForRepo": 126,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,35 @@\n   private void processOverReplicatedBlock(final BlockInfo block,\n       final short replication, final DatanodeDescriptor addedNode,\n       DatanodeDescriptor delNodeHint) {\n     assert namesystem.hasWriteLock();\n     if (addedNode \u003d\u003d delNodeHint) {\n       delNodeHint \u003d null;\n     }\n     Collection\u003cDatanodeStorageInfo\u003e nonExcess \u003d new ArrayList\u003c\u003e();\n     Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n         .getNodes(block);\n     for(DatanodeStorageInfo storage : blocksMap.getStorages(block, State.NORMAL)) {\n       final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n       if (storage.areBlockContentsStale()) {\n         LOG.info(\"BLOCK* processOverReplicatedBlock: \" +\n             \"Postponing processing of over-replicated \" +\n             block + \" since storage + \" + storage\n             + \"datanode \" + cur + \" does not yet have up-to-date \" +\n             \"block information.\");\n         postponeBlock(block);\n         return;\n       }\n-      LightWeightLinkedSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n+      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n           cur.getDatanodeUuid());\n       if (excessBlocks \u003d\u003d null || !excessBlocks.contains(block)) {\n         if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()) {\n           // exclude corrupt replicas\n           if (corruptNodes \u003d\u003d null || !corruptNodes.contains(cur)) {\n             nonExcess.add(storage);\n           }\n         }\n       }\n     }\n     chooseExcessReplicates(nonExcess, block, replication, addedNode,\n         delNodeHint);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processOverReplicatedBlock(final BlockInfo block,\n      final short replication, final DatanodeDescriptor addedNode,\n      DatanodeDescriptor delNodeHint) {\n    assert namesystem.hasWriteLock();\n    if (addedNode \u003d\u003d delNodeHint) {\n      delNodeHint \u003d null;\n    }\n    Collection\u003cDatanodeStorageInfo\u003e nonExcess \u003d new ArrayList\u003c\u003e();\n    Collection\u003cDatanodeDescriptor\u003e corruptNodes \u003d corruptReplicas\n        .getNodes(block);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(block, State.NORMAL)) {\n      final DatanodeDescriptor cur \u003d storage.getDatanodeDescriptor();\n      if (storage.areBlockContentsStale()) {\n        LOG.info(\"BLOCK* processOverReplicatedBlock: \" +\n            \"Postponing processing of over-replicated \" +\n            block + \" since storage + \" + storage\n            + \"datanode \" + cur + \" does not yet have up-to-date \" +\n            \"block information.\");\n        postponeBlock(block);\n        return;\n      }\n      LightWeightHashSet\u003cBlockInfo\u003e excessBlocks \u003d excessReplicateMap.get(\n          cur.getDatanodeUuid());\n      if (excessBlocks \u003d\u003d null || !excessBlocks.contains(block)) {\n        if (!cur.isDecommissionInProgress() \u0026\u0026 !cur.isDecommissioned()) {\n          // exclude corrupt replicas\n          if (corruptNodes \u003d\u003d null || !corruptNodes.contains(cur)) {\n            nonExcess.add(storage);\n          }\n        }\n      }\n    }\n    chooseExcessReplicates(nonExcess, block, replication, addedNode,\n        delNodeHint);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    }
  }
}