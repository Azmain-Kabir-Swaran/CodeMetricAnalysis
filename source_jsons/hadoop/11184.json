{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "WebHdfsHandler.java",
  "functionName": "onOpen",
  "functionId": "onOpen___ctx-ChannelHandlerContext",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java",
  "functionStartLine": 247,
  "functionEndLine": 286,
  "numCommitsSeen": 19,
  "timeTaken": 1653,
  "changeHistory": [
    "bf74dbf80dc9379d669779a598950908adffb8a7",
    "8c40e88d5de51a273f6ae5cd11c40f44248bbfcd",
    "bf8e4332cb4c33d0287ae6ecca61b335402ac1c4"
  ],
  "changeHistoryShort": {
    "bf74dbf80dc9379d669779a598950908adffb8a7": "Ybodychange",
    "8c40e88d5de51a273f6ae5cd11c40f44248bbfcd": "Ybodychange",
    "bf8e4332cb4c33d0287ae6ecca61b335402ac1c4": "Yintroduced"
  },
  "changeHistoryDetails": {
    "bf74dbf80dc9379d669779a598950908adffb8a7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7959. WebHdfs logging is missing on Datanode (Kihwal Lee via sjlee)\n",
      "commitDate": "24/06/16 2:44 PM",
      "commitName": "bf74dbf80dc9379d669779a598950908adffb8a7",
      "commitAuthor": "Sangjin Lee",
      "commitDateOld": "23/05/16 3:52 PM",
      "commitNameOld": "4b0f55b6ea1665e2118fd573f72a6fcd1fce20d6",
      "commitAuthorOld": "Allen Wittenauer",
      "daysBetweenCommits": 31.95,
      "commitsBetweenForRepo": 255,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   private void onOpen(ChannelHandlerContext ctx) throws IOException {\n     final String nnId \u003d params.namenodeId();\n     final int bufferSize \u003d params.bufferSize();\n     final long offset \u003d params.offset();\n     final long length \u003d params.length();\n \n-    DefaultHttpResponse response \u003d new DefaultHttpResponse(HTTP_1_1, OK);\n-    HttpHeaders headers \u003d response.headers();\n+    resp \u003d new DefaultHttpResponse(HTTP_1_1, OK);\n+    HttpHeaders headers \u003d resp.headers();\n     // Allow the UI to access the file\n     headers.set(ACCESS_CONTROL_ALLOW_METHODS, GET);\n     headers.set(ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n     headers.set(CONTENT_TYPE, APPLICATION_OCTET_STREAM);\n     headers.set(CONNECTION, CLOSE);\n \n     final DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n     HdfsDataInputStream in \u003d dfsclient.createWrappedInputStream(\n       dfsclient.open(path, bufferSize, true));\n     in.seek(offset);\n \n     long contentLength \u003d in.getVisibleLength() - offset;\n     if (length \u003e\u003d 0) {\n       contentLength \u003d Math.min(contentLength, length);\n     }\n     final InputStream data;\n     if (contentLength \u003e\u003d 0) {\n       headers.set(CONTENT_LENGTH, contentLength);\n       data \u003d new LimitInputStream(in, contentLength);\n     } else {\n       data \u003d in;\n     }\n \n-    ctx.write(response);\n+    ctx.write(resp);\n     ctx.writeAndFlush(new ChunkedStream(data) {\n       @Override\n       public void close() throws Exception {\n         super.close();\n         dfsclient.close();\n       }\n     }).addListener(ChannelFutureListener.CLOSE);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void onOpen(ChannelHandlerContext ctx) throws IOException {\n    final String nnId \u003d params.namenodeId();\n    final int bufferSize \u003d params.bufferSize();\n    final long offset \u003d params.offset();\n    final long length \u003d params.length();\n\n    resp \u003d new DefaultHttpResponse(HTTP_1_1, OK);\n    HttpHeaders headers \u003d resp.headers();\n    // Allow the UI to access the file\n    headers.set(ACCESS_CONTROL_ALLOW_METHODS, GET);\n    headers.set(ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n    headers.set(CONTENT_TYPE, APPLICATION_OCTET_STREAM);\n    headers.set(CONNECTION, CLOSE);\n\n    final DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n    HdfsDataInputStream in \u003d dfsclient.createWrappedInputStream(\n      dfsclient.open(path, bufferSize, true));\n    in.seek(offset);\n\n    long contentLength \u003d in.getVisibleLength() - offset;\n    if (length \u003e\u003d 0) {\n      contentLength \u003d Math.min(contentLength, length);\n    }\n    final InputStream data;\n    if (contentLength \u003e\u003d 0) {\n      headers.set(CONTENT_LENGTH, contentLength);\n      data \u003d new LimitInputStream(in, contentLength);\n    } else {\n      data \u003d in;\n    }\n\n    ctx.write(resp);\n    ctx.writeAndFlush(new ChunkedStream(data) {\n      @Override\n      public void close() throws Exception {\n        super.close();\n        dfsclient.close();\n      }\n    }).addListener(ChannelFutureListener.CLOSE);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java",
      "extendedDetails": {}
    },
    "8c40e88d5de51a273f6ae5cd11c40f44248bbfcd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7945. The WebHdfs system on DN does not honor the length parameter. Contributed by Haohui Mai.\n",
      "commitDate": "18/03/15 4:19 PM",
      "commitName": "8c40e88d5de51a273f6ae5cd11c40f44248bbfcd",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "27/01/15 12:58 PM",
      "commitNameOld": "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 50.1,
      "commitsBetweenForRepo": 474,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,40 @@\n   private void onOpen(ChannelHandlerContext ctx) throws IOException {\n     final String nnId \u003d params.namenodeId();\n     final int bufferSize \u003d params.bufferSize();\n     final long offset \u003d params.offset();\n+    final long length \u003d params.length();\n \n     DefaultHttpResponse response \u003d new DefaultHttpResponse(HTTP_1_1, OK);\n     HttpHeaders headers \u003d response.headers();\n     // Allow the UI to access the file\n     headers.set(ACCESS_CONTROL_ALLOW_METHODS, GET);\n     headers.set(ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n     headers.set(CONTENT_TYPE, APPLICATION_OCTET_STREAM);\n     headers.set(CONNECTION, CLOSE);\n \n     final DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n     HdfsDataInputStream in \u003d dfsclient.createWrappedInputStream(\n       dfsclient.open(path, bufferSize, true));\n     in.seek(offset);\n \n-    if (in.getVisibleLength() \u003e\u003d offset) {\n-      headers.set(CONTENT_LENGTH, in.getVisibleLength() - offset);\n+    long contentLength \u003d in.getVisibleLength() - offset;\n+    if (length \u003e\u003d 0) {\n+      contentLength \u003d Math.min(contentLength, length);\n+    }\n+    final InputStream data;\n+    if (contentLength \u003e\u003d 0) {\n+      headers.set(CONTENT_LENGTH, contentLength);\n+      data \u003d new LimitInputStream(in, contentLength);\n+    } else {\n+      data \u003d in;\n     }\n \n     ctx.write(response);\n-    ctx.writeAndFlush(new ChunkedStream(in) {\n+    ctx.writeAndFlush(new ChunkedStream(data) {\n       @Override\n       public void close() throws Exception {\n         super.close();\n         dfsclient.close();\n       }\n     }).addListener(ChannelFutureListener.CLOSE);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void onOpen(ChannelHandlerContext ctx) throws IOException {\n    final String nnId \u003d params.namenodeId();\n    final int bufferSize \u003d params.bufferSize();\n    final long offset \u003d params.offset();\n    final long length \u003d params.length();\n\n    DefaultHttpResponse response \u003d new DefaultHttpResponse(HTTP_1_1, OK);\n    HttpHeaders headers \u003d response.headers();\n    // Allow the UI to access the file\n    headers.set(ACCESS_CONTROL_ALLOW_METHODS, GET);\n    headers.set(ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n    headers.set(CONTENT_TYPE, APPLICATION_OCTET_STREAM);\n    headers.set(CONNECTION, CLOSE);\n\n    final DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n    HdfsDataInputStream in \u003d dfsclient.createWrappedInputStream(\n      dfsclient.open(path, bufferSize, true));\n    in.seek(offset);\n\n    long contentLength \u003d in.getVisibleLength() - offset;\n    if (length \u003e\u003d 0) {\n      contentLength \u003d Math.min(contentLength, length);\n    }\n    final InputStream data;\n    if (contentLength \u003e\u003d 0) {\n      headers.set(CONTENT_LENGTH, contentLength);\n      data \u003d new LimitInputStream(in, contentLength);\n    } else {\n      data \u003d in;\n    }\n\n    ctx.write(response);\n    ctx.writeAndFlush(new ChunkedStream(data) {\n      @Override\n      public void close() throws Exception {\n        super.close();\n        dfsclient.close();\n      }\n    }).addListener(ChannelFutureListener.CLOSE);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java",
      "extendedDetails": {}
    },
    "bf8e4332cb4c33d0287ae6ecca61b335402ac1c4": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7279. Use netty to implement DatanodeWebHdfsMethods. Contributed by Haohui Mai.\n",
      "commitDate": "17/11/14 11:42 AM",
      "commitName": "bf8e4332cb4c33d0287ae6ecca61b335402ac1c4",
      "commitAuthor": "Haohui Mai",
      "diff": "@@ -0,0 +1,31 @@\n+  private void onOpen(ChannelHandlerContext ctx) throws IOException {\n+    final String nnId \u003d params.namenodeId();\n+    final int bufferSize \u003d params.bufferSize();\n+    final long offset \u003d params.offset();\n+\n+    DefaultHttpResponse response \u003d new DefaultHttpResponse(HTTP_1_1, OK);\n+    HttpHeaders headers \u003d response.headers();\n+    // Allow the UI to access the file\n+    headers.set(ACCESS_CONTROL_ALLOW_METHODS, GET);\n+    headers.set(ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n+    headers.set(CONTENT_TYPE, APPLICATION_OCTET_STREAM);\n+    headers.set(CONNECTION, CLOSE);\n+\n+    final DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n+    HdfsDataInputStream in \u003d dfsclient.createWrappedInputStream(\n+      dfsclient.open(path, bufferSize, true));\n+    in.seek(offset);\n+\n+    if (in.getVisibleLength() \u003e\u003d offset) {\n+      headers.set(CONTENT_LENGTH, in.getVisibleLength() - offset);\n+    }\n+\n+    ctx.write(response);\n+    ctx.writeAndFlush(new ChunkedStream(in) {\n+      @Override\n+      public void close() throws Exception {\n+        super.close();\n+        dfsclient.close();\n+      }\n+    }).addListener(ChannelFutureListener.CLOSE);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void onOpen(ChannelHandlerContext ctx) throws IOException {\n    final String nnId \u003d params.namenodeId();\n    final int bufferSize \u003d params.bufferSize();\n    final long offset \u003d params.offset();\n\n    DefaultHttpResponse response \u003d new DefaultHttpResponse(HTTP_1_1, OK);\n    HttpHeaders headers \u003d response.headers();\n    // Allow the UI to access the file\n    headers.set(ACCESS_CONTROL_ALLOW_METHODS, GET);\n    headers.set(ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n    headers.set(CONTENT_TYPE, APPLICATION_OCTET_STREAM);\n    headers.set(CONNECTION, CLOSE);\n\n    final DFSClient dfsclient \u003d newDfsClient(nnId, conf);\n    HdfsDataInputStream in \u003d dfsclient.createWrappedInputStream(\n      dfsclient.open(path, bufferSize, true));\n    in.seek(offset);\n\n    if (in.getVisibleLength() \u003e\u003d offset) {\n      headers.set(CONTENT_LENGTH, in.getVisibleLength() - offset);\n    }\n\n    ctx.write(response);\n    ctx.writeAndFlush(new ChunkedStream(in) {\n      @Override\n      public void close() throws Exception {\n        super.close();\n        dfsclient.close();\n      }\n    }).addListener(ChannelFutureListener.CLOSE);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java"
    }
  }
}