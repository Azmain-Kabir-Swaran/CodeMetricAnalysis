{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockReceiver.java",
  "functionName": "sendAckUpstream",
  "functionId": "sendAckUpstream___ack-PipelineAck__seqno-long__totalAckTimeNanos-long__offsetInBlock-long__myHeader-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
  "functionStartLine": 1559,
  "functionEndLine": 1587,
  "numCommitsSeen": 181,
  "timeTaken": 4073,
  "changeHistory": [
    "c4980a2f343778544ca20ebea1338651793ea0d9",
    "1c6b5d2b5841e5219a98937088cde4ae63869f80",
    "98a692fd6361365db4afb9523a5d83ee32774112",
    "7e56bfe40589a1aa9b5ef20b342e421823cd0592"
  ],
  "changeHistoryShort": {
    "c4980a2f343778544ca20ebea1338651793ea0d9": "Ymultichange(Yparameterchange,Ybodychange)",
    "1c6b5d2b5841e5219a98937088cde4ae63869f80": "Ybodychange",
    "98a692fd6361365db4afb9523a5d83ee32774112": "Ymultichange(Yparameterchange,Ybodychange)",
    "7e56bfe40589a1aa9b5ef20b342e421823cd0592": "Yintroduced"
  },
  "changeHistoryDetails": {
    "c4980a2f343778544ca20ebea1338651793ea0d9": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7270. Add congestion signaling capability to DataNode write protocol. Contributed by Haohui Mai.\n",
      "commitDate": "05/02/15 10:58 AM",
      "commitName": "c4980a2f343778544ca20ebea1338651793ea0d9",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7270. Add congestion signaling capability to DataNode write protocol. Contributed by Haohui Mai.\n",
          "commitDate": "05/02/15 10:58 AM",
          "commitName": "c4980a2f343778544ca20ebea1338651793ea0d9",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "22/01/15 10:37 AM",
          "commitNameOld": "5f124efb3e090f96f217bee22f3c8897f9772f14",
          "commitAuthorOld": "yliu",
          "daysBetweenCommits": 14.01,
          "commitsBetweenForRepo": 115,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,29 @@\n     private void sendAckUpstream(PipelineAck ack, long seqno,\n         long totalAckTimeNanos, long offsetInBlock,\n-        Status myStatus) throws IOException {\n+        int myHeader) throws IOException {\n       try {\n         // Wait for other sender to finish. Unless there is an OOB being sent,\n         // the responder won\u0027t have to wait.\n         synchronized(this) {\n           while(sending) {\n             wait();\n           }\n           sending \u003d true;\n         }\n \n         try {\n           if (!running) return;\n           sendAckUpstreamUnprotected(ack, seqno, totalAckTimeNanos,\n-              offsetInBlock, myStatus);\n+              offsetInBlock, myHeader);\n         } finally {\n           synchronized(this) {\n             sending \u003d false;\n             notify();\n           }\n         }\n       } catch (InterruptedException ie) {\n         // The responder was interrupted. Make it go down without\n         // interrupting the receiver(writer) thread.  \n         running \u003d false;\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private void sendAckUpstream(PipelineAck ack, long seqno,\n        long totalAckTimeNanos, long offsetInBlock,\n        int myHeader) throws IOException {\n      try {\n        // Wait for other sender to finish. Unless there is an OOB being sent,\n        // the responder won\u0027t have to wait.\n        synchronized(this) {\n          while(sending) {\n            wait();\n          }\n          sending \u003d true;\n        }\n\n        try {\n          if (!running) return;\n          sendAckUpstreamUnprotected(ack, seqno, totalAckTimeNanos,\n              offsetInBlock, myHeader);\n        } finally {\n          synchronized(this) {\n            sending \u003d false;\n            notify();\n          }\n        }\n      } catch (InterruptedException ie) {\n        // The responder was interrupted. Make it go down without\n        // interrupting the receiver(writer) thread.  \n        running \u003d false;\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
          "extendedDetails": {
            "oldValue": "[ack-PipelineAck, seqno-long, totalAckTimeNanos-long, offsetInBlock-long, myStatus-Status]",
            "newValue": "[ack-PipelineAck, seqno-long, totalAckTimeNanos-long, offsetInBlock-long, myHeader-int]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7270. Add congestion signaling capability to DataNode write protocol. Contributed by Haohui Mai.\n",
          "commitDate": "05/02/15 10:58 AM",
          "commitName": "c4980a2f343778544ca20ebea1338651793ea0d9",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "22/01/15 10:37 AM",
          "commitNameOld": "5f124efb3e090f96f217bee22f3c8897f9772f14",
          "commitAuthorOld": "yliu",
          "daysBetweenCommits": 14.01,
          "commitsBetweenForRepo": 115,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,29 +1,29 @@\n     private void sendAckUpstream(PipelineAck ack, long seqno,\n         long totalAckTimeNanos, long offsetInBlock,\n-        Status myStatus) throws IOException {\n+        int myHeader) throws IOException {\n       try {\n         // Wait for other sender to finish. Unless there is an OOB being sent,\n         // the responder won\u0027t have to wait.\n         synchronized(this) {\n           while(sending) {\n             wait();\n           }\n           sending \u003d true;\n         }\n \n         try {\n           if (!running) return;\n           sendAckUpstreamUnprotected(ack, seqno, totalAckTimeNanos,\n-              offsetInBlock, myStatus);\n+              offsetInBlock, myHeader);\n         } finally {\n           synchronized(this) {\n             sending \u003d false;\n             notify();\n           }\n         }\n       } catch (InterruptedException ie) {\n         // The responder was interrupted. Make it go down without\n         // interrupting the receiver(writer) thread.  \n         running \u003d false;\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private void sendAckUpstream(PipelineAck ack, long seqno,\n        long totalAckTimeNanos, long offsetInBlock,\n        int myHeader) throws IOException {\n      try {\n        // Wait for other sender to finish. Unless there is an OOB being sent,\n        // the responder won\u0027t have to wait.\n        synchronized(this) {\n          while(sending) {\n            wait();\n          }\n          sending \u003d true;\n        }\n\n        try {\n          if (!running) return;\n          sendAckUpstreamUnprotected(ack, seqno, totalAckTimeNanos,\n              offsetInBlock, myHeader);\n        } finally {\n          synchronized(this) {\n            sending \u003d false;\n            notify();\n          }\n        }\n      } catch (InterruptedException ie) {\n        // The responder was interrupted. Make it go down without\n        // interrupting the receiver(writer) thread.  \n        running \u003d false;\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
          "extendedDetails": {}
        }
      ]
    },
    "1c6b5d2b5841e5219a98937088cde4ae63869f80": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5583. Make DN send an OOB Ack on shutdown before restarting. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1571491 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/02/14 3:38 PM",
      "commitName": "1c6b5d2b5841e5219a98937088cde4ae63869f80",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "19/02/14 3:38 PM",
      "commitNameOld": "0369aff403012f8dd02486a3dd2f8e346ad23b03",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 5.0,
      "commitsBetweenForRepo": 39,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,29 @@\n     private void sendAckUpstream(PipelineAck ack, long seqno,\n         long totalAckTimeNanos, long offsetInBlock,\n         Status myStatus) throws IOException {\n-      Status[] replies \u003d null;\n-      if (mirrorError) { // ack read error\n-        replies \u003d MIRROR_ERROR_STATUS;\n-      } else {\n-        short ackLen \u003d type \u003d\u003d PacketResponderType.LAST_IN_PIPELINE ? 0 : ack\n-            .getNumOfReplies();\n-        replies \u003d new Status[1 + ackLen];\n-        replies[0] \u003d myStatus;\n-        for (int i \u003d 0; i \u003c ackLen; i++) {\n-          replies[i + 1] \u003d ack.getReply(i);\n+      try {\n+        // Wait for other sender to finish. Unless there is an OOB being sent,\n+        // the responder won\u0027t have to wait.\n+        synchronized(this) {\n+          while(sending) {\n+            wait();\n+          }\n+          sending \u003d true;\n         }\n-        // If the mirror has reported that it received a corrupt packet,\n-        // do self-destruct to mark myself bad, instead of making the \n-        // mirror node bad. The mirror is guaranteed to be good without\n-        // corrupt data on disk.\n-        if (ackLen \u003e 0 \u0026\u0026 replies[1] \u003d\u003d Status.ERROR_CHECKSUM) {\n-          throw new IOException(\"Shutting down writer and responder \"\n-              + \"since the down streams reported the data sent by this \"\n-              + \"thread is corrupt\");\n+\n+        try {\n+          if (!running) return;\n+          sendAckUpstreamUnprotected(ack, seqno, totalAckTimeNanos,\n+              offsetInBlock, myStatus);\n+        } finally {\n+          synchronized(this) {\n+            sending \u003d false;\n+            notify();\n+          }\n         }\n-      }\n-      PipelineAck replyAck \u003d new PipelineAck(seqno, replies,\n-          totalAckTimeNanos);\n-      if (replyAck.isSuccess()\n-          \u0026\u0026 offsetInBlock \u003e replicaInfo.getBytesAcked()) {\n-        replicaInfo.setBytesAcked(offsetInBlock);\n-      }\n-\n-      // send my ack back to upstream datanode\n-      replyAck.write(upstreamOut);\n-      upstreamOut.flush();\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(myString + \", replyAck\u003d\" + replyAck);\n-      }\n-\n-      // If a corruption was detected in the received data, terminate after\n-      // sending ERROR_CHECKSUM back. \n-      if (myStatus \u003d\u003d Status.ERROR_CHECKSUM) {\n-        throw new IOException(\"Shutting down writer and responder \"\n-            + \"due to a checksum error in received data. The error \"\n-            + \"response has been sent upstream.\");\n+      } catch (InterruptedException ie) {\n+        // The responder was interrupted. Make it go down without\n+        // interrupting the receiver(writer) thread.  \n+        running \u003d false;\n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void sendAckUpstream(PipelineAck ack, long seqno,\n        long totalAckTimeNanos, long offsetInBlock,\n        Status myStatus) throws IOException {\n      try {\n        // Wait for other sender to finish. Unless there is an OOB being sent,\n        // the responder won\u0027t have to wait.\n        synchronized(this) {\n          while(sending) {\n            wait();\n          }\n          sending \u003d true;\n        }\n\n        try {\n          if (!running) return;\n          sendAckUpstreamUnprotected(ack, seqno, totalAckTimeNanos,\n              offsetInBlock, myStatus);\n        } finally {\n          synchronized(this) {\n            sending \u003d false;\n            notify();\n          }\n        }\n      } catch (InterruptedException ie) {\n        // The responder was interrupted. Make it go down without\n        // interrupting the receiver(writer) thread.  \n        running \u003d false;\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
      "extendedDetails": {}
    },
    "98a692fd6361365db4afb9523a5d83ee32774112": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-3875. Issue handling checksum errors in write pipeline. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1484808 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/05/13 6:42 AM",
      "commitName": "98a692fd6361365db4afb9523a5d83ee32774112",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-3875. Issue handling checksum errors in write pipeline. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1484808 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/05/13 6:42 AM",
          "commitName": "98a692fd6361365db4afb9523a5d83ee32774112",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "06/03/13 11:15 AM",
          "commitNameOld": "638801cce16fc1dc3259c541dc30a599faaddda1",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 75.77,
          "commitsBetweenForRepo": 469,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,28 +1,46 @@\n     private void sendAckUpstream(PipelineAck ack, long seqno,\n-        long totalAckTimeNanos, long offsetInBlock) throws IOException {\n+        long totalAckTimeNanos, long offsetInBlock,\n+        Status myStatus) throws IOException {\n       Status[] replies \u003d null;\n       if (mirrorError) { // ack read error\n         replies \u003d MIRROR_ERROR_STATUS;\n       } else {\n         short ackLen \u003d type \u003d\u003d PacketResponderType.LAST_IN_PIPELINE ? 0 : ack\n             .getNumOfReplies();\n         replies \u003d new Status[1 + ackLen];\n-        replies[0] \u003d Status.SUCCESS;\n+        replies[0] \u003d myStatus;\n         for (int i \u003d 0; i \u003c ackLen; i++) {\n           replies[i + 1] \u003d ack.getReply(i);\n         }\n+        // If the mirror has reported that it received a corrupt packet,\n+        // do self-destruct to mark myself bad, instead of making the \n+        // mirror node bad. The mirror is guaranteed to be good without\n+        // corrupt data on disk.\n+        if (ackLen \u003e 0 \u0026\u0026 replies[1] \u003d\u003d Status.ERROR_CHECKSUM) {\n+          throw new IOException(\"Shutting down writer and responder \"\n+              + \"since the down streams reported the data sent by this \"\n+              + \"thread is corrupt\");\n+        }\n       }\n       PipelineAck replyAck \u003d new PipelineAck(seqno, replies,\n           totalAckTimeNanos);\n       if (replyAck.isSuccess()\n           \u0026\u0026 offsetInBlock \u003e replicaInfo.getBytesAcked()) {\n         replicaInfo.setBytesAcked(offsetInBlock);\n       }\n \n       // send my ack back to upstream datanode\n       replyAck.write(upstreamOut);\n       upstreamOut.flush();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(myString + \", replyAck\u003d\" + replyAck);\n       }\n+\n+      // If a corruption was detected in the received data, terminate after\n+      // sending ERROR_CHECKSUM back. \n+      if (myStatus \u003d\u003d Status.ERROR_CHECKSUM) {\n+        throw new IOException(\"Shutting down writer and responder \"\n+            + \"due to a checksum error in received data. The error \"\n+            + \"response has been sent upstream.\");\n+      }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private void sendAckUpstream(PipelineAck ack, long seqno,\n        long totalAckTimeNanos, long offsetInBlock,\n        Status myStatus) throws IOException {\n      Status[] replies \u003d null;\n      if (mirrorError) { // ack read error\n        replies \u003d MIRROR_ERROR_STATUS;\n      } else {\n        short ackLen \u003d type \u003d\u003d PacketResponderType.LAST_IN_PIPELINE ? 0 : ack\n            .getNumOfReplies();\n        replies \u003d new Status[1 + ackLen];\n        replies[0] \u003d myStatus;\n        for (int i \u003d 0; i \u003c ackLen; i++) {\n          replies[i + 1] \u003d ack.getReply(i);\n        }\n        // If the mirror has reported that it received a corrupt packet,\n        // do self-destruct to mark myself bad, instead of making the \n        // mirror node bad. The mirror is guaranteed to be good without\n        // corrupt data on disk.\n        if (ackLen \u003e 0 \u0026\u0026 replies[1] \u003d\u003d Status.ERROR_CHECKSUM) {\n          throw new IOException(\"Shutting down writer and responder \"\n              + \"since the down streams reported the data sent by this \"\n              + \"thread is corrupt\");\n        }\n      }\n      PipelineAck replyAck \u003d new PipelineAck(seqno, replies,\n          totalAckTimeNanos);\n      if (replyAck.isSuccess()\n          \u0026\u0026 offsetInBlock \u003e replicaInfo.getBytesAcked()) {\n        replicaInfo.setBytesAcked(offsetInBlock);\n      }\n\n      // send my ack back to upstream datanode\n      replyAck.write(upstreamOut);\n      upstreamOut.flush();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(myString + \", replyAck\u003d\" + replyAck);\n      }\n\n      // If a corruption was detected in the received data, terminate after\n      // sending ERROR_CHECKSUM back. \n      if (myStatus \u003d\u003d Status.ERROR_CHECKSUM) {\n        throw new IOException(\"Shutting down writer and responder \"\n            + \"due to a checksum error in received data. The error \"\n            + \"response has been sent upstream.\");\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
          "extendedDetails": {
            "oldValue": "[ack-PipelineAck, seqno-long, totalAckTimeNanos-long, offsetInBlock-long]",
            "newValue": "[ack-PipelineAck, seqno-long, totalAckTimeNanos-long, offsetInBlock-long, myStatus-Status]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3875. Issue handling checksum errors in write pipeline. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1484808 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/05/13 6:42 AM",
          "commitName": "98a692fd6361365db4afb9523a5d83ee32774112",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "06/03/13 11:15 AM",
          "commitNameOld": "638801cce16fc1dc3259c541dc30a599faaddda1",
          "commitAuthorOld": "Suresh Srinivas",
          "daysBetweenCommits": 75.77,
          "commitsBetweenForRepo": 469,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,28 +1,46 @@\n     private void sendAckUpstream(PipelineAck ack, long seqno,\n-        long totalAckTimeNanos, long offsetInBlock) throws IOException {\n+        long totalAckTimeNanos, long offsetInBlock,\n+        Status myStatus) throws IOException {\n       Status[] replies \u003d null;\n       if (mirrorError) { // ack read error\n         replies \u003d MIRROR_ERROR_STATUS;\n       } else {\n         short ackLen \u003d type \u003d\u003d PacketResponderType.LAST_IN_PIPELINE ? 0 : ack\n             .getNumOfReplies();\n         replies \u003d new Status[1 + ackLen];\n-        replies[0] \u003d Status.SUCCESS;\n+        replies[0] \u003d myStatus;\n         for (int i \u003d 0; i \u003c ackLen; i++) {\n           replies[i + 1] \u003d ack.getReply(i);\n         }\n+        // If the mirror has reported that it received a corrupt packet,\n+        // do self-destruct to mark myself bad, instead of making the \n+        // mirror node bad. The mirror is guaranteed to be good without\n+        // corrupt data on disk.\n+        if (ackLen \u003e 0 \u0026\u0026 replies[1] \u003d\u003d Status.ERROR_CHECKSUM) {\n+          throw new IOException(\"Shutting down writer and responder \"\n+              + \"since the down streams reported the data sent by this \"\n+              + \"thread is corrupt\");\n+        }\n       }\n       PipelineAck replyAck \u003d new PipelineAck(seqno, replies,\n           totalAckTimeNanos);\n       if (replyAck.isSuccess()\n           \u0026\u0026 offsetInBlock \u003e replicaInfo.getBytesAcked()) {\n         replicaInfo.setBytesAcked(offsetInBlock);\n       }\n \n       // send my ack back to upstream datanode\n       replyAck.write(upstreamOut);\n       upstreamOut.flush();\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(myString + \", replyAck\u003d\" + replyAck);\n       }\n+\n+      // If a corruption was detected in the received data, terminate after\n+      // sending ERROR_CHECKSUM back. \n+      if (myStatus \u003d\u003d Status.ERROR_CHECKSUM) {\n+        throw new IOException(\"Shutting down writer and responder \"\n+            + \"due to a checksum error in received data. The error \"\n+            + \"response has been sent upstream.\");\n+      }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    private void sendAckUpstream(PipelineAck ack, long seqno,\n        long totalAckTimeNanos, long offsetInBlock,\n        Status myStatus) throws IOException {\n      Status[] replies \u003d null;\n      if (mirrorError) { // ack read error\n        replies \u003d MIRROR_ERROR_STATUS;\n      } else {\n        short ackLen \u003d type \u003d\u003d PacketResponderType.LAST_IN_PIPELINE ? 0 : ack\n            .getNumOfReplies();\n        replies \u003d new Status[1 + ackLen];\n        replies[0] \u003d myStatus;\n        for (int i \u003d 0; i \u003c ackLen; i++) {\n          replies[i + 1] \u003d ack.getReply(i);\n        }\n        // If the mirror has reported that it received a corrupt packet,\n        // do self-destruct to mark myself bad, instead of making the \n        // mirror node bad. The mirror is guaranteed to be good without\n        // corrupt data on disk.\n        if (ackLen \u003e 0 \u0026\u0026 replies[1] \u003d\u003d Status.ERROR_CHECKSUM) {\n          throw new IOException(\"Shutting down writer and responder \"\n              + \"since the down streams reported the data sent by this \"\n              + \"thread is corrupt\");\n        }\n      }\n      PipelineAck replyAck \u003d new PipelineAck(seqno, replies,\n          totalAckTimeNanos);\n      if (replyAck.isSuccess()\n          \u0026\u0026 offsetInBlock \u003e replicaInfo.getBytesAcked()) {\n        replicaInfo.setBytesAcked(offsetInBlock);\n      }\n\n      // send my ack back to upstream datanode\n      replyAck.write(upstreamOut);\n      upstreamOut.flush();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(myString + \", replyAck\u003d\" + replyAck);\n      }\n\n      // If a corruption was detected in the received data, terminate after\n      // sending ERROR_CHECKSUM back. \n      if (myStatus \u003d\u003d Status.ERROR_CHECKSUM) {\n        throw new IOException(\"Shutting down writer and responder \"\n            + \"due to a checksum error in received data. The error \"\n            + \"response has been sent upstream.\");\n      }\n    }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java",
          "extendedDetails": {}
        }
      ]
    },
    "7e56bfe40589a1aa9b5ef20b342e421823cd0592": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4200. Reduce the size of synchronized sections in PacketResponder. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1413826 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/11/12 12:47 PM",
      "commitName": "7e56bfe40589a1aa9b5ef20b342e421823cd0592",
      "commitAuthor": "Suresh Srinivas",
      "diff": "@@ -0,0 +1,28 @@\n+    private void sendAckUpstream(PipelineAck ack, long seqno,\n+        long totalAckTimeNanos, long offsetInBlock) throws IOException {\n+      Status[] replies \u003d null;\n+      if (mirrorError) { // ack read error\n+        replies \u003d MIRROR_ERROR_STATUS;\n+      } else {\n+        short ackLen \u003d type \u003d\u003d PacketResponderType.LAST_IN_PIPELINE ? 0 : ack\n+            .getNumOfReplies();\n+        replies \u003d new Status[1 + ackLen];\n+        replies[0] \u003d Status.SUCCESS;\n+        for (int i \u003d 0; i \u003c ackLen; i++) {\n+          replies[i + 1] \u003d ack.getReply(i);\n+        }\n+      }\n+      PipelineAck replyAck \u003d new PipelineAck(seqno, replies,\n+          totalAckTimeNanos);\n+      if (replyAck.isSuccess()\n+          \u0026\u0026 offsetInBlock \u003e replicaInfo.getBytesAcked()) {\n+        replicaInfo.setBytesAcked(offsetInBlock);\n+      }\n+\n+      // send my ack back to upstream datanode\n+      replyAck.write(upstreamOut);\n+      upstreamOut.flush();\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(myString + \", replyAck\u003d\" + replyAck);\n+      }\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void sendAckUpstream(PipelineAck ack, long seqno,\n        long totalAckTimeNanos, long offsetInBlock) throws IOException {\n      Status[] replies \u003d null;\n      if (mirrorError) { // ack read error\n        replies \u003d MIRROR_ERROR_STATUS;\n      } else {\n        short ackLen \u003d type \u003d\u003d PacketResponderType.LAST_IN_PIPELINE ? 0 : ack\n            .getNumOfReplies();\n        replies \u003d new Status[1 + ackLen];\n        replies[0] \u003d Status.SUCCESS;\n        for (int i \u003d 0; i \u003c ackLen; i++) {\n          replies[i + 1] \u003d ack.getReply(i);\n        }\n      }\n      PipelineAck replyAck \u003d new PipelineAck(seqno, replies,\n          totalAckTimeNanos);\n      if (replyAck.isSuccess()\n          \u0026\u0026 offsetInBlock \u003e replicaInfo.getBytesAcked()) {\n        replicaInfo.setBytesAcked(offsetInBlock);\n      }\n\n      // send my ack back to upstream datanode\n      replyAck.write(upstreamOut);\n      upstreamOut.flush();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(myString + \", replyAck\u003d\" + replyAck);\n      }\n    }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java"
    }
  }
}