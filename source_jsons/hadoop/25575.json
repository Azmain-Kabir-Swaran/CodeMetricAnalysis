{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LocalContainerLauncher.java",
  "functionName": "run",
  "functionId": "run",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
  "functionStartLine": 232,
  "functionEndLine": 314,
  "numCommitsSeen": 32,
  "timeTaken": 9982,
  "changeHistory": [
    "4a1cedc010d3fa1d8ef3f2773ca12acadfee5ba5",
    "826715622e6937887a4e20b3ce327d7e2fd89009",
    "444836b3dcd3ee28238af7b5e753d644e8095788",
    "6957745c2c73cae038ac7960115ffc32de05b953",
    "3d95049f79fe7edb92dd6d20c3a60ccdc46c4b0e",
    "b7ae5a6cb7b2d3e3112ac53007e984caeb07de58",
    "ade0f0560f729e50382c6992f713f29e2dd5b270",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "4a1cedc010d3fa1d8ef3f2773ca12acadfee5ba5": "Ybodychange",
    "826715622e6937887a4e20b3ce327d7e2fd89009": "Ybodychange",
    "444836b3dcd3ee28238af7b5e753d644e8095788": "Ybodychange",
    "6957745c2c73cae038ac7960115ffc32de05b953": "Ybodychange",
    "3d95049f79fe7edb92dd6d20c3a60ccdc46c4b0e": "Ybodychange",
    "b7ae5a6cb7b2d3e3112ac53007e984caeb07de58": "Ybodychange",
    "ade0f0560f729e50382c6992f713f29e2dd5b270": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Ymultichange(Ymovefromfile,Ybodychange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "4a1cedc010d3fa1d8ef3f2773ca12acadfee5ba5": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5044. Have AM trigger jstack on task attempts that timeout before killing them. (Eric Payne and Gera Shegalov via mingma)\n",
      "commitDate": "06/06/16 2:30 PM",
      "commitName": "4a1cedc010d3fa1d8ef3f2773ca12acadfee5ba5",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "18/02/16 12:48 AM",
      "commitNameOld": "2440671a117f165dcda5056404bc898df3c50803",
      "commitAuthorOld": "Varun Vasudev",
      "daysBetweenCommits": 109.53,
      "commitsBetweenForRepo": 694,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,83 @@\n     public void run() {\n       ContainerLauncherEvent event \u003d null;\n \n       // Collect locations of map outputs to give to reduces\n       final Map\u003cTaskAttemptID, MapOutputFile\u003e localMapFiles \u003d\n           new HashMap\u003cTaskAttemptID, MapOutputFile\u003e();\n       \n       // _must_ either run subtasks sequentially or accept expense of new JVMs\n       // (i.e., fork()), else will get weird failures when maps try to create/\n       // write same dirname or filename:  no chdir() in Java\n       while (!Thread.currentThread().isInterrupted()) {\n         try {\n           event \u003d eventQueue.take();\n         } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n           LOG.warn(\"Returning, interrupted : \" + e);\n           break;\n         }\n \n         LOG.info(\"Processing the event \" + event.toString());\n \n         if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n \n           final ContainerRemoteLaunchEvent launchEv \u003d\n               (ContainerRemoteLaunchEvent)event;\n           \n           // execute the task on a separate thread\n           Future\u003c?\u003e future \u003d taskRunner.submit(new Runnable() {\n             public void run() {\n               runTask(launchEv, localMapFiles);\n             }\n           });\n           // remember the current attempt\n           futures.put(event.getTaskAttemptID(), future);\n \n         } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n \n+          if (event.getDumpContainerThreads()) {\n+            try {\n+              // Construct full thread dump header\n+              System.out.println(new java.util.Date());\n+              RuntimeMXBean rtBean \u003d ManagementFactory.getRuntimeMXBean();\n+              System.out.println(\"Full thread dump \" + rtBean.getVmName()\n+                  + \" (\" + rtBean.getVmVersion()\n+                  + \" \" + rtBean.getSystemProperties().get(\"java.vm.info\")\n+                  + \"):\\n\");\n+              // Dump threads\u0027 states and stacks\n+              ThreadMXBean tmxBean \u003d ManagementFactory.getThreadMXBean();\n+              ThreadInfo[] tInfos \u003d tmxBean.dumpAllThreads(\n+                  tmxBean.isObjectMonitorUsageSupported(),\n+                  tmxBean.isSynchronizerUsageSupported());\n+              for (ThreadInfo ti : tInfos) {\n+                System.out.println(ti.toString());\n+              }\n+            } catch (Throwable t) {\n+              // Failure to dump stack shouldn\u0027t cause method failure.\n+              System.out.println(\"Could not create full thread dump: \"\n+                  + t.getMessage());\n+            }\n+          }\n+\n           // cancel (and interrupt) the current running task associated with the\n           // event\n           TaskAttemptId taId \u003d event.getTaskAttemptID();\n           Future\u003c?\u003e future \u003d futures.remove(taId);\n           if (future !\u003d null) {\n             LOG.info(\"canceling the task attempt \" + taId);\n             future.cancel(true);\n           }\n \n           // send \"cleaned\" event to task attempt to move us from\n           // SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state (or \n           // {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n           context.getEventHandler().handle(\n               new TaskAttemptEvent(taId,\n                   TaskAttemptEventType.TA_CONTAINER_CLEANED));\n         } else if (event.getType() \u003d\u003d EventType.CONTAINER_COMPLETED) {\n           LOG.debug(\"Container completed \" + event.toString());\n         } else {\n           LOG.warn(\"Ignoring unexpected event \" + event.toString());\n         }\n \n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      ContainerLauncherEvent event \u003d null;\n\n      // Collect locations of map outputs to give to reduces\n      final Map\u003cTaskAttemptID, MapOutputFile\u003e localMapFiles \u003d\n          new HashMap\u003cTaskAttemptID, MapOutputFile\u003e();\n      \n      // _must_ either run subtasks sequentially or accept expense of new JVMs\n      // (i.e., fork()), else will get weird failures when maps try to create/\n      // write same dirname or filename:  no chdir() in Java\n      while (!Thread.currentThread().isInterrupted()) {\n        try {\n          event \u003d eventQueue.take();\n        } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n          LOG.warn(\"Returning, interrupted : \" + e);\n          break;\n        }\n\n        LOG.info(\"Processing the event \" + event.toString());\n\n        if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n\n          final ContainerRemoteLaunchEvent launchEv \u003d\n              (ContainerRemoteLaunchEvent)event;\n          \n          // execute the task on a separate thread\n          Future\u003c?\u003e future \u003d taskRunner.submit(new Runnable() {\n            public void run() {\n              runTask(launchEv, localMapFiles);\n            }\n          });\n          // remember the current attempt\n          futures.put(event.getTaskAttemptID(), future);\n\n        } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n\n          if (event.getDumpContainerThreads()) {\n            try {\n              // Construct full thread dump header\n              System.out.println(new java.util.Date());\n              RuntimeMXBean rtBean \u003d ManagementFactory.getRuntimeMXBean();\n              System.out.println(\"Full thread dump \" + rtBean.getVmName()\n                  + \" (\" + rtBean.getVmVersion()\n                  + \" \" + rtBean.getSystemProperties().get(\"java.vm.info\")\n                  + \"):\\n\");\n              // Dump threads\u0027 states and stacks\n              ThreadMXBean tmxBean \u003d ManagementFactory.getThreadMXBean();\n              ThreadInfo[] tInfos \u003d tmxBean.dumpAllThreads(\n                  tmxBean.isObjectMonitorUsageSupported(),\n                  tmxBean.isSynchronizerUsageSupported());\n              for (ThreadInfo ti : tInfos) {\n                System.out.println(ti.toString());\n              }\n            } catch (Throwable t) {\n              // Failure to dump stack shouldn\u0027t cause method failure.\n              System.out.println(\"Could not create full thread dump: \"\n                  + t.getMessage());\n            }\n          }\n\n          // cancel (and interrupt) the current running task associated with the\n          // event\n          TaskAttemptId taId \u003d event.getTaskAttemptID();\n          Future\u003c?\u003e future \u003d futures.remove(taId);\n          if (future !\u003d null) {\n            LOG.info(\"canceling the task attempt \" + taId);\n            future.cancel(true);\n          }\n\n          // send \"cleaned\" event to task attempt to move us from\n          // SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state (or \n          // {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n          context.getEventHandler().handle(\n              new TaskAttemptEvent(taId,\n                  TaskAttemptEventType.TA_CONTAINER_CLEANED));\n        } else if (event.getType() \u003d\u003d EventType.CONTAINER_COMPLETED) {\n          LOG.debug(\"Container completed \" + event.toString());\n        } else {\n          LOG.warn(\"Ignoring unexpected event \" + event.toString());\n        }\n\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
      "extendedDetails": {}
    },
    "826715622e6937887a4e20b3ce327d7e2fd89009": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6420. Interrupted Exception in LocalContainerLauncher should be logged in warn/info level. Contributed by Chang Li\n",
      "commitDate": "01/07/15 10:50 AM",
      "commitName": "826715622e6937887a4e20b3ce327d7e2fd89009",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "14/05/15 4:07 PM",
      "commitNameOld": "6b710a42e00acca405e085724c89cda016cf7442",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 47.78,
      "commitsBetweenForRepo": 345,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,59 @@\n     public void run() {\n       ContainerLauncherEvent event \u003d null;\n \n       // Collect locations of map outputs to give to reduces\n       final Map\u003cTaskAttemptID, MapOutputFile\u003e localMapFiles \u003d\n           new HashMap\u003cTaskAttemptID, MapOutputFile\u003e();\n       \n       // _must_ either run subtasks sequentially or accept expense of new JVMs\n       // (i.e., fork()), else will get weird failures when maps try to create/\n       // write same dirname or filename:  no chdir() in Java\n       while (!Thread.currentThread().isInterrupted()) {\n         try {\n           event \u003d eventQueue.take();\n         } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n-          LOG.error(\"Returning, interrupted : \" + e);\n+          LOG.warn(\"Returning, interrupted : \" + e);\n           break;\n         }\n \n         LOG.info(\"Processing the event \" + event.toString());\n \n         if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n \n           final ContainerRemoteLaunchEvent launchEv \u003d\n               (ContainerRemoteLaunchEvent)event;\n           \n           // execute the task on a separate thread\n           Future\u003c?\u003e future \u003d taskRunner.submit(new Runnable() {\n             public void run() {\n               runTask(launchEv, localMapFiles);\n             }\n           });\n           // remember the current attempt\n           futures.put(event.getTaskAttemptID(), future);\n \n         } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n \n           // cancel (and interrupt) the current running task associated with the\n           // event\n           TaskAttemptId taId \u003d event.getTaskAttemptID();\n           Future\u003c?\u003e future \u003d futures.remove(taId);\n           if (future !\u003d null) {\n             LOG.info(\"canceling the task attempt \" + taId);\n             future.cancel(true);\n           }\n \n           // send \"cleaned\" event to task attempt to move us from\n           // SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state (or \n           // {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n           context.getEventHandler().handle(\n               new TaskAttemptEvent(taId,\n                   TaskAttemptEventType.TA_CONTAINER_CLEANED));\n         } else if (event.getType() \u003d\u003d EventType.CONTAINER_COMPLETED) {\n           LOG.debug(\"Container completed \" + event.toString());\n         } else {\n           LOG.warn(\"Ignoring unexpected event \" + event.toString());\n         }\n \n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      ContainerLauncherEvent event \u003d null;\n\n      // Collect locations of map outputs to give to reduces\n      final Map\u003cTaskAttemptID, MapOutputFile\u003e localMapFiles \u003d\n          new HashMap\u003cTaskAttemptID, MapOutputFile\u003e();\n      \n      // _must_ either run subtasks sequentially or accept expense of new JVMs\n      // (i.e., fork()), else will get weird failures when maps try to create/\n      // write same dirname or filename:  no chdir() in Java\n      while (!Thread.currentThread().isInterrupted()) {\n        try {\n          event \u003d eventQueue.take();\n        } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n          LOG.warn(\"Returning, interrupted : \" + e);\n          break;\n        }\n\n        LOG.info(\"Processing the event \" + event.toString());\n\n        if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n\n          final ContainerRemoteLaunchEvent launchEv \u003d\n              (ContainerRemoteLaunchEvent)event;\n          \n          // execute the task on a separate thread\n          Future\u003c?\u003e future \u003d taskRunner.submit(new Runnable() {\n            public void run() {\n              runTask(launchEv, localMapFiles);\n            }\n          });\n          // remember the current attempt\n          futures.put(event.getTaskAttemptID(), future);\n\n        } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n\n          // cancel (and interrupt) the current running task associated with the\n          // event\n          TaskAttemptId taId \u003d event.getTaskAttemptID();\n          Future\u003c?\u003e future \u003d futures.remove(taId);\n          if (future !\u003d null) {\n            LOG.info(\"canceling the task attempt \" + taId);\n            future.cancel(true);\n          }\n\n          // send \"cleaned\" event to task attempt to move us from\n          // SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state (or \n          // {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n          context.getEventHandler().handle(\n              new TaskAttemptEvent(taId,\n                  TaskAttemptEventType.TA_CONTAINER_CLEANED));\n        } else if (event.getType() \u003d\u003d EventType.CONTAINER_COMPLETED) {\n          LOG.debug(\"Container completed \" + event.toString());\n        } else {\n          LOG.warn(\"Ignoring unexpected event \" + event.toString());\n        }\n\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
      "extendedDetails": {}
    },
    "444836b3dcd3ee28238af7b5e753d644e8095788": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5465. Tasks are often killed before they exit on their own. Contributed by Ming Ma\n",
      "commitDate": "11/05/15 3:37 PM",
      "commitName": "444836b3dcd3ee28238af7b5e753d644e8095788",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "21/04/15 1:57 PM",
      "commitNameOld": "725eb52ddc647074f0bf1cc73c3029f1352f51d5",
      "commitAuthorOld": "Gera Shegalov",
      "daysBetweenCommits": 20.07,
      "commitsBetweenForRepo": 261,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,58 +1,59 @@\n     public void run() {\n       ContainerLauncherEvent event \u003d null;\n \n       // Collect locations of map outputs to give to reduces\n       final Map\u003cTaskAttemptID, MapOutputFile\u003e localMapFiles \u003d\n           new HashMap\u003cTaskAttemptID, MapOutputFile\u003e();\n       \n       // _must_ either run subtasks sequentially or accept expense of new JVMs\n       // (i.e., fork()), else will get weird failures when maps try to create/\n       // write same dirname or filename:  no chdir() in Java\n       while (!Thread.currentThread().isInterrupted()) {\n         try {\n           event \u003d eventQueue.take();\n         } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n           LOG.error(\"Returning, interrupted : \" + e);\n           break;\n         }\n \n         LOG.info(\"Processing the event \" + event.toString());\n \n         if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n \n           final ContainerRemoteLaunchEvent launchEv \u003d\n               (ContainerRemoteLaunchEvent)event;\n           \n           // execute the task on a separate thread\n           Future\u003c?\u003e future \u003d taskRunner.submit(new Runnable() {\n             public void run() {\n               runTask(launchEv, localMapFiles);\n             }\n           });\n           // remember the current attempt\n           futures.put(event.getTaskAttemptID(), future);\n \n         } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n \n           // cancel (and interrupt) the current running task associated with the\n           // event\n           TaskAttemptId taId \u003d event.getTaskAttemptID();\n           Future\u003c?\u003e future \u003d futures.remove(taId);\n           if (future !\u003d null) {\n             LOG.info(\"canceling the task attempt \" + taId);\n             future.cancel(true);\n           }\n \n           // send \"cleaned\" event to task attempt to move us from\n           // SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state (or \n           // {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n           context.getEventHandler().handle(\n               new TaskAttemptEvent(taId,\n                   TaskAttemptEventType.TA_CONTAINER_CLEANED));\n-\n+        } else if (event.getType() \u003d\u003d EventType.CONTAINER_COMPLETED) {\n+          LOG.debug(\"Container completed \" + event.toString());\n         } else {\n           LOG.warn(\"Ignoring unexpected event \" + event.toString());\n         }\n \n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      ContainerLauncherEvent event \u003d null;\n\n      // Collect locations of map outputs to give to reduces\n      final Map\u003cTaskAttemptID, MapOutputFile\u003e localMapFiles \u003d\n          new HashMap\u003cTaskAttemptID, MapOutputFile\u003e();\n      \n      // _must_ either run subtasks sequentially or accept expense of new JVMs\n      // (i.e., fork()), else will get weird failures when maps try to create/\n      // write same dirname or filename:  no chdir() in Java\n      while (!Thread.currentThread().isInterrupted()) {\n        try {\n          event \u003d eventQueue.take();\n        } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n          LOG.error(\"Returning, interrupted : \" + e);\n          break;\n        }\n\n        LOG.info(\"Processing the event \" + event.toString());\n\n        if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n\n          final ContainerRemoteLaunchEvent launchEv \u003d\n              (ContainerRemoteLaunchEvent)event;\n          \n          // execute the task on a separate thread\n          Future\u003c?\u003e future \u003d taskRunner.submit(new Runnable() {\n            public void run() {\n              runTask(launchEv, localMapFiles);\n            }\n          });\n          // remember the current attempt\n          futures.put(event.getTaskAttemptID(), future);\n\n        } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n\n          // cancel (and interrupt) the current running task associated with the\n          // event\n          TaskAttemptId taId \u003d event.getTaskAttemptID();\n          Future\u003c?\u003e future \u003d futures.remove(taId);\n          if (future !\u003d null) {\n            LOG.info(\"canceling the task attempt \" + taId);\n            future.cancel(true);\n          }\n\n          // send \"cleaned\" event to task attempt to move us from\n          // SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state (or \n          // {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n          context.getEventHandler().handle(\n              new TaskAttemptEvent(taId,\n                  TaskAttemptEventType.TA_CONTAINER_CLEANED));\n        } else if (event.getType() \u003d\u003d EventType.CONTAINER_COMPLETED) {\n          LOG.debug(\"Container completed \" + event.toString());\n        } else {\n          LOG.warn(\"Ignoring unexpected event \" + event.toString());\n        }\n\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
      "extendedDetails": {}
    },
    "6957745c2c73cae038ac7960115ffc32de05b953": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5841. uber job doesn\u0027t terminate on getting mapred job kill. Contributed by Sangjin Lee\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1589524 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/04/14 2:53 PM",
      "commitName": "6957745c2c73cae038ac7960115ffc32de05b953",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "13/11/13 11:56 PM",
      "commitNameOld": "3d95049f79fe7edb92dd6d20c3a60ccdc46c4b0e",
      "commitAuthorOld": "Sanford Ryza",
      "daysBetweenCommits": 160.58,
      "commitsBetweenForRepo": 1119,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,98 +1,58 @@\n     public void run() {\n       ContainerLauncherEvent event \u003d null;\n \n       // Collect locations of map outputs to give to reduces\n-      Map\u003cTaskAttemptID, MapOutputFile\u003e localMapFiles \u003d\n+      final Map\u003cTaskAttemptID, MapOutputFile\u003e localMapFiles \u003d\n           new HashMap\u003cTaskAttemptID, MapOutputFile\u003e();\n       \n       // _must_ either run subtasks sequentially or accept expense of new JVMs\n       // (i.e., fork()), else will get weird failures when maps try to create/\n       // write same dirname or filename:  no chdir() in Java\n       while (!Thread.currentThread().isInterrupted()) {\n         try {\n           event \u003d eventQueue.take();\n         } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n           LOG.error(\"Returning, interrupted : \" + e);\n-          return;\n+          break;\n         }\n \n         LOG.info(\"Processing the event \" + event.toString());\n \n         if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n \n-          ContainerRemoteLaunchEvent launchEv \u003d\n+          final ContainerRemoteLaunchEvent launchEv \u003d\n               (ContainerRemoteLaunchEvent)event;\n-          TaskAttemptId attemptID \u003d launchEv.getTaskAttemptID(); \n-\n-          Job job \u003d context.getAllJobs().get(attemptID.getTaskId().getJobId());\n-          int numMapTasks \u003d job.getTotalMaps();\n-          int numReduceTasks \u003d job.getTotalReduces();\n-\n-          // YARN (tracking) Task:\n-          org.apache.hadoop.mapreduce.v2.app.job.Task ytask \u003d\n-              job.getTask(attemptID.getTaskId());\n-          // classic mapred Task:\n-          org.apache.hadoop.mapred.Task remoteTask \u003d launchEv.getRemoteTask();\n-\n-          // after \"launching,\" send launched event to task attempt to move\n-          // state from ASSIGNED to RUNNING (also nukes \"remoteTask\", so must\n-          // do getRemoteTask() call first)\n           \n-          //There is no port number because we are not really talking to a task\n-          // tracker.  The shuffle is just done through local files.  So the\n-          // port number is set to -1 in this case.\n-          context.getEventHandler().handle(\n-              new TaskAttemptContainerLaunchedEvent(attemptID, -1));\n-\n-          if (numMapTasks \u003d\u003d 0) {\n-            doneWithMaps \u003d true;\n-          }\n-\n-          try {\n-            if (remoteTask.isMapOrReduce()) {\n-              JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n-              jce.addCounterUpdate(JobCounter.TOTAL_LAUNCHED_UBERTASKS, 1);\n-              if (remoteTask.isMapTask()) {\n-                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBMAPS, 1);\n-              } else {\n-                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBREDUCES, 1);\n-              }\n-              context.getEventHandler().handle(jce);\n+          // execute the task on a separate thread\n+          Future\u003c?\u003e future \u003d taskRunner.submit(new Runnable() {\n+            public void run() {\n+              runTask(launchEv, localMapFiles);\n             }\n-            runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks,\n-                       (numReduceTasks \u003e 0), localMapFiles);\n-            \n-          } catch (RuntimeException re) {\n-            JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n-            jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);\n-            context.getEventHandler().handle(jce);\n-            // this is our signal that the subtask failed in some way, so\n-            // simulate a failed JVM/container and send a container-completed\n-            // event to task attempt (i.e., move state machine from RUNNING\n-            // to FAIL_CONTAINER_CLEANUP [and ultimately to FAILED])\n-            context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n-                TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n-          } catch (IOException ioe) {\n-            // if umbilical itself barfs (in error-handler of runSubMap()),\n-            // we\u0027re pretty much hosed, so do what YarnChild main() does\n-            // (i.e., exit clumsily--but can never happen, so no worries!)\n-            LOG.fatal(\"oopsie...  this can never happen: \"\n-                + StringUtils.stringifyException(ioe));\n-            System.exit(-1);\n-          }\n+          });\n+          // remember the current attempt\n+          futures.put(event.getTaskAttemptID(), future);\n \n         } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n \n-          // no container to kill, so just send \"cleaned\" event to task attempt\n-          // to move us from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state\n-          // (or {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n+          // cancel (and interrupt) the current running task associated with the\n+          // event\n+          TaskAttemptId taId \u003d event.getTaskAttemptID();\n+          Future\u003c?\u003e future \u003d futures.remove(taId);\n+          if (future !\u003d null) {\n+            LOG.info(\"canceling the task attempt \" + taId);\n+            future.cancel(true);\n+          }\n+\n+          // send \"cleaned\" event to task attempt to move us from\n+          // SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state (or \n+          // {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n           context.getEventHandler().handle(\n-              new TaskAttemptEvent(event.getTaskAttemptID(),\n+              new TaskAttemptEvent(taId,\n                   TaskAttemptEventType.TA_CONTAINER_CLEANED));\n \n         } else {\n           LOG.warn(\"Ignoring unexpected event \" + event.toString());\n         }\n \n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      ContainerLauncherEvent event \u003d null;\n\n      // Collect locations of map outputs to give to reduces\n      final Map\u003cTaskAttemptID, MapOutputFile\u003e localMapFiles \u003d\n          new HashMap\u003cTaskAttemptID, MapOutputFile\u003e();\n      \n      // _must_ either run subtasks sequentially or accept expense of new JVMs\n      // (i.e., fork()), else will get weird failures when maps try to create/\n      // write same dirname or filename:  no chdir() in Java\n      while (!Thread.currentThread().isInterrupted()) {\n        try {\n          event \u003d eventQueue.take();\n        } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n          LOG.error(\"Returning, interrupted : \" + e);\n          break;\n        }\n\n        LOG.info(\"Processing the event \" + event.toString());\n\n        if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n\n          final ContainerRemoteLaunchEvent launchEv \u003d\n              (ContainerRemoteLaunchEvent)event;\n          \n          // execute the task on a separate thread\n          Future\u003c?\u003e future \u003d taskRunner.submit(new Runnable() {\n            public void run() {\n              runTask(launchEv, localMapFiles);\n            }\n          });\n          // remember the current attempt\n          futures.put(event.getTaskAttemptID(), future);\n\n        } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n\n          // cancel (and interrupt) the current running task associated with the\n          // event\n          TaskAttemptId taId \u003d event.getTaskAttemptID();\n          Future\u003c?\u003e future \u003d futures.remove(taId);\n          if (future !\u003d null) {\n            LOG.info(\"canceling the task attempt \" + taId);\n            future.cancel(true);\n          }\n\n          // send \"cleaned\" event to task attempt to move us from\n          // SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state (or \n          // {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n          context.getEventHandler().handle(\n              new TaskAttemptEvent(taId,\n                  TaskAttemptEventType.TA_CONTAINER_CLEANED));\n\n        } else {\n          LOG.warn(\"Ignoring unexpected event \" + event.toString());\n        }\n\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
      "extendedDetails": {}
    },
    "3d95049f79fe7edb92dd6d20c3a60ccdc46c4b0e": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5481. Enable uber jobs to have multiple reducers (Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1541844 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/11/13 11:56 PM",
      "commitName": "3d95049f79fe7edb92dd6d20c3a60ccdc46c4b0e",
      "commitAuthor": "Sanford Ryza",
      "commitDateOld": "16/06/13 11:39 PM",
      "commitNameOld": "b9efe6bd4a1277b4067ecde715a7713a85968886",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 150.05,
      "commitsBetweenForRepo": 919,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,94 +1,98 @@\n     public void run() {\n       ContainerLauncherEvent event \u003d null;\n \n+      // Collect locations of map outputs to give to reduces\n+      Map\u003cTaskAttemptID, MapOutputFile\u003e localMapFiles \u003d\n+          new HashMap\u003cTaskAttemptID, MapOutputFile\u003e();\n+      \n       // _must_ either run subtasks sequentially or accept expense of new JVMs\n       // (i.e., fork()), else will get weird failures when maps try to create/\n       // write same dirname or filename:  no chdir() in Java\n       while (!Thread.currentThread().isInterrupted()) {\n         try {\n           event \u003d eventQueue.take();\n         } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n           LOG.error(\"Returning, interrupted : \" + e);\n           return;\n         }\n \n         LOG.info(\"Processing the event \" + event.toString());\n \n         if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n \n           ContainerRemoteLaunchEvent launchEv \u003d\n               (ContainerRemoteLaunchEvent)event;\n           TaskAttemptId attemptID \u003d launchEv.getTaskAttemptID(); \n \n           Job job \u003d context.getAllJobs().get(attemptID.getTaskId().getJobId());\n           int numMapTasks \u003d job.getTotalMaps();\n           int numReduceTasks \u003d job.getTotalReduces();\n \n           // YARN (tracking) Task:\n           org.apache.hadoop.mapreduce.v2.app.job.Task ytask \u003d\n               job.getTask(attemptID.getTaskId());\n           // classic mapred Task:\n           org.apache.hadoop.mapred.Task remoteTask \u003d launchEv.getRemoteTask();\n \n           // after \"launching,\" send launched event to task attempt to move\n           // state from ASSIGNED to RUNNING (also nukes \"remoteTask\", so must\n           // do getRemoteTask() call first)\n           \n           //There is no port number because we are not really talking to a task\n           // tracker.  The shuffle is just done through local files.  So the\n           // port number is set to -1 in this case.\n           context.getEventHandler().handle(\n               new TaskAttemptContainerLaunchedEvent(attemptID, -1));\n \n           if (numMapTasks \u003d\u003d 0) {\n             doneWithMaps \u003d true;\n           }\n \n           try {\n             if (remoteTask.isMapOrReduce()) {\n               JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n               jce.addCounterUpdate(JobCounter.TOTAL_LAUNCHED_UBERTASKS, 1);\n               if (remoteTask.isMapTask()) {\n                 jce.addCounterUpdate(JobCounter.NUM_UBER_SUBMAPS, 1);\n               } else {\n                 jce.addCounterUpdate(JobCounter.NUM_UBER_SUBREDUCES, 1);\n               }\n               context.getEventHandler().handle(jce);\n             }\n             runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks,\n-                       (numReduceTasks \u003e 0));\n+                       (numReduceTasks \u003e 0), localMapFiles);\n             \n           } catch (RuntimeException re) {\n             JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n             jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);\n             context.getEventHandler().handle(jce);\n             // this is our signal that the subtask failed in some way, so\n             // simulate a failed JVM/container and send a container-completed\n             // event to task attempt (i.e., move state machine from RUNNING\n             // to FAIL_CONTAINER_CLEANUP [and ultimately to FAILED])\n             context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n                 TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n           } catch (IOException ioe) {\n             // if umbilical itself barfs (in error-handler of runSubMap()),\n             // we\u0027re pretty much hosed, so do what YarnChild main() does\n             // (i.e., exit clumsily--but can never happen, so no worries!)\n             LOG.fatal(\"oopsie...  this can never happen: \"\n                 + StringUtils.stringifyException(ioe));\n             System.exit(-1);\n           }\n \n         } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n \n           // no container to kill, so just send \"cleaned\" event to task attempt\n           // to move us from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state\n           // (or {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n           context.getEventHandler().handle(\n               new TaskAttemptEvent(event.getTaskAttemptID(),\n                   TaskAttemptEventType.TA_CONTAINER_CLEANED));\n \n         } else {\n           LOG.warn(\"Ignoring unexpected event \" + event.toString());\n         }\n \n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      ContainerLauncherEvent event \u003d null;\n\n      // Collect locations of map outputs to give to reduces\n      Map\u003cTaskAttemptID, MapOutputFile\u003e localMapFiles \u003d\n          new HashMap\u003cTaskAttemptID, MapOutputFile\u003e();\n      \n      // _must_ either run subtasks sequentially or accept expense of new JVMs\n      // (i.e., fork()), else will get weird failures when maps try to create/\n      // write same dirname or filename:  no chdir() in Java\n      while (!Thread.currentThread().isInterrupted()) {\n        try {\n          event \u003d eventQueue.take();\n        } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n          LOG.error(\"Returning, interrupted : \" + e);\n          return;\n        }\n\n        LOG.info(\"Processing the event \" + event.toString());\n\n        if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n\n          ContainerRemoteLaunchEvent launchEv \u003d\n              (ContainerRemoteLaunchEvent)event;\n          TaskAttemptId attemptID \u003d launchEv.getTaskAttemptID(); \n\n          Job job \u003d context.getAllJobs().get(attemptID.getTaskId().getJobId());\n          int numMapTasks \u003d job.getTotalMaps();\n          int numReduceTasks \u003d job.getTotalReduces();\n\n          // YARN (tracking) Task:\n          org.apache.hadoop.mapreduce.v2.app.job.Task ytask \u003d\n              job.getTask(attemptID.getTaskId());\n          // classic mapred Task:\n          org.apache.hadoop.mapred.Task remoteTask \u003d launchEv.getRemoteTask();\n\n          // after \"launching,\" send launched event to task attempt to move\n          // state from ASSIGNED to RUNNING (also nukes \"remoteTask\", so must\n          // do getRemoteTask() call first)\n          \n          //There is no port number because we are not really talking to a task\n          // tracker.  The shuffle is just done through local files.  So the\n          // port number is set to -1 in this case.\n          context.getEventHandler().handle(\n              new TaskAttemptContainerLaunchedEvent(attemptID, -1));\n\n          if (numMapTasks \u003d\u003d 0) {\n            doneWithMaps \u003d true;\n          }\n\n          try {\n            if (remoteTask.isMapOrReduce()) {\n              JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n              jce.addCounterUpdate(JobCounter.TOTAL_LAUNCHED_UBERTASKS, 1);\n              if (remoteTask.isMapTask()) {\n                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBMAPS, 1);\n              } else {\n                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBREDUCES, 1);\n              }\n              context.getEventHandler().handle(jce);\n            }\n            runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks,\n                       (numReduceTasks \u003e 0), localMapFiles);\n            \n          } catch (RuntimeException re) {\n            JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n            jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);\n            context.getEventHandler().handle(jce);\n            // this is our signal that the subtask failed in some way, so\n            // simulate a failed JVM/container and send a container-completed\n            // event to task attempt (i.e., move state machine from RUNNING\n            // to FAIL_CONTAINER_CLEANUP [and ultimately to FAILED])\n            context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n                TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n          } catch (IOException ioe) {\n            // if umbilical itself barfs (in error-handler of runSubMap()),\n            // we\u0027re pretty much hosed, so do what YarnChild main() does\n            // (i.e., exit clumsily--but can never happen, so no worries!)\n            LOG.fatal(\"oopsie...  this can never happen: \"\n                + StringUtils.stringifyException(ioe));\n            System.exit(-1);\n          }\n\n        } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n\n          // no container to kill, so just send \"cleaned\" event to task attempt\n          // to move us from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state\n          // (or {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n          context.getEventHandler().handle(\n              new TaskAttemptEvent(event.getTaskAttemptID(),\n                  TaskAttemptEventType.TA_CONTAINER_CLEANED));\n\n        } else {\n          LOG.warn(\"Ignoring unexpected event \" + event.toString());\n        }\n\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
      "extendedDetails": {}
    },
    "b7ae5a6cb7b2d3e3112ac53007e984caeb07de58": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3426. Fixed MR AM in uber mode to write map intermediate outputs in the correct directory to work properly in secure mode. Contributed by Hitesh Shah.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213987 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 3:35 PM",
      "commitName": "b7ae5a6cb7b2d3e3112ac53007e984caeb07de58",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "24/10/11 10:09 AM",
      "commitNameOld": "7ce1c4ab352bca4b59ecbafdf237e5817cf833e5",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 50.27,
      "commitsBetweenForRepo": 346,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,95 +1,94 @@\n     public void run() {\n       ContainerLauncherEvent event \u003d null;\n \n       // _must_ either run subtasks sequentially or accept expense of new JVMs\n       // (i.e., fork()), else will get weird failures when maps try to create/\n       // write same dirname or filename:  no chdir() in Java\n       while (!Thread.currentThread().isInterrupted()) {\n         try {\n           event \u003d eventQueue.take();\n         } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n           LOG.error(\"Returning, interrupted : \" + e);\n           return;\n         }\n \n         LOG.info(\"Processing the event \" + event.toString());\n \n         if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n \n           ContainerRemoteLaunchEvent launchEv \u003d\n               (ContainerRemoteLaunchEvent)event;\n-          TaskAttemptId attemptID \u003d launchEv.getTaskAttemptID(); //FIXME:  can attemptID ever be null?  (only if retrieved over umbilical?)\n+          TaskAttemptId attemptID \u003d launchEv.getTaskAttemptID(); \n \n           Job job \u003d context.getAllJobs().get(attemptID.getTaskId().getJobId());\n           int numMapTasks \u003d job.getTotalMaps();\n           int numReduceTasks \u003d job.getTotalReduces();\n \n           // YARN (tracking) Task:\n           org.apache.hadoop.mapreduce.v2.app.job.Task ytask \u003d\n               job.getTask(attemptID.getTaskId());\n           // classic mapred Task:\n           org.apache.hadoop.mapred.Task remoteTask \u003d launchEv.getRemoteTask();\n \n           // after \"launching,\" send launched event to task attempt to move\n           // state from ASSIGNED to RUNNING (also nukes \"remoteTask\", so must\n           // do getRemoteTask() call first)\n           \n           //There is no port number because we are not really talking to a task\n           // tracker.  The shuffle is just done through local files.  So the\n           // port number is set to -1 in this case.\n           context.getEventHandler().handle(\n               new TaskAttemptContainerLaunchedEvent(attemptID, -1));\n-          //FIXME:  race condition here?  or do we have same kind of lock on TA handler \u003d\u003e MapTask can\u0027t send TA_UPDATE before TA_CONTAINER_LAUNCHED moves TA to RUNNING state?  (probably latter)\n \n           if (numMapTasks \u003d\u003d 0) {\n             doneWithMaps \u003d true;\n           }\n \n           try {\n             if (remoteTask.isMapOrReduce()) {\n               JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n               jce.addCounterUpdate(JobCounter.TOTAL_LAUNCHED_UBERTASKS, 1);\n               if (remoteTask.isMapTask()) {\n                 jce.addCounterUpdate(JobCounter.NUM_UBER_SUBMAPS, 1);\n               } else {\n                 jce.addCounterUpdate(JobCounter.NUM_UBER_SUBREDUCES, 1);\n               }\n               context.getEventHandler().handle(jce);\n             }\n             runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks,\n                        (numReduceTasks \u003e 0));\n             \n           } catch (RuntimeException re) {\n             JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n             jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);\n             context.getEventHandler().handle(jce);\n             // this is our signal that the subtask failed in some way, so\n             // simulate a failed JVM/container and send a container-completed\n             // event to task attempt (i.e., move state machine from RUNNING\n             // to FAIL_CONTAINER_CLEANUP [and ultimately to FAILED])\n             context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n                 TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n           } catch (IOException ioe) {\n             // if umbilical itself barfs (in error-handler of runSubMap()),\n             // we\u0027re pretty much hosed, so do what YarnChild main() does\n             // (i.e., exit clumsily--but can never happen, so no worries!)\n             LOG.fatal(\"oopsie...  this can never happen: \"\n                 + StringUtils.stringifyException(ioe));\n             System.exit(-1);\n           }\n \n         } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n \n           // no container to kill, so just send \"cleaned\" event to task attempt\n           // to move us from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state\n           // (or {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n           context.getEventHandler().handle(\n               new TaskAttemptEvent(event.getTaskAttemptID(),\n                   TaskAttemptEventType.TA_CONTAINER_CLEANED));\n \n         } else {\n           LOG.warn(\"Ignoring unexpected event \" + event.toString());\n         }\n \n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      ContainerLauncherEvent event \u003d null;\n\n      // _must_ either run subtasks sequentially or accept expense of new JVMs\n      // (i.e., fork()), else will get weird failures when maps try to create/\n      // write same dirname or filename:  no chdir() in Java\n      while (!Thread.currentThread().isInterrupted()) {\n        try {\n          event \u003d eventQueue.take();\n        } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n          LOG.error(\"Returning, interrupted : \" + e);\n          return;\n        }\n\n        LOG.info(\"Processing the event \" + event.toString());\n\n        if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n\n          ContainerRemoteLaunchEvent launchEv \u003d\n              (ContainerRemoteLaunchEvent)event;\n          TaskAttemptId attemptID \u003d launchEv.getTaskAttemptID(); \n\n          Job job \u003d context.getAllJobs().get(attemptID.getTaskId().getJobId());\n          int numMapTasks \u003d job.getTotalMaps();\n          int numReduceTasks \u003d job.getTotalReduces();\n\n          // YARN (tracking) Task:\n          org.apache.hadoop.mapreduce.v2.app.job.Task ytask \u003d\n              job.getTask(attemptID.getTaskId());\n          // classic mapred Task:\n          org.apache.hadoop.mapred.Task remoteTask \u003d launchEv.getRemoteTask();\n\n          // after \"launching,\" send launched event to task attempt to move\n          // state from ASSIGNED to RUNNING (also nukes \"remoteTask\", so must\n          // do getRemoteTask() call first)\n          \n          //There is no port number because we are not really talking to a task\n          // tracker.  The shuffle is just done through local files.  So the\n          // port number is set to -1 in this case.\n          context.getEventHandler().handle(\n              new TaskAttemptContainerLaunchedEvent(attemptID, -1));\n\n          if (numMapTasks \u003d\u003d 0) {\n            doneWithMaps \u003d true;\n          }\n\n          try {\n            if (remoteTask.isMapOrReduce()) {\n              JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n              jce.addCounterUpdate(JobCounter.TOTAL_LAUNCHED_UBERTASKS, 1);\n              if (remoteTask.isMapTask()) {\n                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBMAPS, 1);\n              } else {\n                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBREDUCES, 1);\n              }\n              context.getEventHandler().handle(jce);\n            }\n            runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks,\n                       (numReduceTasks \u003e 0));\n            \n          } catch (RuntimeException re) {\n            JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n            jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);\n            context.getEventHandler().handle(jce);\n            // this is our signal that the subtask failed in some way, so\n            // simulate a failed JVM/container and send a container-completed\n            // event to task attempt (i.e., move state machine from RUNNING\n            // to FAIL_CONTAINER_CLEANUP [and ultimately to FAILED])\n            context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n                TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n          } catch (IOException ioe) {\n            // if umbilical itself barfs (in error-handler of runSubMap()),\n            // we\u0027re pretty much hosed, so do what YarnChild main() does\n            // (i.e., exit clumsily--but can never happen, so no worries!)\n            LOG.fatal(\"oopsie...  this can never happen: \"\n                + StringUtils.stringifyException(ioe));\n            System.exit(-1);\n          }\n\n        } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n\n          // no container to kill, so just send \"cleaned\" event to task attempt\n          // to move us from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state\n          // (or {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n          context.getEventHandler().handle(\n              new TaskAttemptEvent(event.getTaskAttemptID(),\n                  TaskAttemptEventType.TA_CONTAINER_CLEANED));\n\n        } else {\n          LOG.warn(\"Ignoring unexpected event \" + event.toString());\n        }\n\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
      "extendedDetails": {}
    },
    "ade0f0560f729e50382c6992f713f29e2dd5b270": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2652. Enabled multiple NMs to be runnable on a single node by making shuffle service port to be truely configurable. Contributed by Robert Joseph Evans.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1163585 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "31/08/11 4:38 AM",
      "commitName": "ade0f0560f729e50382c6992f713f29e2dd5b270",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "24/08/11 5:14 PM",
      "commitNameOld": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 6.48,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,91 +1,95 @@\n     public void run() {\n       ContainerLauncherEvent event \u003d null;\n \n       // _must_ either run subtasks sequentially or accept expense of new JVMs\n       // (i.e., fork()), else will get weird failures when maps try to create/\n       // write same dirname or filename:  no chdir() in Java\n       while (!Thread.currentThread().isInterrupted()) {\n         try {\n           event \u003d eventQueue.take();\n         } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n           LOG.error(\"Returning, interrupted : \" + e);\n           return;\n         }\n \n         LOG.info(\"Processing the event \" + event.toString());\n \n         if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n \n           ContainerRemoteLaunchEvent launchEv \u003d\n               (ContainerRemoteLaunchEvent)event;\n           TaskAttemptId attemptID \u003d launchEv.getTaskAttemptID(); //FIXME:  can attemptID ever be null?  (only if retrieved over umbilical?)\n \n           Job job \u003d context.getAllJobs().get(attemptID.getTaskId().getJobId());\n           int numMapTasks \u003d job.getTotalMaps();\n           int numReduceTasks \u003d job.getTotalReduces();\n \n           // YARN (tracking) Task:\n           org.apache.hadoop.mapreduce.v2.app.job.Task ytask \u003d\n               job.getTask(attemptID.getTaskId());\n           // classic mapred Task:\n           org.apache.hadoop.mapred.Task remoteTask \u003d launchEv.getRemoteTask();\n \n           // after \"launching,\" send launched event to task attempt to move\n           // state from ASSIGNED to RUNNING (also nukes \"remoteTask\", so must\n           // do getRemoteTask() call first)\n+          \n+          //There is no port number because we are not really talking to a task\n+          // tracker.  The shuffle is just done through local files.  So the\n+          // port number is set to -1 in this case.\n           context.getEventHandler().handle(\n-              new TaskAttemptEvent(attemptID,\n-                  TaskAttemptEventType.TA_CONTAINER_LAUNCHED)); //FIXME:  race condition here?  or do we have same kind of lock on TA handler \u003d\u003e MapTask can\u0027t send TA_UPDATE before TA_CONTAINER_LAUNCHED moves TA to RUNNING state?  (probably latter)\n+              new TaskAttemptContainerLaunchedEvent(attemptID, -1));\n+          //FIXME:  race condition here?  or do we have same kind of lock on TA handler \u003d\u003e MapTask can\u0027t send TA_UPDATE before TA_CONTAINER_LAUNCHED moves TA to RUNNING state?  (probably latter)\n \n           if (numMapTasks \u003d\u003d 0) {\n             doneWithMaps \u003d true;\n           }\n \n           try {\n             if (remoteTask.isMapOrReduce()) {\n               JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n               jce.addCounterUpdate(JobCounter.TOTAL_LAUNCHED_UBERTASKS, 1);\n               if (remoteTask.isMapTask()) {\n                 jce.addCounterUpdate(JobCounter.NUM_UBER_SUBMAPS, 1);\n               } else {\n                 jce.addCounterUpdate(JobCounter.NUM_UBER_SUBREDUCES, 1);\n               }\n               context.getEventHandler().handle(jce);\n             }\n             runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks,\n                        (numReduceTasks \u003e 0));\n             \n           } catch (RuntimeException re) {\n             JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n             jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);\n             context.getEventHandler().handle(jce);\n             // this is our signal that the subtask failed in some way, so\n             // simulate a failed JVM/container and send a container-completed\n             // event to task attempt (i.e., move state machine from RUNNING\n             // to FAIL_CONTAINER_CLEANUP [and ultimately to FAILED])\n             context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n                 TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n           } catch (IOException ioe) {\n             // if umbilical itself barfs (in error-handler of runSubMap()),\n             // we\u0027re pretty much hosed, so do what YarnChild main() does\n             // (i.e., exit clumsily--but can never happen, so no worries!)\n             LOG.fatal(\"oopsie...  this can never happen: \"\n                 + StringUtils.stringifyException(ioe));\n             System.exit(-1);\n           }\n \n         } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n \n           // no container to kill, so just send \"cleaned\" event to task attempt\n           // to move us from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state\n           // (or {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n           context.getEventHandler().handle(\n               new TaskAttemptEvent(event.getTaskAttemptID(),\n                   TaskAttemptEventType.TA_CONTAINER_CLEANED));\n \n         } else {\n           LOG.warn(\"Ignoring unexpected event \" + event.toString());\n         }\n \n       }\n     }\n\\ No newline at end of file\n",
      "actualSource": "    public void run() {\n      ContainerLauncherEvent event \u003d null;\n\n      // _must_ either run subtasks sequentially or accept expense of new JVMs\n      // (i.e., fork()), else will get weird failures when maps try to create/\n      // write same dirname or filename:  no chdir() in Java\n      while (!Thread.currentThread().isInterrupted()) {\n        try {\n          event \u003d eventQueue.take();\n        } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n          LOG.error(\"Returning, interrupted : \" + e);\n          return;\n        }\n\n        LOG.info(\"Processing the event \" + event.toString());\n\n        if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n\n          ContainerRemoteLaunchEvent launchEv \u003d\n              (ContainerRemoteLaunchEvent)event;\n          TaskAttemptId attemptID \u003d launchEv.getTaskAttemptID(); //FIXME:  can attemptID ever be null?  (only if retrieved over umbilical?)\n\n          Job job \u003d context.getAllJobs().get(attemptID.getTaskId().getJobId());\n          int numMapTasks \u003d job.getTotalMaps();\n          int numReduceTasks \u003d job.getTotalReduces();\n\n          // YARN (tracking) Task:\n          org.apache.hadoop.mapreduce.v2.app.job.Task ytask \u003d\n              job.getTask(attemptID.getTaskId());\n          // classic mapred Task:\n          org.apache.hadoop.mapred.Task remoteTask \u003d launchEv.getRemoteTask();\n\n          // after \"launching,\" send launched event to task attempt to move\n          // state from ASSIGNED to RUNNING (also nukes \"remoteTask\", so must\n          // do getRemoteTask() call first)\n          \n          //There is no port number because we are not really talking to a task\n          // tracker.  The shuffle is just done through local files.  So the\n          // port number is set to -1 in this case.\n          context.getEventHandler().handle(\n              new TaskAttemptContainerLaunchedEvent(attemptID, -1));\n          //FIXME:  race condition here?  or do we have same kind of lock on TA handler \u003d\u003e MapTask can\u0027t send TA_UPDATE before TA_CONTAINER_LAUNCHED moves TA to RUNNING state?  (probably latter)\n\n          if (numMapTasks \u003d\u003d 0) {\n            doneWithMaps \u003d true;\n          }\n\n          try {\n            if (remoteTask.isMapOrReduce()) {\n              JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n              jce.addCounterUpdate(JobCounter.TOTAL_LAUNCHED_UBERTASKS, 1);\n              if (remoteTask.isMapTask()) {\n                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBMAPS, 1);\n              } else {\n                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBREDUCES, 1);\n              }\n              context.getEventHandler().handle(jce);\n            }\n            runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks,\n                       (numReduceTasks \u003e 0));\n            \n          } catch (RuntimeException re) {\n            JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n            jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);\n            context.getEventHandler().handle(jce);\n            // this is our signal that the subtask failed in some way, so\n            // simulate a failed JVM/container and send a container-completed\n            // event to task attempt (i.e., move state machine from RUNNING\n            // to FAIL_CONTAINER_CLEANUP [and ultimately to FAILED])\n            context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n                TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n          } catch (IOException ioe) {\n            // if umbilical itself barfs (in error-handler of runSubMap()),\n            // we\u0027re pretty much hosed, so do what YarnChild main() does\n            // (i.e., exit clumsily--but can never happen, so no worries!)\n            LOG.fatal(\"oopsie...  this can never happen: \"\n                + StringUtils.stringifyException(ioe));\n            System.exit(-1);\n          }\n\n        } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n\n          // no container to kill, so just send \"cleaned\" event to task attempt\n          // to move us from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state\n          // (or {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n          context.getEventHandler().handle(\n              new TaskAttemptEvent(event.getTaskAttemptID(),\n                  TaskAttemptEventType.TA_CONTAINER_CLEANED));\n\n        } else {\n          LOG.warn(\"Ignoring unexpected event \" + event.toString());\n        }\n\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    public void run() {\n      ContainerLauncherEvent event \u003d null;\n\n      // _must_ either run subtasks sequentially or accept expense of new JVMs\n      // (i.e., fork()), else will get weird failures when maps try to create/\n      // write same dirname or filename:  no chdir() in Java\n      while (!Thread.currentThread().isInterrupted()) {\n        try {\n          event \u003d eventQueue.take();\n        } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n          LOG.error(\"Returning, interrupted : \" + e);\n          return;\n        }\n\n        LOG.info(\"Processing the event \" + event.toString());\n\n        if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n\n          ContainerRemoteLaunchEvent launchEv \u003d\n              (ContainerRemoteLaunchEvent)event;\n          TaskAttemptId attemptID \u003d launchEv.getTaskAttemptID(); //FIXME:  can attemptID ever be null?  (only if retrieved over umbilical?)\n\n          Job job \u003d context.getAllJobs().get(attemptID.getTaskId().getJobId());\n          int numMapTasks \u003d job.getTotalMaps();\n          int numReduceTasks \u003d job.getTotalReduces();\n\n          // YARN (tracking) Task:\n          org.apache.hadoop.mapreduce.v2.app.job.Task ytask \u003d\n              job.getTask(attemptID.getTaskId());\n          // classic mapred Task:\n          org.apache.hadoop.mapred.Task remoteTask \u003d launchEv.getRemoteTask();\n\n          // after \"launching,\" send launched event to task attempt to move\n          // state from ASSIGNED to RUNNING (also nukes \"remoteTask\", so must\n          // do getRemoteTask() call first)\n          context.getEventHandler().handle(\n              new TaskAttemptEvent(attemptID,\n                  TaskAttemptEventType.TA_CONTAINER_LAUNCHED)); //FIXME:  race condition here?  or do we have same kind of lock on TA handler \u003d\u003e MapTask can\u0027t send TA_UPDATE before TA_CONTAINER_LAUNCHED moves TA to RUNNING state?  (probably latter)\n\n          if (numMapTasks \u003d\u003d 0) {\n            doneWithMaps \u003d true;\n          }\n\n          try {\n            if (remoteTask.isMapOrReduce()) {\n              JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n              jce.addCounterUpdate(JobCounter.TOTAL_LAUNCHED_UBERTASKS, 1);\n              if (remoteTask.isMapTask()) {\n                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBMAPS, 1);\n              } else {\n                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBREDUCES, 1);\n              }\n              context.getEventHandler().handle(jce);\n            }\n            runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks,\n                       (numReduceTasks \u003e 0));\n            \n          } catch (RuntimeException re) {\n            JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n            jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);\n            context.getEventHandler().handle(jce);\n            // this is our signal that the subtask failed in some way, so\n            // simulate a failed JVM/container and send a container-completed\n            // event to task attempt (i.e., move state machine from RUNNING\n            // to FAIL_CONTAINER_CLEANUP [and ultimately to FAILED])\n            context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n                TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n          } catch (IOException ioe) {\n            // if umbilical itself barfs (in error-handler of runSubMap()),\n            // we\u0027re pretty much hosed, so do what YarnChild main() does\n            // (i.e., exit clumsily--but can never happen, so no worries!)\n            LOG.fatal(\"oopsie...  this can never happen: \"\n                + StringUtils.stringifyException(ioe));\n            System.exit(-1);\n          }\n\n        } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n\n          // no container to kill, so just send \"cleaned\" event to task attempt\n          // to move us from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state\n          // (or {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n          context.getEventHandler().handle(\n              new TaskAttemptEvent(event.getTaskAttemptID(),\n                  TaskAttemptEventType.TA_CONTAINER_CLEANED));\n\n        } else {\n          LOG.warn(\"Ignoring unexpected event \" + event.toString());\n        }\n\n      }\n    }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Ymultichange(Ymovefromfile,Ybodychange)",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,71 +1,91 @@\n     public void run() {\n-      while (!Thread.interrupted()) {\n+      ContainerLauncherEvent event \u003d null;\n+\n+      // _must_ either run subtasks sequentially or accept expense of new JVMs\n+      // (i.e., fork()), else will get weird failures when maps try to create/\n+      // write same dirname or filename:  no chdir() in Java\n+      while (!Thread.currentThread().isInterrupted()) {\n         try {\n-          TaskInProgress tip;\n-          Task task;\n-          synchronized (tasksToLaunch) {\n-            while (tasksToLaunch.isEmpty()) {\n-              tasksToLaunch.wait();\n-            }\n-            //get the TIP\n-            tip \u003d tasksToLaunch.remove(0);\n-            task \u003d tip.getTask();\n-            LOG.info(\"Trying to launch : \" + tip.getTask().getTaskID() + \n-                     \" which needs \" + task.getNumSlotsRequired() + \" slots\");\n-          }\n-          //wait for free slots to run\n-          synchronized (numFreeSlots) {\n-            boolean canLaunch \u003d true;\n-            while (numFreeSlots.get() \u003c task.getNumSlotsRequired()) {\n-              //Make sure that there is no kill task action for this task!\n-              //We are not locking tip here, because it would reverse the\n-              //locking order!\n-              //Also, Lock for the tip is not required here! because :\n-              // 1. runState of TaskStatus is volatile\n-              // 2. Any notification is not missed because notification is\n-              // synchronized on numFreeSlots. So, while we are doing the check,\n-              // if the tip is half way through the kill(), we don\u0027t miss\n-              // notification for the following wait(). \n-              if (!tip.canBeLaunched()) {\n-                //got killed externally while still in the launcher queue\n-                LOG.info(\"Not blocking slots for \" + task.getTaskID()\n-                    + \" as it got killed externally. Task\u0027s state is \"\n-                    + tip.getRunState());\n-                canLaunch \u003d false;\n-                break;\n-              }              \n-              LOG.info(\"TaskLauncher : Waiting for \" + task.getNumSlotsRequired() + \n-                       \" to launch \" + task.getTaskID() + \", currently we have \" + \n-                       numFreeSlots.get() + \" free slots\");\n-              numFreeSlots.wait();\n-            }\n-            if (!canLaunch) {\n-              continue;\n-            }\n-            LOG.info(\"In TaskLauncher, current free slots : \" + numFreeSlots.get()+\n-                     \" and trying to launch \"+tip.getTask().getTaskID() + \n-                     \" which needs \" + task.getNumSlotsRequired() + \" slots\");\n-            numFreeSlots.set(numFreeSlots.get() - task.getNumSlotsRequired());\n-            assert (numFreeSlots.get() \u003e\u003d 0);\n-          }\n-          synchronized (tip) {\n-            //to make sure that there is no kill task action for this\n-            if (!tip.canBeLaunched()) {\n-              //got killed externally while still in the launcher queue\n-              LOG.info(\"Not launching task \" + task.getTaskID() + \" as it got\"\n-                + \" killed externally. Task\u0027s state is \" + tip.getRunState());\n-              addFreeSlots(task.getNumSlotsRequired());\n-              continue;\n-            }\n-            tip.slotTaken \u003d true;\n-          }\n-          //got a free slot. launch the task\n-          startNewTask(tip);\n-        } catch (InterruptedException e) { \n-          return; // ALL DONE\n-        } catch (Throwable th) {\n-          LOG.error(\"TaskLauncher error \" + \n-              StringUtils.stringifyException(th));\n+          event \u003d eventQueue.take();\n+        } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n+          LOG.error(\"Returning, interrupted : \" + e);\n+          return;\n         }\n+\n+        LOG.info(\"Processing the event \" + event.toString());\n+\n+        if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n+\n+          ContainerRemoteLaunchEvent launchEv \u003d\n+              (ContainerRemoteLaunchEvent)event;\n+          TaskAttemptId attemptID \u003d launchEv.getTaskAttemptID(); //FIXME:  can attemptID ever be null?  (only if retrieved over umbilical?)\n+\n+          Job job \u003d context.getAllJobs().get(attemptID.getTaskId().getJobId());\n+          int numMapTasks \u003d job.getTotalMaps();\n+          int numReduceTasks \u003d job.getTotalReduces();\n+\n+          // YARN (tracking) Task:\n+          org.apache.hadoop.mapreduce.v2.app.job.Task ytask \u003d\n+              job.getTask(attemptID.getTaskId());\n+          // classic mapred Task:\n+          org.apache.hadoop.mapred.Task remoteTask \u003d launchEv.getRemoteTask();\n+\n+          // after \"launching,\" send launched event to task attempt to move\n+          // state from ASSIGNED to RUNNING (also nukes \"remoteTask\", so must\n+          // do getRemoteTask() call first)\n+          context.getEventHandler().handle(\n+              new TaskAttemptEvent(attemptID,\n+                  TaskAttemptEventType.TA_CONTAINER_LAUNCHED)); //FIXME:  race condition here?  or do we have same kind of lock on TA handler \u003d\u003e MapTask can\u0027t send TA_UPDATE before TA_CONTAINER_LAUNCHED moves TA to RUNNING state?  (probably latter)\n+\n+          if (numMapTasks \u003d\u003d 0) {\n+            doneWithMaps \u003d true;\n+          }\n+\n+          try {\n+            if (remoteTask.isMapOrReduce()) {\n+              JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n+              jce.addCounterUpdate(JobCounter.TOTAL_LAUNCHED_UBERTASKS, 1);\n+              if (remoteTask.isMapTask()) {\n+                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBMAPS, 1);\n+              } else {\n+                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBREDUCES, 1);\n+              }\n+              context.getEventHandler().handle(jce);\n+            }\n+            runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks,\n+                       (numReduceTasks \u003e 0));\n+            \n+          } catch (RuntimeException re) {\n+            JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n+            jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);\n+            context.getEventHandler().handle(jce);\n+            // this is our signal that the subtask failed in some way, so\n+            // simulate a failed JVM/container and send a container-completed\n+            // event to task attempt (i.e., move state machine from RUNNING\n+            // to FAIL_CONTAINER_CLEANUP [and ultimately to FAILED])\n+            context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n+                TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n+          } catch (IOException ioe) {\n+            // if umbilical itself barfs (in error-handler of runSubMap()),\n+            // we\u0027re pretty much hosed, so do what YarnChild main() does\n+            // (i.e., exit clumsily--but can never happen, so no worries!)\n+            LOG.fatal(\"oopsie...  this can never happen: \"\n+                + StringUtils.stringifyException(ioe));\n+            System.exit(-1);\n+          }\n+\n+        } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n+\n+          // no container to kill, so just send \"cleaned\" event to task attempt\n+          // to move us from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state\n+          // (or {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n+          context.getEventHandler().handle(\n+              new TaskAttemptEvent(event.getTaskAttemptID(),\n+                  TaskAttemptEventType.TA_CONTAINER_CLEANED));\n+\n+        } else {\n+          LOG.warn(\"Ignoring unexpected event \" + event.toString());\n+        }\n+\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public void run() {\n      ContainerLauncherEvent event \u003d null;\n\n      // _must_ either run subtasks sequentially or accept expense of new JVMs\n      // (i.e., fork()), else will get weird failures when maps try to create/\n      // write same dirname or filename:  no chdir() in Java\n      while (!Thread.currentThread().isInterrupted()) {\n        try {\n          event \u003d eventQueue.take();\n        } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n          LOG.error(\"Returning, interrupted : \" + e);\n          return;\n        }\n\n        LOG.info(\"Processing the event \" + event.toString());\n\n        if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n\n          ContainerRemoteLaunchEvent launchEv \u003d\n              (ContainerRemoteLaunchEvent)event;\n          TaskAttemptId attemptID \u003d launchEv.getTaskAttemptID(); //FIXME:  can attemptID ever be null?  (only if retrieved over umbilical?)\n\n          Job job \u003d context.getAllJobs().get(attemptID.getTaskId().getJobId());\n          int numMapTasks \u003d job.getTotalMaps();\n          int numReduceTasks \u003d job.getTotalReduces();\n\n          // YARN (tracking) Task:\n          org.apache.hadoop.mapreduce.v2.app.job.Task ytask \u003d\n              job.getTask(attemptID.getTaskId());\n          // classic mapred Task:\n          org.apache.hadoop.mapred.Task remoteTask \u003d launchEv.getRemoteTask();\n\n          // after \"launching,\" send launched event to task attempt to move\n          // state from ASSIGNED to RUNNING (also nukes \"remoteTask\", so must\n          // do getRemoteTask() call first)\n          context.getEventHandler().handle(\n              new TaskAttemptEvent(attemptID,\n                  TaskAttemptEventType.TA_CONTAINER_LAUNCHED)); //FIXME:  race condition here?  or do we have same kind of lock on TA handler \u003d\u003e MapTask can\u0027t send TA_UPDATE before TA_CONTAINER_LAUNCHED moves TA to RUNNING state?  (probably latter)\n\n          if (numMapTasks \u003d\u003d 0) {\n            doneWithMaps \u003d true;\n          }\n\n          try {\n            if (remoteTask.isMapOrReduce()) {\n              JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n              jce.addCounterUpdate(JobCounter.TOTAL_LAUNCHED_UBERTASKS, 1);\n              if (remoteTask.isMapTask()) {\n                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBMAPS, 1);\n              } else {\n                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBREDUCES, 1);\n              }\n              context.getEventHandler().handle(jce);\n            }\n            runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks,\n                       (numReduceTasks \u003e 0));\n            \n          } catch (RuntimeException re) {\n            JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n            jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);\n            context.getEventHandler().handle(jce);\n            // this is our signal that the subtask failed in some way, so\n            // simulate a failed JVM/container and send a container-completed\n            // event to task attempt (i.e., move state machine from RUNNING\n            // to FAIL_CONTAINER_CLEANUP [and ultimately to FAILED])\n            context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n                TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n          } catch (IOException ioe) {\n            // if umbilical itself barfs (in error-handler of runSubMap()),\n            // we\u0027re pretty much hosed, so do what YarnChild main() does\n            // (i.e., exit clumsily--but can never happen, so no worries!)\n            LOG.fatal(\"oopsie...  this can never happen: \"\n                + StringUtils.stringifyException(ioe));\n            System.exit(-1);\n          }\n\n        } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n\n          // no container to kill, so just send \"cleaned\" event to task attempt\n          // to move us from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state\n          // (or {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n          context.getEventHandler().handle(\n              new TaskAttemptEvent(event.getTaskAttemptID(),\n                  TaskAttemptEventType.TA_CONTAINER_CLEANED));\n\n        } else {\n          LOG.warn(\"Ignoring unexpected event \" + event.toString());\n        }\n\n      }\n    }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
          "extendedDetails": {
            "oldPath": "mapreduce/src/java/org/apache/hadoop/mapred/TaskTracker.java",
            "newPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
            "oldMethodName": "run",
            "newMethodName": "run"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "18/08/11 4:07 AM",
          "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
          "commitAuthor": "Vinod Kumar Vavilapalli",
          "commitDateOld": "17/08/11 8:02 PM",
          "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 0.34,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,71 +1,91 @@\n     public void run() {\n-      while (!Thread.interrupted()) {\n+      ContainerLauncherEvent event \u003d null;\n+\n+      // _must_ either run subtasks sequentially or accept expense of new JVMs\n+      // (i.e., fork()), else will get weird failures when maps try to create/\n+      // write same dirname or filename:  no chdir() in Java\n+      while (!Thread.currentThread().isInterrupted()) {\n         try {\n-          TaskInProgress tip;\n-          Task task;\n-          synchronized (tasksToLaunch) {\n-            while (tasksToLaunch.isEmpty()) {\n-              tasksToLaunch.wait();\n-            }\n-            //get the TIP\n-            tip \u003d tasksToLaunch.remove(0);\n-            task \u003d tip.getTask();\n-            LOG.info(\"Trying to launch : \" + tip.getTask().getTaskID() + \n-                     \" which needs \" + task.getNumSlotsRequired() + \" slots\");\n-          }\n-          //wait for free slots to run\n-          synchronized (numFreeSlots) {\n-            boolean canLaunch \u003d true;\n-            while (numFreeSlots.get() \u003c task.getNumSlotsRequired()) {\n-              //Make sure that there is no kill task action for this task!\n-              //We are not locking tip here, because it would reverse the\n-              //locking order!\n-              //Also, Lock for the tip is not required here! because :\n-              // 1. runState of TaskStatus is volatile\n-              // 2. Any notification is not missed because notification is\n-              // synchronized on numFreeSlots. So, while we are doing the check,\n-              // if the tip is half way through the kill(), we don\u0027t miss\n-              // notification for the following wait(). \n-              if (!tip.canBeLaunched()) {\n-                //got killed externally while still in the launcher queue\n-                LOG.info(\"Not blocking slots for \" + task.getTaskID()\n-                    + \" as it got killed externally. Task\u0027s state is \"\n-                    + tip.getRunState());\n-                canLaunch \u003d false;\n-                break;\n-              }              \n-              LOG.info(\"TaskLauncher : Waiting for \" + task.getNumSlotsRequired() + \n-                       \" to launch \" + task.getTaskID() + \", currently we have \" + \n-                       numFreeSlots.get() + \" free slots\");\n-              numFreeSlots.wait();\n-            }\n-            if (!canLaunch) {\n-              continue;\n-            }\n-            LOG.info(\"In TaskLauncher, current free slots : \" + numFreeSlots.get()+\n-                     \" and trying to launch \"+tip.getTask().getTaskID() + \n-                     \" which needs \" + task.getNumSlotsRequired() + \" slots\");\n-            numFreeSlots.set(numFreeSlots.get() - task.getNumSlotsRequired());\n-            assert (numFreeSlots.get() \u003e\u003d 0);\n-          }\n-          synchronized (tip) {\n-            //to make sure that there is no kill task action for this\n-            if (!tip.canBeLaunched()) {\n-              //got killed externally while still in the launcher queue\n-              LOG.info(\"Not launching task \" + task.getTaskID() + \" as it got\"\n-                + \" killed externally. Task\u0027s state is \" + tip.getRunState());\n-              addFreeSlots(task.getNumSlotsRequired());\n-              continue;\n-            }\n-            tip.slotTaken \u003d true;\n-          }\n-          //got a free slot. launch the task\n-          startNewTask(tip);\n-        } catch (InterruptedException e) { \n-          return; // ALL DONE\n-        } catch (Throwable th) {\n-          LOG.error(\"TaskLauncher error \" + \n-              StringUtils.stringifyException(th));\n+          event \u003d eventQueue.take();\n+        } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n+          LOG.error(\"Returning, interrupted : \" + e);\n+          return;\n         }\n+\n+        LOG.info(\"Processing the event \" + event.toString());\n+\n+        if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n+\n+          ContainerRemoteLaunchEvent launchEv \u003d\n+              (ContainerRemoteLaunchEvent)event;\n+          TaskAttemptId attemptID \u003d launchEv.getTaskAttemptID(); //FIXME:  can attemptID ever be null?  (only if retrieved over umbilical?)\n+\n+          Job job \u003d context.getAllJobs().get(attemptID.getTaskId().getJobId());\n+          int numMapTasks \u003d job.getTotalMaps();\n+          int numReduceTasks \u003d job.getTotalReduces();\n+\n+          // YARN (tracking) Task:\n+          org.apache.hadoop.mapreduce.v2.app.job.Task ytask \u003d\n+              job.getTask(attemptID.getTaskId());\n+          // classic mapred Task:\n+          org.apache.hadoop.mapred.Task remoteTask \u003d launchEv.getRemoteTask();\n+\n+          // after \"launching,\" send launched event to task attempt to move\n+          // state from ASSIGNED to RUNNING (also nukes \"remoteTask\", so must\n+          // do getRemoteTask() call first)\n+          context.getEventHandler().handle(\n+              new TaskAttemptEvent(attemptID,\n+                  TaskAttemptEventType.TA_CONTAINER_LAUNCHED)); //FIXME:  race condition here?  or do we have same kind of lock on TA handler \u003d\u003e MapTask can\u0027t send TA_UPDATE before TA_CONTAINER_LAUNCHED moves TA to RUNNING state?  (probably latter)\n+\n+          if (numMapTasks \u003d\u003d 0) {\n+            doneWithMaps \u003d true;\n+          }\n+\n+          try {\n+            if (remoteTask.isMapOrReduce()) {\n+              JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n+              jce.addCounterUpdate(JobCounter.TOTAL_LAUNCHED_UBERTASKS, 1);\n+              if (remoteTask.isMapTask()) {\n+                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBMAPS, 1);\n+              } else {\n+                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBREDUCES, 1);\n+              }\n+              context.getEventHandler().handle(jce);\n+            }\n+            runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks,\n+                       (numReduceTasks \u003e 0));\n+            \n+          } catch (RuntimeException re) {\n+            JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n+            jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);\n+            context.getEventHandler().handle(jce);\n+            // this is our signal that the subtask failed in some way, so\n+            // simulate a failed JVM/container and send a container-completed\n+            // event to task attempt (i.e., move state machine from RUNNING\n+            // to FAIL_CONTAINER_CLEANUP [and ultimately to FAILED])\n+            context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n+                TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n+          } catch (IOException ioe) {\n+            // if umbilical itself barfs (in error-handler of runSubMap()),\n+            // we\u0027re pretty much hosed, so do what YarnChild main() does\n+            // (i.e., exit clumsily--but can never happen, so no worries!)\n+            LOG.fatal(\"oopsie...  this can never happen: \"\n+                + StringUtils.stringifyException(ioe));\n+            System.exit(-1);\n+          }\n+\n+        } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n+\n+          // no container to kill, so just send \"cleaned\" event to task attempt\n+          // to move us from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state\n+          // (or {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n+          context.getEventHandler().handle(\n+              new TaskAttemptEvent(event.getTaskAttemptID(),\n+                  TaskAttemptEventType.TA_CONTAINER_CLEANED));\n+\n+        } else {\n+          LOG.warn(\"Ignoring unexpected event \" + event.toString());\n+        }\n+\n       }\n     }\n\\ No newline at end of file\n",
          "actualSource": "    public void run() {\n      ContainerLauncherEvent event \u003d null;\n\n      // _must_ either run subtasks sequentially or accept expense of new JVMs\n      // (i.e., fork()), else will get weird failures when maps try to create/\n      // write same dirname or filename:  no chdir() in Java\n      while (!Thread.currentThread().isInterrupted()) {\n        try {\n          event \u003d eventQueue.take();\n        } catch (InterruptedException e) {  // mostly via T_KILL? JOB_KILL?\n          LOG.error(\"Returning, interrupted : \" + e);\n          return;\n        }\n\n        LOG.info(\"Processing the event \" + event.toString());\n\n        if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_LAUNCH) {\n\n          ContainerRemoteLaunchEvent launchEv \u003d\n              (ContainerRemoteLaunchEvent)event;\n          TaskAttemptId attemptID \u003d launchEv.getTaskAttemptID(); //FIXME:  can attemptID ever be null?  (only if retrieved over umbilical?)\n\n          Job job \u003d context.getAllJobs().get(attemptID.getTaskId().getJobId());\n          int numMapTasks \u003d job.getTotalMaps();\n          int numReduceTasks \u003d job.getTotalReduces();\n\n          // YARN (tracking) Task:\n          org.apache.hadoop.mapreduce.v2.app.job.Task ytask \u003d\n              job.getTask(attemptID.getTaskId());\n          // classic mapred Task:\n          org.apache.hadoop.mapred.Task remoteTask \u003d launchEv.getRemoteTask();\n\n          // after \"launching,\" send launched event to task attempt to move\n          // state from ASSIGNED to RUNNING (also nukes \"remoteTask\", so must\n          // do getRemoteTask() call first)\n          context.getEventHandler().handle(\n              new TaskAttemptEvent(attemptID,\n                  TaskAttemptEventType.TA_CONTAINER_LAUNCHED)); //FIXME:  race condition here?  or do we have same kind of lock on TA handler \u003d\u003e MapTask can\u0027t send TA_UPDATE before TA_CONTAINER_LAUNCHED moves TA to RUNNING state?  (probably latter)\n\n          if (numMapTasks \u003d\u003d 0) {\n            doneWithMaps \u003d true;\n          }\n\n          try {\n            if (remoteTask.isMapOrReduce()) {\n              JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n              jce.addCounterUpdate(JobCounter.TOTAL_LAUNCHED_UBERTASKS, 1);\n              if (remoteTask.isMapTask()) {\n                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBMAPS, 1);\n              } else {\n                jce.addCounterUpdate(JobCounter.NUM_UBER_SUBREDUCES, 1);\n              }\n              context.getEventHandler().handle(jce);\n            }\n            runSubtask(remoteTask, ytask.getType(), attemptID, numMapTasks,\n                       (numReduceTasks \u003e 0));\n            \n          } catch (RuntimeException re) {\n            JobCounterUpdateEvent jce \u003d new JobCounterUpdateEvent(attemptID.getTaskId().getJobId());\n            jce.addCounterUpdate(JobCounter.NUM_FAILED_UBERTASKS, 1);\n            context.getEventHandler().handle(jce);\n            // this is our signal that the subtask failed in some way, so\n            // simulate a failed JVM/container and send a container-completed\n            // event to task attempt (i.e., move state machine from RUNNING\n            // to FAIL_CONTAINER_CLEANUP [and ultimately to FAILED])\n            context.getEventHandler().handle(new TaskAttemptEvent(attemptID,\n                TaskAttemptEventType.TA_CONTAINER_COMPLETED));\n          } catch (IOException ioe) {\n            // if umbilical itself barfs (in error-handler of runSubMap()),\n            // we\u0027re pretty much hosed, so do what YarnChild main() does\n            // (i.e., exit clumsily--but can never happen, so no worries!)\n            LOG.fatal(\"oopsie...  this can never happen: \"\n                + StringUtils.stringifyException(ioe));\n            System.exit(-1);\n          }\n\n        } else if (event.getType() \u003d\u003d EventType.CONTAINER_REMOTE_CLEANUP) {\n\n          // no container to kill, so just send \"cleaned\" event to task attempt\n          // to move us from SUCCESS_CONTAINER_CLEANUP to SUCCEEDED state\n          // (or {FAIL|KILL}_CONTAINER_CLEANUP to {FAIL|KILL}_TASK_CLEANUP)\n          context.getEventHandler().handle(\n              new TaskAttemptEvent(event.getTaskAttemptID(),\n                  TaskAttemptEventType.TA_CONTAINER_CLEANED));\n\n        } else {\n          LOG.warn(\"Ignoring unexpected event \" + event.toString());\n        }\n\n      }\n    }",
          "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
          "extendedDetails": {}
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,48 @@\n+  public void run() {\n+    try {\n+      startCleanupThreads();\n+      boolean denied \u003d false;\n+      while (running \u0026\u0026 !shuttingDown \u0026\u0026 !denied) {\n+        boolean staleState \u003d false;\n+        try {\n+          // This while-loop attempts reconnects if we get network errors\n+          while (running \u0026\u0026 !staleState \u0026\u0026 !shuttingDown \u0026\u0026 !denied) {\n+            try {\n+              State osState \u003d offerService();\n+              if (osState \u003d\u003d State.STALE) {\n+                staleState \u003d true;\n+              } else if (osState \u003d\u003d State.DENIED) {\n+                denied \u003d true;\n+              }\n+            } catch (Exception ex) {\n+              if (!shuttingDown) {\n+                LOG.info(\"Lost connection to JobTracker [\" +\n+                         jobTrackAddr + \"].  Retrying...\", ex);\n+                try {\n+                  Thread.sleep(5000);\n+                } catch (InterruptedException ie) {\n+                }\n+              }\n+            }\n+          }\n+        } finally {\n+          close();\n+        }\n+        if (shuttingDown) { return; }\n+        LOG.warn(\"Reinitializing local state\");\n+        initialize();\n+      }\n+      if (denied) {\n+        shutdown();\n+      }\n+    } catch (IOException iex) {\n+      LOG.error(\"Got fatal exception while reinitializing TaskTracker: \" +\n+                StringUtils.stringifyException(iex));\n+      return;\n+    }\n+    catch (InterruptedException i) {\n+      LOG.error(\"Got interrupted while reinitializing TaskTracker: \" + \n+          i.getMessage());\n+      return;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void run() {\n    try {\n      startCleanupThreads();\n      boolean denied \u003d false;\n      while (running \u0026\u0026 !shuttingDown \u0026\u0026 !denied) {\n        boolean staleState \u003d false;\n        try {\n          // This while-loop attempts reconnects if we get network errors\n          while (running \u0026\u0026 !staleState \u0026\u0026 !shuttingDown \u0026\u0026 !denied) {\n            try {\n              State osState \u003d offerService();\n              if (osState \u003d\u003d State.STALE) {\n                staleState \u003d true;\n              } else if (osState \u003d\u003d State.DENIED) {\n                denied \u003d true;\n              }\n            } catch (Exception ex) {\n              if (!shuttingDown) {\n                LOG.info(\"Lost connection to JobTracker [\" +\n                         jobTrackAddr + \"].  Retrying...\", ex);\n                try {\n                  Thread.sleep(5000);\n                } catch (InterruptedException ie) {\n                }\n              }\n            }\n          }\n        } finally {\n          close();\n        }\n        if (shuttingDown) { return; }\n        LOG.warn(\"Reinitializing local state\");\n        initialize();\n      }\n      if (denied) {\n        shutdown();\n      }\n    } catch (IOException iex) {\n      LOG.error(\"Got fatal exception while reinitializing TaskTracker: \" +\n                StringUtils.stringifyException(iex));\n      return;\n    }\n    catch (InterruptedException i) {\n      LOG.error(\"Got interrupted while reinitializing TaskTracker: \" + \n          i.getMessage());\n      return;\n    }\n  }",
      "path": "mapreduce/src/java/org/apache/hadoop/mapred/TaskTracker.java"
    }
  }
}