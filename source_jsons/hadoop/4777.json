{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FileDiffList.java",
  "functionName": "combineAndCollectSnapshotBlocks",
  "functionId": "combineAndCollectSnapshotBlocks___reclaimContext-INode.ReclaimContext__file-INodeFile__removed-FileDiff",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
  "functionStartLine": 99,
  "functionEndLine": 143,
  "numCommitsSeen": 42,
  "timeTaken": 7575,
  "changeHistory": [
    "8d898ab25f1c2032a07c9bbd96ba3d0c4eb5be87",
    "164cbe643988f878f0f4100a4de51783e5b6738e",
    "f4c523b69ba55b1fd35e8995c3011a9f546ac835",
    "4fdd9abd7e43a0fb7b569982954a8f9660b9268b",
    "7e091de1366f4b57b5433bc19d738199dc05313d",
    "c17439c2ddd921b63b1635e6f1cba634b8da8557",
    "4928f5473394981829e5ffd4b16ea0801baf5c45",
    "11585883a9eb30ba080b9aa49dba42cb0a797d75",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b",
    "b2c85db86c9a62b0a03ee87547265077f664970a",
    "4536399d47f6c061e149e2504600804a0f1e093d",
    "6acb7f2110897264241df44d564db2f85260348f",
    "5dae97a584d30cef3e34141edfaca49c4ec57913",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
    "08ac06283a3e9bf0d49d873823aabd419b08e41f"
  ],
  "changeHistoryShort": {
    "8d898ab25f1c2032a07c9bbd96ba3d0c4eb5be87": "Ybodychange",
    "164cbe643988f878f0f4100a4de51783e5b6738e": "Ybodychange",
    "f4c523b69ba55b1fd35e8995c3011a9f546ac835": "Ybodychange",
    "4fdd9abd7e43a0fb7b569982954a8f9660b9268b": "Ybodychange",
    "7e091de1366f4b57b5433bc19d738199dc05313d": "Ybodychange",
    "c17439c2ddd921b63b1635e6f1cba634b8da8557": "Ybodychange",
    "4928f5473394981829e5ffd4b16ea0801baf5c45": "Ybodychange",
    "11585883a9eb30ba080b9aa49dba42cb0a797d75": "Ybodychange",
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": "Ybodychange",
    "b2c85db86c9a62b0a03ee87547265077f664970a": "Ybodychange",
    "4536399d47f6c061e149e2504600804a0f1e093d": "Ymultichange(Yparameterchange,Ybodychange)",
    "6acb7f2110897264241df44d564db2f85260348f": "Ybodychange",
    "5dae97a584d30cef3e34141edfaca49c4ec57913": "Ymultichange(Yparameterchange,Ybodychange)",
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": "Ybodychange",
    "08ac06283a3e9bf0d49d873823aabd419b08e41f": "Yintroduced"
  },
  "changeHistoryDetails": {
    "8d898ab25f1c2032a07c9bbd96ba3d0c4eb5be87": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12884. BlockUnderConstructionFeature.truncateBlock should be of type BlockInfo. Contributed by chencan.",
      "commitDate": "21/03/18 4:46 PM",
      "commitName": "8d898ab25f1c2032a07c9bbd96ba3d0c4eb5be87",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "15/02/18 3:33 AM",
      "commitNameOld": "6ea7d78ccb0d1c4af9bcac02a4cff89bdffff252",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 34.51,
      "commitsBetweenForRepo": 238,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   void combineAndCollectSnapshotBlocks(\n       INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n     BlockInfo[] removedBlocks \u003d removed.getBlocks();\n     if (removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n         sf.collectBlocksAndClear(reclaimContext, file);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if (earlierDiff !\u003d null) {\n       earlierDiff.setBlocks(removedBlocks);\n     }\n     BlockInfo[] earlierBlocks \u003d\n         (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n     BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n     laterBlocks \u003d (laterBlocks \u003d\u003d null) ? file.getBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Check if last block is part of truncate recovery\n     BlockInfo lastBlock \u003d file.getLastBlock();\n-    Block dontRemoveBlock \u003d null;\n+    BlockInfo dontRemoveBlock \u003d null;\n     if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n         HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n       dontRemoveBlock \u003d lastBlock.getUnderConstructionFeature()\n           .getTruncateBlock();\n     }\n     // Collect the remaining blocks of the file, ignoring truncate block\n     for (;i \u003c removedBlocks.length; i++) {\n       if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n         reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void combineAndCollectSnapshotBlocks(\n      INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n    BlockInfo[] removedBlocks \u003d removed.getBlocks();\n    if (removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(reclaimContext, file);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if (earlierDiff !\u003d null) {\n      earlierDiff.setBlocks(removedBlocks);\n    }\n    BlockInfo[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks \u003d\u003d null) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Check if last block is part of truncate recovery\n    BlockInfo lastBlock \u003d file.getLastBlock();\n    BlockInfo dontRemoveBlock \u003d null;\n    if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n        HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n      dontRemoveBlock \u003d lastBlock.getUnderConstructionFeature()\n          .getTruncateBlock();\n    }\n    // Collect the remaining blocks of the file, ignoring truncate block\n    for (;i \u003c removedBlocks.length; i++) {\n      if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n        reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
      "extendedDetails": {}
    },
    "164cbe643988f878f0f4100a4de51783e5b6738e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8909. Erasure coding: update BlockInfoContiguousUC and BlockInfoStripedUC to use BlockUnderConstructionFeature. Contributed by Jing Zhao.\n",
      "commitDate": "27/08/15 1:02 AM",
      "commitName": "164cbe643988f878f0f4100a4de51783e5b6738e",
      "commitAuthor": "Walter Su",
      "commitDateOld": "24/08/15 12:59 PM",
      "commitNameOld": "6b6a63bbbda920315d3d24b61ed3344a78a981b6",
      "commitAuthorOld": "",
      "daysBetweenCommits": 2.5,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   void combineAndCollectSnapshotBlocks(\n       INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n     BlockInfo[] removedBlocks \u003d removed.getBlocks();\n     if (removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n         sf.collectBlocksAndClear(reclaimContext, file);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if (earlierDiff !\u003d null) {\n       earlierDiff.setBlocks(removedBlocks);\n     }\n     BlockInfo[] earlierBlocks \u003d\n         (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n     BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n     laterBlocks \u003d (laterBlocks \u003d\u003d null) ? file.getBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Check if last block is part of truncate recovery\n     BlockInfo lastBlock \u003d file.getLastBlock();\n     Block dontRemoveBlock \u003d null;\n     if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n         HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n-      dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n+      dontRemoveBlock \u003d lastBlock.getUnderConstructionFeature()\n           .getTruncateBlock();\n     }\n     // Collect the remaining blocks of the file, ignoring truncate block\n     for (;i \u003c removedBlocks.length; i++) {\n       if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n         reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void combineAndCollectSnapshotBlocks(\n      INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n    BlockInfo[] removedBlocks \u003d removed.getBlocks();\n    if (removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(reclaimContext, file);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if (earlierDiff !\u003d null) {\n      earlierDiff.setBlocks(removedBlocks);\n    }\n    BlockInfo[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks \u003d\u003d null) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Check if last block is part of truncate recovery\n    BlockInfo lastBlock \u003d file.getLastBlock();\n    Block dontRemoveBlock \u003d null;\n    if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n        HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n      dontRemoveBlock \u003d lastBlock.getUnderConstructionFeature()\n          .getTruncateBlock();\n    }\n    // Collect the remaining blocks of the file, ignoring truncate block\n    for (;i \u003c removedBlocks.length; i++) {\n      if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n        reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
      "extendedDetails": {}
    },
    "f4c523b69ba55b1fd35e8995c3011a9f546ac835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8499. Refactor BlockInfo class hierarchy with static helper class. Contributed by Zhe Zhang.\"\n\nThis reverts commit c17439c2ddd921b63b1635e6f1cba634b8da8557.\n",
      "commitDate": "06/08/15 10:21 AM",
      "commitName": "f4c523b69ba55b1fd35e8995c3011a9f546ac835",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "12/06/15 11:38 AM",
      "commitNameOld": "c17439c2ddd921b63b1635e6f1cba634b8da8557",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 54.95,
      "commitsBetweenForRepo": 341,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   void combineAndCollectSnapshotBlocks(\n       INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n     BlockInfo[] removedBlocks \u003d removed.getBlocks();\n     if (removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n         sf.collectBlocksAndClear(reclaimContext, file);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if (earlierDiff !\u003d null) {\n       earlierDiff.setBlocks(removedBlocks);\n     }\n     BlockInfo[] earlierBlocks \u003d\n         (earlierDiff \u003d\u003d null ? new BlockInfo[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n     BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n     laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Check if last block is part of truncate recovery\n     BlockInfo lastBlock \u003d file.getLastBlock();\n     Block dontRemoveBlock \u003d null;\n     if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n         HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n-      dontRemoveBlock \u003d ((BlockInfoUnderConstruction) lastBlock)\n+      dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n           .getTruncateBlock();\n     }\n     // Collect the remaining blocks of the file, ignoring truncate block\n     for (;i \u003c removedBlocks.length; i++) {\n       if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n         reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void combineAndCollectSnapshotBlocks(\n      INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n    BlockInfo[] removedBlocks \u003d removed.getBlocks();\n    if (removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(reclaimContext, file);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if (earlierDiff !\u003d null) {\n      earlierDiff.setBlocks(removedBlocks);\n    }\n    BlockInfo[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfo[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Check if last block is part of truncate recovery\n    BlockInfo lastBlock \u003d file.getLastBlock();\n    Block dontRemoveBlock \u003d null;\n    if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n        HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n      dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n          .getTruncateBlock();\n    }\n    // Collect the remaining blocks of the file, ignoring truncate block\n    for (;i \u003c removedBlocks.length; i++) {\n      if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n        reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
      "extendedDetails": {}
    },
    "4fdd9abd7e43a0fb7b569982954a8f9660b9268b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8787. Erasure coding: rename BlockInfoContiguousUC and BlockInfoStripedUC to be consistent with trunk.\n",
      "commitDate": "15/07/15 8:13 PM",
      "commitName": "4fdd9abd7e43a0fb7b569982954a8f9660b9268b",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "15/07/15 9:49 AM",
      "commitNameOld": "7e091de1366f4b57b5433bc19d738199dc05313d",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 0.43,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   void combineAndCollectSnapshotBlocks(\n       INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n     BlockInfo[] removedBlocks \u003d removed.getBlocks();\n     if (removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n         sf.collectBlocksAndClear(reclaimContext, file);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if (earlierDiff !\u003d null) {\n       earlierDiff.setBlocks(removedBlocks);\n     }\n     BlockInfo[] earlierBlocks \u003d\n         (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n     BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n     laterBlocks \u003d (laterBlocks \u003d\u003d null) ? file.getBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Check if last block is part of truncate recovery\n     BlockInfo lastBlock \u003d file.getLastBlock();\n     Block dontRemoveBlock \u003d null;\n     if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n         HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n-      dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n+      dontRemoveBlock \u003d ((BlockInfoUnderConstructionContiguous) lastBlock)\n           .getTruncateBlock();\n     }\n     // Collect the remaining blocks of the file, ignoring truncate block\n     for (;i \u003c removedBlocks.length; i++) {\n       if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n         reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void combineAndCollectSnapshotBlocks(\n      INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n    BlockInfo[] removedBlocks \u003d removed.getBlocks();\n    if (removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(reclaimContext, file);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if (earlierDiff !\u003d null) {\n      earlierDiff.setBlocks(removedBlocks);\n    }\n    BlockInfo[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks \u003d\u003d null) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Check if last block is part of truncate recovery\n    BlockInfo lastBlock \u003d file.getLastBlock();\n    Block dontRemoveBlock \u003d null;\n    if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n        HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n      dontRemoveBlock \u003d ((BlockInfoUnderConstructionContiguous) lastBlock)\n          .getTruncateBlock();\n    }\n    // Collect the remaining blocks of the file, ignoring truncate block\n    for (;i \u003c removedBlocks.length; i++) {\n      if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n        reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
      "extendedDetails": {}
    },
    "7e091de1366f4b57b5433bc19d738199dc05313d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8058. Erasure coding: use BlockInfo[] for both striped and contiguous blocks in INodeFile. Contributed by Zhe Zhang and Yi Liu.\n",
      "commitDate": "15/07/15 9:49 AM",
      "commitName": "7e091de1366f4b57b5433bc19d738199dc05313d",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "26/05/15 11:32 AM",
      "commitNameOld": "11585883a9eb30ba080b9aa49dba42cb0a797d75",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 49.93,
      "commitsBetweenForRepo": 160,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   void combineAndCollectSnapshotBlocks(\n       INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n-    BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n+    BlockInfo[] removedBlocks \u003d removed.getBlocks();\n     if (removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n         sf.collectBlocksAndClear(reclaimContext, file);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if (earlierDiff !\u003d null) {\n       earlierDiff.setBlocks(removedBlocks);\n     }\n-    BlockInfoContiguous[] earlierBlocks \u003d\n+    BlockInfo[] earlierBlocks \u003d\n         (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n-    BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n-    laterBlocks \u003d (laterBlocks \u003d\u003d null) ? file.getContiguousBlocks() : laterBlocks;\n+    BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n+    laterBlocks \u003d (laterBlocks \u003d\u003d null) ? file.getBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Check if last block is part of truncate recovery\n     BlockInfo lastBlock \u003d file.getLastBlock();\n     Block dontRemoveBlock \u003d null;\n     if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n         HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n       dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n           .getTruncateBlock();\n     }\n     // Collect the remaining blocks of the file, ignoring truncate block\n     for (;i \u003c removedBlocks.length; i++) {\n       if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n         reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void combineAndCollectSnapshotBlocks(\n      INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n    BlockInfo[] removedBlocks \u003d removed.getBlocks();\n    if (removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(reclaimContext, file);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if (earlierDiff !\u003d null) {\n      earlierDiff.setBlocks(removedBlocks);\n    }\n    BlockInfo[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks \u003d\u003d null) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Check if last block is part of truncate recovery\n    BlockInfo lastBlock \u003d file.getLastBlock();\n    Block dontRemoveBlock \u003d null;\n    if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n        HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n      dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n          .getTruncateBlock();\n    }\n    // Collect the remaining blocks of the file, ignoring truncate block\n    for (;i \u003c removedBlocks.length; i++) {\n      if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n        reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
      "extendedDetails": {}
    },
    "c17439c2ddd921b63b1635e6f1cba634b8da8557": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8499. Refactor BlockInfo class hierarchy with static helper class. Contributed by Zhe Zhang.\n",
      "commitDate": "12/06/15 11:38 AM",
      "commitName": "c17439c2ddd921b63b1635e6f1cba634b8da8557",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "27/05/15 3:42 PM",
      "commitNameOld": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 15.83,
      "commitsBetweenForRepo": 122,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   void combineAndCollectSnapshotBlocks(\n       INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n     BlockInfo[] removedBlocks \u003d removed.getBlocks();\n     if (removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n         sf.collectBlocksAndClear(reclaimContext, file);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if (earlierDiff !\u003d null) {\n       earlierDiff.setBlocks(removedBlocks);\n     }\n     BlockInfo[] earlierBlocks \u003d\n         (earlierDiff \u003d\u003d null ? new BlockInfo[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n     BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n     laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Check if last block is part of truncate recovery\n     BlockInfo lastBlock \u003d file.getLastBlock();\n     Block dontRemoveBlock \u003d null;\n     if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n         HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n-      dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n+      dontRemoveBlock \u003d ((BlockInfoUnderConstruction) lastBlock)\n           .getTruncateBlock();\n     }\n     // Collect the remaining blocks of the file, ignoring truncate block\n     for (;i \u003c removedBlocks.length; i++) {\n       if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n         reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void combineAndCollectSnapshotBlocks(\n      INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n    BlockInfo[] removedBlocks \u003d removed.getBlocks();\n    if (removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(reclaimContext, file);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if (earlierDiff !\u003d null) {\n      earlierDiff.setBlocks(removedBlocks);\n    }\n    BlockInfo[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfo[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Check if last block is part of truncate recovery\n    BlockInfo lastBlock \u003d file.getLastBlock();\n    Block dontRemoveBlock \u003d null;\n    if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n        HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n      dontRemoveBlock \u003d ((BlockInfoUnderConstruction) lastBlock)\n          .getTruncateBlock();\n    }\n    // Collect the remaining blocks of the file, ignoring truncate block\n    for (;i \u003c removedBlocks.length; i++) {\n      if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n        reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
      "extendedDetails": {}
    },
    "4928f5473394981829e5ffd4b16ea0801baf5c45": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8482. Rename BlockInfoContiguous to BlockInfo. Contributed by Zhe Zhang.\n",
      "commitDate": "27/05/15 3:42 PM",
      "commitName": "4928f5473394981829e5ffd4b16ea0801baf5c45",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "13/05/15 9:50 PM",
      "commitNameOld": "b2c85db86c9a62b0a03ee87547265077f664970a",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 13.74,
      "commitsBetweenForRepo": 96,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   void combineAndCollectSnapshotBlocks(\n       INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n-    BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n+    BlockInfo[] removedBlocks \u003d removed.getBlocks();\n     if (removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n         sf.collectBlocksAndClear(reclaimContext, file);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if (earlierDiff !\u003d null) {\n       earlierDiff.setBlocks(removedBlocks);\n     }\n-    BlockInfoContiguous[] earlierBlocks \u003d\n-        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n+    BlockInfo[] earlierBlocks \u003d\n+        (earlierDiff \u003d\u003d null ? new BlockInfo[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n-    BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n+    BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n     laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Check if last block is part of truncate recovery\n-    BlockInfoContiguous lastBlock \u003d file.getLastBlock();\n+    BlockInfo lastBlock \u003d file.getLastBlock();\n     Block dontRemoveBlock \u003d null;\n     if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n         HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n       dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n           .getTruncateBlock();\n     }\n     // Collect the remaining blocks of the file, ignoring truncate block\n     for (;i \u003c removedBlocks.length; i++) {\n       if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n         reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void combineAndCollectSnapshotBlocks(\n      INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n    BlockInfo[] removedBlocks \u003d removed.getBlocks();\n    if (removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(reclaimContext, file);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if (earlierDiff !\u003d null) {\n      earlierDiff.setBlocks(removedBlocks);\n    }\n    BlockInfo[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfo[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Check if last block is part of truncate recovery\n    BlockInfo lastBlock \u003d file.getLastBlock();\n    Block dontRemoveBlock \u003d null;\n    if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n        HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n      dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n          .getTruncateBlock();\n    }\n    // Collect the remaining blocks of the file, ignoring truncate block\n    for (;i \u003c removedBlocks.length; i++) {\n      if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n        reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
      "extendedDetails": {}
    },
    "11585883a9eb30ba080b9aa49dba42cb0a797d75": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7936. Erasure coding: resolving conflicts when merging with HDFS-7903 and HDFS-7435. Contributed by Zhe Zhang.\n",
      "commitDate": "26/05/15 11:32 AM",
      "commitName": "11585883a9eb30ba080b9aa49dba42cb0a797d75",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "26/05/15 11:07 AM",
      "commitNameOld": "9f2f583f401189c3f4a2687795a9e3e0b288322b",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.02,
      "commitsBetweenForRepo": 9,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   void combineAndCollectSnapshotBlocks(\n       INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n     BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n     if (removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n         sf.collectBlocksAndClear(reclaimContext, file);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if (earlierDiff !\u003d null) {\n       earlierDiff.setBlocks(removedBlocks);\n     }\n     BlockInfoContiguous[] earlierBlocks \u003d\n         (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n     BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n     laterBlocks \u003d (laterBlocks \u003d\u003d null) ? file.getContiguousBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Check if last block is part of truncate recovery\n-    BlockInfoContiguous lastBlock \u003d file.getLastBlock();\n+    BlockInfo lastBlock \u003d file.getLastBlock();\n     Block dontRemoveBlock \u003d null;\n     if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n         HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n       dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n           .getTruncateBlock();\n     }\n     // Collect the remaining blocks of the file, ignoring truncate block\n     for (;i \u003c removedBlocks.length; i++) {\n       if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n         reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void combineAndCollectSnapshotBlocks(\n      INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n    BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n    if (removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(reclaimContext, file);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if (earlierDiff !\u003d null) {\n      earlierDiff.setBlocks(removedBlocks);\n    }\n    BlockInfoContiguous[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks \u003d\u003d null) ? file.getContiguousBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Check if last block is part of truncate recovery\n    BlockInfo lastBlock \u003d file.getLastBlock();\n    Block dontRemoveBlock \u003d null;\n    if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n        HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n      dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n          .getTruncateBlock();\n    }\n    // Collect the remaining blocks of the file, ignoring truncate block\n    for (;i \u003c removedBlocks.length; i++) {\n      if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n        reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
      "extendedDetails": {}
    },
    "9f2f583f401189c3f4a2687795a9e3e0b288322b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7749. Erasure Coding: Add striped block support in INodeFile. Contributed by Jing Zhao.\n",
      "commitDate": "26/05/15 11:07 AM",
      "commitName": "9f2f583f401189c3f4a2687795a9e3e0b288322b",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "13/05/15 9:50 PM",
      "commitNameOld": "b2c85db86c9a62b0a03ee87547265077f664970a",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 12.55,
      "commitsBetweenForRepo": 94,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,45 +1,45 @@\n   void combineAndCollectSnapshotBlocks(\n       INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n     BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n     if (removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n         sf.collectBlocksAndClear(reclaimContext, file);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if (earlierDiff !\u003d null) {\n       earlierDiff.setBlocks(removedBlocks);\n     }\n     BlockInfoContiguous[] earlierBlocks \u003d\n         (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n     BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n-    laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n+    laterBlocks \u003d (laterBlocks \u003d\u003d null) ? file.getContiguousBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Check if last block is part of truncate recovery\n     BlockInfoContiguous lastBlock \u003d file.getLastBlock();\n     Block dontRemoveBlock \u003d null;\n     if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n         HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n       dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n           .getTruncateBlock();\n     }\n     // Collect the remaining blocks of the file, ignoring truncate block\n     for (;i \u003c removedBlocks.length; i++) {\n       if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n         reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void combineAndCollectSnapshotBlocks(\n      INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n    BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n    if (removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(reclaimContext, file);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if (earlierDiff !\u003d null) {\n      earlierDiff.setBlocks(removedBlocks);\n    }\n    BlockInfoContiguous[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks \u003d\u003d null) ? file.getContiguousBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Check if last block is part of truncate recovery\n    BlockInfoContiguous lastBlock \u003d file.getLastBlock();\n    Block dontRemoveBlock \u003d null;\n    if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n        HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n      dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n          .getTruncateBlock();\n    }\n    // Collect the remaining blocks of the file, ignoring truncate block\n    for (;i \u003c removedBlocks.length; i++) {\n      if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n        reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
      "extendedDetails": {}
    },
    "b2c85db86c9a62b0a03ee87547265077f664970a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7728. Avoid updating quota usage while loading edits. Contributed by Jing Zhao.\n",
      "commitDate": "13/05/15 9:50 PM",
      "commitName": "b2c85db86c9a62b0a03ee87547265077f664970a",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "09/05/15 10:51 PM",
      "commitNameOld": "4536399d47f6c061e149e2504600804a0f1e093d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 3.96,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,44 +1,45 @@\n   void combineAndCollectSnapshotBlocks(\n       INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n     BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n-    if(removedBlocks \u003d\u003d null) {\n+    if (removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n         sf.collectBlocksAndClear(reclaimContext, file);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n-    if(earlierDiff !\u003d null)\n+    if (earlierDiff !\u003d null) {\n       earlierDiff.setBlocks(removedBlocks);\n+    }\n     BlockInfoContiguous[] earlierBlocks \u003d\n         (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n     BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n     laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Check if last block is part of truncate recovery\n     BlockInfoContiguous lastBlock \u003d file.getLastBlock();\n     Block dontRemoveBlock \u003d null;\n-    if(lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n+    if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n         HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n       dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n           .getTruncateBlock();\n     }\n     // Collect the remaining blocks of the file, ignoring truncate block\n-    for(;i \u003c removedBlocks.length; i++) {\n+    for (;i \u003c removedBlocks.length; i++) {\n       if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n         reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void combineAndCollectSnapshotBlocks(\n      INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n    BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n    if (removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(reclaimContext, file);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if (earlierDiff !\u003d null) {\n      earlierDiff.setBlocks(removedBlocks);\n    }\n    BlockInfoContiguous[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Check if last block is part of truncate recovery\n    BlockInfoContiguous lastBlock \u003d file.getLastBlock();\n    Block dontRemoveBlock \u003d null;\n    if (lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n        HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n      dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n          .getTruncateBlock();\n    }\n    // Collect the remaining blocks of the file, ignoring truncate block\n    for (;i \u003c removedBlocks.length; i++) {\n      if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n        reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
      "extendedDetails": {}
    },
    "4536399d47f6c061e149e2504600804a0f1e093d": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8357. Consolidate parameters of INode.CleanSubtree() into a parameter objects. Contributed by Li Lu.\n",
      "commitDate": "09/05/15 10:51 PM",
      "commitName": "4536399d47f6c061e149e2504600804a0f1e093d",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8357. Consolidate parameters of INode.CleanSubtree() into a parameter objects. Contributed by Li Lu.\n",
          "commitDate": "09/05/15 10:51 PM",
          "commitName": "4536399d47f6c061e149e2504600804a0f1e093d",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "13/03/15 1:12 PM",
          "commitNameOld": "6acb7f2110897264241df44d564db2f85260348f",
          "commitAuthorOld": "Konstantin V Shvachko",
          "daysBetweenCommits": 57.4,
          "commitsBetweenForRepo": 587,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,46 +1,44 @@\n-  void combineAndCollectSnapshotBlocks(BlockStoragePolicySuite bsps, INodeFile file,\n-                                       FileDiff removed,\n-                                       BlocksMapUpdateInfo collectedBlocks,\n-                                       List\u003cINode\u003e removedINodes) {\n+  void combineAndCollectSnapshotBlocks(\n+      INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n     BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n     if(removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n-        sf.collectBlocksAndClear(bsps, file, collectedBlocks, removedINodes);\n+        sf.collectBlocksAndClear(reclaimContext, file);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if(earlierDiff !\u003d null)\n       earlierDiff.setBlocks(removedBlocks);\n     BlockInfoContiguous[] earlierBlocks \u003d\n         (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n     BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n     laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Check if last block is part of truncate recovery\n     BlockInfoContiguous lastBlock \u003d file.getLastBlock();\n     Block dontRemoveBlock \u003d null;\n     if(lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n         HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n       dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n           .getTruncateBlock();\n     }\n     // Collect the remaining blocks of the file, ignoring truncate block\n     for(;i \u003c removedBlocks.length; i++) {\n       if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n-        collectedBlocks.addDeleteBlock(removedBlocks[i]);\n+        reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void combineAndCollectSnapshotBlocks(\n      INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n    BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n    if(removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(reclaimContext, file);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if(earlierDiff !\u003d null)\n      earlierDiff.setBlocks(removedBlocks);\n    BlockInfoContiguous[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Check if last block is part of truncate recovery\n    BlockInfoContiguous lastBlock \u003d file.getLastBlock();\n    Block dontRemoveBlock \u003d null;\n    if(lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n        HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n      dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n          .getTruncateBlock();\n    }\n    // Collect the remaining blocks of the file, ignoring truncate block\n    for(;i \u003c removedBlocks.length; i++) {\n      if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n        reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
          "extendedDetails": {
            "oldValue": "[bsps-BlockStoragePolicySuite, file-INodeFile, removed-FileDiff, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e]",
            "newValue": "[reclaimContext-INode.ReclaimContext, file-INodeFile, removed-FileDiff]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8357. Consolidate parameters of INode.CleanSubtree() into a parameter objects. Contributed by Li Lu.\n",
          "commitDate": "09/05/15 10:51 PM",
          "commitName": "4536399d47f6c061e149e2504600804a0f1e093d",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "13/03/15 1:12 PM",
          "commitNameOld": "6acb7f2110897264241df44d564db2f85260348f",
          "commitAuthorOld": "Konstantin V Shvachko",
          "daysBetweenCommits": 57.4,
          "commitsBetweenForRepo": 587,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,46 +1,44 @@\n-  void combineAndCollectSnapshotBlocks(BlockStoragePolicySuite bsps, INodeFile file,\n-                                       FileDiff removed,\n-                                       BlocksMapUpdateInfo collectedBlocks,\n-                                       List\u003cINode\u003e removedINodes) {\n+  void combineAndCollectSnapshotBlocks(\n+      INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n     BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n     if(removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n-        sf.collectBlocksAndClear(bsps, file, collectedBlocks, removedINodes);\n+        sf.collectBlocksAndClear(reclaimContext, file);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if(earlierDiff !\u003d null)\n       earlierDiff.setBlocks(removedBlocks);\n     BlockInfoContiguous[] earlierBlocks \u003d\n         (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n     BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n     laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Check if last block is part of truncate recovery\n     BlockInfoContiguous lastBlock \u003d file.getLastBlock();\n     Block dontRemoveBlock \u003d null;\n     if(lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n         HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n       dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n           .getTruncateBlock();\n     }\n     // Collect the remaining blocks of the file, ignoring truncate block\n     for(;i \u003c removedBlocks.length; i++) {\n       if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n-        collectedBlocks.addDeleteBlock(removedBlocks[i]);\n+        reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void combineAndCollectSnapshotBlocks(\n      INode.ReclaimContext reclaimContext, INodeFile file, FileDiff removed) {\n    BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n    if(removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(reclaimContext, file);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if(earlierDiff !\u003d null)\n      earlierDiff.setBlocks(removedBlocks);\n    BlockInfoContiguous[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Check if last block is part of truncate recovery\n    BlockInfoContiguous lastBlock \u003d file.getLastBlock();\n    Block dontRemoveBlock \u003d null;\n    if(lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n        HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n      dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n          .getTruncateBlock();\n    }\n    // Collect the remaining blocks of the file, ignoring truncate block\n    for(;i \u003c removedBlocks.length; i++) {\n      if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n        reclaimContext.collectedBlocks().addDeleteBlock(removedBlocks[i]);\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
          "extendedDetails": {}
        }
      ]
    },
    "6acb7f2110897264241df44d564db2f85260348f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7903. Cannot recover block after truncate and delete snapshot. Contributed by Plamen Jeliazkov.",
      "commitDate": "13/03/15 1:12 PM",
      "commitName": "6acb7f2110897264241df44d564db2f85260348f",
      "commitAuthor": "Konstantin V Shvachko",
      "commitDateOld": "24/02/15 10:31 AM",
      "commitNameOld": "73bcfa99af61e5202f030510db8954c17cba43cc",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 17.07,
      "commitsBetweenForRepo": 136,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,46 @@\n   void combineAndCollectSnapshotBlocks(BlockStoragePolicySuite bsps, INodeFile file,\n                                        FileDiff removed,\n                                        BlocksMapUpdateInfo collectedBlocks,\n                                        List\u003cINode\u003e removedINodes) {\n     BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n     if(removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n         sf.collectBlocksAndClear(bsps, file, collectedBlocks, removedINodes);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if(earlierDiff !\u003d null)\n       earlierDiff.setBlocks(removedBlocks);\n     BlockInfoContiguous[] earlierBlocks \u003d\n         (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n     BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n     laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n-    // Collect the remaining blocks of the file\n-    while(i \u003c removedBlocks.length) {\n-      collectedBlocks.addDeleteBlock(removedBlocks[i++]);\n+    // Check if last block is part of truncate recovery\n+    BlockInfoContiguous lastBlock \u003d file.getLastBlock();\n+    Block dontRemoveBlock \u003d null;\n+    if(lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n+        HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n+      dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n+          .getTruncateBlock();\n+    }\n+    // Collect the remaining blocks of the file, ignoring truncate block\n+    for(;i \u003c removedBlocks.length; i++) {\n+      if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n+        collectedBlocks.addDeleteBlock(removedBlocks[i]);\n+      }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void combineAndCollectSnapshotBlocks(BlockStoragePolicySuite bsps, INodeFile file,\n                                       FileDiff removed,\n                                       BlocksMapUpdateInfo collectedBlocks,\n                                       List\u003cINode\u003e removedINodes) {\n    BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n    if(removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(bsps, file, collectedBlocks, removedINodes);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if(earlierDiff !\u003d null)\n      earlierDiff.setBlocks(removedBlocks);\n    BlockInfoContiguous[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Check if last block is part of truncate recovery\n    BlockInfoContiguous lastBlock \u003d file.getLastBlock();\n    Block dontRemoveBlock \u003d null;\n    if(lastBlock !\u003d null \u0026\u0026 lastBlock.getBlockUCState().equals(\n        HdfsServerConstants.BlockUCState.UNDER_RECOVERY)) {\n      dontRemoveBlock \u003d ((BlockInfoContiguousUnderConstruction) lastBlock)\n          .getTruncateBlock();\n    }\n    // Collect the remaining blocks of the file, ignoring truncate block\n    for(;i \u003c removedBlocks.length; i++) {\n      if(dontRemoveBlock \u003d\u003d null || !removedBlocks[i].equals(dontRemoveBlock)) {\n        collectedBlocks.addDeleteBlock(removedBlocks[i]);\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
      "extendedDetails": {}
    },
    "5dae97a584d30cef3e34141edfaca49c4ec57913": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7723. Quota By Storage Type namenode implemenation. (Contributed by Xiaoyu Yao)\n",
      "commitDate": "11/02/15 10:41 AM",
      "commitName": "5dae97a584d30cef3e34141edfaca49c4ec57913",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7723. Quota By Storage Type namenode implemenation. (Contributed by Xiaoyu Yao)\n",
          "commitDate": "11/02/15 10:41 AM",
          "commitName": "5dae97a584d30cef3e34141edfaca49c4ec57913",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "08/02/15 11:51 AM",
          "commitNameOld": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 2.95,
          "commitsBetweenForRepo": 40,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,36 @@\n-  void combineAndCollectSnapshotBlocks(INodeFile file,\n+  void combineAndCollectSnapshotBlocks(BlockStoragePolicySuite bsps, INodeFile file,\n                                        FileDiff removed,\n                                        BlocksMapUpdateInfo collectedBlocks,\n                                        List\u003cINode\u003e removedINodes) {\n     BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n     if(removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n-        sf.collectBlocksAndClear(file, collectedBlocks, removedINodes);\n+        sf.collectBlocksAndClear(bsps, file, collectedBlocks, removedINodes);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if(earlierDiff !\u003d null)\n       earlierDiff.setBlocks(removedBlocks);\n     BlockInfoContiguous[] earlierBlocks \u003d\n         (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n     BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n     laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Collect the remaining blocks of the file\n     while(i \u003c removedBlocks.length) {\n       collectedBlocks.addDeleteBlock(removedBlocks[i++]);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void combineAndCollectSnapshotBlocks(BlockStoragePolicySuite bsps, INodeFile file,\n                                       FileDiff removed,\n                                       BlocksMapUpdateInfo collectedBlocks,\n                                       List\u003cINode\u003e removedINodes) {\n    BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n    if(removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(bsps, file, collectedBlocks, removedINodes);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if(earlierDiff !\u003d null)\n      earlierDiff.setBlocks(removedBlocks);\n    BlockInfoContiguous[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Collect the remaining blocks of the file\n    while(i \u003c removedBlocks.length) {\n      collectedBlocks.addDeleteBlock(removedBlocks[i++]);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
          "extendedDetails": {
            "oldValue": "[file-INodeFile, removed-FileDiff, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e]",
            "newValue": "[bsps-BlockStoragePolicySuite, file-INodeFile, removed-FileDiff, collectedBlocks-BlocksMapUpdateInfo, removedINodes-List\u003cINode\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7723. Quota By Storage Type namenode implemenation. (Contributed by Xiaoyu Yao)\n",
          "commitDate": "11/02/15 10:41 AM",
          "commitName": "5dae97a584d30cef3e34141edfaca49c4ec57913",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "08/02/15 11:51 AM",
          "commitNameOld": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 2.95,
          "commitsBetweenForRepo": 40,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,36 +1,36 @@\n-  void combineAndCollectSnapshotBlocks(INodeFile file,\n+  void combineAndCollectSnapshotBlocks(BlockStoragePolicySuite bsps, INodeFile file,\n                                        FileDiff removed,\n                                        BlocksMapUpdateInfo collectedBlocks,\n                                        List\u003cINode\u003e removedINodes) {\n     BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n     if(removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n-        sf.collectBlocksAndClear(file, collectedBlocks, removedINodes);\n+        sf.collectBlocksAndClear(bsps, file, collectedBlocks, removedINodes);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if(earlierDiff !\u003d null)\n       earlierDiff.setBlocks(removedBlocks);\n     BlockInfoContiguous[] earlierBlocks \u003d\n         (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n     BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n     laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Collect the remaining blocks of the file\n     while(i \u003c removedBlocks.length) {\n       collectedBlocks.addDeleteBlock(removedBlocks[i++]);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void combineAndCollectSnapshotBlocks(BlockStoragePolicySuite bsps, INodeFile file,\n                                       FileDiff removed,\n                                       BlocksMapUpdateInfo collectedBlocks,\n                                       List\u003cINode\u003e removedINodes) {\n    BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n    if(removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(bsps, file, collectedBlocks, removedINodes);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if(earlierDiff !\u003d null)\n      earlierDiff.setBlocks(removedBlocks);\n    BlockInfoContiguous[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Collect the remaining blocks of the file\n    while(i \u003c removedBlocks.length) {\n      collectedBlocks.addDeleteBlock(removedBlocks[i++]);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
          "extendedDetails": {}
        }
      ]
    },
    "1382ae525c67bf95d8f3a436b547dbc72cfbb177": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7743. Code cleanup of BlockInfo and rename BlockInfo to BlockInfoContiguous. Contributed by Jing Zhao.\n",
      "commitDate": "08/02/15 11:51 AM",
      "commitName": "1382ae525c67bf95d8f3a436b547dbc72cfbb177",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "02/02/15 4:32 PM",
      "commitNameOld": "8cb473124c1cf1c6f68ead7bde06558ebf7ce47e",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 5.8,
      "commitsBetweenForRepo": 69,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,36 +1,36 @@\n   void combineAndCollectSnapshotBlocks(INodeFile file,\n                                        FileDiff removed,\n                                        BlocksMapUpdateInfo collectedBlocks,\n                                        List\u003cINode\u003e removedINodes) {\n-    BlockInfo[] removedBlocks \u003d removed.getBlocks();\n+    BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n     if(removedBlocks \u003d\u003d null) {\n       FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n       assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n       if(sf.isCurrentFileDeleted())\n         sf.collectBlocksAndClear(file, collectedBlocks, removedINodes);\n       return;\n     }\n     int p \u003d getPrior(removed.getSnapshotId(), true);\n     FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n     // Copy blocks to the previous snapshot if not set already\n     if(earlierDiff !\u003d null)\n       earlierDiff.setBlocks(removedBlocks);\n-    BlockInfo[] earlierBlocks \u003d\n-        (earlierDiff \u003d\u003d null ? new BlockInfo[]{} : earlierDiff.getBlocks());\n+    BlockInfoContiguous[] earlierBlocks \u003d\n+        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n     // Find later snapshot (or file itself) with blocks\n-    BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n+    BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n     laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n     // Skip blocks, which belong to either the earlier or the later lists\n     int i \u003d 0;\n     for(; i \u003c removedBlocks.length; i++) {\n       if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n         continue;\n       if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n         continue;\n       break;\n     }\n     // Collect the remaining blocks of the file\n     while(i \u003c removedBlocks.length) {\n       collectedBlocks.addDeleteBlock(removedBlocks[i++]);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void combineAndCollectSnapshotBlocks(INodeFile file,\n                                       FileDiff removed,\n                                       BlocksMapUpdateInfo collectedBlocks,\n                                       List\u003cINode\u003e removedINodes) {\n    BlockInfoContiguous[] removedBlocks \u003d removed.getBlocks();\n    if(removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(file, collectedBlocks, removedINodes);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if(earlierDiff !\u003d null)\n      earlierDiff.setBlocks(removedBlocks);\n    BlockInfoContiguous[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfoContiguous[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfoContiguous[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Collect the remaining blocks of the file\n    while(i \u003c removedBlocks.length) {\n      collectedBlocks.addDeleteBlock(removedBlocks[i++]);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java",
      "extendedDetails": {}
    },
    "08ac06283a3e9bf0d49d873823aabd419b08e41f": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7056. Snapshot support for truncate. Contributed by Konstantin Shvachko and Plamen Jeliazkov.",
      "commitDate": "13/01/15 12:24 AM",
      "commitName": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
      "commitAuthor": "Konstantin V Shvachko",
      "diff": "@@ -0,0 +1,36 @@\n+  void combineAndCollectSnapshotBlocks(INodeFile file,\n+                                       FileDiff removed,\n+                                       BlocksMapUpdateInfo collectedBlocks,\n+                                       List\u003cINode\u003e removedINodes) {\n+    BlockInfo[] removedBlocks \u003d removed.getBlocks();\n+    if(removedBlocks \u003d\u003d null) {\n+      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n+      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n+      if(sf.isCurrentFileDeleted())\n+        sf.collectBlocksAndClear(file, collectedBlocks, removedINodes);\n+      return;\n+    }\n+    int p \u003d getPrior(removed.getSnapshotId(), true);\n+    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n+    // Copy blocks to the previous snapshot if not set already\n+    if(earlierDiff !\u003d null)\n+      earlierDiff.setBlocks(removedBlocks);\n+    BlockInfo[] earlierBlocks \u003d\n+        (earlierDiff \u003d\u003d null ? new BlockInfo[]{} : earlierDiff.getBlocks());\n+    // Find later snapshot (or file itself) with blocks\n+    BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n+    laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n+    // Skip blocks, which belong to either the earlier or the later lists\n+    int i \u003d 0;\n+    for(; i \u003c removedBlocks.length; i++) {\n+      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n+        continue;\n+      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n+        continue;\n+      break;\n+    }\n+    // Collect the remaining blocks of the file\n+    while(i \u003c removedBlocks.length) {\n+      collectedBlocks.addDeleteBlock(removedBlocks[i++]);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void combineAndCollectSnapshotBlocks(INodeFile file,\n                                       FileDiff removed,\n                                       BlocksMapUpdateInfo collectedBlocks,\n                                       List\u003cINode\u003e removedINodes) {\n    BlockInfo[] removedBlocks \u003d removed.getBlocks();\n    if(removedBlocks \u003d\u003d null) {\n      FileWithSnapshotFeature sf \u003d file.getFileWithSnapshotFeature();\n      assert sf !\u003d null : \"FileWithSnapshotFeature is null\";\n      if(sf.isCurrentFileDeleted())\n        sf.collectBlocksAndClear(file, collectedBlocks, removedINodes);\n      return;\n    }\n    int p \u003d getPrior(removed.getSnapshotId(), true);\n    FileDiff earlierDiff \u003d p \u003d\u003d Snapshot.NO_SNAPSHOT_ID ? null : getDiffById(p);\n    // Copy blocks to the previous snapshot if not set already\n    if(earlierDiff !\u003d null)\n      earlierDiff.setBlocks(removedBlocks);\n    BlockInfo[] earlierBlocks \u003d\n        (earlierDiff \u003d\u003d null ? new BlockInfo[]{} : earlierDiff.getBlocks());\n    // Find later snapshot (or file itself) with blocks\n    BlockInfo[] laterBlocks \u003d findLaterSnapshotBlocks(removed.getSnapshotId());\n    laterBlocks \u003d (laterBlocks\u003d\u003dnull) ? file.getBlocks() : laterBlocks;\n    // Skip blocks, which belong to either the earlier or the later lists\n    int i \u003d 0;\n    for(; i \u003c removedBlocks.length; i++) {\n      if(i \u003c earlierBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d earlierBlocks[i])\n        continue;\n      if(i \u003c laterBlocks.length \u0026\u0026 removedBlocks[i] \u003d\u003d laterBlocks[i])\n        continue;\n      break;\n    }\n    // Collect the remaining blocks of the file\n    while(i \u003c removedBlocks.length) {\n      collectedBlocks.addDeleteBlock(removedBlocks[i++]);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/FileDiffList.java"
    }
  }
}