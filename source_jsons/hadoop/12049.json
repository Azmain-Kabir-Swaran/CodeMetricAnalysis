{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetImpl.java",
  "functionName": "removeOldReplica",
  "functionId": "removeOldReplica___replicaInfo-ReplicaInfo__newReplicaInfo-ReplicaInfo__bpid-String(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
  "functionStartLine": 3172,
  "functionEndLine": 3194,
  "numCommitsSeen": 330,
  "timeTaken": 7377,
  "changeHistory": [
    "aa45faf0b20c922b0d147ece9fa01fb95a5b0dec",
    "86c9862bec0248d671e657aa56094a2919b8ac14",
    "d1d4e16690cc85f7f22fbead9cf596260819b561",
    "e453989a5722e653bd97e3e54f9bbdffc9454fba",
    "058af60c56207907f2bedf76df4284e86d923e0c"
  ],
  "changeHistoryShort": {
    "aa45faf0b20c922b0d147ece9fa01fb95a5b0dec": "Ybodychange",
    "86c9862bec0248d671e657aa56094a2919b8ac14": "Ymultichange(Yparameterchange,Ybodychange)",
    "d1d4e16690cc85f7f22fbead9cf596260819b561": "Ybodychange",
    "e453989a5722e653bd97e3e54f9bbdffc9454fba": "Ybodychange",
    "058af60c56207907f2bedf76df4284e86d923e0c": "Yintroduced"
  },
  "changeHistoryDetails": {
    "aa45faf0b20c922b0d147ece9fa01fb95a5b0dec": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12942. Synchronization issue in FSDataSetImpl#moveBlock. Contributed by Ajay Kumar.\n",
      "commitDate": "01/02/18 6:03 PM",
      "commitName": "aa45faf0b20c922b0d147ece9fa01fb95a5b0dec",
      "commitAuthor": "Anu Engineer",
      "commitDateOld": "19/01/18 5:51 PM",
      "commitNameOld": "62c9e7fa99da1b1c8af222436102b8dea02fcde8",
      "commitAuthorOld": "Chen Liang",
      "daysBetweenCommits": 13.01,
      "commitsBetweenForRepo": 101,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,23 @@\n   private void removeOldReplica(ReplicaInfo replicaInfo,\n       ReplicaInfo newReplicaInfo, final String bpid) {\n     // Before deleting the files from old storage we must notify the\n     // NN that the files are on the new storage. Else a blockReport from\n     // the transient storage might cause the NN to think the blocks are lost.\n     // Replicas must be evicted from client short-circuit caches, because the\n     // storage will no longer be same, and thus will require validating\n     // checksum.  This also stops a client from holding file descriptors,\n     // which would prevent the OS from reclaiming the memory.\n     ExtendedBlock extendedBlock \u003d\n         new ExtendedBlock(bpid, newReplicaInfo);\n     datanode.getShortCircuitRegistry().processBlockInvalidation(\n         ExtendedBlockId.fromExtendedBlock(extendedBlock));\n     datanode.notifyNamenodeReceivedBlock(\n         extendedBlock, null, newReplicaInfo.getStorageUuid(),\n         newReplicaInfo.isOnTransientStorage());\n \n     // Remove the old replicas\n-    if (replicaInfo.deleteBlockData() || !replicaInfo.blockDataExists()) {\n-      FsVolumeImpl volume \u003d (FsVolumeImpl) replicaInfo.getVolume();\n-      volume.onBlockFileDeletion(bpid, replicaInfo.getBytesOnDisk());\n-      if (replicaInfo.deleteMetadata() || !replicaInfo.metadataExists()) {\n-        volume.onMetaFileDeletion(bpid, replicaInfo.getMetadataLength());\n-      }\n-    }\n+    cleanupReplica(bpid, replicaInfo);\n \n     // If deletion failed then the directory scanner will cleanup the blocks\n     // eventually.\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void removeOldReplica(ReplicaInfo replicaInfo,\n      ReplicaInfo newReplicaInfo, final String bpid) {\n    // Before deleting the files from old storage we must notify the\n    // NN that the files are on the new storage. Else a blockReport from\n    // the transient storage might cause the NN to think the blocks are lost.\n    // Replicas must be evicted from client short-circuit caches, because the\n    // storage will no longer be same, and thus will require validating\n    // checksum.  This also stops a client from holding file descriptors,\n    // which would prevent the OS from reclaiming the memory.\n    ExtendedBlock extendedBlock \u003d\n        new ExtendedBlock(bpid, newReplicaInfo);\n    datanode.getShortCircuitRegistry().processBlockInvalidation(\n        ExtendedBlockId.fromExtendedBlock(extendedBlock));\n    datanode.notifyNamenodeReceivedBlock(\n        extendedBlock, null, newReplicaInfo.getStorageUuid(),\n        newReplicaInfo.isOnTransientStorage());\n\n    // Remove the old replicas\n    cleanupReplica(bpid, replicaInfo);\n\n    // If deletion failed then the directory scanner will cleanup the blocks\n    // eventually.\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "86c9862bec0248d671e657aa56094a2919b8ac14": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "13/09/16 12:54 PM",
      "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
      "commitAuthor": "Lei Xu",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
          "commitDate": "13/09/16 12:54 PM",
          "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "10/09/16 6:22 PM",
          "commitNameOld": "a99bf26a0899bcc4307c3a242c8414eaef555aa7",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 2.77,
          "commitsBetweenForRepo": 15,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,30 +1,29 @@\n   private void removeOldReplica(ReplicaInfo replicaInfo,\n-      ReplicaInfo newReplicaInfo, File blockFile, File metaFile,\n-      long blockFileUsed, long metaFileUsed, final String bpid) {\n+      ReplicaInfo newReplicaInfo, final String bpid) {\n     // Before deleting the files from old storage we must notify the\n     // NN that the files are on the new storage. Else a blockReport from\n     // the transient storage might cause the NN to think the blocks are lost.\n     // Replicas must be evicted from client short-circuit caches, because the\n     // storage will no longer be same, and thus will require validating\n     // checksum.  This also stops a client from holding file descriptors,\n     // which would prevent the OS from reclaiming the memory.\n     ExtendedBlock extendedBlock \u003d\n         new ExtendedBlock(bpid, newReplicaInfo);\n     datanode.getShortCircuitRegistry().processBlockInvalidation(\n         ExtendedBlockId.fromExtendedBlock(extendedBlock));\n     datanode.notifyNamenodeReceivedBlock(\n         extendedBlock, null, newReplicaInfo.getStorageUuid(),\n         newReplicaInfo.isOnTransientStorage());\n \n     // Remove the old replicas\n-    if (blockFile.delete() || !blockFile.exists()) {\n+    if (replicaInfo.deleteBlockData() || !replicaInfo.blockDataExists()) {\n       FsVolumeImpl volume \u003d (FsVolumeImpl) replicaInfo.getVolume();\n-      volume.onBlockFileDeletion(bpid, blockFileUsed);\n-      if (metaFile.delete() || !metaFile.exists()) {\n-        volume.onMetaFileDeletion(bpid, metaFileUsed);\n+      volume.onBlockFileDeletion(bpid, replicaInfo.getBytesOnDisk());\n+      if (replicaInfo.deleteMetadata() || !replicaInfo.metadataExists()) {\n+        volume.onMetaFileDeletion(bpid, replicaInfo.getMetadataLength());\n       }\n     }\n \n     // If deletion failed then the directory scanner will cleanup the blocks\n     // eventually.\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void removeOldReplica(ReplicaInfo replicaInfo,\n      ReplicaInfo newReplicaInfo, final String bpid) {\n    // Before deleting the files from old storage we must notify the\n    // NN that the files are on the new storage. Else a blockReport from\n    // the transient storage might cause the NN to think the blocks are lost.\n    // Replicas must be evicted from client short-circuit caches, because the\n    // storage will no longer be same, and thus will require validating\n    // checksum.  This also stops a client from holding file descriptors,\n    // which would prevent the OS from reclaiming the memory.\n    ExtendedBlock extendedBlock \u003d\n        new ExtendedBlock(bpid, newReplicaInfo);\n    datanode.getShortCircuitRegistry().processBlockInvalidation(\n        ExtendedBlockId.fromExtendedBlock(extendedBlock));\n    datanode.notifyNamenodeReceivedBlock(\n        extendedBlock, null, newReplicaInfo.getStorageUuid(),\n        newReplicaInfo.isOnTransientStorage());\n\n    // Remove the old replicas\n    if (replicaInfo.deleteBlockData() || !replicaInfo.blockDataExists()) {\n      FsVolumeImpl volume \u003d (FsVolumeImpl) replicaInfo.getVolume();\n      volume.onBlockFileDeletion(bpid, replicaInfo.getBytesOnDisk());\n      if (replicaInfo.deleteMetadata() || !replicaInfo.metadataExists()) {\n        volume.onMetaFileDeletion(bpid, replicaInfo.getMetadataLength());\n      }\n    }\n\n    // If deletion failed then the directory scanner will cleanup the blocks\n    // eventually.\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[replicaInfo-ReplicaInfo, newReplicaInfo-ReplicaInfo, blockFile-File, metaFile-File, blockFileUsed-long, metaFileUsed-long, bpid-String(modifiers-final)]",
            "newValue": "[replicaInfo-ReplicaInfo, newReplicaInfo-ReplicaInfo, bpid-String(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
          "commitDate": "13/09/16 12:54 PM",
          "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
          "commitAuthor": "Lei Xu",
          "commitDateOld": "10/09/16 6:22 PM",
          "commitNameOld": "a99bf26a0899bcc4307c3a242c8414eaef555aa7",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 2.77,
          "commitsBetweenForRepo": 15,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,30 +1,29 @@\n   private void removeOldReplica(ReplicaInfo replicaInfo,\n-      ReplicaInfo newReplicaInfo, File blockFile, File metaFile,\n-      long blockFileUsed, long metaFileUsed, final String bpid) {\n+      ReplicaInfo newReplicaInfo, final String bpid) {\n     // Before deleting the files from old storage we must notify the\n     // NN that the files are on the new storage. Else a blockReport from\n     // the transient storage might cause the NN to think the blocks are lost.\n     // Replicas must be evicted from client short-circuit caches, because the\n     // storage will no longer be same, and thus will require validating\n     // checksum.  This also stops a client from holding file descriptors,\n     // which would prevent the OS from reclaiming the memory.\n     ExtendedBlock extendedBlock \u003d\n         new ExtendedBlock(bpid, newReplicaInfo);\n     datanode.getShortCircuitRegistry().processBlockInvalidation(\n         ExtendedBlockId.fromExtendedBlock(extendedBlock));\n     datanode.notifyNamenodeReceivedBlock(\n         extendedBlock, null, newReplicaInfo.getStorageUuid(),\n         newReplicaInfo.isOnTransientStorage());\n \n     // Remove the old replicas\n-    if (blockFile.delete() || !blockFile.exists()) {\n+    if (replicaInfo.deleteBlockData() || !replicaInfo.blockDataExists()) {\n       FsVolumeImpl volume \u003d (FsVolumeImpl) replicaInfo.getVolume();\n-      volume.onBlockFileDeletion(bpid, blockFileUsed);\n-      if (metaFile.delete() || !metaFile.exists()) {\n-        volume.onMetaFileDeletion(bpid, metaFileUsed);\n+      volume.onBlockFileDeletion(bpid, replicaInfo.getBytesOnDisk());\n+      if (replicaInfo.deleteMetadata() || !replicaInfo.metadataExists()) {\n+        volume.onMetaFileDeletion(bpid, replicaInfo.getMetadataLength());\n       }\n     }\n \n     // If deletion failed then the directory scanner will cleanup the blocks\n     // eventually.\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void removeOldReplica(ReplicaInfo replicaInfo,\n      ReplicaInfo newReplicaInfo, final String bpid) {\n    // Before deleting the files from old storage we must notify the\n    // NN that the files are on the new storage. Else a blockReport from\n    // the transient storage might cause the NN to think the blocks are lost.\n    // Replicas must be evicted from client short-circuit caches, because the\n    // storage will no longer be same, and thus will require validating\n    // checksum.  This also stops a client from holding file descriptors,\n    // which would prevent the OS from reclaiming the memory.\n    ExtendedBlock extendedBlock \u003d\n        new ExtendedBlock(bpid, newReplicaInfo);\n    datanode.getShortCircuitRegistry().processBlockInvalidation(\n        ExtendedBlockId.fromExtendedBlock(extendedBlock));\n    datanode.notifyNamenodeReceivedBlock(\n        extendedBlock, null, newReplicaInfo.getStorageUuid(),\n        newReplicaInfo.isOnTransientStorage());\n\n    // Remove the old replicas\n    if (replicaInfo.deleteBlockData() || !replicaInfo.blockDataExists()) {\n      FsVolumeImpl volume \u003d (FsVolumeImpl) replicaInfo.getVolume();\n      volume.onBlockFileDeletion(bpid, replicaInfo.getBytesOnDisk());\n      if (replicaInfo.deleteMetadata() || !replicaInfo.metadataExists()) {\n        volume.onMetaFileDeletion(bpid, replicaInfo.getMetadataLength());\n      }\n    }\n\n    // If deletion failed then the directory scanner will cleanup the blocks\n    // eventually.\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "d1d4e16690cc85f7f22fbead9cf596260819b561": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9710. DN can be configured to send block receipt IBRs in batches.\n",
      "commitDate": "26/02/16 3:32 PM",
      "commitName": "d1d4e16690cc85f7f22fbead9cf596260819b561",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "21/02/16 7:59 PM",
      "commitNameOld": "342c9572bf6a623287f34c5cc0bc3be6038c191a",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 4.81,
      "commitsBetweenForRepo": 44,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,30 @@\n   private void removeOldReplica(ReplicaInfo replicaInfo,\n       ReplicaInfo newReplicaInfo, File blockFile, File metaFile,\n       long blockFileUsed, long metaFileUsed, final String bpid) {\n     // Before deleting the files from old storage we must notify the\n     // NN that the files are on the new storage. Else a blockReport from\n     // the transient storage might cause the NN to think the blocks are lost.\n     // Replicas must be evicted from client short-circuit caches, because the\n     // storage will no longer be same, and thus will require validating\n     // checksum.  This also stops a client from holding file descriptors,\n     // which would prevent the OS from reclaiming the memory.\n     ExtendedBlock extendedBlock \u003d\n         new ExtendedBlock(bpid, newReplicaInfo);\n     datanode.getShortCircuitRegistry().processBlockInvalidation(\n         ExtendedBlockId.fromExtendedBlock(extendedBlock));\n     datanode.notifyNamenodeReceivedBlock(\n-        extendedBlock, null, newReplicaInfo.getStorageUuid());\n+        extendedBlock, null, newReplicaInfo.getStorageUuid(),\n+        newReplicaInfo.isOnTransientStorage());\n \n     // Remove the old replicas\n     if (blockFile.delete() || !blockFile.exists()) {\n       FsVolumeImpl volume \u003d (FsVolumeImpl) replicaInfo.getVolume();\n       volume.onBlockFileDeletion(bpid, blockFileUsed);\n       if (metaFile.delete() || !metaFile.exists()) {\n         volume.onMetaFileDeletion(bpid, metaFileUsed);\n       }\n     }\n \n     // If deletion failed then the directory scanner will cleanup the blocks\n     // eventually.\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void removeOldReplica(ReplicaInfo replicaInfo,\n      ReplicaInfo newReplicaInfo, File blockFile, File metaFile,\n      long blockFileUsed, long metaFileUsed, final String bpid) {\n    // Before deleting the files from old storage we must notify the\n    // NN that the files are on the new storage. Else a blockReport from\n    // the transient storage might cause the NN to think the blocks are lost.\n    // Replicas must be evicted from client short-circuit caches, because the\n    // storage will no longer be same, and thus will require validating\n    // checksum.  This also stops a client from holding file descriptors,\n    // which would prevent the OS from reclaiming the memory.\n    ExtendedBlock extendedBlock \u003d\n        new ExtendedBlock(bpid, newReplicaInfo);\n    datanode.getShortCircuitRegistry().processBlockInvalidation(\n        ExtendedBlockId.fromExtendedBlock(extendedBlock));\n    datanode.notifyNamenodeReceivedBlock(\n        extendedBlock, null, newReplicaInfo.getStorageUuid(),\n        newReplicaInfo.isOnTransientStorage());\n\n    // Remove the old replicas\n    if (blockFile.delete() || !blockFile.exists()) {\n      FsVolumeImpl volume \u003d (FsVolumeImpl) replicaInfo.getVolume();\n      volume.onBlockFileDeletion(bpid, blockFileUsed);\n      if (metaFile.delete() || !metaFile.exists()) {\n        volume.onMetaFileDeletion(bpid, metaFileUsed);\n      }\n    }\n\n    // If deletion failed then the directory scanner will cleanup the blocks\n    // eventually.\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "e453989a5722e653bd97e3e54f9bbdffc9454fba": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8157. Writes to RAM DISK reserve locked memory for block files. (Arpit Agarwal)\n",
      "commitDate": "16/05/15 9:05 AM",
      "commitName": "e453989a5722e653bd97e3e54f9bbdffc9454fba",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "06/05/15 9:11 PM",
      "commitNameOld": "6633a8474d7e92fa028ede8fd6c6e41b6c5887f5",
      "commitAuthorOld": "cnauroth",
      "daysBetweenCommits": 9.5,
      "commitsBetweenForRepo": 157,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,29 @@\n   private void removeOldReplica(ReplicaInfo replicaInfo,\n       ReplicaInfo newReplicaInfo, File blockFile, File metaFile,\n       long blockFileUsed, long metaFileUsed, final String bpid) {\n     // Before deleting the files from old storage we must notify the\n     // NN that the files are on the new storage. Else a blockReport from\n     // the transient storage might cause the NN to think the blocks are lost.\n     // Replicas must be evicted from client short-circuit caches, because the\n     // storage will no longer be same, and thus will require validating\n     // checksum.  This also stops a client from holding file descriptors,\n     // which would prevent the OS from reclaiming the memory.\n     ExtendedBlock extendedBlock \u003d\n         new ExtendedBlock(bpid, newReplicaInfo);\n     datanode.getShortCircuitRegistry().processBlockInvalidation(\n         ExtendedBlockId.fromExtendedBlock(extendedBlock));\n     datanode.notifyNamenodeReceivedBlock(\n         extendedBlock, null, newReplicaInfo.getStorageUuid());\n \n     // Remove the old replicas\n     if (blockFile.delete() || !blockFile.exists()) {\n-      ((FsVolumeImpl) replicaInfo.getVolume()).decDfsUsed(bpid, blockFileUsed);\n+      FsVolumeImpl volume \u003d (FsVolumeImpl) replicaInfo.getVolume();\n+      volume.onBlockFileDeletion(bpid, blockFileUsed);\n       if (metaFile.delete() || !metaFile.exists()) {\n-        ((FsVolumeImpl) replicaInfo.getVolume()).decDfsUsed(bpid, metaFileUsed);\n+        volume.onMetaFileDeletion(bpid, metaFileUsed);\n       }\n     }\n \n     // If deletion failed then the directory scanner will cleanup the blocks\n     // eventually.\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void removeOldReplica(ReplicaInfo replicaInfo,\n      ReplicaInfo newReplicaInfo, File blockFile, File metaFile,\n      long blockFileUsed, long metaFileUsed, final String bpid) {\n    // Before deleting the files from old storage we must notify the\n    // NN that the files are on the new storage. Else a blockReport from\n    // the transient storage might cause the NN to think the blocks are lost.\n    // Replicas must be evicted from client short-circuit caches, because the\n    // storage will no longer be same, and thus will require validating\n    // checksum.  This also stops a client from holding file descriptors,\n    // which would prevent the OS from reclaiming the memory.\n    ExtendedBlock extendedBlock \u003d\n        new ExtendedBlock(bpid, newReplicaInfo);\n    datanode.getShortCircuitRegistry().processBlockInvalidation(\n        ExtendedBlockId.fromExtendedBlock(extendedBlock));\n    datanode.notifyNamenodeReceivedBlock(\n        extendedBlock, null, newReplicaInfo.getStorageUuid());\n\n    // Remove the old replicas\n    if (blockFile.delete() || !blockFile.exists()) {\n      FsVolumeImpl volume \u003d (FsVolumeImpl) replicaInfo.getVolume();\n      volume.onBlockFileDeletion(bpid, blockFileUsed);\n      if (metaFile.delete() || !metaFile.exists()) {\n        volume.onMetaFileDeletion(bpid, metaFileUsed);\n      }\n    }\n\n    // If deletion failed then the directory scanner will cleanup the blocks\n    // eventually.\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "058af60c56207907f2bedf76df4284e86d923e0c": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-7310. Mover can give first priority to local DN if it has target storage type available in local DN. (Vinayakumar B via umamahesh)\n",
      "commitDate": "26/11/14 9:57 AM",
      "commitName": "058af60c56207907f2bedf76df4284e86d923e0c",
      "commitAuthor": "Uma Maheswara Rao G",
      "diff": "@@ -0,0 +1,28 @@\n+  private void removeOldReplica(ReplicaInfo replicaInfo,\n+      ReplicaInfo newReplicaInfo, File blockFile, File metaFile,\n+      long blockFileUsed, long metaFileUsed, final String bpid) {\n+    // Before deleting the files from old storage we must notify the\n+    // NN that the files are on the new storage. Else a blockReport from\n+    // the transient storage might cause the NN to think the blocks are lost.\n+    // Replicas must be evicted from client short-circuit caches, because the\n+    // storage will no longer be same, and thus will require validating\n+    // checksum.  This also stops a client from holding file descriptors,\n+    // which would prevent the OS from reclaiming the memory.\n+    ExtendedBlock extendedBlock \u003d\n+        new ExtendedBlock(bpid, newReplicaInfo);\n+    datanode.getShortCircuitRegistry().processBlockInvalidation(\n+        ExtendedBlockId.fromExtendedBlock(extendedBlock));\n+    datanode.notifyNamenodeReceivedBlock(\n+        extendedBlock, null, newReplicaInfo.getStorageUuid());\n+\n+    // Remove the old replicas\n+    if (blockFile.delete() || !blockFile.exists()) {\n+      ((FsVolumeImpl) replicaInfo.getVolume()).decDfsUsed(bpid, blockFileUsed);\n+      if (metaFile.delete() || !metaFile.exists()) {\n+        ((FsVolumeImpl) replicaInfo.getVolume()).decDfsUsed(bpid, metaFileUsed);\n+      }\n+    }\n+\n+    // If deletion failed then the directory scanner will cleanup the blocks\n+    // eventually.\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void removeOldReplica(ReplicaInfo replicaInfo,\n      ReplicaInfo newReplicaInfo, File blockFile, File metaFile,\n      long blockFileUsed, long metaFileUsed, final String bpid) {\n    // Before deleting the files from old storage we must notify the\n    // NN that the files are on the new storage. Else a blockReport from\n    // the transient storage might cause the NN to think the blocks are lost.\n    // Replicas must be evicted from client short-circuit caches, because the\n    // storage will no longer be same, and thus will require validating\n    // checksum.  This also stops a client from holding file descriptors,\n    // which would prevent the OS from reclaiming the memory.\n    ExtendedBlock extendedBlock \u003d\n        new ExtendedBlock(bpid, newReplicaInfo);\n    datanode.getShortCircuitRegistry().processBlockInvalidation(\n        ExtendedBlockId.fromExtendedBlock(extendedBlock));\n    datanode.notifyNamenodeReceivedBlock(\n        extendedBlock, null, newReplicaInfo.getStorageUuid());\n\n    // Remove the old replicas\n    if (blockFile.delete() || !blockFile.exists()) {\n      ((FsVolumeImpl) replicaInfo.getVolume()).decDfsUsed(bpid, blockFileUsed);\n      if (metaFile.delete() || !metaFile.exists()) {\n        ((FsVolumeImpl) replicaInfo.getVolume()).decDfsUsed(bpid, metaFileUsed);\n      }\n    }\n\n    // If deletion failed then the directory scanner will cleanup the blocks\n    // eventually.\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java"
    }
  }
}