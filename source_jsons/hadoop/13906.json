{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "updateNeededReconstructions",
  "functionId": "updateNeededReconstructions___block-BlockInfo(modifiers-final)__curReplicasDelta-int(modifiers-final)__expectedReplicasDelta-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 4621,
  "functionEndLine": 4644,
  "numCommitsSeen": 1046,
  "timeTaken": 22066,
  "changeHistory": [
    "ad1e3e4d9f105fac246ce1bdae80e92e013b8ba5",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "8078a5efd0fe26b82c3768e06ccd2faddc619a7f",
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
    "5411dc559d5f73e4153e76fdff94a26869c17a37",
    "715b9c649982bff91d1f9eae656ba3b82178e1a3",
    "745d04be59accf80feda0ad38efcc74ba362f2ca",
    "6e3fcffe291faec40fa9214f4880a35a952836c4",
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b",
    "0c1450ca5d922b5bf713bb8bb17459dc11a97330",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "371f4a59059322000a40eb4bdf5386b96b626ece",
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "ad1e3e4d9f105fac246ce1bdae80e92e013b8ba5": "Ybodychange",
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ybodychange",
    "8078a5efd0fe26b82c3768e06ccd2faddc619a7f": "Ybodychange",
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd": "Ybodychange",
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": "Ymultichange(Yrename,Ybodychange)",
    "5411dc559d5f73e4153e76fdff94a26869c17a37": "Ybodychange",
    "715b9c649982bff91d1f9eae656ba3b82178e1a3": "Ybodychange",
    "745d04be59accf80feda0ad38efcc74ba362f2ca": "Ybodychange",
    "6e3fcffe291faec40fa9214f4880a35a952836c4": "Yparameterchange",
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b": "Ybodychange",
    "0c1450ca5d922b5bf713bb8bb17459dc11a97330": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "371f4a59059322000a40eb4bdf5386b96b626ece": "Ymultichange(Ymodifierchange,Yparametermetachange)",
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a": "Ymultichange(Yfilerename,Ymodifierchange)",
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13": "Ymultichange(Yfilerename,Ymodifierchange)",
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d": "Ymultichange(Yfilerename,Ymodifierchange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "ad1e3e4d9f105fac246ce1bdae80e92e013b8ba5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11755. Underconstruction blocks can be considered missing. Contributed by Nathan Roberts.\n",
      "commitDate": "10/05/17 12:15 PM",
      "commitName": "ad1e3e4d9f105fac246ce1bdae80e92e013b8ba5",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "05/05/17 12:01 PM",
      "commitNameOld": "a3954ccab148bddc290cb96528e63ff19799bcc9",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 5.01,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,24 @@\n   private void updateNeededReconstructions(final BlockInfo block,\n       final int curReplicasDelta, int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n-      if (!isPopulatingReplQueues()) {\n+      if (!isPopulatingReplQueues() || !block.isComplete()) {\n         return;\n       }\n       NumberReplicas repl \u003d countNodes(block);\n       int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n       int curExpectedReplicas \u003d getExpectedRedundancyNum(block);\n       if (!hasEnoughEffectiveReplicas(block, repl, pendingNum)) {\n         neededReconstruction.update(block, repl.liveReplicas() + pendingNum,\n             repl.readOnlyReplicas(), repl.outOfServiceReplicas(),\n             curExpectedReplicas, curReplicasDelta, expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas() + pendingNum - curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReconstruction.remove(block, oldReplicas, repl.readOnlyReplicas(),\n             repl.outOfServiceReplicas(), oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void updateNeededReconstructions(final BlockInfo block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      if (!isPopulatingReplQueues() || !block.isComplete()) {\n        return;\n      }\n      NumberReplicas repl \u003d countNodes(block);\n      int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n      int curExpectedReplicas \u003d getExpectedRedundancyNum(block);\n      if (!hasEnoughEffectiveReplicas(block, repl, pendingNum)) {\n        neededReconstruction.update(block, repl.liveReplicas() + pendingNum,\n            repl.readOnlyReplicas(), repl.outOfServiceReplicas(),\n            curExpectedReplicas, curReplicasDelta, expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas() + pendingNum - curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReconstruction.remove(block, oldReplicas, repl.readOnlyReplicas(),\n            repl.outOfServiceReplicas(), oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "14/10/16 6:13 PM",
      "commitNameOld": "391ce535a739dc92cb90017d759217265a4fd969",
      "commitAuthorOld": "Vinitha Reddy Gankidi",
      "daysBetweenCommits": 2.98,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,24 @@\n   private void updateNeededReconstructions(final BlockInfo block,\n       final int curReplicasDelta, int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       if (!isPopulatingReplQueues()) {\n         return;\n       }\n       NumberReplicas repl \u003d countNodes(block);\n       int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n-      int curExpectedReplicas \u003d getRedundancy(block);\n-      if (!hasEnoughEffectiveReplicas(block, repl, pendingNum,\n-          curExpectedReplicas)) {\n+      int curExpectedReplicas \u003d getExpectedRedundancyNum(block);\n+      if (!hasEnoughEffectiveReplicas(block, repl, pendingNum)) {\n         neededReconstruction.update(block, repl.liveReplicas() + pendingNum,\n-            repl.readOnlyReplicas(), repl.decommissionedAndDecommissioning(),\n+            repl.readOnlyReplicas(), repl.outOfServiceReplicas(),\n             curExpectedReplicas, curReplicasDelta, expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas() + pendingNum - curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReconstruction.remove(block, oldReplicas, repl.readOnlyReplicas(),\n-            repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n+            repl.outOfServiceReplicas(), oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void updateNeededReconstructions(final BlockInfo block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      if (!isPopulatingReplQueues()) {\n        return;\n      }\n      NumberReplicas repl \u003d countNodes(block);\n      int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n      int curExpectedReplicas \u003d getExpectedRedundancyNum(block);\n      if (!hasEnoughEffectiveReplicas(block, repl, pendingNum)) {\n        neededReconstruction.update(block, repl.liveReplicas() + pendingNum,\n            repl.readOnlyReplicas(), repl.outOfServiceReplicas(),\n            curExpectedReplicas, curReplicasDelta, expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas() + pendingNum - curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReconstruction.remove(block, oldReplicas, repl.readOnlyReplicas(),\n            repl.outOfServiceReplicas(), oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "8078a5efd0fe26b82c3768e06ccd2faddc619a7f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10810. setreplication removing block from underconstrcution temporarily. Contributed by Brahma Reddy Battula\n",
      "commitDate": "03/10/16 6:50 PM",
      "commitName": "8078a5efd0fe26b82c3768e06ccd2faddc619a7f",
      "commitAuthor": "Mingliang Liu",
      "commitDateOld": "03/10/16 9:27 AM",
      "commitNameOld": "744208431f7365bf054e6b773b86af2583001e1d",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.39,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,25 @@\n   private void updateNeededReconstructions(final BlockInfo block,\n       final int curReplicasDelta, int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       if (!isPopulatingReplQueues()) {\n         return;\n       }\n       NumberReplicas repl \u003d countNodes(block);\n+      int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n       int curExpectedReplicas \u003d getRedundancy(block);\n-      if (isNeededReconstruction(block, repl.liveReplicas())) {\n-        neededReconstruction.update(block, repl.liveReplicas(),\n+      if (!hasEnoughEffectiveReplicas(block, repl, pendingNum,\n+          curExpectedReplicas)) {\n+        neededReconstruction.update(block, repl.liveReplicas() + pendingNum,\n             repl.readOnlyReplicas(), repl.decommissionedAndDecommissioning(),\n             curExpectedReplicas, curReplicasDelta, expectedReplicasDelta);\n       } else {\n-        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n+        int oldReplicas \u003d repl.liveReplicas() + pendingNum - curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReconstruction.remove(block, oldReplicas, repl.readOnlyReplicas(),\n             repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void updateNeededReconstructions(final BlockInfo block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      if (!isPopulatingReplQueues()) {\n        return;\n      }\n      NumberReplicas repl \u003d countNodes(block);\n      int pendingNum \u003d pendingReconstruction.getNumReplicas(block);\n      int curExpectedReplicas \u003d getRedundancy(block);\n      if (!hasEnoughEffectiveReplicas(block, repl, pendingNum,\n          curExpectedReplicas)) {\n        neededReconstruction.update(block, repl.liveReplicas() + pendingNum,\n            repl.readOnlyReplicas(), repl.decommissionedAndDecommissioning(),\n            curExpectedReplicas, curReplicasDelta, expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas() + pendingNum - curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReconstruction.remove(block, oldReplicas, repl.readOnlyReplicas(),\n            repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "8c84a2a93c22a93b4ff46dd917f6efb995675fbd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10236. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-3]. Contributed by Rakesh R.\n",
      "commitDate": "26/05/16 4:50 PM",
      "commitName": "8c84a2a93c22a93b4ff46dd917f6efb995675fbd",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "28/04/16 10:44 AM",
      "commitNameOld": "6243eabb48390fffada2418ade5adf9e0766afbe",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 28.25,
      "commitsBetweenForRepo": 196,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   private void updateNeededReconstructions(final BlockInfo block,\n       final int curReplicasDelta, int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       if (!isPopulatingReplQueues()) {\n         return;\n       }\n       NumberReplicas repl \u003d countNodes(block);\n-      int curExpectedReplicas \u003d getReplication(block);\n+      int curExpectedReplicas \u003d getRedundancy(block);\n       if (isNeededReconstruction(block, repl.liveReplicas())) {\n         neededReconstruction.update(block, repl.liveReplicas(),\n             repl.readOnlyReplicas(), repl.decommissionedAndDecommissioning(),\n             curExpectedReplicas, curReplicasDelta, expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReconstruction.remove(block, oldReplicas, repl.readOnlyReplicas(),\n             repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void updateNeededReconstructions(final BlockInfo block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      if (!isPopulatingReplQueues()) {\n        return;\n      }\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getRedundancy(block);\n      if (isNeededReconstruction(block, repl.liveReplicas())) {\n        neededReconstruction.update(block, repl.liveReplicas(),\n            repl.readOnlyReplicas(), repl.decommissionedAndDecommissioning(),\n            curExpectedReplicas, curReplicasDelta, expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReconstruction.remove(block, oldReplicas, repl.readOnlyReplicas(),\n            repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "32d043d9c5f4615058ea4f65a58ba271ba47fcb5": {
      "type": "Ymultichange(Yrename,Ybodychange)",
      "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
      "commitDate": "16/03/16 4:53 PM",
      "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
          "commitDate": "16/03/16 4:53 PM",
          "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "10/03/16 7:03 PM",
          "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 5.87,
          "commitsBetweenForRepo": 26,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n-  private void updateNeededReplications(final BlockInfo block,\n+  private void updateNeededReconstructions(final BlockInfo block,\n       final int curReplicasDelta, int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       if (!isPopulatingReplQueues()) {\n         return;\n       }\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n-      if (isNeededReplication(block, repl.liveReplicas())) {\n-        neededReplications.update(block, repl.liveReplicas(), repl.readOnlyReplicas(),\n-            repl.decommissionedAndDecommissioning(), curExpectedReplicas,\n-            curReplicasDelta, expectedReplicasDelta);\n+      if (isNeededReconstruction(block, repl.liveReplicas())) {\n+        neededReconstruction.update(block, repl.liveReplicas(),\n+            repl.readOnlyReplicas(), repl.decommissionedAndDecommissioning(),\n+            curExpectedReplicas, curReplicasDelta, expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n-        neededReplications.remove(block, oldReplicas, repl.readOnlyReplicas(),\n+        neededReconstruction.remove(block, oldReplicas, repl.readOnlyReplicas(),\n             repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void updateNeededReconstructions(final BlockInfo block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      if (!isPopulatingReplQueues()) {\n        return;\n      }\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReconstruction(block, repl.liveReplicas())) {\n        neededReconstruction.update(block, repl.liveReplicas(),\n            repl.readOnlyReplicas(), repl.decommissionedAndDecommissioning(),\n            curExpectedReplicas, curReplicasDelta, expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReconstruction.remove(block, oldReplicas, repl.readOnlyReplicas(),\n            repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "updateNeededReplications",
            "newValue": "updateNeededReconstructions"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9857. Erasure Coding: Rename replication-based names in BlockManager to more generic [part-1]. Contributed by Rakesh R.\n",
          "commitDate": "16/03/16 4:53 PM",
          "commitName": "32d043d9c5f4615058ea4f65a58ba271ba47fcb5",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "10/03/16 7:03 PM",
          "commitNameOld": "e01c6ea688e62f25c4310e771a0cd85b53a5fb87",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 5.87,
          "commitsBetweenForRepo": 26,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,23 @@\n-  private void updateNeededReplications(final BlockInfo block,\n+  private void updateNeededReconstructions(final BlockInfo block,\n       final int curReplicasDelta, int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       if (!isPopulatingReplQueues()) {\n         return;\n       }\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n-      if (isNeededReplication(block, repl.liveReplicas())) {\n-        neededReplications.update(block, repl.liveReplicas(), repl.readOnlyReplicas(),\n-            repl.decommissionedAndDecommissioning(), curExpectedReplicas,\n-            curReplicasDelta, expectedReplicasDelta);\n+      if (isNeededReconstruction(block, repl.liveReplicas())) {\n+        neededReconstruction.update(block, repl.liveReplicas(),\n+            repl.readOnlyReplicas(), repl.decommissionedAndDecommissioning(),\n+            curExpectedReplicas, curReplicasDelta, expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n-        neededReplications.remove(block, oldReplicas, repl.readOnlyReplicas(),\n+        neededReconstruction.remove(block, oldReplicas, repl.readOnlyReplicas(),\n             repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void updateNeededReconstructions(final BlockInfo block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      if (!isPopulatingReplQueues()) {\n        return;\n      }\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReconstruction(block, repl.liveReplicas())) {\n        neededReconstruction.update(block, repl.liveReplicas(),\n            repl.readOnlyReplicas(), repl.decommissionedAndDecommissioning(),\n            curExpectedReplicas, curReplicasDelta, expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReconstruction.remove(block, oldReplicas, repl.readOnlyReplicas(),\n            repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "5411dc559d5f73e4153e76fdff94a26869c17a37": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9205. Do not schedule corrupt blocks for replication.  (szetszwo)\n",
      "commitDate": "15/10/15 3:07 AM",
      "commitName": "5411dc559d5f73e4153e76fdff94a26869c17a37",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "14/10/15 4:17 PM",
      "commitNameOld": "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.45,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   private void updateNeededReplications(final BlockInfo block,\n       final int curReplicasDelta, int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       if (!isPopulatingReplQueues()) {\n         return;\n       }\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n       if (isNeededReplication(block, repl.liveReplicas())) {\n-        neededReplications.update(block, repl.liveReplicas(), repl\n-            .decommissionedAndDecommissioning(), curExpectedReplicas,\n+        neededReplications.update(block, repl.liveReplicas(), repl.readOnlyReplicas(),\n+            repl.decommissionedAndDecommissioning(), curExpectedReplicas,\n             curReplicasDelta, expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n-        neededReplications.remove(block, oldReplicas,\n+        neededReplications.remove(block, oldReplicas, repl.readOnlyReplicas(),\n             repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void updateNeededReplications(final BlockInfo block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      if (!isPopulatingReplQueues()) {\n        return;\n      }\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl.readOnlyReplicas(),\n            repl.decommissionedAndDecommissioning(), curExpectedReplicas,\n            curReplicasDelta, expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas, repl.readOnlyReplicas(),\n            repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "715b9c649982bff91d1f9eae656ba3b82178e1a3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8984. Move replication queues related methods in FSNamesystem to BlockManager. Contributed by Haohui Mai.\n",
      "commitDate": "04/09/15 11:45 AM",
      "commitName": "715b9c649982bff91d1f9eae656ba3b82178e1a3",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "02/09/15 1:46 PM",
      "commitNameOld": "afc88b396f06488c331564e0f6987013bb920d3e",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.92,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   private void updateNeededReplications(final BlockInfo block,\n       final int curReplicasDelta, int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n-      if (!namesystem.isPopulatingReplQueues()) {\n+      if (!isPopulatingReplQueues()) {\n         return;\n       }\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n       if (isNeededReplication(block, repl.liveReplicas())) {\n         neededReplications.update(block, repl.liveReplicas(), repl\n             .decommissionedAndDecommissioning(), curExpectedReplicas,\n             curReplicasDelta, expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReplications.remove(block, oldReplicas,\n             repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void updateNeededReplications(final BlockInfo block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      if (!isPopulatingReplQueues()) {\n        return;\n      }\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedAndDecommissioning(), curExpectedReplicas,\n            curReplicasDelta, expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas,\n            repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "745d04be59accf80feda0ad38efcc74ba362f2ca": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8823. Move replication factor into individual blocks. Contributed by Haohui Mai.\n",
      "commitDate": "22/08/15 12:09 AM",
      "commitName": "745d04be59accf80feda0ad38efcc74ba362f2ca",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "19/08/15 3:11 PM",
      "commitNameOld": "4e14f7982a6e57bf08deb3b266806c2b779a157d",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 2.37,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   private void updateNeededReplications(final BlockInfo block,\n       final int curReplicasDelta, int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       if (!namesystem.isPopulatingReplQueues()) {\n         return;\n       }\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n-      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n+      if (isNeededReplication(block, repl.liveReplicas())) {\n         neededReplications.update(block, repl.liveReplicas(), repl\n             .decommissionedAndDecommissioning(), curExpectedReplicas,\n             curReplicasDelta, expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReplications.remove(block, oldReplicas,\n             repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void updateNeededReplications(final BlockInfo block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      if (!namesystem.isPopulatingReplQueues()) {\n        return;\n      }\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedAndDecommissioning(), curExpectedReplicas,\n            curReplicasDelta, expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas,\n            repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "6e3fcffe291faec40fa9214f4880a35a952836c4": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-8608. Merge HDFS-7912 to trunk and branch-2 (track BlockInfo instead of Block in UnderReplicatedBlocks and PendingReplicationBlocks). Contributed by Zhe Zhang.\n",
      "commitDate": "17/06/15 8:05 AM",
      "commitName": "6e3fcffe291faec40fa9214f4880a35a952836c4",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "12/06/15 11:38 AM",
      "commitNameOld": "c17439c2ddd921b63b1635e6f1cba634b8da8557",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.85,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n-  private void updateNeededReplications(final Block block,\n+  private void updateNeededReplications(final BlockInfo block,\n       final int curReplicasDelta, int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       if (!namesystem.isPopulatingReplQueues()) {\n         return;\n       }\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n       if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n         neededReplications.update(block, repl.liveReplicas(), repl\n             .decommissionedAndDecommissioning(), curExpectedReplicas,\n             curReplicasDelta, expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReplications.remove(block, oldReplicas,\n             repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void updateNeededReplications(final BlockInfo block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      if (!namesystem.isPopulatingReplQueues()) {\n        return;\n      }\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedAndDecommissioning(), curExpectedReplicas,\n            curReplicasDelta, expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas,\n            repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldValue": "[block-Block(modifiers-final), curReplicasDelta-int(modifiers-final), expectedReplicasDelta-int]",
        "newValue": "[block-BlockInfo(modifiers-final), curReplicasDelta-int(modifiers-final), expectedReplicasDelta-int]"
      }
    },
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7933. fsck should also report decommissioning replicas. Contributed by Xiaoyu Yao.\n",
      "commitDate": "11/04/15 1:23 PM",
      "commitName": "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b",
      "commitAuthor": "cnauroth",
      "commitDateOld": "10/04/15 4:36 PM",
      "commitNameOld": "36e4cd3be6f7fec8db82d3d1bcb258af470ece2e",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.87,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n   private void updateNeededReplications(final Block block,\n       final int curReplicasDelta, int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       if (!namesystem.isPopulatingReplQueues()) {\n         return;\n       }\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n       if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n         neededReplications.update(block, repl.liveReplicas(), repl\n-            .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n-            expectedReplicasDelta);\n+            .decommissionedAndDecommissioning(), curExpectedReplicas,\n+            curReplicasDelta, expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n-        neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n-                                  oldExpectedReplicas);\n+        neededReplications.remove(block, oldReplicas,\n+            repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void updateNeededReplications(final Block block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      if (!namesystem.isPopulatingReplQueues()) {\n        return;\n      }\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedAndDecommissioning(), curExpectedReplicas,\n            curReplicasDelta, expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas,\n            repl.decommissionedAndDecommissioning(), oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "0c1450ca5d922b5bf713bb8bb17459dc11a97330": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2795. Standby NN takes a long time to recover from a dead DN starting up. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1232285 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/01/12 7:21 PM",
      "commitName": "0c1450ca5d922b5bf713bb8bb17459dc11a97330",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "10/01/12 9:55 PM",
      "commitNameOld": "190dc1c91b0ae0f3f128cc6603e354a3ec83288a",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 5.89,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,23 @@\n   private void updateNeededReplications(final Block block,\n       final int curReplicasDelta, int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n+      if (!namesystem.isPopulatingReplQueues()) {\n+        return;\n+      }\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n       if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n         neededReplications.update(block, repl.liveReplicas(), repl\n             .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n             expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                   oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void updateNeededReplications(final Block block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      if (!namesystem.isPopulatingReplQueues()) {\n        return;\n      }\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n            expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                  oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void updateNeededReplications(final Block block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n            expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                  oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void updateNeededReplications(final Block block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n            expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                  oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "371f4a59059322000a40eb4bdf5386b96b626ece": {
      "type": "Ymultichange(Ymodifierchange,Yparametermetachange)",
      "commitMessage": "HDFS-2228. Move block and datanode code from FSNamesystem to BlockManager and DatanodeManager.  (szetszwo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154899 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/08/11 3:06 AM",
      "commitName": "371f4a59059322000a40eb4bdf5386b96b626ece",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2228. Move block and datanode code from FSNamesystem to BlockManager and DatanodeManager.  (szetszwo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154899 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/08/11 3:06 AM",
          "commitName": "371f4a59059322000a40eb4bdf5386b96b626ece",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "04/08/11 3:55 PM",
          "commitNameOld": "7fac946ac983e31613fd62836c8ac9c4a579210a",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 3.47,
          "commitsBetweenForRepo": 8,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-  public void updateNeededReplications(Block block, int curReplicasDelta,\n-      int expectedReplicasDelta) {\n+  private void updateNeededReplications(final Block block,\n+      final int curReplicasDelta, int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n       if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n         neededReplications.update(block, repl.liveReplicas(), repl\n             .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n             expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                   oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void updateNeededReplications(final Block block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n            expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                  oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Yparametermetachange",
          "commitMessage": "HDFS-2228. Move block and datanode code from FSNamesystem to BlockManager and DatanodeManager.  (szetszwo)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1154899 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "08/08/11 3:06 AM",
          "commitName": "371f4a59059322000a40eb4bdf5386b96b626ece",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "04/08/11 3:55 PM",
          "commitNameOld": "7fac946ac983e31613fd62836c8ac9c4a579210a",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 3.47,
          "commitsBetweenForRepo": 8,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-  public void updateNeededReplications(Block block, int curReplicasDelta,\n-      int expectedReplicasDelta) {\n+  private void updateNeededReplications(final Block block,\n+      final int curReplicasDelta, int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n       if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n         neededReplications.update(block, repl.liveReplicas(), repl\n             .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n             expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                   oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void updateNeededReplications(final Block block,\n      final int curReplicasDelta, int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n            expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                  oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[block-Block, curReplicasDelta-int, expectedReplicasDelta-int]",
            "newValue": "[block-Block(modifiers-final), curReplicasDelta-int(modifiers-final), expectedReplicasDelta-int]"
          }
        }
      ]
    },
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 6:31 PM",
      "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 6:31 PM",
          "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 5:26 PM",
          "commitNameOld": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-  void updateNeededReplications(Block block, int curReplicasDelta,\n+  public void updateNeededReplications(Block block, int curReplicasDelta,\n       int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n       if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n         neededReplications.update(block, repl.liveReplicas(), repl\n             .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n             expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                   oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void updateNeededReplications(Block block, int curReplicasDelta,\n      int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n            expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                  oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 6:31 PM",
          "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 5:26 PM",
          "commitNameOld": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-  void updateNeededReplications(Block block, int curReplicasDelta,\n+  public void updateNeededReplications(Block block, int curReplicasDelta,\n       int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n       if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n         neededReplications.update(block, repl.liveReplicas(), repl\n             .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n             expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                   oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void updateNeededReplications(Block block, int curReplicasDelta,\n      int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n            expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                  oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[public]"
          }
        }
      ]
    },
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 5:26 PM",
      "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 5:26 PM",
          "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 4:57 PM",
          "commitNameOld": "d58e3efe9269efe00c309ed0e9726d2f94bcd03a",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-  public void updateNeededReplications(Block block, int curReplicasDelta,\n+  void updateNeededReplications(Block block, int curReplicasDelta,\n       int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n       if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n         neededReplications.update(block, repl.liveReplicas(), repl\n             .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n             expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                   oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void updateNeededReplications(Block block, int curReplicasDelta,\n      int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n            expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                  oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 5:26 PM",
          "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 4:57 PM",
          "commitNameOld": "d58e3efe9269efe00c309ed0e9726d2f94bcd03a",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-  public void updateNeededReplications(Block block, int curReplicasDelta,\n+  void updateNeededReplications(Block block, int curReplicasDelta,\n       int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n       if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n         neededReplications.update(block, repl.liveReplicas(), repl\n             .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n             expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                   oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void updateNeededReplications(Block block, int curReplicasDelta,\n      int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n            expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                  oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[]"
          }
        }
      ]
    },
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 4:43 PM",
      "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 4:43 PM",
          "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 9:21 AM",
          "commitNameOld": "1834fb99f516b2f2cd5e0ab1f89d407f98a7237a",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.31,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-  void updateNeededReplications(Block block, int curReplicasDelta,\n+  public void updateNeededReplications(Block block, int curReplicasDelta,\n       int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n       if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n         neededReplications.update(block, repl.liveReplicas(), repl\n             .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n             expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                   oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void updateNeededReplications(Block block, int curReplicasDelta,\n      int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n            expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                  oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 4:43 PM",
          "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 9:21 AM",
          "commitNameOld": "1834fb99f516b2f2cd5e0ab1f89d407f98a7237a",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.31,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,20 @@\n-  void updateNeededReplications(Block block, int curReplicasDelta,\n+  public void updateNeededReplications(Block block, int curReplicasDelta,\n       int expectedReplicasDelta) {\n     namesystem.writeLock();\n     try {\n       NumberReplicas repl \u003d countNodes(block);\n       int curExpectedReplicas \u003d getReplication(block);\n       if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n         neededReplications.update(block, repl.liveReplicas(), repl\n             .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n             expectedReplicasDelta);\n       } else {\n         int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n         int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n         neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                   oldExpectedReplicas);\n       }\n     } finally {\n       namesystem.writeUnlock();\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public void updateNeededReplications(Block block, int curReplicasDelta,\n      int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n            expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                  oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[public]"
          }
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,20 @@\n+  void updateNeededReplications(Block block, int curReplicasDelta,\n+      int expectedReplicasDelta) {\n+    namesystem.writeLock();\n+    try {\n+      NumberReplicas repl \u003d countNodes(block);\n+      int curExpectedReplicas \u003d getReplication(block);\n+      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n+        neededReplications.update(block, repl.liveReplicas(), repl\n+            .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n+            expectedReplicasDelta);\n+      } else {\n+        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n+        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n+        neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n+                                  oldExpectedReplicas);\n+      }\n+    } finally {\n+      namesystem.writeUnlock();\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void updateNeededReplications(Block block, int curReplicasDelta,\n      int expectedReplicasDelta) {\n    namesystem.writeLock();\n    try {\n      NumberReplicas repl \u003d countNodes(block);\n      int curExpectedReplicas \u003d getReplication(block);\n      if (isNeededReplication(block, curExpectedReplicas, repl.liveReplicas())) {\n        neededReplications.update(block, repl.liveReplicas(), repl\n            .decommissionedReplicas(), curExpectedReplicas, curReplicasDelta,\n            expectedReplicasDelta);\n      } else {\n        int oldReplicas \u003d repl.liveReplicas()-curReplicasDelta;\n        int oldExpectedReplicas \u003d curExpectedReplicas-expectedReplicasDelta;\n        neededReplications.remove(block, oldReplicas, repl.decommissionedReplicas(),\n                                  oldExpectedReplicas);\n      }\n    } finally {\n      namesystem.writeUnlock();\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java"
    }
  }
}