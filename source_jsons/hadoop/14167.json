{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "QuorumJournalManager.java",
  "functionName": "recoverUnfinalizedSegments",
  "functionId": "recoverUnfinalizedSegments",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java",
  "functionStartLine": 474,
  "functionEndLine": 500,
  "numCommitsSeen": 42,
  "timeTaken": 1637,
  "changeHistory": [
    "663e7484c04c197eed53f10a7808140f1c955277",
    "8a8c9c18d37f0c8b219264796c0df4bcae6f4e38",
    "74d4573a23db5586c6e47ff2277aa7c35237da34"
  ],
  "changeHistoryShort": {
    "663e7484c04c197eed53f10a7808140f1c955277": "Ybodychange",
    "8a8c9c18d37f0c8b219264796c0df4bcae6f4e38": "Ybodychange",
    "74d4573a23db5586c6e47ff2277aa7c35237da34": "Yintroduced"
  },
  "changeHistoryDetails": {
    "663e7484c04c197eed53f10a7808140f1c955277": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3950. QJM: misc TODO cleanup, improved log messages, etc. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1387704 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/09/12 11:52 AM",
      "commitName": "663e7484c04c197eed53f10a7808140f1c955277",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "10/09/12 11:31 PM",
      "commitNameOld": "8a8c9c18d37f0c8b219264796c0df4bcae6f4e38",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 8.51,
      "commitsBetweenForRepo": 40,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,27 @@\n   public void recoverUnfinalizedSegments() throws IOException {\n     Preconditions.checkState(!isActiveWriter, \"already active writer\");\n     \n+    LOG.info(\"Starting recovery process for unclosed journal segments...\");\n     Map\u003cAsyncLogger, NewEpochResponseProto\u003e resps \u003d createNewUniqueEpoch();\n-    LOG.info(\"newEpoch(\" + loggers.getEpoch() + \") responses:\\n\" +\n+    LOG.info(\"Successfully started new epoch \" + loggers.getEpoch());\n+\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"newEpoch(\" + loggers.getEpoch() + \") responses:\\n\" +\n         QuorumCall.mapToString(resps));\n+    }\n     \n     long mostRecentSegmentTxId \u003d Long.MIN_VALUE;\n     for (NewEpochResponseProto r : resps.values()) {\n       if (r.hasLastSegmentTxId()) {\n         mostRecentSegmentTxId \u003d Math.max(mostRecentSegmentTxId,\n             r.getLastSegmentTxId());\n       }\n     }\n     \n     // On a completely fresh system, none of the journals have any\n     // segments, so there\u0027s nothing to recover.\n     if (mostRecentSegmentTxId !\u003d Long.MIN_VALUE) {\n       recoverUnclosedSegment(mostRecentSegmentTxId);\n     }\n     isActiveWriter \u003d true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void recoverUnfinalizedSegments() throws IOException {\n    Preconditions.checkState(!isActiveWriter, \"already active writer\");\n    \n    LOG.info(\"Starting recovery process for unclosed journal segments...\");\n    Map\u003cAsyncLogger, NewEpochResponseProto\u003e resps \u003d createNewUniqueEpoch();\n    LOG.info(\"Successfully started new epoch \" + loggers.getEpoch());\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"newEpoch(\" + loggers.getEpoch() + \") responses:\\n\" +\n        QuorumCall.mapToString(resps));\n    }\n    \n    long mostRecentSegmentTxId \u003d Long.MIN_VALUE;\n    for (NewEpochResponseProto r : resps.values()) {\n      if (r.hasLastSegmentTxId()) {\n        mostRecentSegmentTxId \u003d Math.max(mostRecentSegmentTxId,\n            r.getLastSegmentTxId());\n      }\n    }\n    \n    // On a completely fresh system, none of the journals have any\n    // segments, so there\u0027s nothing to recover.\n    if (mostRecentSegmentTxId !\u003d Long.MIN_VALUE) {\n      recoverUnclosedSegment(mostRecentSegmentTxId);\n    }\n    isActiveWriter \u003d true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java",
      "extendedDetails": {}
    },
    "8a8c9c18d37f0c8b219264796c0df4bcae6f4e38": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3906. QJM: quorum timeout on failover with large log segment. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1383251 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/09/12 11:31 PM",
      "commitName": "8a8c9c18d37f0c8b219264796c0df4bcae6f4e38",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "05/09/12 11:57 PM",
      "commitNameOld": "437948ea1c0c9c61c2b5049b82ffd9525f33be97",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 4.98,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,22 @@\n   public void recoverUnfinalizedSegments() throws IOException {\n     Preconditions.checkState(!isActiveWriter, \"already active writer\");\n     \n-    Map\u003cAsyncLogger, NewEpochResponseProto\u003e resps \u003d\n-        loggers.createNewUniqueEpoch(nsInfo);\n+    Map\u003cAsyncLogger, NewEpochResponseProto\u003e resps \u003d createNewUniqueEpoch();\n     LOG.info(\"newEpoch(\" + loggers.getEpoch() + \") responses:\\n\" +\n         QuorumCall.mapToString(resps));\n     \n     long mostRecentSegmentTxId \u003d Long.MIN_VALUE;\n     for (NewEpochResponseProto r : resps.values()) {\n       if (r.hasLastSegmentTxId()) {\n         mostRecentSegmentTxId \u003d Math.max(mostRecentSegmentTxId,\n             r.getLastSegmentTxId());\n       }\n     }\n     \n     // On a completely fresh system, none of the journals have any\n     // segments, so there\u0027s nothing to recover.\n     if (mostRecentSegmentTxId !\u003d Long.MIN_VALUE) {\n       recoverUnclosedSegment(mostRecentSegmentTxId);\n     }\n     isActiveWriter \u003d true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void recoverUnfinalizedSegments() throws IOException {\n    Preconditions.checkState(!isActiveWriter, \"already active writer\");\n    \n    Map\u003cAsyncLogger, NewEpochResponseProto\u003e resps \u003d createNewUniqueEpoch();\n    LOG.info(\"newEpoch(\" + loggers.getEpoch() + \") responses:\\n\" +\n        QuorumCall.mapToString(resps));\n    \n    long mostRecentSegmentTxId \u003d Long.MIN_VALUE;\n    for (NewEpochResponseProto r : resps.values()) {\n      if (r.hasLastSegmentTxId()) {\n        mostRecentSegmentTxId \u003d Math.max(mostRecentSegmentTxId,\n            r.getLastSegmentTxId());\n      }\n    }\n    \n    // On a completely fresh system, none of the journals have any\n    // segments, so there\u0027s nothing to recover.\n    if (mostRecentSegmentTxId !\u003d Long.MIN_VALUE) {\n      recoverUnclosedSegment(mostRecentSegmentTxId);\n    }\n    isActiveWriter \u003d true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java",
      "extendedDetails": {}
    },
    "74d4573a23db5586c6e47ff2277aa7c35237da34": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-3077. Quorum-based protocol for reading and writing edit logs. Contributed by Todd Lipcon based on initial work from Brandon Li and Hari Mankude.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1363596 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/07/12 5:25 PM",
      "commitName": "74d4573a23db5586c6e47ff2277aa7c35237da34",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,23 @@\n+  public void recoverUnfinalizedSegments() throws IOException {\n+    Preconditions.checkState(!isActiveWriter, \"already active writer\");\n+    \n+    Map\u003cAsyncLogger, NewEpochResponseProto\u003e resps \u003d\n+        loggers.createNewUniqueEpoch(nsInfo);\n+    LOG.info(\"newEpoch(\" + loggers.getEpoch() + \") responses:\\n\" +\n+        QuorumCall.mapToString(resps));\n+    \n+    long mostRecentSegmentTxId \u003d Long.MIN_VALUE;\n+    for (NewEpochResponseProto r : resps.values()) {\n+      if (r.hasLastSegmentTxId()) {\n+        mostRecentSegmentTxId \u003d Math.max(mostRecentSegmentTxId,\n+            r.getLastSegmentTxId());\n+      }\n+    }\n+    \n+    // On a completely fresh system, none of the journals have any\n+    // segments, so there\u0027s nothing to recover.\n+    if (mostRecentSegmentTxId !\u003d Long.MIN_VALUE) {\n+      recoverUnclosedSegment(mostRecentSegmentTxId);\n+    }\n+    isActiveWriter \u003d true;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void recoverUnfinalizedSegments() throws IOException {\n    Preconditions.checkState(!isActiveWriter, \"already active writer\");\n    \n    Map\u003cAsyncLogger, NewEpochResponseProto\u003e resps \u003d\n        loggers.createNewUniqueEpoch(nsInfo);\n    LOG.info(\"newEpoch(\" + loggers.getEpoch() + \") responses:\\n\" +\n        QuorumCall.mapToString(resps));\n    \n    long mostRecentSegmentTxId \u003d Long.MIN_VALUE;\n    for (NewEpochResponseProto r : resps.values()) {\n      if (r.hasLastSegmentTxId()) {\n        mostRecentSegmentTxId \u003d Math.max(mostRecentSegmentTxId,\n            r.getLastSegmentTxId());\n      }\n    }\n    \n    // On a completely fresh system, none of the journals have any\n    // segments, so there\u0027s nothing to recover.\n    if (mostRecentSegmentTxId !\u003d Long.MIN_VALUE) {\n      recoverUnclosedSegment(mostRecentSegmentTxId);\n    }\n    isActiveWriter \u003d true;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java"
    }
  }
}