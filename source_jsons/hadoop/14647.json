{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSTopologyNodeImpl.java",
  "functionName": "add",
  "functionId": "add___n-Node",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java",
  "functionStartLine": 192,
  "functionEndLine": 268,
  "numCommitsSeen": 5,
  "timeTaken": 2412,
  "changeHistory": [
    "97c2e576c91c2316c2b52bfc948bae9bff8ca49f",
    "615ac09499dc0b391cbb99bb0e9877959a9173a6",
    "9832ae0ed8853d29072c9ea7031cd2373e6b16f9",
    "eeca8b0c4e2804b0fee5b012ea14b58383425ec3"
  ],
  "changeHistoryShort": {
    "97c2e576c91c2316c2b52bfc948bae9bff8ca49f": "Ybodychange",
    "615ac09499dc0b391cbb99bb0e9877959a9173a6": "Ybodychange",
    "9832ae0ed8853d29072c9ea7031cd2373e6b16f9": "Yfilerename",
    "eeca8b0c4e2804b0fee5b012ea14b58383425ec3": "Yintroduced"
  },
  "changeHistoryDetails": {
    "97c2e576c91c2316c2b52bfc948bae9bff8ca49f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11530. Use HDFS specific network topology to choose datanode in BlockPlacementPolicyDefault. Contributed by Yiqun Lin and Chen Liang.\n",
      "commitDate": "04/05/17 8:54 PM",
      "commitName": "97c2e576c91c2316c2b52bfc948bae9bff8ca49f",
      "commitAuthor": "Yiqun Lin",
      "commitDateOld": "15/03/17 12:28 PM",
      "commitNameOld": "615ac09499dc0b391cbb99bb0e9877959a9173a6",
      "commitAuthorOld": "Chen Liang",
      "daysBetweenCommits": 50.35,
      "commitsBetweenForRepo": 290,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,77 @@\n   public boolean add(Node n) {\n+    LOG.debug(\"adding node {}\", n.getName());\n     if (!isAncestor(n)) {\n       throw new IllegalArgumentException(n.getName()\n           + \", which is located at \" + n.getNetworkLocation()\n           + \", is not a descendant of \" + getPath(this));\n     }\n     // In HDFS topology, the leaf node should always be DatanodeDescriptor\n     if (!(n instanceof DatanodeDescriptor)) {\n       throw new IllegalArgumentException(\"Unexpected node type \"\n           + n.getClass().getName());\n     }\n     DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor) n;\n     if (isParent(n)) {\n       // this node is the parent of n; add n directly\n       n.setParent(this);\n       n.setLevel(this.level + 1);\n       Node prev \u003d childrenMap.put(n.getName(), n);\n       if (prev !\u003d null) {\n         for(int i\u003d0; i\u003cchildren.size(); i++) {\n           if (children.get(i).getName().equals(n.getName())) {\n             children.set(i, n);\n+            updateExistingDatanode(dnDescriptor);\n             return false;\n           }\n         }\n       }\n       children.add(n);\n       numOfLeaves++;\n       if (!childrenStorageInfo.containsKey(dnDescriptor.getName())) {\n         childrenStorageInfo.put(\n             dnDescriptor.getName(), new EnumMap\u003c\u003e(StorageType.class));\n       }\n       for (StorageType st : dnDescriptor.getStorageTypes()) {\n         childrenStorageInfo.get(dnDescriptor.getName()).put(st, 1);\n         incStorageTypeCount(st);\n       }\n       return true;\n     } else {\n       // find the next ancestor node\n       String parentName \u003d getNextAncestorName(n);\n       InnerNode parentNode \u003d (InnerNode)childrenMap.get(parentName);\n       if (parentNode \u003d\u003d null) {\n         // create a new InnerNode\n         parentNode \u003d createParentNode(parentName);\n         children.add(parentNode);\n         childrenMap.put(parentNode.getName(), parentNode);\n       }\n       // add n to the subtree of the next ancestor node\n       if (parentNode.add(n)) {\n         numOfLeaves++;\n         if (!childrenStorageInfo.containsKey(parentNode.getName())) {\n           childrenStorageInfo.put(\n               parentNode.getName(), new EnumMap\u003c\u003e(StorageType.class));\n           for (StorageType st : dnDescriptor.getStorageTypes()) {\n             childrenStorageInfo.get(parentNode.getName()).put(st, 1);\n           }\n         } else {\n           EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n               childrenStorageInfo.get(parentNode.getName());\n           for (StorageType st : dnDescriptor.getStorageTypes()) {\n             if (currentCount.containsKey(st)) {\n               currentCount.put(st, currentCount.get(st) + 1);\n             } else {\n               currentCount.put(st, 1);\n             }\n           }\n         }\n         for (StorageType st : dnDescriptor.getStorageTypes()) {\n           incStorageTypeCount(st);\n         }\n         return true;\n       } else {\n         return false;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean add(Node n) {\n    LOG.debug(\"adding node {}\", n.getName());\n    if (!isAncestor(n)) {\n      throw new IllegalArgumentException(n.getName()\n          + \", which is located at \" + n.getNetworkLocation()\n          + \", is not a descendant of \" + getPath(this));\n    }\n    // In HDFS topology, the leaf node should always be DatanodeDescriptor\n    if (!(n instanceof DatanodeDescriptor)) {\n      throw new IllegalArgumentException(\"Unexpected node type \"\n          + n.getClass().getName());\n    }\n    DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor) n;\n    if (isParent(n)) {\n      // this node is the parent of n; add n directly\n      n.setParent(this);\n      n.setLevel(this.level + 1);\n      Node prev \u003d childrenMap.put(n.getName(), n);\n      if (prev !\u003d null) {\n        for(int i\u003d0; i\u003cchildren.size(); i++) {\n          if (children.get(i).getName().equals(n.getName())) {\n            children.set(i, n);\n            updateExistingDatanode(dnDescriptor);\n            return false;\n          }\n        }\n      }\n      children.add(n);\n      numOfLeaves++;\n      if (!childrenStorageInfo.containsKey(dnDescriptor.getName())) {\n        childrenStorageInfo.put(\n            dnDescriptor.getName(), new EnumMap\u003c\u003e(StorageType.class));\n      }\n      for (StorageType st : dnDescriptor.getStorageTypes()) {\n        childrenStorageInfo.get(dnDescriptor.getName()).put(st, 1);\n        incStorageTypeCount(st);\n      }\n      return true;\n    } else {\n      // find the next ancestor node\n      String parentName \u003d getNextAncestorName(n);\n      InnerNode parentNode \u003d (InnerNode)childrenMap.get(parentName);\n      if (parentNode \u003d\u003d null) {\n        // create a new InnerNode\n        parentNode \u003d createParentNode(parentName);\n        children.add(parentNode);\n        childrenMap.put(parentNode.getName(), parentNode);\n      }\n      // add n to the subtree of the next ancestor node\n      if (parentNode.add(n)) {\n        numOfLeaves++;\n        if (!childrenStorageInfo.containsKey(parentNode.getName())) {\n          childrenStorageInfo.put(\n              parentNode.getName(), new EnumMap\u003c\u003e(StorageType.class));\n          for (StorageType st : dnDescriptor.getStorageTypes()) {\n            childrenStorageInfo.get(parentNode.getName()).put(st, 1);\n          }\n        } else {\n          EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n              childrenStorageInfo.get(parentNode.getName());\n          for (StorageType st : dnDescriptor.getStorageTypes()) {\n            if (currentCount.containsKey(st)) {\n              currentCount.put(st, currentCount.get(st) + 1);\n            } else {\n              currentCount.put(st, 1);\n            }\n          }\n        }\n        for (StorageType st : dnDescriptor.getStorageTypes()) {\n          incStorageTypeCount(st);\n        }\n        return true;\n      } else {\n        return false;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java",
      "extendedDetails": {}
    },
    "615ac09499dc0b391cbb99bb0e9877959a9173a6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11419. DFSTopologyNodeImpl#chooseRandom optimizations. Contributed by Chen Liang.\n",
      "commitDate": "15/03/17 12:28 PM",
      "commitName": "615ac09499dc0b391cbb99bb0e9877959a9173a6",
      "commitAuthor": "Chen Liang",
      "commitDateOld": "13/03/17 5:30 PM",
      "commitNameOld": "9832ae0ed8853d29072c9ea7031cd2373e6b16f9",
      "commitAuthorOld": "Chen Liang",
      "daysBetweenCommits": 1.79,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,75 @@\n   public boolean add(Node n) {\n     if (!isAncestor(n)) {\n       throw new IllegalArgumentException(n.getName()\n           + \", which is located at \" + n.getNetworkLocation()\n           + \", is not a descendant of \" + getPath(this));\n     }\n     // In HDFS topology, the leaf node should always be DatanodeDescriptor\n     if (!(n instanceof DatanodeDescriptor)) {\n       throw new IllegalArgumentException(\"Unexpected node type \"\n           + n.getClass().getName());\n     }\n     DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor) n;\n     if (isParent(n)) {\n       // this node is the parent of n; add n directly\n       n.setParent(this);\n       n.setLevel(this.level + 1);\n       Node prev \u003d childrenMap.put(n.getName(), n);\n       if (prev !\u003d null) {\n         for(int i\u003d0; i\u003cchildren.size(); i++) {\n           if (children.get(i).getName().equals(n.getName())) {\n             children.set(i, n);\n             return false;\n           }\n         }\n       }\n       children.add(n);\n       numOfLeaves++;\n-      synchronized (childrenStorageInfo) {\n-        if (!childrenStorageInfo.containsKey(dnDescriptor.getName())) {\n-          childrenStorageInfo.put(\n-              dnDescriptor.getName(), new EnumMap\u003c\u003e(StorageType.class));\n-        }\n-        for (StorageType st : dnDescriptor.getStorageTypes()) {\n-          childrenStorageInfo.get(dnDescriptor.getName()).put(st, 1);\n-        }\n+      if (!childrenStorageInfo.containsKey(dnDescriptor.getName())) {\n+        childrenStorageInfo.put(\n+            dnDescriptor.getName(), new EnumMap\u003c\u003e(StorageType.class));\n+      }\n+      for (StorageType st : dnDescriptor.getStorageTypes()) {\n+        childrenStorageInfo.get(dnDescriptor.getName()).put(st, 1);\n+        incStorageTypeCount(st);\n       }\n       return true;\n     } else {\n       // find the next ancestor node\n       String parentName \u003d getNextAncestorName(n);\n       InnerNode parentNode \u003d (InnerNode)childrenMap.get(parentName);\n       if (parentNode \u003d\u003d null) {\n         // create a new InnerNode\n         parentNode \u003d createParentNode(parentName);\n         children.add(parentNode);\n         childrenMap.put(parentNode.getName(), parentNode);\n       }\n       // add n to the subtree of the next ancestor node\n       if (parentNode.add(n)) {\n         numOfLeaves++;\n-        synchronized (childrenStorageInfo) {\n-          if (!childrenStorageInfo.containsKey(parentNode.getName())) {\n-            childrenStorageInfo.put(\n-                parentNode.getName(), new EnumMap\u003c\u003e(StorageType.class));\n-            for (StorageType st : dnDescriptor.getStorageTypes()) {\n-              childrenStorageInfo.get(parentNode.getName()).put(st, 1);\n-            }\n-          } else {\n-            EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n-                childrenStorageInfo.get(parentNode.getName());\n-            for (StorageType st : dnDescriptor.getStorageTypes()) {\n-              if (currentCount.containsKey(st)) {\n-                currentCount.put(st, currentCount.get(st) + 1);\n-              } else {\n-                currentCount.put(st, 1);\n-              }\n+        if (!childrenStorageInfo.containsKey(parentNode.getName())) {\n+          childrenStorageInfo.put(\n+              parentNode.getName(), new EnumMap\u003c\u003e(StorageType.class));\n+          for (StorageType st : dnDescriptor.getStorageTypes()) {\n+            childrenStorageInfo.get(parentNode.getName()).put(st, 1);\n+          }\n+        } else {\n+          EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n+              childrenStorageInfo.get(parentNode.getName());\n+          for (StorageType st : dnDescriptor.getStorageTypes()) {\n+            if (currentCount.containsKey(st)) {\n+              currentCount.put(st, currentCount.get(st) + 1);\n+            } else {\n+              currentCount.put(st, 1);\n             }\n           }\n         }\n+        for (StorageType st : dnDescriptor.getStorageTypes()) {\n+          incStorageTypeCount(st);\n+        }\n         return true;\n       } else {\n         return false;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean add(Node n) {\n    if (!isAncestor(n)) {\n      throw new IllegalArgumentException(n.getName()\n          + \", which is located at \" + n.getNetworkLocation()\n          + \", is not a descendant of \" + getPath(this));\n    }\n    // In HDFS topology, the leaf node should always be DatanodeDescriptor\n    if (!(n instanceof DatanodeDescriptor)) {\n      throw new IllegalArgumentException(\"Unexpected node type \"\n          + n.getClass().getName());\n    }\n    DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor) n;\n    if (isParent(n)) {\n      // this node is the parent of n; add n directly\n      n.setParent(this);\n      n.setLevel(this.level + 1);\n      Node prev \u003d childrenMap.put(n.getName(), n);\n      if (prev !\u003d null) {\n        for(int i\u003d0; i\u003cchildren.size(); i++) {\n          if (children.get(i).getName().equals(n.getName())) {\n            children.set(i, n);\n            return false;\n          }\n        }\n      }\n      children.add(n);\n      numOfLeaves++;\n      if (!childrenStorageInfo.containsKey(dnDescriptor.getName())) {\n        childrenStorageInfo.put(\n            dnDescriptor.getName(), new EnumMap\u003c\u003e(StorageType.class));\n      }\n      for (StorageType st : dnDescriptor.getStorageTypes()) {\n        childrenStorageInfo.get(dnDescriptor.getName()).put(st, 1);\n        incStorageTypeCount(st);\n      }\n      return true;\n    } else {\n      // find the next ancestor node\n      String parentName \u003d getNextAncestorName(n);\n      InnerNode parentNode \u003d (InnerNode)childrenMap.get(parentName);\n      if (parentNode \u003d\u003d null) {\n        // create a new InnerNode\n        parentNode \u003d createParentNode(parentName);\n        children.add(parentNode);\n        childrenMap.put(parentNode.getName(), parentNode);\n      }\n      // add n to the subtree of the next ancestor node\n      if (parentNode.add(n)) {\n        numOfLeaves++;\n        if (!childrenStorageInfo.containsKey(parentNode.getName())) {\n          childrenStorageInfo.put(\n              parentNode.getName(), new EnumMap\u003c\u003e(StorageType.class));\n          for (StorageType st : dnDescriptor.getStorageTypes()) {\n            childrenStorageInfo.get(parentNode.getName()).put(st, 1);\n          }\n        } else {\n          EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n              childrenStorageInfo.get(parentNode.getName());\n          for (StorageType st : dnDescriptor.getStorageTypes()) {\n            if (currentCount.containsKey(st)) {\n              currentCount.put(st, currentCount.get(st) + 1);\n            } else {\n              currentCount.put(st, 1);\n            }\n          }\n        }\n        for (StorageType st : dnDescriptor.getStorageTypes()) {\n          incStorageTypeCount(st);\n        }\n        return true;\n      } else {\n        return false;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java",
      "extendedDetails": {}
    },
    "9832ae0ed8853d29072c9ea7031cd2373e6b16f9": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-11482. Add storage type demand to into DFSNetworkTopology#chooseRandom. Contributed by Chen Liang.\n",
      "commitDate": "13/03/17 5:30 PM",
      "commitName": "9832ae0ed8853d29072c9ea7031cd2373e6b16f9",
      "commitAuthor": "Chen Liang",
      "commitDateOld": "13/03/17 2:24 PM",
      "commitNameOld": "55796a0946f80a35055701a34379e374399009c5",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 0.13,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public boolean add(Node n) {\n    if (!isAncestor(n)) {\n      throw new IllegalArgumentException(n.getName()\n          + \", which is located at \" + n.getNetworkLocation()\n          + \", is not a descendant of \" + getPath(this));\n    }\n    // In HDFS topology, the leaf node should always be DatanodeDescriptor\n    if (!(n instanceof DatanodeDescriptor)) {\n      throw new IllegalArgumentException(\"Unexpected node type \"\n          + n.getClass().getName());\n    }\n    DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor) n;\n    if (isParent(n)) {\n      // this node is the parent of n; add n directly\n      n.setParent(this);\n      n.setLevel(this.level + 1);\n      Node prev \u003d childrenMap.put(n.getName(), n);\n      if (prev !\u003d null) {\n        for(int i\u003d0; i\u003cchildren.size(); i++) {\n          if (children.get(i).getName().equals(n.getName())) {\n            children.set(i, n);\n            return false;\n          }\n        }\n      }\n      children.add(n);\n      numOfLeaves++;\n      synchronized (childrenStorageInfo) {\n        if (!childrenStorageInfo.containsKey(dnDescriptor.getName())) {\n          childrenStorageInfo.put(\n              dnDescriptor.getName(), new EnumMap\u003c\u003e(StorageType.class));\n        }\n        for (StorageType st : dnDescriptor.getStorageTypes()) {\n          childrenStorageInfo.get(dnDescriptor.getName()).put(st, 1);\n        }\n      }\n      return true;\n    } else {\n      // find the next ancestor node\n      String parentName \u003d getNextAncestorName(n);\n      InnerNode parentNode \u003d (InnerNode)childrenMap.get(parentName);\n      if (parentNode \u003d\u003d null) {\n        // create a new InnerNode\n        parentNode \u003d createParentNode(parentName);\n        children.add(parentNode);\n        childrenMap.put(parentNode.getName(), parentNode);\n      }\n      // add n to the subtree of the next ancestor node\n      if (parentNode.add(n)) {\n        numOfLeaves++;\n        synchronized (childrenStorageInfo) {\n          if (!childrenStorageInfo.containsKey(parentNode.getName())) {\n            childrenStorageInfo.put(\n                parentNode.getName(), new EnumMap\u003c\u003e(StorageType.class));\n            for (StorageType st : dnDescriptor.getStorageTypes()) {\n              childrenStorageInfo.get(parentNode.getName()).put(st, 1);\n            }\n          } else {\n            EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n                childrenStorageInfo.get(parentNode.getName());\n            for (StorageType st : dnDescriptor.getStorageTypes()) {\n              if (currentCount.containsKey(st)) {\n                currentCount.put(st, currentCount.get(st) + 1);\n              } else {\n                currentCount.put(st, 1);\n              }\n            }\n          }\n        }\n        return true;\n      } else {\n        return false;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTopologyNodeImpl.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSTopologyNodeImpl.java"
      }
    },
    "eeca8b0c4e2804b0fee5b012ea14b58383425ec3": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11419. HDFS specific network topology classes with storage type info included. Contributed by Chen Liang.\n",
      "commitDate": "02/03/17 9:21 AM",
      "commitName": "eeca8b0c4e2804b0fee5b012ea14b58383425ec3",
      "commitAuthor": "Arpit Agarwal",
      "diff": "@@ -0,0 +1,75 @@\n+  public boolean add(Node n) {\n+    if (!isAncestor(n)) {\n+      throw new IllegalArgumentException(n.getName()\n+          + \", which is located at \" + n.getNetworkLocation()\n+          + \", is not a descendant of \" + getPath(this));\n+    }\n+    // In HDFS topology, the leaf node should always be DatanodeDescriptor\n+    if (!(n instanceof DatanodeDescriptor)) {\n+      throw new IllegalArgumentException(\"Unexpected node type \"\n+          + n.getClass().getName());\n+    }\n+    DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor) n;\n+    if (isParent(n)) {\n+      // this node is the parent of n; add n directly\n+      n.setParent(this);\n+      n.setLevel(this.level + 1);\n+      Node prev \u003d childrenMap.put(n.getName(), n);\n+      if (prev !\u003d null) {\n+        for(int i\u003d0; i\u003cchildren.size(); i++) {\n+          if (children.get(i).getName().equals(n.getName())) {\n+            children.set(i, n);\n+            return false;\n+          }\n+        }\n+      }\n+      children.add(n);\n+      numOfLeaves++;\n+      synchronized (childrenStorageInfo) {\n+        if (!childrenStorageInfo.containsKey(dnDescriptor.getName())) {\n+          childrenStorageInfo.put(\n+              dnDescriptor.getName(), new EnumMap\u003c\u003e(StorageType.class));\n+        }\n+        for (StorageType st : dnDescriptor.getStorageTypes()) {\n+          childrenStorageInfo.get(dnDescriptor.getName()).put(st, 1);\n+        }\n+      }\n+      return true;\n+    } else {\n+      // find the next ancestor node\n+      String parentName \u003d getNextAncestorName(n);\n+      InnerNode parentNode \u003d (InnerNode)childrenMap.get(parentName);\n+      if (parentNode \u003d\u003d null) {\n+        // create a new InnerNode\n+        parentNode \u003d createParentNode(parentName);\n+        children.add(parentNode);\n+        childrenMap.put(parentNode.getName(), parentNode);\n+      }\n+      // add n to the subtree of the next ancestor node\n+      if (parentNode.add(n)) {\n+        numOfLeaves++;\n+        synchronized (childrenStorageInfo) {\n+          if (!childrenStorageInfo.containsKey(parentNode.getName())) {\n+            childrenStorageInfo.put(\n+                parentNode.getName(), new EnumMap\u003c\u003e(StorageType.class));\n+            for (StorageType st : dnDescriptor.getStorageTypes()) {\n+              childrenStorageInfo.get(parentNode.getName()).put(st, 1);\n+            }\n+          } else {\n+            EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n+                childrenStorageInfo.get(parentNode.getName());\n+            for (StorageType st : dnDescriptor.getStorageTypes()) {\n+              if (currentCount.containsKey(st)) {\n+                currentCount.put(st, currentCount.get(st) + 1);\n+              } else {\n+                currentCount.put(st, 1);\n+              }\n+            }\n+          }\n+        }\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public boolean add(Node n) {\n    if (!isAncestor(n)) {\n      throw new IllegalArgumentException(n.getName()\n          + \", which is located at \" + n.getNetworkLocation()\n          + \", is not a descendant of \" + getPath(this));\n    }\n    // In HDFS topology, the leaf node should always be DatanodeDescriptor\n    if (!(n instanceof DatanodeDescriptor)) {\n      throw new IllegalArgumentException(\"Unexpected node type \"\n          + n.getClass().getName());\n    }\n    DatanodeDescriptor dnDescriptor \u003d (DatanodeDescriptor) n;\n    if (isParent(n)) {\n      // this node is the parent of n; add n directly\n      n.setParent(this);\n      n.setLevel(this.level + 1);\n      Node prev \u003d childrenMap.put(n.getName(), n);\n      if (prev !\u003d null) {\n        for(int i\u003d0; i\u003cchildren.size(); i++) {\n          if (children.get(i).getName().equals(n.getName())) {\n            children.set(i, n);\n            return false;\n          }\n        }\n      }\n      children.add(n);\n      numOfLeaves++;\n      synchronized (childrenStorageInfo) {\n        if (!childrenStorageInfo.containsKey(dnDescriptor.getName())) {\n          childrenStorageInfo.put(\n              dnDescriptor.getName(), new EnumMap\u003c\u003e(StorageType.class));\n        }\n        for (StorageType st : dnDescriptor.getStorageTypes()) {\n          childrenStorageInfo.get(dnDescriptor.getName()).put(st, 1);\n        }\n      }\n      return true;\n    } else {\n      // find the next ancestor node\n      String parentName \u003d getNextAncestorName(n);\n      InnerNode parentNode \u003d (InnerNode)childrenMap.get(parentName);\n      if (parentNode \u003d\u003d null) {\n        // create a new InnerNode\n        parentNode \u003d createParentNode(parentName);\n        children.add(parentNode);\n        childrenMap.put(parentNode.getName(), parentNode);\n      }\n      // add n to the subtree of the next ancestor node\n      if (parentNode.add(n)) {\n        numOfLeaves++;\n        synchronized (childrenStorageInfo) {\n          if (!childrenStorageInfo.containsKey(parentNode.getName())) {\n            childrenStorageInfo.put(\n                parentNode.getName(), new EnumMap\u003c\u003e(StorageType.class));\n            for (StorageType st : dnDescriptor.getStorageTypes()) {\n              childrenStorageInfo.get(parentNode.getName()).put(st, 1);\n            }\n          } else {\n            EnumMap\u003cStorageType, Integer\u003e currentCount \u003d\n                childrenStorageInfo.get(parentNode.getName());\n            for (StorageType st : dnDescriptor.getStorageTypes()) {\n              if (currentCount.containsKey(st)) {\n                currentCount.put(st, currentCount.get(st) + 1);\n              } else {\n                currentCount.put(st, 1);\n              }\n            }\n          }\n        }\n        return true;\n      } else {\n        return false;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTopologyNodeImpl.java"
    }
  }
}